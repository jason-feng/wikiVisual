<doc id="15242" url="http://en.wikipedia.org/wiki?curid=15242" title="Instrument flight rules">
Instrument flight rules

Instrument flight rules (IFR) is one of two sets of regulations governing all aspects of civil aviation aircraft operations; the other is visual flight rules (VFR).
FAA's "Instrument Flying Handbook" defines IFR as: "Rules and regulations established by the FAA to govern flight under conditions in which flight by outside visual reference is not safe. IFR flight depends upon flying by reference to instruments in the flight deck, and navigation is accomplished by reference to electronic signals. It is also a term used by pilots and controllers to indicate the type of flight plan an aircraft is flying, such as an IFR or VFR flight plan.
Basic Information.
Visual flight rules.
To put instrument flight rules into context, a brief overview of VFR is necessary. Flights operating under VFR are flown solely by reference to outside visual cues (horizon, buildings, flora, etc.) which permit navigation, orientation, and separation from terrain and other traffic. Thus, cloud ceiling and flight visibility are the most important variables for safe operations during all phases of flight.<ref name='NASA/CR-2000-210288'> 
</ref> The minimum weather conditions for ceiling and visibility for VFR flights are defined in FAR Part 91.155, and vary depending on the type of airspace in which the aircraft is operating, and on whether the flight is conducted during daytime or nighttime. However, typical daytime VFR minimums for most airspace is 3 statute miles of flight visibility and a cloud distance of 500' below, 1,000' above, and 2,000' feet horizontally. Flight conditions reported as equal to or greater than these VFR minimums are referred to as visual meteorological conditions (VMC).
Visual flight rules can be simpler than IFR, and require significantly less training and practice. VFR provides a great degree of freedom, allowing pilots to go where they want, when they want, and allows them a much wider latitude in determining how they get there. Pilots are not required to file a flight plan, do not have to communicate with ATC (unless flying in certain types of "busier" airspace), and are not limited to following predefined published routes or flight procedures.
VFR pilots "may" use cockpit instruments as secondary aids to navigation and orientation, but are not required to. However, any aircraft operating under VFR must have the required equipment on board, as described in FAR Part 91.205 (which includes instruments necessary for IFR flight); but the view outside of the aircraft is the primary source for keeping the aircraft straight and level (orientation), flying to the intended destination (navigation), and not hitting anything (separation).
Instrument flight rules.
Instrument flight rules permit an aircraft to operate in instrument meteorological conditions (IMC) in contrast to VFR. They are also an integral part of flying in class A airspace. "Class A" airspace exists over and near the 48 contiguous U.S. states and Alaska from 18,000 feet above mean sea level to flight level 600 (approximately 60,000 feet in altitude depending on variables such as atmospheric pressure). Flight in "class A" airspace requires pilots and aircraft to be instrument equipped and rated and to be operating under Instrument Flight Rules (IFR). Most jet aircraft operate in "class A" airspace for the cruise portion of their flight and are therefore required to utilize IFR procedures. Procedures and training are significantly more complex as a pilot must demonstrate competency in conducting an entire cross-country flight in IMC conditions, while controlling the aircraft solely by reference to instruments.
Instrument pilots must meticulously evaluate weather, create a very detailed flight plan based around specific instrument departure, en route, and arrival procedures, and dispatch the flight.
Separation and clearance.
The distance by which an aircraft avoids obstacles or other aircraft is termed "separation". The most important concept of IFR flying is that separation is maintained regardless of weather conditions. In controlled airspace, air traffic control (ATC) separates IFR aircraft from obstacles and other aircraft using a flight "clearance" based on route, time, distance, speed, and altitude. ATC monitors IFR flights on radar, or through aircraft position reports in areas where radar coverage is not available. Aircraft position reports are sent as voice radio transmissions. In the United States, a flight operating under IFR is required to provide position reports unless ATC advises a pilot that the plane is in radar contact. The pilot must resume position reports after ATC advises that radar contact has been lost, or that radar services are terminated.
IFR flights in controlled airspace require an ATC "clearance" for each part of the flight. A clearance always specifies a "clearance limit", which is the farthest the aircraft can fly without a new clearance. In addition, a clearance typically provides a heading or route to follow, altitude, and communication parameters, such as frequencies and transponder codes.
In uncontrolled airspace, ATC clearances are unavailable. In some states a form of separation is provided to certain aircraft in uncontrolled airspace as far as is practical (often known under ICAO as an advisory service in class G airspace), but separation is not mandated nor widely provided.
Despite the protection offered by flight in controlled airspace under IFR, the ultimate responsibility for the safety of the aircraft rests with the pilot in command, who can refuse clearances.
Weather.
It is essential to differentiate between flight plan type (VFR or IFR) and weather conditions (VMC or IMC). While current and forecast weather may be a factor in deciding which type of flight plan to file, weather conditions themselves do not affect one's filed flight plan. For example, an IFR flight that encounters visual meteorological conditions (VMC) en route does not automatically change to a VFR flight, and the flight must still follow all IFR procedures regardless of weather conditions. In the US, weather conditions are forecast broadly as VFR, MVFR (Marginal Visual Flight Rules), IFR, or LIFR (Low Instrument Flight Rules).
The main purpose of IFR is the safe operation of aircraft in instrument meteorological conditions (IMC). The weather is considered to be MVFR or IMC when it does not meet the minimum requirements for visual meteorological conditions (VMC). To operate safely in IMC ("actual instrument conditions"), a pilot controls the aircraft relying on flight instruments and ATC provides separation.
It is important not to confuse IFR with IMC. A significant amount of IFR flying is conducted in Visual Meteorological Conditions (VMC). Anytime a flight is operating in VMC, the crew is responsible for seeing and avoiding VFR traffic; however, because the flight is conducted under Instrument Flight Rules, ATC still provides separation services from other IFR traffic.
Although dangerous and illegal, a certain amount of VFR flying is conducted in instrument meteorological conditions (IMC). A scenario is a VFR pilot taking off in VMC conditions, but encountering deteriorating visibility while en route. "Continued VFR flight into IMC" can lead to spatial disorientation of the pilot which is the cause of a significant number of general aviation crashes. VFR flight into IMC is distinct from VFR-on-top, an IFR procedure in which the aircraft operates above IMC but remains in contact with ATC, and VFR over the top, a VFR procedure in which the aircraft takes off and lands in VMC but flies above an intervening area of IMC, both of which are legal in the US.
During flight under IFR, there are no visibility requirements, so flying through clouds (or other conditions where there is zero visibility outside the aircraft) is legal and safe. However, there are still minimum weather conditions that must be present in order for the aircraft to take off or to land; these vary according to the kind of operation, the type of navigation aids available, the location and height of terrain and obstructions in the vicinity of the airport, equipment on the aircraft, and the qualifications of the crew. For example, Reno-Tahoe International Airport (KRNO) in a mountainous region has significantly different instrument approaches for aircraft landing on the same runway surface, but from opposite directions. Aircraft approaching from the north must make visual contact with the airport at a higher altitude than when approaching from the south because of rapidly rising terrain south of the airport. This higher altitude allows a flight crew to clear the obstacle if a landing is aborted. In general, each specific instrument approach specifies the minimum weather conditions to permit landing.
Although large airliners, and increasingly, smaller aircraft, carry their own terrain awareness and warning system (TAWS), these are primarily backup systems providing a last layer of defense if a sequence of errors or omissions causes a dangerous situation.
Navigation.
Because IFR flights often take place without visual reference to the ground, a means of navigation other than looking outside the window is required. A number of navigational aids are available to pilots, including ground-based systems such as DME/VORs and NDBs as well as the satellite-based GPS/GNSS system. Air traffic control may assist in navigation by assigning pilots specific headings ("radar vectors"). The majority of IFR navigation is given by ground- and satellite-based systems, while radar vectors are usually reserved by ATC for sequencing aircraft for a busy approach or transitioning aircraft from takeoff to cruise, among other things.
Autopilot.
Autopilot allows automatic piloting.
Modern flight management systems have evolved to allow a crew to plan a flight as to route and altitude and to specific time of arrival at specific locations. This capability is used in several trial projects experimenting with "four-dimensional" approach clearances for commercial aircraft, with time as the fourth dimension. These clearances allow ATC to optimize the arrival of aircraft at major airports, which increases airport capacity and uses less fuel providing monetary and environmental benefits to airlines and the public.
Procedures.
Specific procedures allow IFR aircraft to transition safely through every stage of flight. These procedures specify how an IFR pilot should respond, even in the event of a complete radio failure, and loss of communications with ATC, including the expected aircraft course and altitude.
Departures are described in an IFR clearance issued by ATC prior to takeoff. The departure clearance may contain an assigned heading, one or more waypoints, and an initial altitude to fly. The clearance can also specify a departure procedure (DP) or standard instrument departure (SID) that should be followed unless "NO DP" is specified in the notes section of the filed flight plan.
Here is an example of an IFR clearance for a Cessna aircraft traveling from Palo Alto airport (KPAO) to Stockton airport (KSCK).
Detailed explanation:
The clearance sceme, used by ATC, can be easily remembered using the acronym
En route flight is described by IFR charts showing navigation aids, fixes, and standard routes called "airways". Aircraft with appropriate navigational equipment such as GPS, are also often cleared for a "direct-to" routing, where only the destination, or a few navigational waypoints are used to describe the route that the flight will follow. ATC will assign altitudes in its initial clearance or amendments thereto, and navigational charts indicate minimum safe altitudes for airways.
The approach portion of an IFR flight may begin with a standard terminal arrival route (STAR), describing common routes to fly to arrive at an initial approach fix (IAF) from which an instrument approach commences. 
An instrument approach terminates either by the pilot acquiring sufficient visual reference to proceed to the runway, or with a missed approach because the required visual reference is not seen in time.
Qualifications.
Pilot.
To fly under IFR, a pilot must have an instrument rating and must be "current" (meet recency of experience requirements).
In the United States, to file and fly under IFR, a pilot must be instrument-rated and, within the preceding six months, have flown six instrument approaches, as well as holding procedures and course interception and tracking with navaids. Flight under IFR beyond six months after meeting these requirements is not permitted; however, currency may be reestablished within the next six months by completing the requirements above. Beyond the twelfth month, examination ("instrument proficiency check") by an instructor is required.
Practicing instrument approaches can be done either in the instrument meteorological conditions or in visual meteorological conditions – in the latter case, a safety pilot is required so that the pilot practicing instrument approaches can wear a view-limiting device which restricts his field of view to the instrument panel. A safety pilot's primary duty is to observe and avoid other traffic.
For all ILS Cat II or Cat III approaches, additional crew training is required and a certain number of low visibility approaches must either be performed or simulated within a fixed time for pilots to be 'current' in performing them.
In the UK, an IR (UK restricted) - formerly the "IMC rating" - which permits flight under IFR in airspace classes B to G in instrument meteorological conditions, a non-instrument-rated pilot can also elect to fly under IFR in visual meteorological conditions outside controlled airspace. Compared to the rest of the world, the UK's flight crew licensing regime is somewhat unusual in its licensing for meteorological conditions and airspace, rather than flight rules.
Aircraft.
The aircraft must be equipped and type-certified for instrument flight, and the related navigational equipment must have been inspected or tested within a specific period of time prior to the instrument flight.
In the United States, instruments required for IFR flight in addition to those that are required for VFR flight are: heading indicator, sensitive altimeter adjustable for barometric pressure, clock with a sweep-second pointer or digital equivalent, attitude indicator, radios and suitable avionics for the route to be flown, alternator or generator, gyroscopic rate-of-turn indicator that is either a turn coordinator or the turn and bank indicator. From 1999 single-engine helicopters could not be FAA-certified for IFR, and Helicopter Association International estimates that 326 lives were lost in 133 accidents that would likely not have happened if those helicopters had been flying in IFR.

</doc>
<doc id="15245" url="http://en.wikipedia.org/wiki?curid=15245" title="Ismail Khan">
Ismail Khan

Mohammad Ismail Khan (Persian: محمد اسماعیل خان) (born 1946) is a warlord and politician in Afghanistan, serving as Minister of Water and Energy since 2005. He was previously the Governor of Herat Province. He is widely known as a warlord because of his rise to power during the Soviet war in Afghanistan. He controlled a large sized mujahideen force, mainly his fellow Tajiks from western Afghanistan. He is a key member of the political party Jamiat-e Islami and was a member of the now defunct United National Front party.
Early years and rise to power.
Khan was born in or about 1946 in the Shindand District of Herat Province in Afghanistan. His family are Tajiks from the Chahar-Mahal neighbourhood of Shindand.
In early 1979 Ismail Khan was a Captain in the Afghan National Army based in the western city of Herat. In early March of that year, there was a protest in front of the Communist governor's palace against the arrests and assassinations being carried out in the countryside. The governor's troops opened fire on the demonstrators, who proceeded to storm the palace and hunt down Soviet advisers. The Herat garrison mutinied and joined the revolt, with Ismail Khan and other officers distributing all available weapons to the insurgents. Hundreds of civil workers and people not dressed in traditional Muslim clothes were murdered. A garrison of Soviet advisors was overtaken and all of its inhabitants: Soviet advisors along with their wives and children were massacred. The mob put severed heads of the victims on sticks and paraded them through the city of Herat. The government led by Nur Mohammed Taraki responded, pulverizing the city using Soviet supplied bombers and killing an estimated 24,000 citizens in less than a week. This event marked the opening salvo of the rebellion which led to the Soviet invasion of Afghanistan in December 1979. Ismail Khan escaped to the countryside where he began to assemble a local mujahideen rebel army.
During the ensuing war, he became the leader of the western command of Burhanuddin Rabbani's Jamiat-e-Islami, political party associated with neighboring Pakistan's Jamaat-e-Islami. With Ahmad Shah Massoud, he was one of the most respected mujahideen leaders. In 1992, two years after the Soviet withdrawal from Afghanistan, the mujahideen captured Herat and Ismail Khan became Governor.
Escaping to Iran.
In 1995, he successfully defended his province against the Taliban, in cooperation with defense minister Ahmad Shah Massoud. Khan even attacked the Taliban stronghold of Kandahar, but was repulsed. Later, an ally of the Jamiat, Uzbek General Abdul Rashid Dostum changed sides, and attacked Herat. Ismail Khan was forced to flee to neighboring Iran with 8,000 men and the Taliban took over Herat Province.
Two years later, while organising opposition to the Taliban in Faryab area, he was betrayed and captured by Abdul Majid Rouzi who had defected to the Taliban along with Abdul Malik Pahlawan, then one of Dostum's deputies. Then in March 1999 he escaped from Kandahar prison. During the U.S. intervention in Afghanistan, he fought against the Taliban within the United Islamic Front for the Salvation of Afghanistan (Northern Alliance) and thus regained his position as Governor of Herat.
Karzai administration and return to Afghanistan.
After returning to Herat, Ismail Khan quickly consolidated his control over the region. He took over control of the city from the local ulema and quickly established control over the trade route between Herat and Iran, a large source of revenue. As Emir of Herat, Ismail Khan exercised great autonomy, providing social welfare for Heratis, expanding his power into neighbouring provinces, and maintaining direct international contacts. Although hated by the educated in Herat and often accused of human rights abuses, Ismail Khan's regime provided security, paid government employees, and made investments in public services. However, during his tenure as Governor, Ismail Khan was accused of ruling his province like a private fiefdom, leading to increasing tensions with the Afghan Transitional Administration. In particular, he refused to pass on to the government the revenues gained from custom taxes on goods from Iran and Turkmenistan.
On 13 August 2003, President Karzai removed Governor Ismail Khan from his command of the 4th Corps. This was announced as part of a programme removing the ability of officials to hold both civilian and military posts.
Ismail Khan was ultimately removed from power in March 2004 due to pressure by neighbouring warlords and the central Afghan government. Various sources have presented different versions of the story, and the exact dynamics cannot be known with certainty. What is known is that Ismail Khan found himself at odds with a few regional commanders who, although theoretically his subordinates, attempted to remove him from power. Ismail Khan claims that these efforts began with a botched assassination attempt. Afterwards, these commanders moved their forces near Herat. Ismail Khan, unpopular with the Herati military class, was slow to mobilise his forces, perhaps waiting for the threat to Herat to become existential as a means to motivate his forces. However, the conflict was stopped with the intervention of International Security Assistance Force forces and soldiers of the Afghan National Army, freezing the conflict in its tracks. Ismail Khan's forces even fought skirmishes with the Afghan National Army, in which his son, Mirwais Sadiq was killed. Because Ismail Khan was contained by the Afghan National Army, the warlords who opposed him were quickly able to occupy strategic locations unopposed. Ismail Khan was forced to give up his governorship and to go to Kabul, where he served in Hamid Karzai's cabinet as the Minister of Energy.
In 2005 Ismail Khan became the Minister of Water and Energy.
In late 2012, the Government of Afghanistan accused Ismail Khan of illegally distributing weapons to his supporters. About 40 members of the country's Parliament requested Ismail Khan to answer their queries. The government believes that Khan is attempting to create some kind of disruption in the country.
Assassination attempt.
On September 27, 2009, Ismail Khan survived a suicide blast that killed 4 of his bodyguards in Herat, in western Afghanistan. He was driving to Herat Airport when a powerful explosion occurred on the way there. Taliban spokesman, Zabiullah Mujahid, claimed responsibility and said the target was Khan.
Testimony requested by a Guantanamo captive.
Guantanamo captive Abdul Razzaq Hekmati requested Ismail Khan's testimony, when he was called before a Combatant Status Review Tribunal. 
Ismail Khan, like Afghan Minister of Defense Rahim Wardak, was one of the high profile Afghans that those conducting the Tribunals ruled were "not reasonably available" to give a statement on a captive's behalf because they could not be located.
Hekmati had played a key role in helping Ismail Khan escape from the Taliban in 1999.
Hekmati stood accused of helping Taliban leaders escape from the custody of Hamid Karzai's government.
Carlotta Gall and Andy Worthington interviewed Ismail Khan for a new "New York Times" article after Hekmati died of cancer in Guantanamo. 
According to the "New York Times"
Ismail Khan said he personally buttonholed the American ambassador to tell him that Hekmati was innocent, and should be released. In contrast, Hekmati was told that the State Department had been unable to locate Khan.
Controversy.
Ismail Khan is a controversial figure. Reporters Without Borders has charged him with muzzling the press and ordering attacks on journalists. Also Human Rights Watch has accused him of human rights abuses.
Nevertheless, he remains a popular figure for some in Afghanistan. Unlike other mujahideen commanders, Khan has not been linked to large-scale massacres and atrocities such as those committed after the capture of Kabul in 1992. Following news of his dismissal, rioting broke out in the streets of Herat, and President Karzai had to ask him to make a personal appeal for calm.
Notes and references.
</dl>

</doc>
<doc id="15250" url="http://en.wikipedia.org/wiki?curid=15250" title="Indigo">
Indigo

Indigo is a color that is traditionally regarded as a color on the visible spectrum, as well as one of the seven colors of the rainbow: the color between blue and violet. Although traditionally considered one of seven major spectral colors, its actual position in the electromagnetic spectrum is controversial. Indigo is a deep and bright color close to the color wheel blue (a primary color in the RGB color space), as well as to some variants of ultramarine.
The color indigo was named after the indigo dye derived from the plant "Indigofera tinctoria" and related species.
The first known recorded use of indigo as a color name in English was in 1289.
History.
India is believed to be the oldest center of indigo dyeing in the Old World. It was a primary supplier of indigo dye, derived from the plant "Indigofera tinctoria", to Europe as early as the Greco-Roman era. The association of India with indigo is reflected in the Greek word for the 'dye', which was "indikon" (ινδικόν). The Romans used the term "indicum", which passed into Italian dialect and eventually into English as the word indigo. El Salvador has lately been the biggest producer of indigo.
The same indigo dye is contained in the woad plant, "Isatis tinctoria", for a long time the main source of blue dye in Europe. Woad was replaced by true indigo as trade routes opened up, and both are now largely replaced by synthetic dyes.
Though the word indigo has existed in English since the 13th century, it may never have been a common part of the basic color-naming system.
Classification as a spectral color.
Many modern books place indigo on the spectrum between 450 and 420 nanometers, which lies on the short-wave side of color wheel (RGB) blue, towards (spectral) violet. However, the correspondence of this definition with colors of actual indigo dyes is disputed. Optical scientists Hardy and Perrin list indigo as between 446 and 464 nm wavelength, which occupies a spectrum segment from roughly the color wheel (RGB) blue extending to the long-wave side, towards azure.
Isaac Newton introduced indigo as one of the seven colors in his spectrum. In the mid-1660s, when Newton bought a pair of prisms at a fair near Cambridge, the East India Company had begun importing indigo dye into England, supplanting the homegrown woad as the source of blue dye. In a pivotal experiment in the history of optics, the young Newton shone a narrow beam of sunlight through a prism to produce a rainbow-like band of colors on the wall. In describing this optical spectrum, Newton acknowledged that the spectrum had a continuum of colors, but named seven colors: "The originall or primary colours are Red, yellow, Green, Blue, & a violet purple; together with Orange, Indico, & an indefinite varietie of intemediate gradations." He linked the seven prismatic colors to the seven notes of a western major scale, as shown in his color wheel, with orange and indigo as the semitones. Having decided upon seven colors, he asked a friend to repeatedly divide up the spectrum that was projected from the prism onto the wall:
"I desired a friend to draw with a pencil lines cross the image, or pillar of colours, where every one of the seven aforenamed colours was most full and brisk, and also where he judged the truest confines of them to be, whilst I held the paper so, that the said image might fall within a certain compass marked on it. And this I did, partly because my own eyes are not very critical in distinguishing colours, partly because another, to whom I had not communicated my thoughts about this matter, could have nothing but his eyes to determine his fancy in making those marks."
Indigo is therefore counted as one of the traditional colors of the rainbow, the order of which is given by the mnemonic Roy G. Biv. James Clerk Maxwell and Hermann von Helmholtz accepted indigo as an appropriate name for the color flanking violet in the spectrum.
Later scientists conclude that Newton named the colors differently from current usage.
According to Gary Waldman, "A careful reading of Newton's work indicates that the color he called indigo, we would normally call blue; his blue is then what we would name blue-green or cyan." If this is true, Newton's seven spectral colors would have been:
Red:     Orange:     Yellow:     Green:     Blue:     Indigo:     Violet:    
The human eye does not readily differentiate hues in the wavelengths between blue and violet. If this is where Newton meant indigo to lie, most individuals would have difficulty distinguishing indigo from its neighbors. According to Isaac Asimov, "It is customary to list indigo as a color lying between blue and violet, but it has never seemed to me that indigo is worth the dignity of being considered a separate color. To my eyes it seems merely deep blue."
Modern color scientists typically divide the spectrum between violet and blue at about 450 nm, with no indigo.
Distinction between the four major tones of indigo.
Like many other colors (orange, rose, and violet are the best-known), indigo gets its name from an object in the natural world—the plant named indigo once used for dyeing cloth (see also Indigo dye).
The color 'electric indigo' is a bright and saturated color between the traditional indigo and violet. This is the brightest color indigo that can be approximated on a computer screen—it is a color located between the (primary) blue and the color violet on the RGB color wheel.
The web color 'blue violet' or 'deep indigo' is a tone of indigo brighter than pigment indigo, but not as bright as electric indigo.
The color 'pigment indigo' is equivalent to the web color indigo and approximates the color indigo that is usually reproduced in pigments and colored pencils.
The color of indigo dye is a different color from either spectrum indigo or pigment indigo. This is the actual color of the dye. A vat full of this dye is a darker color, approximating the web color midnight blue.
Below are displayed these four major tones of indigo. When specifying the color indigo, it is important to indicate which of these four major tones is desired.
Electric indigo.
The color 'electric indigo' is much brighter than the pigment indigo reproduced below. When plotted on the CIE chromaticity diagram, this color is at 435 nanometers, in the middle of the portion of the spectrum traditionally considered indigo, i.e., between 450 and 420 nanometers. This color is only an approximation of spectral indigo, since actual spectral colors are outside the gamut of the sRGB color system.
Deep indigo (web color blue-violet).
At right is displayed the web color 'blue-violet', a color intermediate in brightness between electric indigo and pigment indigo. This color is also called 'deep indigo'.
Light indigo (web color indigo).
The color box at right displays the web color indigo which is equivalent to 'light indigo', the color indigo as it would be reproduced by artists' paints as opposed to the brighter indigo above (electric indigo) that is possible to reproduce on a computer screen. Its hue is closer to violet than to indigo dye for which the color is named. Pigment indigo can be obtained by mixing 55% pigment cyan with about 45% pigment magenta.
Compare the subtractive colors to the additive colors in the two primary color charts in the article on primary colors to see the distinction between electric colors as reproducible from light on a computer screen (additive colors) and the pigment colors reproducible with pigments (subtractive colors); the additive colors are significantly brighter because they are produced from light instead of pigment.
Light indigo (web color indigo) represents the way the color indigo was always reproduced in pigments, paints, or colored pencils in the 1950s. By the 1970s, because of the advent of psychedelic art, artists became used to brighter pigments, and pigments called "bright indigo" or "bright blue-violet" that are the pigment equivalent of the electric indigo reproduced in the section above became available in artists' pigments and colored pencils.
Tropical indigo.
'Tropical Indigo' is the color that is called "añil" (the Spanish word for "tropical indigo") in the "Guía de coloraciones" (Guide to colorations) by Rosa Gallego and
Juan Carlos Sanz, a color dictionary published in 2005 that is widely popular in the Hispanophone realm.
Additional variations of indigo.
Bright indigo.
At right is displayed the 'bright indigo' color called 'indigo' in Crayola crayons. This color was added by Crayola in 2000.
Denim.
'Denim' is a tone of indigo Crayola which resembles the shade of indigo used in denim. Crayola created this color in 1993 as one of the new 16 colors. Denim is the color of denim cloth, which after being dyed with indigo dye, is used to make jeans.
The Denim Revolution is a term used by the radical opposition in Belarus and their supporters in the West, who support a color revolution to implement the reintroduction of democracy, to describe their effort and aspirations.
In the 1960s, denim symbolized youth culture because so many young baby boomers wore denim jeans.
Violet-blue.
The color 'violet-blue' is a bluish tone of blue-violet. The Crayola color called 'violet-blue' is displayed at right. The Crayola crayon color called "violet-blue" was formulated in 1949 and continued as a Crayola color until 1990.
Imperial blue.
"See also Imperial red"
At right is displayed the color 'imperial blue'. Imperial blue is a representation of the blue color of the .
The first recorded use of 'imperial blue' as a color name in English was in the 1820s (exact year uncertain).
Persian indigo.
The color 'Persian indigo' is displayed at right. Another name for this color (seldom used nowadays) is 'regimental'. The color was called regimental because in the 19th century it was commonly used by many nations for navy uniforms. 'Persian indigo' is named for an association with a product from Persia: Persian cloth dyed with indigo dye.
The first recorded use of 'regimental' (the original name for the color now called Persian indigo) as a color name in English was in 1912.
Midnight blue.
At right is displayed the web color 'midnight blue'.
Dark imperial blue.
The color 'dark imperial blue' is called 'imperial blue' on the ISCC-NBS color list.
Japanese indigo.
The color 'Japanese indigo' is shown at right. This is the color that is called 'indigo' in the Japanese traditional colors, a group of colors in use beginning in 660 CE in the form of various dyes used in designing kimonos.
The name of this color in Japanese is "ai-iro", which means indigo color.
In nature.
Fungi
Birds
In culture.
Spirituality.
The tone of indigo used in the spiritualist applications is electric indigo because the color is represented as being the color of the spectrum between blue and violet.

</doc>
<doc id="15251" url="http://en.wikipedia.org/wiki?curid=15251" title="International Monetary Fund">
International Monetary Fund

The International Monetary Fund (IMF) is an international organization headquartered in Washington, D.C., in the United States, of 188 countries working to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world. Formed in 1944 at the Bretton Woods Conference, it came into formal existence in 1945 with 29 member countries and the goal of reconstructing the international payment system. Countries contribute funds to a pool through a quota system from which countries with payment imbalances can borrow. As of 2010, the fund had SDR476.8 billion, about US$755.7 billion at then-current exchange rates.
Through this fund, and other activities such as statistics keeping and analysis, surveillance of its members' economies and the demand for self-correcting policies, the IMF works to improve the economies of its member countries. The organization's objectives stated in the Articles of Agreement are: to promote international economic cooperation, international trade, employment, and exchange-rate stability, including by making financial resources available to member countries to meet balance-of-payments needs.
Functions.
The IMF works to foster global growth and economic stability by providing policy, advice and financing to members, by working with developing nations to help them achieve macroeconomic stability, and by reducing poverty. The rationale for this is that private international capital markets function imperfectly and many countries have limited access to financial markets. Such market imperfections, together with balance-of-payments financing, provide the justification for official financing, without which many countries could only correct large external payment imbalances through measures with adverse economic consequences. The IMF provides alternate sources of financing.
Upon initial IMF formation, its two primary functions were: to oversee the fixed exchange rate arrangements between countries, thus helping national governments manage their exchange rates and allowing these governments to prioritise economic growth, and to provide short-term capital to aid balance of payments. This assistance was meant to prevent the spread of international economic crises. The IMF was also intended to help mend the pieces of the international economy post the Great Depression and World War II.
The IMF's role was fundamentally altered after the floating exchange rates post 1971. It shifted to examining the economic policies of countries with IMF loan agreements to determine if a shortage of capital was due to economic fluctuations or economic policy. The IMF also researched what types of government policy would ensure economic recovery. The new challenge is to promote and implement policy that reduces the frequency of crises among the emerging market countries, especially the middle-income countries that are vulnerable to massive capital outflows. Rather than maintaining a position of oversight of only exchange rates, their function became one of “surveillance” of the overall macroeconomic performance of member countries. Their role became a lot more active because the IMF now manages economic policy rather than just exchange rates.
In addition, the IMF negotiates conditions on lending and loans under their policy of conditionality, which was established in the 1950s. Low-income countries can borrow on concessional terms, which means there is a period of time with no interest rates, through the Extended Credit Facility (ECF), the Standby Credit Facility (SCF) and the Rapid Credit Facility (RCF). Nonconcessional loans, which include interest rates, are provided mainly through Stand-By Arrangements (SBA), the Flexible Credit Line (FCL), the Precautionary and Liquidity Line (PLL), and the Extended Fund Facility. The IMF provides emergency assistance via the Rapid Financing Instrument (RFI) to members facing urgent balance-of-payments needs.
Surveillance of the global economy.
The IMF is mandated to oversee the international monetary and financial system and monitor the economic and financial policies of its member countries. This activity is known as surveillance and facilitates international cooperation. Since the demise of the Bretton Woods system of fixed exchange rates in the early 1970s, surveillance has evolved largely by way of changes in procedures rather than through the adoption of new obligations. The responsibilities changed from those of guardian to those of overseer of members’ policies.
The Fund typically analyses the appropriateness of each member country’s economic and financial policies for achieving orderly economic growth, and assesses the consequences of these policies for other countries and for the global economy.
In 1995 the International Monetary Fund began work on data dissemination standards with the view of guiding IMF member countries to disseminate their economic and financial data to the public. The International Monetary and Financial Committee (IMFC) endorsed the guidelines for the dissemination standards and they were split into two tiers: The General Data Dissemination System (GDDS) and the Special Data Dissemination Standard (SDDS).
The executive board approved the SDDS and GDDS in 1996 and 1997 respectively, and subsequent amendments were published in a revised "Guide to the General Data Dissemination System". The system is aimed primarily at statisticians and aims to improve many aspects of statistical systems in a country. It is also part of the World Bank Millennium Development Goals and Poverty Reduction Strategic Papers.
The primary objective of the GDDS is to encourage member countries to build a framework to improve data quality and statistical capacity building in order to evaluate statistical needs, set priorities in improving the timeliness, transparency, reliability and accessibility of financial and economic data. Some countries initially used the GDDS, but later upgraded to SDDS.
Some entities that are not themselves IMF members also contribute statistical data to the systems:
Conditionality of loans.
IMF conditionality is a set of policies or conditions that the IMF requires in exchange for financial resources. The IMF does require collateral from countries for loans but also requires the government seeking assistance to correct its macroeconomic imbalances in the form of policy reform. If the conditions are not met, the funds are withheld. Conditionality is perhaps the most controversial aspect of IMF policies. The concept of conditionality was introduced in a 1952 Executive Board decision and later incorporated into the Articles of Agreement.
Conditionality is associated with economic theory as well as an enforcement mechanism for repayment. Stemming primarily from the work of Jacques Polak, the theoretical underpinning of conditionality was the "monetary approach to the balance of payments".
Structural adjustment.
Some of the conditions for structural adjustment can include:
These conditions have also been sometimes labelled as the Washington Consensus.
Benefits.
These loan conditions ensure that the borrowing country will be able to repay the IMF and that the country will not attempt to solve their balance-of-payment problems in a way that would negatively impact the international economy. The incentive problem of moral hazard—when economic agents maximize their own utility to the detriment of others because they do not bear the full consequences of their actions—is mitigated through conditions rather than providing collateral; countries in need of IMF loans do not generally possess internationally valuable collateral anyway.
Conditionality also reassures the IMF that the funds lent to them will be used for the purposes defined by the Articles of Agreement and provides safeguards that country will be able to rectify its macroeconomic and structural imbalances. In the judgment of the IMF, the adoption by the member of certain corrective measures or policies will allow it to repay the IMF, thereby ensuring that the resources will be available to support other members.
s of 2004[ [update]], borrowing countries have had a very good track record for repaying credit extended under the IMF's regular lending facilities with full interest over the duration of the loan. This indicates that IMF lending does not impose a burden on creditor countries, as lending countries receive market-rate interest on most of their quota subscription, plus any of their own-currency subscriptions that are loaned out by the IMF, plus all of the reserve assets that they provide the IMF.
History.
The IMF was originally laid out as a part of the Bretton Woods system exchange agreement in 1944. During the Great Depression, countries sharply raised barriers to trade in an attempt to improve their failing economies. This led to the devaluation of national currencies and a decline in world trade.
This breakdown in international monetary co-operation created a need for oversight. The representatives of 45 governments met at the Bretton Woods Conference in the Mount Washington Hotel in Bretton Woods, New Hampshire, in the United States, to discuss a framework for postwar international economic cooperation and how to rebuild Europe.
There were two views on the role the IMF should assume as a global economic institution. British economist John Maynard Keynes imagined that the IMF would be a coöperative fund upon which member states could draw to maintain economic activity and employment through periodic crises. This view suggested an IMF that helped governments and to act as the U.S. government had during the New Deal in response to World War II. American delegate Harry Dexter White foresaw an IMF that functioned more like a bank, making sure that borrowing states could repay their debts on time. Most of White's plan was incorporated into the final acts adopted at Bretton Woods.
The IMF formally came into existence on 27 December 1945, when the first 29 countries ratified its Articles of Agreement. By the end of 1946 the IMF had grown to 39 members. On 1 March 1947, the IMF began its financial operations, and on 8 May France became the first country to borrow from it.
The IMF was one of the key organisations of the international economic system; its design allowed the system to balance the rebuilding of international capitalism with the maximisation of national economic sovereignty and human welfare, also known as embedded liberalism. The IMF's influence in the global economy steadily increased as it accumulated more members. The increase reflected in particular the attainment of political independence by many African countries and more recently the 1991 dissolution of the Soviet Union because most countries in the Soviet sphere of influence did not join the IMF.
The Bretton Woods system prevailed until 1971, when the U.S. government suspended the convertibility of the US$ (and dollar reserves held by other governments) into gold. This is known as the Nixon Shock.
Since 2000.
In May 2010, the IMF participated, in 3:11 proportion, in the first Greek bailout that totalled €110 billion. This bailout was notable for several reasons: the funds were funnelled directly to the (largely European) private bondholders, which endured no haircuts, to the chagrin of the Swiss, Brazilian, Indian, Russian, and Argentinian Directors; the Greek authorities (at the time, George Papandreou and Giorgos Papakonstantinou) themselves ruled out a haircut of the private bondholders; the Greek private sector was happy to curtail the 13th and 14th month civil service pay scheme, because the Greek government was otherwise impotent.
 A second bailout package of more than €100 billion was agreed over the course of a few months from October 2011, during which time Papandreou was forced from office. The so-called Troika, of which the IMF is part, are joint managers of this programme, which was approved by the Executive Directors of the IMF on 15 March 2012 for SDR23.8 billion, and which saw private bondholders take a haircut of upwards of 50%. In the interval between May 2010 and February 2012 the private banks of Holland, France and Germany reduced exposure to Greek debt from €122 billion to €66 billion.
As of January 2012, the largest borrowers from the IMF in order were Greece, Portugal, Ireland, Romania, and Ukraine.
On 25 March 2013, a €10 billion international bailout of Cyprus was agreed by the Troika, at the cost to the Cypriots of its agreement: to close the country's second-largest bank; to impose a one-time bank deposit levy on Bank of Cyprus uninsured deposits. No insured deposit of €100k or less were to be affected under the terms of a novel bail-in scheme.
The topic of sovereign debt restructuring was taken up by the IMF in April 2013 for the first time since 2005, in a report entitled "Sovereign Debt Restructuring: Recent Developments and Implications for the Fund’s Legal and Policy Framework". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize, and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the "Wall Street Journal".
In the October 2013 Financial Monitor publication, the IMF suggested that a capital levy capable of reducing Euro-area government debt ratios to "end-2007 levels" would require a very high tax rate of about 10%.
The Fiscal Affairs department of the IMF, headed by , produced in January 2014 a report entitled "Fiscal Policy and Income Inequality" which stated that "Some taxes levied on wealth, especially on immovable property, are also an option for economies seeking more progressive taxation...Property taxes are equitable and efficient, but underutilized in many economies...There is considerable scope to exploit this tax more fully, both as a revenue source and as a redistributive instrument."
At the end of March 2014, the IMF secured an $18 billion bailout fund for the provisional government of the Ukraine in the aftermath of the 2014 Ukrainian revolution.
The U.S. executive board veto was brought up again by IMF junior members in April 2014. The countries were fed up with the failure to ratify a four-year-old agreement to restructure the lender. Singaporean Finance Minister and IMF steering committee chairman Tharman Shanmugaratnam said it could cause "disruptive change" in the global economy: "We are more likely over time to see a weakening of multilateralism, the emergence of regionalism, bilateralism and other ways of dealing with global problems", and that would make the world a "less safe" place.
Member countries.
Not all member countries of the IMF are sovereign states, and therefore not all "member countries" of the IMF are members of the United Nations. Amidst "member countries" of the IMF that are not member states of the UN are non-sovereign areas with special jurisdictions that are officially under the sovereignty of full UN member states, such as Aruba, Curaçao, Hong Kong, and Macau, as well as Kosovo. The corporate members appoint "ex-officio" voting members, who are listed below. All members of the IMF are also International Bank for Reconstruction and Development (IBRD) members and vice versa.
Former members are Cuba (which left in 1964) and the Republic of China, which was ejected from the UN in 1980 after losing the support of then U.S. President Jimmy Carter and was replaced by the People's Republic of China. However, "Taiwan Province of China" is still listed in the official IMF indices.
Apart from Cuba, the other UN states that do not belong to the IMF are Andorra, Liechtenstein, Monaco, Nauru, and North Korea.
The former Czechoslovakia was expelled in 1954 for "failing to provide required data" and was readmitted in 1990, after the Velvet Revolution. Poland withdrew in 1950—allegedly pressured by the Soviet Union—but returned in 1986.
Qualifications.
Any country may apply to be a part of the IMF. Post-IMF formation, in the early postwar period, rules for IMF membership were left relatively loose. Members needed to make periodic membership payments towards their quota, to refrain from currency restrictions unless granted IMF permission, to abide by the Code of Conduct in the IMF Articles of Agreement, and to provide national economic information. However, stricter rules were imposed on governments that applied to the IMF for funding.
The countries that joined the IMF between 1945 and 1971 agreed to keep their exchange rates secured at rates that could be adjusted only to correct a "fundamental disequilibrium" in the balance of payments, and only with the IMF's agreement.
Some members have a very difficult relationship with the IMF and even when they are still members they do not allow themselves to be monitored. Argentina, for example, refuses to participate in an Article IV Consultation with the IMF.
Benefits.
Member countries of the IMF have access to information on the economic policies of all member countries, the opportunity to influence other members’ economic policies, technical assistance in banking, fiscal affairs, and exchange matters, financial support in times of payment difficulties, and increased opportunities for trade and investment.
Leadership.
Board of Governors.
The Board of Governors consists of one governor and one alternate governor for each member country. Each member country appoints its two governors. The Board normally meets once a year and is responsible for electing or appointing executive directors to the Executive Board. While the Board of Governors is officially responsible for approving quota increases, Special Drawing Right allocations, the admittance of new members, compulsory withdrawal of members, and amendments to the Articles of Agreement and By-Laws, in practice it has delegated most of its powers to the IMF's Executive Board.
The Board of Governors is advised by the International Monetary and Financial Committee and the Development Committee. The International Monetary and Financial Committee has 24 members and monitors developments in global liquidity and the transfer of resources to developing countries. The Development Committee has 25 members and advises on critical development issues and on financial resources required to promote economic development in developing countries. They also advise on trade and environmental issues.
Executive Board.
24 Executive Directors make up Executive Board. The Executive Directors represent all 188 member countries in a geographically based roster. Countries with large economies have their own Executive Director, but most countries are grouped in constituencies representing four or more countries.
Following the "2008 Amendment on Voice and Participation" which came into effect in March 2011, eight countries each appoint an Executive Director: the United States, Japan, Germany, France, the UK, China, the Russian Federation, and Saudi Arabia. The remaining 16 Directors represent constituencies consisting of 4 to 22 countries. The Executive Director representing the largest constituency of 22 countries accounts for 1.55% of the vote. This Board usually meets several times each week. The Board membership and constituency is scheduled for periodic review every eight years.
List of Executive Directors of the IMF, as of April 2015
Managing Director.
The IMF is led by a managing director, who is head of the staff and serves as Chairman of the Executive Board. The managing director is assisted by a First Deputy managing director and three other Deputy Managing Directors. Historically the IMF's managing director has been European and the president of the World Bank has been from the United States. However, this standard is increasingly being questioned and competition for these two posts may soon open up to include other qualified candidates from any part of the world.
In 2011 the world's largest developing countries, the BRIC nations, issued a statement declaring that the tradition of appointing a European as managing director undermined the legitimacy of the IMF and called for the appointment to be merit-based.
Previous managing director Dominique Strauss-Kahn was arrested in connection with charges of sexually assaulting a New York hotel room attendant and resigned on 18 May. On 28 June 2011 Christine Lagarde was confirmed as managing director of the IMF for a five-year term starting on 5 July 2011. In 2012, Lagarde was paid a tax-exempt salary of US$467,940, and this is automatically increased every year according to inflation. In addition, the director receives an allowance of US$83,760 and additional expenses for entertainment.
Voting power.
Voting power in the IMF is based on a quota system. Each member has a number of basic votes (each member's number of basic votes equals 5.502% of the total votes), plus one additional vote for each Special Drawing Right (SDR) of 100,000 of a member country's quota. The Special Drawing Right is the unit of account of the IMF and represents a claim to currency. It is based on a basket of key international currencies. The basic votes generate a slight bias in favour of small countries, but the additional votes determined by SDR outweigh this bias.
Effects of the quota system.
The IMF's quota system was created to raise funds for loans. Each IMF member country is assigned a quota, or contribution, that reflects the country's relative size in the global economy. Each member's quota also determines its relative voting power. Thus, financial contributions from member governments are linked to voting power in the organisation.
This system follows the logic of a shareholder-controlled organisation: wealthy countries have more say in the making and revision of rules. Since decision making at the IMF reflects each member's relative economic position in the world, wealthier countries that provide more money to the IMF have more influence than poorer members that contribute less; nonetheless, the IMF focuses on redistribution.
Developing countries.
Quotas are normally reviewed every five years and can be increased when deemed necessary by the Board of Governors. Currently, reforming the representation of developing countries within the IMF has been suggested. These countries' economies represent a large portion of the global economic system but this is not reflected in the IMF's decision making process through the nature of the quota system. Joseph Stiglitz argues, "There is a need to provide more effective voice and representation for developing countries, which now represent a much larger portion of world economic activity since 1944, when the IMF was created." In 2008, a number of quota reforms were passed including shifting 6% of quota shares to dynamic emerging markets and developing countries.
United States influence.
A second criticism is that the United States' transition to neoliberalism and global capitalism also led to a change in the identity and functions of international institutions like the IMF. Because of the high involvement and voting power of the United States, the global economic ideology could effectively be transformed to match that of the United States. This is consistent with the IMF's function change during the 1970s after the Nixon Shock ended the Bretton Woods system. Allies of the United States are said to receive bigger loans with fewer conditions.
Overcoming borrower/creditor divide.
The IMF's membership is divided along income lines: certain countries provide the financial resources while others use these resources. Both developed country "creditors" and developing country "borrowers" are members of the IMF. The developed countries provide the financial resources but rarely enter into IMF loan agreements; they are the creditors. Conversely, the developing countries use the lending services but contribute little to the pool of money available to lend because their quotas are smaller; they are the borrowers. Thus, tension is created around governance issues because these two groups, creditors and borrowers, have fundamentally different interests.
The criticism is that the system of voting power distribution through a quota system institutionalises borrower subordination and creditor dominance. The resulting division of the IMF's membership into borrowers and non-borrowers has increased the controversy around conditionality because the borrowers are interested in increasing loan access while creditors want to maintain reassurance that the loans will be repaid.
Use.
A recent source reveals that the average overall use of IMF credit per decade increased, in real terms, by 21% between the 1970s and 1980s, and increased again by just over 22% from the 1980s to the 1991–2005 period. Another study has suggested that since 1950 the continent of Africa alone has received $300 billion from the IMF, the World Bank, and affiliate institutions.
A study by Bumba Mukherjee found that developing democratic countries benefit more from IMF programs than developing autocratic countries because policy-making, and the process of deciding where loaned money is used, is more transparent within a democracy. One study done by Randall Stone found that although earlier studies found little impact of IMF programs on balance of payments, more recent studies using more sophisticated methods and larger samples "usually found IMF programs improved the balance of payments".
Exceptional Access Framework – Sovereign Debt.
The Exceptional Access Framework was created in 2003 when John B. Taylor was Under Secretary of the U.S. Treasury for International Affairs. The new Framework became fully operational in February 2003 and it was applied in the subsequent decisions on Argentina and Brazil. Its purpose was to place some sensible rules and limits on the way the IMF makes loans to support governments with debt problem—especially in emerging markets—and thereby move away from the bailout mentality of the 1990s. Such a reform was essential for ending the crisis atmosphere that then existed in emerging markets. The reform was closely related to, and put in place nearly simultaneously with, the actions of several emerging market countries to place collective action clauses in their bond contracts.
In 2010, the framework was abandoned so the IMF could make loans to Greece in an unsustainable and political situation.
The topic of sovereign debt restructuring was taken up by IMF staff in April 2013 for the first time since 2005, in a report entitled "Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the "Wall Street Journal".
The staff was directed to formulate an updated policy, which was accomplished on 22 May 2014 with a report entitled "The Fund's Lending Framework and Sovereign Debt: Preliminary Considerations", and taken up by the Executive Board on 13 June. The staff proposed that "in circumstances where a (Sovereign) member has lost market access and debt is considered sustainable...the IMF would be able to provide Exceptional Access on the basis of a debt operation that involves an extension of maturities", which was labelled a "reprofiling operation". These reprofiling operations would "generally be less costly to the debtor and creditors—and thus to the system overall—relative to either an upfront debt reduction operation or a bail-out that is followed by debt reduction... (and) would be envisaged only when both (a) a member has lost market access and (b) debt is assessed to be sustainable, but not with high probability...Creditors will only agree if they understand that such an amendment is necessary to avoid a worse outcome: namely, a default and/or an operation involving debt reduction...Collective action clauses, which now exist in most—but not all—bonds, would be relied upon to address collective action problems."
IMF and globalization.
Globalization encompasses three institutions: global financial markets and transnational companies, national governments linked to each other in economic and military alliances led by the United States, and rising "global governments" such as World Trade Organization (WTO), IMF, and World Bank. Charles Derber argues in his book "People Before Profit," "These interacting institutions create a new global power system where sovereignty is globalized, taking power and constitutional authority away from nations and giving it to global markets and international bodies". Titus Alexander argues that this system institutionalises global inequality between western countries and the Majority World in a form of global apartheid, in which the IMF is a key pillar.
The establishment of globalised economic institutions has been both a symptom of and a stimulus for globalization. The development of the World Bank, the IMF regional development banks such as the European Bank for Reconstruction and Development (EBRD), and multilateral trade institutions such as the WTO signals a move away from the dominance of the state as the exclusive unit of analysis in international affairs. Globalization has thus been transformative in terms of a reconceptualising of state sovereignty.
Following U.S. President Bill Clinton's administration's aggressive financial deregulation campaign in the 1990s, globalisation leaders overturned longstanding restrictions by governments that limited foreign ownership of their banks, deregulated currency exchange, and eliminated restrictions on how quickly money could be withdrawn by foreign investors.
Criticisms.
Overseas Development Institute (ODI) research undertaken in 1980 pointed to five main criticisms of the IMF which support the analysis that it is a pillar of what activist Titus Alexander calls global apartheid. Firstly, developed countries were seen to have a more dominant role and control over less developed countries (LDCs) primarily due to the Western bias favoring capitalism.
Secondly, the Fund worked on the incorrect assumption that all payments disequilibria were caused domestically. The Group of 24 (G-24), on behalf of LDC members, and the United Nations Conference on Trade and Development (UNCTAD) complained that the IMF did not distinguish sufficiently between disequilibria with predominantly external as opposed to internal causes. This criticism was voiced in the aftermath of the 1973 oil crisis. Then LDCs found themselves with payments deficits due to adverse changes in their terms of trade, with the Fund prescribing stabilisation programmes similar to those suggested for deficits caused by government over-spending. Faced with long-term, externally generated disequilibria, the G-24 argued for more time for LDCs to adjust their economies.
The third criticism was that IMF policies were anti-developmental. The deflationary effects of IMF programmes quickly led to losses of output and employment in economies where incomes were low and unemployment was high. Moreover, the burden of the deflation is disproportionately borne by the poor.
Fourth, harsh policy conditions are self-defeating when a vicious circle developed when members refused loans due to harsh conditionality, exacerbating the economy and eventually taking loans as a drastic medicine.
Lastly is the point that the IMF's policies lack a clear economic rationale. Its policy foundations were theoretical and unclear due to differing opinions and departmental rivalries whilst dealing with countries with widely varying economic circumstances.
ODI conclusions were that the IMF's very nature of promoting market-oriented approaches attracted unavoidable criticism. On the other hand, the IMF could serve as a scapegoat while allowing governments to blame international bankers. The ODI conceded that the IMF was insensitive to political aspirations of LDCs, while its policy conditions were inflexible.
Argentina, which had been considered by the IMF to be a model country in its compliance to policy proposals by the Bretton Woods institutions, experienced a catastrophic economic crisis in 2001, which some believe to have been caused by IMF-induced budget restrictions—which undercut the government's ability to sustain national infrastructure even in crucial areas such as health, education, and security—and privatisation of strategically vital national resources. Others attribute the crisis to Argentina's misdesigned fiscal federalism, which caused subnational spending to increase rapidly. The crisis added to widespread hatred of this institution in Argentina and other South American countries, with many blaming the IMF for the region's economic problems. The current—as of early 2006—trend toward moderate left-wing governments in the region and a growing concern with the development of a regional economic policy largely independent of big business pressures has been ascribed to this crisis.
In an interview, the former Romanian Prime Minister Călin Popescu-Tăriceanu claimed that "Since 2005, IMF is constantly making mistakes when it appreciates the country's economic performances". Former Tanzanian President Julius Nyerere, who claimed that debt-ridden African states were ceding sovereignty to the IMF and the World Bank, famously asked, "Who elected the IMF to be the ministry of finance for every country in the world?"
Conditionality.
The IMF has been criticised for being "out of touch" with local economic conditions, cultures, and environments in the countries they are requiring policy reform. The economic advice the IMF gives might not always take into consideration the difference between what spending means on paper and how it is felt by citizens.
For example, some people believe that Jeffrey Sachs' work shows that the IMF's "usual prescription is 'budgetary belt tightening to countries who are much too poor to own belts'". It has been said that the IMF's role as a generalist institution specialising in macroeconomic issues needs reform. Conditionality has also been criticised because a country can pledge collateral of "acceptable assets" to obtain waivers—if one assumes that all countries are able to provide "acceptable collateral".
One view is that conditionality undermines domestic political institutions. The recipient governments are sacrificing policy autonomy in exchange for funds, which can lead to public resentment of the local leadership for accepting and enforcing the IMF conditions. Political instability can result from more leadership turnover as political leaders are replaced in electoral backlashes. IMF conditions are often criticised for reducing government services, thus increasing unemployment.
Another criticism is that IMF programs are only designed to address poor governance, excessive government spending, excessive government intervention in markets, and too much state ownership. This assumes that this narrow range of issues represents the only possible problems; everything is standardised and differing contexts are ignored. A country may also be compelled to accept conditions it would not normally accept had they not been in a financial crisis in need of assistance.
On top of that, regardless of what methodologies and data sets used, it comes to same conclusion of exacerbating income inequality. With Gini coefficient, it became clear that countries with IMF programs face increased income inequality.
It is claimed that conditionalities retard social stability and hence inhibit the stated goals of the IMF, while Structural Adjustment Programs lead to an increase in poverty in recipient countries. The IMF sometimes advocates “austerity programmes”, cutting public spending and increasing taxes even when the economy is weak, to bring budgets closer to a balance, thus reducing budget deficits. Countries are often advised to lower their corporate tax rate. In "Globalization and Its Discontents", Joseph E. Stiglitz, former chief economist and senior vice-president at the World Bank, criticizes these policies. He argues that by converting to a more monetarist approach, the purpose of the fund is no longer valid, as it was designed to provide funds for countries to carry out Keynesian reflations, and that the IMF "was not participating in a conspiracy, but it was reflecting the interests and ideology of the Western financial community".
International politics play an important role in IMF decision making. The clout of member states is roughly proportional to its contribution to IMF finances. The United States has the greatest number of votes and therefore wields the most influence. Domestic politics often come into play, with politicians in developing countries using conditionality to gain leverage over the opposition in order to influence policy.
Reform.
The IMF is only one of many international organisations and it is a generalist institution for macroeconomic issues only; its core areas of concern in developing countries are very narrow. One proposed reform is a movement towards close partnership with other specialist agencies such as UNICEF, the Food and Agriculture Organization (FAO), or the United Nations Development Program (UNDP).
Jeffrey Sachs argues in "The End of Poverty" that the IMF and the World Bank have "the brightest economists and the lead in advising poor countries on how to break out of poverty, but the problem is development economics". Development economics needs the reform, not the IMF. He also notes that IMF loan conditions should be paired with other reforms—e.g., trade reform in developed nations, debt cancellation, and increased financial assistance for investments in basic infrastructure. IMF loan conditions cannot stand alone and produce change; they need to be partnered with other reforms or other conditions as applicable.
Reforms to give more powers to emerging economies were agreed by the G20 in 2010; however, as of April 2014, the U.S. Congress has not agreed to these reforms.
Support of military dictatorships.
The role of the Bretton Woods institutions has been controversial since the late Cold War, due to claims that the IMF policy makers supported military dictatorships friendly to American and European corporations and other anti-communist regimes. Critics also claim that the IMF is generally apathetic or hostile to human rights, and labour rights. The controversy has helped spark the anti-globalization movement.
An example of IMF's support for a dictatorship was its ongoing support for Mobutu's rule in Zaire, although its own envoy, Erwin Blumenthal, provided a sobering report about the entrenched corruption and embezzlement and the inability of the country to pay back any loans.
Arguments in favour of the IMF say that economic stability is a precursor to democracy; however, critics highlight various examples in which democratised countries fell after receiving IMF loans.
Impact on access to food.
A number of civil society organisations have criticised the IMF's policies for their impact on access to food, particularly in developing countries. In October 2008, former U.S. president Bill Clinton delivered a speech to the United Nations on World Food Day, criticizing the World Bank and IMF for their policies on food and agriculture:
We need the World Bank, the IMF, all the big foundations, and all the governments to admit that, for 30 years, we all blew it, including me when I was president. We were wrong to believe that food was like some other product in international trade, and we all have to go back to a more responsible and sustainable form of agriculture.—Former U.S. president Bill Clinton, Speech at United Nations World Food Day, October 16, 2008
Impact on public health.
A 2009 study concluded that the strict conditions resulted in thousands of deaths in Eastern Europe by tuberculosis as public health care had to be weakened. In the 21 countries to which the IMF had given loans, tuberculosis deaths rose by 16.6%.
In 2009, a book by Rick Rowden titled "The Deadly Ideas of Neoliberalism: How the IMF has Undermined Public Health and the Fight Against AIDS", claimed that the IMF’s monetarist approach towards prioritising price stability (low inflation) and fiscal restraint (low budget deficits) was unnecessarily restrictive and has prevented developing countries from scaling up long-term investment in public health infrastructure. The book claimed the consequences have been chronically underfunded public health systems, leading to demoralising working conditions that have fuelled a "brain drain" of medical personnel, all of which has undermined public health and the fight against HIV/AIDS in developing countries.
Impact on environment.
IMF policies have been repeatedly criticised for making it difficult for indebted countries to say no to environmentally harmful projects that nevertheless generate revenues such as oil, coal, and forest-destroying lumber and agriculture projects. Ecuador for example had to defy IMF advice repeatedly to pursue the protection of its rain forests, though paradoxically this need was cited in IMF argument to support that country. The IMF acknowledged this paradox in the 2010 report that proposed the IMF Green Fund, a mechanism to issue special drawing rights directly to pay for climate harm prevention and potentially other ecological protection as pursued generally by other environmental finance.
While the response to these moves was generally positive possibly because ecological protection and energy and infrastructure transformation are more politically neutral than pressures to change social policy. Some experts voiced concern that the IMF was not representative, and that the IMF proposals to generate only US$200 billion a year by 2020 with the SDRs as seed funds, did not go far enough to undo the general incentive to pursue destructive projects inherent in the world commodity trading and banking systems—criticisms often levelled at the World Trade Organization and large global banking institutions.
In the context of the European debt crisis, some observers noted that Spain and California, two troubled economies within Europe and the United States, and also Germany, the primary and politically most fragile supporter of a euro currency bailout would benefit from IMF recognition of their leadership in green technology, and directly from Green Fund–generated demand for their exports, which could also improve their credit ratings.
Proposed Alternatives.
In March 2011 the Ministers of Economy and Finance of the African Union proposed to establish an African Monetary Fund.
At the 6th BRICS summit in July 2014 the BRICS nations (Brazil, Russia, India, China, and South Africa) announced the BRICS Contingent Reserve Arrangement (CRA) with an initial size of US$100 billion, a framework to provide liquidity through currency swaps in response to actual or potential short-term balance-of-payments pressures.
In the media.
"Life and Debt", a documentary film, deals with the IMF's policies' influence on Jamaica and its economy from a critical point of view. "Debtocracy", a 2011 independent Greek documentary film, also criticizes the IMF.
Notes and references.
Notes:
References:

</doc>
<doc id="15252" url="http://en.wikipedia.org/wiki?curid=15252" title="Islands of the Clyde">
Islands of the Clyde

The Islands of the Firth of Clyde are the fifth largest of the major Scottish island groups after the Inner and Outer Hebrides, Orkney and Shetland. They are situated in the Firth of Clyde between Ayrshire and Argyll. There are about forty islands and skerries, of which only four are inhabited and only nine larger than 40 ha. The largest and most populous are Arran and Bute, and Great Cumbrae and Holy Isle are also served by dedicated ferry routes. Unlike the four larger Scottish archipelagos, none of the isles in this group are connected to one another or to the mainland by bridges.
The geology and geomorphology of the area is complex and the islands and the surrounding sea lochs each have distinctive features. The influence of the Atlantic Ocean and the North Atlantic Drift create a mild, damp oceanic climate.
The larger islands have been continuously inhabited since Neolithic times, were influenced by the emergence of the kingdom of Dál Riata from 500 AD and then absorbed into the emerging Kingdom of Alba under Kenneth MacAlpin. They experienced Norse incursions during the early Middle Ages and then became part of the Kingdom of Scotland in the 13th century. There is a diversity of wildlife, including three species of rare endemic tree.
Geology and geography.
The Highland Boundary Fault runs past Bute and through the northern part of Arran, so from a geological perspective some of the islands are in the Highlands and some in the Central Lowlands. As a result, Arran is sometimes referred to as "Scotland in miniature" and the island is a popular destination for geologists, who come to see intrusive igneous landforms such as sills and dykes as well as sedimentary and metasedimentary rocks ranging widely in age. Visiting in 1787, the geologist James Hutton found his first example of an unconformity there and this spot is one of the most famous places in the study of geology. A group of weakly metamorphosed rocks that form the Highland Border Complex lie discontinuously along the Highland Boundary Fault. One of the most prominent exposures is along Loch Fad on Bute. Ailsa Craig, which lies some 25 km south of Arran, has been quarried for a rare type of micro-granite containing riebeckite known as "Ailsite" which is used to make curling stones. As of 2004, 60 to 70% of all curling stones in use were made from granite from the island.
In common with the rest of Scotland the Firth of Clyde was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. Arran's highest peaks may have been nunataks at this time. After the last retreat of the ice sea level changes and the isostatic rise of land makes charting post glacial coastlines a complex task but the resultant clifflines behind raised beaches are a prominent feature of the entire coastline.
The soils of the islands reflect the diverse geology. Bute has the most productive land, and a pattern of deposits that is typical of the southwest of Scotland. There is a mixture of boulder clay and other glacial deposits in the eroded valleys, and raised beach and marine deposits elsewhere, especially to the south and west which result in a machair landscape in places, inland from the sandy bays, such as Stravanan.
The Firth of Clyde, in which these island lie, is north of the Irish Sea and has numerous branching inlets, some of them substantial features in their own right. These include Loch Goil, Loch Long, Gare Loch, Loch Fyne and the estuary of the River Clyde. In places the effect of glaciation on the seabed is pronounced. For example, the Firth is 320 m deep between Arran and Bute, although they are only 8 km apart. The islands are all exposed to wind and tide and various lighthouses, such as those on Ailsa Craig, Pladda and Davaar act as an aid to navigation.
Climate.
The Firth of Clyde lies between 55 and 56 degrees north, at the same latitude as Labrador in Canada and north of the Aleutian Islands, but the influence of the North Atlantic Drift—the northern extension of the Gulf Stream—ameliorates the winter weather and the area enjoys a mild, damp oceanic climate. Temperatures are generally cool, averaging about 6 °C in January and 14 °C in July at sea level. Snow seldom lies at sea level and frosts are generally less frequent than the mainland. In common with most islands of the west coast of Scotland, rainfall is generally high at between 1300 mm per annum on Bute, the Cumbraes and in the south of Arran and 1900 mm per annum in the north of Arran. The Arran mountains are wetter still with the summits receiving over 2550 mm annually. May, June and July are the sunniest months, with upwards of 200 hours of bright sunshine being recorded on average, southern Bute benefiting from a particularly high level of sunny days.
History.
Prehistory.
Mesolithic humans arrived in the Firth of the Clyde during the fourth millennium BC, probably from Ireland. This was followed by a wave of Neolithic peoples using the same route and there is some evidence that the Firth of Clyde was a significant route via which mainland Scotland was colonised at this time. A particular style of megalithic structure developed in Argyll, the Clyde estuary and elsewhere in western Scotland that has become known as the Clyde cairn. They are rectangular or trapezoidal in shape with a small enclosing chamber faced with large slabs of stone set on end and sometimes subdivided into smaller compartments. A forecourt area may have been used for displays or rituals associated with the interment of the dead, who were placed inside the chambers. They are concentrated in Arran, Bute and Kintyre and it is likely that the Clyde cairns were the earliest forms of Neolithic monument constructed by incoming settlers although few of the 100 or so examples have been given a radiocarbon dating. An example at Monamore on Arran has been dated to 3160 BC, although it was almost certainly built earlier than that, possibly c. 4000BC. There are also numerous standing stones dating from prehistoric times, including six stone circles on Machrie Moor, Arran and other examples on Great Cumbrae and Bute.
Bronze Age settlers also constructed megaliths at various sites, many of them dating from the second millennium BC, although the chambered cairns were replaced by burial cists, found on for example, Inchmarnock. Settlement evidence, especially from the early part of this era is however poor. The Queen of the Inch necklace is an article of jewellery made of jet found on Bute that dates from circa 2000 BC. During the early Iron Age Brythonic culture held sway, there being no evidence that the Roman occupation of southern Scotland extended to these islands.
Early Scots rule.
During the 2nd century AD Irish influence was at work in the region and by the 6th century the kingdom of Dál Riata was established. Unlike the P-Celtic speaking Brythons, these Gaels spoke a form of Gaelic that still survives in the Hebrides. Through the efforts of Saint Ninian and others Christianity slowly supplanted Druidism. Dál Riata flourished from the time of Fergus Mór in the late fifth century until the Viking incursions that commenced in the late eighth century. Islands close to the shores of modern Ayrshire would have remained part of the Kingdom of Strathclyde during this period, whilst the main islands became part of the emerging Kingdom of Alba founded by Kenneth MacAlpin (Cináed mac Ailpín).
Viking influence.
The Islands of the Clyde historically formed the border zone between the Norse-dominated "Sudreyor" and Scotland. As such many of these islands fell under Norwegian and Ui Imair hegemony between the 9th and 13th centuries and this Norse influence would see almost constant warfare on the western seaboard of Scotland until the partitioning of the Hebrides in 1156.
Latterly, the Outer Hebrides remained under the control of Godred V of the Isle of Man while the Inner Hebrides south of Ardnamurchan and the islands of the Clyde became part of the Kingdom of the Hebrides controlled by Somerled. This began a process whereby the islands of the Clyde became Scottish in language and culture rather than Norse. After Somerled's death in 1164 his kingdom was split between his three sons, Ragnall in Islay and Kintyre, Dughall in Lorne and the other Argyll islands, and Angus holding Arran and Bute.
Nearly a century later in the 1230s invading Norse forces took Rothesay Castle, hacking through the walls with their axes. In 1263 troops commanded by Haakon Haakonarson repeated the feat but the ensuing Battle of Largs between Scots and Norse forces, which took place on the shores of the Firth of Clyde, was inconclusive as a military contest. This marked an ultimately terminal weakening of Norse power in Scotland. Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. Following this ill-fated expedition, all rights that the Norwegian crown "had of old therein" in relation to the islands were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth.
Modern Scotland.
From the mid thirteenth century to the present day all of the islands of the Clyde have remained part of Scotland.
From the commencement of the early medieval period until 1387 all of these isles were part of the Diocese of Sodor and Man, based at Peel, on the Isle of Man. Thereafter, the seat of the Bishopric of the Isles was relocated to the north, firstly to Snizort on Skye and then Iona, a state of affairs which continued until the 16th century Scottish Reformation.
The century following 1750 was time of significant change. New forms of transport, industry and agriculture brought sweeping changes, and an end to traditional ways of life that had endured for centuries. The aftermath of the Battle of Culloden marked the beginning of the end for the clan system and whilst there were marked improvements in living standards for some, these transformations came at a cost for others. In the early 19th century Alexander, 10th Duke of Hamilton (1767–1852) embarked on a programme of clearances that had a devastating effect on Arran's population. Whole villages were removed and the Gaelic culture of the island dealt a terminal blow. A memorial to this early form of ethnic cleansing has been constructed on the shore at Lamlash, paid for by a Canadian descendant of the emigrants.
From the 1850s to the late 20th century the Clyde Puffer, made famous by the "Vital Spark", was the workhorse of the islands, carrying all kinds of produce and products to and from the islands. The Caledonian Steam Packet Company (CSP) was formed in May 1889 to operate steamer services to and from Gourock for the Caledonian Railway and soon expanded by taking over rival steamer operators. David MacBrayne Ltd operated the Glasgow to Ardrishaig steamer service, as part of the "Royal Route" to Oban. During the 20th century many of the islands were developed as tourist resorts for Glaswegians who went "Doon the Watter", in parallel to mainland resorts such as Largs and Troon.
In 1973 CSP and MacBraynes commenced joint Clyde and West Highland operations under the new name of Caledonian MacBrayne. A publicly owned company, they serve Great Cumbrae, Arran and Bute as well as running mainland-to-mainland ferries across the firth. Private companies operate services from Arran to Holy Isle and from McInroy's Point (Gourock) to Hunter's Quay on the Cowal peninsula.
The majority of the islands at one time made up the traditional County of Bute. Today the islands are split more or less equally between the modern unitary authorities of Argyll and Bute and North Ayrshire with only Ailsa Craig and Lady Isle in South Ayrshire falling outwith these two council areas.
Islands.
The following table gives a list of the islands of the Firth of Clyde with an area greater than 40 hectares (approximately 100 acres) plus adjacent smaller uninhabited islets, tidal islets only separated at higher stages of the tide, and skerries which are only exposed at lower stages of the tide.
Six islands were inhabited in 2001 including Davaar and Sanda with 2 and 1 residents respectively. By the time of the 2011 census neither had a usually resident population.
Outlying islands.
Some islets lie remote from the larger islands and are listed separately here by location.
Gare Loch is a small loch which hosts the Faslane Naval Base, the home of the UK's Trident nuclear submarines. At its southern end, the loch opens into the Firth of Clyde, via the Rhu narrows. It contains two islets: Green Island and Perch Rock.
The Kilbrannan Sound, which lies between Arran and the Kintyre peninsula, contains several islets: An Struthlag, Cour Island, Eilean Carrach (Carradale), Eilean Carrach (Skipness), Eilean Grianain, Eilean Sunadale, Gull Isle, Island Ross and Thorn Isle. In the late 11th century Magnus Barefoot, King of Norway, made an arrangement with King Malcolm III of Scotland that he could take possession of land on the west coast around which a ship could sail. He had his longship dragged across the 1.5 km long isthmus in the north of Kintyre between East Loch Tarbert and West Loch Tarbert as part of a campaign to increase his possessions. Magnus declared that Kintyre had "better land than the best of the Hebrides", and by taking command of his ship's tiller and "sailing" across the isthmus he was able to claim the entire peninsula was an island, which remained under Norse rule for more than a dozen years as a result.
Loch Fyne, which extends 65 km inland from the Sound of Bute is the longest of Scotland's sea lochs and contains several islets and skerries. These are Duncuan Island, Eilean Ardgaddan, Eilean a' Bhuic, Eilean Aoghainn, Eilean a' Chomhraig, Eilean an Dúnain, Eilean Buidhe (Ardmarnock), Eilean Buidhe (Portavadie), Eilean Fraoch, Eilean Math-ghamhna, Eilean Mór, Glas Eilean, Heather Island, Inverneil Island, Kilbride Island and Liath Eilean.
The North Ayrshire islets of Broad Rock, East Islet, Halftide Rock, High Rock and North Islet are all found surrounding Horse Isle. Lady Isle, which lies off the South Ayrshire coast near Troon once housed "ane old chapell with an excellent spring of water". However, in June 1821 someone set fire to the "turf and pasture", and permanently destroyed the island's grazing, with gales blowing much of the island's soil into the sea.
Neither Loch Goil nor Loch Long, which are fjord-like arms of the firth to the north, contain islands.
Non-islands.
The following are places along that shores of the Firth of Clyde that are not islands and have misleading names, "eilean" being Gaelic for "island": Eilean na Beithe, Portavadie; Eilean Beag, Cove; Eilean Dubh, Dalchenna, Loch Fyne; Eilean nan Gabhar, Melldalloch, Kyles of Bute; Barmore Island, just north of Tarbert, Kintyre; Eilean Aoidh, south of Portavadie; Eilean Leathan, Kilbrannan Sound just south of Torrisdale Bay; Island Muller, Kilbrannan Sound north of Campbeltown.
Natural history.
There are populations of red deer, red squirrel, badger, otter, adder and common lizard. Offshore there are harbour porpoises, basking sharks and various species of dolphin. Davaar is home to a population of wild goats.
Over 200 species of bird have been recorded in the area including black guillemot, eider, peregrine falcon and the golden eagle. In 1981 there were 28 ptarmigan on Arran, but in 2009 it was reported that extensive surveys had been unable to record any. Similarly, the red-billed chough no longer breeds on the island.
Arran also has three rare endemic species of tree, the Arran Whitebeams. These are the Scottish or Arran whitebeam, the cut-leaved whitebeam and the Catacol whitebeam, which are amongst the most endangered tree species in the world. They are found in a protected national nature reserve, and are monitored by staff from Scottish Natural Heritage. Only 283 Arran whitebeam and 236 cut-leaved whitebeam were recorded as mature trees in 1980. The Catacol whitebeam was discovered in 2007 and steps have been taken to protect the two known specimens.
Etymology.
The Roman historian Tacitus refers to the "Clota" meaning the Clyde. The derivation is not certain but probably from the Brythonic "Clouta" which became "Clut" in Old Welsh. The name's literal meaning is "wash" but probably refers to the idea of a river goddess being "the washer" or "strongly flowing one". Bute's derivation is also uncertain. "Bót" is the Norse name and this is the Old Irish word for "fire", possibly a reference to signal fires. The etymology of Arran is no more clear—Haswell-Smith (2004) offers a Brythonic derivation and a meaning of "high place" although Watson (1926) suggests it may be pre-Celtic.
References.
</dl>

</doc>
<doc id="15253" url="http://en.wikipedia.org/wiki?curid=15253" title="International Bank Account Number">
International Bank Account Number

The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. It was originally adopted by the European Committee for Banking Standards (ECBS), and later as an international standard under ISO 13616:1997. The current standard is ISO 13616:2007, which indicates SWIFT as the formal registrar. Initially developed to facilitate payments within the European Union, it has been implemented by most European countries and many countries in the developing world, especially in the Middle East and in the Caribbean. As at September 2014, 66 countries were using the IBAN numbering system.
The IBAN consists of up to 34 alphanumeric characters, comprising a country code, two check digits and a long and detailed bank account-number. The check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.
Background.
Before IBAN, differing national standards for bank account identification (i.e. bank, branch, routing codes, and account number) were confusing for some users. This often led to necessary routing information being missing from payments. Routing information as specified by ISO 9362 (also known as Business Identifier Codes (BIC code), SWIFT ID or SWIFT code, and SWIFT-BIC) does not require a specific format for the transaction so the identification of accounts and transaction types is left to agreements of the transaction partners. It also does not contain check digits, so errors of transcription were not detectable and it was not possible for a sending bank to validate the routing information prior to submitting the payment. Routing errors caused delayed payments and incurred extra costs to the sending and receiving banks and often to intermediate routing banks.
In 1997, to overcome these difficulties, the International Organization for Standardization (ISO) published ISO 13616:1997. This proposal had a degree of flexibility, which the European Committee for Banking Standards (ECBS) believed would make it unworkable, and they produced a "slimmed down" version of the standard which, amongst other things, permitted only upper-case letters and required that the IBAN for each country had a fixed length. ISO 13616:1997 was subsequently withdrawn and replaced by ISO 13616:2003. The standard was revised again in 2007 when it was split into two parts. ISO 13616-1:2007 "specifies the elements of an international bank account number (IBAN) used to facilitate the processing of data internationally in data interchange, in financial environments as well as within and between other industries" but "does not specify internal procedures, file organization techniques, storage media, languages, etc. to be used in its implementation". ISO 13616-2:2007 describes "the Registration Authority (RA) responsible for the registry of IBAN formats that are compliant with ISO 13616-1 [and] the procedures for registering ISO 13616-compliant IBAN formats". The official IBAN registrar under ISO 13616-2:2007 is SWIFT.
IBAN imposes a flexible but regular format sufficient for account identification and contains validation information to avoid errors of transcription. It carries all the routing information needed to get a payment from one bank to another wherever it may be; it contains key bank account details such as country code, branch codes (known as sort codes in the UK and Ireland) and account numbers, and it contains "check digits" which can be validated at source according to a single standard procedure. Where used, IBANs have reduced trans-national money transfer errors to under 0.1% of total payments.
Structure.
The IBAN consists of up to 34 alphanumeric characters, as follows:
The check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.
The IBAN should not contain spaces when transmitted electronically. When printed it is expressed in groups of four characters separated by a single space, the last group being of variable length as shown in the example below:
Permitted IBAN characters are the digits "0" to "9" and the 26 upper-case Latin alphabetic characters "A" to "Z". This applies even in countries (e.g., Thailand) where these characters are not used in the national language.
Basic Bank Account Number.
The Basic Bank Account Number (BBAN) format is decided by the national central bank or designated payment authority of each country. There is no consistency between the formats adopted. The national authority may register its BBAN format with SWIFT, but is not obliged to do so. It may adopt IBAN without registration. SWIFT also acts as the registration authority for the SWIFT system, which is used by most countries that have not adopted IBAN. A major difference between the two systems is that under SWIFT there is no requirement that BBANs used within a country be of a pre-defined length.
The BBAN must be of a fixed length for the country and comprise case-insensitive alphanumeric characters. It includes the domestic bank account number, branch identifier, and potential routing information. Each country can have a different national routing/account numbering system, up to a maximum of 30 alphanumeric characters.
Check digits.
The check digits enable the sending bank (or its customer) to perform a sanity check of the routing destination and account number from a single string of data at the time of data entry. This check is guaranteed to detect any instances where a single character has been omitted, duplicated, mistyped or where two characters have been transposed. Thus routing and account number errors are virtually eliminated.
Processing.
One of the design aims of the IBAN was to enable as much validation as possible to be done at the point of data entry. In particular, the computer program that accepts an IBAN will be able to validate:
The check digits are calculated using MOD-97-10 as per ISO/IEC 7064:2003 (abbreviated to "mod-97" in this article), which specifies a set of check character systems capable of protecting strings against errors which occur when people copy or key data. In particular, the standard states that the following can be detected:
The underlying rules for IBANs is that the account-servicing financial institution should issue an IBAN, as there are a number of areas where different IBANs could be generated from the same account and branch numbers that would satisfy the generic IBAN validation rules. In particular cases where 00 is a valid check digit, 97 will not be a valid check digit, likewise, if 01 is a valid check digit, 98 will not be a valid check digit, similarly with 02 and 99.
The UN CEFACT TBG5 has published a free IBAN validation service in 32 languages for all 57 countries that have adopted the IBAN standard. They have also published the Javascript source code of the verification algorithm.
An English language IBAN checker for ECBS member country bank accounts is available on its website.
Algorithms.
Validating the IBAN.
An IBAN is validated by converting it into an integer and performing a basic "mod-97" operation (as described in ISO 7064) on it. If the IBAN is valid, the remainder equals 1. The algorithm of IBAN validation is as follows:
If the remainder is 1, the check digit test is passed and the IBAN might be valid.
Example (fictitious United Kingdom bank, sort code 12-34-56, account number 98765432):
Generating IBAN check digits.
According to the ECBS "generation of the IBAN shall be the exclusive responsibility of the bank/branch servicing the account". The ECBS document replicates part of the ISO/IEC 7064:2003 standard as a method for generating check digits in the range 02 to 98. Check digits in the ranges 00 to 96, 01 to 97, and 03 to 99 will also provide validation of an IBAN, but the standard is silent as to whether or not these ranges may be used.
The preferred algorithm is:
Modulo operation on IBAN.
Any computer programming language or software package that is used to compute "D" mod "97" directly must have the ability to handle integers of more than 30 digits. In practice, this can only be done by software that either supports arbitrary-precision arithmetic or that can handle 220 bit (unsigned) integers, features that are often not standard. If the application software in use does not provide the ability to handle integers of this size, the modulo operation can be performed in a piece-wise manner (as is the case with the UN CEFACT TBG5 Javascript program).
Piece-wise calculation "D" mod "97" can be done in many ways. One such way is as follows:
The result of the final calculation in step 2 will be "D" mod 97 = "N" mod "97".
Example
In this example, the above algorithm for "D" mod 97 will be applied to "D" = 3214282912345698765432161182. (The digits are colour-coded to aid the description below.) If the result is one, the IBAN corresponding to "D" passes the check digit test.
From step 8, the final result is "D" mod 97 = 1 and the IBAN has passed this check digit test.
Adoption.
International bank transactions use either an IBAN or the ISO 9362 Bank Identifier Code system (BIC or SWIFT code) in conjunction with the BBAN.
The banks of most countries in Europe publish account numbers using both the IBAN format and the nationally recognised identifiers, this being mandatory within the European Economic Area. Banks in many other countries including most states of the Middle East, North Africa and the Caribbean have implemented the IBAN format for account identification.
Day-to-day administration of banking in British Overseas Territories varies from territory to territory - some, such as South Georgia and the South Sandwich Islands, have too small a population to warrant a banking system while others, such as Bermuda, have a thriving financial sector. The use of the IBAN is up to the local government - Gibraltar, being part of the European Union is required to use the IBAN, as are the Crown dependencies, which use the British clearing system and the British Virgin Islands have elected to do so. s of 2013[ [update]], no other British Overseas Territories have elected to use the IBAN. Banks in the Caribbean Netherlands also do not use the IBAN. In some countries the IBAN is used on an "ad hoc" basis, an example being Ukraine where account numbers used for international transfers of four of the national banks have additional aliases that follow the IBAN format as a precursor to formal SWIFT registration.
The degree to which bank verifies the validity of a recipient's bank account number depends of the configuration of the transmitting bank's software—many major software packages supply bank account validation as a standard function. Some banks outside Europe may not recognize IBAN, though this is expected to diminish with time. Non-European banks usually accept IBANs for accounts in Europe, although they might not treat IBANs differently from other foreign bank account numbers. In particular, they might not check the IBAN's validity prior to sending the transfer.
Banks in the United States do not use IBAN as account numbers for US accounts. Any adoption of the IBAN standard by US banks would likely be initiated by ANSI ASC X9, the US financial services standards development organization: a working group (WGAB20) was established as an X9 subcommittee to generate an IBAN construction for US Bank accounts. Canadian financial institutions have not adopted IBAN and use bank transit numbers issued by the Canadian Payments Association for domestic transfers, and SWIFT for international transfers. There is no formal governmental or private sector regulatory requirement in Canada for the major banks to use IBAN. Financial institutions in Australia and New Zealand do not use IBAN. They use Bank State Branch codes for domestic transfers and SWIFT for international transfers.
Single Euro Payments Area (SEPA).
The IBAN designation scheme was chosen as the foundation for electronic straight-through processing in the European Economic Area. The European Parliament mandated that a bank charge needs to be the same amount for domestic credit transfers as for cross-border credit transfers regulated in decision 2560/2001 (updated in 924/2009). This regulation took effect in 2003. Only payments in euro up to €12,500 to a bank account designated by its IBAN were covered by the regulation.
The Euro Payments regulation has been the foundation for the decision to create a Single Euro Payments Area (SEPA). The European Central Bank has created the TARGET2 interbank network that unifies the technical infrastructure of the 26 central banks of the European Union (although Sweden and the UK have opted-out). SEPA is a self-regulatory initiative by the banking sector of Europe as represented in the European Payments Council (EPC). The European Union made the scheme mandatory through the Payment Services Directive published in 2007. Since January 2008, all countries must support SEPA credit transfer, and SEPA direct debit must be supported since November 2009. The regulation on SEPA payments increases the charge cap (same price for domestic payments as for cross-border payments) to €50,000.
With a further decision of the European Parliament, the IBAN scheme for bank accounts fully replaced the domestic numbering schemes from 31 December 2012. On 16 December 2010, the European Commission published proposed regulations that will make IBAN support mandatory for domestic credit transfer by 2013 and for domestic direct debit by 2014 (with a 12 and 24 months transition period respectively). Some countries have already replaced their traditional bank account scheme by IBAN. This includes Switzerland where IBAN was introduced for national credit transfer on 1 January 2006 and the support for the old bank account numbers has not been required from 1 January 2010.
Based on a 20 December 2011 memorandum, the EU parliament resolved the mandatory dates for the adoption of the IBAN on 14 February 2012. From 1 February 2014, all national systems for credit transfer and direct debit must be abolished and replaced by an IBAN-based system. This will be extended to all cross-border SEPA transactions from 1 February 2016 (Article 5 Section 7). After these dates the IBAN will be sufficient to identify an account for home and foreign financial transactions in SEPA countries and banks will no longer be permitted to require that the customer supply the BIC for the beneficiary's bank.
In the run-up to the 1 February 2014 deadline, it became apparent that many old bank account numbers had not been allocated IBANs—an issue that has to be addressed on a country-by-country basis. In Germany, for example, the German Federal Bank and the German Banking Industry Committee require that all holders of German bank codes ("Bankleitzahl") publish the specifics of their IBAN generation format taking into account not only the generation of check digits but also the handling of legacy bank codes, thereby enabling third parties to generate IBANs independently of the bank. The first such catalogue was published in June 2013 as a variant of the old bank code catalog ("Bankleitzahlendatei").
IBAN formats by country.
This table summarises the IBAN formats by country:
In addition to the above list, Nordea has catalogued IBANs for countries listed below.
In this list

</doc>
<doc id="15254" url="http://en.wikipedia.org/wiki?curid=15254" title="Infinitive">
Infinitive

Infinitive is a grammatical term used to refer to certain verb forms that exist in many languages. As with many linguistic concepts, there is not a single definition applicable to all languages. The word is derived from Late Latin "[modus] infinitivus", a derivative of "infinitus" meaning "infinite". Infinitives are used mostly as non-finite verbs.
In traditional descriptions of English, the infinitive is the basic dictionary form of a verb when used non-finitely, with or without the particle "to". Thus "to go" is an infinitive, as is "go" in a sentence like "I must go there" (but not in "I go there", where it is a finite verb). The form without "to" is called the bare infinitive, and the form with "to" is called the full infinitive or "to"-infinitive.
In many other languages the infinitive is a single word, often with a characteristic inflective ending, such as "morir" ("(to) die") in Spanish, "manger" ("(to) eat") in French, "portare" ("(to) carry") in Latin, "lieben" ("(to) love") in German, etc. However some languages do not have any forms identifiable as infinitives. Many Native American languages and some languages in Africa and Australia do not have direct equivalents to infinitives or verbal nouns; in their place they use finite verb forms in ordinary clauses or various special constructions.
As a verb, an infinitive may take objects and other complements and modifiers to form a verb phrase (called an infinitive phrase). Like other non-finite verb forms (such as participles, converbs, gerunds and gerundives) infinitives do not generally have an expressed subject; thus an infinitive verb phrase also constitutes a complete non-finite clause, called an infinitive (infinitival) clause. Such phrases or clauses may play a variety of roles within sentences, often as nouns (for example as the subject of a sentence or as a complement of another verb), and sometimes as adverbs or other types of modifier. Infinitives are not usually inflected for tense, person, etc. as finite verbs are, although some degree of inflection sometimes occurs; for example Latin has distinct active and passive infinitives.
Infinitive phrases and clauses.
An "infinitive phrase" is a verb phrase constructed with the verb in infinitive form. This consists of the verb together with its objects and other complements and modifiers. Some examples of infinitive phrases in English are given below – these may be based on either the full infinitive (introduced by the particle "to") or the bare infinitive (without the particle "to").
Infinitive phrases often have an implied grammatical subject making them effectively clauses rather than phrases. Such "infinitive clauses" or "infinitival clauses", are one of several kinds of non-finite clause. They can play various grammatical roles as a constituent of a larger clause or sentence; for example it may serve as a noun phrase or adverb. Infinitival clauses may be embedded within each other in complex ways, as in the sentence:
Here the infinitival clause "to get married" is contained within the finite dependent clause "that John is going to get married"; this in turn is contained within another infinitival clause, which is contained in the finite independent clause (the whole sentence).
The grammatical structure of an infinitival clause may differ from that of a corresponding finite clause. For example, in German, the infinitive form of the verb usually goes to the end of its clause, whereas a finite verb (in an independent clause) typically comes in second position.
Infinitive clauses with subject in the objective case.
Following certain verbs or prepositions, infinitives commonly "do" have an expressed subject, e.g. 
As these examples illustrate, the subject of the infinitive is in the objective case (them, him) in contrast to the nominative case that would be used with a finite verb, e.g. "They ate their dinner." 
Such accusative and infinitive constructions are present in Latin and Ancient Greek, as well as many modern languages. The unusual case for the subject of an infinitive is an example of exceptional case-marking, where the infinitive clause's role as object of a verb or preposition (want, for) overpowers the pronoun's subjective role within the clause.
Marking for tense, aspect and voice <span id="Perfect" />.
In some languages, infinitives may be marked for grammatical categories such as voice, aspect, and to some extent tense. This may be done by inflection, as with the Latin perfect and passive infinitives, or by periphrasis (with the use of auxiliary verbs), as with the Latin future infinitives or the English perfect and progressive infinitives.
Latin has present, perfect and future infinitives, with active and passive forms of each. For details see Latin conjugation: Infinitives.
English has infinitive constructions which are marked (periphrastically) for aspect: perfect, progressive (continuous), or a combination of the two (perfect progressive). These can also be marked for passive voice (as can the plain infinitive):
Further constructions can be made with other auxiliary-like expressions, such as "(to) be going to eat" or "(to) be about to eat", which have future meaning. For more examples of the above types of construction, see Uses of English verb forms: Perfect and progressive non-finite constructions.
Perfect infinitives are also found in other European languages which have perfect forms with auxiliaries similarly to English. For example, "avoir mangé" means "(to) have eaten" in French.
English.
Regarding English, the term "infinitive" is traditionally applied to the unmarked form of the verb when used as a non-finite verb, whether or not introduced by the particle "to". Hence "sit" and "to sit", as used in the following sentences, would each be described as an infinitive:
The form without "to" is called the "bare infinitive"; the form introduced by "to" is called the "full infinitive" or "to-infinitive".
The other non-finite verb forms in English are the gerund or present participle (the "-ing" form), and the past participle – these are not referred to as infinitives. Moreover the unmarked form of the verb is not regarded as an infinitive when it is used as a finite verb: as a present indicative ("I "sit" every day"), subjunctive ("I suggest that he "sit"), or imperative ("Sit" down!"). (For some irregular verbs the form of the infinitive coincides additionally with that of the past tense and/or past participle, as in the case of "put".)
Certain auxiliary verbs are defective in that they do not have infinitives (or any other non-finite forms). This applies to the modal verbs ("can", "must", etc.), as well as certain related auxiliaries such as the "had" of "had better" and the "used" of "used to". (Periphrases can be used instead in some cases, such as "(to) be able to" for "can", and "(to) have to" for "must".) It also applies to the auxiliary "do", as used in questions, negatives and emphasis as described under "do"-support. (Infinitives are negated by simply preceding them with "not". Of course the verb "do" when used as a main verb can appear in the infinitive.) However, the auxiliary verbs "have" (used to form the perfect) and "be" (used to form the passive voice and continuous aspect) both commonly appear in the infinitive: "I should have finished by now"; "It's thought to have been a burial site"; "Let him be released"; "I hope to be working tomorrow."
A matter of controversy among prescriptive grammarians and style writers has been the appropriateness of separating the two words of the "to"-infinitive (as in "I expect "to" happily "sit" here"). For details of this, see split infinitive. Modern linguistic theories typically do not consider the "to"-infinitive to be a distinct constituent, instead regarding the particle "to" as operating on an entire verb phrase; so, "to buy a car" is parsed as "to [buy [a car]]", not as "[to buy] [a car]".
Huddleston and Pullum's "Cambridge Grammar of the English Language" (2002) does not use the notion of the "infinitive", only that of the "infinitival clause", noting that English uses the same form of the verb, the "plain form", in infinitival clauses that it uses in imperative and present-subjunctive clauses.
Uses of the infinitive.
The bare infinitive and the "to"-infinitive have a variety of uses in English. The two forms are mostly in complementary distribution – certain contexts call for one, and certain contexts for the other; they are not normally interchangeable, except in occasional instances such as after the verb "help", where either can be used.
The main uses of infinitives (or infinitive phrases) are as follows:
The infinitive is also the usual dictionary form or citation form of a verb. The form listed in dictionaries is the bare infinitive, although the "to"-infinitive is often used in referring to verbs or in defining other verbs: "The word 'amble' means 'to walk slowly'"; "How do we conjugate the verb "to go"?"
For further detail and examples of the uses of infinitives in English, see Bare infinitive and "To"-infinitive in the article on uses of English verb forms.
Other Germanic languages.
The original Proto-Germanic ending of the infinitive was "-an", with verbs derived from other words ending in "-jan" or "-janan".
In German it is "-en" ("sagen"), with "-eln" or "-ern" endings on a few words based on -l or -r roots ("segeln", "ändern"). The use of "zu" with infinitives is similar to English "to", but is less frequent than in English. German infinitives can function as nouns, often expressing abstractions of the action, in which case they are of neuter gender: "das Essen" means "the eating", but also "the food".
In Dutch infinitives also end in "-en" ("zeggen" — "to say"), sometimes used with "te" similar to English "to", e.g. "Het is niet moeilijk te begrijpen" → "It is not hard to understand." The few verbs with stems ending in "-a" have infinitives in -n ("gaan" — "to go", "slaan" — "to hit"). Afrikaans has lost the distinction between the infinitive and present forms of verbs, with the exception of the verbs "wees" (to be), which admits the present form "is", and the verb "hê" (to have), whose present form is "het".
In North Germanic languages the final "-n" was lost from the infinitive as early as 500–540 AD, reducing the suffix to "-a". Later it has been further reduced to "-e" in Danish and some Norwegian dialects (including the written majority language bokmål). In the majority of Eastern Norwegian dialects and a few bordering Western Swedish dialects the reduction to "-e" was only partial, leaving some infinitives in "-a" and others in "-e" (å laga vs. å kaste). In northern parts of Norway the infinitive suffix is completely lost (å lag’ vs. å kast’) or only the "-a" is kept (å laga vs. å kast’). The infinitives of these languages are inflected for passive voice through the addition of "-s" or "-st" to the active form. This suffix appeared in Old Norse as a contraction of "mik" (“me”, forming "-mk") or "sik" (reflexive pronoun, forming "-sk") and was originally expressing reflexive actions: (hann) "kallar" (“(he) calls”) + "-sik" (“himself”) > (hann) "kallask" (“(he) calls himself”). The suffixes "-mk" and "-sk" later merged to "-s", which evolved to "-st" in the western dialects. The loss or reduction of "-a" in active voice in Norwegian did not occur in the passive forms ("-ast", "-as"), except for some dialects that have "-es". The other North Germanic languages have the same vowel in both forms.
Latin and Romance languages.
The formation of the infinitive in the Romance languages reflects that in their ancestor, Latin, almost all verbs had an infinitive ending with "-re" (preceded by one of various thematic vowels). For example, in Italian infinitives end in "-are", "-ere", "-rre" (rare), or "-ire", and in "-arsi", "-ersi", "-rsi", "-irsi" for the reflexive forms. In Spanish and Portuguese, infinitives end in "-ar", "-er", or "-ir", while similarly in French they typically end in "-re", "-er", "oir", and "-ir". In Romanian the so-called "long infinitives" end in "-are, -ere, -ire" and they are converted into verbal nouns by articulation (verbs that cannot be converted into the nominal long infinitive are very rare). The "short infinitives" used in verbal contexts (e.g. after an auxiliary verb) have the endings "-a","-ea", "-e", and "-i" (basically removing the ending in "-re"). In Romanian, the infinitive is usually replaced by a clause containing the preposition "sǎ" plus the subjunctive mood. The only verb that is modal in common modern Romanian is the verb "a putea", to be able to. But in popular speech, the infinitive after a putea is also increasingly replaced by the subjunctive.
In all Romance languages, infinitives can also be used as nouns.
Latin infinitives challenged several of the generalizations about infinitives. They did inflect for voice ("amare", "to love", "amari", to be loved) and for aspect ("amare", "to love", "amavisse", "to have loved"), and allowed for an overt expression of the subject ("video Socratem currere", "I see Socrates running"). See Latin conjugation: Infinitives.
Romance languages inherited from Latin the possibility of an overt expression of the subject (as in Italian "vedo Socrate correre"). Moreover, the "inflected infinitive" (or "personal infinitive") found in Portuguese and Galician inflects for person and number. These are the only Indo-European languages that allow infinitives to take person and number endings. This helps to make infinitive clauses very common in these languages; for example, the English finite clause "in order that you/she/we have..." would be translated to Portuguese as "para teres/ela ter/termos..." (Portuguese is a null-subject language). The Portuguese personal infinitive has no proper tenses, only aspects (imperfect and perfect), but tenses can be expressed using periphrastic structures. For instance, "even though you sing/have sung/are going to sing" could be translated to "apesar de cantares/teres cantado/ires cantar".
Other Romance languages (including Spanish, Romanian, Catalan, and some Italian dialects) allow uninflected infinitives to combine with overt nominative subjects. For example, Spanish "al abrir yo los ojos" ("when I opened my eyes") or "sin yo saberlo" ("without my knowing about it").
Hellenic languages.
Ancient Greek.
In Ancient Greek the infinitive has four tenses (present, future, aorist, perfect) and three voices (active, middle, passive). Unique forms for the middle are found only in the future and aorist; in the present and perfect, middle and passive are the same.
Thematic verbs form present active infinitives by adding to the stem the theme vowel -ε- and the infinitive ending -εν, and contract to form an -ειν (from εεν) ending, e.g. παιδεύειν. Athematic verbs add the sole suffix -ναι instead, e.g. διδόναι. In the middle voice, the present middle infinitive ending is -σθαι, e.g. δίδοσθαι and thematic verbs add an additional -ε- between the ending and the stem, e.g. παιδεύεσθαι.
Modern Greek.
The infinitive per se does not exist in Modern Greek. To see this, consider the ancient Greek "ἐθέλω γράφειν" "I want to write". In modern Greek this become "θέλω να γράψω" "I want that I write". In modern Greek, the infinitive has thus changed form and is used mainly in the formation of tenses and not with an article or alone. Instead of the Ancient Greek infinitive "γράφειν", Modern Greek uses the aorist infinitive "γράψει", which does not inflect. The modern Greek infinitive has only two forms according to voice: for example, "γράψει" for the active voice and "γραφ(τ)εί" for the passive voice.
Balto-Slavic languages.
The infinitive in Russian usually ends in "-t’" (ть) preceded by a thematic vowel, or "-ti" (ти), if not preceded by one; some verbs have a stem ending in a consonant and change the "t" to "č’", such as "*mogt’ → moč’" (*могть → мочь) "can". Some other Balto-Slavic languages have the infinitive typically ending in, for example, "-ć" (sometimes "-c") in Polish, "-t’" in Slovak, "-t" (formerly "-ti") in Czech and Latvian (with a handful ending in -s on the latter), "-ty" (-ти) in Ukrainian, -ць ("-ts"') in Belarusian. Lithuanian infinitives end in -"ti", Slovenian end on -"ti" or -"či", and Croatian on -"ti" or -"ći".
Serbian officially retains infinitives -"ti" or -"ći", but is more flexible than the other Slavs in breaking the infinitive through a clause. The infinitive nevertheless remains the dictionary form. Bulgarian and Macedonian have lost the infinitive altogether (it usually ended in -ти). For that reason, the present first-person singular conjugation is used as the dictionary form in Bulgarian, where as Macedonian uses the third person singular form of the verb in present tense.
Biblical Hebrew.
Hebrew has "two" infinitives, the infinitive absolute and the infinitive construct. The infinitive construct is used after prepositions and is inflected with pronominal endings to indicate its subject or object: "bikhtōbh hassōphēr" "when the scribe wrote", "ahare lekhtō" "after his going". When the infinitive construct is preceded by ל ("lə-", "li-", "lā-") "to", it has a similar meaning as the English "to"-infinitive, and this is its most frequent use in Modern Hebrew. The infinitive absolute is used for verb focus, as in מות ימות "mōth yāmūth" (literally "a dying he will die"; figuratively, "he shall indeed/surely die"). This usage is commonplace in the Bible, but in Modern Hebrew it is restricted to high-flown literary works.
Note, however, that the "to"-infinitive of Hebrew is not the dictionary form; that is the third person singular perfect form.
Finnish.
The Finnish grammatical tradition includes many non-finite forms that are generally labeled as (numbered) infinitives although many of these are functionally converbs. To form the so-called first infinitive, the strong form of the root (without consonant gradation or epenthetic 'e') is used, and these changes occur:
As such, it is inconvenient for dictionary use, because the imperative would be closer to the root word. Nevertheless, dictionaries use the first infinitive.
There are also four other infinitives, plus a "long" form of the first:
Note that all of these must change to reflect vowel harmony, so the fifth infinitive (with a third-person suffix) of "hypätä" "jump" is "hyppäämäisillään" "he was about to jump", not "*hyppäämaisillaan".
Seri.
The Seri language of northwestern Mexico has infinitival forms which are used in two constructions (with the verb meaning 'want' and with the verb meaning 'be able'). The infinitive is formed by adding a prefix to the stem: either "iha-" [iʔa-] (plus a vowel change of certain vowel-initial stems) if the complement clause is transitive, or "ica-" [ika-] (and no vowel change) if the complement clause is intransitive. The infinitive shows agreement in number with the controlling subject. Examples are: "icatax ihmiimzo" 'I want to go', where "icatax" is the singular infinitive of the verb 'go' (singular root is "-atax"), and "icalx hamiimcajc" 'we want to go', where "icalx" is the plural infinitive. Examples of the transitive infinitive: "ihaho" 'to see it/him/her/them' (root "-aho"), and "ihacta" 'to look at it/him/her/them' (root "-oocta").
Translation to languages without an infinitive.
In languages without an infinitive, the infinitive is translated either as a "that"-clause or as a verbal noun. For example, in Literary Arabic the sentence "I want to write a book" is translated as either "urīdu an aktuba kitāban" (lit. "I want that I write a book", with a verb in the subjunctive mood) or "urīdu kitābata kitābin" (lit. "I want the writing of a book", with the "masdar" or verbal noun), and in Levantine Colloquial Arabic "biddi aktub kitāb" (subordinate clause with verb in subjunctive).
Even in languages that have infinitives, similar constructions are sometimes necessary where English would allow the infinitive. For example, in French the sentence "I want you to come" translates to "Je veux que vous veniez" (lit. "I want that you come", with "come" being in the subjunctive mood). However, "I want to come" is simply "Je veux venir", using the infinitive, just as in English. In Russian, sentences such as "I want you to leave" do not use an infinitive. Rather, they use the conjunction чтобы "in order to/so that" with the past tense form (most probably remnant of subjunctive) of the verb: "Я хочу, чтобы вы ушли" (literally, "I want so that you left").

</doc>
<doc id="15256" url="http://en.wikipedia.org/wiki?curid=15256" title="Immaculate Conception">
Immaculate Conception

The Immaculate Conception, according to the teaching of the Catholic Church, was the conception of the Blessed Virgin Mary in her mother's womb free from original sin by virtue of the foreseen merits of her son Jesus Christ.
The Immaculate Conception is commonly confused with the doctrine of the Incarnation and the virgin birth of Jesus, though the two deal with separate subjects. The Catholic Church teaches that Mary was conceived by normal biological means, but her soul was acted upon by God (kept "immaculate") at the time of her conception.
Although the belief that Mary was sinless and conceived immaculate has been widely held since Late Antiquity, the doctrine was not dogmatically defined until 1854, by Pope Pius IX in his papal bull "Ineffabilis Deus". The Catholic Church celebrates the Feast of the Immaculate Conception on December 8; in many Catholic countries, it is a holy day of obligation or patronal feast, and in some a national public holiday.
Protestants generally reject the doctrine because they do not consider the development of dogmatic theology to be authoritative apart from Biblical exegesis. It is accepted by some Anglo-Catholics, but is rejected by most in the Anglican Communion.
Distinctions.
Original sin and actual (personal) sin.
The defined dogma of the Immaculate Conception regards original sin only, saying that Mary was preserved from any "stain" (in Latin, "macula" or "labes", the second of these two synonymous words being the one used in the formal definition). The proclaimed Roman Catholic dogma states "that the most Blessed Virgin Mary, in the first instance of her conception, by a singular grace and privilege granted by Almighty God, in view of the merits of Jesus Christ, the Saviour of the human race, was preserved free from all stain of original sin." Therefore, being always free from original sin, the doctrine teaches that from her conception Mary received the sanctifying grace that would normally come with baptism after birth.
The definition makes no declaration about the Church's belief that the Blessed Virgin was sinless in the sense of freedom from actual or personal sin. However, the Church holds that Mary was also sinless personally, "free from all sin, original or personal". The Council of Trent decreed: "If anyone shall say that a man once justified can sin no more, nor lose grace, and that therefore he who falls and sins was never truly justified; or, on the contrary, that throughout his whole life he can avoid all sins even venial sins, except by a special privilege of God, as the Church holds in regard to the Blessed Virgin: let him be anathema."
Virginal conception.
The doctrine of the immaculate conception (Mary being conceived free from original sin) is not to be confused with her virginal conception of her son Jesus. This misunderstanding of the term "immaculate conception" is frequently met in the mass media. Catholics believe that Mary was not the product of a virginal conception herself but was the daughter of a human father and mother, traditionally known by the names of Saint Joachim and Saint Anne. In 1677, the Holy See condemned the belief that Mary was virginally conceived, which had been a belief surfacing occasionally since the 4th century. The Church celebrates the Feast of the Immaculate Conception (when Mary was conceived free from original sin) on 8 December, exactly nine months before celebrating the Nativity of Mary. The feast of the Annunciation (which commemorates the virginal conception and the Incarnation of Jesus) is celebrated on 25 March, nine months before Christmas Day.
Redemption.
Another misunderstanding is that, by her immaculate conception, Mary did not need a saviour. When defining the dogma in "Ineffabilis Deus", Pope Pius IX explicitly affirmed that Mary was redeemed in a manner more sublime. He stated that Mary, rather than being cleansed after sin, was completely prevented from contracting Original Sin in view of the foreseen merits of Jesus Christ, the Savior of the human race. In , Mary proclaims: "My spirit has rejoiced in God my Saviour." This is referred to as Mary's pre-redemption by Christ. Since the Council of Orange II against semi-pelagianism, the Catholic Church has taught that even had man never sinned in the Garden of Eden and was sinless, he would still require God's grace to remain sinless.
History.
A feast of the Conception of the Most Holy and All Pure Mother of God was celebrated in Syria on 8 December perhaps as early as the 5th century. Note that the title of "achrantos" (spotless, immaculate, all-pure) refers to the holiness of Mary, not specifically to the holiness of her conception.
Mary's complete sinlessness and concomitant exemption from any taint from the first moment of her existence was a doctrine familiar to Greek theologians of Byzantium. Beginning with St. Gregory Nazianzen, his explanation of the "purification" of Jesus and Mary at the circumcision (Luke 2:22) prompted him to consider the primary meaning of "purification" in Christology (and by extension in Mariology) to refer to a perfectly sinless nature that manifested itself in glory in a moment of grace (e.g., Jesus at his Baptism). St. Gregory Nazianzen designated Mary as “prokathartheisa (prepurified).” Gregory likely attempted to solve the riddle of the Purification of Jesus and Mary in the Temple through considering the human natures of Jesus and Mary as equally holy and therefore both purified in this manner of grace and glory. Gregory's doctrines surrounding Mary's purification were likely related to the burgeoning commemoration of the Mother of God in and around Constantinople very close to the date of Christmas. Nazianzen's title of Mary at the Annunciation as "prepurified" was subsequently adopted by all theologians interested in his Mariology to justify the Byzantine equivalent of the Immaculate Conception. This is especially apparent in the Fathers St. Sophronios of Jerusalem and St. John Damascene, who will be treated below in this article at the section on Church Fathers. About the time of Damascene, the public celebration of the "Conception of St. Ann [i.e., of the Theotokos in her womb]" was becoming popular. After this period, the "purification" of the perfect natures of Jesus and Mary would not only mean moments of grace and glory at the Incarnation and Baptism and other public Byzantine liturgical feasts, but purification was eventually associated with the feast of Mary's very conception (along with her Presentation in the Temple as a toddler) by Orthodox authors of the 2nd millennium (e.g., St. Nicholas Cabasilas and Joseph Bryennius).
Church Fathers.
It is admitted that the doctrine as defined by Pius IX was not explicitly mooted before the 12th century. It is also agreed that "no direct or categorical and stringent proof of the dogma can be brought forward from Scripture". But it is claimed that the doctrine is implicitly contained in the teaching of the Fathers. Their expressions on the subject of the sinlessness of Mary are, it is pointed out, so ample and so absolute that they must be taken to include original sin as well as actual. Thus in the first five centuries such epithets as "in every respect holy", "in all things unstained", "super-innocent", and "singularly holy" are applied to her; she is compared to Eve before the fall, as ancestress of a redeemed people; she is "the earth before it was accursed". The well-known words of St. Augustine (d. 430) may be cited: "As regards the mother of God," he says, "I will not allow any question whatever of sin." It is true that he is here speaking directly of actual or personal sin. But his argument is that all men are sinners; that they are so through original depravity; that this original depravity may be overcome by the grace of God, and he adds that he does not know but that Mary may have had sufficient grace to overcome sin "of every sort" ("omni ex parte").
Although the doctrine of Mary's Immaculate Conception appears only later among Latin (and particularly Frankish) theologians, It became ever more manifest among Byzantine theologians reliant on Gregory Nazianzen's Mariology in the Medieval or Byzantine East. Although hymnographers and scholars, like the Emperor Justinian I, were accustomed to call Mary "prepurified" in their poetic and credal statements, the first point of departure for more fully commenting on Nazianzen's meaning occurs in Sophronius of Jerusalem. In other places Sophronius explains that the Theotokos was already immaculate, when she was "purified" at the Annunciation and goes so far as to note that John the Baptist is literally "holier than all 'Men' born of woman" since Mary's surpassing holiness signifies that she was holier than even John after his sanctification in utero. Sophronius' teaching is augmented and incorporated by St. John Damascene (d. 749/750). John, besides many passages wherein he extolls the Theotokos for her purification at the Annunciation, grants her the unique honor of "purifying the waters of baptism by touching them." This honor was most famously and firstly attributed to Christ, especially in the legacy of Nazianzen. As such, Nazianzen's assertion of parallel holiness between the prepurified Mary and purified Jesus of the New Testament is made even more explicit in Damascene in his discourse on Mary's holiness to also imitate Christ's baptism at the Jordan. The Damascene's hymnongraphy and De fide Orthodoxa explicitly use Mary's "pre purification" as a key to understanding her absolute holiness and unsullied human nature. In fact, Damascene (along with Nazianzen) serves as the source for nearly all subsequent promotion of Mary's complete holiness from her Conception by the "all pure seed" of Joachim and the womb "wider than heaven" of St. Ann.
Feast day.
By 750 the feast of her conception was widely celebrated in the Byzantine East, under the name of the Conception (active) of Saint Anne. In the West it was known as the feast of the Conception (passive) of Mary, and was associated particularly with the Normans, whether these introduced it directly from the East or took it from English usage. The spread of the feast, by now with the adjective "Immaculate" attached to its title, met opposition on the part of some, on the grounds that sanctification was possible only after conception. Critics included Saints Bernard of Clairvaux, Albertus Magnus and Thomas Aquinas. Other theologians defended the expression "Immaculate Conception", pointing out that sanctification could be conferred at the first moment of conception in view of the foreseen merits of Christ, a view held especially by Franciscans. 
William of Ware and Blessed John Duns Scotus pointed out that Mary’s Immaculate Conception enhances Jesus’ redemptive work. One of the chief proponents of the doctrine was the Hungarian Franciscan Pelbartus Ladislaus of Temesvár.
On 28 February 1476, Pope Sixtus IV, authorized those dioceses that wished to introduce the feast to do so, and introduced it to his own diocese of Rome in 1477, with a specially composed Mass and Office of the feast. With his bull "Cum praeexcelsa" of 28 February 1477, in which he referred to the feast as that of the Conception of Mary, without using the word "Immaculate", he granted indulgences to those who would participate in the specially composed Mass or Office on the feast itself or during its octave, and he used the word "immaculate" of Mary, but applied instead the adjective "miraculous" to her conception. On 4 September 1483, referring to the feast as that of "the Conception of Immaculate Mary ever Virgin", he condemned both those who called it mortally sinful and heretical to hold that the "glorious and immaculate mother of God was conceived "without" the stain of original sin" and those who called it mortally sinful and heretical to hold that "the glorious Virgin Mary was conceived "with" original sin", since, he said, "up to this time there has been no decision made by the Roman Church and the Apostolic See." This decree was reaffirmed by the Council of Trent.
Pope Pius V, while including the feast in the Tridentine Calendar, removed the adjective "Immaculate" and suppressed the existing special Mass for the feast, directing that the Mass for the Nativity of Mary (with the word "Nativity" replaced by "Conception") be used instead. Part of that earlier Mass was revived in the Mass that Pope Pius IX ordered to be used on the feast and that is still in use.
On 6 December 1708, Pope Clement XI made the feast of the Conception of Mary, at that time still with the Nativity of Mary formula for the Mass, a Holy Day of Obligation. Until Pope Pius X reduced in 1911 the number of Holy Days of Obligation to 8, there were in the course of the year 36 such days, apart from Sundays.Writers such as Sarah Jane Boss interpret the existence of the feast as a strong indication of the Church's traditional belief in the Immaculate Conception.
Definition of the dogma.
During the reign of Pope Gregory XVI the bishops in various countries began to press for a definition as dogma of the teaching of Mary's immaculate conception.
In 1839 Mariano Spada (1796 - 1872), professor of theology at the Roman College of Saint Thomas, published "Esame Critico sulla dottrina dell’ Angelico Dottore S. Tommaso di Aquino circa il Peccato originale, relativamente alla Beatissima Vergine Maria" [A critical examination of the doctrine of St. Thomas Aquinas, the Angelic Doctor, regarding original sin with respect to the Most Blessed Virgin Mary], in which Aquinas is interpreted not as treating the question of the Immaculate Conception later formulated in the papal bull "Ineffabilis Deus" but rather the sanctification of the fetus within Mary's womb. Spada furnished an interpretation whereby Pius IX was relieved of the problem of seeming to foster a doctrine not in agreement with the Aquinas' teaching. Pope Pius IX would later appoint Spada Master of the Sacred Palace in 1867.
Pius IX, at the beginning of his pontificate, and again after 1851, appointed commissions to investigate the whole subject, and he was advised that the doctrine was one which could be defined and that the time for a definition was opportune.
It was not until 1854 that Pope Pius IX, with the support of the overwhelming majority of Roman Catholic bishops, whom he had consulted between 1851–1853, promulgated the papal bull "Ineffabilis Deus" (Latin for "Ineffable God"), which defined "ex cathedra" the dogma of the Immaculate Conception:
We declare, pronounce and define that the doctrine which holds that the Blessed Virgin Mary, at the first instant of her conception, by a singular privilege and grace of the Omnipotent God, in virtue of the merits of Jesus Christ, the Saviour of mankind, was preserved immaculate from all stain of original sin, has been revealed by God, and therefore should firmly and constantly be believed by all the faithful.—Pope Pius IX, "Ineffabilis Deus", December 8, 1854
The dogma was defined in accordance with the conditions of papal infallibility, which would be defined in 1870 by the First Vatican Council.
The papal definition of the dogma declares with absolute certainty and authority that Mary possessed sanctifying grace from the first instant of her existence and was free from the lack of grace caused by the original sin at the beginning of human history. Mary's salvation was won by her son Jesus Christ through his passion, death, and resurrection and was not due to her own merits.
Later developments.
For the Roman Catholic Church the dogma of the Immaculate Conception gained additional significance from the reputed apparitions of Our Lady of Lourdes in 1858. At Lourdes a 14-year-old girl, Bernadette Soubirous, claimed that a beautiful woman appeared to her and said, "I am the Immaculate Conception". Many believe the woman to have been the Blessed Virgin Mary and pray to her as such.
Pope Pius IX defined the dogma of the Immaculate Conception "not so much because of proofs in Scripture or ancient tradition, but due to a profound "sensus fidelium" and the Magisterium".
Speaking of the witness of the Church Fathers in claiming for Mary titles such as "Free from all contagion of sin", Pope Pius XII wrote:
If the popular praises of the Blessed Virgin Mary be given the careful consideration they deserve, who will dare to doubt that she, who was purer than the angels and at all times pure, was at any moment, even for the briefest instant, not free from every stain of sin?
The Roman Catholic tradition has a well-established philosophy for the study of the Immaculate Conception and the veneration of the Blessed Virgin Mary in the field of Mariology, with Pontifical schools such as the Marianum specifically devoted to this.
According to Bernard Ullathorne, a 19th-century English Roman Catholic prelate, "the expressions - The Immaculate Conception - The Immaculate Preservation - The Immunity - and Exception from original sin, are all phrases which bear the same signification, and are used equally to express one and the same mystery."
Medieval dispute about the doctrine.
It seems to have been St Bernard of Clairvaux who, in the 12th century, explicitly raised the question of the Immaculate Conception. A feast of the Conception of the Blessed Virgin had already begun to be celebrated in some churches of the West. St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See. In doing so, he takes occasion to repudiate altogether the view that the conception of Mary was sinless. It is doubtful, however, whether he was using the term "conception" in the same sense in which it is used in the definition of Pope Pius IX. Bernard would seem to have been speaking of conception in the active sense of the mother's cooperation, for in his argument he says: "How can there be absence of sin where there is concupiscence ("libido")?" and stronger expressions follow, showing that he is speaking of the mother and not of the child.
Saint Thomas Aquinas refused to concede the Immaculate Conception, on the ground that, unless the Blessed Virgin had at one time or other been one of the sinful, she could not justly be said to have been redeemed by Christ.
Saint Bonaventure (d. 1274), second only to Saint Thomas in his influence on the Christian schools of his age, hesitated to accept it for a similar reason. He believed that Mary was completely free from sin, but that she was not given this grace at the instant of her conception.
The celebrated John Duns Scotus (d. 1308), a Friar Minor like Saint Bonaventure, argued, on the contrary, that from a rational point of view it was certainly as little derogatory to the merits of Christ to assert that Mary was by him preserved from all taint of sin, as to say that she first contracted it and then was delivered. Proposing a solution to the theological problem of reconciling the doctrine with that of universal redemption in Christ, he argued that Mary's immaculate conception did not remove her from redemption by Christ; rather it was the result of a more perfect redemption granted her because of her special role in salvation history.
The arguments of Scotus, combined with a better acquaintance with the language of the early Fathers, gradually prevailed in the schools of the Western Church. In 1387 the university of Paris strongly condemned the opposite view.
Scotus's arguments remained controversial, however, particularly among the Dominicans, who were willing enough to celebrate Mary's "sanctificatio" (being made free from sin) but, following the Dominican Thomas Aquinas' arguments, continued to insist that her sanctification could not have occurred until after her conception.
Popular opinion remained firmly behind the celebration of Mary's conception. In 1439, the Council of Basel, which is not reckoned an ecumenical council, stated that belief in the immaculate conception of Mary is in accord with the Catholic faith. By the end of the 15th century the belief was widely professed and taught in many theological faculties, but such was the influence of the Dominicans, and the weight of the arguments of Thomas Aquinas (who had been canonised in 1323 and declared "Doctor Angelicus" of the Church in 1567) that the Council of Trent (1545–63)—which might have been expected to affirm the doctrine—instead declined to take a position.
The papal bull defining the dogma, "Ineffabilis Deus", mentioned in particular the patrististic interpretation of as referring to a woman, Mary, who would be eternally at enmity with the evil serpent and completely triumphing over him. It said the Fathers saw foreshadowings of Mary's "wondrous abundance of divine gifts and original innocence" "in that ark of Noah, which was built by divine command and escaped entirely safe and sound from the common shipwreck of the whole world; in the ladder which Jacob saw reaching from the earth to heaven, by whose rungs the angels of God ascended and descended, and on whose top the Lord himself leaned; in that bush which Moses saw in the holy place burning on all sides, which was not consumed or injured in any way but grew green and blossomed beautifully; in that impregnable tower before the enemy, from which hung a thousand bucklers and all the armor of the strong; in that garden enclosed on all sides, which cannot be violated or corrupted by any deceitful plots; in that resplendent city of God, which has its foundations on the holy mountains; in that most august temple of God, which, radiant with divine splendours, is full of the glory of God; and in very many other biblical types of this kind."
The bull recounts that the Fathers interpreted the angel's address to Mary, "highly favoured one" or "full of grace", as indicating that "she was never subject to the curse and was, together with her Son, the only partaker of perpetual benediction"; they "frequently compare her to Eve while yet a virgin, while yet innocence, while yet incorrupt, while not yet deceived by the deadly snares of the most treacherous serpent".
Patronages.
A number of countries are considered to be under the patronage of the Immaculate Conception by pontifical decree.
These include Argentina, Brazil, Korea, Nicaragua, Paraguay, Philippines, Spain (old kingdoms and the present state), the United States and Uruguay.
By royal decree under the House of Braganza, it is the principal Patroness of Portugal.
Other churches.
For differing reasons, belief in Mary's immaculate conception in the Catholic doctrinal form is not part of the official doctrines of the Eastern Orthodox, Oriental Orthodox, Anglican and Protestant churches.
Anglicanism.
Belief in Mary's immaculate conception is not a doctrine within Anglicanism, although it is shared by many Anglo-Catholics. In the Church of England's "Common Worship" prayer book, 8 December is designated a Lesser Festival of the "Conception of the Blessed Virgin Mary" (without the adjective "immaculate").
The report "Mary: Faith and Hope in Christ", by the Anglican-Roman Catholic International Commission, concluded that the teaching about Mary in the two definitions of the Assumption and the Immaculate Conception can be said to be consonant with the teaching of the Scriptures and the ancient common traditions. But the report expressed concerns that the Roman Catholic dogmatic definitions of these concepts implies them to be "revealed by God", stating: "The question arises for Anglicans, however, as to whether these doctrines concerning Mary are revealed by God in a way which must be held by believers as a matter of faith."
Some Anglicans reject the doctrine that Mary was sinless and conceived without original sin, often citing that it is not within the Holy Scripture and is against the Redemptive role and purpose of Jesus Christ merited for all human beings.
Eastern and Oriental Orthodox.
Contemporary Eastern Orthodox Christians often object to the dogmatic declaration of her immaculate conception as an "over-elaboration" of the faith and because they see it as too closely connected with a particular interpretation of the doctrine of ancestral sin. All the same, the historical and authentic tradition of Mariology in Byzantium took its historical point of departure from Sophronios, Damascene, and their imitators. The most famous Eastern Orthodox theologian to imply Mary's Immaculate Conception was St. Gregory Palamas. Though many passages from his works were long known to extol and attribute to Mary a Christlike holiness in her human nature, traditional objections to Palamas' disposition toward the Immaculate Conception typically rely on a poor understanding of his doctrine of "the purification of Mary" at the Annunciation. Not only did he explicitly cite St. Gregory Nazianzen for his understanding of Jesus' purification at His baptism and Mary's at the Annunciation, but Theophanes of Nicaea, Joseph Bryennius, and Gennadios Scholarios all explicitly placed Mary's Conception as the first moment of her all-immaculate participation in the divine energies to such a degree that she was always completely without spot and graced. In addition to Emperor Manuel II and Gennadius Scholarius, St. Mark of Ephesus also fervently defended Mary's title as "prepurified" against the Dominican, Manuel Calecas, who was perhaps promoting thomistic Mariology that denied Mary's all-holiness from the first moment of her existence.
In the tradition of Ethiopian Orthodoxy, the Kebra Nagast says:
He cleansed eve's body and sanctified it and made for it a dwelling in her for adam's salvation. "She [i.e., mary] was born without blemish", for He made her pure, without pollution, and she redeemed his debt without carnal union and embrace...Through the transgression of eve we died and were buried, and by the purity of mary we receive honour, and are exalted to the heights (emphasis added).
Old Catholic.
While Old Catholics do not reject the Immaculate Conception of Mary, and some of their parishes venerate Mary as immaculately conceived and celebrate the feast of her Immaculate Conception, they do not accept its definition as a dogma, since they reject papal infallibility and with it the Pope's authority to define dogma.
Protestantism.
Martin Luther, who initiated the Protestant Reformation, said: "Mary is full of grace, proclaimed to be entirely without sin. God's grace fills her with everything good and makes her devoid of all evil." But in 1532 he denied Mary's immaculate conception, declaring: "Mary is conceived in sin just like us." However, some Lutherans, such as the members of the Anglo-Lutheran Catholic Church, support the doctrine.
Most Protestants reject the doctrine because they do not consider the development of dogmatic theology to be authoritative apart from biblical exegesis, and because the doctrine of the Immaculate Conception is not taught in the Bible. The formal pronouncement of Mary's Immaculate Conception by the Catholic Church in 1854 alienated some Protestant churches partly due to its implication that not all have sinned.
Islam.
Official Islamic teachings highly regard the Virgin Mary as both a sublime model of purity and piety. An entire Sura chapter of the Qur'an is dedicated to her nobility, holiness, and fiat obedience to God. Among Islamic circles and discussions, she is often given a prominent status being the supreme feminine model of sanctity and maternal virtue.
Some Western writers claim that the immaculate conception of Mary is a teaching of Islam. Thus, commenting in 1734 on the passage in the Qur'an, "I have called her Mary; and I commend her to thy protection, and also her issue, against Satan driven away with stones", George Sale stated: "It is not improbable that the pretended immaculate conception of the virgin Mary is intimated in this passage. For according to a tradition of Mohammed, every person that comes into the world, is touched at his birth by the devil, and therefore cries out, Mary and her son only excepted; between whom, and the evil spirit God placed a veil, so that his touch did not reach them. And for this reason they say, neither of them were guilty of any sin, like the rest of the children of Adam."
In addition, further claims were made that the Roman Catholic Church derives its doctrine from the Islamic teaching. In volume 5 of his "Decline and Fall of the Roman Empire", published in 1788, Edward Gibbon wrote: "The Latin Church has not disdained to borrow from the Koran the immaculate conception of his virgin mother." That he was speaking of her immaculate conception by her mother, not of her own virginal conception of Jesus, is shown by his footnote: "In the xiith century the immaculate conception was condemned by St. Bernard as a presumptuous novelty." In the aftermath of the definition of the dogma in 1854, this charge was repeated: "Strange as it may appear, that the doctrine which the church of Rome has promulgated, with so much pomp and ceremony, 'for the destruction of all heresies, and the confirmation of the faith of her adherents', should have its origin in the Mohametan Bible; yet the testimony of such authorities as Gibbon, and Sale, and Forster, and Gagnier, and Maracci, leave no doubt as to the marvellous fact."
Without making Islamic belief the origin of the doctrine defined in 1854, a similarity between the two has been noted also by Roman Catholic writers such as Thomas Patrick Hughes, William Bernard Ullathorne, Giancarlo Finazzo.
Certain books that speak of the "immaculate conception" in relation to Islam confuse this term with the virgin birth of Jesus, his conception by Mary, not Mary's own conception by her mother. Among writers who do not mistake Mary's "immaculate conception" as her virginal conception of Jesus, some say that Mary "is credited in Islam with extraordinary holiness, herself immaculately conceived", while others say that, "with regard to Mary's putative immaculate conception, while classical Islam holds the Qur'an to affirm Mary virginity and virtue, it repudiates the doctrine as it is known in Christianity. There is no inherited fault in the genesis of humans that requires any further form of salvation than moral repentance and an interior act of commitment and surrender."
Prayers and hymns.
The Roman Missal and the Roman Rite Liturgy of the Hours naturally includes references to Mary's immaculate conception in the feast of the Immaculate Conception. An example is the antiphon that begins: "Tota pulchra es, Maria, et macula originalis non est in te" (You are all beautiful, Mary, and the original stain [of sin] is not in you. Your clothing is white as snow, and your face is like the sun. You are all beautiful, Mary, and the original stain [of sin] is not in you. You are the glory of Jerusalem, you are the joy of Israel, you give honour to our people. You are all beautiful, Mary.) On the basis of the original Gregorian chant music, polyphonic settings have been composed by Anton Bruckner, Pablo Casals, Maurice Duruflé, Grzegorz Gerwazy Gorczycki, , José Maurício Nunes Garcia, and ,
Other prayers honouring Mary's immaculate conception are in use outside the formal liturgy. The hymn "Immaculate Mary", addressed to Mary as the Immaculately Conceived One, is closely associated with Lourdes. The Immaculata prayer, composed by Saint Maximillian Kolbe, is a prayer of entrustment to Mary as the Immaculata. A novena of prayers, with a specific prayer for each of the nine days has been composed under the title of the Immaculate Conception Novena.
Artistic representations.
The 1476 extension of the feast of the Immaculate Conception to the entire Latin Church reduced the likelihood of controversy for the artist or patron in depicting an image, so that emblems depicting "The Immaculate Conception" began to appear.
Many artists in the 15th century faced the problem of how to depict an abstract idea such as the Immaculate Conception, and the problem was not fully solved for 150 years. The Italian Renaissance artist Piero di Cosimo was among those artists who tried new solutions, but none of these became generally adopted so that the subject matter would be immediately recognisable to the faithful.
The definitive iconography for the Immaculate Conception, drawing on the emblem tradition, seems to have been finally established by the master and then father-in-law of Diego Velázquez, the painter and theorist Francisco Pacheco. Pacheco's iconography influenced other Spanish artists such as Bartolomé Murillo, Diego Velázquez, and Francisco Zurbarán, who each produced a number of artistic masterpieces based on the use of these same symbols.
The popularity of this particular representation of "The Immaculate Conception" spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The moon is under her feet and a halo of twelve stars surround her head, possibly a reference to "a woman clothed with the sun" from Revelation 12:1-2. Additional imagery may include clouds, a golden light, and cherubs. In some paintings the cherubim are holding lilies and roses, flowers often associated with Mary.

</doc>
<doc id="15260" url="http://en.wikipedia.org/wiki?curid=15260" title="Islands of the North Atlantic">
Islands of the North Atlantic

IONA (Islands of the North Atlantic) is an acronym suggested in 1980 by Sir John Biggs-Davison to refer to a loose linkage of England, Wales, Scotland, Ireland, the Isle of Man and Channel Islands, similar to the present day British-Irish Council. Its intended purpose was as a more politically acceptable alternative to British Isles, which is disliked by many people in Ireland.
It has been criticised on the grounds that it in fact excludes most of the islands in the North Atlantic, including Iceland, Greenland, the Faroe Islands, Newfoundland, Prince Edward Island, Cape Breton Island, and the Azores, and also that the only island referred to by the term that is actually in the North Atlantic Ocean is Ireland. Great Britain is in fact in between the Irish Sea and The North Sea. It has been used particularly in the context of the Northern Irish peace process during the negotiation of the Good Friday Agreement, as a neutral name for the proposed council.
One feature of this name is that IONA has the same spelling as the island of Iona which is off the coast of Scotland but with which Irish people have strong cultural associations. It is therefore a name with which people of both main islands might identify. Taoiseach Bertie Ahern noted the symbolism in a 2006 address in Edinburgh:[The Island of] Iona is a powerful symbol of relationships between these islands, with its ethos of service not dominion. Iona also radiated out towards the Europe of the Dark Ages, not to mention Pagan England at Lindisfarne. The British-Irish Council is the expression of a relationship that at the origin of the Anglo-Irish process in 1981 was sometimes given the name Iona, islands of the North Atlantic, and sometimes Council of the Isles, with its evocation of the Lords of the Isles of the 14th and 15th centuries who spanned the North Channel. In the 17th century, Highland warriors and persecuted Presbyterian Ministers criss-crossed the North Channel.
In a Dáil Éireann debate, Proinsias De Rossa was less enthusiastic: The acronym IONA is a useful way of addressing the coming together of these two islands. However, the island of Iona is probably a green heaven in that nobody lives on it and therefore it cannot be polluted in any way.
Outside the Northern Ireland peace process the term IONA is used by the World Universities Debating Championship and in inter-varsity debating competitions throughout Britain and Ireland. In this context IONA is one of the regions which appoint a representative onto the committee of the World Universities Debating Council. Interestingly Greenland, the Faroe Islands and Iceland would actually be included in the definition of IONA used in this context while Newfoundland and Prince Edward Island would be in North America. However none of these islands have yet participated in the World Universities Debating Championships. Otherwise, the term has achieved very little popular usage in any context.

</doc>
<doc id="15261" url="http://en.wikipedia.org/wiki?curid=15261" title="Intel DX4">
Intel DX4

The IntelDX4 is a clock-tripled i486 microprocessor with 16 kB L1 cache. Intel named it DX4 (rather than "DX3") as a consequence of litigation with AMD over trademarks. The product was officially named the IntelDX4, but OEMs continued using the i486 naming convention.
Intel produced IntelDX4s with two clock speed steppings: A 75 MHz version (3× 25 MHz multiplier), and a 100 MHz version (usually 3× 33.3 MHz, but sometimes also 2× 50 MHz). Both chips were released in March 1994. A version of the IntelDX4 featuring write-back cache was released in October 1994. The original write-through versions of the chip are marked with a laser embossed "&E", while the write-back enabled versions are marked "&EW". i486 OverDrive editions of the IntelDX4 had locked multipliers, and therefore can only run at 3× the external clock-speed. The 100 MHz model of the processor had an iCOMP rating of 435, while the 75 MHz processor had a rating of 319. The IntelDX4 was an OEM-only product, but the DX4 Overdrive could be purchased at a retail store.
The IntelDX4 microprocessor is mostly pin-compatible with the 80486, but requires a lower 3.3V supply. Normal 80486 and DX2 processors use a 5V supply; plugging a DX4 into an unmodified socket will destroy it. Motherboards lacking support for the 3.3V CPUs can sometimes make use of them using a voltage regulator (VRM) that fits between the socket and the CPU.

</doc>
<doc id="15264" url="http://en.wikipedia.org/wiki?curid=15264" title="Iapetus">
Iapetus

Iapetus may refer to:

</doc>
<doc id="15266" url="http://en.wikipedia.org/wiki?curid=15266" title="Interactive Fiction Competition">
Interactive Fiction Competition

The Interactive Fiction Competition (also known as IFComp) is one of the best known of several annual competitions for works of interactive fiction. It has been held since 1995. It is intended for fairly short games, as judges are only allowed to spend two hours playing a game before deciding how many points to award it. The competition has been described as the "Super Bowl" of interactive fiction.
A reviewer for "The A.V. Club" said of the 2008 competition, "Once again, the IF Competition delivers some of the best writing in games."
The 2008 competition was described as containing "some real standouts both in quality of puzzles and a willingness to stretch the definition of text adventures/interactive fiction." The competition differs from the XYZZY Awards, as authors must specifically submit games to the Interactive Fiction Competition, but all games released in the past year are eligible for the XYZZY Awards. Many games win awards in both competitions.
The competition is organized by Stephen Granade. Although the first competition had separate sections for Inform and TADS games, subsequent competitions have not been divided into sections and are open to games produced by any method, provided that the software used to play the game is freely available. Anyone can judge the games, and anyone can donate a prize. Almost always, there are enough prizes donated that anyone who enters will get one. Entries are required to be released as freeware or public domain, reflecting the general non-profit ethos of the IF community.
In addition to the main competition, the entries take part in the Miss Congeniality contest, where the participating authors vote for three games (not including their own). This was started in 1998 to distribute that year's surplus prizes; this additional contest has remained unchanged since then, even without the original reason for its existence.
The following is a list of winners to date:

</doc>
<doc id="15267" url="http://en.wikipedia.org/wiki?curid=15267" title="Immunity">
Immunity

Immunity may refer to:

</doc>
<doc id="15268" url="http://en.wikipedia.org/wiki?curid=15268" title="Inquests in England and Wales">
Inquests in England and Wales

Inquests in England and Wales are held into sudden and unexplained deaths and also into the circumstances of discovery of a certain class of valuable artefacts known as "treasure trove". In England and Wales inquests are the responsibility of a coroner, who operates under the jurisdiction of the Coroners and Justice Act 2009.
Where an inquest is needed.
There is a general duty upon every person to report a death to the coroner if an inquest is likely to be required. However, this duty is largely unenforceable in practice and the duty falls on the responsible registrar. The registrar must report a death where:
The coroner must hold an inquest where the death was:
Where the cause of death is unknown, the coroner may order a post mortem examination in order to determine whether the death was violent. If the death is found to be non-violent, an inquest is unnecessary.
In 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury. In 2014 the Royal College of Pathologists claimed that up to 10,000 deaths a year recorded as being from natural causes should have been investigated by inquests. They were particularly concerned about people whose death occurred as a result of medical errors. "We believe a medical examiner would have been alerted to what was going on in Mid-Staffordshire long before this long list of avoidable deaths reached the total it did," said Archie Prentice, the pathologists' president.
Juries.
A coroner must summon a jury for an inquest if the death occurred in prison or in police custody, or in the execution of a police officer's duty, or if it falls under the Health and Safety at Work etc. Act 1974, or if it affects public health or safety. The coroner can also call a jury at her or his own discretion. This discretion has been heavily litigated in light of the Human Rights Act 1998, which means that juries are required now in a broader range of situations than expressly required by statute.
Scope of inquest.
The purpose of the inquest is to answer four questions:
Evidence must be solely for the purpose of answering these questions and no other evidence is admitted. It is not for the inquest to ascertain "how the deceased died" or "in what broad circumstances", but "how the deceased came by his death", a more limited question. Moreover, it is not the purpose of the inquest to determine, or appear to determine, criminal or civil liability, to apportion guilt or attribute blame. For example, where a prisoner hanged himself in a cell, he came by his death by hanging and it was not the role of the inquest to enquire into the broader circumstances such as the alleged neglect of the prison authorities that might have contributed to his state of mind or given him the opportunity. However, the inquest should set out as many of the facts as the public interest requires.
Under the terms of article 2 of the European Convention of Human Rights, governments are required to "establish a framework of laws, precautions, procedures and means of enforcement which will, to the greatest extent reasonably practicable, protect life." The European Court of Human Rights has interpreted this as mandating independent official investigation of any death where public servants may be implicated. Since the Human Rights Act 1998 came into force, in those cases alone, the inquest is now to consider the broader question "by what means and in what circumstances".
In disasters, such as the King's Cross fire, a single inquest may be held into several deaths. However, when several protesters were shot and killed by police in Mitchelstown in 1887, the findings of a common inquest were quashed because the killings had taken place at different times and in different places.
Procedure.
Inquests are governed by the Coroners Rules. The coroner gives notice to near relatives, those entitled to examine witnesses and those whose conduct is likely to be scrutinised. Inquests are held in public except where there are real issues of national security.
Individuals with an interest in the proceedings, such as relatives of the deceased, individuals appearing as witnesses, and organisations or individuals who may face some responsibility in the death of the individual, may be represented by lawyers at the discretion of the coroner. Witnesses may be compelled to testify subject to the privilege against self-incrimination.
Verdict.
The following verdicts are not mandatory but are strongly recommended:
In 2004, 37% of inquests recorded an outcome of death by accident / misadventure, 21% by natural causes, 13% suicide, 10% open verdicts, and 19% other outcomes.
If an open verdict is returned, the inquest can be reopened if new evidence is found and presented to the coroner.
Reform.
Owing to dissatisfaction with the current system, and in particular because of perceived failures to arrest the murder spree of Harold Shipman, proposals have been made for reform. A draft bill was published on 12 June 2006. The principal draft reforms are:

</doc>
<doc id="15270" url="http://en.wikipedia.org/wiki?curid=15270" title="Index">
Index

An index is an indirect shortcut derived from and pointing into, a greater volume of values, data, information or knowledge.
Index may refer to:

</doc>
<doc id="15271" url="http://en.wikipedia.org/wiki?curid=15271" title="Information retrieval">
Information retrieval

Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text (or other content-based) indexing.
Automated information retrieval systems are used to reduce what has been called "information overload". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.
Overview.
An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.
An object is an entity that is represented by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.
History.
The idea of using computers to search for relevant pieces of information was popularized in the article "As We May Think" by Vannevar Bush in 1945. The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.
In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.
Model types.
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.
Performance and correctness measures.
Many different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be ill-posed and there may be different shades of relevancy.
Precision.
Precision is the fraction of the documents retrieved that are relevant to the user's information need.
In binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called "precision at n" or "P@n".
Note that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of accuracy and precision within other branches of science and statistics.
Recall.
Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.
In binary classification, recall is often called sensitivity. So it can be looked at as "the probability that a relevant document is retrieved by the query".
It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.
Fall-out.
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:
In binary classification, fall-out is closely related to specificity and is equal to formula_4. It can be looked at as "the probability that a non-relevant document is retrieved by the query".
It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.
F-measure.
The weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is:
This is also known as the formula_6 measure, because recall and precision are evenly weighted.
The general formula for non-negative real formula_7 is:
Two other commonly used F measures are the formula_9 measure, which weights recall twice as much as precision, and the formula_10 measure, which weights precision twice as much as recall.
The F-measure was derived by van Rijsbergen (1979) so that formula_11 "measures the effectiveness of retrieval with respect to a user who attaches formula_7 times as much importance to recall as precision". It is based on van Rijsbergen's effectiveness measure formula_13. Their relationship is formula_14 where formula_15.
Average precision.
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision formula_16 as a function of recall formula_17. Average precision computes the average value of formula_16 over the interval from formula_19 to formula_20:
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
where formula_23 is the rank in the sequence of retrieved documents, formula_24 is the number of retrieved documents, formula_25 is the precision at cut-off formula_23 in the list, and formula_27 is the change in recall from items formula_28 to formula_23.
This finite sum is equivalent to:
where formula_31 is an indicator function equaling 1 if the item at rank formula_23 is a relevant document, zero otherwise. Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.
Some authors choose to interpolate the formula_16 function to reduce the impact of "wiggles" in the curve. For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:
where formula_35 is an interpolated precision that takes the maximum precision over all recalls greater than formula_17:
An alternative is to derive an analytical formula_16 function by assuming a particular parametric distribution for the underlying decision values. For example, a "binormal precision-recall curve" can be obtained by assuming decision values in both classes to follow a Gaussian distribution.
R-Precision.
Precision at R-th position in the ranking of results for a query that has R relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the R-th position.
Mean average precision.
Mean average precision for a set of queries is the mean of the average precision scores for each query.
where "Q" is the number of queries.
Discounted cumulative gain.
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.
The DCG accumulated at a particular rank position formula_40 is defined as:
Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (formula_42), which normalizes the score:
The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the formula_44 will be the same as the formula_42 producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

</doc>
<doc id="15272" url="http://en.wikipedia.org/wiki?curid=15272" title="List of Italian-language poets">
List of Italian-language poets

List of poets who wrote in Italian (or Italian dialects):

</doc>
<doc id="15274" url="http://en.wikipedia.org/wiki?curid=15274" title="International Criminal Tribunal for the former Yugoslavia">
International Criminal Tribunal for the former Yugoslavia

The International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of the Former Yugoslavia since 1991, more commonly referred to as the International Criminal Tribunal for the former Yugoslavia or ICTY, is a body of the United Nations established to prosecute serious crimes committed during the wars in the former Yugoslavia, and to try their perpetrators. The tribunal is an ad hoc court which is located in The Hague, Netherlands.
The Court was established by Resolution 827 of the United Nations Security Council, which was passed on 25 May 1993. It has jurisdiction over four clusters of crimes committed on the territory of the former Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crimes against humanity. The maximum sentence it can impose is life imprisonment. Various countries have signed agreements with the UN to carry out custodial sentences.
The final indictments were issued in December 2004, the last of which were confirmed and unsealed in the spring of 2005. The Tribunal aims to complete all trials by the end of 2012 and all appeals by 2015, with the exception of Radovan Karadžić whose trial is expected to end in 2014 and recently arrested Ratko Mladić and Goran Hadžić.
The United Nations Security Council called upon the Tribunal to finish its work by 31 December 2014 to prepare for its closure and transfer of its responsibilities to the International Residual Mechanism for Criminal Tribunals which began functioning for the ICTY branch on 1 July 2013. The Tribunal will conduct and complete all outstanding first instance trials, including those of Karadžić, Mladić and Hadžić. It will conduct and complete all appeal proceedings for which the notice of appeal against the judgement or sentence is filed before 1 July 2013. Any appeals for which notice is filed after that date will be handled by the Residual Mechanism. Hadžić became the last of 161 indicted fugitives to be arrested after Serbian President Boris Tadić announced his arrest on 20 July 2011.
History.
Creation.
United Nations Security Council Resolution 808 of 22 February 1993 decided that "an international tribunal shall be established for the prosecution of persons responsible for serious violations of international humanitarian law committed in the territory of the former Yugoslavia since 1991" and calling on the Secretary-General to "submit for consideration by the Council … a report on all aspects of this matter, including specific proposals and where appropriate options … taking into account suggestions put forward in this regard by Member States".
The Court was originally proposed by German Foreign Minister Klaus Kinkel. By 25 May 1993, the international community had tried to pressure the leaders of the former Yugoslavian republics diplomatically, militarily, politically, economically, and – with Resolution 827 – through juridical means. Resolution 827 of 25 May 1993 approved report of the Secretary-General and adopted the Statute of the International Tribunal annexed to it, formally creating the ICTY. It would have jurisdiction over four clusters of crime committed on the territory of the former Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crime against humanity. The maximum sentence it can impose is life imprisonment.
Implementation.
In 1993, the ICTY built its internal infrastructure. 17 states have signed an agreement with the ICTY to carry out custodial sentences
1993-1994: In the first year of its existence, the Tribunal laid the foundations for its existence as a judicial organ. The Tribunal established the legal framework for its operations by adopting the rules of procedure and evidence, as well as its rules of detention and directive for the assignment of defense counsel. Together these rules established a legal aid system for the Tribunal. As the ICTY is part of the United Nations and as it was the first "international" court for "criminal" justice, the development of a juridical infrastructure was considered quite a challenge. However after the first year the first ICTY judges had drafted and adopted all the rules for court proceedings.
1994-1995: The ICTY established its offices within the Aegon Insurance Building in The Hague (which was, at the time, still partially in use by Aegon) and detention facilities in Scheveningen in The Hague (The Netherlands). The ICTY hired now many staff members. By July 1994 there were sufficient staff members in the office of the prosecutor to begin field investigations and by November 1994 the first indictment was presented and confirmed. In 1995, the entire staff numbered more than 200 persons and came from all over the world. Moreover, some governments assigned their legally trained people to the ICTY.
Operation.
In 1994 the first indictment was issued against the Bosnian-Serb concentration camp commander Dragan Nikolić. This was followed on 13 February 1995 by two indictments comprising 21 individuals which were issued against a group of 21 Bosnian-Serbs charged with committing atrocities against Muslim and Croat civilian prisoners. While the war in the former Yugoslavia was still raging, the ICTY prosecutors showed that an international court was viable. However, no accused was arrested.
The court confirmed 8 indictments against 46 individuals and issued arrest warrants. Bosnian Serb indictee Duško Tadić became the subject of the Tribunal's first trial. Tadić was arrested by German police in Munich in 1994 for his alleged actions in the Prijedor region in Bosnia-Herzegovina (especially his actions in the Omarska, Trnopolje and Keraterm detention camps). He made his first appearance before the ICTY Trial Chamber on 26 April 1995, and pleaded not guilty to all of the charges in the indictment.
1995–96: Between June 1995 and June 1996, 10 public indictments had been confirmed against a total of 33 individuals. Six of the newly indicted persons were transferred in the Tribunal's detention unit. In addition to Duško Tadic, by June 1996 the tribunal had Tihomir Blaškić, Dražen Erdemović, , , and Hazim Delić in custody. The accused Erdemović became the first person to enter a guilty plea before the tribunal's court. Between 1995 and 1996, the ICTY dealt with miscellaneous cases involving several detainees, which never reached the trial stage. Some of the accused had been arrested and others surrendered to the ICTY.
Accomplishments.
In 2004, the ICTY published a list of five successes which it claimed it had accomplished:
Organization.
The Tribunal employs around 900 staff. Its organisational components are Chambers, Registry and the Office of the Prosecutor (OTP).
Prosecutors.
The Prosecutor is responsible for investigating crimes, gathering evidence and prosecutions and is head of the Office of the Prosecutor (OTP). The Prosecutor is appointed by the UN Security Council upon nomination by the UN Secretary-General.
The current prosecutor is Serge Brammertz. Previous Prosecutors have been Ramón Escovar Salom of Venezuela (1993–1994), Richard Goldstone of South Africa (1994–1996), Louise Arbour of Canada (1996–1999), Eric Östberg of Sweden, and Carla Del Ponte of Switzerland (1999–2007), who until 2003, simultaneously served as the Prosecutor of the International Criminal Tribunal for Rwanda where she led the OTP since 1999. David Tolbert, the President of the International Center for Transitional Justice, was also appointed Deputy Prosecutor of the ICTY.
Chambers.
Chambers encompasses the judges and their aides. The Tribunal operates three Trial Chambers and one Appeals Chamber. The President of the Tribunal is also the presiding Judge of the Appeals Chamber.
Judges.
There are 20 permanent judges and three "ad litem" judges who serve on the Tribunal.
UN member and observer states can each submit up to two nominees of different nationalities to the UN Secretary-General. The UN Secretary-General submits this list to the UN Security Council which selects from 28 to 42 nominees and submits these nominees to the UN General Assembly. The UN General Assembly then elects 14 judges from that list. Judges serve for 4 years and are eligible for re-election. The UN Secretary-General appoints replacements in case of vacancy for the remainder of the term of office concerned.
On 19 October 2011, Judge Theodor Meron (United States) was elected the new President of the ICTY by the permanent judges in a Special Plenary Session. Judge Carmel Agius (Malta) was elected Vice-President. His predecessors were Antonio Cassese of Italy (1993–1997), Gabrielle Kirk McDonald of the United States (1997–1999), Claude Jorda of France (1999–2002), Theodor Meron of the United States (2002–2005), Fausto Pocar of Italy (2005–2008) and Patrick Robinson of Jamaica (2008-2011).
Registry.
The Registry is responsible for handling the administration of the Tribunal; activities include keeping court records, translating court documents, transporting and accommodating those who appear to testify, operating the Public Information Section, and such general duties as payroll administration, personnel management and procurement. It is also responsible for the Detention Unit for indictees being held during their trial and the Legal Aid program for indictees who cannot pay for their own defence. It is headed by the Registrar, currently John Hocking of Australia (since May 2009). His predecessors were Hans Holthuis of the Netherlands (2001–2009), Dorothée de Sampayo Garrido-Nijgh of the Netherlands (1995–2000), and Theo van Boven of the Netherlands (February 1994 to December 1994).
Detention facilities.
Those defendants on trial and those who were denied a provisional release are detained at the United Nations Detention Unit on the premises of the Penitentiary Institution Haaglanden, location Scheveningen, located some 3 km by road from the courthouse. The indicted are housed in private cells which have a toilet, shower, radio, satellite TV, personal computer (without Internet access) and other luxuries. They are allowed to phone family and friends daily and can have conjugal visits. There is also a library, a gym and various rooms used for religious observances. The inmates are allowed to cook for themselves. All of the inmates mix freely and are not segregated on the basis of nationality. As the cells are more akin to a university residence instead of a jail, some have derisively referred to the ICT as the “Hague Hilton”. The reason for this luxury relative to other prisons is that the first president of the court wanted to emphasise that the indictees are innocent until proven guilty.
Indictees.
The very first hearing at the ICTY was referral request in the Tadić case on 8 November 1994. s of August 2014[ [update]], the Tribunal has indicted 161 individuals, and has already completed proceedings with regard to 141 of them: 18 have been acquitted, 74 sentenced, 13 have had their cases transferred to courts in Bosnia and Herzegovina (10), Croatia (2) and Serbia (1). Another 36 cases have been terminated either because indictments were withdrawn or because the indictees died before or after transfer to the Tribunal. Of the 74 convicted, 51 were transferred to 14 different states where they served their prison sentences, excluding those whose sentences amounted to time spent in detention during trial. 50 have served their term, and 3 died while serving their sentences. Proceedings for another 20 indictees are still ongoing — 4 are in the trial phase and 26 are before the Appeals Chamber.
The indictees ranged from common soldiers to generals and police commanders all the way to prime ministers. Slobodan Milošević was the first sitting head of state indicted for war crimes. Other "high level" indictees included Milan Babić, former President of the Republika Srpska Krajina; Ramush Haradinaj, former Prime Minister of Kosovo; Radovan Karadžić, former President of the Republika Srpska; Ratko Mladić, former Commander of the Bosnian Serb Army and Ante Gotovina, former General of the Croatian Army.
Haradinaj's trial began at The Hague on 5 March 2007 and the closing brief was given on 23 January 2008. The final decision of the ICTY was expected in March 2008. On 3 April 2008, ICTY issued a public notice of the Haradinaj verdict, in which he was acquitted of all charges. The judge said much of the evidence had been non-existent against Haradinaj or at best inconclusive. But he complained of witness intimidation, saying some witnesses had not testified because they had been afraid. On 21 July 2010, the cases of UÇK (Kosovo Liberation Army) commanders Ramush Haradinaj, Idriz Balaj and Lahi Brahimaj were re-opened for trial. However on 29 November 2012 all three were acquitted of all charges for a second time.
s of August 2014[ [update]], there are four ongoing trials. Five further cases are at the appeals stage—Đorđević, Perišić, Popović et al, Šainović et al, and Tolimir cases. A further 23 individuals have been the subject of contempt proceedings. Croat Serb General and former President of the Republic of Serbian Krajina Goran Hadžić was the last fugitive wanted by the Tribunal to be arrested on 20 July 2011.
Criticism.
Skeptics argued that an international court could not function while the war in the former Yugoslavia was still going on. This would be a huge undertaking for any court, but for the ICTY it would be an even greater one, as the new tribunal still needed judges, a prosecutor, a registrar, investigative and support staff, an extensive interpretation and translation system, a legal aid structure, premises, equipment, courtrooms, detention facilities, guards and all the related funding.
Criticisms of the court include:
Response to criticism.
Supporters of the work of the ICTY responded to critics in various publications. In a response to David Harland's "Selective Justice", Jelena Subotić, an assistant professor of political science at Georgia State University and author of "Hijacked Justice: Dealing with the Past in the Balkans", responded that the critics of the Tribunal miss the point, "which is not to deliver justice for past wrongs equally for 'all sides', fostering reconciliation, but to carefully measure each case on its own merits ... We should judge the work of the tribunal by its legal expertise, not by the political outcomes we desire."
Marko Hoare claims the accusations of the tribunal's "selective justice" stem from Serbian nationalist propaganda. He wrote: "This is, of course, the claim that hardline Serb nationalists and supporters of Slobodan Milosevic have been making for about the last two decades. Instead of carrying out any research into the actual record of the ICTY in order to support his thesis, Harland simply repeats a string of cliches of the kind that frequently appear in anti-Hague diatribes by Serb nationalists."
Further reading.
</dl>

</doc>
<doc id="15275" url="http://en.wikipedia.org/wiki?curid=15275" title="ISO 216">
ISO 216

ISO 216 specifies international standard (ISO) paper sizes used in most countries in the world today. The standard defines the "A" and "B" series of paper sizes, including A4, the most commonly available size. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 "C" series is commonly listed alongside the A and B sizes.
All ISO 216, ISO 217 and ISO 269 paper sizes (except DL) have the same aspect ratio, . This ratio has the unique property that when cut or folded in half widthwise, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size.
History.
The advantages of basing a paper size upon an aspect ratio of were already noted in 1786 by the German scientist Georg Christoph Lichtenberg, in a letter to Johann Beckmann. The formats that became A2, A3, B3, B4 and B5 were developed in France, and published in 1798 during the French Revolution.
Early in the twentieth century, Dr Walter Porstmann turned Lichtenberg's idea into a proper system of different paper sizes. Porstmann's system was introduced as a DIN standard (DIN 476) in Germany in 1922, replacing a vast variety of other paper formats. Even today the paper sizes are called "DIN A"x"" in everyday use in Germany, Austria, Spain and Portugal.
The main advantage of this system is its scaling: if a sheet with an aspect ratio of is divided into two equal halves parallel to its shortest sides, then the halves will again have an aspect ratio of . Folded brochures of any size can be made by using sheets of the next larger size, e.g. A4 sheets are folded to make A5 brochures. The system allows scaling without compromising the aspect ratio from one size to another – as provided by office photocopiers, e.g. enlarging A4 to A3 or reducing A3 to A4. Similarly, two sheets of A4 can be scaled down to fit exactly one A4 sheet without any cutoff or margins.
The weight of each sheet is also easy to calculate given the basis weight in grams per square metre (g/m2 or "gsm"). Since an A0 sheet has an area of 1 m2, its weight in grams is the same as its basis weight in g/m2. A standard A4 sheet made from 80 g/m2 paper weighs 5 g, as it is one 16th (four halvings) of an A0 page. Thus the weight, and the associated postage rate, can be easily calculated by counting the number of sheets used.
ISO 216 and its related standards were first published between 1975 and 1995:
A series.
Paper in the A series format has a formula_1 aspect ratio, although this is rounded to the nearest millimetre. A0 is defined so that it has an area of 1 square metre, prior to the rounding. Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the preceding paper size, cutting parallel to its shorter side so that the long side of A("n"+1) is the same length as the short side of A"n" prior to rounding.
The most frequently used of this series is the size A4 which is 210 x. For comparison, the letter paper size commonly used in North America (8.5 x) is approximately 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4.
The geometric rationale behind the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, "x", and a shorter side, "y", ensuring that its aspect ratio, formula_2, will be the same as that of a rectangle half its size, formula_3, means that formula_4, which reduces to formula_5; in other words, an aspect ratio of formula_6.
The formula that gives the larger border of the paper size Aformula_7 in metres and without rounding off is the geometric sequence: formula_8. The paper size Aformula_7 thus has the dimension formula_10 × formula_11 and area (prior to rounding off) formula_12.
The exact millimetre measurement of the long side of Aformula_7 is given by formula_14.
B series.
The B series is defined in the standard as follows: "A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence." The use of the geometric mean means that each step in size: B0, A0, B1, A1, B2 … is smaller than the previous by an equal scaling. As with the A series, the lengths of the B series have the ratio formula_15, and folding one in half gives the next in the series. The shorter side of B0 is exactly 1m.
There is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are formula_16 times those of A-series paper. By comparison, the lengths of ISO B series paper are formula_17 times those of A-series paper.
For the ISO B series, the exact millimetre measurement of the long side of Bformula_7 is given by formula_19.
C series.
The C series formats are geometric means between the B series and A series formats with the same number (e.g., C2 is the geometric mean between B2 and A2). The width to height ratio is as in the A and B series. The C series formats are used mainly for envelopes. An A4 page will fit into a C4 envelope. C series envelopes follow the same ratio principle as the A series pages. For example, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half). The lengths of ISO C series paper are therefore formula_20 times those of A-series paper - i.e. about 9% larger.
A, B, and C paper fit together as part of a geometric progression, with ratio of successive side lengths of 21/8, though there is no size half-way between B"n" and A"n"-1: A4, C4, B4, "D4", A3, …; there is such a D-series in the Swedish extensions to the system.
The exact millimetre measurement of the long side of Cformula_7 is given by formula_22.
Tolerances.
The tolerances specified in the standard are:
Application.
The ISO 216 formats are organized around the ratio formula_15; two sheets next to each other together have the same ratio, sideways. In scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet, in each case there is neither waste nor want.
The principal countries not generally using the ISO paper sizes are the United States and Canada, which use the Letter, Legal and Executive system. Although they have also officially adopted the ISO 216 paper format, Mexico, Panama, Venezuela, Colombia, the Philippines and Chile also use mostly U.S. paper sizes.
Rectangular sheets of paper with the ratio formula_15 are popular in paper folding, such as origami, where they are sometimes called "A4 rectangles" or "silver rectangles". In other contexts, the term "silver rectangle" can also refer to a rectangle in the proportion formula_25, known as the silver ratio.

</doc>
<doc id="15276" url="http://en.wikipedia.org/wiki?curid=15276" title="ISO 3864">
ISO 3864

ISO 3864 specifies international standards for safety symbols. These labels are graphical, to overcome language barriers.
ANSI standard ANSI Z535.6-2006 defines an optional accompanying text in one or more languages.

</doc>
<doc id="15281" url="http://en.wikipedia.org/wiki?curid=15281" title="Isaac Abendana">
Isaac Abendana

Isaac Abendana (ca. 1640 – 1699) was the younger brother of Jacob Abendana, and became "hakam" of the Spanish Portuguese Synagogue in London after his brother died. 
Abendana moved to England before his brother, in 1662, and taught Hebrew at Cambridge University. He completed an unpublished Latin translation of the "Mishnah" for the university in 1671. 
While he was at Cambridge, Abendana sold Hebrew books to the Bodleian Library of Oxford, and in 1689 he took a teaching position in Magdalen College. In Oxford, he wrote a series of Jewish almanacs for Christians, which he later collected and compiled as the "Discourses on the Ecclesiastical and Civil Polity of the Jews" (1706). Like his brother, he maintained an extensive correspondence with leading Christian scholars of his time, most notably with the philosopher Ralph Cudworth, master of Christ's College, Cambridge. 

</doc>
<doc id="15284" url="http://en.wikipedia.org/wiki?curid=15284" title="List of intelligence agencies">
List of intelligence agencies

This is a list of intelligence agencies. It includes only currently operational institutions.
Agencies by country.
Ethiopia.
Ethiopian National Intelligence and Security Service (NISS)
Ireland.
Foreign & Domestic Military Intelligence (Defence Forces)
Domestic Police Intelligence ("Garda Síochána")
Malawi.
In the Presidents Office

</doc>
<doc id="15285" url="http://en.wikipedia.org/wiki?curid=15285" title="Internet Engineering Task Force">
Internet Engineering Task Force

The Internet Engineering Task Force (IETF) develops and promotes voluntary Internet standards, in particular the standards that comprise the Internet protocol suite (TCP/IP). It is an open standards organization, with no formal membership or membership requirements. All participants and managers are volunteers, though their work is usually funded by their employers or sponsors. 
The IETF started out as an activity supported by the US federal government, but since 1993 it has operated as a standards development function under the auspices of the Internet Society, an international membership-based non-profit organization.
Organization.
The IETF is organized into a large number of working groups and informal discussion groups (BoFs), each dealing with a specific topic and operates in a bottom-up task creation mode, largely driven by these working groups. Each working group has an appointed chairperson (or sometimes several co-chairs), along with a charter that describes its focus, and what and when it is expected to produce. It is open to all who want to participate, and holds discussions on an open mailing list or at IETF meetings, where the entry fee in July 2014 was USD $650 per person. 
Rough consensus is the primary basis for decision making. There are no formal voting procedures. Because the majority of the IETF's work is done via mailing lists, meeting attendance is not required for contributors. Each working group is intended to complete work on its topic and then disband. In some cases, the WG will instead have its charter updated to take on new tasks as appropriate.
The working groups are organized into areas by subject matter. Current areas are Applications, General, Internet, Operations and Management, Real-time Applications and Infrastructure, Routing, Security, and Transport. Each area is overseen by an "area director" (AD), with most areas having two co-ADs. The ADs are responsible for appointing working group chairs. The area directors, together with the IETF Chair, form the Internet Engineering Steering Group (IESG), which is responsible for the overall operation of the IETF. 
The IETF is overseen by the Internet Architecture Board (IAB), which oversees its external relationships, and relations with the RFC Editor. The IAB is also jointly responsible for the IETF Administrative Oversight Committee (IAOC), which oversees the IETF Administrative Support Activity (IASA), which provides logistical, etc. support for the IETF. The IAB also manages the Internet Research Task Force (IRTF), with which the IETF has a number of cross-group relations.
A Nominating Committee (NomCom) of ten randomly chosen volunteers who participate regularly at meetings is vested with the power to appoint, reappoint, and remove members of the IESG, IAB, IASA, and the IAOC. To date, no one has been removed by a NomCom, although several people have resigned their positions, requiring replacements.
In 1993 the IETF changed from an activity supported by the U.S. government to an independent, international activity associated with the Internet Society, an international membership-based non-profit organization. Because the IETF itself does not have members, nor is it an organization "per se", the Internet Society provides the financial and legal framework for the activities of the IETF and its sister bodies (IAB, IRTF, …). IETF activities are funded by meeting fees, meeting sponsors and by the Internet Society via its organizational membership and the proceeds of the Public Interest Registry. 
In December 2005 the IETF Trust was established to manage the copyrighted materials produced by the IETF. 
Meetings.
The first IETF meeting was attended by 21 U.S.-government-funded researchers on 16 January 1986. It was a continuation of the work of the earlier GADS Task Force. Representatives from non-governmental entities were invited to attend starting with the fourth IETF meeting in October 1986. Since that time all IETF meetings have been open to the public.
Initially, the IETF met quarterly, but from 1991, it has been meeting three times a year. The initial meetings were very small, with fewer than 35 people in attendance at each of the first five meetings. The maximum attendance during the first 13 meetings was only 120 attendees. This occurred at the 12th meeting held during January 1989. These meetings have grown in both participation and scope a great deal since the early 1990s; it had a maximum attendance of 2,810 at the December 2000 IETF held in San Diego, CA. Attendance declined with industry restructuring during the early 2000s, and is currently around 1,200.
The location for IETF meetings vary greatly. A list of past and future meeting locations can be found on the IETF meetings page. The IETF strives to hold its meetings near where most of the IETF volunteers are located. For many years, the goal was three meetings a year, with two in North America and one in either Europe or Asia, alternating between them every other year. The current goal is to hold three meetings in North America, two in Europe and one in Asia during a two-year period. However, corporate sponsorship of the meetings is also an important factor and the schedule has been modified from time to time in order to decrease operational costs.
Operations.
The details of IETF operations have changed considerably as the organization has grown, but the basic mechanism remains publication of proposed specifications, development based on the proposals, review and independent testing by participants, and republication as a revised proposal, a draft proposal, or eventually as an Internet Standard. IETF standards are developed in an open, all-inclusive process in which any interested individual can participate. All IETF documents are freely available over the Internet and can be reproduced at will. Multiple, working, useful, interoperable implementations are the chief requirement before an IETF proposed specification can become a standard. Most specifications are focused on single protocols rather than tightly interlocked systems. This has allowed the protocols to be used in many different systems, and its standards are routinely re-used by bodies which create full-fledged architectures (e.g. 3GPP IMS).
Because it relies on volunteers and uses "rough consensus and running code" as its touchstone, results can be slow whenever the number of volunteers is either too small to make progress, or so large as to make consensus difficult, or when volunteers lack the necessary expertise. For protocols like SMTP, which is used to transport e-mail for a user community in the many hundreds of millions, there is also considerable resistance to any change that is not fully backwards compatible. Work within the IETF on ways to improve the speed of the standards-making process is ongoing but, because the number of volunteers with opinions on it is very great, consensus on improvements have been slow to develop.
The IETF cooperates with the W3C, ISO/IEC, ITU, and other standards bodies.
Statistics are available that show who the top contributors by RFC publication are. While the IETF only allows for participation by individuals, and not by corporations or governments, sponsorship information is available from these statistics.
Chairs.
The IETF Chairperson is selected by the Nominating Committee (NomCom) process for a 2-year renewable term. Before 1993, the IETF Chair was selected by the IAB.

</doc>
<doc id="15286" url="http://en.wikipedia.org/wiki?curid=15286" title="ISM band">
ISM band

The industrial, scientific and medical (ISM) radio bands are radio bands (portions of the radio spectrum) reserved internationally for the use of radio frequency (RF) energy for industrial, scientific and medical purposes other than telecommunications. 
Examples of applications in these bands include radio-frequency process heating, microwave ovens, and medical diathermy machines. The powerful emissions of these devices can create electromagnetic interference and disrupt radio communication using the same frequency, so these devices were limited to certain bands of frequencies. In general, communications equipment operating in these bands must tolerate any interference generated by ISM equipment, and users have no regulatory protection from ISM device operation.
Despite the intent of the original allocations, and because there are multiple allocations, in recent years the fastest-growing uses of these bands have been for short-range, low power communications systems. Cordless phones, Bluetooth devices, near field communication (NFC) devices, and wireless computer networks all use frequencies allocated to low power communications as well as ISM, although these low power emitters are not considered ISM.
ISM bands.
The ISM bands are defined by the ITU-R in 5.138, 5.150, and 5.280 of the Radio Regulations. Individual countries' use of the bands designated in these sections may differ due to variations in national radio regulations. Because communication devices using the ISM bands must tolerate any interference from ISM equipment, unlicensed operations are typically permitted to use these bands, since unlicensed operation typically needs to be tolerant of interference from other devices anyway. The ISM bands share allocations with unlicensed and licensed operations; however, due to the high likelihood of harmful interference, licensed use of the bands is typically low. In the United States, uses of the ISM bands are governed by Part 18 of the Federal Communications Commission (FCC) rules, while Part 15 contains the rules for unlicensed communication devices, even those that share ISM frequencies. In Europe, the ETSI is responsible for governing ISM bands.
The ISM bands defined by the ITU-R are:
Regulatory authorities may allocate parts of the radio spectrum for unlicensed communications that may or may not also be allocated as ISM bands.
History.
The ISM bands were first established at the International Telecommunications Conference of the ITU in Atlantic City, 1947. The American delegation specifically proposed several bands, including the now commonplace 2.4 GHz band, to accommodate the then nascent process of microwave heating; however, FCC annual reports of that time suggest that much preparation was done ahead of these presentations.
From the proceedings:
"The delegate of the United States, referring to his request that the frequency 2450 Mc/s be allocated for I.S.M., indicated that there was in existence in the United States, and working on this frequency a diathermy machine and an electronic cooker, and that the latter might eventually be installed in transatlantic ships and airplanes. There was therefore some point in attempting to reach world agreement on this subject."
Radio frequencies in the ISM bands have been used for communication purposes, although such devices may experience interference from non-communication sources. In the United States, as early as 1958 Class D Citizens Band, a Part 95 service, was allocated to frequencies that are also allocated to ISM. [1]
In the U.S., the FCC first made unlicensed spread spectrum available in the ISM bands in rules adopted on May 9, 1985.
Many other countries later developed similar regulations, enabling use of this technology. The FCC action was proposed by Michael Marcus of the FCC staff in 1980 and the subsequent regulatory action took five more years. It was part of a broader proposal to allow civil use of spread spectrum technology and was opposed at the time by mainstream equipment manufacturers and many radio system operators.
ISM uses.
For many people, the most commonly encountered ISM device is the home microwave oven operating at 2.45 GHz; however, many different kinds of ISM devices exist, which are predominantly found outside dwellings. Many industrial settings may use ISM devices in plastic welding processes. In medical settings, shortwave and microwave diathermy machines are ISM devices mostly commonly used for muscle relaxation. Microwave ablation, a type of interventional radiology, is an ISM application which treats solid tumors through the use of RF heating.
Some electrodeless lamp designs are ISM devices, which use RF emissions to excite fluorescent tubes. Sulfur lamps are commercially available plasma lamps, which use a 2.45 GHz magnetron to heat sulfur into a brightly glowing plasma.
Long-distance wireless power systems have been proposed and experimented with which would use high-power transmitters and rectennas, in lieu of overhead transmission lines and underground cables, to send power to remote locations. NASA has studied using microwave power transmission on 2.45 GHz to send energy collected by solar power satellites back to the ground.
Also in space applications, a Helicon Double Layer ion thruster is a prototype spacecraft propulsion engine which uses a 13.56 MHz transmission to break down and heat gas into plasma.
Non-ISM uses.
In recent years ISM bands have also been shared with (non-ISM) license-free error-tolerant communications applications such as wireless sensor networks in the 915 MHz and 2.450 GHz bands, as well as wireless LANs and cordless phones in the 915 MHz, 2.450 GHz, and 5.800 GHz bands. Because unlicensed devices are required to be tolerant of ISM emissions in these bands, unlicensed low power users are generally able to operate in these bands without causing problems for ISM users. ISM equipment does not necessarily include a radio receiver in the ISM band (e.g. a microwave oven does not have a receiver).
In the United States, according to 47 CFR Part 15.5, low power communication devices must accept interference from licensed users of that frequency band, and the Part 15 device must not cause interference to licensed users. Note that the 915 MHz band should not be used in countries outside Region 2, except those that specifically allow it, such as Australia and Israel, especially those that use the GSM-900 band for cellphones. The ISM bands are also widely used for Radio-frequency identification (RFID) applications with the most commonly used band being the 13.56 MHz band used by systems compliant with ISO/IEC 14443 including those used by biometric passports and contactless smart cards.
In Europe, the use of the ISM band is covered by Short Range Device regulations issued by European Commission, based on technical recommendations by CEPT and standards by ETSI. In most of Europe, LPD433 band is allowed for license-free voice communication in addition to PMR446.
Wireless LAN devices use wavebands as follows:
IEEE 802.15.4, ZigBee and other personal area networks may use the 915 MHz and 2450 MHz ISM bands because of frequency sharing between different allocations.
Wireless LANs and cordless phones can also use bands other than those shared with ISM, but such uses require approval on a country by country basis. DECT phones use allocated spectrum outside the ISM bands that differs in Europe and North America. Ultra-wideband LANs require more spectrum than the ISM bands can provide, so the relevant standards such as IEEE 802.15.4a are designed to make use of spectrum outside the ISM bands. Despite the fact that these additional bands are outside the official ITU-R ISM bands, because they are used for the same types of low power personal communications, they are sometimes incorrectly referred to as ISM bands as well.
Also note that several brands of radio control equipment use the 2.4 GHz band range for low power remote control of toys, from gas powered cars to miniature aircraft.
Worldwide Digital Cordless Telecommunications or WDCT is a technology that uses the 2.4 GHz radio spectrum.
Google's Project Loon uses ISM bands (specifically 2.4 and 5.8 GHz bands) for balloon-to-balloon and balloon-to-ground communications.
Pursuant to 47 CFR Part 97 some ISM bands are used by licensed amateur radio operators for communication - including amateur television.

</doc>
<doc id="15287" url="http://en.wikipedia.org/wiki?curid=15287" title="Series (mathematics)">
Series (mathematics)

A series is, informally speaking, the sum of the terms of a sequence. Finite sequences and series have defined first and last terms, whereas infinite sequences and series continue indefinitely.
In mathematics, given an infinite sequence of numbers { "a""n" }, a series is informally the result of adding all those terms together: "a"1 + "a"2 + "a"3 + · · ·. These can be written more compactly using the summation symbol ∑. An example is the famous series from Zeno's dichotomy and its mathematical representation:
The terms of the series are often produced according to a certain rule, such as by a formula, or by an algorithm. As there are an infinite number of terms, this notion is often called an infinite series. Unlike finite summations, infinite series need tools from mathematical analysis, and specifically the notion of limits, to be fully understood and manipulated. In addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, and finance.
Basic properties.
Definition.
For any sequence formula_2 of rational numbers, real numbers, complex numbers, functions thereof, etc., the associated series is defined as the ordered formal sum
The sequence of partial sums formula_4 associated to a series formula_5 is defined for each formula_6 as the sum of the sequence formula_2 from formula_8 to formula_9
By definition the series formula_11 converges to a limit formula_12 if and only if the associated sequence of partial sums formula_4 converges to formula_12. This definition is usually written as
More generally, if formula_16 is a function from an index set I to a set G, then the series associated to formula_17 is the formal sum of the elements formula_18 over the index elements formula_19 denoted by the
When the index set is the natural numbers formula_21, the function formula_22 is a sequence denoted by formula_23. A series indexed on the natural numbers is an ordered formal sum and so we rewrite formula_24 as formula_25 in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers
When the set formula_27 is a semigroup, the sequence of partial sums formula_28 associated to a sequence formula_29 is defined for each formula_6 as the sum of the terms formula_31
When the semigroup formula_27 is also a topological space, then the series formula_11 converges to an element formula_35 if and only if the associated sequence of partial sums formula_4 converges to formula_12. This definition is usually written as
Convergent series.
A series  ∑"an"  is said to 'converge' or to 'be convergent' when the sequence "S""N" of partial sums has a finite limit. If the limit of "S""N" is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the sum of the series
An easy way that an infinite series can converge is if all the "a""n" are zero for "n" sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense.
Working out the properties of the series that converge even if infinitely many terms are non-zero is the essence of the study of series. Consider the example
It is possible to "visualize" its convergence on the real number line: we can imagine a line of length 2, with successive segments marked off of lengths 1, ½, ¼, etc. There is always room to mark the next segment, because the amount of line remaining is always the same as the last segment marked: when we have marked off ½, we still have a piece of length ½ unmarked, so we can certainly mark the next ¼. This argument does not prove that the sum is "equal" to 2 (although it is), but it does prove that it is "at most" 2. In other words, the series has an upper bound. Given that the series converges, proving that it is equal to 2 requires only elementary algebra. If the series is denoted "S", it can be seen that
Therefore,
Mathematicians extend the idiom discussed earlier to other, equivalent notions of series. For instance, when we talk about a recurring decimal, as in
we are talking, in fact, just about the series
But since these series always converge to real numbers (because of what is called the completeness property of the real numbers), to talk about the series in this way is the same as to talk about the numbers for which they stand. In particular, it should offend no sensibilities if we make no distinction between 0.111… and 1/9. Less clear is the argument that 9 × 0.111… = 0.999… = 1, but it is not untenable when we consider that we can formalize the proof knowing only that limit laws preserve the arithmetic operations. See 0.999... for more.
Calculus and partial summation as an operation on sequences.
Partial summation takes as input a sequence, { "a""n" }, and gives as output another sequence, { "S""N" }. It is thus a unary operation on sequences. Further, this function is linear, and thus is a linear operator on the vector space of sequences, denoted Σ. The inverse operator is the finite difference operator, Δ. These behave as discrete analogs of integration and differentiation, only for series (functions of a natural number) instead of functions of a real variable. For example, the sequence {1, 1, 1, ...} has series {1, 2, 3, 4, ...} as its partial summation, which is analogous to the fact that formula_53
In computer science it is known as prefix sum.
Properties of series.
Series are classified not only by whether they converge or diverge, but also by the properties of the terms an (absolute or conditional convergence); type of convergence of the series (pointwise, uniform); the class of the term an (whether it is a real number, arithmetic progression, trigonometric function); etc.
Non-negative terms.
When "an" is a non-negative real number for every "n", the sequence "SN" of partial sums is non-decreasing. It follows that a series ∑"an" with non-negative terms converges if and only if the sequence "SN" of partial sums is bounded.
For example, the series
is convergent, because the inequality
and a telescopic sum argument implies that the partial sums are bounded by 2.
Absolute convergence.
A series
is said to converge absolutely if the series of absolute values
converges. This is sufficient to guarantee not only that the original series converges to a limit, but also that any reordering of it converges to the same limit.
Conditional convergence.
A series of real or complex numbers is said to be conditionally convergent (or semi-convergent) if it is convergent but not absolutely convergent. A famous example is the alternating series
which is convergent (and its sum is equal to ln 2), but the series formed by taking the absolute value of each term is the divergent harmonic series. The Riemann series theorem says that any conditionally convergent series can be reordered to make a divergent series, and moreover, if the "a""n" are real and "S" is any real number, that one can find a reordering so that the reordered series converges with sum equal to "S".
Abel's test is an important tool for handling semi-convergent series. If a series has the form
where the partial sums "B""N" = "b"0 + ··· + "bn" are bounded, "λ""n" has bounded variation, and lim λ"n" "B""n" exists:
then the series ∑ "an" is convergent. This applies to the pointwise convergence of many trigonometric series, as in
with 0 < "x" < 2π. Abel's method consists in writing "b""n"+1 = "B""n"+1 − "B""n", and in performing a transformation similar to integration by parts (called summation by parts), that relates the given series ∑ "an" to the absolutely convergent series
Series of functions.
A series of real- or complex-valued functions
converges pointwise on a set "E", if the series converges for each "x" in "E" as an ordinary series of real or complex numbers. Equivalently, the partial sums
converge to "ƒ"("x") as "N" → ∞ for each "x" ∈ "E".
A stronger notion of convergence of a series of functions is called uniform convergence. The series converges uniformly if it converges pointwise to the function "ƒ"("x"), and the error in approximating the limit by the "N"th partial sum,
can be made minimal "independently" of "x" by choosing a sufficiently large "N".
Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the "ƒ""n" are integrable on a closed and bounded interval "I" and converge uniformly, then the series is also integrable on "I" and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.
More sophisticated types of convergence of a series of functions can also be defined. In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a certain set of measure zero. Other modes of convergence depend on a different metric space structure on the space of functions under consideration. For instance, a series of functions converges in mean on a set "E" to a limit function "ƒ" provided
as "N" → ∞.
Power series.
A power series is a series of the form
The Taylor series at a point "c" of a function is a power series that, in many cases, converges to the function in a neighborhood of "c". For example, the series
is the Taylor series of formula_69 at the origin and converges to it for every "x".
Unless it converges only at "x"="c", such a series converges on a certain open disc of convergence centered at the point "c" in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients "a""n". The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.
Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent.
When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required.
However, the formal operation with non-convergent series has been retained in rings of formal power series which are studied in abstract algebra. Formal power series are also used in combinatorics to describe and study sequences that are otherwise difficult to handle; this is the method of generating functions.
Laurent series.
Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form
If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence.
Dirichlet series.
A Dirichlet series is one of the form
where "s" is a complex number. For example, if all "a""n" are equal to 1, then the Dirichlet series is the Riemann zeta function
Like the zeta function, Dirichlet series in general play an important role in analytic number theory. Generally a Dirichlet series converges if the real part of "s" is greater than a number called the abscissa of convergence. In many cases, a Dirichlet series can be extended to an analytic function outside the domain of convergence by analytic continuation. For example, the Dirichlet series for the zeta function converges absolutely when Re "s" > 1, but the zeta function can be extended to a holomorphic function defined on formula_73  with a simple pole at 1.
This series can be directly generalized to general Dirichlet series.
Trigonometric series.
A series of functions in which the terms are trigonometric functions is called a trigonometric series:
The most important example of a trigonometric series is the Fourier series of a function.
History of the theory of infinite series.
Development of infinite series.
Greek mathematician Archimedes produced the first known summation of an infinite series with a
method that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of π.
In the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.
Convergence criteria.
The investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series
on which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.
Cauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms "convergence" and "divergence" had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.
Abel (1826) in his memoir on the binomial series
corrected certain of Cauchy's conclusions, and gave a completely
scientific summation of the series for complex values of formula_77 and formula_78. He showed the necessity of considering the subject of continuity in questions of convergence.
Cauchy's methods led to special rather than general criteria, and
the same may be said of Raabe (1832), who made the first elaborate
investigation of the subject, of De Morgan (from 1842), whose
logarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have
shown to fail within a certain region; of Bertrand (1842), Bonnet
(1843), Malmsten (1846, 1847, the latter without integration);
Stokes (1847), Paucker (1852), Chebyshev (1852), and Arndt
(1853).
General criteria began with Kummer (1835), and have been
studied by Eisenstein (1847), Weierstrass in his various
contributions to the theory of functions, Dini (1867),
DuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.
Uniform convergence.
The theory of uniform convergence was treated by Cauchy (1821), his
limitations being pointed out by Abel, but the first to attack it
successfully were Seidel and Stokes (1847–48). Cauchy took up the
problem again (1853), acknowledging Abel's criticism, and reaching
the same conclusions which Stokes had already found. Thomae used the
doctrine (1866), but there was great delay in recognizing the
importance of distinguishing between uniform and non-uniform
convergence, in spite of the demands of the theory of functions.
Semi-convergence.
A series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.
Semi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834),
who attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schlömilch ("Zeitschrift", Vol.I, p. 192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function
Genocchi (1852) has further contributed to the theory.
Among the early writers was Wronski, whose "loi suprême" (1815) was hardly recognized until Cayley (1873) brought it into
prominence.
Fourier series.
Fourier series were being investigated
as the result of physical considerations at the same time that
Gauss, Abel, and Cauchy were working out the theory of infinite
series. Series for the expansion of sines and cosines, of multiple
arcs in powers of the sine and cosine of the arc had been treated by
Jacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still
earlier by Vieta. Euler and Lagrange simplified the subject,
as did Poinsot, Schröter, Glaisher, and Kummer.
Fourier (1807) set for himself a different problem, to
expand a given function of "x" in terms of the sines or cosines of
multiples of "x", a problem which he embodied in his "Théorie analytique de la chaleur" (1822). Euler had already given the
formulas for determining the coefficients in the series;
Fourier was the first to assert and attempt to prove the general
theorem. Poisson (1820–23) also attacked the problem from a
different standpoint. Fourier did not, however, settle the question
of convergence of his series, a matter left for Cauchy (1826) to
attempt and for Dirichlet (1829) to handle in a thoroughly
scientific manner (see convergence of Fourier series). Dirichlet's treatment ("Crelle", 1829), of trigonometric series was the subject of criticism and improvement by
Riemann (1854), Heine, Lipschitz, Schläfli, and
du Bois-Reymond. Among other prominent contributors to the theory of
trigonometric and Fourier series were Dini, Hermite, Halphen,
Krause, Byerly and Appell.
Generalizations.
Asymptotic series.
Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge. But they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers.
Divergent series.
Under many circumstances, it is desirable to assign a limit to a series which fails to converge in the usual sense. A summability method is such an assignment of a limit to a subset of the set of divergent series which properly extends the classical notion of convergence. Summability methods include Cesàro summation, ("C","k") summation, Abel summation, and Borel summation, in increasing order of generality (and hence applicable to increasingly divergent series).
A variety of general results concerning possible summability methods are known. The Silverman–Toeplitz theorem characterizes "matrix summability methods", which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients. The most general method for summing a divergent series is non-constructive, and concerns Banach limits.
Series in Banach spaces.
The notion of series can be easily extended to the case of a Banach space. If "x""n" is a sequence of elements of a Banach space "X", then the series Σ"x""n" converges to "x" ∈ "X" if the sequence of partial sums of the series tends to "x"; to wit,
as "N" → ∞.
More generally, convergence of series can be defined in any abelian Hausdorff topological group. Specifically, in this case, Σ"x""n" converges to "x" if the sequence of partial sums converges to "x".
Summations over arbitrary index sets.
Definitions may be given for sums over an arbitrary index set "I". There are two main differences with the usual notion of series: first, there is no specific order given on the set "I"; second, this set "I" may be uncountable.
Families of non-negative numbers.
When summing a family {"a""i"}, "i" ∈ "I", of non-negative numbers, one may define
When the sum is finite, the set of "i" ∈ "I" such that "ai" > 0 is countable. Indeed for every "n" ≥ 1, the set formula_82 is finite, because
If "I"  is countably infinite and enumerated as "I" = {"i"0, "i"1...} then the above defined sum satisfies
provided the value ∞ is allowed for the sum of the series.
Any sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.
Abelian topological groups.
Let "a" : "I" → "X", where "I"  is any set and "X"  is an abelian Hausdorff topological group. Let "F"  be the collection of all finite subsets of "I". Note that "F"  is a directed set ordered under inclusion with union as join. Define the sum "S"  of the family "a" as the limit
if it exists and say that the family "a" is unconditionally summable. Saying that the sum "S"  is the limit of finite partial sums means that for every neighborhood "V"  of 0 in "X", there is a finite subset "A"0 of "I"  such that
Because "F"  is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.
For every "W", neighborhood of 0 in "X", there is a smaller neighborhood "V"  such that "V" − "V" ⊂ "W". It follows that the finite partial sums of an unconditionally summable family "ai", "i" ∈ "I", form a "Cauchy net", that is: for every "W", neighborhood of 0 in "X", there is a finite subset "A"0 of "I"  such that
When "X"  is complete, a family "a" is unconditionally summable in "X"  if and only if the finite sums satisfy the latter Cauchy net condition. When "X"  is complete and "ai", "i" ∈ "I", is unconditionally summable in "X", then for every subset "J" ⊂ "I", the corresponding subfamily "aj", "j" ∈ "J", is also unconditionally summable in "X".
When the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group "X" = R.
If a family "a" in "X"  is unconditionally summable, then for every "W", neighborhood of 0 in "X", there is a finite subset "A"0 of "I"  such that "a""i" ∈ "W"  for every "i" not in "A"0. If "X"  is first-countable, it follows that the set of "i" ∈ "I"  such that "ai" ≠ 0 is countable. This need not be true in a general abelian topological group (see examples below).
Unconditionally convergent series.
Suppose that "I" = N. If a family "a""n", "n" ∈ N, is unconditionally summable in an abelian Hausdorff topological group "X", then the series in the usual sense converges and has the same sum,
By nature, the definition of unconditional summability is insensitive to the order of the summation. When ∑"a""n" is unconditionally summable, then the series remains convergent after any permutation "σ" of the set N of indices, with the same sum,
Conversely, if every permutation of a series ∑"a""n" converges, then the series is unconditionally convergent. When "X"  is complete, then unconditional convergence is also equivalent to the fact that all subseries are convergent; if "X"  is a Banach space, this is equivalent to say that for every sequence of signs "ε""n" = 1 or &minu: this is not about convergence of functions, even less about uniform convergence. -->s;1, the series
converges in "X". If "X"  is a Banach space, then one may define the notion of absolute convergence. A series ∑"a""n" of vectors in "X"  converges absolutely if
If a series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of ).
Well-ordered sums.
Conditionally convergent series can be considered if "I" is a well-ordered set, for example an ordinal number "α"0. One may define by transfinite recursion:
and for a limit ordinal "α",
if this limit exists. If all limits exist up to "α"0, then the series converges.

</doc>
<doc id="15289" url="http://en.wikipedia.org/wiki?curid=15289" title="Interrupt">
Interrupt

In systems programming, an interrupt is a signal to the processor emitted by hardware or software indicating an event that needs immediate attention. An interrupt alerts the processor to a high-priority condition requiring the interruption of the current code the processor is executing. The processor responds by suspending its current activities, saving its state, and executing a function called an "interrupt handler" (or an interrupt service routine, ISR) to deal with the event. This interruption is temporary, and, after the interrupt handler finishes, the processor resumes normal activities. There are two types of interrupts: hardware interrupts and software interrupts.
Hardware interrupts are used by devices to communicate that they require attention from the operating system. Internally, hardware interrupts are implemented using electronic alerting signals that are sent to the processor from an external device, which is either a part of the computer itself, such as a disk controller, or an external peripheral. For example, pressing a key on the keyboard or moving the mouse triggers hardware interrupts that cause the processor to read the keystroke or mouse position. Unlike the software type (described below), hardware interrupts are asynchronous and can occur in the middle of instruction execution, requiring additional care in programming. The act of initiating a hardware interrupt is referred to as an interrupt request (IRQ).
A software interrupt is caused either by an exceptional condition in the processor itself, or a special instruction in the instruction set which causes an interrupt when it is executed. The former is often called a "trap" or "exception" and is used for errors or events occurring during program execution that are exceptional enough that they cannot be handled within the program itself. For example, if the processor's arithmetic logic unit is commanded to divide a number by zero, this impossible demand will cause a "divide-by-zero exception", perhaps causing the computer to abandon the calculation or display an error message. Software interrupt instructions function similarly to subroutine calls and are used for a variety of purposes, such as to request services from low-level system software such as device drivers. For example, computers often use software interrupt instructions to communicate with the disk controller to request data be read or written to the disk. 
Each interrupt has its own interrupt handler. The number of hardware interrupts is limited by the number of interrupt request (IRQ) lines to the processor, but there may be hundreds of different software interrupts. Interrupts are a commonly used technique for computer multitasking, especially in real-time computing. Such a system is said to be interrupt-driven.
Overview.
Hardware interrupts were introduced as a way to reduce wasting the processor's valuable time in polling loops, waiting for external events. They may be implemented in hardware as a distinct system with control lines, or they may be integrated into the memory subsystem.
If implemented in hardware, an interrupt controller circuit such as the IBM PC's Programmable Interrupt Controller (PIC) may be connected between the interrupting device and the processor's interrupt pin to multiplex several sources of interrupt onto the one or two CPU lines typically available. If implemented as part of the memory controller, interrupts are mapped into the system's memory address space.
Interrupts can be categorized into these different types:
Processors typically have an internal "interrupt mask" which allows software to ignore all external hardware interrupts while it is set. Setting or clearing this mask may be faster than accessing an interrupt mask register (IMR) in a PIC or disabling interrupts in the device itself. In some cases, such as the x86 architecture, disabling and enabling interrupts on the processor itself act as a memory barrier; however, it may actually be slower.
An interrupt that leaves the machine in a well-defined state is called a "precise interrupt". Such an interrupt has four properties:
An interrupt that does not meet these requirements is called an "imprecise interrupt".
The phenomenon where the overall system performance is severely hindered by excessive amounts of processing time spent handling interrupts is called an interrupt storm.
Types of interrupts.
Level-triggered.
A "level-triggered interrupt" is an interrupt signalled by maintaining the interrupt line at a high or low level. A device wishing to signal a Level-triggered interrupt drives the interrupt request line to its active level (high or low), and then holds it at that level until it is serviced. It ceases asserting the line when the CPU commands it to or otherwise handles the condition that caused it to signal the interrupt.
Typically, the processor samples the interrupt input at predefined times during each bus cycle such as state T2 for the Z80 microprocessor. If the interrupt isn't active when the processor samples it, the CPU doesn't see it. One possible use for this type of interrupt is to minimize spurious signals from a noisy interrupt line: a spurious pulse will often be so short that it is not noticed.
Multiple devices may share a level-triggered interrupt line if they are designed to. The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to its inactive state. Devices actively assert the line to indicate an outstanding interrupt, but let the line float (do not actively drive it) when not signalling an interrupt. The line is then in its asserted state when any (one or more than one) of the sharing devices is signalling an outstanding interrupt.
Level-triggered interrupt is favored by some because it is easy to share the interrupt request line without losing the interrupts, when multiple shared devices interrupt at the same time. Upon detecting assertion of the interrupt line, the CPU must search through the devices sharing the interrupt request line until one who triggered the interrupt is detected. After servicing this device, the CPU may recheck the interrupt line status to determine whether any other devices also needs service. If the line is now de-asserted, the CPU avoids checking the remaining devices on the line. Since some devices interrupt more frequently than others, and other device interrupts are particularly expensive, a careful ordering of device checks is employed to increase efficiency. The original PCI standard mandated level-triggered interrupts because of this advantage of sharing interrupts. 
There are also serious problems with sharing level-triggered interrupts. As long as any device on the line has an outstanding request for service the line remains asserted, so it is not possible to detect a change in the status of any other device. Deferring servicing a low-priority device is not an option, because this would prevent detection of service requests from higher-priority devices. If there is a device on the line that the CPU does not know how to service, then any interrupt from that device permanently blocks all interrupts from the other devices.
Edge-triggered.
An "edge-triggered interrupt" is an interrupt signalled by a level transition on the interrupt line, either a falling edge (high to low) or a rising edge (low to high). A device, wishing to signal an interrupt, drives a pulse onto the line and then releases the line to its inactive state. If the pulse is too short to be detected by polled I/O then special hardware may be required to detect the edge.
Multiple devices may share an edge-triggered interrupt line if they are designed to. The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to one particular state. Devices signal an interrupt by briefly driving the line to its non-default state, and let the line float (do not actively drive it) when not signalling an interrupt. This type of connection is also referred to as open collector. The line then carries all the pulses generated by all the devices. (This is analogous to the pull cord on some buses and trolleys that any passenger can pull to signal the driver that they are requesting a stop.) However, interrupt pulses from different devices may merge if they occur close in time. To avoid losing interrupts the CPU must trigger on the trailing edge of the pulse (e.g. the rising edge if the line is pulled up and driven low). After detecting an interrupt the CPU must check all the devices for service requirements.
Edge-triggered interrupts do not suffer the problems that level-triggered interrupts have with sharing. Service of a low-priority device can be postponed arbitrarily, and interrupts will continue to be received from the high-priority devices that are being serviced. If there is a device that the CPU does not know how to service, it may cause a spurious interrupt, or even periodic spurious interrupts, but it does not interfere with the interrupt signalling of the other devices. However, it is fairly easy for an edge triggered interrupt to be missed - for example if interrupts have to be masked for a period - and unless there is some type of hardware latch that records the event it is impossible to recover. Such problems caused many "lockups" in early computer hardware because the processor did not know it was expected to do something. More modern hardware often has one or more interrupt status registers that latch the interrupt requests; well written edge-driven interrupt software often checks such registers to ensure events are not missed.
The elderly Industry Standard Architecture (ISA) bus uses edge-triggered interrupts, but does not mandate that devices be able to share them. The parallel port also uses edge-triggered interrupts. Many older devices assume that they have exclusive use of their interrupt line, making it electrically unsafe to share them. However, ISA motherboards include pull-up resistors on the IRQ lines, so well-behaved devices share ISA interrupts just fine.
Hybrid.
Some systems use a hybrid of level-triggered and edge-triggered signalling. The hardware not only looks for an edge, but it also verifies that the interrupt signal stays active for a certain period of time.
A common use of a hybrid interrupt is for the NMI (non-maskable interrupt) input. Because NMIs generally signal major – or even catastrophic – system events, a good implementation of this signal tries to ensure that the interrupt is valid by verifying that it remains active for a period of time. This 2-step approach helps to eliminate false interrupts from affecting the system.
Message-signaled.
A "message-signalled interrupt" does not use a physical interrupt line. Instead, a device signals its request for service by sending a short message over some communications medium, typically a computer bus. The message might be of a type reserved for interrupts, or it might be of some pre-existing type such as a memory write.
Message-signalled interrupts behave very much like edge-triggered interrupts, in that the interrupt is a momentary signal rather than a continuous condition. Interrupt-handling software treats the two in much the same manner. Typically, multiple pending message-signalled interrupts with the same message (the same virtual interrupt line) are allowed to merge, just as closely spaced edge-triggered interrupts can merge.
Message-signalled interrupt vectors can be shared, to the extent that the underlying communication medium can be shared. No additional effort is required.
Because the identity of the interrupt is indicated by a pattern of data bits, not requiring a separate physical conductor, many more distinct interrupts can be efficiently handled. This reduces the need for sharing. Interrupt messages can also be passed over a serial bus, not requiring any additional lines.
PCI Express, a serial computer bus, uses message-signalled interrupts exclusively.
Doorbell.
In a push button analogy applied to computer systems, the term "doorbell" or "doorbell interrupt" is often used to describe a mechanism whereby a software system can signal or notify a computer hardware device that there is some work to be done. Typically, the software system will place data in some well known and mutually agreed upon memory location(s), and "ring the doorbell" by writing to a different memory location. This different memory location is often called the doorbell region, and there may even be multiple doorbells serving different purposes in this region. It is this act of writing to the doorbell region of memory that "rings the bell" and notifies the hardware device that the data are ready and waiting. The hardware device would now know that the data are valid and can be acted upon. It would typically write the data to a hard disk drive, or send them over a network, or encrypt them, etc. 
The term "doorbell interrupt" is usually a misnomer. It's similar to an interrupt, because it causes some work to be done by the device; however, the doorbell region is sometimes implemented as a polled region, sometimes the doorbell region writes through to physical device registers, and sometimes the doorbell region is hardwired directly to physical device registers. When either writing through or directly to physical device registers, this may cause a real interrupt to occur at the device's central processor unit (CPU), if it has one.
Doorbell interrupts can be compared to Message Signaled Interrupts, as they have some similarities.
Difficulty with sharing interrupt lines.
Multiple devices sharing an interrupt line (of any triggering style) all act as spurious interrupt sources with respect to each other. With many devices on one line the workload in servicing interrupts grows in proportion to the square of the number of devices. It is therefore preferred to spread devices evenly across the available interrupt lines. Shortage of interrupt lines is a problem in older system designs where the interrupt lines are distinct physical conductors. Message-signalled interrupts, where the interrupt line is virtual, are favored in new system architectures (such as PCI Express) and relieve this problem to a considerable extent.
Some devices with a poorly designed programming interface provide no way to determine whether they have requested service. They may lock up or otherwise misbehave if serviced when they do not want it. Such devices cannot tolerate spurious interrupts, and so also cannot tolerate sharing an interrupt line. ISA cards, due to often cheap design and construction, are notorious for this problem. Such devices are becoming much rarer, as hardware logic becomes cheaper and new system architectures mandate shareable interrupts.
Performance issues.
Interrupts provide low overhead and good latency at low load, but degrade significantly at high interrupt rate unless care is taken to prevent several pathologies. These are various forms of livelocks, when the system spends all of its time processing interrupts to the exclusion of other required tasks. Under extreme conditions, a large number of interrupts (like very high network traffic) may completely stall the system. To avoid such problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution.
With multi-core processors, additional performance improvements in interrupt handling can be achieved through receive-side scaling (RSS) when multiqueue NICs are used. Such NICs provide multiple receive queues associated to separate interrupts; by routing each of those interrupts to different cores, processing of the interrupt requests triggered by the network traffic received by a single NIC can be distributed among multiple cores. Distribution of the interrupts among cores can be performed automatically by the operating system, or the routing of interrupts (usually referred to as "IRQ affinity") can be manually configured.
A purely software-based implementation of the receiving traffic distribution, known as "receive packet steering" (RPS), distributes received traffic among cores later in the data path, as part of the interrupt handler functionality. Advantages of RPS over RSS include no requirements for specific hardware, more advanced traffic distribution filters, and reduced rate of interrupts produced by a NIC. As a downside, RPS increases the rate of inter-processor interrupts (IPIs). "Receive flow steering" (RFS) takes the software-based approach further by accounting for application locality; further performance improvements are achieved by processing interrupt requests by the same cores on which particular network packets will be consumed by the targeted application.
Typical uses.
Typical uses of interrupts include the following: system timers, disk I/O, power-off signals, and traps. Other interrupts exist to transfer data bytes using UARTs or Ethernet; sense key-presses; control motors; or anything else the equipment must do.
One typical use is to generate interrupts periodically by dividing the output of a crystal oscillator and having an interrupt handler count the interrupts in order to keep time. These periodic interrupts are often used by the OS's task scheduler to reschedule the priorities of running processes. Some older computers generated periodic interrupts from the power line frequency because it was controlled by the utilities to eliminate long-term drift of electric clocks.
For example, a disk interrupt signals the completion of a data transfer from or to the disk peripheral; a process waiting to read or write a file starts up again. As another example, a power-off interrupt predicts or requests a loss of power, allowing the computer equipment to perform an orderly shut-down. Also, interrupts are used in typeahead features for buffering events like keystrokes.

</doc>
<doc id="15290" url="http://en.wikipedia.org/wiki?curid=15290" title="Intercalation (timekeeping)">
Intercalation (timekeeping)

Intercalation is the insertion of a leap day, week or month into some calendar years to make the calendar follow the seasons or moon phases. Lunisolar calendars may require intercalations of both days and months.
Solar calendars.
The solar or tropical year does not have a whole number of days (it is about 365.24 days), but a calendar year must have a whole number of days. The most common way to reconcile the two is to vary the number of days in the calendar year.
In solar calendars, this is done by adding to a common year of 365 days, an extra day ("leap day" or "intercalary day") about every four years, causing a leap year to have 366 days (Julian, Gregorian and Indian national calendars).
The Decree of Canopus, which was issued by the pharaoh Ptolemy III, Euergetes of Ancient Egypt in 239 BC, decreed a solar leap day system; an Egyptian leap year was not adopted until 25 BC, when the Roman Emperor Augustus successfully instituted a reformed Alexandrian calendar.
In the Julian calendar, as well as in the Gregorian calendar, which improved upon it, intercalation is done by adding an extra day to February in each leap year. In the Julian calendar this was done every four years. In the Gregorian, years divisible by 100 but not 400, were exempted in order to improve accuracy. Thus, 2000 was a leap year; 1700, 1800, and 1900 were not.
Epagomenal days are days within a solar calendar that are outside any regular month. Usually five epagomenal days are included within every year (Egyptian, Coptic, Ethiopian, Maya Haab and French Republican calendars), but a sixth epagomenal day is intercalated every four years in some (Coptic, Ethiopian and French Republican calendars). The Bahá'í calendar, prior to 2015 AD, included four epagomenal days, with a fifth intercalary day matching the pattern of intercalation in the Gregorian calendar. Starting in 2015 (172 B.E.), four or five days are set to ensure that the following year will start on the day of the vernal equinox.
Lunisolar calendars.
The solar year does not have a whole number of lunar months (it is about 12.37 lunations), so a lunisolar calendar must have a variable number of months in a year. Regular years have 12 months, but embolismic years insert a 13th "intercalary" or "embolismic" month every second or third year (see blue moon). Whether to insert an intercalary month in a given year may be determined using regular cycles such as the 19-year Metonic cycle (Hebrew calendar and in the determination of Easter) or using calculations of lunar phases (Hindu lunisolar and Chinese calendars). The Buddhist calendar adds both an intercalary day and month on a usually regular cycle.
Islamic calendars.
The tabular Islamic calendar usually has 12 lunar months that alternate between 30 and 29 days every year, but an intercalary day is added to the last month of the year 11 times within a 30-year cycle. Some historians also linked the pre-Islamic practice of Nasi' to intercalation.
The Hijri-Shamsi calendar is based on solar calculations and is similar to the Gregorian calendar in its structure, and hence the intercalation, with the exception that the calendar starts with Hijra.
Leap seconds.
The International Earth Rotation and Reference Systems Service can insert or remove leap seconds from the last day of any month (June and December are preferred). These are sometimes described as intercalary.
Other uses.
ISO 8601 includes a specification for a 52-week year. Any year that has 53 Thursdays has 53 weeks; this extra week may be regarded as intercalary.

</doc>
<doc id="15291" url="http://en.wikipedia.org/wiki?curid=15291" title="Intercourse">
Intercourse

Intercourse may refer to:

</doc>
<doc id="15292" url="http://en.wikipedia.org/wiki?curid=15292" title="Ink">
Ink

Ink is a liquid or paste that contains pigments or dyes and is used to color a surface to produce an image, text, or design. Ink is used for drawing or writing with a pen, brush, or quill. Thicker inks, in paste form, are used extensively in letterpress and lithographic printing.
Ink can be a complex medium, composed of solvents, pigments, dyes, resins, lubricants, solubilizers, surfactants, particulate matter, fluorescents, and other materials. The components of inks serve many purposes; the ink’s carrier, colorants, and other additives affect the flow and thickness of the ink and its appearance when dry.
Types.
Ink formulas vary, but commonly involve four components:
Inks generally fall into four classes:
Colorants.
Pigment inks are used more frequently than dyes because they are more color-fast, but they are also more expensive, less consistent in color, and have less of a color range than dyes.
Pigments.
Pigments are solid, opaque particles suspended in ink to provide color. Pigment molecules typically link together in crystalline structures that are 0.1–2 µm in size and comprise 5–30 percent of the ink volume. Qualities such as hue, saturation, and lightness vary depending on the source and type of pigment.
Dyes.
Dye-based inks are generally much stronger than pigment-based inks and can produce much more color of a given density per unit of mass. However, because dyes are dissolved in the liquid phase, they have a tendency to soak into paper, making the ink less efficient and potentially allowing the ink to bleed at the edges of an image.
To circumvent this problem, dye-based inks are made with solvents that dry rapidly or are used with quick-drying methods of printing, such as blowing hot air on the fresh print. Other methods include harder paper sizing and more specialized paper coatings. The latter is particularly suited to inks used in non-industrial settings (which must conform to tighter toxicity and emission controls), such as inkjet printer inks. Another technique involves coating the paper with a charged coating. If the dye has the opposite charge, it is attracted to and retained by this coating, while the solvent soaks into the paper. Cellulose, the wood-derived material most paper is made of, is naturally charged, and so a compound that complexes with both the dye and the paper's surface aids retention at the surface. Such a compound is commonly used in ink-jet printing inks.
An additional advantage of dye-based ink systems is that the dye molecules can interact with other ink ingredients, potentially allowing greater benefit as compared to pigmented inks from optical brighteners and color-enhancing agents designed to increase the intensity and appearance of dyes.
A more recent development in dye-based inks are dyes that react with cellulose to permanently color the paper. Such inks are not affected by water, alcohol, and other solvents. As such, their use is recommended to prevent frauds that involve removing signatures, such as check washing. This kind of ink is most commonly found in gel inks and in certain fountain pen inks.
History.
 Many ancient cultures around the world have independently discovered and formulated inks for the purposes of writing and drawing. The knowledge of the inks, their recipes and the techniques for their production comes from archaeological analysis or from written text itself.
The history of Chinese inks can be traced back to the 23rd century BC, with the utilization of natural plant (plant dyes), animal, and mineral inks based on such materials as graphite that were ground with water and applied with ink brushes. Evidence for the earliest Chinese inks, similar to modern inksticks, is around 256 BC in the end of the Warring States period and produced from soot and animal glue. The best inks for drawing or painting on paper or silk are produced from the resin of the pine tree. They must be between 50 and 100 years old. The Chinese inkstick is produced with a fish glue, whereas Japanese glue (膠 "nikawa") is from cow or stag.
The process of making India ink was known in China as early as the middle of the 3rd millennium BC, during Neolithic China. India ink was first invented in China, although the source of materials to make the carbon pigment in India ink was later often traded from India, thus the term "India ink" was coined. The traditional Chinese method of making the ink was to grind a mixture of hide glue, carbon black, lampblack, and bone black pigment with a pestle and mortar, then pouring it into a ceramic dish where it could dry. To use the dry mixture, a wet brush would be applied until it reliquified. The manufacture of India ink was well-established by the Cao Wei Dynasty (220–265 AD). Indian documents written in Kharosthi with ink have been unearthed in Chinese Turkestan. The practice of writing with ink and a sharp pointed needle was common in early South India. Several Buddhist and Jain sutras in India were compiled in ink.
In ancient Rome, atramentum was used. In an article for the "Christian Science Monitor", Sharon J. Huntington describes these other historical inks:
About 1,600 years ago, a popular ink recipe was created. The recipe was used for centuries. Iron salts, such as ferrous sulfate (made by treating iron with sulfuric acid), were mixed with tannin from gallnuts (they grow on trees) and a thickener. When first put to paper, this ink is bluish-black. Over time it fades to a dull brown.
Scribes in medieval Europe (about AD 800 to 1500) wrote principally on parchment or vellum. One 12th century ink recipe called for hawthorn branches to be cut in the spring and left to dry. Then the bark was pounded from the branches and soaked in water for eight days. The water was boiled until it thickened and turned black. Wine was added during boiling. The ink was poured into special bags and hung in the sun. Once dried, the mixture was mixed with wine and iron salt over a fire to make the final ink.
The reservoir pen, which may have been the first fountain pen, dates back to 953, when Ma'ād al-Mu'izz, the caliph of Egypt, demanded a pen that would not stain his hands or clothes, and was provided with a pen that held ink in a reservoir.
In the 15th century, a new type of ink had to be developed in Europe for the printing press by Johannes Gutenberg. According to Martyn Lyons in his book "Books: A Living History", Gutenberg’s dye was indelible, oil-based, and made from the soot of lamps (lamp-black) mixed with varnish and egg white. Two types of ink were prevalent at the time: the Greek and Roman writing ink (soot, glue, and water) and the 12th century variety composed of ferrous sulfate, gall, gum, and water. Neither of these handwriting inks could adhere to printing surfaces without creating blurs. Eventually an oily, varnish-like ink made of soot, turpentine, and walnut oil was created specifically for the printing press.
In 2011 worldwide consumption of printing inks generated revenues of more than 20 billion US-dollars. Demand by traditional print media is shrinking, on the other hand more and more printing inks are consumed for packagings.
Health and environmental aspects.
There is a misconception that ink is non-toxic even if swallowed. Once ingested, ink can be hazardous to one's health. Certain inks, such as those used in digital printers, and even those found in a common pen can be harmful. Though ink does not easily cause death, inappropriate contact can cause effects such as severe headaches, skin irritation, or nervous system damage. These effects can be caused by solvents, or by pigment ingredients such as p-Anisidine, which helps create some inks' color and shine.
Three main environmental issues with ink are:
Some regulatory bodies have set standards for the amount of heavy metals in ink. There is a trend toward vegetable oils rather than petroleum oils in recent years in response to a demand for better environmental sustainability.
Writing and preservation.
The two most used black writing inks in history are carbon inks and iron gall inks. Both types create problems for preservationists.
Carbon.
Carbon inks were commonly made from lampblack or soot and a binding agent such as gum arabic or animal glue. The binding agent keeps the carbon particles in suspension and adhered to paper. The carbon particles do not fade over time even when in sunlight or when bleached. One benefit of carbon ink is that it is not harmful to the paper. Over time, the ink is chemically stable and therefore does not threaten the strength of the paper. Despite these benefits, carbon ink is not ideal for permanence and ease of preservation. Carbon ink has a tendency to smudge in humid environments and can be washed off a surface. The best method of preserving a document written in carbon ink is to ensure it is stored in a dry environment (Barrow 1972).
Recently, carbon inks made from carbon nanotubes have been successfully created. They are similar in composition to the traditional inks in that they use a polymer to suspend the carbon nanotubes. These inks can be used in inkjet printers and produce electrically conductive patterns.
Iron gall.
Iron gall inks became prominent in the early 12th century; they were used for centuries and were widely thought to be the best type of ink. However, iron gall ink is corrosive and damages the paper it is on (Waters 1940). Items containing this ink can become brittle and the writing fades to brown. The original scores of Johann Sebastian Bach are threatened by the destructive properties of iron gall ink. The majority of his works are held by the German State Library, and about 25% of those are in advanced stages of decay (American Libraries 2000). The rate at which the writing fades is based on several factors, such as proportions of ink ingredients, amount deposited on the paper, and paper composition (Barrow 1972:16). Corrosion is caused by acid catalysed hydrolysis and iron(II)-catalysed oxidation of cellulose (Rouchon-Quillet 2004:389).
Treatment is a controversial subject. No treatment undoes damage already caused by acidic ink. Deterioration can only be stopped or slowed. Some think it best not to treat the item at all for fear of the consequences. Others believe that non-aqueous procedures are the best solution. Yet others think an aqueous procedure may preserve items written with iron gall ink. Aqueous treatments include distilled water at different temperatures, calcium hydroxide, calcium bicarbonate, magnesium carbonate, magnesium bicarbonate, and calcium phytate. There are many possible side effects from these treatments. There can be mechanical damage, which further weakens the paper. Paper color or ink color may change, and ink may bleed. Other consequences of aqueous treatment are a change of ink texture or formation of plaque on the surface of the ink (Reibland & de Groot 1999).
Iron gall inks require storage in a stable environment, because fluctuating relative humidity increases the rate that formic acid, acetic acid, and furan derivatives form in the material the ink was used on. Sulfuric acid acts as a catalyst to cellulose hydrolysis, and iron (II) sulfate acts as a catalyst to cellulose oxidation. These chemical reactions physically weaken the paper, causing brittleness.
Indelible ink.
"Indelible" means "un-removable". Some types of indelible ink have a very short shelf life because of the quickly evaporating solvents used. India, Mexico, Indonesia, Malaysia and other developing countries have used indelible ink in the form of electoral stain to prevent electoral fraud. The Election Commission in India has used indelible ink for many elections. Indonesia used it in their last election in Aceh. In Mali, the ink is applied to the fingernail. Indelible ink itself is not infallible as it can be used to commit electoral fraud by marking opponent party members before they have chances to cast their votes. There are also reports of "indelible" ink washing off voters' fingers.

</doc>
<doc id="15294" url="http://en.wikipedia.org/wiki?curid=15294" title="Islamabad Capital Territory">
Islamabad Capital Territory

The Islamabad Capital Territory (ICT) (Urdu: وفاقی دارالحکومت‎) is one of the two federal territories of Pakistan. It includes Islamabad, the capital city of Pakistan, and covers an area of 1,165.5 km² (450 mi²) of which 906 km² (349.8 mi²) is Islamabad proper. It is represented in the National Assembly by two constituencies, namely NA-48 and NA-49.
History.
The land was acquired from Khyber Pakhtunkhwa and Punjab in 1960, for the purpose of establishing Pakistan's new capital. According to the 1960 master plan, the ICT included the city of Rawalpindi, and was to be composed of the following parts:
However, the city of Rawalpindi was eventually excluded from the ICT on its creation in the 1980s. The remainder of the territory is now subdivided into five zones, with Zone I designated to house all residential, industrial and government institutions. 
Punjab is located to the south of the ICT, and Khyber Pakhtunkhwa is located to the north west.
Introduction.
Islamabad was designed and built to be a modern capital for Pakistan. It was established in 1960, on the orders of then President General Ayub Khan.
Islamabad nestles against the backdrop of the Margalla Hills at the northern end of Potohar Plateau. Its climate is healthy, relatively pollution free, plentiful in water resources and lush green. It is a modern and carefully planned residential city with wide roads and avenues, many public buildings and well-organised bazaars, markets, and shopping centres.
Government.
Islamabad is situated between the province of Khyber Pakhtunkhwa and Punjab but Islamabad is not a part of any province. The federal Government of Pakistan controls it and is one of the two federal Territories of Pakistan (the other being FATA) which is directly governed by the Federal Government of Pakistan.
Union Councils.
Islamabad is administratively divided into two segments, namely Islamabad Urban and Islamabad rural. The rural area is further divided into twelve union councils, comprising some 133 villages. The table below lists the 12 union councils, each union council is named after the main town (e.g. Rewat or Tarnol), and shows the areas under its jurisdiction.
Area and population.
The city is divided into eight basic zone types: Administrative zone, Diplomatic Enclave zone, Residential Areas, Educational Sectors, Industrial Sectors, Commercial Areas, Rural Areas, Green Areas.
Each sector has its own shopping area, a green belt (which goes across the whole sector in a straight line) and public park. The population of the city is around 950,000 people of which 66% is urban. It has an area of about 910 square kilometres. The city lies at latitudes 33° 49' north and longitudes 72° 24' east with altitudes ranging from 457 to 610 meters.
Tribes.
While urban Islamabad is home to people from all over Pakistan as well as expatriates, in the rural areas a number of Pothohari speaking tribal communities can still be recognized. The main ones are:
Dhanial.
They are one of the largest tribes living in the areas on the Potohar plateau and Lower Himalayas. This tribe traces their lineage to Ali ibn Abi-Talib. Most of the Dhanyals are settled in the cities of Rawalpindi and Islamabad and in the Muree Hills. Other branches of the tribe live in Azad Kashmir, Abbottabad, Sialkot and Hazara. Their main villages in the Capital Territory are Tumair Chirah Pehont Kirpa Pind Begwal and Merabegwal.
Abbasi.
Abbasi inhibits in almost all the villages of Islamabad. But they are majority in bhara koh, Pindorian, Tarlai, Ali pur.
Mir.
Mir clan,basically from Occupied Kashmir also resides here from last many years in different parts of the city.
Awan.
Awans around the town of Golra Sharif,village Malpur Islamabad and the village Sohan. Some are also found in the villages of Tarlai Kalan and Malot, as well as along the border with Khyber-Pakhtunkhwa.
Dhamial.
Dhamial rajputs are one of the largest tribes of Pothohar. There ancestor Sultan Dhami Khan was ruler of Pothohar and they originate from Kot Dhamiak. They also claim to be the bloodline of Hazrat Yousaf(AS). There are about 3 villages in capital Islamabad like Sher Dhamial, Hun Dhamial, Saab Dhamial and village Dhamial etc.
Mughal.
The Mughal are another large tribe, who claim Mughal ancestry as descendents of various Central Asian Turco-Mongol armies that invaded Iran and South Asia such as those of Genghis Khan, Timur, Babur and beyond.
Gakhar or Kayanis.
The region is home a large community of Gakhar or Kayanis, who at one time were rulers of the region. Rawalpindi had been the capital of the Gakhars. The last Gakhar ruler Sultan Muqrab Khan was defeated by the Sikhs. Ghakhar villages include Malpur, Rehara, Chatta (kund rajgan), Malot, Seevra, Bharakau, New Parian, New Malper,Chuchkal and Dodocha.
Gujar.
The Gujar are found throughout rural ICT. The largest Gujar settlement was the village of Dhok Chaudhary Jevan, which now part of Sector E 7. Most Gujars are now living in Chaudhary Umar Abad. Other Gujar villages include Ahdi Paswal, Turnol and Talhar.
Jat.
The Jat can be found in the Union Council Kuri in the village of Mohrian and Tarlai Kalan where the Thathal clan, and Chatha clan populate the village of Bakhtawar Chatha. Other Jat villages include Thandapani, Nilore, Tumair, Darkalla and Alipur.
Rajputs.
The ICT territory is home a large community of Rajputs, who once were rulers of the region. Among the clans of Rajputs, the Thathal or [THOTHAL] are found in the village of Tarlai KalanPhulgran Harno Thanda Pani and Mohrian. The Matyal can be encountered in Gagri, Union Council Sihala, while the Janjuas are found in Union Council Bhara Kahu, jagiot, chanol. Other Rajput villages include Bhangreel Kalan, Bhangreel Khurd, Kortara, Takht Pari, Shadi Dhamial, Mohra Amir, Sood Gangal, Mohri Khumbal, Hoon Dhamial,[Khadrapar],Chaper Mir Khanal] Hreno Thanda Pani, Thamir, bangyal and Bhima Kanait.
Clans include the Minhas, [Thathal] Dhamial, Bangial, Ranial, Chohan, Bhains, Baghial, Khumbal, Gangal, Bashan, Janjua, Nagiyal Rajput and Hon Rajputs.
Climate.
The average humidity level is 55%, with an average rainfall of 1450 millimeters each year. The maximum average temperature is 29 °C and the minimum average temperature attained here during the year is generally around 11 °C. Reminiscent of tropical climate, Islamabad retains mild winters and has never recorded snowfall.
Education.
Islamabad has the highest literacy rate of Pakistan at 85%. and also has some of Pakistan's major universities, including Quaid-i-Azam University, the International Islamic University, and the National University of Sciences and Technology.
Private School Network Islamabad is working for Private Educational Institutions. The president of PSN is Dr.Muhammad Afzal Babur from Bhara Kahu. PSN is divided in 8 zone in islamabad,In Tarlai Zone Chaudhary Faisal Ali from Faisal Academy Tarlai Kalan is Zonal General Sectary of PSN.]].
Quaid-e-Azam University has several faculties. The institute is located in a semi-hilly area, east of the Secretariat buildings and near the base of Margalla Hills. This Post-Graduate institute is spread over 1,705 acre. The nucleus of the campus has been designed as an axial spine with a library as its center.
Other universities include the following:
External links.
 travel guide from Wikivoyage

</doc>
<doc id="15295" url="http://en.wikipedia.org/wiki?curid=15295" title="Intelligent design">
Intelligent design

Intelligent design (ID) is the pseudoscientific view that "certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection." Educators, philosophers, and the scientific community have demonstrated that ID is a religious argument, a form of creationism which lacks empirical support and offers no testable or tenable hypotheses. Proponents argue that it is "an evidence-based scientific theory about life's origins" that challenges the methodological naturalism inherent in modern science, while conceding that they have yet to produce a scientific theory. The leading proponents of ID are associated with the Discovery Institute, a politically conservative think tank based in the United States. Although they state that ID is not creationism and deliberately avoid assigning a personality to the designer, many of these proponents express belief that the designer is the Christian deity.
ID presents negative arguments against evolutionary explanations, and its positive argument is an analogy between natural systems and human artifacts, a version of the theological argument from design for the existence of God. Both irreducible complexity and specified complexity present detailed negative assertions that certain features (biological and informational, respectively) are too complex to be the result of natural processes. Proponents then conclude by analogy that these features are evidence of design. Detailed scientific examination has rebutted the claims that evolutionary explanations are inadequate, and this premise of intelligent design—that evidence against evolution constitutes evidence for design—has been criticized as a false dichotomy.
Though the phrase "intelligent design" had featured previously in theological discussions of the design argument, the first publication of the term "intelligent design" in its present use as an alternative term for creationism was in "Of Pandas and People", a 1989 textbook intended for high school biology classes. The term was substituted into drafts of the book after the 1987 United States Supreme Court's "Edwards v. Aguillard" decision, which barred the teaching of "creation science" in public schools on constitutional grounds. From the mid-1990s, the intelligent design movement (IDM), supported by the Discovery Institute, advocated inclusion of intelligent design in public school biology curricula. This led to the 2005 "Kitzmiller v. Dover Area School District" trial in which U.S. District Judge John E. Jones III ruled that intelligent design is not science, that it "cannot uncouple itself from its creationist, and thus religious, antecedents," and that the school district's promotion of it therefore violated the Establishment Clause of the First Amendment to the United States Constitution, often described as the "wall of separation between church and state".
History.
Origin of the concept.
By 1910 evolution was not a topic of major religious controversy in America, but in the 1920s the Fundamentalist–Modernist Controversy in theology resulted in Fundamentalist Christian opposition to teaching evolution, and the origins of modern creationism. Teaching of evolution was effectively suspended in U.S. public schools until the 1960s, and when evolution was then reintroduced into the curriculum, there was a series of court cases in which attempts were made to get creationism taught alongside evolution in science classes. Young Earth creationists (YEC) promoted creation science as "an alternative scientific explanation of the world in which we live." This frequently invoked the argument from design to explain complexity in nature as demonstrating the existence of God.
The argument from design, the teleological argument or "argument from intelligent design," has been advanced in theology for centuries. It can be summarised briefly as "Wherever complex design exists, there must have been a designer; nature is complex; therefore nature must have had an intelligent designer." Thomas Aquinas presented it in his fifth proof of God's existence as a syllogism. In 1802, William Paley's "Natural Theology" presented examples of intricate purpose in organisms. His version of the watchmaker analogy argued that, in the same way that a watch has evidently been designed by a craftsman, complexity and adaptation seen in nature must have been designed, and the perfection and diversity of these designs shows the designer to be omnipotent, the Christian God. Like creation science, intelligent design centers on Paley's religious argument from design, but while Paley's natural theology was open to deistic design through God-given laws, intelligent design seeks scientific confirmation of repeated miraculous interventions in the history of life. Creation science prefigured the intelligent design arguments of irreducible complexity, even featuring the bacterial flagellum. In the United States, attempts to introduce creation science in schools led to court rulings that it is religious in nature, and thus cannot be taught in public school science classrooms. Intelligent design is also presented as science, and shares other arguments with creation science but avoids literal Biblical references to such things as the Flood story from the Book of Genesis or using Bible verses to age the Earth.
Barbara Forrest writes that the intelligent design movement began in 1984 with the book "The Mystery of Life's Origin: Reassessing Current Theories", co-written by creationist Charles B. Thaxton, a chemist, with two other authors, and published by Jon A. Buell's Foundation for Thought and Ethics. Thaxton held a conference in 1988, "Sources of Information Content in DNA," which attracted creationists such as Stephen C. Meyer.
In March 1986, a review by Meyer used information theory to suggest that messages transmitted by DNA in the cell show "specified complexity" specified by intelligence, and must have originated with an intelligent agent. In November of that year, Thaxton described his reasoning as a more sophisticated form of Paley's argument from design. At the "Sources of Information Content in DNA" conference in 1988, he said that his intelligent cause view was compatible with both metaphysical naturalism and supernaturalism.
Intelligent design avoids identifying or naming the intelligent designer—it merely states that one (or more) must exist—but leaders of the movement have said the designer is the Christian God. Whether this lack of specificity about the designer's identity in public discussions is a genuine feature of the concept, or just a posture taken to avoid alienating those who would separate religion from the teaching of science, has been a matter of great debate between supporters and critics of intelligent design. The "Kitzmiller v. Dover Area School District" court ruling held the latter to be the case.
Origin of the term.
Since the Middle Ages, discussion of the religious "argument from design" or "teleological argument" in theology, with its concept of "intelligent design," has persistently referred to the theistic Creator God. Although ID proponents chose this provocative label for their proposed alternative to evolutionary explanations, they have de-emphasized their religious antecendents and denied that ID is natural theology, while still presenting ID as supporting the argument for the existence of God.
While intelligent design proponents have pointed out past examples of the phrase "intelligent design" which they said were not creationist and faith-based, they have failed to show that these usages had any influence on those who introduced the label in the intelligent design movement.
Variations on the phrase appeared in YEC publications: a 1967 book co-written by Percival Davis referred to "design according to which basic organisms were created." In 1970, A. E. Wilder-Smith published "The Creation of Life: A Cybernetic Approach to Evolution" which defended Paley's design argument with computer calculations of the improbability of genetic sequences, which he said could not be explained by evolution but required "the abhorred necessity of divine intelligent activity behind nature," and that "the same problem would be expected to beset the relationship between the designer behind nature and the intelligently designed part of nature known as man." In a 1984 article as well as in his affidavit to "Edwards v. Aguillard", Dean H. Kenyon defended creation science by stating that "biomolecular systems require intelligent design and engineering know-how," citing Wilder-Smith. Creationist Richard B. Bliss used the phrase "creative design" in "Origins: Two Models: Evolution, Creation" (1976), and in "Origins: Creation or Evolution" (1988) wrote that "while evolutionists are trying to find non-intelligent ways for life to occur, the creationist insists that an intelligent design must have been there in the first place." The first systematic use of the term, defined in a glossary and claimed to be other than creationism, was in "Of Pandas and People", co-authored by Davis and Kenyon.
"Of Pandas and People".
The most common modern use of the words "intelligent design" as a term intended to describe a field of inquiry began after the United States Supreme Court ruled in 1987 in the case of "Edwards v. Aguillard" that creationism is unconstitutional in public school science curricula.
A Discovery Institute report says that Charles B. Thaxton, editor of "Pandas", had picked the phrase up from a NASA scientist, and thought "That's just what I need, it's a good engineering term." In drafts of the book, over one hundred uses of the root word "creation," such as "creationism" and "Creation Science," were changed, almost without exception, to "intelligent design," while "creationists" was changed to "design proponents" or, in one instance, "cdesign proponentsists" ["sic"]. In June 1988, Thaxton held a conference titled "Sources of Information Content in DNA" in Tacoma, Washington, and in December decided to use the label "intelligent design" for his new creationist movement. Stephen C. Meyer was at the conference, and later recalled that "The term "intelligent design" came up..."
"Of Pandas and People" was published in 1989, and in addition to including all the current arguments for ID, was the first book to make systematic use of the terms "intelligent design" and "design proponents" as well as the phrase "design theory," defining the term "intelligent design" in a glossary and representing it as not being creationism. It thus represents the start of the modern intelligent design movement. "Intelligent design" was the most prominent of around fifteen new terms it introduced as a new lexicon of creationist terminology to oppose evolution without using religious language. It was the first place where the phrase "intelligent design" appeared in its primary present use, as stated both by its publisher Jon A. Buell, and by William A. Dembski in his expert witness report for "Kitzmiller v. Dover Area School District".
The National Center for Science Education (NCSE) has criticized the book for presenting all of the basic arguments of intelligent design proponents and being actively promoted for use in public schools before any research had been done to support these arguments. Although presented as a scientific textbook, philosopher of science Michael Ruse considers the contents "worthless and dishonest." An ACLU lawyer described it as a political tool aimed at students who did not "know science or understand the controversy over evolution and creationism." One of the authors of the science framework used by California schools, Kevin Padian, condemned it for its "sub-text," "intolerance for honest science" and "incompetence."
Concepts.
Irreducible complexity.
The term "irreducible complexity" was introduced by biochemist Michael Behe in his 1996 book "Darwin's Black Box", though he had already described the concept in his contributions to the 1993 revised edition of "Of Pandas and People". Behe defines it as "a single system which is composed of several well-matched interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning."
Behe uses the analogy of a mousetrap to illustrate this concept. A mousetrap consists of several interacting pieces—the base, the catch, the spring and the hammer—all of which must be in place for the mousetrap to work. Removal of any one piece destroys the function of the mousetrap. Intelligent design advocates assert that natural selection could not create irreducibly complex systems, because the selectable function is present only when all parts are assembled. Behe argued that irreducibly complex biological mechanisms include the bacterial flagellum of "E. coli", the blood clotting cascade, cilia, and the adaptive immune system.
Critics point out that the irreducible complexity argument assumes that the necessary parts of a system have always been necessary and therefore could not have been added sequentially. They argue that something that is at first merely advantageous can later become necessary as other components change. Furthermore, they argue, evolution often proceeds by altering preexisting parts or by removing them from a system, rather than by adding them. This is sometimes called the "scaffolding objection" by an analogy with scaffolding, which can support an "irreducibly complex" building until it is complete and able to stand on its own.
Behe has acknowledged using "sloppy prose," and that his "argument against Darwinism does not add up to a logical proof." Irreducible complexity has remained a popular argument among advocates of intelligent design; in the Dover trial, the court held that "Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large."
Specified complexity.
In 1986, Charles B. Thaxton, a physical chemist and creationist, used the term "specified complexity" from information theory when claiming that messages transmitted by DNA in the cell were specified by intelligence, and must have originated with an intelligent agent.
The intelligent design concept of "specified complexity" was developed in the 1990s by mathematician, philosopher, and theologian William A. Dembski. Dembski states that when something exhibits specified complexity (i.e., is both complex and "specified," simultaneously), one can infer that it was produced by an intelligent cause (i.e., that it was designed) rather than being the result of natural processes. He provides the following examples: "A single letter of the alphabet is specified without being complex. A long sentence of random letters is complex without being specified. A Shakespearean sonnet is both complex and specified." He states that details of living things can be similarly characterized, especially the "patterns" of molecular sequences in functional biological molecules such as DNA.
Dembski defines complex specified information (CSI) as anything with a less than 1 in 10150 chance of occurring by (natural) chance. Critics say that this renders the argument a tautology: complex specified information cannot occur naturally because Dembski has defined it thus, so the real question becomes whether or not CSI actually exists in nature.
The conceptual soundness of Dembski's specified complexity/CSI argument has been discredited in the scientific and mathematical communities. Specified complexity has yet to be shown to have wide applications in other fields, as Dembski asserts. John Wilkins and Wesley R. Elsberry characterize Dembski's "explanatory filter" as "eliminative" because it eliminates explanations sequentially: first regularity, then chance, finally defaulting to design. They argue that this procedure is flawed as a model for scientific inference because the asymmetric way it treats the different possible explanations renders it prone to making false conclusions.
Richard Dawkins, another critic of intelligent design, argues in "The God Delusion" (2006) that allowing for an intelligent designer to account for unlikely complexity only postpones the problem, as such a designer would need to be at least as complex. Other scientists have argued that evolution through selection is better able to explain the observed complexity, as is evident from the use of selective evolution to design certain electronic, aeronautic and automotive systems that are considered problems too complex for human "intelligent designers."
Fine-tuned Universe.
Intelligent design proponents have also occasionally appealed to broader teleological arguments outside of biology, most notably an argument based on the fine-tuning of universal constants that make matter and life possible and which are argued not to be solely attributable to chance. These include the values of fundamental physical constants, the relative strength of nuclear forces, electromagnetism, and gravity between fundamental particles, as well as the ratios of masses of such particles. Intelligent design proponent and Center for Science and Culture fellow Guillermo Gonzalez argues that if any of these values were even slightly different, the universe would be dramatically different, making it impossible for many chemical elements and features of the Universe, such as galaxies, to form. Thus, proponents argue, an intelligent designer of life was needed to ensure that the requisite features were present to achieve that particular outcome.
Scientists have generally responded that these arguments are poorly supported by existing evidence. Victor J. Stenger and other critics say both intelligent design and the weak form of the anthropic principle are essentially a tautology; in his view, these arguments amount to the claim that life is able to exist because the Universe is able to support life. The claim of the improbability of a life-supporting universe has also been criticized as an argument by lack of imagination for assuming no other forms of life are possible. Life as we know it might not exist if things were different, but a different sort of life might exist in its place. A number of critics also suggest that many of the stated variables appear to be interconnected and that calculations made by mathematicians and physicists suggest that the emergence of a universe similar to ours is quite probable.
Intelligent designer.
The contemporary intelligent design movement formulates its arguments in secular terms and intentionally avoids identifying the intelligent agent (or agents) they posit. Although they do not state that God is the designer, the designer is often implicitly hypothesized to have intervened in a way that only a god could intervene. Dembski, in "The Design Inference" (1998), speculates that an alien culture could fulfill these requirements. "Of Pandas and People" proposes that SETI illustrates an appeal to intelligent design in science. In 2000, philosopher of science Robert T. Pennock suggested the Raëlian UFO religion as a real-life example of an extraterrestrial intelligent designer view that "make[s] many of the same bad arguments against evolutionary theory as creationists". The authoritative description of intelligent design, however, explicitly states that the "Universe" displays features of having been designed. Acknowledging the paradox, Dembski concludes that "no intelligent agent who is strictly physical could have presided over the origin of the universe or the origin of life." The leading proponents have made statements to their supporters that they believe the designer to be the Christian God, to the exclusion of all other religions.
Beyond the debate over whether intelligent design is scientific, a number of critics argue that existing evidence makes the design hypothesis appear unlikely, irrespective of its status in the world of science. For example, Jerry Coyne asks why a designer would "give us a pathway for making vitamin C, but then destroy it by disabling one of its enzymes" (see pseudogene) and why a designer would not "stock oceanic islands with reptiles, mammals, amphibians, and freshwater fish, despite the suitability of such islands for these species". Coyne also points to the fact that "the flora and fauna on those islands resemble that of the nearest mainland, even when the environments are very different" as evidence that species were not placed there by a designer. Previously, in "Darwin's Black Box", Behe had argued that we are simply incapable of understanding the designer's motives, so such questions cannot be answered definitively. Odd designs could, for example, "...have been placed there by the designer for a reason—for artistic reasons, for variety, to show off, for some as-yet-undetected practical purpose, or for some unguessable reason—or they might not." Coyne responds that in light of the evidence, "either life resulted not from intelligent design, but from evolution; or the intelligent designer is a cosmic prankster who designed everything to make it look as though it had evolved."
Intelligent design proponents such as Paul Nelson avoid the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design. Behe cites Paley as his inspiration, but he differs from Paley's expectation of a perfect Creation and proposes that designers do not necessarily produce the best design they can. Behe suggests that, like a parent not wanting to spoil a child with extravagant toys, the designer can have multiple motives for not giving priority to excellence in engineering. He says that "Another problem with the argument from imperfection is that it critically depends on a psychoanalysis of the unidentified designer. Yet the reasons that a designer would or would not do anything are virtually impossible to know unless the designer tells you specifically what those reasons are." This reliance on inexplicable motives of the designer makes intelligent design scientifically untestable. Retired UC Berkeley law professor, author and intelligent design advocate Phillip E. Johnson puts forward a core definition that the designer creates for a purpose, giving the example that in his view AIDS was created to punish immorality and was not caused by HIV, but such motives cannot be tested by scientific methods.
Asserting the need for a designer of complexity also raises the question "What designed the designer?" Intelligent design proponents say that the question is irrelevant to or outside the scope of intelligent design. Richard Wein counters that "...scientific explanations often create new unanswered questions. But, in assessing the value of an explanation, these questions are not irrelevant. They must be balanced against the improvements in our understanding which the explanation provides. Invoking an unexplained being to explain the origin of other beings (ourselves) is little more than question-begging. The new question raised by the explanation is as problematic as the question which the explanation purports to answer." Richard Dawkins sees the assertion that the designer does not need to be explained as a thought-terminating cliché. In the absence of observable, measurable evidence, the very question "What designed the designer?" leads to an infinite regression from which intelligent design proponents can only escape by resorting to religious creationism or logical contradiction.
Movement.
The intelligent design movement is a direct outgrowth of the creationism of the 1980s. The scientific and academic communities, along with a U.S. federal court, view intelligent design as either a form of creationism or as a direct descendant that is closely intertwined with traditional creationism; and several authors explicitly refer to it as "intelligent design creationism."
The movement is headquartered in the Center for Science and Culture, established in 1996 as the creationist wing of the Discovery Institute to promote a religious agenda calling for broad social, academic and political changes. The Discovery Institute's intelligent design campaigns have been staged primarily in the United States, although efforts have been made in other countries to promote intelligent design. Leaders of the movement say intelligent design exposes the limitations of scientific orthodoxy and of the secular philosophy of naturalism. Intelligent design proponents allege that science should not be limited to naturalism and should not demand the adoption of a naturalistic philosophy that dismisses out-of-hand any explanation that includes a supernatural cause. The overall goal of the movement is to "reverse the stifling dominance of the materialist worldview" represented by the theory of evolution in favor of "a science consonant with Christian and theistic convictions."
Phillip E. Johnson stated that the goal of intelligent design is to cast creationism as a scientific concept. All leading intelligent design proponents are fellows or staff of the Discovery Institute and its Center for Science and Culture. Nearly all intelligent design concepts and the associated movement are the products of the Discovery Institute, which guides the movement and follows its wedge strategy while conducting its "Teach the Controversy" campaign and their other related programs.
Leading intelligent design proponents have made conflicting statements regarding intelligent design. In statements directed at the general public, they say intelligent design is not religious; when addressing conservative Christian supporters, they state that intelligent design has its foundation in the Bible. Recognizing the need for support, the Institute affirms its Christian, evangelistic orientation:
 Alongside a focus on influential opinion-makers, we also seek to build up a popular base of support among our natural constituency, namely, Christians. We will do this primarily through apologetics seminars. We intend these to encourage and equip believers with new scientific evidences that support the faith, as well as to "popularize" our ideas in the broader culture.
Barbara Forrest, an expert who has written extensively on the movement, describes this as being due to the Discovery Institute's obfuscating its agenda as a matter of policy. She has written that the movement's "activities betray an aggressive, systematic agenda for promoting not only intelligent design creationism, but the religious worldview that undergirds it."
Religion and leading proponents.
Although arguments for intelligent design by the intelligent design movement are formulated in secular terms and intentionally avoid positing the identity of the designer, the majority of principal intelligent design advocates are publicly religious Christians who have stated that, in their view, the designer proposed in intelligent design is the Christian conception of God. Stuart Burgess, Phillip E. Johnson, William A. Dembski, and Stephen C. Meyer are evangelical Protestants; Michael Behe is a Roman Catholic; and Jonathan Wells is a member of the Unification Church. Non-Christian proponents include David Klinghoffer, who is Jewish, Michael Denton and David Berlinski, who are agnostic, and Muzaffar Iqbal, a Pakistani-Canadian Muslim. Phillip E. Johnson has stated that cultivating ambiguity by employing secular language in arguments that are carefully crafted to avoid overtones of theistic creationism is a necessary first step for ultimately reintroducing the Christian concept of God as the designer. Johnson explicitly calls for intelligent design proponents to obfuscate their religious motivations so as to avoid having intelligent design identified "as just another way of packaging the Christian evangelical message." Johnson emphasizes that "...the first thing that has to be done is to get the Bible out of the discussion. ...This is not to say that the biblical issues are unimportant; the point is rather that the time to address them will be after we have separated materialist prejudice from scientific fact."
The strategy of deliberately disguising the religious intent of intelligent design has been described by William A. Dembski in "The Design Inference". In this work, Dembski lists a god or an "alien life force" as two possible options for the identity of the designer; however, in his book "Intelligent Design: The Bridge Between Science and Theology" (1999), Dembski states:
 Christ is indispensable to any scientific theory, even if its practitioners don't have a clue about him. The pragmatics of a scientific theory can, to be sure, be pursued without recourse to Christ. But the conceptual soundness of the theory can in the end only be located in Christ.
Dembski also stated, "ID is part of God's general revelation [...] Not only does intelligent design rid us of this ideology [ materialism ], which suffocates the human spirit, but, in my personal experience, I've found that it opens the path for people to come to Christ." Both Johnson and Dembski cite the Bible's Gospel of John as the foundation of intelligent design.
Barbara Forrest contends such statements reveal that leading proponents see intelligent design as essentially religious in nature, not merely a scientific concept that has implications with which their personal religious beliefs happen to coincide. She writes that the leading proponents of intelligent design are closely allied with the ultra-conservative Christian Reconstructionism movement. She lists connections of (current and former) Discovery Institute Fellows Phillip E. Johnson, Charles B. Thaxton, Michael Behe, Richard Weikart, Jonathan Wells and Francis J. Beckwith to leading Christian Reconstructionist organizations, and the extent of the funding provided the Institute by Howard Ahmanson, Jr., a leading figure in the Reconstructionist movement.
Reaction from other creationist groups.
Not all creationist organizations have embraced the intelligent design movement. According to Thomas Dixon, "Religious leaders have come out against ID too. An open letter affirming the compatibility of Christian faith and the teaching of evolution, first produced in response to controversies in Wisconsin in 2004, has now been signed by over ten thousand clergy from different Christian denominations across America. In 2006, the director of the Vatican Observatory, the Jesuit astronomer George Coyne, condemned ID as a kind of 'crude creationism' which reduced God to a mere engineer." Hugh Ross of Reasons to Believe, a proponent of Old Earth creationism, believes that the efforts of intelligent design proponents to divorce the concept from Biblical Christianity make its hypothesis too vague. In 2002, he wrote: "Winning the argument for design without identifying the designer yields, at best, a sketchy origins model. Such a model makes little if any positive impact on the community of scientists and other scholars. [...] ...the time is right for a direct approach, a single leap into the origins fray. Introducing a biblically based, scientifically verifiable creation model represents such a leap."
Likewise, two of the most prominent YEC organizations in the world have attempted to distinguish their views from those of the intelligent design movement. Henry M. Morris of the Institute for Creation Research (ICR) wrote, in 1999, that ID, "even if well-meaning and effectively articulated, will not work! It has often been tried in the past and has failed, and it will fail today. The reason it won't work is because it is not the Biblical method." According to Morris: "The evidence of intelligent design… must be either followed by or accompanied by a sound presentation of true Biblical creationism if it is to be meaningful and lasting." In 2002, Carl Wieland, then of Answers in Genesis (AiG), criticized design advocates who, though well-intentioned, "'left the Bible out of it'" and thereby unwittingly aided and abetted the modern rejection of the Bible. Wieland explained that "AiG's major 'strategy' is to boldly, but humbly, call the church back to its Biblical foundations… [so] we neither count ourselves a part of this movement nor campaign against it."
Reaction from the scientific community.
The unequivocal consensus in the scientific community is that intelligent design is not science and has no place in a science curriculum. The U.S. National Academy of Sciences has stated that "creationism, intelligent design, and other claims of supernatural intervention in the origin of life or of species are not science because they are not testable by the methods of science." The U.S. National Science Teachers Association and the American Association for the Advancement of Science have termed it pseudoscience. Others in the scientific community have denounced its tactics, accusing the ID movement of manufacturing false attacks against evolution, of engaging in misinformation and misrepresentation about science, and marginalizing those who teach it. More recently, in September 2012, Bill Nye warned that creationist views threaten science education and innovations in the United States.
In 2001, the Discovery Institute published advertisements under the heading "A Scientific Dissent From Darwinism", with the claim that listed scientists had signed this statement expressing skepticism:
 We are skeptical of claims for the ability of random mutation and natural selection to account for the complexity of life. Careful examination of the evidence for Darwinian theory should be encouraged.
The ambiguous statement did not exclude other known evolutionary mechanisms, and most signatories were not scientists in relevant fields, but starting in 2004 the Institute claimed the increasing number of signatures indicated mounting doubts about evolution among scientists. The statement formed a key component of Discovery Institute campaigns to present intelligent design as scientifically valid by claiming that evolution lacks broad scientific support, with Institute members continued to cite the list through at least 2011. As part of a strategy to counter these claims, scientists organised Project Steve which gained more signatories named Steve (or variants) than the Institute's petition, and a counter-petition, "A Scientific Support for Darwinism", which quickly gained similar numbers of signatories.
Polls.
Several surveys were conducted prior to the December 2005 decision in "Kitzmiller v. Dover School District", which sought to determine the level of support for intelligent design among certain groups. According to a 2005 Harris poll, 10% of adults in the United States viewed human beings as "so complex that they required a powerful force or intelligent being to help create them." Although Zogby polls commissioned by the Discovery Institute show more support, these polls suffer from considerable flaws, such as having a very low response rate (248 out of 16,000), being conducted on behalf of an organization with an expressed interest in the outcome of the poll, and containing leading questions.
A series of Gallup polls in the United States from 1982 through 2008 on "Evolution, Creationism, Intelligent Design" found support for "human beings have developed over millions of years from less advanced formed of life, but God guided the process" of between 35% and 40%, support for "God created human beings in pretty much their present form at one time within the last 10,000 years or so" varied from 43% to 47%, and support for "human beings have developed over millions of years from less advanced forms of life, but God had no part in the process" varied from 9% to 14%. The polls also noted answers to a series of more detailed questions.
Allegations of discrimination against ID proponents.
There have been allegations that ID proponents have met discrimination, such as being refused tenure or being harshly criticized on the Internet. In the documentary film "", released in 2008, host Ben Stein presents five such cases. The film contends that the mainstream science establishment, in a "scientific conspiracy to keep God out of the nation's laboratories and classrooms," suppresses academics who believe they see evidence of intelligent design in nature or criticize evidence of evolution. Investigation into these allegations turned up alternative explanations for perceived persecution.
The film portrays intelligent design as motivated by science, rather than religion, though it does not give a detailed definition of the phrase or attempt to explain it on a scientific level. Other than briefly addressing issues of irreducible complexity, "Expelled" examines it as a political issue. The scientific theory of evolution is portrayed by the film as contributing to fascism, the Holocaust, communism, atheism, and eugenics.
"Expelled" has been used in private screenings to legislators as part of the Discovery Institute intelligent design campaign for Academic Freedom bills. Review screenings were restricted to churches and Christian groups, and at a special pre-release showing, one of the interviewees, PZ Myers, was refused admission. The American Association for the Advancement of Science describes the film as dishonest and divisive propaganda aimed at introducing religious ideas into public school science classrooms, and the Anti-Defamation League has denounced the film's allegation that evolutionary theory influenced the Holocaust. The film includes interviews with scientists and academics who were misled into taking part by misrepresentation of the topic and title of the film. Skeptic Michael Shermer describes his experience of being repeatedly asked the same question without context as "surreal."
Criticism.
Scientific criticism.
Advocates of intelligent design seek to keep God and the Bible out of the discussion, and present intelligent design in the language of science as though it were a scientific hypothesis. For a theory to qualify as scientific, it is expected to be:
For any theory, hypothesis or conjecture to be considered scientific, it must meet most, and ideally all, of these criteria. The fewer criteria are met, the less scientific it is; and if it meets only a few or none at all, then it cannot be treated as scientific in any meaningful sense of the word. Typical objections to defining intelligent design as science are that it lacks consistency, violates the principle of parsimony, is not scientifically useful, is not falsifiable, is not empirically testable, and is not correctable, dynamic, progressive or provisional.
Intelligent design proponents seek to change this fundamental basis of science by eliminating "methodological naturalism" from science and replacing it with what the leader of the intelligent design movement, Phillip E. Johnson, calls "theistic realism." Intelligent design proponents argue that naturalistic explanations fail to explain certain phenomena and that supernatural explanations provide a very simple and intuitive explanation for the origins of life and the universe. Many intelligent design followers believe that "scientism" is itself a religion that promotes secularism and materialism in an attempt to erase theism from public life, and they view their work in the promotion of intelligent design as a way to return religion to a central role in education and other public spheres.
The failure to follow the procedures of scientific discourse and the failure to submit work to the scientific community that withstands scrutiny have weighed against intelligent design being accepted as valid science. The intelligent design movement has not published a properly peer-reviewed article supporting ID in a scientific journal, and has failed to publish supporting peer-reviewed research or data. The only article published in a peer-reviewed scientific journal that made a case for intelligent design was quickly withdrawn by the publisher for having circumvented the journal's peer-review standards. The Discovery Institute says that a number of intelligent design articles have been published in peer-reviewed journals, but critics, largely members of the scientific community, reject this claim and state intelligent design proponents have set up their own journals with peer review that lack impartiality and rigor, consisting entirely of intelligent design supporters.
Further criticism stems from the fact that the phrase "intelligent" design makes use of an assumption of the quality of an observable intelligence, a concept that has no scientific consensus definition. The characteristics of intelligence are assumed by intelligent design proponents to be observable without specifying what the criteria for the measurement of intelligence should be. Critics say that the design detection methods proposed by intelligent design proponents are radically different from conventional design detection, undermining the key elements that make it possible as legitimate science. Intelligent design proponents, they say, are proposing both searching for a designer without knowing anything about that designer's abilities, parameters, or intentions (which scientists do know when searching for the results of human intelligence), as well as denying the very distinction between natural/artificial design that allows scientists to compare complex designed artifacts against the background of the sorts of complexity found in nature.
Among a significant proportion of the general public in the United States, the major concern is whether conventional evolutionary biology is compatible with belief in God and in the Bible, and how this issue is taught in schools. The Discovery Institute's "Teach the Controversy" campaign promotes intelligent design while attempting to discredit evolution in United States public high school science courses. The scientific community and science education organizations have replied that there is no scientific controversy regarding the validity of evolution and that the controversy exists solely in terms of religion and politics.
Arguments from ignorance.
Eugenie C. Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent design proponents are arguments from ignorance.
In the argument from ignorance, a lack of evidence for one view is erroneously argued to constitute proof of the correctness of another view. Scott and Branch say that intelligent design is an argument from ignorance because it relies on a lack of knowledge for its conclusion: lacking a natural explanation for certain specific aspects of evolution, we assume intelligent cause. They contend most scientists would reply that the unexplained is not unexplainable, and that "we don't know yet" is a more appropriate response than invoking a cause outside science. Particularly, Michael Behe's demands for ever more detailed explanations of the historical evolution of molecular systems seem to assume a false dichotomy, where either evolution or design is the proper explanation, and any perceived failure of evolution becomes a victory for design. Scott and Branch also contend that the supposedly novel contributions proposed by intelligent design proponents have not served as the basis for any productive scientific research.
In his conclusion to the Kitzmiller trial, Judge John E. Jones III wrote that "ID is at bottom premised upon a false dichotomy, namely, that to the extent evolutionary theory is discredited, ID is confirmed." This same argument had been put forward to support creation science at the "McLean v. Arkansas" (1982) trial which found it was "contrived dualism," the false premise of a "two model approach." Behe's argument of irreducible complexity puts forward negative arguments against evolution but does not make any positive scientific case for intelligent design. It fails to allow for scientific explanations continuing to be found, as has been the case with several examples previously put forward as supposed cases of irreducible complexity.
Possible theological implications.
Intelligent design proponents often insist that their claims do not require a religious component. However, various philosophical and theological issues are naturally raised by the claims of intelligent design.
Intelligent design proponents attempt to demonstrate scientifically that features such as irreducible complexity and specified complexity could not arise through natural processes, and therefore required repeated direct miraculous interventions by a Designer (often a Christian concept of God). They reject the possibility of a Designer who works merely through setting natural laws in motion at the outset, in contrast to theistic evolution (to which even Charles Darwin was open). Intelligent design is distinct because it asserts repeated miraculous interventions in addition to designed laws. This contrasts with other major religious traditions of a created world in which God's interactions and influences do not work in the same way as physical causes. The Roman Catholic tradition makes a careful distinction between ultimate metaphysical explanations and secondary, natural causes.
The concept of direct miraculous intervention raises other potential theological implications. If such a Designer does not intervene to alleviate suffering even though capable of intervening for other reasons, some imply the designer is not omnibenevolent (see problem of evil and related theodicy).
Further, repeated interventions imply that the original design was not perfect and final, and thus pose a problem for any who believe that the Creator's work had been both perfect and final. Intelligent design proponents seek to explain the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design (for example, proposing that vestigial organs have unknown purposes), or by proposing that designers do not necessarily produce the best design they can, and may have unknowable motives for their actions.
God of the gaps.
Intelligent design has also been characterized as a God-of-the-gaps argument, which has the following form:
A God-of-the-gaps argument is the theological version of an argument from ignorance. A key feature of this type of argument is that it merely answers outstanding questions with explanations (often supernatural) that are unverifiable and ultimately themselves subject to unanswerable questions. Historians of science observe that the astronomy of the earliest civilizations, although astonishing and incorporating mathematical constructions far in excess of any practical value, proved to be misdirected and of little importance to the development of science because they failed to inquire more carefully into the mechanisms that drove the heavenly bodies across the sky. It was the Greek civilization that first practiced science, although not yet a mathematically oriented experimental science, but nevertheless an attempt to rationalize the world of natural experience without recourse to divine intervention. In this historically motivated definition of science any appeal to an intelligent creator is explicitly excluded for the paralysing effect it may have on the scientific progress.
Kitzmiller trial.
"Kitzmiller v. Dover Area School District" was the first direct challenge brought in the United States federal courts against a public school district that required the presentation of intelligent design as an alternative to evolution. The plaintiffs successfully argued that intelligent design is a form of creationism, and that the school board policy thus violated the Establishment Clause of the First Amendment to the United States Constitution.
Eleven parents of students in Dover, Pennsylvania, sued the Dover Area School District over a statement that the school board required be read aloud in ninth-grade science classes when evolution was taught. The plaintiffs were represented by the American Civil Liberties Union (ACLU), Americans United for Separation of Church and State (AU) and Pepper Hamilton LLP. The National Center for Science Education acted as consultants for the plaintiffs. The defendants were represented by the Thomas More Law Center. The suit was tried in a bench trial from September 26 to November 4, 2005, before Judge John E. Jones III. Kenneth R. Miller, Kevin Padian, Brian Alters, Robert T. Pennock, Barbara Forrest and John F. Haught served as expert witnesses for the plaintiffs. Michael Behe, Steve Fuller and Scott Minnich served as expert witnesses for the defense.
On December 20, 2005, Judge Jones issued his 139-page findings of fact and decision, ruling that the Dover mandate was unconstitutional, and barring intelligent design from being taught in Pennsylvania's Middle District public school science classrooms. The eight Dover school board members who voted for the intelligent design requirement were all defeated in a November 8, 2005, election by challengers who opposed the teaching of intelligent design in a science class, and the current school board president stated that the board does not intend to appeal the ruling.
In his finding of facts, Judge Jones made the following condemnation of the "Teach the Controversy" strategy:
 ...Moreover, ID's backers have sought to avoid the scientific scrutiny which we have now determined that it cannot withstand by advocating that the "controversy", but not ID itself, should be taught in science class. This tactic is at best disingenuous, and at worst a . The goal of the IDM is not to encourage critical thought, but to foment a revolution which would supplant evolutionary theory with ID.
Reaction.
Judge Jones himself anticipated that his ruling would be criticized, saying in his decision that:
 ...Those who disagree with our holding will likely mark it as the product of an activist judge. If so, they will have erred as this is manifestly not an activist Court.<br>
Rather, this case came to us as the result of the activism of an ill-informed faction on a school board, aided by a national public interest law firm eager to find a constitutional test case on ID, who in combination drove the Board to adopt an imprudent and ultimately unconstitutional policy. The breathtaking inanity of the Board's decision is evident when considered against the factual backdrop which has now been fully revealed through this trial. The students, parents, and teachers of the Dover Area School District deserved better than to be dragged into this legal maelstrom, with its resulting utter waste of monetary and personal resources. ...
As Jones had predicted, John G. West, Associate Director of the Center for Science and Culture, said:
 "The Dover decision is an attempt by an activist federal judge to stop the spread of a scientific idea and even to prevent criticism of Darwinian evolution through government-imposed censorship rather than open debate, and it won't work. He has conflated Discovery Institute's position with that of the Dover school board, and he totally misrepresents intelligent design and the motivations of the scientists who research it."
Newspapers have noted with interest that the judge is "a Republican and a churchgoer."
Subsequently, the decision has been examined in a search for flaws and conclusions, partly by intelligent design supporters aiming to avoid future defeats in court. In the Winter of 2007, the "Montana Law Review" published three articles.
In the first, David K. DeWolf, John G. West and Casey Luskin, all of the Discovery Institute, argued that intelligent design is a valid scientific theory, the Jones court should not have addressed the question of whether it was a scientific theory, and that the Kitzmiller decision will have no effect at all on the development and adoption of intelligent design as an alternative to standard evolutionary theory. In the second Peter H. Irons responded, arguing that the decision was extremely well reasoned and spells the death knell for the intelligent design efforts to introduce creationism in public schools, while in the third, DeWolf, "et al.", answer the points made by Irons. However, fear of a similar lawsuit has resulted in other school boards abandoning intelligent design "teach the controversy" proposals.
In April 2010, the American Academy of Religion issued "Guidelines for Teaching About Religion in K‐12 Public Schools in the United States", which included guidance that creation science or intelligent design should not be taught in science classes, as "Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning." However, they, as well as other "worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others."
Status outside the United States.
Europe.
In June 2007, the Council of Europe's Committee on Culture, Science and Education issued a report, "The dangers of creationism in education", which states "Creationism in any of its forms, such as 'intelligent design', is not based on facts, does not use any scientific reasoning and its contents are pathetically inadequate for science classes." In describing the dangers posed to education by teaching creationism, it described intelligent design as "anti-science" and involving "blatant scientific fraud" and "intellectual deception" that "blurs the nature, objectives and limits of science" and links it and other forms of creationism to denialism. On October 4, 2007, the Council of Europe's Parliamentary Assembly approved a resolution stating that schools should "resist presentation of creationist ideas in any discipline other than religion," including "intelligent design," which it described as "the latest, more refined version of creationism," "presented in a more subtle way." The resolution emphasises that the aim of the report is not to question or to fight a belief, but to "warn against certain tendencies to pass off a belief as science."
In the United Kingdom, public education includes religious education as a compulsory subject, and there are many faith schools that teach the ethos of particular denominations. When it was revealed that a group called Truth in Science had distributed DVDs produced by Illustra Media featuring Discovery Institute fellows making the case for design in nature, and claimed they were being used by 59 schools, the Department for Education and Skills (DfES) stated that "Neither creationism nor intelligent design are taught as a subject in schools, and are not specified in the science curriculum" (part of the National Curriculum, which does not apply to independent schools or to education in Scotland). The DfES subsequently stated that "Intelligent design is not a recognised scientific theory; therefore, it is not included in the science curriculum," but left the way open for it to be explored in religious education in relation to different beliefs, as part of a syllabus set by a local Standing Advisory Council on Religious Education. In 2006, the Qualifications and Curriculum Authority produced a "Religious Education" model unit in which pupils can learn about religious and nonreligious
views about creationism, intelligent design and evolution by natural selection.
On June 25, 2007, the UK Government responded to an e-petition by saying that creationism and intelligent design should not be taught as science, though teachers would be expected to answer pupils' questions within the standard framework of established scientific theories. Detailed government "Creationism teaching guidance" for schools in England was published on September 18, 2007. It states that "Intelligent design lies wholly outside of science," has no underpinning scientific principles, or explanations, and is not accepted by the science community as a whole. Though it should not be taught as science, "Any questions about creationism and intelligent design which arise in science lessons, for example as a result of media coverage, could provide the opportunity to explain or explore why they are not considered to be scientific theories and, in the right context, why evolution is considered to be a scientific theory." However, "Teachers of subjects such as RE, history or citizenship may deal with creationism and intelligent design in their lessons."
The British Centre for Science Education lobbying group has the goal of "countering creationism within the UK" and has been involved in government lobbying in the UK in this regard. Northern Ireland's Department for Education says that the curriculum provides an opportunity for alternative theories to be taught. The Democratic Unionist Party (DUP)—which has links to fundamentalist Christianity—has been campaigning to have intelligent design taught in science classes. A DUP former Member of Parliament, David Simpson, has sought assurances from the education minister that pupils will not lose marks if they give creationist or intelligent design answers to science questions. In 2007, Lisburn city council voted in favor of a DUP recommendation to write to post-primary schools asking what their plans are to develop teaching material in relation to "creation, intelligent design and other theories of origin."
Plans by Dutch Education Minister Maria van der Hoeven to "stimulate an academic debate" on the subject in 2005 caused a severe public backlash. After the 2006 elections, she was succeeded by Ronald Plasterk, described as a "molecular geneticist, staunch athiest ["sic"] and opponent of intelligent design." As a reaction on this situation in the Netherlands, the Director General of the Flemish Secretariat of Catholic Education (VSKO) in Belgium, Mieke Van Hecke, declared that: "Catholic scientists already accepted the theory of evolution for a long time and that intelligent design and creationism doesn't belong in Flemish Catholic schools. It's not the tasks of the politics to introduce new ideas, that's task and goal of science."
Relation to Islam.
Muzaffar Iqbal, a notable Pakistani-Canadian Muslim, signed the "A Scientific Dissent From Darwinism" petition of the Discovery Institute. Ideas similar to intelligent design have been considered respected intellectual options among Muslims, and in Turkey many intelligent design books have been translated. In Istanbul in 2007, public meetings promoting intelligent design were sponsored by the local government, and David Berlinski of the Discovery Institute was the keynote speaker at a meeting in May 2007.
Relation to ISKCON.
In 2011, the International Society for Krishna Consciousness (ISKCON) Bhaktivedanta Book Trust published an intelligent design book titled "Rethinking Darwin: A Vedic Study of Darwinism and Intelligent Design". The book included contributions from intelligent design advocates William A. Dembski, Jonathan Wells and Michael Behe as well as from Hindu creationists Leif A. Jensen and Michael Cremo.
Australia.
The status of intelligent design in Australia is somewhat similar to that in the UK (see Education in Australia). In 2005, the Australian Minister for Education, Science and Training, Brendan Nelson, raised the notion of intelligent design should be taught in science classes. The public outcry caused the minister to quickly concede that the correct forum for intelligent design, if it were to be taught, is in religion or philosophy classes. The Australian chapter of Campus Crusade for Christ distributed a DVD of the Discovery Institute's documentary "Unlocking the Mystery of Life" (2002) to Australian secondary schools.
The head of one of Australia's leading private schools supported use of the DVD in the classroom at the discretion of teachers and principals.
Further reading.
</dl>

</doc>
<doc id="15302" url="http://en.wikipedia.org/wiki?curid=15302" title="Integrin">
Integrin

Integrins are transmembrane receptors that are the bridges for cell-cell and cell-extracellular matrix (ECM) interactions. When triggered, integrins in turn trigger chemical pathways to the interior (signal transduction), such as the chemical composition and mechanical status of the ECM, which results in a response (activation of transcription) such as regulation of the cell cycle, cell shape, and/or motility; or new receptors being added to the cell membrane. This allows rapid and flexible responses to events at the cell surface, for example to signal platelets to initiate an interaction with coagulation factors.
There are several types of integrins, and a cell may have several types on its surface. Integrins are found in all metazoa.
Integrins work alongside other receptors such as cadherins, the immunoglobulin superfamily cell adhesion molecules, selectins and syndecans to mediate cell–cell and cell–matrix interaction. Ligands for integrins include fibronectin, vitronectin, collagen, and laminin.
Structure.
Integrins have two different chains, the α (alpha) and β (beta) subunits, and are called obligate heterodimers. In mammals, there are eighteen α and eight β subunits, in "Drosophila" five α and two β subunits, and in "Caenorhabditis" nematodes two α subunits and one β subunit. The α and β subunits each penetrate the plasma membrane and possess small cytoplasmic domains. 
Variants of some of the subunits are formed by differential RNA splicing; for example, four variants of the beta-1 subunit exist. Through different combinations of the α and β subunits, around 24 unique integrins are generated.
Integrin subunits span the cell membrane and have short cytoplasmic domains of 40–70 amino acids. The exception is the beta-4 subunit, which has a cytoplasmic domain of 1088 amino acids, one of the largest known cytoplasmic domains of any membrane protein. Outside the cell membrane, the α and β chains lie close together along a length of about 23 nm; the final 5 nm N-termini of each chain forms a ligand-binding region for the ECM. They have been compared to lobster claws, although the don't actually "pinch" their ligand, they chemically interact with it at the insides of the "tips" of their "pinchers".
The molecular mass of the integrin subunits can vary from 90 kDa to 160 kDa. Beta subunits have four cysteine-rich repeated sequences. Both α and β subunits bind several divalent cations. The role of divalent cations in the α subunit is unknown, but may stabilize the folds of the protein. The cations in the β subunits are more interesting: they are directly involved in coordinating at least some of the ligands that integrins bind.
There are various ways of categorizing the integrins. For example, a subset of the α chains has an additional structural element (or "domain") inserted toward the N-terminal, the alpha-A domain (so called because it has a similar structure to the A-domains found in the protein von Willebrand factor; it is also termed the α-I domain). Integrins carrying this domain either bind to collagens (e.g. integrins α1 β1, and α2 β1), or act as cell-cell adhesion molecules (integrins of the β2 family). This α-I domain is the binding site for ligands of such integrins. Those integrins that don't carry this inserted domain also have an A-domain in their ligand binding site, but "this" A-domain is found on the β subunit.
In both cases, the A-domains carry up to three divalent cation binding sites. One is permanently occupied in physiological concentrations of divalent cations, and carries either a calcium or magnesium ion, the principal divalent cations in blood at median concentrations of 1.4 mM (calcium) and 0.8 mM (magnesium). The other two sites become occupied by cations when ligands bind—at least for those ligands involving an acidic amino acid in their interaction sites. An acidic amino acid features in the integrin-interaction site of many ECM proteins, for example as part of the amino acid sequence Arginine-Glycine-Aspartic acid ("RGD" in the one-letter amino acid code).
Structure.
Despite many years of effort, discovering the high-resolution structure of integrins proved to be challenging: membrane proteins are classically difficult to purify, and integrins are also large, complex and linked to many sugar trees ("highly glycosylated"). Low-resolution images of detergent extracts of intact integrin GPIIbIIIa, obtained using electron microscopy, and even data from indirect techniques that investigate the solution properties of integrins using ultracentrifugation and light scattering, were combined with fragmentary high-resolution crystallographic or NMR data from single or paired domains of single integrin chains, and molecular models postulated for the rest of the chains.
Despite these wide-ranging efforts, the X-ray crystal structure obtained for the complete extracellular region of one integrin, αvβ3, was a surprise. It showed the molecule to be folded into an inverted V-shape that potentially brings the ligand-binding sites close to the cell membrane. Perhaps more importantly, the crystal structure was also obtained for the same integrin bound to a small ligand containing the RGD-sequence, the drug cilengitide. As detailed above, this finally revealed why divalent cations (in the A-domains) are critical for RGD-ligand binding to integrins. The interaction of such sequences with integrins is believed to be a primary switch by which ECM exerts its effects on cell behaviour.
The structure poses many questions, especially regarding ligand binding and signal transduction. The ligand binding site is directed towards the C-terminal of the integrin, the region where the molecule emerges from the cell membrane. If it emerges orthogonally from the membrane, the ligand binding site would apparently be obstructed, especially as integrin ligands are typically massive and well cross-linked components of the ECM. In fact, little is known about the angle that membrane proteins subtend to the plane of the membrane; this is a problem difficult to address with available technologies. The default assumption is that they emerge rather like little lollipops, but the evidence for this sweet supposition is noticeable by its absence. The integrin structure has drawn attention to this problem, which may have general implications for how membrane proteins work. It appears that the integrin transmembrane helices are tilted (see "Activation" below), which hints that the extracellular chains may also not be orthogonal with respect to the membrane surface.
Although the crystal structure changed surprisingly little after binding to cilengitide, the current hypothesis is that integrin function involves changes in shape to move the ligand-binding site into a more accessible position, away from the cell surface, and this shape change also triggers intracellular signaling. There is a wide body of cell-biological and biochemical literature that supports this view. Perhaps the most convincing evidence involves the use of antibodies that only recognize integrins when they have bound to their ligands, or are activated. As the "footprint" that an antibody makes on its binding target is roughly a circle about 3 nm in diameter, the resolution of this technique is low. Nevertheless, these so-called LIBS (Ligand-Induced-Binding-Sites) antibodies unequivocally show that dramatic changes in integrin shape routinely occur. However, how the changes detected with antibodies look on the structure is still unknown.
Activation.
When released into the cell membrane, newly synthesized integrin dimers are speculated to be found in the same "bent" conformation revealed by the structural studies described above. One school of thought claims that this bent form prevents them from interacting with their ligands, although bent forms can predominate in high-resolution EM structures of integrin bound to an ECM ligand. Therefore, at least in biochemical experiments, integrin dimers must apparently not be 'unbent' in order to prime them and allow their binding to the ECM. In cells, the priming is accomplished by a protein talin, which binds to the β tail of the integrin dimer and changes its conformation. The α and β integrin chains are both class-I transmembrane proteins: they pass the plasma membrane as single transmembrane alpha-helices. Unfortunately, the helices are too long, and recent studies suggest that, for integrin gpIIbIIIa, they are tilted with respect both to one another and to the plane of the membrane. Talin binding alters the angle of tilt of the β3 chain transmembrane helix in model systems and this may reflect a stage in the process of inside-out signalling which primes integrins. Moreover, talin proteins are able to dimerize and thus are thought to intervene in the clustering of integrin dimers which leads to the formation of a focal adhesion. Recently, the Kindlin-1 and Kindlin-2 proteins have also been found to interact with integrin and activate it.
Function.
Integrins have two main functions:-
However, they are also involved in a wide range of other biological activities, including immune patrolling, cell migration, and binding to cells by certain viruses, such as adenovirus, echovirus, hantavirus, and foot and mouth disease viruses.
A prominent function of the integrins is seen in the molecule GPIIbIIIa, an integrin on the surface of blood platelets (thrombocytes) responsible for attachment to fibrin within a developing blood clot. This molecule dramatically increases its binding affinity for fibrin/fibrinogen through association of platelets with exposed collagens in the wound site. Upon association of platelets with collagen, GPIIbIIIa changes shape, allowing it to bind to fibrin and other blood components to form the clot matrix and stop blood loss.
Attachment of cell to the ECM.
Integrins couple the ECM outside a cell to the cytoskeleton (in particular, the microfilaments) inside the cell. Which ligand in the ECM the integrin can bind to is defined by which α and β subunits the integrin is made of. Among the ligands of integrins are fibronectin, vitronectin, collagen, and laminin. The connection between the cell and the ECM may help the cell to endure pulling forces without being ripped out of the ECM. The ability of a cell to create this kind of bond is also of vital importance in ontogeny.
Cell attachment to the ECM is a basic requirement to build a multicellular organism. Integrins are not simply hooks, but give the cell critical signals about the nature of its surroundings. Together with signals arising from receptors for soluble growth factors like VEGF, EGF, and many others, they enforce a cellular decision on what biological action to take, be it attachment, movement, death, or differentiation. Thus integrins lie at the heart of many cellular biological processes. The attachment of the cell takes place through formation of cell adhesion complexes, which consist of integrins and many cytoplasmic proteins, such as talin, vinculin, paxillin, and alpha-actinin. These act by regulating kinases such as FAK (focal adhesion kinase) and Src kinase family members to phosphorylate substrates such as p130CAS thereby recruiting signaling adaptors such as CRK. These adhesion complexes attach to the actin cytoskeleton. The integrins thus serve to link two networks across the plasma membrane: the extracellular ECM and the intracellular actin filamentous system. Integrin alpha6beta4 is an exception: it links to the keratin intermediate filament system in epithelial cells.
Focal adhesions are large molecular complexes, which are generated following interaction of integrins with ECM, then their clustering. The clusters likely provide sufficient intracellular binding sites to permit the formation of stable signaling complexes on the cytoplasmic side of the cell membrane. So the focal adhesions contain integrin ligand, integrin molecule, and associate plaque proteins. Binding is propelled by changes in free energy. As previously stated, these complexes connect the extracellular matrix to actin bundles. Cryo-electron tomography reveals that the adhesion contains particles on the cell membrane with diameter of 25 +/- 5 nm and spaced at approximately 45 nm. Treatment with Rho-kinase inhibitor Y-27632 reduces the size of the particle, and it is extremely mechanosensitive.
One important function of integrins on cells in tissue culture is their role in cell migration. Cells adhere to a substrate through their integrins. During movement, the cell makes new attachments to the substrate at its front and concurrently releases those at its rear. When released from the substrate, integrin molecules are taken back into the cell by endocytosis; they are transported through the cell to its front by the endocytic cycle, where they are added back to the surface. In this way they are cycled for reuse, enabling the cell to make fresh attachments at its leading front. It is not yet clear whether cell migration in tissue culture is an artefact of integrin processing, or whether such integrin-dependent cell migration also occurs in living organisms.
Signal transduction.
Integrins play an important role in cell signaling by modulating the cell signaling pathways of transmembrane protein kinases such as receptor tyrosine kinases (RTK). While the interaction between integrin and receptor tyrosine kinases originally was thought of as uni-directional and supportive, recent studies indicate that integrins have additional, multi-faceted roles in cell signaling. 
Integrins can regulate the receptor tyrosine kinase signaling by recruiting specific adaptors to the plasma membrane. For example, β1c integrin recruits Gab1/Shp2 and presents Shp2 to IGF1R, resulting in dephosphorylation of the receptor. In a reverse direction, when a receptor tyrosine kinase is activated, integrins co-localise at focal adhesion with the receptor tyrosine kinases and their associated signaling molecules.
The repertoire of integrins expressed on a particular cell can specify the signaling pathway due to the differential binding affinity of ECM ligands for the integrins. The tissue stiffness and matrix composition can initiate specific signaling pathways regulating cell behavior. Clustering and activation of the integrins/actin complexes strengthen the focal adhesion interaction and initiate the framework for cell signaling through assembly of adhesomes.
Depending on the integrin's regulatory impact on specific receptor tyrosine kinases, the cell can experience:
Knowledge of the relationship between integrins and receptor tyrosine kinase has laid a foundation for new approaches to cancer therapy. Specifically, targeting integrins associated with RTKs is an emerging approach for inhibiting angiogenesis.
Vertebrate integrins.
The following are some of the integrins found in vertebrates:
Beta-1 integrins interact with many alpha integrin chains. Gene knockouts of integrins in mice are not always lethal, which suggests that during embryonal development, one integrin may substitute its function for another in order to allow survival. Some integrins are on the cell surface in an inactive state, and can be rapidly primed, or put into a state capable of binding their ligands, by cytokines. Integrins can assume several different well-defined shapes or "conformational states". Once primed, the conformational state changes to stimulate ligand binding, which then activates the receptors — also by inducing a shape change — to trigger outside-in signal transduction.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="15303" url="http://en.wikipedia.org/wiki?curid=15303" title="Ion channel">
Ion channel

Ion channels are pore-forming membrane proteins whose functions include establishing a resting membrane potential, shaping action potentials and other electrical signals by gating the flow of ions across the cell membrane, controlling the flow of ions across secretory and epithelial cells, and regulating cell volume. Ion channels are present in the membranes of all cells. Ion channels are considered to be one of the two traditional classes of ionophoric proteins, with the other class known as ion transporters (including the sodium-potassium pump, sodium-calcium exchanger, and sodium-glucose transport proteins, amongst others).
Study of ion channels (channelomics) often includes biophysics, electrophysiology and pharmacology, utilizing techniques including voltage clamp, patch clamp, immunohistochemistry, X-ray fluorescence, and RT-PCR.
Basic features.
There are two distinctive features of ion channels that differentiate them from other types of ion transporter proteins:
Ion channels are located within the plasma membrane of nearly all cells and many intracellular organelles. They are often described as narrow, water-filled tunnels that allow only ions of a certain size and/or charge to pass through. This characteristic is called selective permeability. The archetypal channel pore is just one or two atoms wide at its narrowest point and is selective for specific species of ion, such as sodium or potassium. However, some channels may be permeable to the passage of more than one type of ion, typically sharing a common charge: positive (cations) or negative (anions). Ions often move through the segments of the channel pore in single file nearly as quickly as the ions move through free solution. In many ion channels, passage through the pore is governed by a "gate", which may be opened or closed in response to chemical or electrical signals, temperature, or mechanical force.
Ion channels are integral membrane proteins, typically formed as assemblies of several individual proteins. Such "multi-subunit" assemblies usually involve a circular arrangement of identical or homologous proteins closely packed around a water-filled pore through the plane of the membrane or lipid bilayer. For most voltage-gated ion channels, the pore-forming subunit(s) are called the α subunit, while the auxiliary subunits are denoted β, γ, and so on.
Biological role.
Because channels underlie the nerve impulse and because "transmitter-activated" channels mediate conduction across the synapses, channels are especially prominent components of the nervous system. Indeed, numerous toxins that organisms have evolved for shutting down the nervous systems of predators and prey (e.g., the venoms produced by spiders, scorpions, snakes, fish, bees, sea snails, and others) work by modulating ion channel conductance and/or kinetics. In addition, ion channels are key components in a wide variety of biological processes that involve rapid changes in cells, such as cardiac, skeletal, and smooth muscle contraction, epithelial transport of nutrients and ions, T-cell activation and pancreatic beta-cell insulin release. In the search for new drugs, ion channels are a frequent target.
Diversity.
There are over 300 types of ion channels in a living cell. Ion channels may be classified by the nature of their gating, the species of ions passing through those gates, the number of gates (pores) and localization of proteins.
Further heterogeneity of ion channels arises when channels with different constitutive subunits give rise to a specific kind of current. Absence or mutation of one or more of the contributing types of channel subunits can result in loss of function and, potentially, underlie neurologic diseases.
Classification by gating.
Ion channels may be classified by gating, i.e. what opens and closes the channels. Voltage-gated ion channels open or close depending on the voltage gradient across the plasma membrane, while ligand-gated ion channels open or close depending on binding of ligands to the channel.
Voltage-gated.
Voltage-gated ion channels open and close in response to membrane potential.
Ligand-gated.
Also known as ionotropic receptors, this group of channels open in response to specific ligand molecules binding to the extracellular domain of the receptor protein. Ligand binding causes a conformational change in the structure of the channel protein that ultimately leads to the opening of the channel gate and subsequent ion flux across the plasma membrane. Examples of such channels include the cation-permeable "nicotinic" Acetylcholine receptor, ionotropic glutamate-gated receptors and ATP-gated P2X receptors, and the anion-permeable γ-aminobutyric acid-gated GABAA receptor.
Ion channels activated by second messengers may also be categorized in this group, although ligands and second messengers are otherwise distinguished from each other.
Other gating.
Other gating include activation/inactivation by, e.g., second messengers from the inside of the cell membrane, rather than from outside, as in the case for ligands. Ions may count to such second messengers, and then causes direct activation, rather than indirect, as in the case were the electric potential of ions cause activation/inactivation of voltage-gated ion channels.
Other classifications.
There are other types of ion channel classifications that are based on less normal characteristics, e.g. multiple pores and transient potentials.
Almost all ion channels have one single pore. However, there are also those with two:
There are channels that are classified by the duration of the response to stimuli:
Detailed structure.
Channels differ with respect to the ion they let pass (for example, Na+, K+, Cl−), the ways in which they may be regulated, the number of subunits of which they are composed and other aspects of structure. Channels belonging to the largest class, which includes the voltage-gated channels that underlie the nerve impulse, consists of four subunits with six transmembrane helices each. On activation, these helices move about and open the pore. Two of these six helices are separated by a loop that lines the pore and is the primary determinant of ion selectivity and conductance in this channel class and some others. The existence and mechanism for ion selectivity was first postulated in the 1960s by Clay Armstrong. He suggested that the pore lining could efficiently replace the water molecules that normally shield potassium ions, but that sodium ions were too small to allow such shielding, and therefore could not pass through. This mechanism was finally confirmed when the structure of the channel was elucidated. The channel subunits of one such other class, for example, consist of just this "P" loop and two transmembrane helices. The determination of their molecular structure by Roderick MacKinnon using X-ray crystallography won a share of the 2003 Nobel Prize in Chemistry.
Because of their small size and the difficulty of crystallizing integral membrane proteins for X-ray analysis, it is only very recently that scientists have been able to directly examine what channels "look like." Particularly in cases where the crystallography required removing channels from their membranes with detergent, many researchers regard images that have been obtained as tentative. An example is the long-awaited crystal structure of a voltage-gated potassium channel, which was reported in May 2003. One inevitable ambiguity about these structures relates to the strong evidence that channels change conformation as they operate (they open and close, for example), such that the structure in the crystal could represent any one of these operational states. Most of what researchers have deduced about channel operation so far they have established through electrophysiology, biochemistry, gene sequence comparison and mutagenesis.
Channels can have single (CLICs) to multiple transmembrane (K channels, P2X receptors, Na channels) domains which span plasma membrane to form pores. Pore can determine the selectivity of the channel. Gate can be formed either inside or outside the pore region.
Ion channel blockers.
A variety of inorganic and organic molecules can modulate ion channel activity and conductance.
Some commonly used blockers include:
Diseases.
There are a number of genetic disorders which disrupt normal functioning of ion channels and have disastrous consequences for the organism. Genetic disorders of ion channels and their modifiers are known as channelopathies. See for a full list.
History.
The fundamental properties of currents mediated by ion channels were analyzed by the British biophysicists Alan Hodgkin and Andrew Huxley as part of their Nobel Prize-winning research on the action potential, published in 1952. They built on the work of other physiologists, such as Cole and Baker's research into voltage-gated membrane pores from 1941. The existence of ion channels was confirmed in the 1970s by Bernard Katz and Ricardo Miledi using noise analysis. It was then shown more directly with an electrical recording technique known as the "patch clamp", which led to a Nobel Prize to Erwin Neher and Bert Sakmann, the technique's inventors. Hundreds if not thousands of researchers continue to pursue a more detailed understanding of how these proteins work. In recent years the development of automated patch clamp devices helped to increase significantly the throughput in ion channel screening.
The Nobel Prize in Chemistry for 2003 was awarded to two American scientists: Roderick MacKinnon for his studies on the physico-chemical properties of ion channel structure and function, including x-ray crystallographic structure studies, and Peter Agre for his similar work on aquaporins.
Culture.
Roderick MacKinnon commissioned "Birth of an Idea", a 5 ft tall sculpture based on the KcsA potassium channel. The artwork contains a wire object representing the channel's interior with a blown glass object representing the main cavity of the channel structure.

</doc>
<doc id="15304" url="http://en.wikipedia.org/wiki?curid=15304" title="IDE">
IDE

IDE or Ide may refer to:

</doc>
<doc id="15305" url="http://en.wikipedia.org/wiki?curid=15305" title="Integrated development environment">
Integrated development environment

An integrated development environment (IDE) or interactive development environment is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a source code editor, build automation tools and a debugger. Most modern IDEs have intelligent code completion.
Some IDEs contain a compiler, interpreter, or both, such as NetBeans and Eclipse; others do not, such as SharpDevelop and Lazarus. The boundary between an integrated development environment and other parts of the broader "software development environment" is not well-defined. Sometimes a version control system and various tools are integrated to simplify the construction of a Graphical User Interface (GUI). Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram, for use in object-oriented software development.
Overview.
Integrated development environments are designed to maximize programmer productivity by providing tight-knit components with similar user interfaces. IDEs present a single program in which all development is done. This program typically provides many features for authoring, modifying, compiling, deploying and debugging software. This contrasts with software development using unrelated tools, such as vi, GCC or make.
One aim of the IDE is to reduce the configuration necessary to piece together multiple development utilities, instead providing the same set of capabilities as a cohesive unit. Reducing that setup time can increase developer productivity, in cases where learning to use the IDE is faster than manually integrating all of the individual tools. Tighter integration of all development tasks has the potential to improve overall productivity beyond just helping with setup tasks. For example, code can be continuously parsed while it is being edited, providing instant feedback when syntax errors are introduced. That can speed learning a new programming language and its associated libraries.
Some IDEs are dedicated to a specific programming language, allowing a feature set that most closely matches the programming paradigms of the language. However, there are many multiple-language IDEs, such as Eclipse, ActiveState Komodo, IntelliJ IDEA, MyEclipse, Oracle JDeveloper, NetBeans, Codenvy and Microsoft Visual Studio. Xcode, Xojo and Delphi are dedicated to a closed language or set of programming languages.
While most modern IDEs are graphical, text-based IDEs such as Turbo Pascal were in popular use before the widespread availability of windowing systems like Microsoft Windows and the X Window System (X11). They commonly use function keys or hotkeys to execute frequently used commands or macros.
History.
IDEs initially became possible when developing via a console or terminal. Early systems could not support one, since programs were prepared using flowcharts, entering programs with punched cards (or paper tape, etc.) before submitting them to a compiler. Dartmouth BASIC was the first language to be created with an IDE (and was also the first to be designed for use while sitting in front of a console or terminal). Its IDE (part of the Dartmouth Time Sharing System) was command-based, and therefore did not look much like the menu-driven, graphical IDEs prevalent today. However it integrated editing, file management, compilation, debugging and execution in a manner consistent with a modern IDE.
Maestro I is a product from Softlab Munich and was the world's first integrated development environment 1975 for software. Maestro I was installed for 22,000 programmers worldwide. Until 1989, 6,000 installations existed in the Federal Republic of Germany. Maestro I was arguably the world leader in this field during the 1970s and 1980s. Today one of the last Maestro I can be found in the Museum of Information Technology at Arlington.
One of the first IDEs with a plug-in concept was Softbench. In 1995 "Computerwoche" commented that the use of an IDE was not well received by developers since it would fence in their creativity.
As of March 2015, the most popular IDE's are Eclipse and Visual Studio.
Topics.
Visual programming.
Visual programming is a usage scenario in which an IDE is generally required. Visual IDEs allow users to create new applications by moving programming, building blocks, or code nodes to create flowcharts or structure diagrams that are then compiled or interpreted. These flowcharts often are based on the Unified Modeling Language.
This interface has been popularized with the Lego Mindstorms system, and is being actively pursued by a number of companies wishing to capitalize on the power of custom browsers like those found at Mozilla. KTechlab supports flowcode and is a popular opensource IDE and Simulator for developing software for microcontrollers. Visual programming is also responsible for the power of distributed programming (cf. LabVIEW and EICASLAB software). An early visual programming system, Max, was modelled after analog synthesizer design and has been used to develop real-time music performance software since the 1980s. Another early example was Prograph, a dataflow-based system originally developed for the Macintosh. The graphical programming environment "Grape" is used to program qfix robot kits.
This approach is also used in specialist software such as Openlab, where the end users want the flexibility of a full programming language, without the traditional learning curve associated with one.
Language support.
Some IDEs support multiple languages, such as GNU Emacs based on C and Emacs Lisp, and IntelliJ IDEA, Eclipse, MyEclipse or NetBeans, all based on Java, or MonoDevelop, based on C#.
Support for alternative languages is often provided by plugins, allowing them to be installed on the same IDE at the same time. For example, Flycheck is a modern on-the-fly syntax checking extension for GNU Emacs 24 with support for 39 languages. Eclipse, and Netbeans have plugins for C/C++, Ada, GNAT (for example AdaGIDE), Perl, Python, Ruby, and PHP, which are selected between automatically based on file extension, environment or project settings.
Attitudes across different computing platforms.
Unix programmers can combine command-line POSIX tools into a complete development environment, capable of developing large programs such as the Linux kernel and its environment. The free software GNU tools (GNU Compiler Collection (GCC), GNU Debugger (gdb), GNU make) are available on many platforms, including Windows. Developers who favor command-line oriented tools can use editors with support for many of the standard Unix and GNU build tools, building an IDE with programs like
Emacs
or Vim. Data Display Debugger is intended to be an advanced graphical front-end for many text-based debugger standard tools. Some programmers prefer managing makefiles and their derivatives to the similar code building tools included in a full IDE. For example, most contributors to the PostgreSQL database use make and gdb directly to develop new features. Even when building PostgreSQL for Microsoft Windows using Visual C++, Perl scripts are used as a replacement for make rather than relying on any IDE features. Some Linux IDEs such as Geany attempt to provide a graphical front end to traditional build operations.
On the various Microsoft Windows platforms, command-line tools for development are seldom used. Accordingly, there are many commercial and non-commercial products. However, each has a different design commonly creating incompatibilities. Most major compiler vendors for Windows still provide free copies of their command-line tools, including Microsoft (Visual C++, Platform SDK, .NET Framework SDK, nmake utility), Embarcadero Technologies (bcc32 compiler, make utility).
IDEs have always been popular on the Apple Macintosh's Mac OS, dating back to Macintosh Programmer's Workshop, Turbo Pascal, THINK Pascal and THINK C environments of the mid-1980s. Currently Mac OS X programmers can choose between native IDEs like Xcode and open-source tools such as Eclipse and Netbeans. ActiveState Komodo is a proprietary multilanguage IDE supported on the Mac OS.
With the advent of cloud computing, some IDEs are available online and run within web browsers.

</doc>
<doc id="15308" url="http://en.wikipedia.org/wiki?curid=15308" title="Ian McKellen">
Ian McKellen

Sir Ian Murray McKellen, 
McKellen was appointed Commander of the Order of the British Empire in 1979, was knighted in 1991 for services to the performing arts, and was made a Companion of Honour for services to drama and to equality, in the 2008 New Year Honours. He was made a Freeman of the City of London in October 2014.
Early life.
McKellen was born on 25 May 1939 in Burnley, Lancashire, the son of Margery Lois (née Sutcliffe) and Denis Murray McKellen, a civil engineer. He was their second child, with a sister, Jean, five years his senior. Shortly before the outbreak of the Second World War in September 1939, his family moved to Wigan. They lived there through the war and his early childhood. They relocated to Bolton in 1951, after his father had been promoted. The experience of living through the war as a young child had some lasting impact on him, and he later claimed that "only after peace resumed ... did I realise that war wasn't normal." In response to an interview question, when an interviewer remarked that he seemed quite calm in the aftermath of 11 September attacks, he said: "Well, darling, you forget—I slept under a steel plate until I was four years old."
McKellen's father was a civil engineer and lay preacher, and was of Scots-Irish and Scottish descent. Both of McKellen's grandfathers were preachers, and his great-great-grandfather, James McKellen, was a "strict, evangelical Protestant minister" in Ballymena, County Antrim. His home environment was strongly Christian, but non-orthodox. "My upbringing was of low nonconformist Christians who felt that you led the Christian life in part by behaving in a Christian manner to everybody you met." When he was 12, his mother died; his father died when he was 24. Of his coming out of the closet to his stepmother, Gladys McKellen, who was a member of the Religious Society of Friends, he said, "Not only was she not fazed, but as a member of a society which declared its indifference to people's sexuality years back, I think she was just glad for my sake that I wasn't lying anymore."
McKellen attended Bolton School (Boys' Division), of which he is still a supporter, attending regularly to talk to pupils. McKellen's acting career started at Bolton Little Theatre, of which he is now the patron. An early fascination with the theatre was encouraged by his parents, who took him on a family outing to "Peter Pan" at the Opera House in Manchester when he was three. When he was nine, his main Christmas present was a wood and bakelite, fold-away Victorian theatre from Pollocks Toy Theatres, with cardboard scenery and wires to push on the cut-outs of Cinderella and of Laurence Olivier's Hamlet.
His sister took him to his first Shakespeare play, "Twelfth Night", by the amateurs of Wigan's Little Theatre, shortly followed by their "Macbeth" and Wigan High School for Girls' production of "A Midsummer Night's Dream", with music by Mendelssohn, with the role of Bottom played by Jean McKellen, who continued to act, direct, and produce amateur theatre until her death.
When he was 18 years old, McKellen won a scholarship to St Catharine's College, Cambridge, where he read English literature.
Career.
Theatre.
While at Cambridge McKellen was a member of the Marlowe Society, appearing in "Henry IV" (as Shallow) alongside Trevor Nunn and Derek Jacobi (March 1959), "Cymbeline" (as Posthumus, opposite Margaret Drabble as Imogen) and "Doctor Faustus". His first professional appearance was in 1961 at the Belgrade Theatre, as Roper in "A Man for All Seasons", although an audio recording of the Marlowe Society's "Cymbeline" had gone on commercial sale as part of the Argo Shakespeare series.
After four years in regional repertory theatres he made his first West End appearance, in "A Scent of Flowers", regarded as a "notable success". In 1965 he was a member of Laurence Olivier's National Theatre Company at the Old Vic, which led to roles at the Chichester Festival. With the Prospect Theatre Company, McKellen made his breakthrough performances of Richard II (directed by Richard Cottrell) and Marlowe's Edward II (directed by Toby Robertson) at the Edinburgh festival in 1969, the latter causing a storm of protest over the enactment of the homosexual Edward's lurid death.
In the 1970s and 1980s McKellen became a well-known figure in British theatre, performing frequently at the Royal Shakespeare Company and the Royal National Theatre, where he played several leading Shakespearean roles, including the titular part in "Macbeth" (which he had first assayed for Trevor Nunn in a "gripping...out of the ordinary" production, with Judi Dench, at Stratford in 1976), and Iago in "Othello", in award-winning productions directed by Nunn. Both of these productions were adapted into television films, also directed by Nunn.
In 2007 he returned to the Royal Shakespeare Company, in productions of "King Lear" and "The Seagull", both directed by Trevor Nunn. In 2009 he appeared in a very popular revival of "Waiting for Godot" at London's Haymarket Theatre, directed by Sean Mathias, and playing opposite Patrick Stewart. He is Patron of English Touring Theatre and also President and Patron of the Little Theatre Guild of Great Britain, an association of amateur theatre organisations throughout the UK. In late August 2012, he took part in the opening ceremony of the London Paralympics, portraying Prospero from "The Tempest".
Popular success.
McKellen had taken film roles throughout his career—beginning in 1969 with his role of George Matthews in "A Touch of Love", and his first leading role was in 1980 as D. H. Lawrence in "Priest of Love", but it was not until the 1990s that he became more widely recognised in this medium after several roles in blockbuster Hollywood films.
In 1993, McKellen had a supporting role as a South African tycoon in the critically acclaimed "Six Degrees of Separation", in which he starred with Stockard Channing, Donald Sutherland, and Will Smith. In the same year, he appeared in minor roles in the television miniseries "Tales of the City", based on the novel by his friend Armistead Maupin, and the film "Last Action Hero", in which he played Death.
Later that same year, he also appeared in the television film "And the Band Played On", about the discovery of the AIDS virus, for which McKellen won a CableACE Award for Supporting Actor in a Movie or Miniseries and was nominated for the Emmy Award for Outstanding Supporting Actor in a Miniseries or a Movie. In 1995, he played the title role in "Richard III", which transported the setting into an alternative 1930s in which England is ruled by fascists. The film was a critical success. McKellen co-produced and co-wrote the film, adapting the play for the screen based on a stage production of Shakespeare's play directed by Richard Eyre for the Royal National Theatre, in which McKellen had appeared. As executive producer he returned his £50,000 fee to complete the filming of the final battle. In his review of the film, "Washington Post" film critic Hal Hinson, called McKellen's performance a "lethally flamboyant incarnation", and said his "florid mastery ... dominates everything". His performance in the title role garnered best actor nominations for the BAFTA Award and Golden Globe, and won the European Film Award for Best Actor. His screenplay was nominated for the BAFTA Award for Best Adapted Screenplay.
He appeared in the modestly acclaimed film "Apt Pupil", which was directed by Bryan Singer and based on a story by Stephen King. McKellen portrayed an old Nazi officer, living under a false name in the US, who was befriended by a curious teenager (Brad Renfro) who threatened to expose him unless he told his story in detail. He was subsequently nominated for the Academy Award for Best Actor for his role in the 1998 film "Gods and Monsters", wherein he played James Whale, the director of "Show Boat" (1936) and "Frankenstein".
In 1999 McKellen was cast, again under the direction of Bryan Singer, to play the comic book supervillain Magneto in the 2000 film "X-Men" and its sequels "X2: X-Men United" and '. While filming "X-Men" McKellen was cast as the wizard Gandalf in Peter Jackson's three-film adaptation of "The Lord of the Rings" (consisting of ', ', and '). He received honors from the Screen Actors Guild for Best Supporting Actor in a Motion Picture for his work in "The Fellowship of the Ring", and was nominated for the Academy Award for Best Supporting Actor for the same role. He also provided the voice of Gandalf for several video game adaptations of the films. On 10 January 2011 it was officially confirmed that Mckellen would reprise the role of Gandalf in the three-part film adaptation of "The Hobbit".
On 16 March 2002, he was the host on "Saturday Night Live". In 2003, McKellen made a guest appearance as himself on the American cartoon show "The Simpsons", in a special British-themed episode entitled "The Regina Monologues", along with Tony Blair and J. K. Rowling. In April and May 2005, he played the role of Mel Hutchwright in Granada Television's long running soap opera, "Coronation Street", fulfilling a lifelong ambition. He narrated Richard Bell's film "Eighteen", as a grandfather who leaves his World War II memoirs on audio-cassette for his teenage grandson.
McKellen has appeared in limited release films, such as "Emile" (which was shot in three weeks following the "X2" shoot), "Neverwas" and "Asylum". He appeared as Sir Leigh Teabing in "The Da Vinci Code". During a 17 May 2006 interview on "The Today Show" with the "Da Vinci Code" cast and director, Matt Lauer posed a question to the group about how they would have felt if the film had borne a prominent disclaimer that it is a work of fiction, as some religious groups wanted. McKellen responded, "I've often thought the Bible should have a disclaimer in the front saying 'This is fiction.' I mean, walking on water? It takes... an act of faith. And I have faith in this movie — not that it's true, not that it's factual, but that it's a jolly good story." He continued, "And I think audiences are clever enough and bright enough to separate out fact and fiction, and discuss the thing when they've seen it". McKellen appeared in the 2006 series of Ricky Gervais' comedy series "Extras", where he played himself directing Gervais' character Andy Millman in a play about gay lovers. McKellen received a 2007 Primetime Emmy Award for Outstanding Guest Actor - Comedy Series nomination for his performance. In 2009 he portrayed Number Two in "The Prisoner", a remake of the 1967 cult series "The Prisoner". He reprised his role, Magneto, in ', released in May 2014; he shared the role with Michael Fassbender, who played a younger version of the character in 2011's '. In November 2013 McKellen appeared in the one-off Doctor Who 50th anniversary comedy homage "The Five(ish) Doctors Reboot".
McKellen was awarded an honorary Doctorate of Letters by Cambridge University on 18 June 2014.
Personal life.
McKellen and his first serious partner, Brian Taylor, a history teacher from Bolton, began their relationship in 1964. It lasted for eight years, ending in 1972. They lived in London, where McKellen continued to pursue his career as an actor. For over a decade, he has lived in a five-storey Victorian conversion in Narrow Street, Limehouse. In 1978 he met his second partner, Sean Mathias, at the Edinburgh Festival. This relationship lasted until 1988. According to Mathias, the ten-year love affair was tempestuous, with conflicts over McKellen's success in acting versus Mathias's somewhat less-successful career. Mathias later directed McKellen in "Waiting For Godot" at the Theatre Royal Haymarket in 2009. The pair entered into a business partnership with Evgeny Lebedev, purchasing the lease on The Grapes public house in Narrow Street.
2001 : Ian McKellen receives the Artist Citizen of the World Award (France) 
In the late 1980s, McKellen lost his appetite for meat except for fish, and so mostly excludes it from his diet.
He has a tattoo of the Elvish number nine, written using Tengwar, on his shoulder in reference to his involvement in the "Lord of the Rings" and the fact that his character was one of the original nine companions of the Fellowship of the Ring. The other actors of "The Fellowship" (Elijah Wood, Sean Astin, Orlando Bloom, Billy Boyd, Sean Bean, Dominic Monaghan and Viggo Mortensen) have the same tattoo. John Rhys-Davies, whose character was also one of the original nine companions, arranged for his stunt double to get the tattoo instead.
He was diagnosed with prostate cancer in 2006. In 2012, McKellen stated on his blog that "[t]here is no cause for alarm. I am examined regularly and the cancer is contained. I've not needed any treatment."
He became an ordained minister of the Universal Life Church in early 2013 in order to preside over the marriage of his X-Men co-star Patrick Stewart to his then fiancée Sunny Ozell. McKellen was awarded an honorary degree by Cambridge University on 18 June 2014, becoming a Doctor of Letters.
He was made a Freeman of the city of London on Thursday 30 October 2014. The ceremony took place at Guildhall in London. McKellen was nominated by London's Lord Mayor Fiona Woolf, who said he was chosen as he was an “exceptional actor” and “tireless campaigner for equality”.
Activism.
LGBT rights campaigning.
While McKellen had made his sexual orientation known to his fellow actors early on in his stage career, it was not until 1988 that he came out to the general public, in a programme on BBC Radio 3. The context that prompted McKellen's decision — overriding any concerns about a possible negative effect on his career — was that the controversial Section 28 of the Local Government Bill, simply known as Section 28, was under consideration in the British Parliament. Section 28 proposed to prohibit local authorities from promoting homosexuality "... as a kind of pretended family relationship". McKellen became active in fighting the proposed law, and declared himself gay on a BBC Radio programme where he debated the subject of Section 28 with the conservative journalist Peregrine Worsthorne. McKellen has stated that he was influenced in his decision by the advice and support of his friends, among them noted gay author Armistead Maupin. In a 1998 interview that discusses the 29th anniversary of the Stonewall riots, McKellen commented, I have many regrets about not having come out earlier, but one of them might be that I didn't engage myself in the politicking. He has said of this period: My own participating in that campaign was a focus for people [to] take comfort that if Ian McKellen was on board for this, perhaps it would be all right for other people to be as well, gay and straight. Section 28 was, however, enacted and remained on the statute books until 2000 in Scotland and 2003 in England and Wales. Section 28 never applied in Northern Ireland.
In 2003, during an appearance on "Have I Got News For You", McKellen claimed when he visited Michael Howard, then Environment Secretary (responsible for local government), in 1988 to lobby against Section 28, Howard refused to change his position but did ask him to leave an autograph for his children. McKellen agreed, but wrote, "Fuck off, I'm gay." McKellen described Howard's junior ministers, Conservatives David Wilshire and Dame Jill Knight, who were the architects of Section 28, as the 'ugly sisters' of a political pantomime.
McKellen has continued to be very active in LGBT rights efforts. In a statement on his website regarding his activism, the actor commented that:
I have been reluctant to lobby on other issues I most care about – nuclear weapons (against), religion (atheist), capital punishment (anti), AIDS (fund-raiser) because I never want to be forever spouting, diluting the impact of addressing my most urgent concern; legal and social equality for gay people worldwide.
McKellen is a co-founder of Stonewall, a LGB rights lobby group in the United Kingdom, named after the Stonewall riots. McKellen is also patron of LGBT History Month, Pride London, GAY-GLOS, The Lesbian & Gay Foundation, and FFLAG where he appears in their video "Parents Talking".
In 1994, at the closing ceremony of the Gay Games, he briefly took the stage to address the crowd, saying, "I'm Sir Ian McKellen, but you can call me Serena": This nickname, given to him by Stephen Fry, had been circulating within the gay community since McKellen's knighthood was conferred. In 2002, he was the Celebrity Grand Marshal of the San Francisco Pride Parade and he attended the Academy Awards with his then-boyfriend, New Zealander Nick Cuthell. In 2006, McKellen spoke at the pre-launch of the 2007 LGBT History Month in the UK, lending his support to the organisation and its founder, Sue Sanders. In 2007, he became a patron of The Albert Kennedy Trust, an organisation that provides support to young, homeless and troubled LGBT people.
In 2006, he became a patron of Oxford Pride, stating:I send my love to all members of Oxford Pride, their sponsors and supporters, of which I am proud to be one... Onlookers can be impressed by our confidence and determination to be ourselves and gay people, of whatever age, can be comforted by the occasion to take the first steps towards coming out and leaving the closet forever behind.
McKellen has taken his activism internationally, and caused a major stir in Singapore, where he was invited to do an interview on a morning show and shocked the interviewer by asking if they could recommend him a gay bar; the programme immediately ended. In December 2008, he was named in "Out"'s annual Out 100 list.
In 2010, McKellen extended his support for Liverpool's Homotopia festival in which a group of gay and lesbian Merseyside teenagers helped to produce an anti-homophobia campaign pack for schools and youth centres across the city. In May 2011, he called Sergey Sobyanin, Moscow's mayor, a "coward" for refusing to allow gay parades in the city.
In 2014, he was named in the top 10 on the World Pride Power list.
Charity work.
In April 2010, along with actors Brian Cox and Eleanor Bron, McKellen appeared in a series of TV advertisements to support Age UK, the charity recently formed from the merger of Age Concern and Help the Aged. All three actors gave their time free of charge.
A cricket fan since childhood, McKellen umpired in March 2011 for a charity cricket match in New Zealand to support earthquake victims of the February 2011 Christchurch earthquake.
McKellen is an honorary board member for the New York and Washington, DC based organization Only Make Believe. Only Make Believe creates and performs interactive plays in children's hospitals and care facilities. He was honoured by the organisation in 2012 and hosted their annual Make Believe on Broadway Gala in November 2013. He garnered publicity for the organisation by stripping down to his Lord of the Rings underwear on stage.
Isaac Theatre Royal, Christchurch.
While in New Zealand filming "The Hobbit" in 2012, McKellen announced a special New Zealand tour 'Shakespeare, Tolkien, and You!', with proceeds from the shows going to help save the Isaac Theatre Royal, which suffered extensive damage during the 2011 Christchurch earthquake. McKellen said he opted to help save the building as it was the last theatre he played in New Zealand ("Waiting For Godot" in 2010) and the locals' love for it made it a place worth supporting.
Selected credits.
Other work.
A friend of actor Ian Charleson and an admirer of his work, McKellen contributed an entire chapter to the 1990 book, "For Ian Charleson: A Tribute". A recording of McKellen's voice is heard before performances at the Royal Festival Hall, reminding patrons to ensure their mobile phones and watch alarms are switched off, and to keep coughing to a minimum. He also took part in the 2012 Summer Paralympics opening ceremony as Prospero.
References.
Notes
Sources.
</dl>

</doc>
<doc id="15309" url="http://en.wikipedia.org/wiki?curid=15309" title="Intellivision">
Intellivision

The Intellivision is a home video game console released by Mattel in 1979. Development of the console began in 1978, less than a year after the introduction of its main competitor, the Atari 2600. The word "intellivision" is a portmanteau of "intelligent television". Over 3 million Intellivision units were sold and a total of 125 games were released for the console.
In 2009, video game website IGN named the Intellivision the No. 14 greatest video game console of all time. It remained Mattel's only video game console until the release of the HyperScan in 2006.
History and development.
The Intellivision was developed by Mattel Electronics, a subsidiary of Mattel formed expressly for the development of electronic games. The console was test marketed in Fresno, California, in 1979 with a total of four games available, and was released nationwide in 1980 with a price tag of US$299 and a pack-in game: "Las Vegas Poker & Blackjack". Though not the first system to challenge Atari, it was the first to pose a serious threat to Atari's dominance. A series of advertisements featuring George Plimpton were produced that demonstrated the superiority of the Intellivision's graphics and sound to those of the Atari 2600, using side-by-side game comparisons.
One of the slogans of the television advertisements stated that Intellivision was "the closest thing to the real thing"; one example in an advertisement compared golf games. The other console's games had a blip sound and cruder graphics, while the Intellivision featured a realistic swing sound and striking of the ball, and graphics that suggested a more 3D look. There was also an advertisement comparing the Atari 2600 to it, featuring the slogan "I didn't know".
Like Atari, Mattel marketed their console to a number of retailers as a rebadged unit. These models include the Radio Shack TandyVision, the GTE-Sylvania Intellivision, and the Sears Super Video Arcade. The Sears model was a specific coup for Mattel, as Sears was already selling a rebadged Atari 2600 unit, and in doing so made a big contribution to Atari's success.
In its first year, Mattel sold 175,000 Intellivision consoles, and the library grew to 35 games. At this time, all Intellivision games were developed by an outside firm, APh Technological Consulting. The company recognized that what had been seen as a secondary product line might be a big business. Realizing that potential profits are much greater with first party software, Mattel formed its own in-house software development group.
The original five members of that Intellivision team were manager Gabriel Baum, Don Daglow, Rick Levine, Mike Minkoff and John Sohl. Levine and Minkoff, a long-time Mattel Toys veteran, both came over from the hand-held Mattel games engineering team. To keep these programmers from being hired away by rival Atari, their identity and work location was kept a closely guarded secret. In public, the programmers were referred to collectively as the Blue Sky Rangers.
By 1982, sales were soaring. Over two million Intellivision consoles had been sold by the end of the year, earning Mattel a $100,000,000 profit. Third-party Atari developers Activision, and Imagic began releasing games for the Intellivision, as did hardware rival Coleco. Mattel created "M Network" branded games for Atari and Coleco's systems. The most popular titles sold over a million units each. The Intellivision was also introduced in Japan by Bandai in 1982.
The original 5-person Mattel game development team had grown to 110 people under now-Vice President Baum, while Daglow led Intellivision development and top engineer Minkoff directed all work on all other platforms.
Keyboard Component.
Intellivision's packaging and promotional materials, as well as television commercials, promised that with the addition of a soon-to-be-available accessory called the "Keyboard Component", originally portrayed in TV ads as a larger box with an opening in the top that the Intellivision fit into. This would turn the Intellivision into a fully functional home computer system.
The unit would bring the system's available RAM up to a full 64kB, a large amount for the time, and would have provided both a built-in cassette drive for data storage and a connection for an optional 40-column thermal printer. The cassette drive would be able to provide both data storage and an audio track simultaneously, allowing for interactive audio recording and playback under computer control, and a secondary 6502 microprocessor inside the Keyboard Component would be programmed to handle all of these extra capabilities independently of the Intellivision's CP1610 CPU. The unit would even provide an extra cartridge slot, allowing the original Intellivision to remain permanently docked with the Keyboard Component while still being able to play standard game cartridges.
Unfortunately, while the Keyboard Component was an ambitious piece of engineering for its time, it suffered from reliability problems and proved to be expensive to produce. Originally slated to be available in 1981, the Keyboard Component was repeatedly delayed as the engineers tried to find ways to overcome the reliability issues and reduce manufacturing costs.
The Keyboard Component's repeated delays became so notorious around Mattel headquarters that comedian Jay Leno, when performing at Mattel's 1981 Christmas party, got his biggest titter of the evening with the line: "You know what the three big lies are, don't you? 'The check is in the mail,' 'I'll still respect you in the morning,' and 'The Keyboard will be out in spring.'"
Complaints from consumers who had chosen to buy the Intellivision specifically on the promise of a "Coming Soon!" personal-computer upgrade that seemed as if it would never materialize eventually caught the attention of the Federal Trade Commission (FTC), who started investigating Mattel Electronics for fraud and false advertising. Mattel said that the Keyboard Component was a real product still being test-marketed and even released a small number of Keyboard Components to a handful of retail stores, along with a handful of software titles in order to support this claim. The FTC eventually ordered Mattel to pay a $10,000/day fine until the promised computer upgrade was in full retail distribution. To protect themselves from the ongoing fines, the Keyboard Component was officially canceled in the fall of 1982 and the Entertainment Computer System (ECS) module offered up in its place.
While approximately four thousand Keyboard Components were manufactured before the module was canceled and recalled, it is not clear how many of them actually found their way into the hands of Intellivision customers. Today, very few of them still exist; when the Keyboard Component was officially canceled, part of Mattel's settlement with the FTC involved offering to buy back all of the existing Keyboard Components from dissatisfied customers. Any customer who opted to keep theirs was required to sign a waiver indicating their understanding that no more software would be written for the system and which absolved Intellivision of any future responsibility for technical support. Several of the units were later used by Mattel Electronics engineers when it was discovered that, with a few minor modifications, a Keyboard Component could be used as an Intellivision software-development system in place of the original hand-built development boards.
The Keyboard Component debacle was ranked as #11 on GameSpy's 25 Dumbest Moments in Gaming.
Entertainment Computer System (ECS).
In mid-1981, Mattel's upper management was becoming concerned that the Keyboard Component division would never be able to produce a sellable product. As a result, Mattel Electronics set up a competing internal engineering team whose stated mission was to produce an inexpensive add-on called the BASIC Development System, or BDS, to be sold as an educational device to introduce kids to the concepts of computer programming.
The rival BDS engineering group, who had to keep the project's real purpose a secret among themselves, fearing that if David Chandler, the head of the Keyboard Component team, found out about it he would use his influence to get the project killed, eventually came up with a much less expensive alternative. Originally dubbed the Lucky, from LUCKI: Low User-Cost Keyboard Interface, it lacked many of the sophisticated features envisioned for the original Keyboard Component. Gone, for example, was the full 64kB of RAM and the secondary 6502 CPU; instead, the ECS offered a mere 2kB RAM expansion, a built-in BASIC that was marginally functional, plus a much-simplified cassette and thermal-printer interface.
Ultimately, this fulfilled the original promises of turning the Intellivision into a computer, making it possible to write programs and store them to tape, and interfacing with a printer well enough to allow Mattel to claim that they had delivered the promised computer upgrade and stop the FTC's mounting fines. It even offered, via an additional AY-3-8910 sound chip inside the ECS module and an optional 49-key Music Synthesizer keyboard, the possibility of turning the Intellivision into a multi-voice synthesizer which could be used to play or learn music.
In the fall of 1982, the LUCKI, now renamed the Entertainment Computer System (ECS), was presented at the annual sales meeting, officially ending the ill-fated Keyboard Component project. A new advertising campaign was aired in time for the 1982 Christmas season, and the ECS itself was shown to the public at the January 1983 Consumer Electronic Show (CES) in Las Vegas at the Las Vegas Convention Center. A few months later, the ECS hit the market, and the FTC agreed to drop the $10K/day fines.
Unfortunately, by the time the ECS made its retail debut, an internal shake-up at the top levels of Mattel Electronics' management had caused the company's focus to shift away from hardware add-ons in favor of software, and the ECS received very little further marketing push. Further hardware developments, including a planned Program Expander that would have added another 16K of RAM and a more intricate, fully featured Extended-BASIC to the system, were halted, and in the end less than a dozen software titles were released for the ECS.
Intellivoice.
In 1982, Mattel introduced a new peripheral for the Intellivision: The Intellivoice, a voice synthesis device which produces speech when used with certain games. The Intellivoice was original in two respects: not only was this capability unique to the Intellivision system at the time (although a similar device was available for the Odyssey²), but the speech-supporting games written for Intellivoice actually made the speech an integral part of the gameplay.
Unfortunately, the amount of speech that could be compressed into a 4K or 8K ROM cartridge was limited, and the system did not sell as well as Mattel had hoped; while the initial orders were as high as 300,000 units for the Intellivoice module and its initial game-cartridge offerings, interest in future titles dropped rapidly until the fourth and last Intellivoice title, "", sold a mere 90,000 units. A fifth game, a children's title called "Magic Carousel", was shelved, and in August 1983 the Intellivoice system was quietly phased out.
The four titles available for the Intellivoice system, in order of their release, were:
A fifth title, "Intellivision World Series Major League Baseball", developed as part of the Entertainment Computer System series, also supports the Intellivoice if both the ECS and Intellivoice are connected concurrently. Unlike the Intellivoice-specific games, however, "World Series Major League Baseball" is also playable without the Intellivoice module (but not without the ECS.)
A further homebrew title, "Space Hunt", also uses the male Intellivoice sounds (especially on its main title screen). This game is a spin-off clone of "Astrosmash", which uses graphics loaned from "Utopia" and the "TRON" game series.
Intellivision II.
In addition to the ECS module, 1982 also saw the introduction of a redesigned model, called the Intellivision II (featuring detachable controllers and sleeker case), the System Changer (which played Atari 2600 games on the Intellivision II), and a music keyboard add-on for the ECS.
Like the ECS, Intellivision II was designed first and foremost to be inexpensive to manufacture. Among other things, the raised bubble keypad of the original hand controller was replaced by a flat membrane keyboard surface. However, because many Intellivision games had been designed for users to play by feeling the buttons without looking down, some of these games were far less playable on Intellivision II.
Instead of an internal power supply like the original system had, the Intellivision II would use an external AC adapter. Its main drawback, however, was that it was a non-standard power supply — running on 16.2V — meant that if the AC adapter was lost or damaged, the system could be rendered useless, as replacement power supplies for that particular voltage requirement were not readily available. It is unknown whether Intellivision II AC adapters were sold separately.
Mattel also changed the Intellivision II's internal ROM program (called the EXEC) in an attempt to lock out unlicensed 3rd party titles. To make room for the lock-out code while retaining compatibility with existing titles, some portions of the EXEC code were moved in a way that changed their timing. While most games were unaffected, a couple of the more popular titles, "Shark! Shark!", and "Space Spartans", had certain sound effects that the Intellivision II reproduced differently than intended, although the games remained playable. "Electric Company Word Fun" did not run at all and INTV's later release "Super Pro Football" has minor display glitches at the start, both due to the modified EXEC. Mattel's attempt to lock out competitors' software titles was only temporarily successful, as the 3rd-party game manufacturers quickly figured out how to get around it.
Intellivision III.
Mattel planned to release Intellivision III, a more powerful console with a price above $200, for Christmas 1983. The company canceled the console after the ColecoVision beat the Atari 5200 in the market for higher-performance consoles, and after home computers became as inexpensive as game consoles.
Competition and market crash.
Amid the flurry of new hardware, there was trouble for the Intellivision. New game systems (ColecoVision, Emerson Arcadia 2001, Atari 5200, and Vectrex, all in 1982) were further subdividing the market, and the video game crash put pressure on the entire industry. The Intellivision team rushed to finish a major new round of games, including "BurgerTime" and the ultra-secret 3D glasses game "Hover Force". Although "BurgerTime" was a popular game on the Intellivision and was programmed by Blue Sky Ranger Ray Kaestner in record time, the five-month manufacturing cycle meant that the game did not appear until the late spring of 1983, after the video game crash had severely damaged game sales.
In the spring of 1983, Mattel went from aggressively hiring game programmers to laying them off within a two-week period. By August, there were massive layoffs, and the price of the Intellivision II (which launched at $150 earlier that year) was lowered to $69. Mattel Electronics posted a $300 million loss. Early in 1984, the division was closed — the first high-profile victim of the crash.
Intellivision game sales continued when a liquidator purchased all rights to the Intellivision and its software from Mattel, as well as all remaining inventory. After much of the existing software inventory had been sold, former Mattel Marketing executive Terry Valeski bought all rights to Intellivision and started a new venture. The new company, INTV Corp., continued to sell old stock via retail and mail order. When the old stock of Intellivision II consoles ran out, they introduced a new console dubbed INTV System III. This unit was actually a cosmetic rebadge of the original Intellivision console (this unit was later renamed the Super Pro System.) In addition to manufacturing new consoles, INTV Corp. also continued to develop new games, releasing a few new titles each year. Eventually, the system was discontinued in 1991.
Rereleases.
Intellivision games became readily available again when Keith Robinson, an early Intellivision programmer responsible for the game "TRON: Solar Sailer", purchased the software rights and founded a new company, Intellivision Productions. As a result, games originally designed for the Intellivision are available on PCs and modern-day consoles including the PlayStation 2, Xbox, and Nintendo GameCube in the "Intellivision Lives!" package, though all are now out of print at retail. However, the Xbox version is available for purchase as a downloadable game through Xbox Live Game Marketplace's Xbox Originals service for the Xbox 360. VH1 Classic and MTV Networks released 6 Intellivision games to iOS. A few licensed Intellivision games are available through the GameTap subscription gaming service. Also, several LCD handheld and direct-to-TV games have been released in recent years.
On March 24, 2010, Microsoft launched the Game Room service for Xbox Live and Games for Windows Live. This service includes support for Intellivision titles and allows players to compete against one another for high scores via online leaderboards. At the 2011 Consumer Electronics Show, Microsoft announced a version of Game Room for Windows Phone, promising a catalog of 44 Intellivision titles.
On October 1, 2014, AtGames Digital Media, Inc., under license from Intellivision Productions, Inc., released the Intellivision Flashback Classic Console, a miniature sized Intellivision console with two original sized controllers. It comes with 60 Intellivision games built into ROM.
Reviews and game guides.
Ken Uston published "Ken Uston's Guide to Buying and Beating the Home Video Games" in 1982 as a guide to potential buyers of console systems/cartridges, as well as a brief strategy guide to numerous cartridge games then in existence. He described Intellivision as "the most mechanically reliable of the systems… The controller (used during "many hours of experimentation") worked with perfect consistency. The unit never had overheating problems, nor were loose wires or other connections encountered." However, Uston rated the controls and control system as "below average" and the worst of the consoles he tested (including Atari 2600, Magnavox Odyssey², Astrovision, and Fairchild Channel F).
Jeff Rovin lists "Intellivision" as one of the seven major suppliers of videogames in 1982, and mentions it as "the unchallenged king of graphics", however stating that the controllers can be "difficult to operate", the fact that if a controller breaks the entire unit must be shipped off for repairs (since they did not detach at first), and that the overlays "are sometimes so stubborn as to tempt one's patience" .
Technical specifications.
Game controller.
The Intellivision controller featured:
The controller was ranked the fourth worst video game controller by IGN editor Craig Harris.

</doc>
<doc id="15316" url="http://en.wikipedia.org/wiki?curid=15316" title="Imperialism">
Imperialism

Imperialism is a type of advocacy of Empire. Its name originated from the Latin word "imperium", meaning to rule over large territories. Imperialism is "a policy of extending a country's power and influence through colonization, use of military force, or other means". Imperialism has greatly shaped the contemporary world.
The term imperialism has been applied to Western political and economic dominance in the 19th and 20th centuries, however its precise meaning continues to be debated by scholars. For example, cartographers of the nineteenth century used cartography to further fuel imperialism. As scholar Bassett notes, "Maps were used in various ways to extend European hegemony over foreign and often unknown territory." It is better to use terms such as cultural or economic imperialism to describe some of these less formal types of domination. Some writers, such as Edward Said, use the term more broadly to describe any system of domination and subordination organised with an imperial center and a periphery. From a Marxist perspective, imperialism is a natural feature of a developed capitalist nation state as it matures into monopoly capitalism. In Lenin's work "Imperialism, the Highest Stage of Capitalism", he observed that as capitalism matured in the Western world, economies shifted away from manufacturing towards banking, finance, and capital markets, as production was outsourced to the empires' colonies. Lenin concluded that competition between Empire and the unfettered drive to maximize profit would lead to wars between the empires themselves, such as the contemporary First World War, as well as continued future military interventions and occupations in the colonies to establish, expand, and exploit less developed markets for the monopolist corporations of the empires.
Imperialism is defined as "an unequal human and territorial relationship, usually in the form of an empire, based on ideas of superiority and practices of dominance, and involving the extension of authority and control of one state or people over another." Imperialism is a process and ideology that does not only focus on political dominance, but rather, conquest over expansion. Imperialism is particularly focused on the control that one group, often a state power, has on another group of people. There are "formal" or "informal" imperialism. "Formal imperialism" is, "the physical control or full-fledged colonial rule". "Informal control" is less direct, however; it is still a powerful form of dominance. 
There are three waves of imperialism; Americas (North, South and the Caribbean), Asia and Africa. From the fifteenth century forward, Spain and Portugal were responsible for colonizing South America. Both Spain and Portugal were soon followed by the British, French and Dutch, who gained territory in North America. Britain, with the support from the East India Company, colonized Asia. Portugal, Netherlands and France also had Asian colonial possessions. The third wave, Africa, was described as "New Imperialism". This was structured by the "Berlin Conference (1884–85), which involved the main European powers and served to divide Africa between them".
The definition of imperialism has not been finalized for centuries and was confusedly seen to represent the policies of European colonial powers, or of the United States, or of any allegedly expansionist power or simply, general-purpose aggressiveness. This confusion was caused by Lenin's theory (Leninism), which was widely persuasive. Further on, some writers used the term imperialism, in slightly more discriminating fashion, to mean all kinds of domination or control by a group of people over another. To clear out this confusion about the definition of imperialism, one could speak of "formal" and "informal" imperialism. The first meaning physical control or "full-fledged colonial rule", while the second implied less direct rule though still contains perceivable kinds of dominance. Informal rule is generally less costly than taking over territories formally. This is because, with informal rule, the control is spread more subtly through technological superiority, enforcing land officials into large debts that cannot be repaid, ownership of private industries thus expanding the controlled area, or having countries agree to uneven trade agreements forcefully. Some examples of formal imperialism are: British rule of the colonies in America before 1776, India (1858 – 1947), Hong Kong (1842 – 1997), and Kenya (1920 – 1963). Some examples of informal imperialism are: Britain's 19th-century hegemony in Chile and Iraq, or the owning of more than 70 percent of banana companies in Guatemala by the USA.
It is mostly accepted that modern-day colonialism is an expression of imperialism and cannot exist without the latter. The extent to which "informal" imperialism with no formal colonies is properly described remains a controversial topic amongst historians. Both colonisation and imperialism have been described by Tom Nairn and Paul James as early forms of globalization:
Even if a particular empire does not have a "global reach" as we would define it today, empires by their nature still tend to contribute to processes of globalization because of the way that imperial power tends to generate counter-power at its edge-lands and send out reverberations far beyond the territories of their immediate control.
The word imperialism became common in the United Kingdom during the 1870s and was used with a negative connotation. In Great Britain, the word had until then mostly been used to refer to the politics of Napoleon III in obtaining favorable public opinion in France through foreign military interventions.
Colonialism vs. imperialism.
"The word ‘empire’ comes from the Latin word imperium; for which the closest modern English equivalent would perhaps be ‘sovereignty’, or simply ‘rule’" (Howe, 13) The greatest distinction of an empire is through the amount of land that a nation has conquered and expanded. Political power grew from conquering land, however cultural and economic aspects flourished through sea and trade routes. Europe and the United States of America had controlled over eighty percent of the globes land area along with holding important seaports at their possession. A distinction about empires is "that although political empires were built mostly by expansion overland, economic and cultural influences spread at least as much by sea". Some of the main aspects of trade that went overseas consisted of animals and plant products. European empires in Asia and Africa "have come to be seen as the classic forms of imperialism: and indeed most books on the subject confine themselves to the European seaborne empires". European expansion caused the world to be divided by how developed and developing nation are portrayed through the world systems theory. The two main regions are the core and the periphery. The core consists of high areas of income and profit; the periphery is on the opposing side of the spectrum consisting of areas of low income and profit. These critical theories of Geo-politics have led to increased discussion of the meaning and impact of imperialism on the modern post-colonial world. The Russian leader Lenin suggested that "imperialism was the highest form of capitalism, claiming that imperialism developed after colonialism, and was distinguished from colonialism by monopoly capitalism". This idea from Lenin stresses how important new political world order has become in out modern era. Geopolitics now focuses on states becoming major economic players in the market; some states today are viewed as empires due to their political and economic authority over other nations.
The term "imperialism" is often conflated with "colonialism", however many scholars have argued that each have their own distinct definition. Imperialism and colonialism have been used in order to describe one's superiority, domination and influence upon a person or group of people. Robert Young writes that while imperialism operates from the center, is a state policy and is developed for ideological as well as financial reasons, colonialism is simply the development for settlement or commercial intentions. Colonialism in modern usage also tends to imply a degree of geographic separation between the colony and the imperial power. Particularly, Edward Said distinguishes the difference between imperialism and colonialism by stating; "imperialism involved 'the practice, the theory and the attitudes of a dominating metropolitan center ruling a distant territory', while colonialism refers to the 'implanting of settlements on a distant territory.' Contiguous land empires such as the Russian or Ottoman are generally excluded from discussions of colonialism.:116 Thus it can be said that imperialism includes some form of colonialism, but colonialism itself does not automatically imply imperialism, as it lacks a political focus. 
Imperialism and colonialism both dictate that the political and economic advantage over a land and the indigenous populations they control, scholars sometimes find it difficult to illustrate the difference between the two. Although imperialism and colonialism focus on the suppression of one another, if Colonialism refers to the process of a country taking physical control of another, Imperialism refers to the political and monetary dominance, either formally or informally. Colonialism is seen to be the architect deciding how to start dominating areas and then imperialism can be seen as creating the idea behind conquest cooperating with colonialism. Colonialism is when the imperial nation begins a conquest over an area and then eventually is able to rule over the areas the previous nation had controlled. Colonialism's core meaning is the exploitation of the valuable assets and supplies of the nation that was conquered and the conquering nation then gaining the benefits from the spoils of the war. The meaning of imperialism is to create an empire, by conquering the other state's lands and therefore increasing its own dominance. Colonialism is the builder and preserver of the colonial possessions in an area by a population coming from a foreign region. Colonialism can completely change the existing social structure, physical structure and economics of an area; it’s not unusual that the characteristics of the conquering peoples are inherited by the conquered indigenous populations.
The greatest distinction of an empire is through the amount of land that a nation has conquered and expanded. Political power grew from conquering land, however cultural and economic aspects flourished through sea and trade routes. Europe and the United States of America had controlled over eighty percent of the globes land area along with holding important seaports at their possession. Some of the main aspects of trade that went overseas consisted of animals and plant products. European expansion caused the world to be divided by how developed and developing nation are portrayed through the world systems theory. The two main regions are the core and the periphery. The core consists of areas of high income and profit; the periphery is on the opposing side of the spectrum consisting of areas of low income and profit. Post colonialism and imperialism bring forward a new stronger idea of geopolitics. Geopolitics now focuses on states becoming major economic players in the market, some states today are viewed as empires due to their political and economic authority over other nations.
Justification.
A controversial aspect of imperialism is the defense and justification of empire-building based on seemingly rational grounds. J. A. Hobson identifies this justification on general grounds as: "It is desirable that the earth should be peopled, governed, and developed, as far as possible, by the races which can do this work best, i.e. by the races of highest 'social efficiency'". Many others argued that imperialism is justified for several different reasons. Friedrich Ratzel believed that in order for a state to survive, imperialism was needed. Halford Mackinder felt that Great Britain needed to be one of the greatest imperialist and therefore justified imperialism. The rhetoric of colonizers being racially superior appears to have achieved its purpose, for example throughout Latin America "whiteness" is still prized today and various forms of blanqueamiento (whitening) are common.
Technology and economic efficiency were often improved in territories subjected to imperialism through the building of roads, other infrastructure and introduction of new technologies.
The principles of imperialism are often generalizable to the policies and practices of the British Empire "during the last generation, and proceeds rather by diagnosis than by historical description". British imperialism often used the concept of Terra nullius (Latin expression which stems from Roman law meaning 'empty land'). The country of Australia serves as a case study in relation to British settlement and colonial rule of the continent in the eighteenth century, as it was premised on "terra nullius", and its settlers considered it unused by its sparse Aboriginal inhabitants.
Imaginative Geographies and Orientalism.
Imperial control, both territorial and non-territorial, is justified through discourses that shape our understanding of different spaces. The concept of imaginative geographies explains how this understanding is limited by our attitudes and ideas which work to obscure the reality of these spaces .
Orientalism, as theorized by Edward Said, refers to how the West developed an imaginative geography of the East. This imaginative geography relies on an essentializing discourse that represents neither the diversity nor the social reality of the East. Rather, by essentializing the East, this discourse uses the idea of place-based identities to create difference and distance between "we" the West and "them" the East, or "here" in the West and "there" in the East. This difference was particularly apparent in textual and visual works of early European studies of the Orient that positioned the East as irrational and backward in opposition to the rational and progressive West. Defining the East as a negative vision of itself, as its inferior, not only increased the West’s sense of self, but also was a way of ordering the East and making it known to the West so that it could be dominated and controlled. The discourse of Orientalism therefore served as an ideological justification of early Western imperialism, as it formed a body of knowledge and ideas that rationalized social, cultural, political, and economic control of other territories. The ward orient has interesting connections in other languages. For Instance in Russian language the ward "orientir", that means landmark, has the same root as the English ward orient. 
The Cultural Imperialism was one of the major parts in E.Said's analysis. The Orientalism became a first part of the trilogy written by Said focusing on the idea of cultural imperialism. His ideas were briefly discussed in K.Morin's work called "Key Contemporary Thinkers on Space and Place". Said's focus on cultural imperialism is described as "showing how the political or economic or administrative fact relies on this legitimating discourse". Another major work that fallowed the discourse on Orientalism and provided more insights on Imperialism came out in 1993 with the title Culture and Imperialism. Under such name, Said wrote a series of essays that introduced the number of ideas or "intellectual conundrums" (riddles) risen out of Orientalist views, that the thinkers and concept makers of the twentieth century continued to argue about. The contradictions in his own work were criticized by another authors, however mostly supporting his main arguments. In his work on Cultural Imperialism Said had chosen to compare the influence of imperialist ideas in different contexts, in other words to look at "cultural imperialism across humanism, Marxism and post-structuralism". 
It is important to note that Orientalism as a discourse was not only influential in the past, but is still present today, shaping the production of images in the West that are used to represent the East. The way in which the East is performed for the West through these images is used to justify the ongoing division of the world between the East and the West and contemporary Western imperialism.
Geography as a justification for imperialism. "The end of the 19th century and the early part of the 20th witnessed the rise of "environmental determinism", an approach that regarded human beings or human society as being the product of the environment within which they lived." Environmental determinism was used as a justification for imperial practices. As European climates were seen to produce a moral, hard working human being, while tropical climates were said to produce a morally degenerate, sexually promiscuous, lazy species. This racial hierarchy was used to justify continued exploitation of lesser peoples.
Cartography.
One of the main tools used by imperialists was cartography. Cartography is "the art, science and technology of making maps" but this definition is problematic. It implies that maps are objective representations of the world when in reality they serve very political means. For Harley, maps serve as an example of Foucault’s power and knowledge concept.
To better illustrate this idea, Bassett focuses his analysis of the role of nineteenth-century maps during the "scramble for Africa". He states that maps "contributed to empire by promoting, assisting, and legitimizing the extension of French and British power into West Africa". During his analysis of nineteenth-century cartographic techniques, he highlights the use of blank space to denote unknown or unexplored territory. This provided incentives for imperial and colonial powers to obtain "information to fill in blank spaces on contemporary maps". It also encouraged empire building as countries were in competition with one another to see who could fill in the blank spaces first.
Although cartographic processes advanced through imperialism, further analysis of their progress reveals many biases linked to eurocentrism. According to Bassett, "[n]ineteenth-century explorers commonly requested Africans to sketch maps of unknown areas on the ground. Many of those maps were highly regarded for their accuracy" but were not printed in Europe unless Europeans verified them.
History.
Imperialism has played a part in the histories of Japan, the Assyrian Empire, the Chinese Empire, the Roman Empire, Greece, the Byzantine Empire, the Persian Empire, the Ottoman Empire, Ancient Egypt, the British Empire, India, and many other empires. Imperialism was a basic component to the conquests of Genghis Khan during the Mongol Empire, and of other war-lords. Historically recognized Muslim empires number in the dozens. Sub-Saharan Africa has also featured dozens of empires that predate the European colonial era, for example the Ethiopian Empire, Oyo Empire, Asante Union, Luba Empire, Lunda Empire, and Mutapa Empire. The Americas during the pre-Columbian era also had large empires such as the Aztec Empire and the Incan Empire.
Although normally used to imply forcible imposition of a foreign government's control over another country or over conquered territory that was previously without a unified government, "imperialism" is sometimes used to describe loose or indirect political or economic influence on weak states by more powerful ones.
Cultural imperialism is when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another. This is more than just "foreign" music, television or film becoming popular with young people, but that popular culture changing their own expectations of life and their desire for their own country to become more like the foreign country depicted. For example depictions of opulent American lifestyles in the soap opera Dallas during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama series in North Korea. The importance of soft power is not lost on authoritarian regimes, fighting such influence with bans on foreign popular culture, control of the internet and unauthorised satellite dishes e.t.c.. Nor is such a usage of culture recent, as part of Roman imperialism local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants to their own annexation.
Imperialism has been subject to moral or immoral censure by its critics, and thus the term is frequently used in international propaganda as a pejorative for expansionist and aggressive foreign policy.
Age of Imperialism.
The Age of Imperialism, a time period beginning around 1700, saw (generally European) industrializing nations engaging in the process of colonizing, influencing, and annexing other parts of the world in order to gain political power. Although imperialist practices have existed for thousands of years, the term "Age of Imperialism" generally refers to the activities of European powers from the early 18th century through to the middle of the 20th century, for example, the "The Great Game" in Persian lands, the "Scramble for Africa" and the "Open Door Policy" in China.
During the 20th century, historians John Gallagher (1919-1980) and Ronald Robinson (1920-1999) constructed a framework for understanding European imperialism. They claim that European imperialism was influential, and Europeans rejected the notion that "imperialism" required formal, legal control by one government over another country. "In their view, historians have been mesmerized by formal empire and maps of the world with regions colored red. The bulk of British emigration, trade, and capital went to areas outside the formal British Empire. Key to their thinking is the idea of empire 'informally if possible and formally if necessary.'" Because of the resources made available by imperialism, the world's economy grew significantly and became much more interconnected in the decades before World War I, making the many imperial powers rich and prosperous.
Europe's expansion into territorial imperialism was largely focused on economic growth by collecting resources from colonies, in combination with assuming political control by military and political means. The colonization of India in the mid-18th century offers an example of this focus: there, the "British exploited the political weakness of the Mughal state, and, while military activity was important at various times, the economic and administrative incorporation of local elites was also of crucial significance" for the establishment of control over the subcontinent's resources, markets, and manpower. Although a substantial number of colonies had been designed to provide economic profit and to ship resources to home ports (mostly through the seventeenth and eighteenth centuries), Fieldhouse suggests that in the nineteenth and twentieth centuries in places such as Africa and Asia, this idea is not necessarily valid:
Modern empires were not artificially constructed economic machines. The second expansion of Europe was a complex historical process in which political, social and emotional forces in Europe and on the periphery were more influential than calculated imperialism. Individual colonies might serve an economic purpose; collectively no empire had any definable function, economic or otherwise. Empires represented only a particular phase in the ever-changing relationship of Europe with the rest of the world: analogies with industrial systems or investment in real estate were simply misleading.
During this time, European merchants had the ability to "roam the high seas and appropriate surpluses from around the world (sometimes peaceably, sometimes violently) and to concentrate them in Europe".
European expansion greatly accelerated in the 19th century. To obtain raw materials, Europe expanded imports from other countries and from the colonies. European industrialists sought raw materials such as dyes, cotton, vegetable oils, and metal ores from overseas. Concurrently, industrialization was quickly making Europe the center of manufacturing and economic growth, driving resource needs.
Communication became much more advanced during European expansion. With the invention of railroads and telegraphs, it became easier to communicate with other countries and to extend the administrative control of a home nation over its colonies. Railroads and globalized shipping assisted in transporting massive amounts of goods to and from colonies.
Along with advancements in communication, Europe also continued to advance in military technology. European chemists made deadly explosives that could be used in combat, and with innovations in machinery they were able to manufacture improved firearms. By the 1880s, the machine gun had become an effective battlefield weapon. This technology gave European armies an advantage over their opponents, as armies in less-developed countries were still fighting with arrows, swords, and leather shields (e.g. the Zulus in Southern Africa during the Anglo-Zulu War of 1879).
Theories of imperialism.
In anglophone academic works, theories regarding imperialism are often based on the British experience. The term "Imperialism" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British prime Minister Benjamin Disraeli. It was shortly appropriated by supporters of "imperialism" such as Joseph Chamberlain. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed. Liberal John A. Hobson and Marxist Lenin added a more theoretical macroeconomic connotation to the term. Lenin in particular exerted substantial influence over later Marxist conceptions of imperialism with his work Imperialism, the Highest Stage of Capitalism. In his writings Lenin portrayed Imperialism as a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. This conception of imperialism as a structural feature of capitalism is echoed in by later Marxist theoreticians. Many theoreticians on the left have followed in emphasizing the structural or systemic character of "imperialism". Such writers have expanded the time period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect - among other shifts in sensibility - a growing unease, even squeamishness, with the fact of power, specifically, Western power.
The correlation between capitalism, aristocracy, and imperialism has long been debated among historians and political theorists. Much of the debate was pioneered by such theorists as J. A. Hobson (1858–1940), Joseph Schumpeter (1883–1950), Thorstein Veblen (1857–1929), and Norman Angell (1872–1967). While these non-Marxist writers were at their most prolific before World War I, they remained active in the interwar years. Their combined work informed the study of imperialism's impact on Europe, as well as contributed to reflections on the rise of the military-political complex in the United States from the 1950s. Hobson argued that domestic social reforms could cure the international disease of imperialism by removing its economic foundation. Hobson theorized that state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order. Conversely, should the state not intervene, rentiers (people who earn income from property or securities) would generate socially negative wealth that fostered imperialism and protectionism.
Environmental determinism.
The concept environmental determinism served as a moral justification for domination of certain territories and peoples. It was believed that a certain person's behaviours were determined by the environment in which they lived and thus validated their domination. For example, people living in tropical environments were seen as "less civilized" therefore justifying colonial control as a civilizing mission. Based on the three waves of European colonialism first in the Americas, second in Asia and lastly in Africa, Environmental determinism was used to categorically place indigenous people in a racial hierarchy.
If the world was split into climatic zones, it was explained that Northern Europe and the Mid-Atlantic climate produced a hard working, moral and upstanding human being. The Mediterranean climate which produced a lazy attitude, sexually promiscuous culture, as well a moral degeneracy. Finally the climates of Sub-Saharan Africa were said to produce a childlike species. One that needed guidance and European intervention to "help" them govern themselves, as they were seen as incapable of such feats.
Imperialism by country.
Britain.
Britain's imperialist ambitions can be seen as early as the fifteenth century. in 1599 the British East India Company was established and was chartered by Queen Elizabeth in the following year. With the establishment of trading posts in India, the British were able to maintain strength relative to others empires such as the Portuguese who already had set up trading posts in India. In 1767 political activity caused exploitation of the East India Company causing the plundering of the local economy, almost bringing the company into bankruptcy.
By the year 1670 Britain imperialist ambitions were well off as she had colonies in Virginia, Bermudas, Honduras, Antigua, Barbados, Jamaica and Nova scotia.
Due to the vast imperialist ambitions of European countries, Britain had several clashes with France. This competition was evident in the colonization of what is now known as Canada. John Cabot claimed Newfoundland for the British while the French established colonies along the St. Lawrence River and claiming it as "New France". It was clear that Britain had formed a strong state because she had a complex network of relations.
Britain continued to expand by colonizing countries such as New Zealand and Australia both of which were not empty land as they had their own locals and cultures. Britain's nationalistic movements were evident with the creation of the common wealth countries where there was a shared nature of national identity.
France.
The "First colonial empire", that existed until 1814, by which time most of it had been lost, and the "Second colonial empire", which began with the conquest of Algiers in 1830 and came for the most part to an end with the granting of independence to Algeria in 1962. The French history was marked by numerous wars, large and small, and also by significant help to France itself from the colonials in the world wars.
During the 16th century, the French colonization of the Americas began with the creation of New France. It was followed by the establishment of trading posts in Asia and Africa in the 17th century.
In the 19th and 20th centuries, it was the second-largest colonial empire in the world behind the British Empire, extending over 12,347,000 km² (4,767,000 sq. miles) at its height in the 1920s and 1930s. France controlled nearly 1/10th of the Earth's land area, with a population of 110 million people on the eve of World War II (5% of the world's population at the time).
France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa, as well as South-East Asia, with other conquests in Central and East Africa, as well as the South Pacific. Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars.
It became a moral justification to lift the world up to French standards by bringing Christianity and French culture. In 1884 the leading exponent of colonialism, Jules Ferry declared France had a civilising mission: "The higher races have a right over the lower races, they have a duty to civilize the inferior". Full citizenship rights – ‘’assimilation’’ – were offered, although in reality assimilation was always on the distant horizon. Contrasting from Britain, France sent small numbers of settlers to its colonies, with the only notable exception of Algeria, where French settlers nevertheless always remained a small minority.
In World War II, Charles de Gaulle and the Free French used the overseas colonies as bases from which they fought to liberate France. However after 1945 anti-colonial movements began to challenge the Empire. France fought and lost bitter wars in Vietnam and Algeria in the 1950's. Its settlers and many local supporters relocated to France. Nearly all of France's colonies gained independence by 1960, but France retained great financial and diplomatic influence. It has repeatedly sent troops to assist its former colonies in Africa in suppressing insurrections and coups d’état.
Germany.
From their original homelands in Scandinavia and northern Europe, Germanic tribes expanded throughout northern and western Europe in the middle period of classical antiquity; southern Europe in late antiquity, conquering Celtic and other peoples; and by 800 CE, forming the Holy Roman Empire, the first German Empire. However, there was no real systemic continuity from the Western Roman Empire to its German successor which was famously described as "not holy, not Roman, and not an empire", as a great number of small states and principalities existed in the loosely autonomous confederation. Although by 1000 CE, the Germanic conquest of central, western, and southern Europe (west of and including Italy) was complete, excluding only Muslim Iberia. There was, however, little cultural integration or national identity, and "Germany" remained largely a conceptual term referring to an amorphous area of central Europe.
Not a maritime power, and not a nation-state, as it would eventually become, Germany’s participation in Western imperialism was negligible until the late 19th century. The participation of Austria was primarily as a result of Habsburg control of the First Empire, the Spanish throne, and other royal houses. After the defeat of Napoleon, who caused the dissolution of that Holy Roman Empire, Prussia and the German states continued to stand aloof from imperialism, preferring to manipulate the European system through the Concert of Europe. After Prussia unified the other states into the second German Empire after the Franco-German War, its long-time Chancellor, Otto von Bismarck (1862–90), long opposed colonial acquisitions, arguing that the burden of obtaining, maintaining, and defending such possessions would outweigh any potential benefits. He felt that colonies did not pay for themselves, that the German bureaucratic system would not work well in the tropics and the diplomatic disputes over colonies would distract Germany from its central interest, Europe itself.
However, in 1883–84 Germany began to build a colonial empire in Africa and the South Pacific, before losing interest in imperialism. Historians have debated exactly why Germany made this sudden and short-lived move. Bismarck was aware that public opinion had started to demand colonies for reasons of German prestige. He was influenced by Hamburg merchants and traders, his neighbors at Friedrichsruh. The establishment of the German colonial empire proceeded smoothly, starting with German New Guinea in 1884.
After the Treaty of Versailles and the collapse of the Third Reich, and the failure of its attempt to create a great land empire in Eurasia, Germany was split between Western and Soviet spheres of influence until the fall of the Berlin Wall and the collapse of the Soviet Union.
Japan.
During the First Sino-Japanese War in 1894, Japan absorbed Taiwan. As a result of the Russo-Japanese War in 1905, Japan took part of Sakhalin Island from Russia. Korea was annexed in 1910. During World War I, Japan took German-leased territories in China’s Shandong Province, as well as the Mariana, Caroline, and Marshall Islands. In 1918, Japan occupied parts of far eastern Russia and parts of eastern Siberia as a participant in the Siberian Intervention. In 1931 Japan conquered Manchuria from China. During the Second Sino-Japanese War in 1937, Japan's military invaded central China and by the end of the Pacific War, Japan had conquered most of the Far East, including what is now Hong Kong, Vietnam, Cambodia, Thailand, Myanmar, the Philippine Islands, Indonesia, New Guinea and many islands of the Pacific Ocean. Its colonial ambitions were ended by the victory of the United States in the Second World War and the following treaties which remanded those territories to American administration or their original owners.
Soviet Union.
By the 18th century, the Russian Empire extended its control to the Pacific, forming a common border with the Qing Empire.
Bolshevik leaders had effectively reestablished a polity with roughly the same extent as that empire by 1921, however with an internationalist ideology: Lenin in particular asserted the right to limited self-determination for national minorities within the new territory. Beginning in 1923, the policy of "Indigenization" [korenizatsiia] was intended to support non-Russians develop their national cultures within a socialist framework. Never formally revoked, it stopped being implemented after 1932. After World War II, the Soviet Union installed socialist regimes modeled on those it had installed in 1919–20 in the old Tsarist Empire in areas its forces occupied in Eastern Europe. The Soviet Union and the People’s Republic of China supported post–World War II communist movements in foreign nations and colonies to advance their own interests, but were not always successful.
Trotsky, and others, believed that the revolution could only succeed in Russia as part of a world revolution. Lenin wrote extensively on the matter and famously declared that Imperialism was the highest stage of capitalism. However, after Lenin's death, Joseph Stalin established 'socialism in one country' for the Soviet Union, creating the model for subsequent inward looking Stalinist states and purging the early Internationalist elements. The internationalist tendencies of the early revolution would be abandoned until they returned in the framework of a client state in competition with the Americans during the Cold War. With the beginning of the new era, the after Stalin period called the "thaw", in late 1950's, the new political leader Nikita Khrushchev 
put even more pressure on the Soviet-American relations starting a new wave of anti-imperialist propaganda. In his speech on the UN conference in 1960, he announced the continuation of the war on imperialism, stating that soon the people of different countries will come together and overthrow their imperialist leaders.
Although the Soviet Union declared itself anti-imperialist, critics argue that it exhibited tendencies common to historic empires. Some scholars hold that the Soviet Union was a hybrid entity containing elements common to both multinational empires and nation states. It has also been argued that the USSR practiced colonialism as did other imperial powers and was carrying on the old Russian tradition of expansion and control. Mao Zedong once argued that the Soviet Union had itself become an imperialist power while maintaining a socialist façade. Moreover the ideas of imperialism were widely spread in action on the higher levels of government. Non Russian Marxists within the Russian Federation and later the USSR, like Sultan Galiev and Vasyl Shakhrai, considered the Soviet Regime a renewed version of the Russian imperialism and colonialism.
United Kingdom.
The First British Empire was based on mercantilism, and involved colonies and holdings primarily in North America, the Caribbean, and India. Its growth was reversed by the loss of the Thirteen American colonies in 1783. Britain made compensating gains in India, Australia, and in constructing an informal economic empire through control of trade and finance in Latin America after the independence of Spanish and Portuguese colonies about 1820. By the 1840s, Britain had adopted a general policy of free trade.
The independence of the Thirteen Colonies in North America in 1783 after the American War of Independence caused Britain to lose some of its oldest and most populous colonies. British then turned their attention towards Asia, Africa, and the Pacific. Following the defeat of Napoleonic France in 1815, Britain enjoyed a century of almost unchallenged dominance and expanded its imperial holdings around the globe. Increasing degrees of autonomy were granted to its white settler colonies, some of which were reclassified as dominions.
A resurgence came in the late 19th century, with the Scramble for Africa and major additions in Asia and the Middle East. The British spirit of imperialism was expressed by Joseph Chamberlain and Lord Rosebury, and implemented in Africa by Cecil Rhodes. Other influential spokesmen included Lord Cromer, Lord Curzon, General Kitchner, Lord Milner, and the writer Rudyard Kipling. The British Empire was the largest Empire that the world has ever seen both in terms of landmass and population. Its power, both military and economic, remained unmatched well into the 20th century.
United States.
The early United States expressed its opposition to Imperialism, at least in a form distinct from its own Manifest Destiny, in policies such as the Monroe Doctrine. However, beginning in the late 19th and early 20th century, policies such as Theodore Roosevelt’s interventionism in Central America and Woodrow Wilson’s mission to "make the world safe for democracy" were often backed by military force, but more often affected from behind the scenes, consistent with the general notion of hegemony and imperium of historical empires. In 1898, Americans who opposed imperialism created the Anti-Imperialist League to oppose the US annexation of the Philippines and Cuba. One year later, a war erupted in the Philippines causing business, labor and government leaders in the US to condemn America's occupation in the Philippines as they also denounced them for causing the deaths of many Filipinos. American foreign policy was denounced as a "racket" by Smedley Butler, an American general. He said, "Looking back on it, I might have given Al Capone a few hints. The best he could do was to operate his racket in three districts. I operated on three continents".
Post World War Two, The United States of America and the Soviet Union joined interests over a global conflict where they both became superpowers, this time frame was known as the Cold War. After the collapse of the Soviet Union, the United States did not diminish its global ability to project force and became a "hyper-power". A system of "Unipolarity" came to define international politics, with the United States at the center.
Imperialism in the United States has also taken an internal form, that is completely distinct from both its modern imperialism in the form of political and financial hegemony. This internal form of imperialism is also distinct from the United State's formation of colonies abroad. Through the treatment of its indigenous peoples during westward expansion, the United States took on the form of an imperial power prior to any attempts at external imperialism. This internal form of empire could be referred to as "internal colonialism". Participation in the African slave trade and the subsequent treatment of its 12 to 15 million Africans is viewed by some to be a more modern extension of America's "internal colonialism". This internal colonialism faced resistance, as external colonialism did, but the anti-colonial presence was far less prominent due to the nearly complete dominance that the United States was able to assert over both indigenous peoples and African-Americans. 
According to Edward Said’s lecture on April 16, 2003 he makes a bold statement on Modern imperialism in the United States, whom use aggressive means of attack towards the contemporary orient, "due to their backward living, lack of democracy and the violation of women’s rights. The western world forgets during this process of converting the other that enlightenment and democracy are concepts that not all will agree upon".
Ottoman Empire.
The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries. 
With Constantinople as its capital and control of lands around the Mediterranean basin, the Ottoman Empire was at the centre of interactions between the Eastern and Western worlds for six centuries. Following a long period of military setbacks against European powers and gradual decline, the empire allied with Germany with the imperial ambition to recover the lost territories but it collapsed and was dissolved in the aftermath of World War I, leading to the emergence of the new state of Turkey in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states.

</doc>
<doc id="15317" url="http://en.wikipedia.org/wiki?curid=15317" title="IPv4">
IPv4

Internet Protocol version 4 (IPv4) is the fourth version in the development of the Internet Protocol (IP) Internet, and routes most traffic on the Internet. However, a successor protocol, IPv6, has been defined and is in various stages of production deployment. IPv4 is described in IETF publication RFC 791 (September 1981), replacing an earlier definition (RFC 760, January 1980).
IPv4 is a connectionless protocol for use on packet-switched networks. It operates on a best effort delivery model, in that it does not guarantee delivery, nor does it assure proper sequencing or avoidance of duplicate delivery. These aspects, including data integrity, are addressed by an upper layer transport protocol, such as the Transmission Control Protocol (TCP).
Addressing.
IPv4 uses 32-bit (four-byte) addresses, which limits the address space to (232) addresses. As addresses were assigned to users, the number of unassigned addresses decreased. IPv4 address exhaustion occurred on February 3, 2011, although it had been significantly delayed by address changes such as classful network design, Classless Inter-Domain Routing, and network address translation (NAT).
This limitation of IPv4 stimulated the development of IPv6 in the 1990s, which has been in commercial deployment since 2006.
IPv4 reserves special address blocks for private networks (~18 million addresses) and multicast addresses (~270 million addresses).
Address representations.
IPv4 addresses may be written in any notation expressing a 32-bit integer value, but for human convenience, they are most often written in the dot-decimal notation, which consists of four octets of the address expressed individually in decimal and separated by periods.
An IP address followed by a slash(/) and a number (i.e. 127.0.0.1/8 ) indicates a block of addresses using a subnet mask. See CIDR notation.
The following table shows several representation formats:
Mixing decimal, octal and hexadecimal is allowed in dotted format per octet.
Note that in non-dotted formats, numbers bigger than 32-bit, can be given in some cases (e.g. Firefox) and will get converted mod 232.
Allocation.
Originally, an IP address was divided into two parts: the network identifier was the most significant (highest order) octet of the address, and the host identifier was the rest of the address. The latter was therefore also called the "rest field". This enabled the creation of a maximum of 256 networks. This was quickly found to be inadequate.
To overcome this limit, the high order octet of the addresses was redefined to create a set of "classes" of networks, in a system which later became known as classful networking.
The system defined five classes, Class A, B, C, D, and E. The Classes A, B, and C had different bit lengths for the new network identification. The rest of an address was used as previously to identify a host within a network, which meant that each network class had a different capacity to address hosts. Class D was allocated for multicast addressing and Class E was reserved for future applications.
Starting around 1985, methods were devised to subdivide IP networks. One method that has proved flexible is the use of the "variable-length subnet mask" (VLSM).
Based on the IETF standard RFC 1517 published in 1993, this system of classes was officially replaced with Classless Inter-Domain Routing (CIDR), which expressed the number of bits (from the most significant) as, for instance, /24, and the class-based scheme was dubbed "classful", by contrast. CIDR was designed to permit repartitioning of any address space so that smaller or larger blocks of addresses could be allocated to users. The hierarchical structure created by CIDR is managed by the Internet Assigned Numbers Authority (IANA) and the regional Internet registries (RIRs). Each RIR maintains a publicly searchable WHOIS database that provides information about IP address assignments.
Special-use addresses.
Private networks.
Of the approximately four billion addresses allowed in IPv4, three ranges of address are reserved for use in private networks. These ranges are not routable outside of private networks, and private machines cannot directly communicate with public networks. They can, however, do so through network address translation.
The following are the three ranges reserved for private networks (RFC 1918):
Virtual private networks.
Packets with a private destination address are ignored by all public routers. Two private networks (e.g., two branch offices) cannot communicate via the public internet, unless they use an IP tunnel or a virtual private network (VPN). When one private network wants to send a packet to another private network, the first private network encapsulates the packet in a protocol layer so that the packet can travel through the public network. Then the packet travels through the public network. When the packet reaches the other private network, its protocol layer is removed, and the packet travels to its destination.
Optionally, encapsulated packets may be encrypted to secure the data while it travels over the public network.
Link-local addressing.
RFC 6890 defines the special address block 169.254.0.0/16 for link-local addressing. These addresses are only valid on links (such as a local network segment or point-to-point connection) connected to a host. These addresses are not routable. Like private addresses, these addresses cannot be the source or destination of packets traversing the internet. These addresses are primarily used for address autoconfiguration (Zeroconf) when a host cannot obtain an IP address from a DHCP server or other internal configuration methods.
When the address block was reserved, no standards existed for address autoconfiguration. Microsoft created an implementation called Automatic Private IP Addressing (APIPA), which was deployed on millions of machines and became a de facto standard. Many years later, in May 2005, the IETF defined a formal standard in RFC 3927, entitled "Dynamic Configuration of IPv4 Link-Local Addresses".
Loopback.
The class A network 127.0.0.0 (classless network 127.0.0.0/8) is reserved for loopback. IP packets whose source addresses belong to this network should never appear outside a host. The modus operandi of this network expands upon that of a loopback interface:
Addresses ending in 0 or 255.
Networks with subnet masks of at least 24 bits, i.e. Class C networks in classful networking, and networks with CIDR suffixes /24 to /32 (255.255.255.0–255.255.255.255) may not have an address ending in 0 or 255.
Classful addressing prescribed only three possible subnet masks: Class A, 255.0.0.0 or /8; Class B, 255.255.0.0 or /16; and Class C, 255.255.255.0 or /24. For example, in the subnet 192.168.5.0/255.255.255.0 (192.168.5.0/24) the identifier 192.168.5.0 commonly is used to refer to the entire subnet. To avoid ambiguity in representation, the address ending in the octet "0" is reserved.
A broadcast address is an address that allows information to be sent to all interfaces in a given subnet, rather than a specific machine. Generally, the broadcast address is found by obtaining the bit complement of the subnet mask and performing a bitwise OR operation with the network identifier. In other words, the broadcast address is the last address in the address range of the subnet. For example, the broadcast address for the network 192.168.5.0 is 192.168.5.255. For networks of size /24 or larger, the broadcast address always ends in 255.
However, this does not mean that every address ending in 0 or 255 cannot be used as a host address. For example, in the /16 subnet 192.168.0.0/255.255.0.0, which is equivalent to the address range 192.168.0.0–192.168.255.255, the broadcast address is 192.168.255.255. One can use the following addresses for hosts, even though they end with 255: 192.168.1.255, 192.168.2.255, etc. Also, 192.168.0.0 is the network identifier and must not be assigned to an interface. The addresses 192.168.1.0, 192.168.2.0, etc., may be assigned, despite ending with 0.
In the past, conflict between network addresses and broadcast addresses arose because some software used non-standard broadcast addresses with zeros instead of ones.
In networks smaller than /24, broadcast addresses do not necessarily end with 255. For example, a CIDR subnet 203.0.113.16/28 has the broadcast address 203.0.113.31.
Address resolution.
Hosts on the Internet are usually known by names, e.g., www.example.com, not primarily by their IP address, which is used for routing and network interface identification. The use of domain names requires translating, called "resolving", them to addresses and vice versa. This is analogous to looking up a phone number in a phone book using the recipient's name.
The translation between addresses and domain names is performed by the Domain Name System (DNS), a hierarchical, distributed naming system which allows for subdelegation of name spaces to other DNS servers.
Address space exhaustion.
Since the 1980s, it was apparent that the pool of available IPv4 addresses was being depleted at a rate that was not initially anticipated in the original design of the network address system. The threat of exhaustion was the motivation for remedial technologies, such as classful networks, Classless Inter-Domain Routing (CIDR) methods, and network address translation (NAT). Eventually, IPv6 was created, which has many more addresses available.
Several market forces accelerated IPv4 address exhaustion:
Some technologies mitigated IPv4 address exhaustion:
The primary address pool of the Internet, maintained by IANA, was exhausted on 3 February 2011, when the last 5 blocks were allocated to the 5 RIRs. APNIC was the first RIR to exhaust its regional pool on 15 April 2011, except for a small amount of address space reserved for the transition to IPv6, which will be allocated under a much more restricted policy.
The accepted and standard long term solution is to use Internet Protocol Version 6. The address size was increased in IPv6 to 128 bits, providing a vastly increased address space that also allows improved route aggregation across the Internet and offers large subnetwork allocations of a minimum of 264 host addresses to end-users. However IPv4-only hosts cannot directly communicate with IPv6-only hosts so IPv6 alone does not provide an immediate solution to the IPv4 exhaustion problem. Migration to IPv6 is in progress but completion is expected to take considerable time. 
Packet structure.
An IP packet consists of a header section and a data section.
An IP packet has no data checksum or any other footer after the data section.
Typically the link layer encapsulates IP packets in frames with a CRC footer that detects most errors,
and typically the end-to-end TCP layer checksum detects most other errors.
Header.
The IPv4 packet header consists of 14 fields, of which 13 are required. The 14th field is optional (red background in table) and aptly named: options. The fields in the header are packed with the most significant byte first (big endian), and for the diagram and discussion, the most significant bits are considered to come first (MSB 0 bit numbering). The most significant bit is numbered 0, so the version field is actually found in the four most significant bits of the first byte, for example.
Originally defined as the Type of service (ToS) field. This field is now defined by RFC 2474 for Differentiated services (DiffServ). New technologies are emerging that require real-time data streaming and therefore make use of the DSCP field. An example is Voice over IP (VoIP), which is used for interactive data voice exchange.
This field is defined in RFC 3168 and allows end-to-end notification of network congestion without dropping packets. ECN is an optional feature that is only used when both endpoints support it and are willing to use it. It is only effective when supported by the underlying network.
This 16-bit field defines the entire packet (fragment) size, including header and data, in bytes. The minimum-length packet is 20 bytes (20-byte header + 0 bytes data) and the maximum is 65,535 bytes — the maximum value of a 16-bit word. All hosts are required to be able to reassemble datagrams of size up to 576 bytes, but most modern hosts handle much larger packets. Sometimes subnetworks impose further restrictions on the packet size, in which case datagrams must be fragmented. Fragmentation is handled in either the host or router in IPv4.
This field is an identification field and is primarily used for uniquely identifying the group of fragments of a single IP datagram. Some experimental work has suggested using the ID field for other purposes, such as for adding packet-tracing information to help trace datagrams with spoofed source addresses, but RFC 6864 now prohibits any such use.
A three-bit field follows and is used to control or identify fragments. They are (in order, from high order to low order):
If the DF flag is set, and fragmentation is required to route the packet, then the packet is dropped. This can be used when sending packets to a host that does not have sufficient resources to handle fragmentation. It can also be used for Path MTU Discovery, either automatically by the host IP software, or manually using diagnostic tools such as ping or traceroute.
For unfragmented packets, the MF flag is cleared. For fragmented packets, all fragments except the last have the MF flag set. The last fragment has a non-zero Fragment Offset field, differentiating it from an unfragmented packet.
The fragment offset field, measured in units of eight-byte blocks (64 bits), is 13 bits long and specifies the offset of a particular fragment relative to the beginning of the original unfragmented IP datagram. The first fragment has an offset of zero. This allows a maximum offset of (213 – 1) × 8 = 65,528 bytes, which would exceed the maximum IP packet length of 65,535 bytes with the header length included (65,528 + 20 = 65,548 bytes).
An eight-bit time to live field helps prevent datagrams from persisting (e.g. going in circles) on an internet. This field limits a datagram's lifetime. It is specified in seconds, but time intervals less than 1 second are rounded up to 1. In practice, the field has become a hop count—when the datagram arrives at a router, the router decrements the TTL field by one. When the TTL field hits zero, the router discards the packet and typically sends an ICMP Time Exceeded message to the sender.
The program traceroute uses these ICMP Time Exceeded messages to print the routers used by packets to go from the source to the destination.
This field defines the protocol used in the data portion of the IP datagram. The Internet Assigned Numbers Authority maintains a list of IP protocol numbers which was originally defined in RFC 790.
The 16-bit checksum field is used for error-checking of the header. When a packet arrives at a router, the router calculates the checksum of the header and compares it to the checksum field. If the values do not match, the router discards the packet. Errors in the data field must be handled by the encapsulated protocol. Both UDP and TCP have checksum fields.
When a packet arrives at a router, the router decreases the TTL field. Consequently, the router must calculate a new checksum. RFC 1071 defines the checksum calculation:
For example, consider Hex 4500003044224000800600008c7c19acae241e2b (20 bytes IP header), using a machine which uses standard two's complement arithmetic:
To validate a header's checksum the same algorithm may be used – the checksum of a header which contains a correct checksum field is a word containing all zeros (value 0):
This field is the IPv4 address of the sender of the packet. Note that this address may be changed in transit by a network address translation device.
This field is the IPv4 address of the receiver of the packet. As with the source address, this may be changed in transit by a network address translation device.
The options field is not often used. Note that the value in the IHL field must include enough extra 32-bit words to hold all the options (plus any padding needed to ensure that the header contains an integer number of 32-bit words). The list of options may be terminated with an EOL (End of Options List, 0x00) option; this is only necessary if the end of the options would not otherwise coincide with the end of the header. The possible options that can be put in the header are as follows:
The following two options are discouraged because they create security concerns: Loose Source and Record Route (LSRR) and Strict Source and Record Route (SSRR). Many routers block packets containing these options.
Data.
The data portion of the packet is not included in the packet checksum. Its contents are interpreted based on the value of the Protocol header field.
Some of the common protocols for the data portion are listed below:
See List of IP protocol numbers for a complete list.
Fragmentation and reassembly.
The Internet Protocol enables networks to communicate with one another. The design accommodates networks of diverse physical nature; it is independent of the underlying transmission technology used in the Link Layer. Networks with different hardware usually vary not only in transmission speed, but also in the maximum transmission unit (MTU). When one network wants to transmit datagrams to a network with a smaller MTU, it may fragment its datagrams. In IPv4, this function was placed at the Internet Layer, and is performed in IPv4 routers, which thus only require this layer as the highest one implemented in their design.
In contrast, IPv6, the next generation of the Internet Protocol, does not allow routers to perform fragmentation; hosts must determine the path MTU before sending datagrams.
Fragmentation.
When a router receives a packet, it examines the destination address and determines the outgoing interface to use and that interface's MTU. If the packet size is bigger than the MTU, and the Do not Fragment (DF) bit in the packet's header is set to 0, then the router may fragment the packet.
The router divides the packet into fragments. The max size of each fragment is the MTU minus the IP header size (20 bytes minimum; 60 bytes maximum). The router puts each fragment into its own packet, each fragment packet having following changes:
For example, for an MTU of 1,500 bytes and a header size of 20 bytes, the fragment offsets would be multiples of (1500–20)/8 = 185. These multiples are 0, 185, 370, 555, 740, ...
It is possible for a packet to be fragmented at one router, and for the fragments to be fragmented at another router. For example, consider a Transport layer segment with size of 4,500 bytes, no options, and IP header size of 20 bytes. So the IP packet size is 4,520 bytes. Assume that the packet travels over a link with an MTU of 2,500 bytes. Then it will become two fragments:
Note that the fragments preserve the data size: 2480 + 2020 = 4500.
Note how we get the offsets from the data sizes:
Assume that these fragments reach a link with an MTU of 1,500 bytes. Each fragment will become two fragments:
Note that the fragments preserve the data size: 1480 + 1000 = 2480, and 1480 + 540 = 2020.
Note how we get the offsets from the data sizes:
We can use the last offset and last data size to calculate the total data size: 495*8 + 540 = 3960 + 540 = 4500.
Reassembly.
A receiver knows that a packet is a fragment if at least one of the following conditions is true:
The receiver identifies matching fragments using the identification field. The receiver will reassemble the data from fragments with the same identification field using both the fragment offset and the more fragments flag. When the receiver receives the last fragment (which has the "more fragments" flag set to 0), it can calculate the length of the original data payload, by multiplying the last fragment's offset by eight, and adding the last fragment's data size. In the example above, this calculation was 495*8 + 540 = 4500 bytes.
When the receiver has all the fragments, it can put them in the correct order, by using their offsets. It can then pass their data up the stack for further processing.
Assistive protocols.
The Internet Protocol is the protocol that defines and enables internetworking at the Internet Layer and thus forms the Internet. It uses a logical addressing system. IP addresses are not tied in any permanent manner to hardware identifications and, indeed, a network interface can have multiple IP addresses. Hosts and routers need additional mechanisms to identify the relationship between device interfaces and IP addresses, in order to properly deliver an IP packet to the destination host on a link. The Address Resolution Protocol (ARP) performs this IP-address-to-hardware-address translation for IPv4. (A hardware address is also called a MAC address.) In addition, the reverse correlation is often necessary. For example, when an IP host is booted or connected to a network it needs to determine its IP address, unless an address is preconfigured by an administrator. Protocols for such inverse correlations exist in the Internet Protocol Suite. Currently used methods are Dynamic Host Configuration Protocol (DHCP), Bootstrap Protocol (BOOTP) and, infrequently, reverse ARP.
External links.
Address exhaustion:

</doc>
<doc id="15318" url="http://en.wikipedia.org/wiki?curid=15318" title="IPv6">
IPv6

Internet Protocol version 6 (IPv6) is the most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion. IPv6 is intended to replace IPv4.
Every device on the Internet is assigned an IP address for identification and location definition. With the rapid growth of the Internet after commercialization in the 1990s, it became evident that far more addresses than the IPv4 address space has available were necessary to connect new devices in the future. By 1998, the Internet Engineering Task Force (IETF) had formalized the successor protocol. IPv6 uses a 128-bit address, allowing 2128, or approximately addresses, or more than times as many as IPv4, which uses 32-bit addresses and provides approximately 4.3 billion addresses. The two protocols are not designed to be interoperable, complicating the transition to IPv6. However, several IPv6 transition mechanisms have been devised to permit communication between IPv4 and IPv6 hosts.
IPv6 provides other technical benefits in addition to a larger addressing space. In particular, it permits hierarchical address allocation methods that facilitate route aggregation across the Internet, and thus limit the expansion of routing tables. The use of multicast addressing is expanded and simplified, and provides additional optimization for the delivery of services. Device mobility, security, and configuration aspects have been considered in the design of the protocol.
IPv6 addresses are represented as eight groups of four hexadecimal digits separated by colons, for example 2001:0db8:85a3:0042:1000:8a2e:0370:7334, but methods to abbreviate this full notation exist.
Main features.
IPv6 is an Internet Layer protocol for packet-switched internetworking and provides end-to-end datagram transmission across multiple IP networks, closely adhering to the design principles developed in the previous version of the protocol, Internet Protocol Version 4 (IPv4). IPv6 was first formally described in Internet standard document RFC 2460, published in December 1998. In addition to offering more addresses, IPv6 also implements features not present in IPv4. It simplifies aspects of address assignment (stateless address autoconfiguration), network renumbering, and router announcements when changing network connectivity providers. It simplifies processing of packets in routers by placing the responsibility for packet fragmentation into the end points. The IPv6 subnet size is standardized by fixing the size of the host identifier portion of an address to 64 bits to facilitate an automatic mechanism for forming the host identifier from link layer addressing information (MAC address). Network security was a design requirement of the IPv6 architecture, and included the original specification of IPsec.
IPv6 does not specify interoperability features with IPv4, but essentially creates a parallel, independent network. Exchanging traffic between the two networks requires translator gateways employing one of several transition mechanisms, such as NAT64, or a tunneling protocol like 6to4, 6in4, or Teredo.
Motivation and origin.
IPv4.
Internet Protocol Version 4 (IPv4) was the first publicly used version of the Internet Protocol. IPv4 was developed as a research project by the Defense Advanced Research Projects Agency (DARPA), a United States Department of Defense agency, before becoming the foundation for the Internet and the World Wide Web. It is currently described by IETF publication RFC 791 (September 1981), which replaced an earlier definition (RFC 760, January 1980). IPv4 included an addressing system that used numerical identifiers consisting of 32 bits. These addresses are typically displayed in quad-dotted notation as decimal values of four octets, each in the range 0 to 255, or 8 bits per number. Thus, IPv4 provides an addressing capability of 232 or approximately 4.3 billion addresses. Address exhaustion was not initially a concern in IPv4 as this version was originally presumed to be a test of DARPA's networking concepts. During the first decade of operation of the Internet, it became apparent that methods had to be developed to conserve address space. In the early 1990s, even after the redesign of the addressing system using a classless network model, it became clear that this would not suffice to prevent IPv4 address exhaustion, and that further changes to the Internet infrastructure were needed.
The last unassigned top-level address blocks of 16 million IPv4 addresses were allocated in February 2011 by the Internet Assigned Numbers Authority (IANA) to the five regional Internet registries (RIRs). However, each RIR still has available address pools and is expected to continue with standard address allocation policies until one /8 Classless Inter-Domain Routing (CIDR) block remains. After that, only blocks of 1024 addresses (/22) will be provided from the RIRs to a local Internet registry (LIR). As of September 2012, both the Asia-Pacific Network Information Centre (APNIC) and the Réseaux IP Européens Network Coordination Centre (RIPE_NCC) have reached this stage.
Working-group proposals.
By the beginning of 1992, several proposals appeared for an expanded Internet addressing system and by the end of 1992 the IETF announced a call for white papers. In September 1993, the IETF created a temporary, ad-hoc "IP Next Generation" (IPng) area to deal specifically with such issues. The new area was led by Allison Mankin and Scott Bradner, and had a directorate with 15 engineers from diverse backgrounds for direction-setting and preliminary document review: The working-group members were J. Allard (Microsoft), Steve Bellovin (AT&T), Jim Bound (Digital Equipment Corporation), Ross Callon (Wellfleet), Brian Carpenter (CERN), Dave Clark (MIT), John Curran (NEARNET), Steve Deering (Xerox), Dino Farinacci (Cisco), Paul Francis (NTT), Eric Fleischmann (Boeing), Mark Knopper (Ameritech), Greg Minshall (Novell), Rob Ullmann (Lotus), and Lixia Zhang (Xerox).
The Internet Engineering Task Force adopted the IPng model on 25 July 1994, with the formation of several IPng working groups. By 1996, a series of RFCs was released defining Internet Protocol version 6 (IPv6), starting with RFC 1883. (Version 5 was used by the experimental Internet Stream Protocol.)
It is widely expected that the Internet will use IPv4 alongside IPv6 for the foreseeable future. Direct communication between the IPv4 and IPv6 network protocols is not possible; therefore, intermediary trans-protocol systems are needed as a communication conduit between IPv4 and IPv6 whether on a single device or among network nodes.
Comparison with IPv4.
On the Internet, data is transmitted in the form of network packets. IPv6 specifies a new packet format, designed to minimize packet header processing by routers. Because the headers of IPv4 packets and IPv6 packets are significantly different, the two protocols are not interoperable. However, in most respects, IPv6 is an extension of IPv4. Most transport and application-layer protocols need little or no change to operate over IPv6; exceptions are application protocols that embed Internet-layer addresses, such as FTP and NTP, where the new address format may cause conflicts with existing protocol syntax.
Larger address space.
The main advantage of IPv6 over IPv4 is its larger address space. The length of an IPv6 address is 128 bits, compared with 32 bits in IPv4. The address space therefore has 2128 or approximately addresses.
In addition, the IPv4 address space is poorly allocated, with approximately 14% of all available addresses utilized. While these numbers are large, it was not the intent of the designers of the IPv6 address space to assure geographical saturation with usable addresses. Rather, the longer addresses simplify allocation of addresses, enable efficient route aggregation, and allow implementation of special addressing features. In IPv4, complex Classless Inter-Domain Routing (CIDR) methods were developed to make the best use of the small address space. The standard size of a subnet in IPv6 is 264 addresses, the square of the size of the entire IPv4 address space. Thus, actual address space utilization rates will be small in IPv6, but network management and routing efficiency are improved by the large subnet space and hierarchical route aggregation.
Renumbering an existing network for a new connectivity provider with different routing prefixes is a major effort with IPv4. With IPv6, however, changing the prefix announced by a few routers can in principle renumber an entire network, since the host identifiers (the least-significant 64 bits of an address) can be independently self-configured by a host.
Multicasting.
Multicasting, the transmission of a packet to multiple destinations in a single send operation, is part of the base specification in IPv6. In IPv4 this is an optional although commonly implemented feature. IPv6 multicast addressing shares common features and protocols with IPv4 multicast, but also provides changes and improvements by eliminating the need for certain protocols. IPv6 does not implement traditional IP broadcast, i.e. the transmission of a packet to all hosts on the attached link using a special "broadcast address", and therefore does not define broadcast addresses. In IPv6, the same result can be achieved by sending a packet to the link-local "all nodes" multicast group at address codice_1, which is analogous to IPv4 multicast to address codice_2. IPv6 also provides for new multicast implementations, including embedding rendezvous point addresses in an IPv6 multicast group address, which simplifies the deployment of inter-domain solutions.
In IPv4 it is very difficult for an organization to get even one globally routable multicast group assignment, and the implementation of inter-domain solutions is arcane. Unicast address assignments by a local Internet registry for IPv6 have at least a 64-bit routing prefix, yielding the smallest subnet size available in IPv6 (also 64 bits). With such an assignment it is possible to embed the unicast address prefix into the IPv6 multicast address format, while still providing a 32-bit block, the least significant bits of the address, or approximately 4.2 billion multicast group identifiers. Thus each user of an IPv6 subnet automatically has available a set of globally routable source-specific multicast groups for multicast applications.
Stateless address autoconfiguration (SLAAC).
IPv6 hosts can configure themselves automatically when connected to an IPv6 network using the Neighbor Discovery Protocol via Internet Control Message Protocol version 6 (ICMPv6) router discovery messages. When first connected to a network, a host sends a link-local router solicitation multicast request for its configuration parameters; routers respond to such a request with a router advertisement packet that contains Internet Layer configuration parameters.
If IPv6 stateless address autoconfiguration is unsuitable for an application, a network may use stateful configuration with the Dynamic Host Configuration Protocol version 6 (DHCPv6) or hosts may be configured manually using static methods.
Routers present a special case of requirements for address configuration, as they often are sources of autoconfiguration information, such as router and prefix advertisements. Stateless configuration of routers can be achieved with a special router renumbering protocol.
Network-layer security.
Internet Protocol Security (IPsec) was originally developed for IPv6, but found widespread deployment first in IPv4, for which it was re-engineered. IPsec was a mandatory specification of the base IPv6 protocol suite, but has since been made optional.
Simplified processing by routers.
In IPv6, the packet header and the process of packet forwarding have been simplified. Although IPv6 packet headers are at least twice the size of IPv4 packet headers, packet processing by routers is generally more efficient, thereby extending the end-to-end principle of Internet design. Specifically:
Mobility.
Unlike mobile IPv4, mobile IPv6 avoids triangular routing and is therefore as efficient as native IPv6. IPv6 routers may also allow entire subnets to move to a new router connection point without renumbering.
Options extensibility.
The IPv6 packet header has a fixed size (40 octets). Options are implemented as additional extension headers after the IPv6 header, which limits their size only by the size of an entire packet. The extension header mechanism makes the protocol extensible in that it allows future services for quality of service, security, mobility, and others to be added without redesign of the basic protocol.
Jumbograms.
IPv4 limits packets to (216−1) octets of payload. An IPv6 node can optionally handle packets over this limit, referred to as jumbograms, which can be as large as (232−1) octets. The use of jumbograms may improve performance over high-MTU links. The use of jumbograms is indicated by the Jumbo Payload Option header.
Privacy.
Like IPv4, IPv6 supports globally unique IP addresses by which the network activity of each device can potentially be tracked. The design of IPv6 intended to re-emphasize the end-to-end principle of network design that was originally conceived during the establishment of the early Internet. In this approach each device on the network has a unique address globally reachable directly from any other location on the Internet.
It is not a requirement for IPv6 hosts to use address auto-configuration, however. Yet, even when an address is not based on the MAC address, the interface's address is globally unique, in contrast to NAT-masqueraded private networks. Privacy extensions for IPv6 have been defined to address these privacy concerns, although Silvia Hagen describes these as being largely due to "misunderstanding". When privacy extensions are enabled, the operating system generates random host identifiers to combine with the assigned network prefix. These ephemeral addresses are used to communicate with remote hosts making it more difficult to track a single device.
Privacy extensions are enabled by default in Windows (since XP SP1), OS X (since 10.7), and iOS (since version 4.3). Some Linux distributions have enabled privacy extensions as well.
Privacy extensions do not protect the user from other forms of activity tracking, such as tracking cookies or browser fingerprinting.
Packet format.
An IPv6 packet has two parts: a header and payload.
The header consists of a fixed portion with minimal functionality required for all packets and may be followed by optional extensions to implement special features.
The fixed header occupies the first 40 octets (320 bits) of the IPv6 packet. It contains the source and destination addresses, traffic classification options, a hop counter, and the type of the optional extension or payload which follows the header. This "Next Header" field tells the receiver how to interpret the data which follows the header. If the packet contains options, this field contains the option type of the next option. The "Next Header" field of the last option, points to the upper-layer protocol that is carried in the packet's payload.
Extension headers carry options that are used for special treatment of a packet in the network, e.g., for routing, fragmentation, and for security using the IPsec framework.
Without special options, a payload must be less than . With a Jumbo Payload option (in a "Hop-By-Hop Options" extension header), the payload must be less than 4 GB.
Unlike with IPv4, routers never fragment a packet. Hosts are expected to use Path MTU Discovery to make their packets small enough to reach the destination without needing to be fragmented. See IPv6 packet fragmentation.
Addressing.
Compared to IPv4, the most obvious advantage of IPv6 is its larger address space. IPv4 addresses are 32 bits long and number about (4.3 billion). IPv6 addresses are 128 bits long and number about (340 undecillion). IPv6's addresses are deemed enough for the foreseeable future.
IPv6 addresses are written in eight groups of four hexadecimal digits separated by colons, such as codice_3. IPv6 unicast addresses other than those that start with binary 000 are logically divided into two parts: a 64-bit (sub-)network prefix, and a 64-bit interface identifier.
Stateless autoconfiguration.
An IPv6 host may generate its own IP address and test its uniqueness in the addressing scope intended. IPv6 addresses consist of two parts. The most-significant 64 bits are the subnet prefix to which the host is connected, and the least-significant 64 bits are the identifier of the host interface on the subnet. This means that the identifier need only be unique on the subnet to which the host is connected, which simplifies the detection of duplicate addresses.
Link local address.
All IPv6 hosts require a link-local address. This is derived from the MAC address of each interface and the link-local prefix FE80::/10. The process involves filling the address space with prefix bits left-justified to the most-significant bit, and filling the MAC address in EUI-64 format into the least-significant bits. If any bits remain to be filled between the two parts, those are set to zero.
The uniqueness of the address on the subnet is tested with the Duplicate Address Detection (DAD) method.
Address uniqueness.
Hosts verify the uniqueness of addresses assigned by sending a neighbor solicitation message asking for the Link Layer address of the IP address. If any other host are using that address, they respond. However, MAC addresses are designed to be unique on each network card which minimizes chances of duplication.
The host first determines if the network is connected to any routers at all, because if not, then all nodes are reachable using the link-local address that already is assigned to the host. The host will send out a Router Solicitation message to the all-routers multicast group with its link local address as source. If there is no answer after a predetermined number of attempts, the host concludes that no routers are connected. If it does get a response from a router, there will be network information inside that is needed to create a globally unique address. There are also two flag bits that tell the host whether it should use DHCP to get further information and addresses:
Global addressing.
The assignment procedure for global addresses is similar to local address construction. The prefix is supplied from router advertisements on the network. Multiple prefix announcements cause multiple addresses to be configured.
Stateless address autoconfiguration (SLAAC) requires a /64 address block, as defined in RFC 4291. Local Internet registries are assigned at least /32 blocks, which they divide among subordinate networks. The initial recommendation stated assignment of a /48 subnet to end-consumer sites (RFC 3177). This was replaced by RFC 6177, which "recommends giving home sites significantly more than a single /64, but does not recommend that every home site be given a /48 either". /56s are specifically considered. It remains to be seen if ISPs will honor this recommendation. For example, during initial trials, Comcast customers were given a single /64 network.
IPv6 addresses are classified by three types of networking methodologies: unicast addresses identify each network interface, anycast addresses identify a group of interfaces, usually at different locations of which the nearest one is automatically selected, and multicast addresses are used to deliver one packet to many interfaces. The broadcast method is not implemented in IPv6. Each IPv6 address has a scope, which specifies in which part of the network it is valid and unique. Some addresses are unique only on the local (sub-)network. Others are globally unique.
Some IPv6 addresses are reserved for special purposes, such as loopback, 6to4 tunneling, and Teredo tunneling, as outlined in RFC 5156. Also, some address ranges are considered special, such as link-local addresses for use on the local link only, Unique Local addresses (ULA), as described in RFC 4193, and solicited-node multicast addresses used in the Neighbor Discovery Protocol.
IPv6 in the Domain Name System.
In the Domain Name System, hostnames are mapped to IPv6 addresses by AAAA resource records, so-called "quad-A" records. For reverse resolution, the IETF reserved the domain codice_4, where the name space is hierarchically divided by the 1-digit hexadecimal representation of nibble units (4 bits) of the IPv6 address. This scheme is defined in RFC 3596.
Address representation.
The 128 bits of an IPv6 address are represented in 8 groups of 16 bits each. Each group is written as 4 hexadecimal digits and the groups are separated by colons (:). The address 2001:0db8:0000:0000:0000:ff00:0042:8329 is an example of this representation.
For convenience, an IPv6 address may be abbreviated to shorter notations by application of the following rules, where possible.
An example of application of these rules:
The loopback address, 0000:0000:0000:0000:0000:0000:0000:0001, may be abbreviated to ::1 by using both rules.
As an IPv6 address may have more than one representation, the IETF has issued a proposed standard for representing them in text.
Transition mechanisms.
IPv6 is not foreseen to supplant IPv4 instantaneously. Both protocols will continue to operate simultaneously for some time. Therefore, some IPv6 transition mechanisms are needed to enable IPv6 hosts to reach IPv4 services and to allow isolated IPv6 hosts and networks to reach each other over IPv4 infrastructure.
Many of these transition mechanisms use tunneling to encapsulate IPv6 traffic within IPv4 networks. This is an imperfect solution, which reduces the maximum transmission unit (MTU) of a link and therefore complicates Path MTU Discovery, and may increase latency. Tunneling protocols are a temporary solution for networks that do not support native dual-stack, where both IPv6 and IPv4 run independently.
Dual IP stack implementation.
Dual-stack (or "native dual-stack") IP implementations provide complete IPv4 and IPv6 protocol stacks in the same network node. This facilitates native communications between nodes using either protocol. The method is defined in RFC 4213.
This is the most desirable IPv6 implementation during the transition from IPv4 to IPv6, as it avoids the complexities of tunneling, such as security, increased latency, management overhead, and a reduced PMTU. However, it is not always possible, since outdated network equipment may not support IPv6.
Dual-stack software design is a transitional technique to facilitate the adoption and deployment of IPv6. However, it might introduce more security threats as hosts could be subject to attacks from both IPv4 and IPv6. It has been argued that dual-stack could ultimately overburden the global networking infrastructure by requiring routers to deal with IPv4 and IPv6 routing simultaneously.
Tunneling.
Many current Internet users do not have IPv6 dual-stack support, and thus cannot reach IPv6 sites directly. Instead, they must use IPv4 infrastructure to carry IPv6 packets. This is done using a technique known as "tunneling", which encapsulates IPv6 packets within IPv4, in effect using IPv4 as a link layer for IPv6.
IP protocol 41 indicates IPv4 packets which encapsulate IPv6 datagrams. Some routers or network address translation devices may block protocol 41. To pass through these devices, UDP packets may be used to encapsulate IPv6 datagrams. Other encapsulation schemes, such as AYIYA or Generic Routing Encapsulation, are also popular.
Conversely, on IPv6-only Internet links, when access to IPv4 network facilities is needed, tunneling of IPv4 over IPv6 protocol occurs, using the IPv6 as a link layer for IPv4.
Automatic tunneling.
"Automatic tunneling" refers to a technique by which the routing infrastructure automatically determines the tunnel endpoints. Some automatic tunneling techniques are below.
6to4 is recommended by RFC 3056. It uses protocol 41 encapsulation. Tunnel endpoints are determined by using a well-known IPv4 anycast address on the remote side, and embedding IPv4 address information within IPv6 addresses on the local side. 6to4 is the most common tunnel protocol currently deployed.
Teredo is an automatic tunneling technique that uses UDP encapsulation and can allegedly cross multiple NAT nodes. IPv6, including 6to4 and Teredo tunneling, are enabled by default in Windows Vista and Windows 7. Most Unix systems implement only 6to4, but Teredo can be provided by third-party software such as Miredo.
ISATAP (Intra-Site Automatic Tunnel Addressing Protocol) uses the IPv4 network as a virtual IPv6 local link, with mappings from each IPv4 address to a link-local IPv6 address. Unlike 6to4 and Teredo, which are "inter-site" tunneling mechanisms, ISATAP is an "intra-site" mechanism, meaning that it is designed to provide IPv6 connectivity between nodes within a single organization.
Configured and automated tunneling (6in4).
6in4 tunneling requires the tunnel endpoints to be explicitly configured, either by an administrator manually or the operating system's configuration mechanisms, or by an automatic service known as a tunnel broker; this is also referred to as "automated tunneling". Configured tunneling is usually more deterministic and easier to debug than automatic tunneling, and is therefore recommended for large, well-administered networks. Automated tunneling provides a compromise between the ease of use of automatic tunneling and the deterministic behavior of configured tunneling.
Raw encapsulation of IPv6 packets using IPv4 protocol number 41 is recommended for configured tunneling; this is sometimes known as 6in4 tunneling. As with automatic tunneling, encapsulation within UDP may be used in order to cross NAT boxes and firewalls.
Proxying and translation for IPv6-only hosts.
After the regional Internet registries have exhausted their pools of available IPv4 addresses, it is likely that hosts newly added to the Internet might only have IPv6 connectivity. For these clients to have backward-compatible connectivity to existing IPv4-only resources, suitable IPv6 transition mechanisms must be deployed.
One form of address translation is the use of a dual-stack application-layer proxy server, for example a web proxy.
NAT-like techniques for application-agnostic translation at the lower layers in routers and gateways have been proposed. The NAT-PT standard was dropped because of criticisms; however, more recently, the continued low adoption of IPv6 has prompted a new standardization effort of a technology called NAT64.
IPv6 readiness.
Compatibility with IPv6 networking is mainly a software or firmware issue. However, much of the older hardware that could in principle be upgraded is likely to be replaced instead. The American Registry for Internet Numbers (ARIN) suggested that all Internet servers be prepared to serve IPv6-only clients by January 2012.
Software.
Host software may have only IPv4 or only IPv6 networking software, or it may support dual-stack, or hybrid dual-stack operation. The majority of personal computers running recent operating system versions support IPv6. Many popular applications with networking capabilities are compliant.
Some software transitioning mechanisms are outlined in RFC 4038, RFC 3493, and RFC 3542.
IPv4-mapped IPv6 addresses.
Hybrid dual-stack IPv6/IPv4 implementations recognize a special class of addresses, the IPv4-mapped IPv6 addresses. These addresses consist of an 80-bit prefix of zeros, the next 16 bits are one, and the remaining, least-significant 32 bits contain the IPv4 address. These addresses are typically written with a 96-bit prefix in the standard IPv6 format, and the remaining 32 bits written in the customary dot-decimal notation of IPv4. For example, ::ffff:192.0.2.128 represents the IPv4 address 192.0.2.128. A deprecated format for IPv4-compatible IPv6 addresses is ::192.0.2.128.
Because of the significant internal differences between IPv4 and IPv6, some of the lower-level functionality available to programmers in the IPv6 stack does not work the same when used with IPv4-mapped addresses. Some common IPv6 stacks do not implement the IPv4-mapped address feature, either because the IPv6 and IPv4 stacks are separate implementations (e.g., Microsoft Windows 2000, XP, and Server 2003), or because of security concerns (OpenBSD). On these operating systems, a program must open a separate socket for each IP protocol it uses. On some systems, e.g., the Linux kernel, NetBSD, and FreeBSD, this feature is controlled by the socket option IPV6_V6ONLY, as specified in RFC 3493.
Hardware and embedded systems.
Basic infrastructure equipment, such as network adapters and network switches, may not be affected by the change, since they transmit link layer frames without inspecting the contents. Most equipment may be IPv6 capable with a software or firmware update if the device has sufficient storage and memory space for the new IPv6 stack.
In some cases, non-compliant equipment needs to be replaced because the manufacturer no longer exists or software updates are not possible, for example, because the network stack is implemented in permanent read-only memory.
The CableLabs consortium published the 160 Mbit/s DOCSIS 3.0 IPv6-ready specification for cable modems in August 2006. The widely used DOCSIS 2.0 does not support IPv6. The new 'DOCSIS 2.0 + IPv6' standard supports IPv6, which may on the cable modem side require only a firmware upgrade. It is expected that only 60% of cable modems' servers and 40% of cable modems will be DOCSIS 3.0 by 2011. However, most ISPs that support DOCSIS 3.0 do not support IPv6 across their networks.
Other equipment which is typically not IPv6-ready ranges from VoIP devices to laboratory equipment and printers.
Shadow networks.
One side effect of IPv6 implementation may be the emergence of so-called "shadow networks" caused by IPv6 traffic flowing into IPv4 networks when IPv6 enabled nodes are added to the existing network, and the IPv4 security in place is unable to properly identify it. This may occur with operating system upgrades, when the newer OS enables IPv6 support by default, while the older one did not. Failing to update the security infrastructure to accommodate IPv6 can lead to IPv6 traffic bypassing it. Shadow networks have been found occurring on business networks in which enterprises are replacing Windows XP systems that do not have an IPv6 stack enabled by default, with Windows 7 systems, that do.
Deployment.
The 1993 introduction of Classless Inter-Domain Routing (CIDR) in the routing and IP address allocation for the Internet, and the extensive use of network address translation (NAT) delayed IPv4 address exhaustion. The final phase of exhaustion started on 3 February 2011. However, despite a decade long development and implementation history as a Standards Track protocol, general worldwide deployment of IPv6 is increasing slowly. s of September 2013[ [update]], about 4% of domain names and 16.2% of the networks on the Internet have IPv6 protocol support.
IPv6 has been implemented on all major operating systems in use in commercial, business, and home consumer environments. Since 2008, the domain name system can be used in IPv6. IPv6 was first used in a major world event during the 2008 Summer Olympic Games, the largest showcase of IPv6 technology since the inception of IPv6. Some governments including the Federal government of the United States and China have issued guidelines and requirements for IPv6 capability.
In 2009, Verizon mandated IPv6 operation and deprecated IPv4 as an optional capability for cellular (LTE) hardware. s of June 2012[ [update]], T-Mobile USA also supports external IPv6 access.
As of 2014, IPv4 still carries more than 99% of worldwide Internet traffic. s of 25 2015[ [update]], the percentage of users reaching Google services with IPv6 surpassed 6.6% for the first time, growing at about 3.0% per year, although varying widely by region. s of 18 2015[ [update]] Deployment of IPv6 on web servers also varied widely, with over half of web pages available via IPv6 in many regions, with about 13% of web servers supporting IPv6.

</doc>
<doc id="15319" url="http://en.wikipedia.org/wiki?curid=15319" title="Inca Empire">
Inca Empire

The Inca Empire (Quechua: "Tawantinsuyu",  "The Four Regions"), also known as the Incan Empire, was the largest empire in pre-Columbian America. The administrative, political, and military center of the empire was located in Cusco in modern-day Peru. The Inca civilization arose from the highlands of Peru sometime in the early 13th century, and the last Inca stronghold was conquered by the Spanish in 1572.
From 1438 to 1533, the Incas used a variety of methods, from conquest to peaceful assimilation, to incorporate a large portion of western South America, centered on the Andean mountain ranges, including, besides Peru, large parts of modern Ecuador, western and south central Bolivia, northwest Argentina, north and central Chile, and a small part of southern Colombia into a state comparable to the historical empires of Eurasia. The official language of the empire was Quechua, although hundreds of local languages and dialects of Quechua were spoken. Many local forms of worship persisted in the empire, most of them concerning local sacred "Huacas", but the Inca leadership encouraged the worship of Inti—their sun god—and imposed its sovereignty above other cults such as that of Pachamama. The Incas considered their king, the Sapa Inca, to be the "son of the sun."
Name.
The Inca referred to their empire as "Tawantinsuyu", "the four "suyo"". In Quechua, "tawa" is four and "-ntin" is a suffix naming a group, so that a "tawantin" is a quartet, a group of four things taken together. The empire was divided into four "suyus" ("region" or "province"), whose corners met at the capital, Cusco ("Qosqo"). The four "suyos" were: Chinchay Suyo (North), Anti Suyo (East. The Amazon jungle), Colla Suyo (South) and Conti Suyo (West). The name "Tawantinsuyu" was, therefore, a descriptive term indicating a union of provinces. The Spanish transliterated the name as "Tahuatinsuyo" or "Tahuatinsuyu" which is often still used today.
The term "Inka" means "ruler" or "lord" in Quechua and was used to refer to the ruling class or the ruling family in the empire. The Spanish adopted the term (transliterated as "Inca" in Spanish) as an ethnic term referring to all subjects of the empire rather than simply the ruling class. As such the name "Imperio inca" ("Inca Empire") referred to the nation that they encountered, and subsequently conquered.
History.
Origin.
Inca oral history mentions three possible places as three caves. The center cave, Tambo Tocco, was named for Capac Tocco. The other caves were Maras Tocco and Sutic Tocco. Four brothers and four sisters stepped out of the middle cave. They were: Ayar Manco, Ayar Cachi, Ayar Auca, and Ayar Uchu; and Mama Ocllo, Mama Raua, Mama Huaca, and Mama Cora. Out of the side caves came the people who were to be the ancestors of all the clans of the Inca people.
Ayar Manco carried a magic staff made of the finest gold. Where this staff landed, the people would all live there. They travelled for a very, very long time. On the way, Ayar Cachi was boasting about his great strength and power, and his siblings tricked him into returning to the cave to get a sacred llama. When he went into the cave, they trapped him inside to get rid of him.
Ayar Uchu decided to stay on the top of the cave to look over the Inca people. The minute he proclaimed that, he turned to stone. They built a shrine around the stone and it became a sacred object. Ayar Auca grew tired of all this and decided to travel alone. Only Ayar Manco and his four sisters remained.
Finally, they reached Cusco. The staff sank into the ground. Before they reached here, Mama Ocllo had already borne Ayar Manco a child, Sinchi Roca. The people who were already living in Cusco fought hard to keep their land, but Mama Huaca was a good fighter. When the enemy attacked, she threw her bolas (several stones tied together that spun through the air when thrown) at a soldier (gualla), and killed him instantly. The other people were so scared, they ran away.
After that, Ayar Manco became known as Manco Cápac, the founder of the Inca. It is said that he and his sisters built the first Inca homes in the valley with their own hands.
When the time came, Manco Cápac turned to stone like his brothers before him. His son, Sinchi Roca, became the second emperor of the Inca.
Kingdom of Cusco.
We can assure your majesty that it is so beautiful and has such fine buildings that it would even be remarkable in Spain.
”
 Francisco Pizarro
The Inca people were a pastoral tribe in the Cusco area around the 12th century. Under the leadership of Manco Cápac, they formed the small city-state Kingdom of Cusco (Quechua "Qusqu', Qosqo"). In 1438, they began a far-reaching expansion under the command of Sapa Inca (paramount leader) Pachacuti-Cusi Yupanqui, whose name literally meant "earth-shaker". The name of Pachacuti was given to him after conquering over the Tribe of Chancas (modern Apurímac). During his reign, he and his son Tupac Yupanqui brought much of the Andes mountains (roughly modern Peru and Ecuador) under Inca control.
Reorganization and formation.
Pachacuti reorganized the kingdom of Cusco into the Tahuantinsuyu, which consisted of a central government with the Inca at its head and four provincial governments with strong leaders: Chinchasuyu (NW), Antisuyu (NE), Kuntisuyu (SW), and Qullasuyu (SE). Pachacuti is also thought to have built Machu Picchu, either as a family home or as a summer retreat, although there is speculation that Machu Picchu was constructed as an agricultural station.
Pachacuti sent spies to regions he wanted in his empire; they brought reports on the political organization, military might and wealth. He would then send messages to the leaders of these lands extolling the benefits of joining his empire, offering them presents of luxury goods such as high quality textiles, and promising that they would be materially richer as subject rulers of the Inca.
Most accepted the rule of the Inca as a "fait accompli" and acquiesced peacefully. The ruler's children would then be brought to Cusco to be taught about Inca administration systems, then return to rule their native lands. This allowed the Inca to indoctrinate the former ruler's children into the Inca nobility, and, with luck, marry their daughters into families at various corners of the empire.
Expansion and consolidation.
Traditionally the Inca's son led the army; Pachacuti's son Túpac Inca Yupanqui began conquests to the north in 1463, and continued them as Inca after Pachacuti's death in 1471. His most important conquest was the Kingdom of Chimor, the Inca's only serious rival for the coast of Peru. Túpac Inca's empire stretched north into modern-day Ecuador and Colombia.
Túpac Inca's son Huayna Cápac added a small portion of land to the north in modern-day Ecuador and in parts of Peru. At its height, the Inca Empire included Peru and Bolivia, most of what is now Ecuador, a large portion of what is today Chile north of the Maule River in central Chile. The advance south halted after the Battle of the Maule where they met determined resistance by the Mapuche. The empire's push into the Amazon Basin near the Chinchipe River was pushed back by the Shuar in 1527. The empire also extended into corners of Argentina and Colombia. However, most of the southern portion of the Inca empire, the portion denominated as Qullasuyu, was located in the Altiplano.
The Inca Empire was an amalgamation of languages, cultures and peoples. The components of the empire were not all uniformly loyal, nor were the local cultures all fully integrated. The Inca empire as a whole had an economy based on exchange and taxation of luxury goods and labour. The following quote reflects a method of taxation:
Inca civil war and Spanish conquest.
Spanish conquistadors led by Francisco Pizarro and his brothers explored south from what is today Panama, reaching Inca territory by 1526. It was clear that they had reached a wealthy land with prospects of great treasure, and after one more expedition in 1529, Pizarro traveled to Spain and received royal approval to conquer the region and be its viceroy. This approval was received as detailed in the following quote: "In July 1529 the queen of Spain signed a charter allowing Pizarro to conquer the Incas. Pizarro was named governor and captain of all conquests in Peru, or New Castile, as the Spanish now called the land."
When they returned to Peru in 1532, a war of the two brothers between Huayna Capac's sons Huáscar and Atahualpa and unrest among newly conquered territories—and perhaps more importantly, smallpox, which had spread from Central America—had considerably weakened the empire.
Pizarro did not have a formidable force; with just 168 men, 1 cannon and 27 horses, he often needed to talk his way out of potential confrontations that could have easily wiped out his party.
The Spanish horsemen, fully armored, had great technological superiority over the Inca forces. The traditional mode of battle in the Andes was a kind of siege warfare where large numbers of usually reluctant draftees were sent to overwhelm opponents. The Spaniards had developed one of the finest military machines in the premodern world, tactics learned in their centuries-long fight against Moorish kingdoms in Iberia. Along with this tactical and material superiority, the Spaniards also had acquired tens of thousands of native allies who sought to end the Inca control of their territories.
Their first engagement was the Battle of Puná, near present-day Guayaquil, Ecuador, on the Pacific Coast; Pizarro then founded the city of Piura in July 1532. Hernando de Soto was sent inland to explore the interior and returned with an invitation to meet the Inca, Atahualpa, who had defeated his brother in the civil war and was resting at Cajamarca with his army of 80,000 troops.
Pizarro and some of his men, most notably a friar named Vincente de Valverde, met with the Inca, who had brought only a small retinue. Through an interpreter Friar Vincente read the "Requerimiento" that demanded that he and his empire accept the yoke of King Charles I of Spain and convert to Christianity. Because of the language barrier and perhaps poor interpretation, Atahualpa became somewhat puzzled by the friar's description of Christian faith and was said to have not fully understood the envoy's intentions. After Atahualpa attempted further enquiry into the doctrines of the Christian faith under which Pizarro's envoy served, the Spanish became frustrated and impatient, attacking the Inca's retinue and capturing Atahualpa as hostage.
Atahualpa offered the Spaniards enough gold to fill the room he was imprisoned in, and twice that amount of silver. The Inca fulfilled this ransom, but Pizarro deceived them, refusing to release the Inca afterwards. During Atahualpa's imprisonment Huáscar was assassinated elsewhere. The Spaniards maintained that this was at Atahualpa's orders; this was used as one of the charges against Atahualpa when the Spaniards finally decided to put him to death, in August 1533.
Last Incas.
The Spanish installed Atahualpa's brother Manco Inca Yupanqui in power; for some time Manco cooperated with the Spanish, while the Spanish fought to put down resistance in the north. Meanwhile an associate of Pizarro's, Diego de Almagro, attempted to claim Cusco for himself. Manco tried to use this intra-Spanish feud to his advantage, recapturing Cusco in 1536, but the Spanish retook the city afterwards. Manco Inca then retreated to the mountains of Vilcabamba, Peru, where he and his successors ruled for another 36 years, sometimes raiding the Spanish or inciting revolts against them. In 1572 the last Inca stronghold was conquered, and the last ruler, Túpac Amaru, Manco's son, was captured and executed. This ended resistance to the Spanish conquest under the political authority of the Inca state.
After the fall of the Inca Empire many aspects of Inca culture were systematically destroyed, including their sophisticated farming system, known as the vertical archipelago model of agriculture. Spanish colonial officials used the Inca mita corvée labor system for colonial aims, sometimes brutally. One member of each family was forced to work in the gold and silver mines, the foremost of which was the titanic silver mine at Potosí. When a family member died, which would usually happen within a year or two, the family would be required to send a replacement.
The effects of smallpox on the Inca empire were even more devastating. Beginning in Colombia, smallpox spread rapidly before the Spanish invaders first arrived in the empire. The spread was probably aided by the efficient Inca road system. Within a few years smallpox claimed between 60% and 94% of the Inca population, with other waves of European disease weakening them further. Smallpox was only the first epidemic.
Typhus (probably) in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, measles in 1618 – all ravaged the remains of Inca culture.
Society.
Population.
There is some debate about the number of people inhabiting Tawantinsuyu at its peak, with estimates ranging from as few as 4 million people, to more than 37 million. The reason for these various estimates is that in spite of the fact that the Inca kept excellent census records using their quipu, knowledge of how to read them has been lost, and almost all of them had been destroyed by the Spaniards in the course of their conquest.
Language.
Since the Inca Empire lacked a written language, the empire's main form of communication and recording came from quipus, ceramics and spoken Quechua, the language the Incas imposed upon the peoples within the empire. The plethora of civilizations in the Andean region provided for a general disunity that the Incas needed to subdue in order to maintain control of the empire. While Quechua had been spoken in the Andean region, like central Peru, for several years prior to the expansion of the Inca civilization, the type of Quechua the Incas imposed was an adaptation from the Kingdom of Cusco (an early form of "Southern Quechua" originally named Qhapaq Runasimi = The great language of the people) of what some historians define as the Cusco dialect.
The language imposed by the Incas further diverted from its original phonetic tone as some societies formed their own regional varieties, or slang. The diversity of Quechua at that point and even today does not come as a direct result from the Incas, who are just a part of the reason for Quechua's diversity. The civilizations within the empire that had previously spoken Quechua kept their own variety distinct from the Quechua the Incas spread. Although these dialects of Quechua have a similar linguistic structure, they differ according to the region in which they are spoken.
Although most of the societies within the empire implemented Quechua into their lives, the Incas allowed several societies to keep their old languages such as Aymara, which still remains a spoken language in contemporary Bolivia where it is the primary indigenous language and various regions of South America surrounding Bolivia. The linguistic body of the Inca Empire was thus largely varied, but it still remains quite an achievement for the Incas that went beyond their time as the Spanish continued the use of Quechua.
Religion.
Inca myths were an oral tradition until early Spanish colonists recorded them; however, some scholars believe that they may have been recorded on quipus, Andean knotted string records.
The Inca believed in reincarnation. Death was a passage to the next world that was full of difficulties. The spirit of the dead, "camaquen". would need to follow a long dark road and during the trip the assistance of a black dog that was able to see in the dark was required. Most Incas imagined the after world to be very similar to the Euro-American notion of heaven, with flower-covered fields and snow-capped mountains. It was important for the Inca to ensure they did not die as a result of burning or that the body of the deceased did not become incinerated. This is because of the underlying belief that a vital force would disappear and threaten their passage to the after world. Those who obeyed the Inca moral code—"ama suwa, ama llulla, ama quella" (do not steal, do not lie, do not be lazy) —"went to live in the Sun's warmth while others spent their eternal days in the cold earth" . The Inca also practiced cranial deformation. They achieved this by wrapping tight cloth straps around the heads of newborns in order to alter the shape of their soft skulls into a more conical form; this cranial deformation was made to distinguish social classes of the communities, with only the nobility having cranial deformation.
The Incas made human sacrifices. As many as 4,000 servants, court officials, favorites, and concubines were killed upon the death of the Inca Huayna Capac in 1527, for example. The Incas also performed child sacrifices during or after important events, such as the death of the Sapa Inca or during a famine. These sacrifices were known as "capacocha".
Economy.
The economy of the Inca Empire has been characterized as involving a high degree of central planning. While evidence of trade between the Inca Empire and outside regions has been uncovered, there is no evidence that the Incas had a substantial internal market economy. While axe-monies were used along the northern coast, presumably by the provincial "mindaláe" trading class, most inhabitants of the empire would have lived in a traditional economy in which male heads of household were required to pay taxes both in kind (e.g., crops, textiles, etc.) and in the form of the "mit'a" corvée labor and military obligations, though barter (or "trueque") was also present in some areas. In return, the state provided security, food in times of hardship through the supply of emergency resources, agricultural projects (e.g. aqueducts and terraces) to increase productivity, and occasional feasts. The economy rested on the material foundations of the vertical archipelago, a system of ecological complementarity in accessing resources, and the cultural foundation of "ayni", or reciprocal exchange.
Government.
Beliefs.
The Sapa Inca was conceptualized as divine and was effectively head of the state religion. Only the "Willaq-Umu" (or Chief Priest) was second to the emperor. Local religious traditions were allowed to continue, and in some cases such as the Oracle at Pachacamac on the Peruvian coast, were officially venerated. Following Pachacuti, the Sapa Inca claimed descent from Inti, which placed a high value on imperial blood; by the end of the empire, it was common to wed brother and sister. He was “son of the sun,” and his people the "intip churin", or “children of the sun,” and both his right to rule and mission to conquer derived from his holy ancestor. The Sapa Inca also presided over ideologically important festivals, notably during the "Inti Raymi", or “warriors’ cultivation,” attended by soldiers, mummified rulers, nobles, clerics, and the general population of Cusco beginning on the auspicious June solstice and culminating nine days later with the ritual breaking of the earth using a foot plow by the Inca himself. Moreover, Cusco itself was considered cosmologically central, loaded as it was with "huacas" and radiating "ceque" lines, and geographic center of the Four Quarters; Inca Garcilaso de la Vega himself called it “the navel of the universe.”
Organization of the empire.
The Inca Empire was a federalist system which consisted of a central government with the Inca at its head and four quarters, or "suyu": Chinchay Suyu (NW), Anti Suyu (NE), Kunti Suyu (SW), and Qulla Suyu (SE). The four corners of these quarters met at the center, Cusco. These "suyu" were likely created around 1460 during the reign of Pachacuti before the empire assumed its largest territorial extent. It is probably the case that at the time the "suyu" were established they were roughly of equal size and only later changing their proportions as the empire expanded north and south along the Andes.
The capital area, Cusco, was likely not organized as a "wamani", or province. Rather, it was probably somewhat akin to a modern federal district, like Washington, D.C. or Mexico City. The city sat at the center of the four "suyu" and served as the preeminent center of politics and religion. While Cusco was essentially governed by the Sapa Inca, his relatives, and the royal "panaqa" lineages, each "suyu" was governed by an "Apu", a term of great esteem used for men of very high status and for venerated mountains. Just as with so much of Andean society and Inca administration, both Cusco as a district and the four "suyu" as administrative regions were grouped into upper "hanan" and lower "hurin" divisions. As the Inca did not have written records, it is impossible to exhaustively list the constituent "wamani". However, records created during the Spanish colonial period allow us to reconstruct a partial list. There were likely more than 86 "wamani", with more than 48 in the highlands and more than 38 on the coast.
The four "suyu".
The most populous "suyu", Chinchaysuyu, encompassed the former lands of the Chimu empire and much of the northern Andes. At its largest extent, this "suyu" extended through much of modern Ecuador and just into modern Colombia. '
Collasuyu or Qollasuyu was named after the Aymara-speaking Qolla people and was the largest of the quarters in terms of area. This "suyu" encompassed the Bolivian Altiplano and much of the southern Andes, running down into Argentina and as far south as the Maipo or Maule river in Central Chile. In Central Chile, historian José Bengoa has pointed out Quillota as being perhaps the foremost Inca settlement.
The second smallest of the "suyu", Antisuyu, was located northwest of Cusco in high Andes. Indeed, it is the root of the word “Andes.”
Cuntisuyu or Kuntisuyu was the smallest "suyu" of all, located along the southern coast of modern Peru, extending into the highlands towards Cusco.
Laws.
The Inca state had no separate judiciary or codified set of laws. While customs, expectations, and traditional local power holders did much in the way of governing behavior, the state, too, had legal force, such as through "tokoyrikoq" (lit. "he who sees all"), or inspectors. The highest such inspector, typically a blood relation to the Sapa Inca, acted independently of the conventional hierarchy, providing a point of view for the Sapa Inca free of bureaucratic influence.
Administration.
The colonial-era sources are not entirely clear or in agreement about the nature of the structure of the Inca government. However, its basic structure can be spoken of broadly, even if the exact duties and functions of government positions cannot be told. At the top of the chain of administration sat the Sapa Inca. Next to the Sapa Inca in terms of power may have been the "Willaq Umu", literally the "priest who recounts", who was the High Priest of the Sun. However, it has been noted that beneath the Sapa Inca also sat the "Inkap rantin", who was at the very least a confidant and assistant to the Sapa Inca, perhaps along the lines of a Prime Minister. From the time of Topa Inca Yupanqui on, there existed a "Council of the Realm" composed of sixteen nobles: two from "hanan" Cusco; two from "hurin" Cusco; four from Chinchaysuyu; two from Cuntisuyu; four from Collasuyu; and two from Antisuyu. This weighting of representation balanced the "hanan" and "hurin" divisions of the empire, both within Cusco and within the Quarters ("hanan suyukuna" and "hurin suyukuna").
While there was a great deal of variation in the form that Inca bureaucracy and government took at the provincial level, the basic form of organization was decimal. In this system of organization, taxpayers—male heads of household of a certain age range—were organized into corvée labor units (which often doubled as military units) that formed the muscle of the state as part of mit'a service. Each level of jurisdiction above one hundred tax-payers was headed by a "kuraka", while those heading smaller units were "kamayuq", a lower, non-hereditary status. However, while "kuraka" status was hereditary, one's actual position within the hierarchy (which was typically served for life) was subject to change based upon the privileges of those above them in the hierarchy; a "pachaka kuraka" (see below) could be appointed to their position by a "waranqa kuraka". Furthermore, it has been suggested that one "kuraka" in each decimal level also served as the head of one of the nine groups at a lower level, so that one "pachaka kuraka" might also be a "waranqa kuraka", in effect directly responsible for one unit of 100 tax-payers and less directly responsible for nine other such units.
Arts and technology.
Monumental architecture.
Architecture was by far the most important of the Inca arts, with textiles reflecting motifs that were at their height in architecture. The main example is the capital city of Cusco. The site of Machu Picchu was constructed by Inca engineers. The stone temples constructed by the Inca used a mortarless construction that fit together so well that a knife could not be fitted through the stonework.
This was a process first used on a large scale by the Pucara (ca. 300 BC–AD 300) peoples to the south in Lake Titicaca, and later in the great city of Tiwanaku (ca. AD 400–1100) in present day Bolivia. The rocks used in construction were sculpted to fit together exactly by repeatedly lowering a rock onto another and carving away any sections on the lower rock where the dust was compressed. The tight fit and the concavity on the lower rocks made them extraordinarily stable.
Measures, calendrics, and mathematics.
Physical measures employed by the Inca were based upon human body parts. Fingers, the distance between thumb to forefinger, palms, cubits, and wingspans were among those units used. The most basic unit of distance was "thatkiy" or "thatki", or one pace. The next largest unit was reported by Cobo to be the "topo" or "tupu", measuring 6,000 "thatkiy"s, or about 4.8 mi; careful study has shown that a range of 2.5 - is likely. Next was the "wamani", composed of 30 "topo"s (roughly 144 mi). To measure area, 25 by 50 wingspans were used, reckoned in "topo"s (roughly 1266 sqmi). It seems likely as well that distance was often conceptualized as being one day's walk; the distance between "tambo" way-stations varies widely in terms of distance, but in far less so in terms of time to walk that distance.
Inca calendrics were strongly tied to astronomy. Inca astronomers understood equinoxes, solstices, and likely zenith passages, not to mention the Venus cycle. They could not, however, predict eclipses. The Inca calendar was essentially lunisolar, as two calendars were maintained in parallel, one solar and one lunar. As twelve lunar months fall 11-days short of a full 365-day solar year, those in charge of the calendar had to adjust every winter solstice. The twelve lunar months were each marked with specific festivals and rituals. There apparently were no names for days of the week, and it may be the case that there were no subdivisions of time into weeks at all. Similarly, months were not grouped into seasons. Time during a given day was not reckoned in hours or minutes, but rather in terms of how far the sun had traveled or in how long it takes to perform a task.
The sophistication of Inca administration, calendrics, and engineering necessitated a certain facility with numbers. Numerical information itself was stored in the knots of "quipu" strings, allowing for large numbers to be stored in a small amount of space. These numbers were stored in base-10 digits, the same base as used by the Quechua language and used in administrative and military units. These numbers, stored in "quipu", could be calculated on "yupanas", grids with squares of positionally varying mathematical values perhaps functioning along the lines of an abacus. Moving piles of tokens, seeds, or pebbles between the different compartments of the "yupana" allowed for calculations to take place. It is likely that, "at minimum", Inca mathematics were capable of division of integers into integers or fractions and multiplication of integers and fractions.
According to the mid-seventeenth-century Jesuit chronicler Bernabé Cobo (1983 [1653]: 253–254), the Inca designated certain officials to perform accounting-related tasks. These officials were called quipo camayos, and the Incas had great confidence in them. In the study of khipu sample VA 42527 (Museum für Völkerkunde, Berlin), Sáez-Rodríguez (2013) observed that the numbers arranged in calendrically significant patterns were used for agricultural purposes in the “farm account books” kept by the khipukamayuq (accountant or warehouse keeper) to facilitate the closing of his accounting books.
Ceramics, precious metal work, and textiles.
Almost all of the gold and silver work of the empire was melted down by the conquistadors.
Ceramics were painted using the polychrome technique portraying numerous motifs including animals, birds, waves, felines (which were popular in the Chavin culture) and geometric patterns found in the Nazca style of ceramics. In place without a written language, ceramics portrayed the very basic scenes of everyday life, including the smelting of metals, relationships and scenes of tribal warfare, it is through these preserved ceramics that we know what life was like for the ancient South Americans. The most distinctive Inca ceramic objects are the Cusco bottles or ¨aryballos¨. Many of these pieces are on display in Lima in the Larco Archaeological Museum and the National Museum of Archaeology, Anthropology and History.
Communication and medicine.
The Inca used assemblages of knotted strings, known as Quipu, to record information, the exact nature of which is no longer known. Originally it was thought that Quipu were used only as mnemonic devices or to record numerical data. Quipus are also believed to record history and literature.
The Inca made many discoveries in medicine. They performed successful skull surgery, which involved cutting holes in the skull in order to alleviate fluid buildup and inflammation caused by head wounds. Anthropologists have discovered evidence which suggests that most skull surgeries performed by Inca surgeons were successful. In pre-Inca times, only one-third of skull surgery patients survived the procedure. However, survival rates rose to 80–90% during the Inca era.
Coca.
The Incas revered the coca plant as being sacred or magical. Its leaves were used in moderate amounts to lessen hunger and pain during work, but were mostly used for religious and health purposes. When the Spaniards realized the effects of chewing the coca leaves, they took advantage of it. The Chasqui (messengers) chewed coca leaves for extra energy to carry on their tasks as runners delivering messages throughout the empire. The coca leaf was also used during surgeries as an anaesthetic.
Weapons, armor, and warfare.
The Inca army was the most powerful in the area at that time, because they could turn an ordinary villager or farmer into a soldier, ready for battle. This is because every male Inca had to take part in war at least once so as to be prepared for warfare again when needed. By the time the empire had reached its largest size, every section of the empire contributed in setting up an army for war.
The Incas had no iron or steel, and their weapons were not much better than those of their enemies. They went into battle with the beating of drums and the blowing of trumpets. The armor used by the Incas included:
The Inca weaponry included:
Roads allowed very quick movement for the Inca army, and shelters called "tambo" were built one day's distance in travelling from each other, so that an army on campaign could always be fed and rested. This can be seen in names of ruins such as Ollantay Tambo, or My Lord's Storehouse. These were set up so the Inca and his entourage would always have supplies (and possibly shelter) ready as he traveled.
Inca flag.
There are 16th and 17th century chronicles and references that support the idea of a banner. However, it represented the Inca himself, not the empire.
Francisco López de Jerez wrote in 1534:
... todos venían repartidos en sus escuadras con sus banderas y capitanes que los mandan, con tanto concierto como turcos.(... all of them came distributed into squads, with their flags and captains commanding them, as well-ordered as Turks.) 
The chronicler, Bernabé Cobo, wrote:
The royal standard or banner was a small square flag, ten or twelve spans around, made of cotton or wool cloth, placed on the end of a long staff, stretched and stiff such that it did not wave in the air, and on it each king painted his arms and emblems, for each one chose different ones, though the sign of the Incas was the rainbow and two parallel snakes along the width with the tassel as a crown, which each king used to add for a badge or blazon those preferred, like a lion, an eagle and other figures.
<br>(... el guión o estandarte real era una banderilla cuadrada y pequeña, de diez o doce palmos de ruedo, hecha de lienzo de algodón o de lana, iba puesta en el remate de una asta larga, tendida y tiesa, sin que ondease al aire, y en ella pintaba cada rey sus armas y divisas, porque cada uno las escogía diferentes, aunque las generales de los Incas eran el arco celeste y dos culebras tendidas a lo largo paralelas con la borda que le servía de corona, a las cuales solía añadir por divisa y blasón cada rey las que le parecía, como un león, un águila y otras figuras.)<br>-
Guaman Poma's 1615 book, "El primer nueva corónica y buen gobierno", shows numerous line drawings of Inca flags. In his 1847 book "A History of the Conquest of Peru", "William H. Prescott ... says that in the Inca army each company had its particular banner, and that the imperial standard, high above all, displayed the glittering device of the rainbow, the armorial ensign of the Incas." A 1917 world flags book says the Inca "heir-apparent ... was entitled to display the royal standard of the rainbow in his military campaigns."
In modern times the rainbow flag has been wrongly associated with the Tawantinsuyu and displayed as a symbol of Inca heritage by some groups in Peru and Bolivia. The city of Cusco also flies the Rainbow Flag, but as an official flag of the city. The Peruvian president Alejandro Toledo (2001–2006) flew the Rainbow Flag in Lima's presidential palace. However, according to Peruvian historiography, the Inca Empire never had a flag. María Rostworowski, a Peruvian historian known for her extensive and detailed publications about Peruvian Ancient Cultures and the Inca Empire, said about this: "«I bet my life, the Inca never had that flag, it never existed, no chronicler mentioned it»". Also, to the Peruvian newspaper "El Comercio", the flag only dates to the first decades of the 20th century, and even the Congress of the Republic of Peru has determined that flag is a fake by citing the conclusion of National Academy of Peruvian History:
"The official use of the wrongly called 'Tawantinsuyu flag' is a mistake. In the Pre-Hispanic Andean World there did not exist the concept of a flag, it did not belong to their historic context".
<br></small>National Academy of Peruvian History</small><br>
People.
Andean civilization probably began c. 9500 BP (c. 7600 BCE). Based in the highlands of Peru, an area now referred to as the "punas", the ancestors of the Incas probably began as a nomadic herding people. Geographical conditions resulted in a distinctive physical development characterized by a small stature and stocky build. Men averaged 1.57 m (5'2") and women averaged 1.45 m (4'9"). Because of the high altitudes, they had unique lung developments with almost one third greater capacity than other humans. The Incas had slower heart rates, blood volume of about 2 l (four pints) more than other humans, and double the amount of hemoglobin which transfers oxygen from the lungs to the rest of the body.
Archaeologists have found traces of permanent habitation as high as 5300 m above sea level in the temperate zone of the high altiplanos. While the Conquistadors may have been a little taller, the Inca surely had the advantage of coping with the extraordinary altitude. It seems that civilizations in this area before the Inca have left no written record, and therefore the Inca seem to appear from nowhere, but the Inca were a product of the past. They borrowed architecture, ceramics, and their empire-state government from previous cultures.
In the Lake Titikaka region, Tiwanaku is recognized by Andean scholars as one of the most important precursors to the Inca Empire, flourishing as the ritual and administrative capital of a major state power for approximately 500 years.

</doc>
<doc id="15321" url="http://en.wikipedia.org/wiki?curid=15321" title="Inca (disambiguation)">
Inca (disambiguation)

The Inca Empire was the largest empire in pre-Columbian America.
Inca, Inka, or İncə may also refer to:

</doc>
<doc id="15323" url="http://en.wikipedia.org/wiki?curid=15323" title="Internet Protocol">
Internet Protocol

The Internet Protocol (IP) is the principal communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. Its routing function enables internetworking, and essentially establishes the Internet.
IP has the task of delivering packets from the source host to the destination host solely based on the IP addresses in the packet headers. For this purpose, IP defines packet structures that encapsulate the data to be delivered. It also defines addressing methods that are used to label the datagram with source and destination information.
Historically, IP was the connectionless datagram service in the original "Transmission Control Program" introduced by Vint Cerf and Bob Kahn in 1974; the other being the connection-oriented Transmission Control Protocol (TCP). The Internet protocol suite is therefore often referred to as TCP/IP.
The first major version of IP, Internet Protocol Version 4 (IPv4), is the dominant protocol of the Internet. Its successor is Internet Protocol Version 6 (IPv6).
Function.
The Internet Protocol is responsible for addressing hosts and for routing datagrams (packets) from a source host to a destination host across one or more IP networks. For this purpose, the Internet Protocol defines the format of packets and provides an addressing system that has two functions: identifying hosts; and providing a logical location service.
Datagram construction.
Each datagram has two components: a header and a payload. The IP header is tagged with the source IP address, the destination IP address, and other meta-data needed to route and deliver the datagram. The payload is the data that is transported. This method of nesting the data payload in a packet with a header is called encapsulation.
IP addressing and routing.
IP addressing entails the assignment of IP addresses and associated parameters to host interfaces. The address space is divided into networks and subnetworks, involving the designation of network or routing prefixes. IP routing is performed by all hosts, but most importantly by routers, which transport packets across network boundaries. Routers communicate with one another via specially designed routing protocols, either interior gateway protocols or exterior gateway protocols, as needed for the topology of the network.
IP routing is also common in local networks. For example, many Ethernet switches support IP multicast operations. These switches use IP addresses and Internet Group Management Protocol to control multicast routing but use MAC addresses for the actual routing.
Reliability.
The design of the Internet protocols is based on the end-to-end principle. The network infrastructure is considered inherently unreliable at any single network element or transmission medium and assumes that it is dynamic in terms of availability of links and nodes. No central monitoring or performance measurement facility exists that tracks or maintains the state of the network. For the benefit of reducing network complexity, the intelligence in the network is purposely mostly located in the end nodes of data transmission. Routers in the transmission path forward packets to the next known, directly reachable gateway matching the routing prefix for the destination address.
As a consequence of this design, the Internet Protocol only provides best effort delivery and its service is characterized as unreliable. In network architectural language, it is a connectionless protocol, in contrast to connection-oriented modes of transmission. Various error conditions may occur, such as data corruption, packet loss, duplication and out-of-order delivery. Because routing is dynamic, meaning every packet is treated independently, and because the network maintains no state based on the path of prior packets, different packets may be routed to the same destination via different paths, resulting in out-of-order sequencing at the receiver.
Internet Protocol Version 4 (IPv4) provides safeguards to ensure that the IP packet header is error-free. A routing node calculates a checksum for a packet. If the checksum is bad, the routing node discards the packet. The routing node does not have to notify either end node, although the Internet Control Message Protocol (ICMP) allows such notification. By contrast, in order to increase performance, and since current link layer technology is assumed to provide sufficient error detection, the IPv6 header has no checksum to protect it.
All error conditions in the network must be detected and compensated by the end nodes of a transmission. The upper layer protocols of the Internet protocol suite are responsible for resolving reliability issues. For example, a host may cache network data to ensure correct ordering before the data is delivered to an application.
Link capacity and capability.
The dynamic nature of the Internet and the diversity of its components provide no guarantee that any particular path is actually capable of, or suitable for, performing the data transmission requested, even if the path is available and reliable. One of the technical constraints is the size of data packets allowed on a given link. An application must assure that it uses proper transmission characteristics. Some of this responsibility lies also in the upper layer protocols. Facilities exist to examine the maximum transmission unit (MTU) size of the local link and Path MTU Discovery can be used for the entire projected path to the destination. The IPv4 internetworking layer has the capability to automatically fragment the original datagram into smaller units for transmission. In this case, IP provides re-ordering of fragments delivered out of order.
The Transmission Control Protocol (TCP) is an example of a protocol that adjusts its segment size to be smaller than the MTU. The User Datagram Protocol (UDP) and the Internet Control Message Protocol (ICMP) disregard MTU size, thereby forcing IP to fragment oversized datagrams.
Version history.
In May 1974, the Institute of Electrical and Electronic Engineers (IEEE) published a paper entitled "A Protocol for Packet Network Intercommunication". The paper's authors, Vint Cerf and Bob Kahn, described an internetworking protocol for sharing resources using packet switching among network nodes. A central control component of this model was the "Transmission Control Program" that incorporated both connection-oriented links and datagram services between hosts. The monolithic Transmission Control Program was later divided into a modular architecture consisting of the Transmission Control Protocol at the transport layer and the Internet Protocol at the network layer. The model became known as the "Department of Defense (DoD) Internet Model" and "Internet Protocol Suite", and informally as "TCP/IP".
The Internet Protocol is one of the elements that define the Internet. The dominant internetworking protocol in the Internet Layer in use today is IPv4; the number 4 is the protocol version number carried in every IP datagram. IPv4 is described in RFC 791 (1981).
The successor to IPv4 is IPv6. Its most prominent modification from version 4 is the addressing system. IPv4 uses 32-bit addresses (c. 4 billion, or , addresses) while IPv6 uses 128-bit addresses (c. 340 undecillion, or addresses). Although adoption of IPv6 has been slow, as of June 2008[ [update]], all United States government systems have demonstrated basic infrastructure support for IPv6 (if only at the backbone level). IPv6 was a result of several years of experimentation and dialog during which various protocol models were proposed, such as TP/IX (RFC 1475), PIP (RFC 1621) and TUBA (TCP and UDP with Bigger Addresses, RFC 1347).
IP versions 0 to 3 were experimental versions, used between 1977 and 1979. The following Internet Experiment Note (IEN) documents describe versions of the Internet Protocol prior to the modern version of IPv4:
Version 5 was used by the Internet Stream Protocol, an experimental streaming protocol.
Other protocol proposals named "IPv9" and "IPv8" briefly surfaced, but had no affiliation with any international standards body, and have had no support.
On April 1, 1994, the IETF published an April Fool's Day joke about IPv9.
Security.
During the design phase of the ARPANET and the early Internet, the security aspects and needs of a public, international network could not be adequately anticipated. Consequently, many Internet protocols exhibited vulnerabilities highlighted by network attacks and later security assessments. In 2008, a thorough security assessment and proposed mitigation of problems was published. The Internet Engineering Task Force (IETF) has been pursuing further studies.

</doc>
<doc id="15328" url="http://en.wikipedia.org/wiki?curid=15328" title="Impeachment">
Impeachment

Impeachment is a formal process in which an official is accused of unlawful activity, the outcome of which, depending on the country, may include the removal of that official from office as well as criminal or civil punishment.
Etymology and history.
The word "impeachment" derives from Latin roots expressing the idea of becoming caught or entrapped, and has analogues in the modern French verb "empêcher" (to prevent) and the modern English "impede". Medieval popular etymology also associated it (wrongly) with derivations from the Latin "impetere" (to attack). (In its more frequent and more technical usage, impeachment of a witness means challenging the honesty or credibility of that person.)
The impeachment process should not be confused with a recall election, which is usually initiated by voters and can be based on "political charges", for example mismanagement. Impeachment is initiated by a constitutional body (usually legislative) and usually—but not always—stems from an indictable offense. The steps that remove the official from office are also different.
Impeachment was first used in the British political system. Specifically, the process was first used by the English "Good Parliament" against Baron Latimer in the second half of the 14th century. Following the British example, the constitutions of Virginia (1776), Massachusetts (1780) and other states thereafter adopted the impeachment mechanism; however, they restricted the punishment to "removal" of the official from office. In private organizations, a motion to impeach can be used to prefer charges.
In various jurisdictions.
Austria.
The Austrian Federal President can be impeached by the Federal Assembly ("Bundesversammlung") before the Constitutional Court. The constitution also provides for the recall of the president by a referendum. Neither of these courses has ever been taken, likely because the President is an unobtrusive and largely ceremonial figurehead who, having little power, is hardly in a position to abuse it.
Brazil.
The President of the Federative Republic of Brazil, state governors and municipal mayors may be impeached. In December 30, 1992, Fernando Collor de Mello, the 32nd President of Brazil, was removed from the presidency by the Brazilian Congress and held ineligible for eight years, due to evidence of bribery and misappropriation. In October 20, 2011, Hélio de Oliveira Santos was impeached from his position as mayor of Campinas, state of São Paulo by the City Council, after accusations of fraud and corruption.
Bulgaria.
The President of Bulgaria can be removed only for high treason or violation of the constitution. The process is started by a two-thirds majority vote of the Parliament to impeach the President, whereupon the Constitutional Court decides whether the President is guilty of the crime he is charged with. If he is found guilty, he is removed from power. No Bulgarian President has ever been impeached. The same procedure can be used to remove the Vice President of Bulgaria, which has also never happened.
Croatia.
President of Croatia: Sabor starts the impeachment process with two-thirds majority in favor of impeachment and then Constitutional Court has to accept that with two-thirds majority of justices in favor of impeachment. This has never happened in the history of the Republic of Croatia.
Czech Republic.
President of the Czech Republic can be impeached only for an act of high treason (which is not specified in the Constitution of the Czech Republic itself). The process has to start Senate of the Czech Republic which only has the right to impeach the president, this passes case to the Constitutional Court of the Czech Republic which has to decide whether the President is guilty or not. If the Court decides that the President is guilty then the President loses his office and the ability to be elected President of the Czech Republic ever again. No Czech president has ever been impeached.
Germany.
The Federal President of Germany can be impeached both by the Bundestag and by the Bundesrat for willfully violating Federal law. Once the Bundestag or the Bundesrat impeaches the president, the Federal Constitutional Court decides whether the President is guilty as charged and, if this is the case, whether to remove him or her from office. The Federal Constitutional Court also has the power to remove federal judges from office for willfully violating core principles of the federal constitution or a state constitution.
India.
The President of India can be impeached by the Parliament before the expiry of the term for violation of the Constitution. Other than impeachment, no other penalty can be given to the President for the violation of the Constitution. No Indian President has faced impeachment proceedings. Hence, the provisions for impeachment have never been tested.
Iran.
The Assembly of Experts can impeach the Supreme Leader of Iran and appoint a new one.
The President of Iran can be impeached jointly by the members of the Assembly (Majlis) and the Supreme Leader. A new presidential election is then triggered. Abolhassan Banisadr, Iran's very first president, was impeached in June 1981 and removed from the office. Mohammad-Ali Rajai was elected as the new president.
Cabinet ministers can be impeached by the members of the Assembly. Presidential appointment of a new minister is subject to a parliamentary vote of confidence. Impeachment of ministers has been a fairly commonly used tactic in the power struggle between the president and the assembly during the last several governments.
Ireland.
In the Republic of Ireland formal impeachment only applies to the Irish president. Article 12 of the Irish Constitution provides that, unless judged to be "permanently incapacitated" by the Supreme Court, the president can only be removed from office by the houses of the Oireachtas (parliament) and only for the commission of "stated misbehaviour". Either house of the Oireachtas may impeach the president, but only by a resolution approved by a majority of at least two-thirds of its total number of members; and a house may not consider a proposal for impeachment unless requested to do so by at least thirty of its number.
Where one house impeaches the president, the remaining house either investigates the charge or commissions another body or committee to do so. The investigating house can remove the president if it decides, by at least a two-thirds majority of its members, both that the president is guilty of the charge, and that the charge is sufficiently serious as to warrant the president's removal. To date no impeachment of an Irish president has ever taken place. The president holds a largely ceremonial office, the dignity of which is considered important, so it is likely that a president would resign from office long before undergoing formal conviction or impeachment.
The Republic's Constitution and law also provide that only a joint resolution of both houses of the Oireachtas may remove a judge. Although often referred to as the 'impeachment' of a judge, this procedure does not technically involve impeachment.
Italy.
In Italy, according to Article 90 of the Constitution, the President of the Republic can be impeached through a majority vote of the Parliament in joint session for high treason and for attempting to overthrow the Constitution. If so then, he is tried by the Constitutional Court integrated with sixteen citizens older than forty chosen by lot from a list compiled by the Parliament every nine years.
Italian press and political forces made use of the term "impeachment" at the attempt by some members of parliamentary opposition, to initiate the procedure provided for in Article 90 against the Presidents Francesco Cossiga (1991) and Giorgio Napolitano (2014) however these attempts failed at an inchoate stage.
Liechtenstein.
The Prince can be disposed by the popular vote. Since 2003, the constitution provides for the recall of the Prince by a referendum.
Norway.
Members of government, representatives of the national assembly (Stortinget) and Supreme Court judges can be impeached for criminal offenses tied to their duties and committed in office, according to the Constitution of 1814, §§ 86 and 87. The procedural rules were modeled after the US rules and are quite similar to them. Impeachment has been used eight times since 1814, last in 1927. Many argue that impeachment has fallen into desuetude. In cases of impeachment, an appointed court (Riksrett) takes effect.
Pakistan.
The country's ruling coalition said on August 7, 2008, that it would seek the impeachment of President Pervez Musharraf, alleging the U.S.-backed former general had "eroded the trust of the nation" and increasing pressure on him to resign. He resigned on 18 August 2008. Another kind of impeachment in Pakistan is known as the vote of less-confidence or vote of mis-understanding and has been practiced by provincial assemblies to weaken the national assembly.
Impeaching a president requires a two-thirds majority support of lawmakers in a joint session of both houses of Parliament.
Philippines.
Impeachment in the Philippines follows procedures similar to the United States. Under Sections 2 and 3, Article XI, Constitution of the Philippines, the House of Representatives of the Philippines has the exclusive power to initiate all cases of impeachment against the President, Vice President, members of the Supreme Court, members of the Constitutional Commissions (Commission on Elections, Civil Service Commission and the Commission on Audit), and the Ombudsman. When a third of its membership has endorsed the impeachment articles, it is then transmitted to the Senate of the Philippines which tries and decide, as impeachment tribunal, the impeachment case.
A main difference from US proceedings however is that only 1/3 of House members are required to approve the motion to impeach the President (as opposed to a simple majority of those present and voting in their US counterpart). In the Senate, selected members of the House of Representatives act as the prosecutors and the Senators act as judges with the Senate President presiding over the proceedings (the Chief Justice jointly presides with the Senate President if the President is on trial). Like the United States, to convict the official in question requires that a minimum of 2/3 (i.e., 16 of 24 members) of all the Members of the Senate vote in favor of conviction. If an impeachment attempt is unsuccessful or the official is acquitted, no new cases can be filed against that impeachable official for at least one full year.
Impeachable offenses and officials.
The 1987 Philippine Constitution says the grounds for impeachment include culpable violation of the Constitution, bribery, graft and corruption, and betrayal of public trust. These offenses are considered "high crimes and misdemeanors" under the Philippine Constitution.
The President, Vice President, Supreme Court justices, and members of the Constitutional Commission and Ombudsman are all considered impeachable officials under the Constitution.
Impeachment proceedings and attempts.
President Joseph Estrada was the first official impeached by the House in 2000, but the trial ended prematurely due to outrage over a vote to open an envelope where that motion was narrowly defeated by his allies. Estrada was deposed days later during the 2001 EDSA Revolution.
In 2005, 2006, 2007 and 2008, impeachment complaints were filed against President Gloria Macapagal-Arroyo, but none of the cases reached the required endorsement of 1/3 of the members for transmittal to, and trial by, the Senate.
In March 2011, the House of Representatives impeached Ombudsman Merceditas Gutierrez, becoming the second person to be impeached. On April, Gutierrez resigned prior to the Senate's convening as an impeachment court.
In December 2011, in what was described as “blitzkrieg fashion,” 188 of the 285 members of the House of Representatives voted to transmit the 56-page Articles of Impeachment against Supreme Court Chief Justice Renato Corona.
As of date, three officials had been successfully impeached by the House of Representatives, and two were not convicted. The latter, Chief Justice Renato C. Corona, on May 29, 2012 has been convicted by the Senate guilty under Article II of the 1987 Philippine Constitution (of betraying public trust.), with 20-3 votes from the Senator Judges.
Romania.
The President can be impeached by Parliament and is then suspended. A referendum then follows to determine whether the suspended President should be removed from office. President Traian Băsescu was impeached twice by the Parliament: in 2007 and more recently in July 2012. A referendum was held on May 19, 2007 and a large majority of the electorate voted against removing the president from office. For the most recent suspension a referendum was held on July 29, 2012, but was invalidated due to low turnout.
Russia.
The President of Russia can be impeached if both the State Duma (which initiates the impeachment process through the formation of a special investigation committee) and the Federation Council of Russia vote by a two-thirds majority in favor of impeachment and, additionally, the Supreme Court finds the President guilty of treason or a similarly heavy crime against the nation and the Constitutional Court confirms that the constitutional procedure of the impeachment process was correctly observed. In 1995–1999, the Duma made several attempts to impeach then-President Boris Yeltsin, but they never had a sufficient amount of votes for the process to reach the Federation Council.
South Africa.
Jacob Zuma, the President of South Africa and head of the African National Congress has been accused of using more than 262 Million Rand of taxpayers money to pay for "security upgrades" at his private Nkandla residence. He is also facing many other fraud and corruption charges. Zuma publicly declared he had taken a bond out to pay for his upgrades. A report has been under way by the but all attempts to release the report so far have been met by court interdicts to prevent "releasing classified information that would affect the Presidents security". If found guilty he faces impeachment charges. 
The 'security cluster' of ministers, have threatened to prosecute those who publish pictures of Pres Zuma's Nkandla residence.The report was finally released on 19 March 2014 by the Public Protector of South Africa, finding that Jacob Zuma's actions were indeed inconsistent with the Constitution of South Africa, the country's foremost opposition party, the Democratic Alliance, has launched a new campaign, including a public petition in which it calls for Jacob Zuma's impeachment.
Republic of China (Taiwan).
In Taiwan (officially called the ROC) officials can be impeached by a two-thirds vote in the Legislative Yuan together with an absolute majority in a referendum.
Ukraine.
During the crisis of which started in November 2013, the increasing political stress of the face-down between the protestors occupying Independence Square in Kiev and the State Security forces under the control of President Yanukovych led to deadly armed force being used on the protestors. Following the negotiated return of Kiev's City Hall on 16 Feb 2014, occupied by the protesters since November 2013, the security forces thought they could also retake "Maidan", Independence Square. The ensuing fighting 17-21 Feb 2014 resulted in a considerable number of deaths and a more generalised alienation of the population, and the withdrawal of President Yanukovych to his support area in the East of Ukraine.
In the wake of the President's departure, Parliament convened on 22 February; it reinstated the 2004 Constitution, which reduced Presidential authority, and voted impeachment of President Yanukovych as "de facto" recognition of his departure from office as President of an integrated Ukraine. The President riposted that Parliament's acts were illegal as they could pass into law only by Presidential signature.
United Kingdom.
In the United Kingdom, at least in theory, all persons, whether peers or commoners, may be prosecuted and tried by the two houses of the Parliament of the United Kingdom for any crimes whatsoever. The first recorded impeachment is that of William Latimer, 4th Baron Latimer during the Good Parliament of 1376. The last was that of Henry Dundas, 1st Viscount Melville in 1806.
Procedure.
In the United Kingdom, it is the House of Commons that holds the power of initiating an impeachment. Any member may make accusations of any crime. The member must support the charges with evidence and move for impeachment. If the Commons carries the motion, the mover receives orders to go to the bar at the House of Lords and to impeach the accused "in the name of the House of Commons, and all the commons of the United Kingdom."
The mover must tell the Lords that the House of Commons will, in due time, exhibit particular articles against the accused, and make good the same. The Commons then usually selects a committee to draw up the charges and create an "Article of Impeachment" for each. (In the case of Warren Hastings, however, the drawing up of the articles preceded the formal impeachment.) Once the committee has delivered the articles to the Lords, replies go between the accused and the Commons via the Lords. If the Commons have impeached a peer, the Lords take custody of the accused, otherwise custody goes to Black Rod. The accused remains in custody unless the Lords allow bail. The Lords set a date for the trial while the Commons appoints managers, who act as prosecutors in the trial. The accused may defend by counsel.
The House of Lords hears the case. The procedure used to be that the Lord Chancellor presided (or the Lord High Steward if the defendant was a peer); but this was when the Lord Chancellor was both the Lords' presiding officer and head of the judiciary of England and Wales. Since both these roles were removed from that office by the Constitutional Reform Act 2005, which created the Lord Speaker to preside over the Lords and made the Lord Chief Justice head of the judiciary, it is not certain who would preside over an impeachment trial today. If Parliament is not in session, then the trial is conducted by a "Court of the Lord High Steward" instead of the House of Lords (even if the defendant is not a peer). The differences between this court and the House of Lords are that in the House all of the peers are judges of both law and fact, whereas in the Court the Lord High Steward is the sole judge of law and the peers decide the facts only; and the bishops are not entitled to sit and vote in the Court.
The hearing resembles an ordinary trial: both sides may call witnesses and present evidence. At the end of the hearing the lords vote on the verdict, which is decided by a simple majority, one charge at a time. Upon being called, a lord must rise and declare "guilty, upon my honour" or "not guilty, upon my honour". After voting on all of the articles has taken place, and if the Lords find the defendant guilty, the Commons may move for judgment; the Lords may not declare the punishment until the Commons have so moved. The Lords may then decide whatever punishment they find fit, within the law. A royal pardon cannot excuse the defendant from trial, but a pardon may reprieve a convicted defendant. However, a pardon cannot override a decision to remove the defendant from the public office they hold.
History.
Parliament has held the power of impeachment since medieval times. Originally, the House of Lords held that impeachment could only apply to members of the peerage (nobles), as the nobility (the Lords) would try their own peers, while commoners ought to try their peers (other commoners) in a jury. However, in 1681, the Commons declared that they had the right to impeach whomsoever they pleased, and the Lords have respected this resolution. Offices held "during good behaviour" are terminable by the writ of either "quo warranto" or "scire facias", which has even been employed by and against well-placed judges.
After the reign of Edward IV, impeachment fell into disuse, the bill of attainder becoming the preferred form of dealing with undesirable subjects of the Crown. However, during the reign of James I and thereafter, impeachments became more popular, as they did not require the assent of the Crown, while bills of attainder did, thus allowing Parliament to resist royal attempts to dominate Parliament. The most recent cases of impeachment dealt with Warren Hastings, Governor-General of India between 1773 and 1786 (impeached in 1788; the Lords found him not guilty in 1795), and Henry Dundas, 1st Viscount Melville, First Lord of the Admiralty, in 1806 (acquitted). The last attempted impeachment occurred in 1848, when David Urquhart accused Lord Palmerston of having signed a secret treaty with Imperial Russia and of receiving monies from the Tsar. Palmerston survived the vote in the Commons; the Lords did not hear the case.
Queen Caroline.
Queen Caroline, consort of King George IV, was tried by the House of Commons and acquitted. Whether or not this was an actual impeachment is controversial.
Modern politics.
The procedure has, over time, become rarely used and some legal authorities (such as Halsbury's Laws of England) consider it to be probably obsolete. The principles of "responsible government" require that the Prime Minister and other executive officers answer to Parliament, rather than to the Sovereign. Thus the Commons can remove such an officer through a— motion of no confidence without a long, drawn-out impeachment. However, it is argued by some that the remedy of impeachment remains as part of British constitutional law, and that legislation would be required to abolish it. Furthermore, impeachment as a means of punishment for wrongdoing, as distinct from being a means of removing a minister, remains a valid reason for accepting that it continues to be available, at least in theory.
The Select Committee on Parliamentary Privilege in 1967 recommended "that the right to impeach, which has long been in disuse, be now formally abandoned". Their recommendation not having been implemented in the meantime, the Select Committee on Privileges in 1977 declared it "to be of continuing validity" and again urged that it be adopted. Shortly before this report was issued, in April 1977 the Young Liberals' annual conference unanimously passed a motion calling on Liberal Party leader David Steel to move for the impeachment of Ronald King Murray QC, the Lord Advocate, over his handling of the Patrick Meehan miscarriage of justice affair. Steel did not move any such motion but Murray (now Lord Murray, a former Senator of the College of Justice of Scotland) agreed that the power still existed.
The Joint Committee on Parliamentary Privilege in 1999 noted the previous recommendations to formally abandon the power impeachment, and stated that "The circumstances in which impeachment has taken place are now so remote from the present that the procedure may be considered obsolete". Notwithstanding, on 25 August 2004, Plaid Cymru MP Adam Price announced his intention to move for the impeachment of Tony Blair for his role in involving Britain in the 2003 invasion of Iraq. He asked the Leader of the House of Commons Peter Hain whether he would confirm that the power to impeach was still available, reminding Hain that as President of the Young Liberals he had supported the attempted impeachment of Murray. Hain responded by quoting the 1999 Joint Committee's report, and the advice of the Clerk of the House of Commons that impeachment "effectively died with the advent of full responsible Parliamentary government".
Lutfur Rahman was the directly elected mayor of Tower Hamlets, in London until he was removed from office for breaching electoral rules.
United States.
Similar to the British system, Article One of the United States Constitution gives the House of Representatives the sole power of impeachment and the Senate the sole power to try impeachments. Unlike the British system, impeachment is only the first of two stages, and conviction requires a two-thirds vote. Impeachment does not necessarily result in removal from office; it is only a legal statement of charges, parallel to an indictment in criminal law. An official who is impeached faces a second legislative vote (whether by the same body or another), which determines conviction, or failure to convict, on the charges embodied by the impeachment. Most constitutions require a supermajority to convict. Although the subject of the charge is criminal action, it does not constitute a criminal trial; the only question under consideration is the removal of the individual from office, and the possibility of a subsequent vote preventing the removed official from ever again holding political office in the jurisdiction where he was removed. Impeachment with respect to political office should not be confused with witness impeachment.
Impeachable offenses.
In the United States, impeachment can occur both at the federal and state level. The Constitution defines impeachment at the federal level and limits impeachment to "The President, Vice President, and all civil officers of the United States" who may be impeached and removed only for "treason, bribery, or other high crimes and misdemeanors". Several commentators have suggested that Congress alone may decide for itself what constitutes a "high crime or misdemeanor", especially since "Nixon v. United States" stated that the Supreme Court did not have the authority to determine whether the Senate properly "tried" a defendant.
In 1970, then-House Minority Leader Gerald R. Ford defined the criterion as he saw it: "An impeachable offense is whatever a majority of the House of Representatives considers it to be at a given moment in history."
Officials subject to impeachment.
The central question regarding the Constitutional dispute about the impeachment of members of the legislature is whether members of Congress are officers of the United States. The Constitution grants the House the power to impeach "The President, the Vice President, and all civil Officers of the United States." It has been suggested that members of Congress are not officers of the United States. Others, however, believe that members are civil officers and are subject to impeachment.
The House of Representatives impeached Senator William Blount in 1798, resulting in his expulsion. However, after initially hearing his impeachment, charges were dismissed for lack of jurisdiction. Left unsettled was the question whether members of Congress were civil officers of the United States. The House has not impeached a Member of Congress since Blount. As each House has the authority to expel its own members without involving the other chamber, expulsion has been the method used for removing Members of Congress.
Jefferson's Manual, which is integral to the Rules of the House of Representatives, states that impeachment is set in motion by charges made on the floor, charges preferred by a memorial, a member's resolution referred to a committee, a message from the president, charges transmitted from the legislature of a state or territory or from a grand jury, or from facts developed and reported by an investigating committee of the House. It further states that a proposition to impeach is a question of high privilege in the House and at once supersedes business otherwise in order under the rules governing the order of business.
Process.
At the federal level, the impeachment process is a two-step procedure. The House of Representatives must first pass, by a simple majority of those present and voting, articles of impeachment, which constitute the formal allegation or allegations. Upon passage, the defendant has been "impeached". Next, the Senate tries the accused. In the case of the impeachment of a president, the Chief Justice of the United States presides over the proceedings. For the impeachment of any other official, the Constitution is silent on who shall preside, suggesting that this role falls to the Senate's usual presiding officer. This may include the impeachment of the vice president, although legal theories suggest that allowing a defendant to be the judge in his own case would be a blatant conflict of interest. If the Vice President did not preside over an impeachment (of anyone besides the President), the duties would fall to the President pro tempore of the Senate.
To convict the accused, a two-thirds majority of the senators present is required. Conviction removes the defendant from office. Following conviction, the Senate may vote to further punish the individual by barring him from holding future federal office, elected or appointed. Conviction by the Senate does not bar criminal prosecution. Even after an accused has left office, it is possible to disqualify the person from future office or from certain emoluments of his prior office (such as a pension). If there is no charge for which a two-thirds majority of the senators present vote "guilty", the defendant is acquitted and no punishment is imposed.
History of federal impeachment proceedings in the United States.
Congress regards impeachment as a power to be used only in extreme cases; the House has initiated impeachment proceedings only 64 times since 1789 (most recently against Judge Thomas Porteous of the United States District Court for the Eastern District of Louisiana) with only the following 19 of these proceedings actually resulting in the House passing Articles of Impeachment:
Richard Nixon was never impeached. While the House Judiciary Committee did approve articles of impeachment against him and did report those articles to the House of Representatives, Nixon resigned before the House could consider the impeachment resolutions and was subsequently pardoned by President Ford.
Venezuela.
Any elected authority (including governors, vice-president and president) can be impeached after half the period for which the authority was elected. This provided that no less than 20% of the registered voters in the jurisdiction ask for the process to start and afterwards more or the same number of people that elected the authority, vote in favor of the removal of the latter. The president can also be removed from charge if the supreme court rules so, or in case of mental insanity or physical incapacity.

</doc>
<doc id="15334" url="http://en.wikipedia.org/wiki?curid=15334" title="Ibizan Hound">
Ibizan Hound

The Ibizan Hound (Catalan: "ca eivissenc") is a lean, agile dog of the hound family. There are two hair types of the breed: smooth and wire. The more commonly seen type is the smooth. Some consider there to be a third type, long, but the longhair is most likely a variation of the wire.
Description.
Appearance.
The Ibizan Hound is an elegant and agile breed, with an athletic and attractive outline and a ground-covering springy trot. Though graceful in appearance, it has good bone girth and is a rugged/hardy breed. Its large upright ears - a hallmark of the breed - are broad at the base and frame a long and elegant headpiece. The neck is long and lean. It has a unique front assembly with well laid-back shoulders and relatively straight upper arm. Coming in both smooth and wire-coated varieties, their coat is a combination of red and white with the nose, ears, eye rims, and pads of feet being a light tan color. Its eyes are a striking amber color and have an alert and intelligent expression. The Ibizan may range in height, depending on which Standard you follow, from 22 to and weigh from 45 to, males being larger than females. The Ibizan Hound coat can also range from smooth to Wire. wire coats are to be hard and can range from 1-3 inches in length. 
Temperament.
Ibizan Hounds are intelligent, active, and engaging by nature. They rank 53rd in Stanley Coren's The Intelligence of Dogs, being of average working/obedience intelligence, but many Ibizan owners will enjoy giving you a multitude of examples of their problem solving abilities. They are true "clowns" of the dog world, delighting in entertaining their people with their antics. Though somewhat independent and stubborn at times, they do take well to training if positive methods are used, but will balk at punitive training methods. They are generally quiet, but will alarm bark if necessary, so they make good watch dogs. They are sensitive hounds, and very good around children and other dogs alike. They generally make good house dogs, but are active and athletic, therefore need a lot of daily exercise. They do not make good kennel dogs. Ibizan hounds are sweet, but they are very stubborn and independent.
Ibizan Hounds are "escapologists": they are able to jump incredible heights from a stand still, so they need very tall fences. They also have been known to climb, and many can escape from crates, open baby gates and even locks. They have a strong prey drive, therefore they cannot be trusted off leash unless in a safely enclosed area. Once off the leash, they might not come back for a long time! A hound that knows where its home is and the surrounding area will usually return unscathed.
Health.
The Ibizan Hound is typical of the Hound Group in that it rarely suffers from hereditary illness. Minor health concerns for the breed include seizures and allergies; very rarely, one will see axonal dystrophy, cataract, retinal dysplasia and deafness in the breed. Ibizan Hound owners should have their dogs' eyes tested by a veterinarian before breeding. CERF and BAER testing is recommended for the breed. Ibizan Hounds are sensitive to barbiturate anesthesia, and typically live between 12 and 14 years.
History and use.
This breed originates in the island of Eivissa and has been traditionally used in the Catalan-speaking areas of Spain and France to hunt rabbits and other small game. The Ibizan Hound is a fast dog that can hunt on all types of terrain, working by scent, sound and sight. Hunters run these dogs in mostly female packs, with perhaps a male or two, as the female is considered the better hunter.
Traditionally a farmer may have 1 dog and a very well off farmer 2 dogs to catch rabbits for food. However in the last 20 years it is seen as a sport where between 5 and 15 dogs can be seen in the chase of one rabbit. Mainly on weekends with pack owners from the towns, some of which are known for not treating their dogs with the respect they deserve.
The Ibizan Hound authority Miquel Rosselló has provided a detailed description of a working trial which characterises their typical hunting technique and action, strikingly illustrated with action photos by Charles Camberoque which demonstrate hunt behaviour and typical hunt terrain. 
While local hunters will at times use one dog or a brace, and frequently packs of 6-8 or as many as 15, the working trial requires an evaluation of one or two braces. A brace is called a "colla". The couples should be tested on at least 2 to 5 rabbits (not hares), without the use of any other hunting aid. An inspection and evaluation of the exterior, fitness, character and obedience of the dogs is recommended prior to the hunt. 
The trial is qualified as having 5 parts. The dogs should show: (1) careful tracking and scenting of the rabbit, without being distracted in the least, 0-30 points; (2) correct signalling of the game, patient stand, strong jump into the air, obedience 0-10 points; (3) chase, giving tongue, speed, sureness, anticipation 0-30 points; (4) putting the game to cover at close quarters, listening, waiting, obedience, correct attack 0-10 point; and (5) good catch, or correct indication of the game’s location, retrieval, obedience 0-20 points.
Individual dogs are expected to show a great degree of discipline, obedience and co-operation. They should be extremely agile, have good speed and a powerful vertical jump from a stationary position in rough and often heavily covered ground. They should have excellent scent-tracking abilities, give tongue at the right time when approaching the game closely, and otherwise be silent so that they can locate the game by sound. 
The Ibizan Hound is similar in function and type to several breeds, such as the Pharaoh Hound, the Cirneco dell'Etna, the Portuguese Podengo, and the Podenco Canario. The Ibizan Hound is the largest of these breeds, classified by the Fédération Cynologique Internationale as primitive types.
This breed is considered by most experts one of the most ancient dog breeds. It is believed the Ibizan Hound evolves from the "tesem", the ancient Egyptian hunting dog. Representations of this dog on the walls of ancient tombs show a striking similarity to the modern Ibizan Hound. These dogs would have been brought to the island of Eivissa by the Phoenicians, who founded settlements there as early as the 8th century BC. A recent DNA analysis 
did not find support for this opinion and did not include the Ibizan Hound among their identified ancient dog breeds. A more recent article 
argues that continued trait selective breeding may be behind this lack of support. Heidi G. Parker, the lead author of the original study has stated recently that indeed their original findings do not imply that the Ibizan Hound is not an ancient breed and that with better tools they would in all likelihood be able to trace a continuous lineage of thousands of years for many dog breeds and add them to their group of ancient dogs.
In the United States, the Ibizan Hound is frequently competed in lure coursing through the AKC and ASFA, and also competes in LGRA straight racing and NOTRA oval track racing. Some parts of the country also use them for coursing live prey, generally jackrabbits.
The Ibizan Hound breed is recognized by the Fédération Cynologique Internationale, Continental Kennel Club, American Kennel Club, United Kennel Club, Kennel Club of Great Britain, Canadian Kennel Club, National Kennel Club, New Zealand Kennel Club, Australian National Kennel Council, America's Pet Registry, and American Canine Registry. It was fully recognized by the American Kennel Club in 1979.
In folk culture.
According to journalist Norman Lewis, when an owner no longer wants to own one of these dogs (having too much of an appetite, for instance), it is considered very bad luck to kill the dog. Instead, they release the dog on the other side of the island, so that someone else might 'adopt' the animal.

</doc>
<doc id="15335" url="http://en.wikipedia.org/wiki?curid=15335" title="Irish Wolfhound">
Irish Wolfhound

The Irish Wolfhound (Irish: "Cú Faoil", ]) is a breed of domestic dog ("Canis lupus familiaris"), specifically a sighthound. The name originates from its purpose (wolf hunting with dogs) rather than from its appearance. The breed was originally developed from war hounds to one used for hunting and guarding. Irish Wolfhounds can be an imposing sight due to their formidable size; they are the tallest of all dog breeds.
Appearance.
The standard of The American Kennel Club describes the breed as "Of great size and commanding appearance, the Irish Wolfhound is remarkable in combining power and swiftness with keen sight. The largest and tallest of the galloping hounds, in general type he is a rough-coated, Greyhound-like breed; very muscular, strong though gracefully built; movements easy and active; head and neck carried high, the tail carried with an upward sweep with a slight curve towards the extremity".
In actuality, the Irish wolfhound is the tallest of the galloping hounds as well as the tallest of any dog in any of the seven AKC dog groups (sporting, non-sporting, herding, hound, working, terrier, and toy). The average height of an Irish wolfhound should be taller than that of a Great Dane. However, the wolfhound is not to be confused with being the heaviest, as its structure should be similar to that of a Greyhound, or any sight-hound for that matter (examples being whippets and Afghan hounds). The hound should have a very broad and deep chest that tucks up. The colours allowed by the American Kennel Club are "grey, brindle, red, black, pure white, fawn, wheaten and steel grey".
The Irish wolfhound was bred for long solitary hunts based solely off of the dog's ability to visualize its landscape and perceive, unlike scent hounds (such as Bloodhounds and Beagles) who rely on scent rather than sight. For this reason, the neck of an Irish wolfhound should be long with the head held high the majority of the time. The Irish wolfhound should also appear to be longer than it is tall. 
Once used to hunt wolves, an Irish wolfhound’s structure should appear as if it is “fast enough to catch a wolf, and strong enough to kill it”. 
The American Kennel Club allows "any other color that appears in the Deerhound". The size as specified by the AKC is "Minimum height for mature males: 32 inches, females: 30 inches. Minimum weight: 120lbs for males, 105 lbs for females. It is not rare to see modern day female hounds reaching the minimal height requirements of those of male hounds; most females are well over 30 inches and in most AKC conformation shows a wolfhound’s height is looked at with as much importance as the hound’s head and face structure. 
Great size, including height of shoulder and proportionate length of body is to be aimed at, and it is desired to firmly establish a breed that shall average (minimum) from 32-34in. in dogs". The height/weight standards in Ireland and England are slightly different.
Temperament.
Unlike many other breeds, Irish wolfhounds have a varied range of personalities and are most often noted for their personal quirks and individualism. An Irish wolfhound however, is rarely mindless, and despite its large size, is rarely found to be destructive in the house or boisterous. This is because the breed is generally introverted, intelligent, and reserved in character. An easygoing animal, Irish Wolfhounds are quiet by nature. Wolfhounds often create a strong bond with their family and can become quite destructive or morose if left alone for long periods of time. An Irish wolfhound is not a guard dog and will protect individuals rather than the house or the owner’s possessions. However independent the wolfhound is, the breed becomes attached to both owners and other dogs they are raised with and is therefore not the most adaptable of breeds. Bred for independence, an Irish wolfhound is not necessarily keen on defending spaces. A wolfhound is most easily described by its historical motto, “gentle when stroked, fierce when provoked”. Despite the need for their own people, Wolfhounds generally are somewhat stand-offish with total strangers. They should not be territorially aggressive to other domestic dogs but are born with specialized skills and it is common for hounds at play to course another dog. This is a specific hunting behavior, not a fighting or territorial domination behavior. Most Wolfhounds are very gentle with children. The Irish Wolfhound is relatively easy to train. They respond well to firm, but gentle, consistent leadership. However, historically these dogs were required to work at great distances from their masters and think independently when hunting rather than waiting for detailed commands and this can still be seen in the breed.
The Wolfhound of today is far from the one that struck fear into the hearts of the Ancient Romans. Irish Wolfhounds are often favored for their loyalty, affection, patience and devotion. Although at some points in history they have been used as watchdogs, unlike some breeds, the Irish Wolfhound is usually unreliable in this role as they are often friendly toward strangers, although their size can be a natural deterrent. However, when protection is required this dog is never found wanting. When they or their family are in any perceived danger they display a fearless nature. Author and Irish Wolfhound breeder Linda Glover believes the dogs' close affinity with humans makes them acutely aware and sensitive to ill will or malicious intentions leading to their excelling as a guardian rather than guard dog.
Health.
Like many large dog breeds, Irish Wolfhounds have a relatively short lifespan. Published lifespan estimations vary between 6 and 10 years with 7 years being the average. Dilated cardiomyopathy and bone cancer are the leading cause of death and like all deep-chested dogs, gastric torsion (bloat) is common; the breed is affected by hereditary intrahepatic portosystemic shunt.
In a privately funded study conducted under the auspices of the Irish Wolfhound Club of America and based on an owner survey, Irish Wolfhounds in the United States from 1966 to 1986 lived to a mean age of 6.47 and died most frequently of bone cancer. A more recent study by the UK Kennel Club puts the average age of death at 7 years.
Studies have shown that Neutering is associated with a higher risk of bone cancer in various breeds, with one study suggesting that castration of male Irish Wolfhounds should be avoided at least until the dog is fully grown.
Irish wolfhounds should not receive additional supplements when a good dog food is used. It is generally accepted that they should be fed a low protein adult dog food (19 to 21% protein) from puppyhood onward. Most breeders today recommend that they not be supplemented to slow their rapid growth.
Irish wolfhounds are the tallest of all dog breeds, sometimes reaching 7 feet tall on their hind legs. They are well suited to rural life, but their medium energy profile allows them to adjust fairly well to suburban and urban life as well, provided they receive appropriate exercise.
Genetically, the Irish wolfhound as a breed is threatened by a bottleneck related to the over-use of a popular sire.
History.
The breed is very old; there are suggestions it may have been brought to Ireland as early as 7000 BC. These dogs are mentioned, as cú (variously translated as hound, Irish hound, war dog, wolf dog, etc.) in Irish laws and in Irish literature which dates from the 5th century or, in the case of the Sagas, from the old Irish period - AD 600-900. The word "Cu" often became an added respected prefix on the names of warriors as well as kings denoting that they were worthy of the respect and loyalty of a Cu.
Pre-19th century.
Ancient wood cuts and writings have placed them in existence as a breed by 273 BC. However there is indication that huge dogs existed even as early as 279 BC when the Tectosages and Tolistobogii Celts sacked Delphi. Survivors left accounts of the fierce Celts and the huge dogs who fought with them and at their side. They were mentioned by Julius Caesar in his treatise, The Gallic Wars, and by 391 AD, they were written about by Roman Consul, Quintus Aurelius Symmachus, who received seven of them, "canes Scotici", as a gift to be used for fighting lions, bears, that in his words, "all Rome viewed with wonder".
Wolfhounds were bred as hunting dogs by the ancients, who called them "Cú Faoil". The Irish continued to breed them for this purpose, as well as to guard their homes and protect their stock. Cúchulain, a name which translates literally as "hound of Culain", gained his name when as a child, known then as Setanta, he slew the ferocious guard dog of Culain forcing him to offer himself as a replacement.
During the English Conquest of Ireland, only the nobility were allowed to own Irish Wolfhounds, the numbers permitted depending on position. They were much coveted and were frequently given as gifts to important personages and foreign nobles. Wolfhounds were the companions of the regal, and were housed themselves alongside them. King John of England, in about 1210 presented an Irish hound, Gelert to Llewellyn, a prince of Wales. The poet The Hon William Robert Spencer immortalised this hound in a poem.
In his "Historie of Ireland" completed 1571, Blessed Edmund Campion gives a description of the hounds used for hunting the wolves on the Dublin and Wicklow mountains. He says: They (the Irish) are not without wolves and greyhounds to hunt them, bigger of bone and limb than a colt. Due to their popularity overseas many were exported to European royal houses leaving numbers in Ireland depleted. This led to a declaration by Oliver Cromwell himself being published in Kilkenny on 27 April 1652 to ensure that sufficient numbers remained to control the wolf population.
References to the Irish wolfhound in the 18th century tell of its great size, strength and greyhound shape as well as its scarcity. Writing in 1790, Bewick described it as the largest and most beautiful of the dog kind; about 36 inches high, generally of a white or cinnamon colour, somewhat like the Greyhound but more robust. He said that their aspect was mild, disposition peaceful, and strength so great that in combat the Mastiff or Bulldog was far from being an equal to them. The last wolf in Ireland is thought to have been killed at Myshall, Co Carlow in 1786 by a pack of wolfdogs kept by a Mr Watson of Ballydarton. The remaining hounds in the hands of a few families who were mainly descendants of the old Irish chieftains, were now symbols of status rather than hunters, they were said to be the last of their race.
Modern wolfhound.
Englishman Captain George Augustus Graham is responsible with a few other breeders for reaffirming the dogs' existence. In 1879 he wrote: "It has been ascertained beyond all question that there are few specimens of the breed still left in Ireland and England to be considered Irish wolfhounds, though falling short of the requisite dimensions. This blood is now in my possession." Captain Graham devoted his life to ensuring the survival of the Irish wolfhound. Owing to the small numbers of surviving specimens outcrossing was used in the breeding programme. It is believed that 
Borzoi, Great Dane, Scottish Deerhound and English Mastiff dogs all played their part in Graham's creation of the dog we currently know. The famous English Mastiff Garnier's Lion was bred to the Deerhound Lufra, and their offspring Marquis enters Wolfhound pedigrees through his granddaughter Young Donagh. Graham included "a single outcross of Tibetan Wolf Dog". This was long assumed to have been a Tibetan Mastiff. However, a photograph of "Wolf" shows a bearded, long-coated dog—what would now be called a "Tibetan Kyi Apso" or "dokhyi apso". In 1885 Captain Graham with other breeders founded the Irish Wolfhound Club, and the Breed Standard of Points to establish and agree the ideal to which breeders should aspire.
The Irish Wolfhound is sometimes regarded as the national dog breed of Ireland but in fact no breed has ever been officially adopted as such. The Wolfhound was historically a dog that only nobles could own and was taken up by the British during their rule in Ireland. This made it unpopular as a national symbol and the Kerry Blue Terrier was adopted by Republicans such as Michael Collins. However, in recent years, the Wolfhound has been adopted as a symbol by both rugby codes, which are organised on an All-Ireland basis. The national rugby league team are nicknamed the wolfhounds, and the Irish Rugby Football Union, which governs rugby union, changed the name of the country's A (second-level) national team in that code to the Ireland Wolfhounds in 2010.
References in modern culture.
The domestic dogs in "" are modeled after the Irish Wolfhound. An Irish Wolfhound appears in the cinematic trailer for as it serves a group of Roman legionaries.
In the 1987 film "The Princess Bride", an Irish Wolfhound is briefly seen as the pet of the villain Prince Humperdinck.
In the "Sam & Max" video game series, Sam is an Anthro-Irish Wolfhound.
The protagonist of "The Iron Druid Chronicles" Atticus O'Sullivan, owns an Irish Wolfhound named Oberon.

</doc>
<doc id="15336" url="http://en.wikipedia.org/wiki?curid=15336" title="Italian Greyhound">
Italian Greyhound

The Italian Greyhound is a small breed of dog of the sight hound type, sometimes called an "I.G." or an "Iggy". The origins of the breed are unknown.
The Italian Greyhound is the smallest of the sighthounds, typically weighing about 8 to and standing about 13 to tall at the withers. They are in the toy group based on their weight, but are larger than other dogs in the category due to their slender bodies.
Description.
Appearance.
The Italian Greyhound is the smallest of the sighthounds, typically weighing about 8 to and standing about 13 to tall at the withers. Though they are in the "toy" group based on their weight, they are larger than other dogs in the category due to their slender bodies, so owners must be careful when sizing clothing or accommodations.
The Italian Greyhound's chest is deep, with a tucked up abdomen, long slender legs and a long neck that tapers down to a small head. The face is long and pointed, like a full sized greyhound. Overall, they look like "miniature" Greyhounds. Though many Italian Greyhound owners dispute the use of the term "miniature Greyhound" in reference to the breed itself, by definition of the American Kennel Club they are true genetic greyhounds, with a bloodline extending back over 2,000 years. Their current small stature is a function of selective breeding. Their gait is distinctive and should be high stepping and free, rather like that of a horse. They are able to run at top speed with a double suspension gallop, and can achieve a top speed of up to 25 mph.
The color of the coat is a subject of much discussion. For The Kennel Club (UK), the American Kennel Club, and the Australian National Kennel Council, parti colored Italian Greyhounds are accepted, while the Fédération Cynologique Internationale standard for international shows allows white only on the chest and feet.
The modern Italian Greyhound's appearance is a result of breeders throughout Europe, particularly Austrian, German, Italian, French and British, making great contributions to the forming of this breed. The Italian Greyhound should resemble a small Greyhound, or rather a Sloughi, though they are in appearance more elegant and graceful.
Temperament.
The Italian Greyhound makes a good companion dog and enjoys the company of people. However, the breed's slim build and short coat make them somewhat fragile, and injury can result from rough or careless play with children. The breed is good with the elderly or a couple without any children for it prefers a quiet household but they are also generally fine with older children. They also are equally at home in the city or the country, although they tend to do best in spacious areas. They are fast, agile and athletic. Like any dog, daily exercise is a must for a happier, well-adjusted pet. Italian greyhounds love to run. The young dog is often particularly active, and this high level of activity may lead them to attempt ill-advised feats of athleticism that can result in injury. Due to their size, and in some lineages poor bone density, they are prone to broken legs. Italian Greyhounds make reasonably good watchdogs, as they bark at unfamiliar sounds. They may also bark at passers-by and other animals. However, they should not be considered "true" guard dogs as they are often aloof with strangers and easily spooked to run.
As gazehounds, Italian Greyhounds instinctively hunt by sight and have an extremely high predator drive. Owners of Italian Greyhounds should typically keep their dogs leashed at all times when not in an enclosed area to avoid the risk of even a well-behaved pet breaking away at high speed after a small animal. Like most sight hounds, the Italian Greyhound's slender skulls are near the same width of the dog's neck, and the use of a Martingale Collar is advised for walking Italian Greyhounds, it tightens up when pulled while remaining comfortably slack when the dog is walking politely. This prevents the dog from backing out and escaping. Breakaway collars are advised for identification, because this active and acrobatic breed could easily injure themselves when put in a collar they cannot escape from, and this leads to possible neck injuries and strangling.
Activities.
Some Italian Greyhounds enjoy dog agility. The breed's lithe body and its love of action provide potential to do well at this sport, although not many Italian Greyhounds participate, and their natural inclination is for straight-out racing rather than for working tightly as a team with a handler on a technical course.
Lure coursing is another activity well-fitted to the Italian Greyhound, and they seem to enjoy it tremendously. Although the Italian Greyhound is a very fast dog, it is not as well suited to racing as its larger cousin. Regardless, many Italian Greyhounds participate in amateur straight-track and oval-track racing.
Grooming.
Dogs of this breed have an extremely short and almost odorless coat that requires little more than an occasional bath about once a month (though many veterinarians suggest that even bathing one per month is too frequent for this breed), but a wipe-down with a damp cloth is recommended after walks as seeds, burrs and floating dust in the air can get into the coat and irritate the skin. This breed sheds medium to little hair.
Oral.
The teeth of an Italian Greyhound should be brushed daily. Their scissor-bite and thin jaw bones make them susceptible to periodontal disease, which can be avoided with good dental care. Daily brushing has been shown to be very beneficial as well as regular dental cleanings from the vet.
Health.
The Italian Greyhound has a median lifespan of 13.5 in a 2004 UK Kennel Club survey. A 1993 US breed club survey gives an average lifespan of 9 years but more than a quarter of the dogs had "accidents" recorded as cause of death.
Health problems that can be found in the breed:
Responsible breeders will routinely check their dogs for the onset of various inherited disorders, these commonly include (but are not limited to): CERF examinations on eyes, OFA patellar examinations, OFA thyroid function panels, von Willebrand's factor, OFA hip and Legg-Perthes disease x-rays, and others. In research by the Ortheopedic Foundation for Animals, the Italian Greyhound was found to be the least affected by hip dysplasia out of 157 breeds. Tests were conducted on 169 individual Italian Greyhounds, of which none were found to have hip dysplasia and 59.2% scored excellent on their hip evaluations.
History.
The name of the breed is a reference to the breed's popularity in Renaissance Italy. Mummified dogs very similar to the Italian Greyhound (or small Greyhounds) have been found in Egypt, and pictorials of small Greyhounds have been found in Pompeii, and they were probably the only accepted companion-dog there. Dogs similar to Italian Greyhounds are recorded as having been seen around Emperor Nero's court in Rome in the first century AD.
Although the small dogs are mainly companionship dogs they have in fact been used for hunting rats or mice, often in combination with hunting. It is believed that this was the reason they were bred in the first place by the Greeks.
The Italian Greyhound is the smallest of the family of gazehounds (dogs that hunt by sight). The breed is an old one and is believed to have originated more than 2,000 years ago in the countries now known as Greece and Turkey. This belief is based on the depiction of miniature greyhounds in the early decorative arts of these countries and on the archaeological discovery of small greyhound skeletons. By the Middle Ages, the breed had become distributed throughout Southern Europe and was later a favorite of the Italians of the sixteenth century, among whom miniature dogs were in great demand. Sadly, though, 'designer' breeders tried, and failed, to make the breed even smaller by crossbreeding it with other breeds of dogs. This only led to mutations with deformed skulls, bulging eyes and dental problems. The original Italian Greyhound had almost disappeared when groups of breeders got together and managed to return the breed to normal. From this period onward the history of the breed can be fairly well traced as it spread through Europe, arriving in England in the seventeenth century.
Cultural significance.
The grace of the breed has prompted several artists to include the dogs in paintings, among others Velázquez, Pisanello, and Giotto.
The breed has been popular with royalty throughout, among the best known royal aficionados were Mary, Queen of Scots, Queen Anne, Queen Victoria, Catherine the Great, Frederick the Great and Maud, Queen of Norway.
The breed is also represented in popular culture. Nelly from the film "Good Boy!" is an Italian Greyhound played by "Motif" and "Imp". Figaro, owned by Lydia Schiavello from the television program "The Real Housewives of Melbourne", is a sagacious little Italian Greyhound who loves to dress up.

</doc>
<doc id="15341" url="http://en.wikipedia.org/wiki?curid=15341" title="Into the Woods">
Into the Woods

Into the Woods is a musical that includes lyrics by Stephen Sondheim and book by James Lapine. It debuted in San Diego at the Old Globe Theatre in 1986, and premiered on Broadway on November 5, 1987. Bernadette Peters's performance as the Witch and Joanna Gleason's portrayal of the Baker's Wife brought acclaim to the production during its original Broadway run. "Into the Woods" won several Tony Awards, including Best Score, Best Book, and Best Actress in a Musical (Joanna Gleason), in a year dominated by "The Phantom of the Opera".
The musical has been produced many times, with a 1988 US national tour, a 1990 West End production, a 1997 tenth anniversary concert, a 2002 Broadway revival, a 2010 London revival and in 2012 as part of New York City's outdoor Shakespeare in the Park series. The musical intertwines mainly the plots of several Brothers Grimm fairy tales and follows them to explore the consequences of the characters' wishes and quests. The main characters are taken from "Little Red Riding Hood", "Jack and the Beanstalk", "Rapunzel", and "Cinderella", as well as several others. The musical is tied together by an original story involving a childless baker and his wife and their quest to begin a family, their interaction with a witch who has placed a curse on them, and their interaction with other storybook characters during their journey.
Plot.
Act I.
Starting with the words "Once Upon a Time," the Narrator introduces four characters who each have a wish: Cinderella, the daughter of a wealthy man who has been reduced by her wicked stepmother and stepsisters into becoming their skivvy, wishes to attend the King's festival; Jack, a simple poor boy, wishes that his cow, Milky White, would give milk; and a Baker and his Wife wish they could have a child.
While Little Red Ridinghood wishes for bread from the Baker to take to her grandmother's house, which they reluctantly give, Jack's weary mother, who wishes for gold, nags him into selling the cow, and Cinderella's stepmother and stepsisters Florinda and Lucinda tease her about wanting to attend the King's festival.
The Baker's neighbor, an ugly old witch, reveals the source of the couple's infertility is a curse she placed on the Baker's line after catching the Baker's father in her garden stealing six "magic" beans. In addition to the curse, the Witch took the Baker's father's newborn child Rapunzel. She explains the curse will be lifted if the Baker and his Wife can find the four ingredients that the Witch needs for a certain potion; "the cow as white as milk, the cape as red as blood, the hair as yellow as corn, and the slipper as pure as gold," all before the chime of midnight in three days' time. All begin their journeys into the woods — Jack goes to the market to sell his beloved pet Milky White, Cinderella's family goes to the Festival while Cinderella goes to her mother's grave to ask for guidance, Little Red goes to her grandmother's house, and the Baker, refusing his wife's help, goes to find the ingredients ("Prologue").
Cinderella visits her mother's grave and receives a beautiful gown and golden slippers from her mother's spirit ("Cinderella at the Grave"). Jack encounters a Mysterious Man who mocks him for trying to sell his cow for more than a "sack of beans" and then vanishes. Little Red Ridinghood meets a hungry Wolf who convinces her to take a detour on her way to Granny's ("Hello, Little Girl"). The Baker sees Little Red Ridinghood in the woods, and when the Witch appears, screaming at him to get the red cape, he is so frightened that he forgets the ingredients he needs. Luckily his wife, who followed him into the forest, reminds him. They are squabbling over her presence when they come across Jack with Milky White. Not having the money necessary to buy the cow, they convince Jack that the beans the Baker has found in his father's old hunting jacket are magic and buy the cow for five of them. Jack bids a tearful goodbye to his cow ("I Guess This Is Goodbye"), and the Baker orders his wife to return to the village with the cow. He has qualms about being so dishonest, but his wife reasons that the chance to have a child justifies their trickery ("Maybe They're Magic".)
The Witch has raised Rapunzel as her own daughter, keeping her locked away from the world in a tall tower in the middle of the woods, accessible only by climbing Rapunzel's long, golden hair ("Our Little World"). However, on this day a handsome prince spies the beautiful Rapunzel and resolves to climb the tower himself. In another part of the wood, the Baker has tracked down Little Red Ridinghood. Following the Witch's advice, he attempts to simply steal the red cape, but her ensuing temper tantrum guilts him into returning it. When Little Red Ridinghood arrives at her grandmother's house, she is swallowed by the Wolf. The Baker, in pursuit of the cape, slays the Wolf, pulling Little Red Ridinghood and her grandmother from the beast's innards. Little Red Ridinghood rewards him with the red cape, reflecting on her new experiences ("I Know Things Now"). Meanwhile, Jack's mother angrily tosses the beans aside and sends her son to bed without supper. As Cinderella flees the Festival, pursued by another handsome prince and his steward, the Wife helps her hide and quizzes Cinderella about the ball. Cinderella explains that it was a nice ball ("A Very Nice Prince") but seems fairly ambivalent about the experience. As a giant beanstalk begins to sprout from the ground next to Jack's cottage, the Baker's Wife spots Cinderella's pure gold slippers. She tries to chase after Cinderella but inadvertently allows Milky White to run off, leaving the Baker's Wife without the slippers or the cow. The characters each state morals and credos as the first midnight chimes ("First Midnight") and they continue their journeys through the woods.
The next morning, Jack describes his thrilling adventure after he returns from climbing the beanstalk and finding a castle of two married giants, whom he robbed unnoticed ("Giants in the Sky"). He gives the Baker five gold pieces he stole from the giants to buy back his cow. When the Baker hesitates, Jack climbs back up the beanstalk to find more. The Mysterious Man emerges and taunts the Baker, stealing the money. The Baker's Wife confesses she has lost the cow, and she and the Baker split up to look for it. Cinderella's Prince and Rapunzel's Prince, who are brothers, meet and compare the misery of their newfound and unobtainable loves ("Agony"). The Baker's Wife, who is eavesdropping, takes note when Rapunzel's prince mentions that he is in love with a girl in a tower with hair "as yellow as corn." The Baker's Wife fools Rapunzel into letting down her hair and pulls out a piece of it. Meanwhile, The Mysterious Man gives Milky White back to the Baker.
The Baker's Wife and Cinderella meet again, and the Baker's Wife makes a desperate grab for her shoes, almost succeeding before Cinderella flees. The Baker and his wife reunite, now with three of the four items. The Baker admits that they will have to work together to fulfill the quest ("It Takes Two"). Jack arrives with a hen that lays golden eggs and attempts to buy Milky White back, but the cow suddenly keels over dead as midnight chimes. Again, the characters exchange morals ("Second Midnight"). The Witch discovers that the Prince has been visiting Rapunzel and begs Rapunzel to stay with her ("Stay with Me"). When Rapunzel refuses, the Witch angrily cuts off Rapunzel's hair and banishes her to a desert. The Mysterious Man gives the Baker the money to buy another cow. Jack encounters Little Red Ridinghood, who is now sporting a wolf skin cape and a knife for protection, and tries to impress her by telling her about the kingdom of the Giant. When she refuses to believe him, he is goaded into returning once again to the Giant's home to steal a magic harp.
Cinderella, returning from the last night of the festival, describes how the Prince had spread pitch on the stairs to prevent her from escaping. Caught between wanting to escape and wanting to stay, she eventually resolves to let the Prince decide, leaving him one of her slippers as a clue to her identity ("On the Steps of the Palace"). The Baker's Wife frantically tries to convince her to give up her other shoe, offering her the sixth magic bean in exchange for it. Cinderella throws the bean aside, but trades shoes with the Baker's Wife and flees, while unbeknownst to anyone a second beanstalk starts to grow. The Baker arrives with another cow; they now have all four items. The Prince's Steward grabs the slipper from the Baker's Wife, and they are fighting over it when a great crash is heard and Jack's mother runs in to report that a Giant seeking revenge from Jack for stealing his magic harp has fallen from the first beanstalk when Jack chopped it and is dead in her backyard. The Prince, more concerned with finding Cinderella, waves her off and departs with one of the slippers, giving the other to the Baker and his wife. Jack, to the relief of his mother, returns with the magic harp. The Witch discovers that the new cow is not pure white; it is covered with flour. However, the Witch is able to bring Milky White back to life and instructs the Baker and his Wife to feed the items to her. Jack tries to milk her, but no milk comes. The Baker's Wife admits that the hair is Rapunzel's, and the Witch furiously explains that the magic will not work because the Witch has already touched Rapunzel's hair. The Mysterious Man tells the Baker to feed the hair-like corn silk to the cow. Now Milky White gives milk which is the potion. The Witch reveals that the Mysterious Man is the Baker's father. The Witch drinks the potion, and suddenly the Mysterious Man falls dead, his reparation complete, the curse is broken, and the Witch is transformed into a beautiful young woman, reversing the effects of the curse of ugliness by which she was punished by her mother, because the Baker's father stole the beans from her.
Cinderella's Prince searches for the girl whose foot fits the slipper; the stepsisters try but can only get it on by cutting off parts of their feet. Cinderella appears, her foot fits the slipper, and she becomes the Prince's bride. Rapunzel has borne twins in the desert where her Prince finds her. The Witch attempts to curse the couple, only to find that her powers have been lost. At Cinderella's wedding to the Prince, Florinda and Lucinda are blinded by birds as they try to win Cinderella's favor. Everyone but the Witch and the stepsisters congratulate themselves on being able to live happily "Ever After," though they fail to notice another beanstalk growing sky-high...
Act II.
The Narrator introduces the action again: "Once Upon a Time...Later." All the characters seem happy but are still wishing: The Baker and his Wife have their precious baby boy, but wish for more room and bicker over the Baker's unwillingness to hold his child; Jack and his mother are rich and well-fed, but Jack misses his kingdom in the sky; Cinderella is living with her Prince Charming in the Palace, but is getting bored ("So Happy").
Suddenly, everyone is knocked over by a loud crash, and enormous footprints from a Giant have destroyed the Witch's garden, sparing only a few beans. The Baker and his Wife decide that they must tell the Royal Family, and the Baker travels to the palace. His news is ignored by the Prince's Steward, and also by Jack's Mother when he stops at her house to ask Jack's aid. When he returns home, Little Red Ridinghood arrives on her way to Granny's: her house has been destroyed and her mother is missing. The Baker and his Wife decide to escort her. Meanwhile, Jack decides that he must slay the Giant and Cinderella learns from her bird friends that her mother's grave was disturbed and decides to investigate, dressed in her old clothes. Once again, everyone heads Into the Woods, but this time the mood is somber and the birds have stopped singing. ("Into the Woods" Reprise).
While everyone else is drawn back into the woods, Rapunzel has fled there in a hysterical fit, her treatment at the hands of the Witch having driven her into madness. Her Prince has followed her, but when he encounters his brother they each confess they have another reason for their presence in the woods. They have grown bored and frustrated with their marriages and now lust after two beautiful women asleep in the woods - Snow White and Sleeping Beauty ("Agony" Reprise).
The Baker, his Wife, and Little Red Ridinghood get lost in the woods and find Cinderella's family and the Steward, who reveal that the castle was set upon by the Giant. The Witch arrives as well, bringing news that the Giant has destroyed the village and the Baker's house. Suddenly, thunderous footsteps are heard and the Giant appears. To the shock of all, this Giant is a woman who has come from the second beanstalk and is the widow of the Giant that Jack killed by chopping down the beanstalk. Her booming voice proclaims that she wants Jack's blood in revenge. To satisfy the Giantess, the group realizes they must give her someone, but are unable to decide on whom until they realize that the Narrator is still commenting on the actions from the sidelines. Everyone offers her the narrator as a sacrifice, but he convinces them how lost they would be without him. Nevertheless, the Witch throws him into the Giantess's arms and he is killed upon being dropped. Jack's mother finds the group and aggressively defends her son, angering the Giantess, and the Steward clubs Jack's mother to quiet her, inadvertently killing her. As the Giantess leaves to search for Jack, Rapunzel runs into her path and is trampled, to the horror of the Witch and her Prince ("Witch's Lament").
The Royal Family continue on their way, fleeing to a hidden Kingdom despite the Baker's pleas for them to stay and fight the Giant. The Witch declares she will find Jack and sacrifice him to the Giant, and the Baker and his Wife decide they must find him first and split up to search. The Baker's Wife meets Cinderella's Prince, and he easily seduces the Wife ("Any Moment"). Meanwhile, the Baker discovers Cinderella at her mother's destroyed grave and convinces her to join their group for safety. The Prince, satisfied, leaves the Baker's Wife with a few platitudes, and she realizes her error and decides to return to her happy life with the Baker and their son ("Moments in the Woods"). However, she has lost her way, stumbles into the path of the Giant, and is consequently killed by a falling tree.
The Baker, Little Red, and Cinderella await the return of the Baker's Wife when The Witch drags in Jack, whom she found weeping over the Baker's Wife's body. The Baker, grief-stricken when he learns of his wife's death, unwittingly agrees to give Jack to the Giantess, causing an argument. The characters first blame each other for their predicament, until finally they all decide to blame the Witch for growing the beans in the first place ("Your Fault"). Disgusted, the Witch curses and scolds them and throws away the rest of her magic beans, reactivating her mother's curse and making her vanish ("Last Midnight").
The grieving Baker flees, but is visited by his father's spirit who convinces him to face his responsibilities ("No More"). The Baker returns and helps plan killing the Giantess, using Cinderella's bird friends to peck out the Giant's eyes at an area smeared with pitch, where Jack and the Baker can finally deliver a fatal blow. Cinderella stays behind to protect the Baker's child and when her Prince passes by, he nearly fails to recognize her. She confronts him, having learned of his infidelity from her birds and he explains his feelings of unfulfillment and his reasons for seducing another woman. She asks him to go, and he sorrowfully leaves.
Little Red returns with the news that her grandmother has been killed by the Giantess. Meanwhile, the Baker tells Jack that his mother is dead. Jack vows to kill the steward in revenge until the Baker convinces him that killing the steward will not benefit anyone. Cinderella comforts Little Red and tries to answer her qualms that killing the Giant makes them no better than she is, while the Baker explains to Jack his inability to say what is morally correct. ("No One Is Alone").
The four remaining characters slay the Giant and the deceased characters now including the Royal Family (who have lost their way and starved to death in the woods) and the Princes (who have their new paramours, Snow White and Sleeping Beauty, on their arms) return to share one last set of morals with the audience. The survivors resolve to band together and rebuild. The spirit of the Baker's Wife appears to comfort her mourning husband advising her husband to tell their child their story. The Baker begins to tell the story using the same words as the narrator did at the beginning of the play as the Witch appears with the final moral: "Careful the things you say, 'Children Will Listen'." All join in on a last reprise of the title song, surmising that we all must venture Into the Woods, but never to forget the past ("Finale"). As the characters conclude the song singing, "Into the woods, and out of the woods and happily ever after" Cinderella closes the show with one last "I wish..."
Productions.
Pre-Broadway San Diego Production.
"Into the Woods" premiered at the Old Globe Theatre in San Diego, California, on December 4, 1986 and ran for 50 performances under the direction of James Lapine. Many of the performers from that production appeared in the Broadway cast but John Cunningham, who played the Narrator, Wolf and Steward and George Coe, as the Mysterious Man and Cinderella's Father were replaced by Tom Aldredge, who played the Narrator and Mysterious Man. Kenneth Marshall as Cinderella's Prince was replaced by Robert Westenberg (who also played the Wolf), LuAnne Ponce, who played Little Red Ridinghood, was replaced by Danielle Ferland, Ellen Foley, the Witch, was replaced by Bernadette Peters. Kay McClelland, who played both Rapunzel and the Stepsister Florinda, stayed with the cast but only played Florinda, Rapunzel being played by Pamela Winslow. 
The show underwent much evolution, but the most notable change was the addition on the song "No One Is Alone" in the middle of the run. 
Original Broadway Production.
"Into The Woods" opened on Broadway at the Martin Beck Theatre on November 5, 1987, and closed on September 3, 1989 after 765 performances. It starred Bernadette Peters, Joanna Gleason, Chip Zien, Kim Crosby, Ben Wright, Danielle Ferland, Chuck Wagner, Merle Louise, Tom Aldredge, and Robert Westenberg. The musical was directed by James Lapine, with musical staging by Lar Lubovitch, settings by Tony Straiges, lighting by Richard Nelson, and costumes by Ann Hould-Ward (based on original concepts by Patricia Zipprodt and Ann Hould-Ward). The original production won the 1988 New York Drama Critics' Circle Award and the Drama Desk Award for Best Musical, and the original cast recording won a Grammy Award. The show was nominated for ten Tony Awards, and won three:
Best Score (Stephen Sondheim), Best Book (James Lapine) and Best Actress in a Musical (Joanna Gleason).
Peters left the show after almost five months due to a prior commitment to film the movie "Slaves of New York". The Witch was then played by: Betsy Joslyn (from March 30, 1988);Phylicia Rashād (from April 14, 1988); Betsy Joslyn (from July 5, 1988); Nancy Dussault (from December 13, 1988); and Ellen Foley (from August 1, 1989 until the closing).
Other cast replacements included Dick Cavett as the Narrator (as of July 19, 1988) (for a temporary engagement after which Tom Aldredge returned), Edmund Lyndeck as the Mysterious Man, Patricia Ben Peterson as Cinderella, LuAnne Ponce returning to the role of Little Red Ridinghood, Jeff Blumenkrantz as Jack, Marin Mazzie as Rapunzel (as of March 7, 1989) and Kay McClelland, Lauren Mitchell, Cynthia Sikes and Mary Gordon Murray as the Baker's Wife.
In 1989, from Thursday, May 23 to Saturday, May 25 the full original cast (with the exception of Cindy Robinson as Snow White instead of Jean Kelly) reunited for only three performances for the taping of the musical in its entirety for the Season 10 premiere episode of PBS’s "American Playhouse", which first aired on March 15, 1991. The show was filmed professionally with seven cameras on the set of the Martin Beck Theater in front of an audience with certain elements changed from its nightly counterpart only slightly for the recording in order to better fit the screen rather than the stage such as the lighting, minor costume differences, and others. There were also pick up shots not filmed in front of an audience for various purposes. This video has since been released on Tape and DVD and on occasion, remastered and re-released. 
This video is considered to be the original "Into The Woods".
Tenth Anniversary benefit performances of this production were held on November 9, 1997 at The Broadway Theatre (New York), with most of original cast. Original cast understudies Chuck Wagner and Jeff Blumenkrantz played Cinderella's Prince/Wolf and The Steward in place of Robert Westenburg and Philip Hoffmann and Jonathan Dokuchitz (who joined the broadway production as an understudy in 1989) played Rapunzel's Prince in place of Mr. Wagner. This concert featured the duet "Our Little World," written for the first London production of the show.
On November 9, 2014, most of the original cast reunited for two reunion concerts and discussion in Costa Mesa, California. Mo Rocca hosted the reunion and interviewed Stephen Sondheim and James Lapine as well as each cast member. Appearing were Bernadette Peters, Joanna Gleason, Chip Zien, Danielle Ferland, Ben Wright and real life husband and wife, Robert Westenberg and Kim Crosby.
1988 US Tour Production.
A United States tour began on November 22, 1988 with Cleo Laine playing the Witch, replaced by Betsy Joslyn in May 1989. Rex Robbins played the Narrator and Mysterious Man, Charlotte Rae played Jack's Mother, and the Princes were played by Chuck Wagner and Douglas Sills. The set was almost completely reconstructed, and there were certain changes to the script, changing certain story elements. The 10-month tour played cities around the country, such as Fort Lauderdale, Florida, Los Angeles, and Atlanta. The tour ran at the John F. Kennedy Center for the Performing Arts from June 1989 to July 16, 1989, with the reviewer for "The Washington Post" writing: "his lovely score -- poised between melody and dissonance -- is the perfect measure of our tenuous condition. The songs invariably follow the characters' thinking patterns, as they weigh their options and digest their experience. Needless to say, that doesn't make for traditional show-stoppers. But it does make for vivacity of another kind. And Sondheim's lyrics...are brilliant... I think you'll find these cast members alert and engaging."
Original London Production.
The original West End production opened on September 25, 1990 at the Phoenix Theatre and closed on February 23, 1991 after 197 performances. It was directed by Richard Jones, and produced by David Mirvish, with choreography by Anthony Van Laast, costumes by Sue Blane and orchestrations by Jonathan Tunick. The cast featured Julia McKenzie as the Witch, Ian Bartholomew as the Baker, Imelda Staunton as the Baker's Wife and Clive Carter as the Wolf/Cinderella's Prince. The show received seven Olivier Award nominations in 1991, winning for Best Actress in a Musical (Staunton) and Best Director of a Musical (Jones).
The song "Our Little World" was added. This song was a duet sung between the Witch and Rapunzel giving further insight into the care the Witch has for her self-proclaimed daughter and the desire Rapunzel has to see the world outside of her tower. The overall feel of the show was a lot darker than that of the original Broadway production. Critic Michael Billington wrote "But the evening's triumph belongs also to director Richard Jones, set designer Richard Hudson and costume designer Sue Blane who evoke exactly the right mood of haunted theatricality. Old-fashioned footlights give the faces a sinister glow. The woods themselves are a semi-circular, black-and-silver screen punctuated with nine doors and a crazy clock: they achieve exactly the 'agreeable terror' of Gustave Dore's children's illustrations. And the effects are terrific: doors open to reveal the rotating magnified eyeball or the admonitory finger of the predatory giant.
1998 London Revival Production.
A new intimate production of the show opened (billed as the first London revival) at the Donmar Warehouse on 16 November 1998, closing on 13 February 1999. This revival was directed by John Crowley and designed by his brother, Bob Crowley. The cast included Clare Burt as the Witch, Nick Holder as the Baker, Sophie Thompson as the Baker's Wife, Jenna Russell as Cinderella, Sheridan Smith as Little Red Ridinghood and Frank Middlemass as the Narrator/Mysterious Man. Russell later appeared as the Baker's Wife in the 2010 Regent's Park production. Thompson won the 1999 Olivier Award for Best Actress in a Musical for her performance, while the production itself was nominated for Outstanding Musical Production.
2002 Broadway Revival Production.
A revival opened at the Ahmanson Theatre in Los Angeles, running from February 1, 2002 to March 24, 2002. This production was directed and choreographed, with the same principal cast, which later ran on Broadway.
The 2002 Broadway revival, directed by James Lapine and choreographed by John Carrafa, began previews on April 13, 2002 and opened April 30, 2002 at the Broadhurst Theatre, closing on December 29 after a run of 18 previews and 279 regular performances. It starred Vanessa L. Williams as the Witch, John McMartin as the Narrator, Stephen DeRosa as the Baker, Kerry O'Malley as the Baker's Wife, Gregg Edelman as Cinderella's Prince/Wolf, Christopher Sieber as Rapunzel's Prince/Wolf, Molly Ephraim as Little Red Ridinghood, Adam Wylie as Jack and Laura Benanti as Cinderella. Judi Dench provided the pre-recorded voice of the Giant.
Lapine revised the script slightly for this production, with a cameo appearance of the "Three Little Pigs" restored from the earlier San Diego production. Other changes, apart from numerous small dialogue changes, included the addition of the song "Our Little World," a duet for the Witch and Rapunzel written for the first London production, the addition of a second wolf in the song "Hello Little Girl" who competes for Little Red's attention with the first Wolf, the portrayal of Jack's cow by a live performer (Chad Kimball) in an intricate costume and new lyrics were written for "The Last Midnight," now sung by the Witch as a menacing lullaby to the Baker's baby.
The revival won the Tony Awards for the Best Revival of a Musical and Best Lighting Design. This Broadway revival wardrobe is on display at the Costume World Broadway Collection in South Florida.
London Royal Opera House, 2007.
A revival at the Royal Opera House's Linbury Studio in Covent Garden had a limited run from June 14 through June 30, 2007 followed by a short stint at The Lowry theatre, Salford Quays, Manchester between 4–7 July. The production mixed Opera singers, Musical Theatre actors as well as Film and television actors; including Anne Reid as Jack's Mother and Gary Waldhorn as the Narrator. The production itself, directed by Will Tuckett, was met with mixed reviews; although there were clear stand out performances.
The production completey sold out three weeks before opening. As this was an 'Opera' production, the show and its performers were overlooked for the 'Musical' nominations in the 2008 Olivier Awards. This production featured Suzie Toase (Little Red), Peter Caulfield (Jack), Beverley Klein (Witch), Anna Francolini (Baker's Wife), Clive Rowe (Baker), Nicholas Garrett (wolf) and Lara Pulver (Lucinda). This was the second Sondheim musical to be staged by the Opera House, following 2003's Sweeney Todd.
Regent's Park Open Air Theatre Production, 2010.
The Olivier Award winning Regent’s Park Open Air Theatre production, directed by Timothy Sheader and choreographed by Liam Steel, ran for a six-week limited season from 6 August to 11 September 2010. The cast included Hannah Waddingham as the Witch, Mark Hadfield as the Baker, Jenna Russell as the Baker’s wife, Helen Dallimore as Cinderella, and Judi Dench as the recorded voice of the Giant. Gareth Valentine was the Musical Director. The musical was performed outdoors in a wooded area. Whilst the book remained mostly unchanged, the subtext of the plot was dramatically altered by casting the role of the Narrator as a young school boy lost in the woods following a family argument – a device used to further illustrate the musical’s themes of parenting and adolescence.
The production opened to wide critical acclaim, much of the press commenting on the effectiveness of the open air setting. The "Telegraph" reviewer, for example, wrote: "It is an inspired idea to stage this show in the magical, sylvan surroundings of Regent’s Park, and designer Soutra Gilmour has come up with a marvellously rickety, adventure playground of a set, all ladders, stairs and elevated walkways, with Rapunzel discovered high up in a tree." The "New York Times" reviewer commented: "The natural environment makes for something genuinely haunting and mysterious as night falls on the audience..." Stephen Sondheim attended twice, reportedly extremely pleased with the production. The production also won the Laurence Olivier Award for Best Musical Revival and Michael Xavier, who played Cinderella's Prince and the Wolf, was nominated for the Laurence Olivier Award for Best Performance in a Supporting Role in a Musical.
The production was recorded in its entirety and released for public download through Digital Theatre, an online video production company.
Central Park's Delacorte Theater Production, 2012.
The Regent's Park Open Air Theatre production transferred to the Public Theater's 2012 summer series of free performances Shakespeare in the Park at the Delacorte Theater in Central Park, New York, with an American cast as well as new designers. Sheader again was the director and Steel served as co-director and choreographer. Performances were originally to run from July 24 (delayed from July 23 due to the weather) to August 25, 2012, but the show was extended till September 1, 2012. The cast included Amy Adams as The Baker's Wife, Donna Murphy as The Witch, Denis O'Hare as The Baker, Chip Zien as the Mysterious Man/Cinderella's Father, Jack Broderick as the young Narrator, Gideon Glick as Jack, Cooper Grodin as Rapunzel’s Prince, Ivan Hernandez as Cinderella’s Prince/Wolf, Tina Johnson as Granny, Josh Lamon as the Steward, Jessie Mueller as Cinderella, Laura Shoop as Cinderella’s Mother, Tess Soltau as Rapunzel and Glenn Close as the Voice of the Giant. The set was a "collaboration between original Open Air Theatre designer Soutra Gilmour and...John Lee Beatty, [and] rises over 50 feet in the air, with a series of tree-covered catwalks and pathways." The production was dedicated to Nora Ephron, who died earlier in 2012. In February 2012 and in May 2012, reports of a possible Broadway transfer surfaced with the production's principal actors in negotiations to reprise their roles. In January 2013, it was announced that the production will not transfer to Broadway due to scheduling conflicts.
Other Productions.
This production played from 19 March 1993 to 5 June 1993 at the Drama Theatre, Sydney Opera House. It starred Judi Connelli, Geraldine Turner, Tony Sheldon, Philip Quast, Sharon Millerchip, Pippa Grandison, Simon Chilvers and DJ Foster.
This production played from 18 November 1993 to 23 December 1993 at the Watershed Theatre, Auckland. It starred Suzanne Lee, Kevin Smith, Miranda Harcourt, and Sophia Hawthorne
This production played from 17 January 1998 to 21 February 1998 at the Playhouse, Victorian Arts Centre. It starred Rhonda Burchmore, John McTernan, Gina Riley, Lisa McCune, Peter Carroll, Anthony Weigh, Tamsin Carroll and Robert Grubb.
"Boscos endins" was the Catalan adaptation by Joan Vives with the collaboration of Joan Lluís Bozzo. It was first presented by the Theatre Company Dagoll Dagom in Girona at the Festival Temporada Alta on November 22, 2007, and in January it was held at Teatre Victòria, in Avinguda del Paral·lel, Barcelona, Spain
The musical was the first production in the Signature Theatre's new venue, opening in January 2007.
In December 2009 the show was presented at the Bloomsbury Theatre London by the UCLU Musical Theatre Society in a brand new production. Produced by Alicia Bennett and directed by Dom O'Hanlon the show was personally supported by Stephen Sondheim and praised for its new interpretation. The production was selected to be performed as part of the Sunday Times National Student Drama Festival 2010 in Scarborough, featuring the original cast and orchestra at the Spa Theatre, Scarborough in April 2010.
The 2011 Singapore production presented by Dream Academy ran from 29 July to 7 August 2011. Directed by Glen Goei, it featured Ria Jones (Witch), Adrian Pang (Baker), Selena Tan (Baker's Wife), Sebastian Tan (Jack), Emma Yong (Cinderella), Denise Tan (Little Red Ridinghood) and Lim Kay Siu (narrator)
The Mysterium Theater presented the production directed by Marla Ladd ran from August 18 to September 24.
Directed by Mark Lamos, Center Stage (in a co-production with Westport Country Playhouse in Westport, Connecticut) in Baltimore, Maryland presented "Into the Woods", running from March 7 to April 15, 2012. The cast included Danielle Ferland, Broadway's original Little Red Ridinghood, as the Baker's wife, Jeffry Denman as the Narrator, Jenny Latimer as Cinderella, Justin Scott Brown as Jack, Erik Liberman as the Baker, Dana Steingold as Little Red Ridinghood, Nik Walker as Wolf/Cinderella's Prince, and Lauren Kennedy as the Witch. The production transferred to Westport Country Playhouse for a run lasting from May 1 to May 25, 2012.
The first professional Spanish language production, "Dentro del Bosque", was produced by University of Puerto Rico Repertory Theatre and premiered in San Juan at Teatro de la Universidad (University Theatre) on March, 14. Directed by Edgar García Rivera, it starred: Víctor Santiago (Baker), Ana Isabelle (Baker's Wife), Lourdes Robles (Witch), Julio Enrique Court (Narrator/Mysterious Man), Xavier Rivera (Jack), Ulises de Orduna (Wolf), Angeliz Nieves (Little Red Ridinghood), Daisy Maeso (Cinderella), Armando Vegerano (Cinderella's Prince), Sara García (Jack's mother), Kedward Avilés (Steward).
Directed by Lee Blakeley. Musical direction by David Charles Abell. April 1–12, 2014 at Théâtre du Châtelet, Paris. It featured Kimy McLaren (Cinderella), Leslie Clack (The Narrator), Nicholas Garrett (The Baker), Beverley Klein (The Witch), Pascal Charbonneau (Jack), Damian Thantrey (Cinderella's Prince), David Curry (Rapunzel's Prince), Francesca Jackson (Little Red Ridinghood), Rebecca de Pont Davies (Jack's Mother), Louise Adler (Rapunzel).
Directed by Amanda Dehnert, costume design by Linda Roethke, and scenic design by Rachel Hauck, the show ran from June 4, 2014 – October 11, 2014 with Miriam A. Laube as the Witch, Anthony Heald as the Narrator and Mysterious Stranger, Javier Muňoz as the Baker, Rachael Warren as the Baker’s Wife, Jennie Greenberry as Cinderella, Kjerstine Rose Anderson as Little Red Ridinghood, Miles Fletcher as Jack, Catherine E. Couson as Milky White, Little Red Ridinghood’s Granny, Giantess, and Cinderella’s Stepmother, Jeremy Peter Johnson as Cinderella’s Prince and the Wolf (vocals), Howie Seago as the Wolf (sign language), John Tufts as Rapunzel’s Prince, Royer Bockus as Rapunzel, Robin Goodrin Nordli as Jack’s Mother, Robert Vincent Frank as Cinderella’s Father, Katie Bradley as Florinda and Sleeping Beauty, Christiana Clark as Lucinda and Snow White, and David Kelly as the Steward.
Directed by Stuart Maunder. Musical Director Benjamin Northey July 19–26, 2014
San Francisco Playhouse presented "Into the Woods" in from June–September 2014. The musical was directed by Susi Damilano and ran for 80 performances.
PlayMakers Repertory Company presented "Into the Woods" from November 1, 2014 to December 7, 2014. It was directed by Joseph Haj.
The Roundabout Theatre production, directed by Noah Brody and Ben Steinfeld, began performances Off-Broadway on December 19, 2014 and officially opened on January 22, 2015, at the Laura Pels Theatre. 
 Like the original Broadway production 28 years prior, this production had a try-out run at the Old Globe Theatre in San Diego, California from July 12, 2014 – August 17, 2014 with the opening night taking place on July 17. This new version is completely minimalistically reimagined by the Fiasco Theater Company. This production featured only ten actors and one piano accompanist, performed as an ensemble effort. Almost all the actors played multiple roles, and the set consists of only one upright piano, a backdrop of giant piano strings, and various common household objects used in different ways to represent certain plot elements. The director, Noah Brody, is featured as Lucinda / the Wolf / Cinderella's Prince. The rest of the cast featured Jennifer Mudge as the Witch, Ben Steinfeld as the Baker, Jessie Austrian as the Baker's Wife, Claire Karpen as Cinderella / Granny, Patrick Mulryan as Jack / the Steward, Liz Hayes as the Stepmother / Jacks Mother, Andy Grotelueschen as Florinda / Rapunzel's Prince, Emily Young as Little Red Ridinghood / Rapunzel, and Paul L. Coffey as the Mysterious Man. Accompanying them all was Matt Castle and occasionally took part in the cast. The roles of the Narrator, Cinderella's Mother, and the Giant were ensemble efforts.
Casting history.
The principal original casts of notable stage productions of "Into the Woods"
Adaptations.
Junior version.
The musical has been adapted into a child-friendly version for use by schools, with the second act completely removed, as well as certain elements from the first. The show is shortened from the original 2-plus hours to fit in a 60 to 80 minute range, and the music transposed into keys that more easily fit young voices.
Film.
A theatrical film adaptation of the musical was produced by Walt Disney Pictures, directed by Rob Marshall, and starring Meryl Streep, Emily Blunt, James Corden, Anna Kendrick, Chris Pine, Tracey Ullman, Christine Baranski, Lilla Crawford, Daniel Huttlestone, MacKenzie Mauzy, Billy Magnussen, and Johnny Depp. The film was released on December 25, 2014. It was a critical and commercial hit, grossing over $204 million worldwide. Meryl Streep was nominated for many accolades for her performance as The Witch, including the Academy Award for Best Supporting Actress.
Analysis of book and music.
In most productions of "Into the Woods", including the original Broadway production, several parts are doubled. Cinderella's Prince and the Wolf, who share the characteristic of being unable to control their appetites, are played by the same actor. Similarly, the Narrator and the Mysterious Man, who share the characteristic of commenting on the story while avoiding any personal involvement or responsibility. Granny and Cinderella's Mother, who are both matriarchal characters in the story, are also typically played by the same person, who also gives voice to the nurturing but later murderous Giant's Wife.
The show covers multiple themes: growing up, parents and children, accepting responsibility, morality, and finally, wish fulfillment and its consequences. The "Time Magazine" reviewers wrote that the play's "basic insight ... is at heart, most fairy tales are about the loving yet embattled relationship between parents and children. Almost everything that goes wrong — which is to say, almost everything that can — arises from a failure of parental or filial duty, despite the best intentions." Stephen Holden wrote that the themes of the show include parent-child relationships and the individual's responsibility to the community. The witch isn't just a scowling old hag, but a key symbol of moral ambivalence. James Lapine said that the most unpleasant person (the Witch) would have the truest things to say and the "nicer" people would be less honest. In the Witch's words: "I'm not good; I'm not nice; I'm just right."
Given the show's debut during the 1980s, the height of the US AIDS crisis, the work has been interpreted to be a parable about AIDS. In this interpretation, the Giant's Wife serves as a metaphor for HIV/AIDS, killing good and bad characters indiscriminately and forcing the survivors to band together to stop the threat and move on from the devastation, reflecting the devastation to many communities during the AIDS crisis. When asked about the thematic connection, Sondheim acknowledged that initial audiences interpreted it as an AIDS metaphor, but stated that the work was not intended to be specific.
The score is also notable in Sondheim's output, because of its intricate reworking and development of small musical motifs. In particular, the opening words, "I wish", are set to the interval of a rising major second and this small unit is both repeated and developed throughout the show, just as Lapine's book explores the consequences of self-interest and "wishing." The dialogue in the show is characterized by the heavy use of syncopated speech. In many instances, the characters' lines are delivered with a fixed beat that follows natural speech rhythms, but is also purposely composed in eighth, sixteenth, and quarter note rhythms as part of a spoken song. Like many Sondheim/Lapine productions, the songs contain thought-process narrative, where characters converse or think aloud.
Sondheim drew on parts of his troubled childhood when writing the show. In 1987, he told "Time Magazine" that the "father uncomfortable with babies [was] his father, and [the] mother who regrets having had children [was] his mother." 

</doc>
<doc id="15342" url="http://en.wikipedia.org/wiki?curid=15342" title="Isaac Klein">
Isaac Klein

Isaac Klein (September 5, 1905 – 1979) was a prominent rabbi and halakhic authority within Conservative Judaism.
Personal life, education, and career.
Klein was born in the small village of Palanka in what was then Hungary and emigrated with his family to the United States in 1921. He earned a BA from City College in New York in 1931. Although nearing ordination at the Yeshiva University's Rabbi Isaac Elchanan Theological Seminary, he transferred to the Jewish Theological Seminary of America (JTSA), where he was ordained in 1934 and received the advanced Jewish legal degree of "Hattarat Hora’ah" under the great talmudic scholar Rabbi Professor Louis Ginzberg. He was one of only three people, along with Boaz Cohen and Louis Finkelstein, to ever to receive this degree from JTSA. Klein subsequently earned a PhD from Harvard under the pioneering academic of Judaic studies Harry Wolfson. 
He married the former Henriette Levine in 1932 and had three daughters, Hannah, Miriam, and Rivke. Devoted to his family, he dedicated his major work, "A Guide to Jewish Religious Practice" to his children, sons-in-law and 13 grandchildren listing each by name. 
Klein served as rabbi at Kadimoh Congregation in Springfield, Massachusetts from 1934–1953; Temple Emanu-El, Buffalo, New York, 1953–1968; Temple Shaarey Zedek, Buffalo, (which was created from the merger of Emanu-El with Temple Beth David in 1968), 1968-1972. A beloved Rabbi, he influenced generations of congregants and visiting students and, together with his wife who was an educator, founded Jewish day schools in both Springfield and Buffalo.
Despite the difficulties facing a congregational Rabbi raising a family, Klein volunteered for the U.S. Army during World War II as a chaplain, motivated by a cause he saw as clearly right with important implications for the Jewish People. He served over 4 years, rising to the rank of Major and was an advisor to the high commissioner of the Occupation government. He also served on special assignments for Jewish soldiers in the U.S. Army in the 1950s, receiving the simulated rank of Brigadier General for these missions. His experiences in the war are described in his book "The Anguish and the Ecstasy of a Jewish Chaplain".
Role within Conservative Judaism.
Klein was a leader of the right-wing of the Conservative movement. He was president of the Rabbinical Assembly, 1958–1960, and a member of its Committee on Jewish Law and Standards, 1948-1979. He was the author of several books, notably, "A Guide to Jewish Religious Practice". One of the outstanding halakhists of the movement, he served as a leading member of the Committee on Jewish Law and Standards from 1948 until his death in 1979.
As a leading authority on halakha he authored many important teshuvot (responsa), many of which were published in his influential "Responsa and Halakhic Studies". From the 1950s to 1970s, he wrote a comprehensive guide to Jewish law that was used to teach halakha at the JTSA. In 1979 he assembled this into "A Guide to Jewish Religious Practice", which is used widely by laypeople and rabbis within Conservative Judaism.
Rabbinic thought.
The philosophy upon which "A Guide to Jewish Religious Practice" is written is stated in the foreword: "The premise on which Torah is based is that all aspects of life - leisure no less than business, worship or rites of passage (birth, bar mitzvah, marriage, divorce, death) - are part of the covenant and mandate under which every Jew is to serve God in everything he does. In the eyes of Torah there is, strictly speaking, no such thing as the purely private domain, for even in solitude - be it the privacy of the bath or the unconsciousness of sleep - one has the capacity and the duty to serve God." This message, of life seen in consonance with the dictates of Judaism, permeates many pages of the book. Rabbi Louis Finkelstein, scholar of the JTSA, wrote: "There are those who would think that we have but two alternatives, to reject or to accept the law, but in either case to treat it as a dead letter. Both of these alternatives are repugnant to the whole tradition of Judaism. Jewish law must be preserved but it is subject to interpretation by those who have mastered it, and the interpretation placed upon it by duly authorized masters in every generation must be accepted with as much reverence as those which were given in previous generations." 
This understanding of traditional preservation of the law through its continuous interpretation lies at the heart of Klein's extensive study of Jewish law. 
Klein's papers are located at the University Archives, State University of New York at Buffalo (see ). The archives include fifteen reels of microfilm. The collection consists of extensive writings by Klein on traditional Jewish practice and law. This includes manuscript material for his books "Guide to Jewish Religious Practice" (1979), "The Ten Commandments in a Changing World " (1963), "The Anguish and the Ecstasy of a Jewish Chaplain" (1974), and his translation of "The Code of Maimonides (Mishneh Torah): Book 7, The Book of Agriculture" (1979). The collection also contains speeches, sermons, articles, and remarks from the Conservative Jewish viewpoint on subjects such as Jewish medical ethics, dietary laws, adoption, and marriage and divorce. Meeting minutes, annual reports, bulletins, and sermons relating to Klein's rabbinical vocations in Springfield, Massachusetts and Buffalo, New York are also included. The papers contain photographs, wartime letters, and military records of Klein documenting his service in World War II as a director of Jewish religious affairs in Germany.

</doc>
<doc id="15343" url="http://en.wikipedia.org/wiki?curid=15343" title="Intron">
Intron

An intron is any nucleotide sequence within a gene that is removed by RNA splicing during maturation of the final RNA product. The term "intron" refers to both the DNA sequence within a gene and the corresponding sequence in RNA transcripts. Sequences that are joined together in the final mature RNA after RNA splicing are exons. Introns are found in the genes of most organisms and many viruses, and can be located in a wide range of genes, including those that generate proteins, ribosomal RNA (rRNA), and transfer RNA (tRNA). When proteins are generated from intron-containing genes, RNA splicing takes place as part of the RNA processing pathway that follows transcription and precedes translation.
The word "intron" is derived from the term "intragenic region", i.e. a region inside a gene. Although introns are sometimes called "intervening sequences", the term "intervening sequence" can refer to any of several families of internal nucleic acid sequences that are not present in the final gene product, including inteins, untranslated sequences (UTR), and nucleotides removed by RNA editing, in addition to introns.
Introduction.
Introns were first discovered in protein-coding genes of adenovirus, and were subsequently identified in genes encoding transfer RNA and ribosomal RNA genes. Introns are now known to occur within a wide variety of genes throughout organisms and viruses within all of the biological kingdoms.
The fact that genes were split or interrupted by introns was discovered independently in 1977 by Phillip Allen Sharp and Richard J. Roberts, for which they shared the Nobel Prize in Physiology or Medicine in 1993. The term "intron" was introduced by American biochemist Walter Gilbert:
"The notion of the cistron [...] must be replaced by that of a transcription unit containing regions which will be lost from the mature messenger - which I suggest we call introns (for intragenic regions) - alternating with regions which will be expressed - exons." (Gilbert 1978)
The frequency of introns within different genomes is observed to vary widely across the spectrum of biological organisms. For example, introns are extremely common within the nuclear genome of higher vertebrates (e.g. humans and mice), where protein-coding genes almost always contain multiple introns, while introns are rare within the nuclear genes of some eukaryotic microorganisms, for example baker's/brewer's yeast ("Saccharomyces cerevisiae"). In contrast, the mitochondrial genomes of vertebrates are entirely devoid of introns, while those of eukaryotic microorganisms may contain many introns. Introns are well known in bacterial and archaeal genes, but occur more rarely than in most eukaryotic genomes.
A particularly extreme case is the "Drosophila dhc7" gene containing a ≥3.6 Mb intron, which takes roughly three days to transcribe.
Classification.
Splicing of all intron-containing RNA molecules is superficially similar, as described above. However, different types of introns were identified through the examination of intron structure by DNA sequence analysis, together with genetic and biochemical analysis of RNA is splicing reactions.
At least four distinct classes of introns have been identified.
Group III introns are proposed to be a fifth family, but little is known about the biochemical apparatus that mediates their splicing. They appear to be related to group II introns, and possibly to spliceosomal introns.
Nuclear pre-mRNA introns (spliceosomal introns) are characterized by specific intron sequences located at the boundaries between introns and exons. These sequences are recognized by spliceosomal RNA molecules when the splicing reactions are initiated. In addition, they contain a branch point, a particular nucleotide sequence near the 3' end of the intron that becomes covalently linked to the 5' end of the intron during the splicing process, generating a branched ("lariat") intron. Apart from these three short conserved elements, nuclear pre-mRNA intron sequences are highly variable. Nuclear pre-mRNA introns are often much longer than their surrounding exons.
Group I and group II introns are found in genes encoding proteins (messenger RNA), transfer RNA and ribosomal RNA in a very wide range of living organisms., Following transcription into RNA, group I and group II introns also make extensive internal interactions that allow them to fold into a specific, complex three-dimensional architecture. These complex architectures allow some group I and group II introns to be "self-splicing", that is, the intron-containing RNA molecule can rearrange its own covalent structure so as to precisely remove the intron and link the exons together in the correct order. In some cases, particular intron-binding proteins are involved in splicing, acting in such a way that they assist the intron in folding into the three-dimensional structure that is necessary for self-splicing activity. Group I and group II introns are distinguished by different sets of internal conserved sequences and folded structures, and by the fact that splicing of RNA molecules containing group II introns generates branched introns (like those of spliceosomal RNAs), while group I introns use a non-encoded guanosine nucleotide (typically GTP) to initiate splicing, adding it on to the 5'-end of the excised intron.
Transfer RNA introns that depend upon proteins for removal occur at a specific location within the anticodon loop of unspliced tRNA precursors, and are removed by a tRNA splicing endonuclease. The exons are then linked together by a second protein, the tRNA splicing ligase. Note that self-splicing introns are also sometimes found within tRNA genes.
Biological functions and evolution.
While introns do not encode protein products, they are integral to gene expression regulation. Some introns themselves encode functional RNAs through further processing after splicing to generate noncoding RNA molecules. Alternative splicing is widely used to generate multiple proteins from a single gene. Furthermore, some introns play essential roles in a wide range of gene expression regulatory functions such as non-sense mediated decay and mRNA export.
The biological origins of introns are obscure. After the initial discovery of introns in protein-coding genes of the eukaryotic nucleus, there was significant debate as to whether introns in modern-day organisms were inherited from a common ancient ancestor (termed the introns-early hypothesis), or whether they appeared in genes rather recently in the evolutionary process (termed the introns-late hypothesis). Another theory is that the spliceosome and the intron-exon structure of genes is a relic of the RNA world (the introns-first hypothesis). There is still considerable debate about the extent to which of these hypotheses is most correct. The popular consensus at the moment is that introns arose within the eukaryote lineage as selfish elements .
Early studies of genomic DNA sequences from a wide range of organisms show that the intron-exon structure of homologous genes in different organisms can vary widely. More recent studies of entire eukaryotic genomes have now shown that the lengths and density (introns/gene) of introns varies considerably between related species. For example, while the human genome contains an average of 8.4 introns/gene (139,418 in the genome), the unicellular fungus "Encephalitozoon cuniculi" contains only 0.0075 introns/gene (15 introns in the genome). Since eukaryotes arose from a common ancestor (Common descent), there must have been extensive gain or loss of introns during evolutionary time. This process is thought to be subject to selection, with a tendency towards intron gain in larger species due to their smaller population sizes, and the converse in smaller (particularly unicellular) species. Biological factors also influence which genes in a genome lose or accumulate introns.
Alternative splicing of introns within a gene acts to introduce greater variability of protein sequences translated from a single gene, allowing multiple related proteins to be generated from a single gene and a single precursor mRNA transcript. The control of alternative RNA splicing is performed by a complex network of signaling molecules that respond to a wide range of intracellular and extracellular signals.
Introns contain several short sequences that are important for efficient splicing, such as acceptor and donor sites at either end of the intron as well as a branch point site, which are required for proper splicing by the spliceosome. Some introns are known to enhance the expression of the gene that they are contained in by a process known as intron-mediated enhancement (IME).
Introns as mobile genetic elements.
Introns may be lost or gained over evolutionary time, as shown by many comparative studies of orthologous genes. Subsequent analyses have identified thousands of examples of intron loss and gain events, and it has been proposed that the emergence of eukaryotes, or the initial stages of eukaryotic evolution, involved an intron invasion. Two definitive mechanisms of intron loss, Reverse Transcriptase-Mediated Intron Loss (RTMIL) and genomic deletions, have been identified, and are known to occur. The definitive mechanisms of intron gain, however, remain elusive and controversial. At least seven mechanisms of intron gain have been reported thus far: Intron Transposition, Transposon Insertion, Tandem Genomic Duplication, Intron Transfer, Intron Gain during Double-Strand Break Repair (DSBR), Insertion of a Group II Intron, and Intronization. In theory it should be easiest to deduce the origin of recently gained introns due to the lack of host-induced mutations, yet even introns gained recently did not arise from any of the aforementioned mechanisms. These findings thus raise the question of whether or not the proposed mechanisms of intron gain fail to describe the mechanistic origin of many novel introns because they are not accurate mechanisms of intron gain, or if there are other, yet to be discovered, processes generating novel introns.
In intron transposition, the most commonly purported intron gain mechanism, a spliced intron is thought to reverse splice into either its own mRNA or another mRNA at a previously intron-less position. This intron-containing mRNA is then reverse transcribed and the resulting intron-containing cDNA may then cause intron gain via complete or partial recombination with its original genomic locus. Transposon insertions can also result in intron creation. Such an insertion could intronize the transposon without disrupting the coding sequence when a transposon inserts into the sequence AGGT, resulting in the duplication of this sequence on each side of the transposon. It is not yet understood why these elements are spliced, whether by chance, or by some preferential action by the transposon. In tandem genomic duplication, due to the similarity between consensus donor and acceptor splice sites, which both closely resemble AGGT, the tandem genomic duplication of an exonic segment harboring an AGGT sequence generates two potential splice sites. When recognized by the spliceosome, the sequence between the original and duplicated AGGT will be spliced, resulting in the creation of an intron without alteration of the coding sequence of the gene. Double-stranded break repair via non-homologous end joining was recently identified as a source of intron gain when researchers identified short direct repeats flanking 43% of gained introns in Daphnia. These numbers must be compared to the number of conserved introns flanked by repeats in other organisms, though, for statistical relevance. For group II intron insertion, the retrohoming of a group II intron into a nuclear gene was proposed to cause recent spliceosomal intron gain.
Intron transfer has been hypothesized to result in intron gain when a paralog or pseudogene gains an intron and then transfers this intron via recombination to an intron-absent location in its sister paralog. Intronization is the process by which mutations create novel introns from formerly exonic sequence. Thus, unlike other proposed mechanisms of intron gain, this mechanism does not require the insertion or generation of DNA to create a novel intron.
The only hypothesized mechanism of recent intron gain lacking any direct evidence is that of group II intron insertion, which when demonstrated in vivo, abolishes gene expression. Group II introns are therefore likely the presumed ancestors of spliceosomal introns, acting as site-specific retroelements, and are no longer responsible for intron gain. Tandem genomic duplication is the only proposed mechanism with supporting in vivo experimental evidence: a short intragenic tandem duplication can insert a novel intron into a protein-coding gene, leaving the corresponding peptide sequence unchanged. This mechanism also has extensive indirect evidence lending support to the idea that tandem genomic duplication is a prevalent mechanism for intron gain. The testing of other proposed mechanisms in vivo, particularly intron gain during DSBR, intron transfer, and intronization, is possible, although these mechanisms must be demonstrated in vivo to solidify them as actual mechanisms of intron gain. Further genomic analyses, especially when executed at the population level, may then quantify the relative contribution of each mechanism, possibly identifying species-specific biases that may shed light on varied rates of intron gain amongst different species.
See also.
Structure:
Splicing:
Function
Others:

</doc>
<doc id="15345" url="http://en.wikipedia.org/wiki?curid=15345" title="IEE">
IEE

IEE may stand for:

</doc>
<doc id="15346" url="http://en.wikipedia.org/wiki?curid=15346" title="Institute of National Remembrance">
Institute of National Remembrance

Institute of National Remembrance – Commission for the Prosecution of Crimes against the Polish Nation (Polish: "Instytut Pamięci Narodowej – Komisja Ścigania Zbrodni przeciwko Narodowi Polskiemu"; IPN) is a Polish government-affiliated research institute with lustration prerogatives and prosecution powers founded by specific legislation. It specialises in the legal and historical sciences and in particular the recent history of Poland. IPN investigates both Nazi and Communist crimes committed in Poland between 1939 and 1989, documents its findings and disseminates the results of its investigations to the public.
The Institute was established by the Polish Parliament on 18 December 1998. The Institute started its activities on 1 July 2000.
According to a new law which went into effect on 15 March 2007, IPN was to be mandated to carry out lustration procedures prescribed by Polish law. However, key articles of that law were judged unconstitutional by Poland's constitutional court on 11 May 2007, so the role of IPN in the lustration process is at present unclear.
The IPN is a founding member organisation of the Platform of European Memory and Conscience.
Purpose.
IPN's main areas of activity and mission statement include:
IPN collects, archives and organises documents about the Polish communist security apparatus (22 July 1944 to 31 December 1989).
Organisation.
IPN was created by special legislation on 18 December 1998. IPN is governed by the chairman. This chairman is chosen by a supermajority (60%) of the Polish Parliament (Sejm) with the approval of the Senate of Poland on a request by a Collegium of IPN. The chairman has a 5-year term of office. The first chairman of the IPN was Leon Kieres, elected by the Sejm for five years in 8 June 2000 (term 30 June 2000 – 29 December 2005). The second chairman was Janusz Kurtyka, elected on 9 December 2005 with a term that started 29 December 2005 until his death in the Smolensk airplane crash on 10 April 2010. Franciszek Gryciuk was acting chairman from 2010 to 2011, when the current chairman, Łukasz Kamiński, was elected by the Sejm.
The IPN is divided into:
On 29 April 2010, acting president Bronislaw Komorowski signed into law a parliamentary act that reformed the Institute of National Remembrance.
Activities.
Research.
The research conducted by IPN from December 2000 falls into four main topical areas:
Among the most widely reported cases investigated by the IPN thus far is the Jedwabne Pogrom, a pogrom of Polish Jews "committed directly by Poles, but inspired by the Germans" in 1941. A selection of other cases include: 
Education.
IPN is involved in dissemination of its research results in the form of publications (particularly the IPN Bulletin (Biuletyn IPN „pamięć.pl”) and "Remembrance and Justice" periodicals), exhibitions, seminars, panel discussions, film reviews, workshops and school lessons. Since December 2000 IPN has organized over 30 academic conferences (particularly the Warsaw Congress of Science organized every year in September); 22 exhibitions in various museums and educational competitions involving thousands of students. "IPN Bulletin" is of an informative and popular-scientific character and contains articles pertaining to the history of Poland in the years 1939–1990 as well as describes the current IPN activities. "Remembrance and Justice" appears every half a year and is a scientific historical magazine. IPN also publishes books which are usually edited as collections of documents, reports and memories, but also scientific elaborations (78 of such publications have appeared till April 2007).
The Public Education Office co-operates on a permanent basis with the Ministry of National Education and Sport, having signed a Co-operation Agreement in 2001. IPN gives opinions of curricula and textbooks on history that are used in Polish schools and is involved in teacher training activities. The IPN also co-organizes postgraduate diploma studies on history at the Jagiellonian University and the University of Maria Curie-Skłodowska.
Boardgames.
The Institution of National Remembrance has created several boardgames to help educate people about recent polish history
Lustration.
On 18 December 2006 Polish law regulating IPN was changed and came into effect on 15 March 2007. This change gave IPN new lustration powers. However, key articles of that law were judged unconstitutional by Poland's Constitutional Court on 11 May 2007, making the role of IPN in lustration unclear and putting the whole process into question.
Criticism.
Role in lustration and Wildstein list.
One of the most controversial aspects of IPN is a by-product of its role in collecting and publishing previously secret archives from the Polish communist security apparatus, the Służba Bezpieczeństwa: revealing secret agents and collaborators (a process called "lustration"). One incident which drew criticism involved the so-called Wildstein list; a partial list of names of people who allegedly worked for the communist era Polish intelligence service, which was copied from IPN archives (without IPN permission) in 2004 by journalist Bronisław Wildstein and published in the Internet in 2005. The list gained much attention in Polish media and politics, and during that time IPN security procedures and handling of the matter came under criticism.
IPN presidential election.
The election of a new IPN president in December 2005 was controversial. Janusz Kurtyka, the incumbent IPN president, was contested by Andrzej Przewoźnik. Przewoźnik's candidature received a severe setback after documents were found which suggested his possible co-operation with Służba Bezpieczeństwa, the Communist Poland's internal intelligence agency and secret police. Przewoźnik was cleared of the accusations only after he had lost the election.
Przewoźnik and Kurtyka both died in the 2010 Polish Air Force Tu-154 crash.
Praise.
IPN actions have also attracted support. In 2006 an open letter was published, declaring that:
"History of Solidarity and anti-communist resistance in Poland cannot be damaged by scientific studies and resulting increase in our knowledge of the past. History of opposition to totalitarianism belongs to millions of Poles and not to one social or political group which usurps the right to decide which parts of national history should be discussed and which forgotten."
This letter was signed by a former Prime Minister of Poland, Jan Olszewski; the Mayor of Zakopane, Piotr Bąk; Polish-American Professor and member of the United States Holocaust Memorial Council Marek Jan Chodakiewicz; Professors Maria Dzielska, Piotr Franaszek and Tomasz Gąsowski of the Jagiellonian University; Professor Marek Czachor of Gdańsk University of Technology, journalist and writer Marcin Wolski; Solidarity co-founder Anna Walentynowicz and dozens of others.

</doc>
<doc id="15347" url="http://en.wikipedia.org/wiki?curid=15347" title="Intelligence (disambiguation)">
Intelligence (disambiguation)

Intelligence is a term describing one or more capacities of the mind.
Intelligence may also refer to:

</doc>
<doc id="15352" url="http://en.wikipedia.org/wiki?curid=15352" title="Identical particles">
Identical particles

Identical particles, also called indistinguishable or indiscernible particles, are particles that cannot be distinguished from one another, even in principle. Species of identical particles include, but are not limited to elementary particles such as electrons, composite subatomic particles such as atomic nuclei, as well as atoms and molecules. Quasiparticles also behave in this way. Although all known indistinguishable particles are "tiny", there is no exhaustive list of all possible sorts of particles nor a clear-cut limit of applicability; see particle statistics #Quantum statistics for detailed explication.
There are two main categories of identical particles: bosons, which can share quantum states, and fermions, which do not share quantum states due to the Pauli exclusion principle. Examples of bosons are photons, gluons, phonons, helium-4 nuclei and all mesons. Examples of fermions are electrons, neutrinos, quarks, protons, neutrons, and helium-3 nuclei.
The fact that particles can be identical has important consequences in statistical mechanics. Calculations in statistical mechanics rely on probabilistic arguments, which are sensitive to whether or not the objects being studied are identical. As a result, identical particles exhibit markedly different statistical behavior from distinguishable particles. For example, the indistinguishability of particles has been proposed as a solution to Gibbs' mixing paradox.
Distinguishing between particles.
There are two ways in which one might distinguish between particles. The first method relies on differences in the particles' intrinsic physical properties, such as mass, electric charge, and spin. If differences exist, we can distinguish between the particles by measuring the relevant properties. However, it is an empirical fact that microscopic particles of the same species have completely equivalent physical properties. For instance, every electron in the universe has exactly the same electric charge; this is why we can speak of such a thing as "the charge of the electron".
Even if the particles have equivalent physical properties, there remains a second method for distinguishing between particles, which is to track the trajectory of each particle. As long as one can measure the position of each particle with infinite precision (even when the particles collide), then there would be no ambiguity about which particle is which.
The problem with the second approach is that it contradicts the principles of quantum mechanics. According to quantum theory, the particles do not possess definite positions during the periods between measurements. Instead, they are governed by wavefunctions that give the probability of finding a particle at each position. As time passes, the wavefunctions tend to spread out and overlap. Once this happens, it becomes impossible to determine, in a subsequent measurement, which of the particle positions correspond to those measured earlier. The particles are then said to be indistinguishable.
Quantum mechanical description of identical particles.
Symmetrical and antisymmetrical states.
We will now make the above discussion concrete, using the formalism developed in the article on the mathematical formulation of quantum mechanics.
Let "n" denote a complete set of (discrete) quantum numbers for specifying single-particle states (for example, for the particle in a box problem we can take "n" to be the quantized wave vector of the wavefunction.) For simplicity, consider a system composed of two identical particles. Suppose that one particle is in the state "n"1, and another is in the state "n"2. What is the quantum state of the system? Intuitively, it should be
which is simply the canonical way of constructing a basis for a tensor product space formula_2 of the combined system from the individual spaces. However, this expression implies the ability to identify the particle with "n"1 as "particle 1" and the particle with "n"2 as "particle 2". If the particles are indistinguishable, this is impossible by definition; either particle can be in either state. It turns out that we must have: 
to see this, imagine a two identical particle system. suppose we know that one of the particles is in state formula_4 and the other is in state formula_5. Prior to the measurement, there is no way to know if particle 1 is in state formula_4 and particle 2 is in state formula_5, or the other way around because the particles are indistinguishable. and so, there are equal probabilities for each of the states to occur - meaning that the system is in superposition of both states prior to the measurement. 
States where this is a sum are known as symmetric; states involving the difference are called antisymmetric. More completely, symmetric states have the form
while antisymmetric states have the form
Note that if "n"1 and "n"2 are the same, the antisymmetric expression gives zero, which cannot be a state vector as it cannot be normalized. In other words, in an antisymmetric state two identical particles cannot occupy the same single-particle states. This is known as the Pauli exclusion principle, and it is the fundamental reason behind the chemical properties of atoms and the stability of matter.
Exchange symmetry.
The importance of symmetric and antisymmetric states is ultimately based on empirical evidence. It appears to be a fact of nature that identical particles do not occupy states of a mixed symmetry, such as
There is actually an exception to this rule, which we will discuss later. On the other hand, we can show that the symmetric and antisymmetric states are in a sense special, by examining a particular symmetry of the multiple-particle states known as exchange symmetry.
Let us define a linear operator "P", called the exchange operator. When it acts on a tensor product of two state vectors, it exchanges the values of the state vectors:
"P" is both Hermitian and unitary. Because it is unitary, we can regard it as a symmetry operator. We can describe this symmetry as the symmetry under the exchange of labels attached to the particles (i.e., to the single-particle Hilbert spaces).
Clearly, formula_12 (the identity operator), so the eigenvalues of "P" are +1 and −1. The corresponding eigenvectors are the symmetric and antisymmetric states:
In other words, symmetric and antisymmetric states are essentially unchanged under the exchange of particle labels: they are only multiplied by a factor of +1 or −1, rather than being "rotated" somewhere else in the Hilbert space. This indicates that the particle labels have no physical meaning, in agreement with our earlier discussion on indistinguishability.
We have mentioned that "P" is Hermitian. As a result, it can be regarded as an observable of the system, which means that we can, in principle, perform a measurement to find out if a state is symmetric or antisymmetric. Furthermore, the equivalence of the particles indicates that the Hamiltonian can be written in a symmetrical form, such as
It is possible to show that such Hamiltonians satisfy the commutation relation
According to the Heisenberg equation, this means that the value of "P" is a constant of motion. If the quantum state is initially symmetric (antisymmetric), it will remain symmetric (antisymmetric) as the system evolves. Mathematically, this says that the state vector is confined to one of the two eigenspaces of "P", and is not allowed to range over the entire Hilbert space. Thus, we might as well treat that eigenspace as the actual Hilbert space of the system. This is the idea behind the definition of Fock space.
Fermions and bosons.
The choice of symmetry or antisymmetry is determined by the species of particle. For example, we must always use symmetric states when describing photons or helium-4 atoms, and antisymmetric states when describing electrons or protons.
Particles which exhibit symmetric states are called bosons. As we will see, the nature of symmetric states has important consequences for the statistical properties of systems composed of many identical bosons. These statistical properties are described as Bose–Einstein statistics.
Particles which exhibit antisymmetric states are called fermions. As we have seen, antisymmetry gives rise to the Pauli exclusion principle, which forbids identical fermions from sharing the same quantum state. Systems of many identical fermions are described by Fermi–Dirac statistics.
Parastatistics are also possible.
In certain two-dimensional systems, mixed symmetry can occur. These exotic particles are known as anyons, and they obey fractional statistics. Experimental evidence for the existence of anyons exists in the fractional quantum Hall effect, a phenomenon observed in the two-dimensional electron gases that form the inversion layer of MOSFETs. There is another type of statistic, known as braid statistics, which are associated with particles known as plektons.
The spin-statistics theorem relates the exchange symmetry of identical particles to their spin. It states that bosons have integer spin, and fermions have half-integer spin. Anyons possess fractional spin.
"N" particles.
The above discussion generalizes readily to the case of "N" particles. Suppose we have "N" particles with quantum numbers "n"1, "n"2, ..., nN. If the particles are bosons, they occupy a totally symmetric state, which is symmetric under the exchange of "any two" particle labels:
Here, the sum is taken over all different states under permutations "p" acting on "N" elements. The square root left to the sum is a normalizing constant. The quantity "mn" stands for the number of times each of the single-particle states "n" appears in the "N"-particle state. Note that "∑n mn = N".
In the same vein, fermions occupy totally antisymmetric states:
Here, formula_19 is the signature of each permutation (i.e.formula_20 if formula_21 is composed of an even number of transpositions, and formula_22 if odd). Note that there is no formula_23 term, because each single-particle state can appear only once in a fermionic state. Otherwise the sum would again be zero due to the antisymmetry, thus representing a physically impossible state. This is the Pauli exclusion principle for many particles.
These states have been normalized so that
Measurements of identical particles.
Suppose we have a system of "N" bosons (fermions) in the symmetric (antisymmetric) state
and we perform a measurement of some other set of discrete observables, "m". In general, this would yield some result "m1" for one particle, "m2" for another particle, and so forth. If the particles are bosons (fermions), the state after the measurement must remain symmetric (antisymmetric), i.e.
The probability of obtaining a particular result for the "m" measurement is
We can show that 
which verifies that the total probability is 1. Note that we have to restrict the sum to "ordered" values of "m1", ..., "mN" to ensure that we do not count each multi-particle state more than once.
Wavefunction representation.
So far, we have worked with discrete observables. We will now extend the discussion to continuous observables, such as the position "x".
Recall that an eigenstate of a continuous observable represents an infinitesimal "range" of values of the observable, not a single value as with discrete observables. For instance, if a particle is in a state |"ψ"⟩, the probability of finding it in a region of volume "d"3"x" surrounding some position "x" is
As a result, the continuous eigenstates |"x"⟩ are normalized to the delta function instead of unity:
We can construct symmetric and antisymmetric multi-particle states out of continuous eigenstates in the same way as before. However, it is customary to use a different normalizing constant:
We can then write a many-body wavefunction,
where the single-particle wavefunctions are defined, as usual, by
The most important property of these wavefunctions is that exchanging any two of the coordinate variables changes the wavefunction by only a plus or minus sign. This is the manifestation of symmetry and antisymmetry in the wavefunction representation:
The many-body wavefunction has the following significance: if the system is initially in a state with quantum numbers "n"1, ..., nN, and we perform a position measurement, the probability of finding particles in infinitesimal volumes near "x"1, "x"2, ..., "x"N is
The factor of "N"! comes from our normalizing constant, which has been chosen so that, by analogy with single-particle wavefunctions,
Because each integral runs over all possible values of "x", each multi-particle state appears "N"! times in the integral. In other words, the probability associated with each event is evenly distributed across "N"! equivalent points in the integral space. Because it is usually more convenient to work with unrestricted integrals than restricted ones, we have chosen our normalizing constant to reflect this.
Finally, it is interesting to note that antisymmetric wavefunction can be written as the determinant of a matrix, known as a Slater determinant:
Statistical properties.
Statistical effects of indistinguishability.
The indistinguishability of particles has a profound effect on their statistical properties. To illustrate this, let us consider a system of "N" distinguishable, non-interacting particles. Once again, let "n""j" denote the state (i.e. quantum numbers) of particle "j". If the particles have the same physical properties, the "n""j"'s run over the same range of values. Let "ε"("n") denote the energy of a particle in state "n". As the particles do not interact, the total energy of the system is the sum of the single-particle energies. The partition function of the system is
where "k" is Boltzmann's constant and "T" is the temperature. We can factor this expression to obtain
where
If the particles are identical, this equation is incorrect. Consider a state of the system, described by the single particle states ["n"1, ..., "n""N"]. In the equation for "Z", every possible permutation of the "n"'s occurs once in the sum, even though each of these permutations is describing the same multi-particle state. We have thus over-counted the actual number of states.
If we neglect the possibility of overlapping states, which is valid if the temperature is high, then the number of times we count each state is approximately "N"!. The correct partition function is
Note that this "high temperature" approximation does not distinguish between fermions and bosons.
The discrepancy in the partition functions of distinguishable and indistinguishable particles was known as far back as the 19th century, before the advent of quantum mechanics. It leads to a difficulty known as the Gibbs paradox. Gibbs showed that if we use the equation "Z" = "ξ""N", the entropy of a classical ideal gas is
where "V" is the volume of the gas and "f" is some function of "T" alone. The problem with this result is that "S" is not extensive – if we double "N" and "V", "S" does not double accordingly. Such a system does not obey the postulates of thermodynamics.
Gibbs also showed that using "Z" = "ξ""N"/"N"! alters the result to
which is perfectly extensive. However, the reason for this correction to the partition function remained obscure until the discovery of quantum mechanics.
Statistical properties of bosons and fermions.
There are important differences between the statistical behavior of bosons and fermions, which are described by Bose–Einstein statistics and Fermi–Dirac statistics respectively. Roughly speaking, bosons have a tendency to clump into the same quantum state, which underlies phenomena such as the laser, Bose–Einstein condensation, and superfluidity. Fermions, on the other hand, are forbidden from sharing quantum states, giving rise to systems such as the Fermi gas. This is known as the Pauli Exclusion Principle, and is responsible for much of chemistry, since the electrons in an atom (fermions) successively fill the many states within shells rather than all lying in the same lowest energy state.
We can illustrate the differences between the statistical behavior of fermions, bosons, and distinguishable particles using a system of two particles. Let us call the particles A and B. Each particle can exist in two possible states, labelled formula_47 and formula_48, which have the same energy.
We let the composite system evolve in time, interacting with a noisy environment. Because the formula_47 and formula_48 states are energetically equivalent, neither state is favored, so this process has the effect of randomizing the states. (This is discussed in the article on quantum entanglement.) After some time, the composite system will have an equal probability of occupying each of the states available to it. We then measure the particle states.
If A and B are distinguishable particles, then the composite system has four distinct states: formula_51, formula_52, formula_53, and formula_54. The probability of obtaining two particles in the formula_47 state is 0.25; the probability of obtaining two particles in the formula_48 state is 0.25; and the probability of obtaining one particle in the formula_47 state and the other in the formula_48 state is 0.5.
If A and B are identical bosons, then the composite system has only three distinct states: formula_51, formula_52, and formula_61. When we perform the experiment, the probability of obtaining two particles in the formula_47 state is now 0.33; the probability of obtaining two particles in the formula_48 state is 0.33; and the probability of obtaining one particle in the formula_47 state and the other in the formula_48 state is 0.33. Note that the probability of finding particles in the same state is relatively larger than in the distinguishable case. This demonstrates the tendency of bosons to "clump."
If A and B are identical fermions, there is only one state available to the composite system: the totally antisymmetric state formula_66. When we perform the experiment, we inevitably find that one particle is in the formula_47 state and the other is in the formula_48 state.
The results are summarized in Table 1:
As can be seen, even a system of two particles exhibits different statistical behaviors between distinguishable particles, bosons, and fermions. In the articles on Fermi–Dirac statistics and Bose–Einstein statistics, these principles are extended to large number of particles, with qualitatively similar results.
The homotopy class.
To understand why particle statistics work the way that they do, note first that particles are point-localized excitations and that particles that are spacelike separated do not interact. In a flat "d"-dimensional space "M", at any given time, the configuration of two identical particles can be specified as an element of "M" × "M". If there is no overlap between the particles, so that they do not interact directly, then their locations must belong to the space ["M" × "M"]/{coincident points}, the subspace with coincident points removed. The element (x, y) describes the configuration with particle I at x and particle II at y, while (y, x) describes the interchanged configuration. With identical particles, the state described by (x, y) ought to be indistinguishable from the state described by (y, x). Now consider the homotopy class of continuous paths from (x, y) to (y, x), within the space ["M" × "M"]/{coincident points}. If "M" is R"d" where "d" ≥ 3, then this homotopy class only has one element. If "M" is R2, then this homotopy class has countably many elements (i.e. a counterclockwise interchange by half a turn, a counterclockwise interchange by one and a half turns, two and a half turns, etc., a clockwise interchange by half a turn, etc.). In particular, a counterclockwise interchange by half a turn is "not" homotopic to a clockwise interchange by half a turn. Lastly, if "M" is R, then this homotopy class is empty.
Suppose first that "d" ≥ 3. The universal covering space of ["M" × "M"]/{coincident points}, which is none other than ["M" × "M"]/{coincident points} itself, only has two points which are physically indistinguishable from (x, y), namely (x, y) itself and (y, x). So, the only permissible interchange is to swap both particles. This interchange is an involution, so its only effect is to multiply the phase by a square root of 1. If the root is +1, then the points have Bose statistics, and if the root is −1, the points have Fermi statistics.
In the case "M" = R2, the universal covering space of ["M" × "M"]/{coincident points} has infinitely many points that are physically indistinguishable from (x, y). This is described by the infinite cyclic group generated by making a counterclockwise half-turn interchange. Unlike the previous case, performing this interchange twice in a row does not recover the original state; so such an interchange can generically result in a multiplication by exp("iθ") for any real "θ" (by unitarity, the absolute value of the multiplication must be 1). This is called anyonic statistics. In fact, even with two "distinguishable" particles, even though (x, y) is now physically distinguishable from (y, x), the universal covering space still contains infinitely many points which are physically indistinguishable from the original point, now generated by a counterclockwise rotation by one full turn. This generator, then, results in a multiplication by exp("iφ"). This phase factor here is called the mutual statistics.
Finally, in the case "M" = R, the space ["M" × "M"]/{coincident points} is not connected, so even if particle I and particle II are identical, they can still be distinguished via labels such as "the particle on the left" and "the particle on the right". There is no interchange symmetry here.

</doc>
<doc id="15354" url="http://en.wikipedia.org/wiki?curid=15354" title="Interstitial cystitis">
Interstitial cystitis

Interstitial cystitis, or bladder pain syndrome (also IC/BPS), is a chronic inflammatory condition of the submucosal and muscular layers of the bladder. The cause of IC/BPS is currently unknown and the condition is regarded as a diagnosis of exclusion. IC/BPS may be associated with urinary urgency, urinary frequency, waking at night to urinate (nocturia), and sterile urine cultures. Those with interstitial cystitis may have symptoms that overlap with other urinary bladder disorders such as: urinary tract infection (UTI), overactive bladder, urethritis, urethral syndrome, and prostatitis. IC/BPS can result in a quality of life comparable to that of a patient with rheumatoid arthritis, chronic cancer pain, or a patient on kidney dialysis.
Signs and symptoms.
The symptoms of IC/BPS are often misdiagnosed as a urinary tract infection. However, IC/BPS has not been shown to be caused by a bacterial infection and antibiotics are an ineffective treatment. The symptoms of IC/BPS may also initially be attributed to prostatitis and epididymitis (in men) and endometriosis and uterine fibroids (in women).
The most common symptoms of IC/BPS are suprapubic pain, urinary frequency, painful sexual intercourse, and waking up from sleep to urinate.
In general, symptoms may include painful urination described as a burning sensation in the urethra during urination, pelvic pain that is worsened with the consumption of certain foods or drinks, urinary urgency, and pressure in the bladder or pelvis. Other frequently described symptoms are urinary hesitancy (needing to wait for the urinary stream to begin, often caused by pelvic floor dysfunction and tension), and discomfort and difficulty driving, working, or traveling. Pelvic pain experienced by those with IC typically worsens with filling of the urinary bladder and may improve with urination. 
During cystoscopy, 5–10% of people with IC are found to have Hunner's ulcers. A person with IC may have discomfort only in the urethra, while another might struggle with pain in the entire pelvis. Interstitial cystitis symptoms usually fall into one of two patterns: significant suprapubic pain with little frequency or a lesser amount of suprapubic pain but with increased urinary frequency.
Association with other conditions.
Some people with IC/BPS have been diagnosed with other conditions such as irritable bowel syndrome (IBS), fibromyalgia, chronic fatigue syndrome, allergies, Sjogren's syndrome, which raises the possibility that interstitial cystitis may be caused by mechanisms that cause these other conditions. In addition, men with IC/PBS are frequently diagnosed as having chronic nonbacterial prostatitis, and there is an extensive overlap of symptoms and treatment between the two conditions, leading researchers to posit that the conditions may share the same etiology and pathology.
Causes.
The cause of IC/BPS is currently unknown. However, several explanations have been proposed and include the following: autoimmune theory, nerve theory, mast cell theory, leaky lining theory, infection theory, and a theory of production of a toxic substance in the urine. Other suggested etiological causes are neurologic, allergic, genetic, and stress-psychological. In addition, recent research shows that those with IC may have a substance in the urine that inhibits the growth of cells in the bladder epithelium. An infection may then predispose those people to develop IC. Current evidence from clinical and laboratory studies confirms that mast cells play a central role in IC/PBS possibly due to their ability to release histamine and cause pain, swelling, scarring, and interfere with healing. Research has shown a proliferation of nerve fibers is present in the bladders of people with IC which is absent in the bladders of people who have not been diagnosed with IC.
Regardless of the origin, the majority of people with IC/BPS struggle with a damaged urothelium, or bladder lining. When the surface glycosaminoglycan (GAG) layer is damaged (via a urinary tract infection (UTI), excessive consumption of coffee or sodas, traumatic injury, etc.), urinary chemicals can "leak" into surrounding tissues, causing pain, inflammation, and urinary symptoms. Oral medications like pentosan polysulfate and medications placed directly into the bladder via a catheter sometimes work to repair and rebuild this damaged/wounded lining, allowing for a reduction in symptoms. Most literature supports the belief that IC's symptoms are associated with a defect in the bladder epithelium lining, allowing irritating substances in the urine to penetrate into the bladder—essentially, a breakdown of the bladder lining (also known as the adherence theory). Deficiency in this glycosaminoglycan layer on the surface of the bladder results in increased permeability of the underlying submucosal tissues.
GP51 has been identified as a possible urinary biomarker for IC with significant variations in GP51 levels in those with IC when compared to individuals without interstitial cystitis.
Numerous studies have noted the link between IC, anxiety, stress, hyper-responsiveness, and panic. Another proposed etiology for interstitial cystitis is that the body's immune system attacks the bladder. Biopsies on the bladder walls of people with IC usually contain mast cells. Mast cells containing histamine packets gather when an allergic reaction is occurring. The body identifies the bladder wall as a foreign agent, and the histamine packets burst open and attack. The body attacks itself, which is the basis of autoimmune disorders. Additionally, IC may be triggered by an unknown toxin or stimulus which causes nerves in the bladder wall to fire uncontrollably. When they fire, they release substances called neuropeptides that induce a cascade of reactions that cause pain in the bladder wall.
Genes.
Some genetic subtypes, in some people, have been linked to the disorder.
Diagnosis.
A diagnosis of IC/BPS is one of exclusion, as well as a review of clinical symptoms. The AUA Guidelines recommend starting with a careful patient history, physical examination and laboratory tests to assess and document symptoms of IC, as well as other potential disorders. 
The KCl test, also known as the "potassium sensitivity test", is no longer recommended. The test uses a mild potassium solution to evaluate the integrity of the bladder wall. Though the latter is not specific for IC/BPS, it has been determined to be helpful in predicting the use of compounds, such as pentosan polysulphate, which are designed to help repair the GAG layer. 
For complicated cases, the use of hydrodistention with cystoscopy may be helpful. Researchers, however, determined that this visual examination of the bladder wall after stretching the bladder was not specific for IC/BPS and that the test, itself, can contribute to the development of small glomerulations (petechial hemorrhages) often found in IC/BPS. Thus, a diagnosis of IC/BPS is one of exclusion, as well as a review of clinical symptoms.
In 2006, the ESSIC society proposed more rigorous and demanding diagnostic methods with specific classification criteria so that it cannot be confused with other, similar conditions. Specifically, they require that a patient must have pain associated with the bladder, accompanied by one other urinary symptom. Thus, a patient with just frequency or urgency would be excluded from a diagnosis. Secondly, they strongly encourage the exclusion of confusable diseases through an extensive and expensive series of tests including (A) a medical history and physical exam, (B) a dipstick urinalysis, various urine cultures, and a serum PSA in men over 40, (C) flowmetry and post-void residual urine volume by ultrasound scanning and (D) cystoscopy. A diagnosis of IC/BPS would be confirmed with a hydrodistention during cystoscopy with biopsy.
They also propose a ranking system based upon the physical findings in the bladder. Patients would receive a numeric and letter based score based upon the severity of their disease as found during the hydrodistention. A score of 1–3 would relate to the severity of the disease and a rating of A–C represents biopsy findings. Thus, a patient with 1A would have very mild symptoms and disease while a patient with 3C would have the worst possible symptoms. Widely recognized scoring systems such as the O'Leary Sant symptom and problem score have emerged to evaluate the severity of IC symptoms such as pain and urinary symptoms.
In 2009, Japanese researchers identified a urinary marker called phenylacetylglutamine that could be used for early diagnosis.
Treatment.
AUA treatment guidelines.
In 2011, the American Urological Association released the first consensus-based guideline for the diagnosis and treatment of IC in the USA. 
The AUA treatment guidelines include a treatment protocol ranging from conservative treatments to more invasive interventions: 
The AUA guidelines also listed several discontinued treatments, including: long-term oral antibiotics, intravesical bacillus Calmette Guerin, intravesical resiniferatoxin), high-pressure and long-duration hydrodistention, and systemic glucocorticoids.
Bladder distension.
Bladder distension while under general anesthesia, also known as hydrodistention (a procedure which stretches the bladder capacity), has shown some success in reducing urinary frequency and giving short-term pain relief to those with IC. However, it is unknown exactly how this procedure causes pain relief. Recent studies show pressure on pelvic trigger points can relieve symptoms. The relief achieved by bladder distensions is only temporary (weeks or months), so is not viable as a long-term treatment for IC/BPS. The proportion of IC/BPS patients who experience relief from hydrodistention is currently unknown and evidence for this modality is limited by a lack of properly controlled studies. Bladder rupture and sepsis may be associated with prolonged, high-pressure hydrodistention.
Bladder Instillations.
Bladder instillation of medication is one of the main forms of treatment of interstitial cystitis, but evidence for its effectiveness is currently limited. Advantages of this treatment approach include direct contact of the medication with the bladder and low systemic side effects due to poor absorption of the medication. Single medications or a mixture of medications are commonly used in bladder instillation preparations. DMSO is the only approved bladder instillation for IC/BPS yet it is much less frequently used in urology clinics. 
Research studies presented at recent conferences of the American Urological Association by C. Subah Packer have demonstrated the FDA-approved dose of a 50% solution of DMSO had the potential to create irreversible muscle contraction. However, a lesser solution of 25% was found to be reversible. Long-term use of DMSO is questionable, as its mechanism of action is not fully understood though DMSO is thought to inhibit mast cells and may have anti-inflammatory, muscle-relaxing, and analgesic effects. Other agents used for bladder instillations to treat interstitial cystitis include: heparin, lidocaine, chondroitin sulfate, hyaluronic acid, pentosan polysulfate, oxybutynin, and botulinum toxin A. Preliminary evidence suggests these agents are efficacious in reducing symptoms of interstitial cystitis, but further study with larger, randomized, controlled clinical trials is needed.
Diet.
Diet modification is often recommended as a first-line method of self-treatment for interstitial cystitis, though rigorous controlled studies examining the impact diet has on interstitial cystitis signs and symptoms are currently lacking. Individuals with interstitial cystitis often experience an increase in symptoms when they consume certain foods and beverages. Avoidance of these potential trigger foods and beverages such as caffeine-containing beverages including coffee, tea, and soda, alcoholic beverages, chocolate, citrus fruits, hot peppers, and artificial sweeteners may be helpful in alleviating symptoms. Diet triggers vary between individuals with IC; the best way for a person to discover his or her own triggers is to use an elimination diet. Sensitivity to trigger foods may be reduced if calcium glycerophosphate and/or sodium bicarbonate is consumed. The foundation of therapy is a modification of diet to help patients avoid those foods which can further irritate the damaged bladder wall.
The mechanism by which dietary modification benefits people with IC is unclear. Integration of neural signals from pelvic organs may mediate the effects of diet on symptoms of IC.
Medications.
The antihistamine hydroxyzine failed to demonstrate superiority over placebo in treatment of IC patients in a randomized, controlled, clinical trial. 
Amitriptyline has been shown to be effective in reducing symptoms such as chronic pelvic pain and nocturia in many patients with IC/BPS with a median dose of 75 mg daily. In one study, the antidepressant duloxetine was found to be ineffective as a treatment, although a patent exists for use of duloxetine in the context of IC, and is known to relieve neuropathic pain. The calcineurin inhibitor cyclosporine A has been studied as a treatment for interstitial cystitis due to its immunosuppressive properties. A prospective randomized study found cyclosporine A to be more effective at treating IC symptoms than pentosan polysulfate, but also had more adverse effects. 
Oral pentosan polysulfate is believed to repair the protective glycosaminoglycan coating of the bladder, but studies have encountered mixed results when attempting to determine if the effect is statistically significant compared to placebo.
Pain control therapies.
Acupuncture alleviates pain associated with IC/BPS as part of multimodal treatment. While a small study showed 11 of 14 (78%) patients had a >50% reduction in pain, another study found no beneficial effect. Despite a scarcity of controlled studies on alternative medicine and IC/BPS, "rather good results have been obtained" when acupuncture is combined with other treatments. Biofeedback, a relaxation technique aimed at helping people control functions of the autonomic nervous system, has shown some benefit in controlling pain associated with IC/BPS as part of a multimodal approach that may also include medication or hydrodistention of the bladder.
Pelvic floor treatments.
Urologic pelvic pain syndromes, such as IC/BPS and CP/CPPS, are characterized by pelvic muscle tenderness, and symptoms may be reduced with pelvic myofascial physical therapy.
This may leave the pelvic area in a sensitized condition, resulting in a loop of muscle tension and heightened neurological feedback (neural wind-up), a form of myofascial pain syndrome. Current protocols, such as the Wise–Anderson Protocol, largely focus on stretches to release overtensed muscles in the pelvic or anal area (commonly referred to as trigger points), physical therapy to the area, and progressive relaxation therapy to reduce causative stress.
Pelvic floor dysfunction is a fairly new area of specialty for physical therapists worldwide. The goal of therapy is to relax and lengthen the pelvic floor muscles, rather than to tighten and/or strengthen them as is the goal of therapy for patients with urinary incontinence. Thus, traditional exercises such as Kegel exercises, can be helpful as they strengthen the muscles, but they can provoke pain and additional muscle tension. A specially trained physical therapist can provide direct, hands on evaluation of the muscles, both externally and internally.
Surgery.
Surgery is rarely used for IC/BPS. Surgical intervention is very unpredictable, and is considered a treatment of last resort for severe refractory cases of interstitial cystitis. Some patients who opt for surgical intervention continue to experience pain after surgery. Typical surgical interventions for refractory cases of IC/BPS include: bladder augmentation, urinary diversion, transurethral fulguration and resection of ulcers, and bladder removal (cystectomy). 
Neuromodulation can be successful in treating IC/BPS symptoms, including pain. One electronic pain-killing option is TENS. Percutaneous tibial nerve stimulation stimulators have also been used, with varying degrees of success. Percutaneous sacral nerve root stimulation was able to produce statistically significant improvements in several parameters, including pain.
Prognosis.
IC/BPS has a profound impact on quality of life. A 2007 Finnish epidemiologic study showed that two-thirds of women at moderate to high risk of having interstitial cystitis reported impairment in their quality of life and 35% of IC patients reported an impact on their sexual life. A 2012 survey showed that among a group of adult women with symptoms of interstitial cystitis, 11% reported suicidal thoughts in the past two weeks. Other research has shown that the impact of IC/BPS on quality of life is severe and may be comparable to the quality of life experienced in endstage renal disease or rheumatoid arthritis.
International recognition of interstitial cystitis has grown and international urology conferences to address the heterogeneity in diagnostic criteria have recently been held. IC/PBS is now recognized with an official disability code in the United States of America.
Epidemiology.
IC/BPS affects men and women of all cultures, socioeconomic backgrounds, and ages. Although the disease was previously believed to be a condition of menopausal women, growing numbers of men and women are being diagnosed in their twenties and younger. IC/BPS is not a rare condition. Early research suggested that IC/BPS prevalence ranged from 1 in 100,000 to 5.1 in 1,000 of the general population. In recent years, the scientific community has achieved a much deeper understanding of the epidemiology of interstitial cystitis. Recent studies have revealed that between 2.7 and 6.53 million women in the USA have symptoms of IC and up to 12% of women may have early symptoms of IC/BPS. Further study has estimated that the condition is far more prevalent in men than previously thought ranging from 1.8 to 4.2 million men having symptoms of interstitial cystitis.
History.
Philadelphia surgeon Joseph Parrish published the earliest record of interstitial cystitis in 1836 describing three cases of severe lower urinary tract symptoms without the presence of a bladder stone. The term "interstitial cystitis" was coined by Dr. Alexander Skene in 1887 to describe the disease. In 2002, the United States amended the Social Security Act to include interstitial cystitis as a disability. The first guideline for diagnosis and treatment of interstitial cystitis is released by a Japanese research team in 2009. The American Urological Association released the first American clinical practice guideline for diagnosing and treating IC/BPS in 2011.
Characterization.
Originally called "interstitial cystitis", this disorder was renamed to "interstitial cystitis/bladder pain syndrome" in the 2002–2010 timeframe. In 2007, the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) began using the umbrella term Urologic Chronic Pelvic Pain Syndromes (UCPPS) to refer to pain syndromes associated with the bladder (i.e., interstitial cystitis/bladder pain syndrome, IC/BPS) and the prostate gland (i.e., Chronic prostatitis/chronic pelvic pain syndrome).
In 2008, terms currently in use in addition to IC/BPS include "painful bladder syndrome", "bladder pain syndrome" and "hypersensitive bladder syndrome", alone and in a variety of combinations. These different terms are being used in different parts of the world. The term "interstitial cystitis" is the primary term used in ICD-10 and MeSH. The condition is officially recognized as a disability in the United States.

</doc>
<doc id="15355" url="http://en.wikipedia.org/wiki?curid=15355" title="ICI">
ICI

ICI or Ici may mean:
ICI is also an abbreviation which may mean:

</doc>
<doc id="15356" url="http://en.wikipedia.org/wiki?curid=15356" title="Imperial Chemical Industries">
Imperial Chemical Industries

Imperial Chemical Industries (ICI) was a British chemical company and was, for much of its history, the largest manufacturer in Britain.
It was formed by the merger of leading British chemical companies in 1926. Its headquarters were at Millbank in London, and it was a constituent of the FT 30 and later the FTSE 100.
It produced paints and speciality products (including ingredients for foods, speciality polymers, electronic materials, fragrances and flavours). ICI was acquired by AkzoNobel in 2008, who immediately sold off parts of ICI to Henkel, and integrated ICI's remaining operations within its existing organisation.
History.
Development of the business.
The company was founded in December 1926 from the merger of four companies: Brunner Mond, Nobel Explosives, the United Alkali Company, and British Dyestuffs Corporation. It established its head office at Millbank in London in 1928.
Competing with DuPont and IG Farben, the new company produced chemicals, explosives, fertilisers, insecticides, dyestuffs, non-ferrous metals, and paints. In its first year turnover was £27 million.
In the 1920s and '30s the company played a key role in the development of new chemical products, including the dyestuff phthalocyanine (1929), the acrylic plastic Perspex (1932), Dulux paints (1932, co-developed with DuPont), polyethylene (1937) and polyethylene terephthalate fibre known as Terylene (1941). In 1940 ICI started British Nylon Spinners as a joint venture with Courtaulds.
ICI also owned the Sunbeam motorcycle business, which had come with Nobel Industries, and continued to build motorcycles until 1937.
In the 1940s and '50s the company established its pharmaceutical business and developed a number of key products including
Paludrine (1940s, an anti-malarial drug), halothane (1951, an anaesthetic agent), Inderal (1965, a beta-blocker), tamoxifen (1978, a frequently used drug for breast cancer),
and PEEK (1979, a high performance thermoplastic). ICI formed ICI Pharmaceuticals in 1957. 
ICI developed a fabric in the 1950s known as Crimplene. Crimplene is a thick polyester yarn used to make a fabric of the same name. The resulting cloth is heavy, wrinkle-resistant and retains its shape well. The California-based fashion designer Edith Flagg was the first to import this fabric from Britain to the USA. During the first two years, ICI gave Flagg a large advertising budget to popularise the fabric across America.
In 1960, Paul Chambers became the first chairman appointed from outside the company. Chambers employed the consultancy firm McKinsey to help with reorganising the company. His eight year tenure in charge saw export sales double, but his reputation was severely damaged by a failed takeover bid for Courtaulds in 1961-2.
ICI was confronted with the nationalisation of its operations in Burma on 1 August 1962 as a consequence of the military coup.
In 1964 ICI acquired British Nylon Spinners (BNS), the company they had set up in 1940 jointly with Courtaulds. ICI surrendered its 37.5 per cent holding in Courtaulds and paid Courtaulds two million pounds a year for five years, “to take account of the future development expenditure of Courtaulds in the nylon field”. In return Courtaulds transferred to ICI their 50 per cent holding in BNS. BNS was absorbed into ICI existing polyester operation, ICI Fibres. The acquisition included BNS production plants in Pontypool, Gloucester and Doncaster together with Research and Development in Pontypool.
Early pesticide development included Gramoxone (1962, a herbicide), the insecticides pirimiphos-methyl in 1967 and pirimicarb in 1970, brodifacoum (a rodenticide) was developed in 1974; in the late 1970s, ICI was involved in the early development of synthetic pyrethroid insecticides such as lambda-cyhalothrin.
Peter Allen was appointed chairman between 1968 and 1971. He presided over the purchase of Viyella. Profits shrank under his tenure.
Jack Callard was appointed chairman from 1971 to 1975. He almost doubled company profits between 1972 and 1974, and made ICI Britain's largest exporter. The company acquired Atlas Chemical Industries Inc., a major American competitor, in 1971.
In 1977 Imperial Metal Industries was divested as an independent quoted company.
During the 1980s (from 1982 to 1987) the company was led by the charismatic John Harvey Jones. Under his leadership the company acquired the Beatrice Chemical Division in 1985 and Glidden Coatings & Resins, a leading paints business in 1986.
From 1991 to 2007: reorganisation of the business.
In 1991 ICI sold the agricultural and merchandising operations of BritAg and Scottish Agricultural Industries to Norsk Hydro.
In 1991, ICI fought off a hostile takeover bid from Hanson, who had acquired 2.8 percent of the company. However the move pressured the company to split-off its pharmaceuticals arm.
It also divested its soda ash products arm to Brunner Mond. This ended an association with the trade that had existed since the company's inception, one that had been inherited from the original Brunner, Mond & Co. Ltd.
In 1992 the company sold its nylon business to DuPont.
In 1993, the company de-merged its pharmaceutical bioscience businesses: pharmaceuticals, agrochemicals, specialities, seeds and biological products were all transferred into a new and independent company called Zeneca Group, which subsequently merged with Astra AB to form AstraZeneca PLC.
Charles Miller Smith was appointed Chief Executive Officer in 1994. This was one of the few times that someone from outside ICI had been appointed to lead the company, Smith having previously been a director at Unilever. Shortly afterwards the company acquired a number of former Unilever businesses, in an attempt to move away from the company's historical reliance on commodity chemicals. 
In 1997 ICI acquired National Starch & Chemical, Quest, Unichema, and Crosfield, the speciality chemicals businesses of Unilever for $8bn. This step was part of a strategy to move away from cyclical bulk chemicals and to progress up the value chain to become a higher growth, higher margin business. Later that year it went on to buy Rutz & Huber, a Swiss paints business.
Having taken on some £4 billion of debt to finance these acquisitions, the company had to sell off its commodity chemicals businesses. 
Having sold off much of its historically profitable commodities businesses, and many of the new speciality businesses, which it had failed to integrate, the company consisted mainly of the Dulux paints business, which quickly found itself the subject of a takeover by AkzoNobel.
2007: takeover by Akzo Nobel.
Dutch firm AkzoNobel (owner of Crown Berger paints) bid £7.2 billion (€10.66 billion or $14.5 billion US) for ICI in June 2007. An area of concern about a potential deal was ICI's British pension fund, which had future liabilities of more than £9 billion at the time. Regulatory issues in the UK and other markets where Dulux and Crown Paints brands each have significant market share were also a cause for concern for the boards of ICI and Akzo Nobel. In the UK, any combined operation without divestments would have seen Akzo Nobel have a 54% market share in the paint market. The initial bid was rejected by the ICI board and the majority of shareholders. However, a subsequent bid for £8 billion (€11.82 billion) was accepted by ICI in August 2007, pending approval by regulators.
At 8.00am on 2 January 2008 completion of the takeover of ICI plc by Akzo Nobel NV was announced. Shareholders of ICI received either £6.70 in cash or Akzo Nobel loan notes to the value of £6.70 per 1 nominal ICI share. The adhesives business of ICI was transferred to Henkel as a result of the deal, while Akzo agreed to sell its Crown Paints subsidiary to satisfy the concerns of the European Commissioner for Competition.
The areas of concern regarding the ICI UK pension scheme were addressed by ICI and Akzo.
Operations.
ICI operated a number of chemical sites around the world. In the UK the main plants were as follows:
Argentina.
ICI subsidiary, formerly called Duperial since 1928 until 1995, when was renamed to ICI. Established in the city of San Lorenzo, Santa Fe. It's operating in an integrated production site with commercial offices in Buenos Aires. Since 2009 called Akzo Nobel Functional Chemicals S.A. and produces sulfuric acid with ISO cerfication.
Australia.
ICI subsidiary ICI Australia Ltd established the Dry Creek Saltfields at Dry Creek north of Adelaide, South Australia, in 1940, with an associated soda ash plant at nearby Osborne. In 1989 these operations were sold to Penrice Soda Products.

</doc>
<doc id="15357" url="http://en.wikipedia.org/wiki?curid=15357" title="Imperial Airways">
Imperial Airways

Imperial Airways was the early British commercial long range air transport company, operating from 1924 to 1939 and serving parts of Europe but principally the British Empire routes to South Africa, India and the Far East, including Malaya and Hong Kong. There were local partnership companies; Qantas (Queensland and Northern Territory Aerial Services Ltd) in Australia and TEAL (Tasman Empire Airways Ltd) in New Zealand.
Imperial Airways was merged into the British Overseas Airways Corporation (BOAC) in 1939, which in turn merged with the British European Airways Corporation to form British Airways.
Background.
The establishment of Imperial Airways occurred in the context of facilitating overseas settlement by making travel to and from the colonies quicker, and that flight would also speed up colonial government and trade that was until then dependent upon ships. The launch of the airline followed a burst of air route survey in the British Empire after the First World War, and after some experimental (and often dangerous) long-distance flying to the margins of Empire.
Formation.
Imperial Airways was created against a background of stiff competition from French and German airlines that enjoyed heavy government subsidies and following the advice of the government's 'Hambling Committee' (formally known as the 'C.A.T Subsidies Committee'). The committee produced a report on 15 February 1923 recommending that four of the largest existing airlines, The Instone Air Line Company, owned by shipping magnate Samuel Instone, Noel Pemberton Billing's British Marine Air Navigation (part of the Supermarine flying-boat company), the Daimler Airway, under the management of George Edward Woods and Handley Page Transport Co Ltd., should be merged.
It was hoped that this would create a company which could compete against French and German competition and would be strong enough to develop Britain's external air services while minimizing government subsidies for duplicated services. With this in view, a £1m subsidy over ten years was offered to encourage the merger. Agreement was made between the President of the Air Council and the British, Foreign and Colonial Corporation on 3 December 1923 for the company, under the title of the 'Imperial Air Transport Company' to acquire existing air transport services in the UK. The agreement set out the government subsidies for the new company: £137,000 in the first year diminishing to £32,000 in the tenth year as well as minimum mileages to be achieved and penalties if these weren't met.
Imperial Airways Limited was formed on 31 March 1924 with equipment from each contributing concern: British Marine Air Navigation Company Ltd, the Daimler Airway, Handley Page Transport Ltd and the Instone Air Line Ltd. The government had appointed two directors Hambling (who was also President of the Institute of Bankers) and Major J. W. Hills a former Treasury Financial Secretary.
The land operations were based at Croydon Airport to the south of London. IAL immediately discontinued its predecessors' service to points north of London, the airline being focused on international and imperial service rather than domestic. Thereafter the only IAL aircraft operating 'North of Watford' were charter flights.
Industrial troubles with the pilots delayed the start of services until 26 April 1924, when a daily London–Paris route was opened with a de Havilland DH.34. Thereafter the task of expanding the routes between England and the Continent began, with Southampton–Guernsey on 1 May 1924, London-Brussels–Cologne on 3 May, London–Amsterdam on 2 June 1924, and a summer service from London–Paris–Basle–Zürich on 17 June 1924. The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Empire services.
Route proving.
Between 16 November 1925 and 13 March 1926 Alan Cobham made an Imperial Airways’ route survey flight from the UK to Cape Town and back in the Armstrong Siddeley Jaguar–powered de Havilland DH.50J floatplane "G-EBFO". The outward route was London–Paris–Marseille–Pisa–Taranto–Athens–Sollum–Cairo–Luxor–Assuan–Wadi Halfa–Atbara–Khartoum–Malakal–Mongalla–Jinja–Kisumu–Tabora–Abercorn–Ndola–Broken Hill–Livingstone–Bulawayo–Pretoria–Johannesburg–Kimberley–Blomfontein–Cape Town. On his return Cobham was awarded the Air Force Cross for his services to aviation.
On 30 June 1926 Alan Cobham took off from the River Medway at Rochester in "G-EBFO" to make an Imperial Airways route survey for a service to Melbourne, arriving on 15 August. He left Melbourne on 29 August and, after completing 28,000 miles in 320 hours flying time over 78 days, he alighted on the Thames at Westminster on 1 October. Cobham was met by the Secretary of State for Air, Sir Samuel Hoare, and was subsequently knighted by HM King George V.
27 December 1926 Imperial Airways de Havilland DH.66 Hercules "G-EBMX City of Delhi" left Croydon for a survey flight to India. The flight reached Karachi on 6 January and Delhi on 8 January. The aircraft was named by Lady Irwin, wife of the Viceroy, on 10 January 1927. The return flight left on 1 February 1927 and arrived at Heliopolis, Cairo on 7 February. The flying time from Croydon to Delhi was 62 hours 27 minutes and Delhi to Heliopolis 32 hours 50 minutes.
The Eastern Route.
Regular services on the Cairo to Basra route began on 12 January 1927 using DH.66 aircraft, replacing the previous RAF mail flight. Following 2 years of negotiations with the Persian authorities regarding overflight rights, a London to Karachi service started on 30 March 1929, taking 7 days and consisting of a flight from London to Basle, a train to Genoa and a Short S.8 Calcutta flying boats to Alexandria, a train to Cairo and finally a DH.66 flight to Karachi. The route was extended as far as Delhi on 29 December 1929. The route across Europe and the Mediterranean changed many times over the next few years but almost always involved a rail journey.
In April 1931 an experimental London-Australia air mail flight took place; the mail was transferred at the Dutch East Indies, and took 26 days in total to reach Sydney. For the passenger flight leaving London on 1 October 1932, the Eastern route was switched from the Persian to the Arabian side of the Persian Gulf, and Handley Page HP 42 airliners were introduced on the Cairo to Karachi sector. The move saw the establishment of an airport and rest house, Al Mahatta Fort, in the Trucial State of Sharjah now part of the United Arab Emirates.
On 29 May 1933 an England to Australia survey flight took off, operated by Imperial Airways Armstrong Whitworth Atalanta G-ABTL "Astraea". Major H G Brackley, Imperial Airways’ Air Superintendent, was in charge of the flight. "Astraea" flew Croydon-Paris-Lyons-Rome-Brindidsi-Athens-Alexandria-Cairo where it followed the normal route to Karachi then onwards to Jodhpur-Delhi-Calcutta-Akyab-Rangoon-Bangkok-Prachuab-Alor Star-Singapore-Palembang-Batavia-Sourabaya-Bima-Koepang-Bathurst Island-Darwin-Newcastle Waters-Camooweal-Cloncurry-Longreach-Roma-Toowoomba reaching Eagle Farm, Brisbane on 23 June. Sydney was visited on 26 June, Canberra on 28 June and Melbourne on 29 June.
There followed a rapid eastern extension. The first London to Calcutta service departed on 1 July 1933, the first London to Rangoon service on 23 September 1933, the first London to Singapore service on 9 December 1933, and the first London to Brisbane service on 8 December 1934, with QANTAS responsible for the Singapore to Brisbane sector. (The 1934 start was for mail; passenger flights to Brisbane began the following April.) The first London to Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
The Africa Route.
On 28 February 1931 a weekly service began between London and Mwanza on Lake Victoria in Tanganyika as part of the proposed route to Cape Town. On 9 December 1931 the Imperial Airways’ service for Central Africa was extended experimentally to Cape Town for the carriage of Christmas mail. The aircraft used on the last sector, DH66 G-AARY "City of Karachi" arrived in Cape Town on 21 December 1931. On 20 January 1932 a mail-only route to London to Cape Town was opened. On 27 April this route was opened to passengers and took 10 days. In early 1933 Atalantas replaced the DH.66s on the Kisumu to Cape Town sector of the London to Cape Town route. On 9 February 1936 the trans-Africa route was opened by Imperial Airways between Khartoum and Kano in Nigeria. This route was extended to Lagos on 15 October 1936.
Short Empire Flying Boats.
In 1937 with the introduction of Short Empire flying boats built at Short Brothers, Imperial Airways could offer a through-service from Southampton to the Empire. The journey to the Cape was via Marseille, Rome, Brindisi, Athens, Alexandria, Khartoum, Port Bell, Kisumu and onwards by land-based craft to Nairobi, Mbeya and eventually Cape Town. Survey flights were also made across the Atlantic and to New Zealand. By mid-1937 Imperial had completed its thousandth service to the Empire. Starting in 1938 Empire flying boats also flew between Britain and Australia via India and the Middle East.
In March 1939 three Shorts a week left Southhampton for Australia, reaching Sydney after ten days of flying and nine overnight stops. Three more left for South Africa, taking six flying days to Durban.
Passengers.
Imperial's aircraft were small, most seating fewer than twenty passengers; about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research. To begin with only the wealthy could afford to fly, but passenger lists gradually diversified. Travel experiences related to flying low and slow, and were reported enthusiastically in newspapers, magazines and books. There was opportunity for sightseeing from the air and at stops.
Crews.
Imperial Airways stationed its all-male flight deck crew, cabin crew and ground crew along the length of its routes. Specialist engineers and inspectors – and ground crew on rotation or leave – travelled on the airline without generating any seat revenue. Several air crew lost their lives in accidents. At the end of the 1930s crew numbers approximated 3,000. All crew were expected to be ambassadors for Britain and the British Empire.
Air Mail.
In 1934 the Government began negotiations with Imperial Airways to establish a service (Empire Air Mail Scheme) to carry mail by air on routes served by the airline. Indirectly these negotiations led to the dismissal in 1936 of Sir Christopher Bullock, the Permanent Under-Secretary at the Air Ministry, who was found by a Board of Inquiry to have abused his position in seeking a position on the board of the company while these negotiations were in train. The Government, including the Prime Minister, regretted the decision to dismiss him, later finding that, in fact, no corruption was alleged and sought Bullock's reinstatement which he declined.
The Empire Air Mail Programme began in July 1937, delivering anywhere for 1½ d./oz. By mid-1938 a hundred tons of mail had been delivered to India and a similar amount to Africa. In the same year, construction was started on the Empire Terminal in Victoria, London, designed by A. Lakeman and with a statue by Eric Broadbent, "Speed Wings Over the World" gracing the portal above the main entrance. From the terminal there were train connections to Imperial's flying boats at Southampton and coaches to its landplane base at Croydon Airport. The terminal operated as recently as 1980.
To help promote use of the Air Mail service, in June and July 1939, Imperial Airways participated with Pan American Airways in providing a special "around the world" service; Imperial carried the souvenir mail from Foynes, Ireland, to Hong Kong, out of the eastbound New York to New York route. Pan American provided service from New York to Foynes (departing 24 June, via the first flight of Northern FAM 18) and Hong Kong to San Francisco (via FAM 14), and United Airlines carried it on the final leg from San Francisco to New York, arriving on 28 July.
Captain H.W.C. Alger was the pilot for the inaugural air mail flight carrying mail from England to Australia for the first time on the Short Empire flyingboat "Castor" for Imperial Airways' Empires Air Routes, in 1937.
In January 2016, 80 years later, the Crete2Cape Vintage Air Rally will be flying this old route with ten vintage aeroplanes - a celebration of the skill and determination of these early aviators.
Aircraft.
Imperial Airways operated many types of aircraft from its formation in 1 April 1924 until 1 April 1940 when all aircraft still in service were transferred to BOAC.

</doc>
<doc id="15358" url="http://en.wikipedia.org/wiki?curid=15358" title="Insanity defense">
Insanity defense

In criminal trials, the insanity defense is the claim that the defendant is not responsible for his or her actions during a mental health episode (psychiatric illness or mental handicap). Exemption of the insane from full criminal punishment dates back to at least the Code of Hammurabi. There are different definitions of legal insanity, such as the "M'Naghten Rules", the Durham Rule, the American Legal Institute definition, and various miscellaneous provisions (e.g., relating to lack of mens rea).
In the United Kingdom, Ireland, and the United States, use of the defense is rare; however, since the Criminal Procedure (Insanity and Unfitness to Plead) Act 1991, insanity pleas have steadily increased in the UK. Mitigating factors, including things not eligible for the insanity defense like intoxication (or, more frequently, diminished capacity), may lead to reduced charges or reduced sentences.
The insanity defense is based on evaluations by forensic mental health professionals with the appropriate test according to the jurisdiction. Their testimony guides the jury, but they are not allowed to testify to the accused's criminal responsibility, as this is a matter for the jury to decide. Similarly, mental health practitioners are restrained from making a judgment on the issue of whether the defendant is or is not insane or what is known as the "ultimate issue".
Some jurisdictions require the evaluation to address the defendant's ability to control their behavior at the time of the offense (the volitional limb). A defendant claiming insanity is pleading "not guilty by reason of insanity" (NGRI) or "guilty but insane/mentally ill" in some jurisdictions which, if successful, may result in the defendant being committed to a psychiatric facility for an indeterminate period.
Mitigating factors and diminished capacity.
The United States Supreme Court (in "Penry v. Lynaugh") and the United States Court of Appeals for the Fifth Circuit (in "Bigby v. Dretke") have been clear in their decisions that jury instructions in death penalty cases that do not ask about mitigating factors regarding the defendant's mental health violate the defendant's Eighth Amendment rights, saying that the jury is to be instructed to consider mitigating factors when answering unrelated questions. This ruling suggests specific explanations to the jury are necessary to weigh mitigating factors.
Diminished responsibility or diminished capacity can be employed as a mitigating factor or partial defense to crimes and, in the United States, is applicable to more circumstances than the insanity defense. Where it is a partial defense, it has the effect of reducing the charge to manslaughter. The Homicide Act 1957 is the statutory basis for the defense of diminished responsibility in England & Wales, whereas in Scotland it is a product of case law. The number of findings of diminished responsibility has been matched by a fall in unfitness to plead and insanity findings (Walker, 1968). A plea of diminished capacity is different from a plea of insanity in that "reason of insanity" is a full defense while "diminished capacity" is merely a plea to a lesser crime.
Withdrawal of successful insanity defense.
Several cases have ruled that persons found not guilty by reason of insanity may not withdraw the defense in a habeas petition to pursue an alternative, although there have been exceptions in other rulings. In State v. Connelly, 700 A.2d 694 (Conn. App. Ct. 1997), the petitioner who had originally been found not guilty by reason of insanity and committed for ten years to the jurisdiction of a Psychiatric Security Review Board, filed a pro se writ of "habeas corpus" and the court vacated his insanity acquittal. He was granted a new trial and found guilty of the original charges, receiving a prison sentence of 40 years.
Refusal of insanity defense.
In the landmark case of "Frendak v. United States" in 1979, the court ruled that the insanity defense cannot be imposed upon an unwilling defendant if an intelligent defendant voluntarily wishes to forego the defense.
Psychiatric treatments.
Those found to have been not guilty by reason of insanity are generally then required to undergo psychiatric treatment, except in the case of temporary insanity (see below). Defendants found not guilty by reason of insanity are generally placed in a mental institution. This is not the case in England and Wales, where under the Criminal Procedure (Insanity and Unfitness to Plead) Act of 1991 (amended by the Domestic Violence, Crime and Victims Act, 2004 to remove the option of a guardianship order), the court can mandate a hospital order, a restriction order (where release from hospital requires the permission of the Home Secretary), a "supervision and treatment" order, or an absolute discharge. Unlike defendants who are found guilty of a crime, they are not institutionalized for a fixed period, but rather held in the institution until they are determined not to be a threat. Authorities making this decision tend to be cautious, and as a result, defendants can often be institutionalized for longer than they would have been incarcerated in prison.
In "Foucha v. Louisiana" (1992) the Supreme Court of the United States ruled that a person could not be held "indefinitely".
So far, in the United States, those acquitted of a federal offense by reason of insanity have not been able to challenge their psychiatric confinement through a writ of habeas corpus or other remedies. In "Archuleta v. Hedrick", 365 F.3d 644 (8th Cir. 2004), the U.S. Court of Appeals for the Eighth Circuit the court ruled persons found not guilty by reason of insanity and later want to challenge their confinement may not attack their initial successful insanity defense:
 The appellate court affirmed the lower court’s judgment: "Having thus elected to make himself a member of that ‘exceptional class’ of persons who seek verdicts of not guilty by reason of insanity...he cannot now be heard to complain of the statutory consequences of his election." The court held that no direct attack upon the final judgment of acquittal by reason of insanity was possible. It also held that the collateral attack that he was not informed that a possible alternative to his commitment was to ask for a new trial was not a meaningful alternative.
Incompetency and mental illness.
An important distinction to be made is the difference between competency and criminal responsibility.
Competency largely deals with the defendant's present condition, while criminal responsibility addresses the condition at the time the crime was committed.
In the United States, a trial in which the insanity defense is invoked typically involves the testimony of psychiatrists or psychologists who will, as expert witnesses, present opinions on the defendant's state of mind at the time of the offense.
Therefore, a person whose mental disorder is not in dispute is determined to be sane if the court decides that despite a "mental illness" the defendant was responsible for the acts committed and will be treated in court as a normal defendant. If the person has a mental illness and it is determined that the mental illness interfered with the person's ability to determine right from wrong (and other associated criteria a jurisdiction may have) and if the person is willing to plead guilty or is proven guilty in a court of law, some jurisdictions have an alternative option known as either a Guilty but Mentally Ill (GBMI) or a Guilty but Insane verdict. The GBMI verdict is available as an alternative to, rather than in lieu of, a "not guilty by reason of insanity" verdict. Michigan (1975) was the first state to create a GBMI verdict, after two prisoners released after being found NGRI committed violent crimes within a year of release, one raping two women and the other killing his wife.
History of the insanity defense.
The concept of defense by insanity has existed since ancient Greece and Rome. However, in colonial America a delusional Dorothy Talbye was hanged in 1638 for murdering her daughter, as at the time Massachusetts's common law made no distinction between insanity (or mental illness) and criminal behavior. Edward II, under English Common law, declared that a person was insane if their mental capacity was no more than that of a "wild beast" (in the sense of a dumb animal, rather than being frenzied). The first complete transcript of an insanity trial dates to 1724. It is likely that the insane, like those under 14, were spared trial by ordeal. When trial by jury replaced this, the jury members were expected to find the insane guilty but then refer the case to the King for a Royal Pardon. From 1500 onwards, juries could acquit the insane, and detention required a separate civil procedure (Walker, 1985). The Criminal Lunatics Act 1800, passed with retrospective effect following the acquittal of James Hadfield, mandated detention at the regent's pleasure (indefinitely) even for those who, although insane at the time of the offence, were now sane.
The M'Naghten Rules of 1843 were not a codification or definition of insanity but rather the responses of a panel of judges to hypothetical questions posed by Parliament in the wake of Daniel M'Naghten's acquittal for the homicide of Edward Drummond, whom he mistook for British Prime Minister Robert Peel. The rules define the defense as "at the time of committing the act the party accused was labouring under such a defect of reason, from disease of the mind, as not to know the nature and quality of the act he was doing, or as not to know that what he was doing was wrong." The key is that the defendant could not appreciate the nature of his actions during the commission of the crime. M'Naghten's Case, 8 Eng. Rep. 718 (1843).
In "Ford v. Wainwright" 477 U.S. 399 (1986), the US Supreme Court upheld the common law rule that the insane cannot be executed. It further stated that a person under the death penalty is entitled to a competency evaluation and to an evidentiary hearing in court on the question of his competency to be executed.
In Wainwright v. Greenfield, the Court ruled that it was fundamentally unfair for the prosecutor to comment during the court proceedings on the petitioner's silence invoked as a result of a Miranda warning. The prosecutor had argued that the respondent's silence after receiving Miranda warnings was evidence of his sanity.
Controversy over the insanity defense.
The insanity plea is used in the U.S Criminal Justice System in less than 1% of all criminal cases. Little is known about the criminal justice system and the mentally ill:
 It should be noted, however, that there is no "definitive" study regarding the percentage of people with mental illness who come into contact with police, appear as criminal defendants, are incarcerated, or are under community supervision. Furthermore, the scope of this issue varies across jurisdictions. Accordingly, advocates should rely as much as possible on statistics collected by local and state government agencies.
Some U.S. states have begun to ban the use of the insanity defense, and in 1994 the Supreme Court denied a petition of certiorari seeking review of a Montana Supreme Court case that upheld Montana's abolition of the defense. Idaho, Kansas, and Utah have also banned the defense. However, a mentally ill defendant/patient can be found unfit to stand trial in these states. In 2001, the Nevada Supreme Court found that their state's abolition of the defense was unconstitutional as a violation of Federal due process. In 2006, the Supreme Court decided "Clark v. Arizona" upheld Arizona's limitations on the insanity defense. In that same ruling, the Court noted "We have never held that the Constitution mandates an insanity defense, nor have we held that the Constitution does not so require."
The insanity defense is also complicated because of the underlying differences in philosophy between psychiatrists/psychologists and legal professionals. In the United States, a psychiatrist, psychologist or other mental health professional is often consulted as an expert witness in insanity cases, but the ultimate "legal" judgment of the defendant's sanity is determined by a jury, not by a psychologist. In other words, psychologists provide testimony and professional opinion but are not ultimately responsible for answering legal questions.
United States law.
In the United States, variances in the insanity defense between states, and in the federal court system, are attributable to differences with respect to three key issues: (1) Whether to provide the insanity defense, (2) how to define "insanity," and (3) the burden of proof.
Availability.
In the United States, a criminal defendant may plead insanity in federal court, and in the state courts of every state except for Idaho, Kansas, Montana, and Utah.
Definition.
Each state and the federal court system currently uses one of the following "tests" to define insanity for purposes of the insanity defense:
M'Naghten test.
The guidelines for the "M'Naghten Rules" (1843) 10 C & F 200, state, "inter alia", and evaluating the criminal responsibility for defendants claiming to be insane were settled in the British courts in the case of Daniel M'Naghten in 1843. M'Naghten was a Scottish woodcutter who killed the secretary to the prime minister, Edward Drummond, in a botched attempt to assassinate the prime minister himself. M'Naghten apparently believed that the prime minister was the architect of the myriad of personal and financial misfortunes that had befallen him. During his trial, nine witnesses testified to the fact that he was insane, and the jury acquitted him, finding him "not guilty by reason of insanity."
The House of Lords asked the judges of the common law courts to answer five questions on insanity as a criminal defence, and the formulation that emerged from their review—that a defendant should not be held responsible for his actions only if, as a result of his mental disease or defect, he (i) did not know that his act would be wrong; or (ii) did not understand the nature and quality of his actions—became the basis of the law governing legal responsibility in cases of insanity in England. Under the rules, loss of control because of mental illness was no defense. The M'Naghten rule was embraced with almost no modification by American courts and legislatures for more than 100 years, until the mid-20th century. M'Naghten's Case, 8 Eng. Rep. 718 (1843).
"Durham"/New Hampshire test.
The strict M'Naghten standard for the insanity defense was used until the 1950s and the "Durham v. United States" case. In the Durham case, the court ruled that a defendant is entitled to acquittal if the crime was the "product of" his mental illness (i.e., crime would not have been committed but for the disease). The test, also called the Product Test, is broader than either the M'Naghten test or the irresistible impulse test. The test has much more lenient guideline for the insanity defense, but it addressed the issue of convicting mentally ill defendants, which was allowed under the M'Naghten Rule. M'Naghten's Case, 8 Eng.Rep. 718 (1843). However, the Durham standard drew much criticism because of its expansive definition of legal insanity.
Model Penal Code test.
The Model Penal Code, published by the American Law Institute, provides a standard for legal insanity that serves as a compromise between the strict M'Naghten Rule, the lenient Durham ruling, and the irresistible impulse test. Under the MPC standard, which represents the modern trend, a defendant is not responsible for criminal conduct "if at the time of such conduct as a result of mental disease or defect he lacks "substantial capacity" either to appreciate the criminality of his conduct or to conform his conduct to the requirements of the law." The test thus takes into account both the cognitive and volitional capacity of insanity.
Insanity Defense Reform Act of 1984.
After the perpetrator of President Reagan's assassination attempt was found not guilty by reason of insanity, Congress passed the Insanity Defense Reform Act of 1984. Under this act, the burden of proof was shifted from the prosecution to the defense and the standard of evidence in federal trials was increased from a preponderance of evidence to clear and convincing evidence. The ALI test was discarded in favor of a new test that more closely resembled M'Naghten's. Under this new test only perpetrators suffering from severe mental illnesses at the time of the crime could successfully employ the insanity defense. The defendant's ability to control himself or herself was no longer a consideration.
The Act also curbed the scope of expert psychiatric testimony and adopted stricter procedures regarding the hospitalization and release of those who found not guilty by reason of insanity.
Additional reforms have taken place besides the major Insanity Defense Reform Act, including the addition of the GBMI (Guilty but Mentally Ill) option to trial, changes in the burden and/or standard of proof in a trial, changes in the test of insanity or in the entering of the plea all together, various alterations in the trial procedures, and changes in commitment and release procedures after the trial has been complete.
Burden of proof.
In a majority of states, the burden is placed on the defendant, who must prove insanity by a preponderance of the evidence.
In a minority of states, the burden is placed on the prosecution, who must prove sanity beyond a reasonable doubt.
In federal court, and in Arizona, the burden is placed on the defendant, who must prove insanity by clear and convincing evidence. See 18 U.S.C.S. Sec. 17(b); see also A.R.S. Sec. 13-502(C).
Temporary insanity.
The notion of temporary insanity argues that a defendant "was" insane, but is now sane. This defense was first used by U.S. Congressman Daniel Sickles of New York in 1859 after he had killed his wife's lover, Philip Barton Key, but was most used during the 1940s and 1950s. Another case around that time was that of Charles J. Guiteau, who assassinated President James Garfield in 1881. Guiteau's Case, 10 F.161 (1882).
Scottish law.
The Scottish Law Commission, in its Discussion Paper No 122 on Insanity and Diminished Responsibility (2003), pp. 16/18 confirms that the law has not substantially changed from the position stated in Hume's Commentaries:
The phrase "absolute alienation of reason" is still regarded as at the core of the defense in the modern law (see "HM Advocate v Kidd" (1960) JC 61 and "Brennan v HM Advocate" (1977)
Nordic countries.
In the Nordic countries, insanity is not a defense; instead, it is the responsibility of the court system as such to consider whether the accused may have been psychotic or suffering from other severe mental defects when perpetrating the criminal act. This explains why, in Norway, the court considered the sanity of Anders Behring Breivik, even if he himself declared to be sane.
Rules differ between Nordic countries.
In Sweden, psychotic perpetrators are seen as accountable, but the sanction is to forensic mental care.
In Denmark and Norway, psychotic perpetrators are declared guilty, but not punished. Instead of prison, they are sentenced to mandatory treatment. Still, important differences exist between Norway and Denmark.
In Norway, §44 of the penal code states specifically that "a person who at the time of the crime was insane or unconscious is not punished".
In Denmark, §16 of the penal code states that "Persons, who, at the time of the act, were irresponsible owing to mental illness or similar conditions or
to a pronounced mental deficiency, are not punishable". This means that in Denmark, 'insanity' is a legal term rather than a medical term and that the court retains the authority to decide whether an accused person is irresponsible or not.
In Finland, punishments can only be administered if the accused is "compos mentis", of sound mind; not if the accused is insane ("syyntakeeton", literally "unable to guarantee [shoulder the responsibility of] guilt"). Thus, an insane defendant may be found guilty based on the facts and his actions just as a sane defendant, but the insanity will only affect the punishment. The definition of insanity is similar to the M'Naught criterion above: "the accused is insane, if during the act, due to a mental illness, profound mental retardation or a severe disruption of mental health or consciousness, he cannot understand the actual nature of his act or its illegality, or that his ability to control his behavior is critically weakened". If an accused is suspected to be insane, the court must consult the National Institute for Health and Welfare (THL), which is obliged to place the accused in involuntary commitment if he is found insane. The offender receives no judicial punishment; he becomes a patient under the jurisdiction of THL, and must be released immediately once the conditions of involuntary commitment are no longer fulfilled. Diminished responsibility is also available, resulting in lighter sentences.
Usage and success rate.
This increased coverage gives the impression that the defense is widely used, but this is not the case. According to an eight-state study, the insanity defense is used in less than 1% of all court cases and, when used, has only a 26% success rate. Of those cases that were successful, 90% of the defendants had been previously diagnosed with mental illness.

</doc>
<doc id="15361" url="http://en.wikipedia.org/wiki?curid=15361" title="Ice age">
Ice age

An ice age is a period of long-term reduction in the temperature of Earth's surface and atmosphere, resulting in the presence or expansion of continental and polar ice sheets and alpine glaciers. Within a long-term ice age, individual pulses of cold climate are termed "glacial periods" (or alternatively "glacials" or "glaciations" or colloquially as "ice age"), and intermittent warm periods are called "interglacials". Glaciologically, "ice age" implies the presence of extensive ice sheets in the northern and southern hemispheres. By this definition, we are in an interglacial period—the Holocene—of the ice age that began 2.6 million years ago at the start of the Pleistocene epoch, because the Greenland, Arctic, and Antarctic ice sheets still exist.
Origin of ice age theory.
In 1742 Pierre Martel (1706–1767), an engineer and geographer living in Geneva, visited the valley of Chamonix in the Alps of Savoy. Two years later he published an account of his journey. He reported that the inhabitants of that valley attributed the dispersal of erratic boulders to the glaciers, saying that they had once extended much farther. Later similar explanations were reported from other regions of the Alps. In 1815 the carpenter and chamois hunter Jean-Pierre Perraudin (1767–1858) explained erratic boulders in the Val de Bagnes in the Swiss canton of Valais as being due to glaciers previously extending further. An unknown woodcutter from Meiringen in the Bernese Oberland advocated a similar idea in a discussion with the Swiss-German geologist Jean de Charpentier (1786–1855) in 1834. Comparable explanations are also known from the Val de Ferret in the Valais and the Seeland in western Switzerland and in Goethe's scientific work. Such explanations could also be found in other parts of the world. When the Bavarian naturalist Ernst von Bibra (1806–1878) visited the Chilean Andes in 1849–1850 the natives attributed fossil moraines to the former action of glaciers.
Meanwhile, European scholars had begun to wonder what had caused the dispersal of erratic material. From the middle of the 18th century some discussed ice as a means of transport. The Swedish mining expert Daniel Tilas (1712–1772) was, in 1742, the first person to suggest drifting sea ice in order to explain the presence of erratic boulders in the Scandinavian and Baltic regions. In 1795, the Scottish philosopher and gentleman naturalist, James Hutton (1726–1797), explained erratic boulders in the Alps with the action of glaciers. Two decades later, in 1818, the Swedish botanist Göran Wahlenberg (1780–1851) published his theory of a glaciation of the Scandinavian peninsula. He regarded glaciation as a regional phenomenon. Only a few years later, the Danish-Norwegian Geologist Jens Esmark (1762–1839) argued a sequence of worldwide ice ages. In a paper published in 1824, Esmark proposed changes in climate as the cause of those glaciations. He attempted to show that they originated from changes in Earth's orbit. During the following years, Esmark’s ideas were discussed and taken over in parts by Swedish, Scottish and German scientists. At the University of Edinburgh Robert Jameson (1774–1854) seemed to be relatively open to Esmark's ideas, as reviewed by Norwegian professor of glaciology Bjørn G. Andersen (1992). Jameson's remarks about ancient glaciers in Scotland were most probably prompted by Esmark. In Germany, Albrecht Reinhard Bernhardi (1797–1849), a geologist and professor of forestry at an academy in Dreissigacker, since incorporated in the southern Thuringian city of Meiningen, adopted Esmark's theory. In a paper published in 1832, Bernhardi speculated about former polar ice caps reaching as far as the temperate zones of the globe.
In 1829, independently of these debates, the Swiss civil engineer Ignaz Venetz (1788–1859) explained the dispersal of erratic boulders in the Alps, the nearby Jura Mountains and the North German Plain as being due to huge glaciers. When he read his paper before the Schweizerische Naturforschende Gesellschaft, most scientists remained sceptical. Finally, Venetz managed to convince his friend Jean de Charpentier. De Charpentier transformed Venetz's idea into a theory with a glaciation limited to the Alps. His thoughts resembled Wahlenberg's theory. In fact, both men shared the same volcanistic, or in de Charpentier’s case rather plutonistic assumptions, about the earth's history. In 1834, de Charpentier presented his paper before the Schweizerische Naturforschende Gesellschaft. In the meantime, the German botanist Karl Friedrich Schimper (1803–1867) was studying mosses which were growing on erratic boulders in the alpine upland of Bavaria. He began to wonder where such masses of stone had come from. During the summer of 1835 he made some excursions to the Bavarian Alps. Schimper came to the conclusion that ice must have been the means of transport for the boulders in the alpine upland. In the winter of 1835 to 1836 he held some lectures in Munich. Schimper then assumed that there must have been global times of obliteration ("Verödungszeiten") with a cold climate and frozen water. Schimper spent the summer months of 1836 at Devens, near Bex, in the Swiss Alps with his former university friend Louis Agassiz (1801–1873) and Jean de Charpentier. Schimper, de Charpentier and possibly Venetz convinced Agassiz that there had been a time of glaciation. During Winter 1836/7 Agassiz and Schimper developed the theory of a sequence of glaciations. They mainly drew upon the preceding works of Venetz, de Charpentier and on their own fieldwork. There are indications that Agassiz was already familiar with Bernhardi's paper at that time. At the beginning of 1837 Schimper coined the term ice age ("Eiszeit"). In July 1837 Agassiz presented their synthesis before the annual meeting of the Schweizerische Naturforschende Gesellschaft at Neuchâtel. The audience was very critical or even opposed the new theory because it contradicted the established opinions on climatic history. Most contemporary scientists thought that the earth had been gradually cooling down since its birth as a molten globe.
In order to overcome this rejection, Agassiz embarked on geological fieldwork. He published his book "Study on glaciers" ("Études sur les glaciers") in 1840. De Charpentier was put out by this as he had also been preparing a book about the glaciation of the Alps. De Charpentier felt that Agassiz should have given him precedence as it was he who had introduced Agassiz to in-depth glacial research. Besides that, Agassiz had, as a result of personal quarrels, omitted any mention of Schimper in his book.
All together, it took several decades until the ice age theory was fully accepted. This happened on an international scale in the second half of the 1870s following the work of James Croll including the publication of "Climate and Time, in Their Geological Relations" in 1875 which provided a credible explanation for the causes of ice ages.
Evidence for ice ages.
There are three main types of evidence for ice ages: geological, chemical, and paleontological.
Geological evidence for ice ages comes in various forms, including rock scouring and scratching, glacial moraines, drumlins, valley cutting, and the deposition of till or tillites and glacial erratics. Successive glaciations tend to distort and erase the geological evidence, making it difficult to interpret. Furthermore, this evidence was difficult to date exactly; early theories assumed that the glacials were short compared to the long interglacials. The advent of sediment and ice cores revealed the true situation: glacials are long, interglacials short. It took some time for the current theory to be worked out.
The chemical evidence mainly consists of variations in the ratios of isotopes in fossils present in sediments and sedimentary rocks and ocean sediment cores. For the most recent glacial periods ice cores provide climate proxies from their ice, and atmospheric samples from included bubbles of air. Because water containing heavier isotopes has a higher heat of evaporation, its proportion decreases with colder conditions. This allows a temperature record to be constructed. However, this evidence can be confounded by other factors recorded by isotope ratios.
The paleontological evidence consists of changes in the geographical distribution of fossils. During a glacial period cold-adapted organisms spread into lower latitudes, and organisms that prefer warmer conditions become extinct or are squeezed into lower latitudes. This evidence is also difficult to interpret because it requires (1) sequences of sediments covering a long period of time, over a wide range of latitudes and which are easily correlated; (2) ancient organisms which survive for several million years without change and whose temperature preferences are easily diagnosed; and (3) the finding of the relevant fossils.
Despite the difficulties, analysis of ice core and ocean sediment cores has shown periods of glacials and interglacials over the past few million years. These also confirm the linkage between ice ages and continental crust phenomena such as glacial moraines, drumlins, and glacial erratics. Hence the continental crust phenomena are accepted as good evidence of earlier ice ages when they are found in layers created much earlier than the time range for which ice cores and ocean sediment cores are available.
Major ice ages.
There have been at least five major ice ages in the earth's past (the Huronian, Cryogenian, Andean-Saharan, Karoo Ice Age and the Quaternary glaciation). Outside these ages, the Earth seems to have been ice-free even in high latitudes.
Rocks from the earliest well established ice age, called the Huronian, formed around 2.4 to 2.1 Ga (billion years) ago during the early Proterozoic Eon. Several hundreds of km of the Huronian Supergroup are exposed 10–100 km north of the north shore of Lake Huron extending from near Sault Ste. Marie to Sudbury, northeast of Lake Huron, with giant layers of now-lithified till beds, dropstones, varves, outwash, and scoured basement rocks. Correlative Huronian deposits have been found near Marquette, Michigan, and correlation has been made with Paleoproterozoic glacial deposits from Western Australia.
The next well-documented ice age, and probably the most severe of the last billion years, occurred from 850 to 630 million years ago (the Cryogenian period) and may have produced a Snowball Earth in which glacial ice sheets reached the equator, possibly being ended by the accumulation of greenhouse gases such as CO2 produced by volcanoes. "The presence of ice on the continents and pack ice on the oceans would inhibit both silicate weathering and photosynthesis, which are the two major sinks for CO2 at present." It has been suggested that the end of this ice age was responsible for the subsequent Ediacaran and Cambrian Explosion, though this model is recent and controversial.
The Andean-Saharan occurred from 460 to 420 million years ago, during the Late Ordovician and the Silurian period.
The evolution of land plants at the onset of the Devonian period caused a long term increase in planetary oxygen levels and reduction of CO2 levels, which resulted in the Karoo Ice Age. It is named after the glacial tills found in the Karoo region of South Africa, where evidence for this ice age was first clearly identified. There were extensive polar ice caps at intervals from 360 to 260 million years ago in South Africa during the Carboniferous and early Permian Periods. Correlatives are known from Argentina, also in the center of the ancient supercontinent Gondwanaland.
The current ice age, the Pliocene-Quaternary glaciation, started about 2.58 million years ago during the late Pliocene, when the spread of ice sheets in the Northern Hemisphere began. Since then, the world has seen cycles of glaciation with ice sheets advancing and retreating on 40,000- and 100,000-year time scales called glacial periods, glacials or glacial advances, and interglacial periods, interglacials or glacial retreats. The earth is currently in an interglacial, and the last glacial period ended about 10,000 years ago. All that remains of the continental ice sheets are the Greenland and Antarctic ice sheets and smaller glaciers such as on Baffin Island.
Ice ages can be further divided by location and time; for example, the names "Riss" (180,000–130,000 years bp) and "Würm" (70,000–10,000 years bp) refer specifically to glaciation in the Alpine region. The maximum extent of the ice is not maintained for the full interval. The scouring action of each glaciation tends to remove most of the evidence of prior ice sheets almost completely, except in regions where the later sheet does not achieve full coverage.
Glacials and interglacials.
Within the ice ages (or at least within the current one), more temperate and more severe periods occur. The colder periods are called "glacial periods", the warmer periods "interglacials", such as the Eemian Stage.
Glacials are characterized by cooler and drier climates over most of the earth and large land and sea ice masses extending outward from the poles. Mountain glaciers in otherwise unglaciated areas extend to lower elevations due to a lower snow line. Sea levels drop due to the removal of large volumes of water above sea level in the icecaps. There is evidence that ocean circulation patterns are disrupted by glaciations. Since the earth has significant continental glaciation in the Arctic and Antarctic, we are currently in a glacial minimum of a glaciation. Such a period between glacial maxima is known as an "interglacial". The glacials and interglacials also coincided with changes in Earth’s orbit called Milankovitch cycles.
The earth has been in an interglacial period known as the Holocene for more than 11,000 years. It was conventional wisdom that the typical interglacial period lasts about 12,000 years, but this has been called into question recently. For example, an article in "Nature" argues that the current interglacial might be most analogous to a previous interglacial that lasted 28,000 years. Predicted changes in orbital forcing suggest that the next glacial period would begin at least 50,000 years from now, even in absence of human-made global warming (see Milankovitch cycles). Moreover, anthropogenic forcing from increased greenhouse gases might outweigh orbital forcing for as long as intensive use of fossil fuels continues.
Positive and negative feedback in glacial periods.
Each glacial period is subject to positive feedback which makes it more severe and negative feedback which mitigates and (in all cases so far) eventually ends it.
Positive feedback processes.
Ice and snow increase Earth's albedo, i.e. they make it reflect more of the sun's energy and absorb less. Hence, when the air temperature decreases, ice and snow fields grow, and this continues until competition with a negative feedback mechanism forces the system to an equilibrium. Also, the reduction in forests caused by the ice's expansion increases albedo.
Another theory proposed by Ewing and Donn in 1956 hypothesized that an ice-free Arctic Ocean leads to increased snowfall at high latitudes. When low-temperature ice covers the Arctic Ocean there is little evaporation or sublimation and the polar regions are quite dry in terms of precipitation, comparable to the amount found in mid-latitude deserts. This low precipitation allows high-latitude snowfalls to melt during the summer. An ice-free Arctic Ocean absorbs solar radiation during the long summer days, and evaporates more water into the Arctic atmosphere. With higher precipitation, portions of this snow may not melt during the summer and so glacial ice can form at lower altitudes "and" more southerly latitudes, reducing the temperatures over land by increased albedo as noted above. Furthermore, under this hypothesis the lack of oceanic pack ice allows increased exchange of waters between the Arctic and the North Atlantic Oceans, warming the Arctic and cooling the North Atlantic. (Current projected consequences of global warming include a largely ice-free Arctic Ocean within 5–20 years, see Arctic shrinkage.) Additional fresh water flowing into the North Atlantic during a warming cycle may also reduce the global ocean water circulation (see "Shutdown of thermohaline circulation"). Such a reduction (by reducing the effects of the Gulf Stream) would have a cooling effect on northern Europe, which in turn would lead to increased low-latitude snow retention during the summer. It has also been suggested that during an extensive glacial, glaciers may move through the Gulf of Saint Lawrence, extending into the North Atlantic ocean far enough to block the Gulf Stream.
Negative feedback processes.
Ice sheets that form during glaciations cause erosion of the land beneath them. After some time, this will reduce land above sea level and thus diminish the amount of space on which ice sheets can form. This mitigates the albedo feedback, as does the lowering in sea level that accompanies the formation of ice sheets.
Another factor is the increased aridity occurring with glacial maxima, which reduces the precipitation available to maintain glaciation. The glacial retreat induced by this or any other process can be amplified by similar inverse positive feedbacks as for glacial advances.
According to research published in "Nature Geoscience", human emissions of carbon dioxide will defer the next ice age. Researchers used data on Earth's orbit to find the historical warm interglacial period that looks most like the current one and from this have predicted that the next ice age would usually begin within 1,500 years. They go on to say that emissions have been so high that it will not.
Causes of ice ages.
The causes of ice ages are not fully understood for either the large-scale ice age periods or the smaller ebb and flow of glacial–interglacial periods within an ice age. The consensus is that several factors are important: atmospheric composition, such as the concentrations of carbon dioxide and methane (the specific levels of the previously mentioned gases are now able to be seen with the new ice core samples from EPICA Dome C in Antarctica over the past 800,000 years ); changes in the earth's orbit around the Sun known as Milankovitch cycles; the motion of tectonic plates resulting in changes in the relative location and amount of continental and oceanic crust on the earth's surface, which affect wind and ocean currents; variations in solar output; the orbital dynamics of the Earth-Moon system; and the impact of relatively large meteorites, and volcanism including eruptions of supervolcanoes.
Some of these factors influence each other. For example, changes in Earth's atmospheric composition (especially the concentrations of greenhouse gases) may alter the climate, while climate change itself can change the atmospheric composition (for example by changing the rate at which weathering removes CO2).
Maureen Raymo, William Ruddiman and others propose that the Tibetan and Colorado Plateaus are immense CO2 "scrubbers" with a capacity to remove enough CO2 from the global atmosphere to be a significant causal factor of the 40 million year Cenozoic Cooling trend. They further claim that approximately half of their uplift (and CO2 "scrubbing" capacity) occurred in the past 10 million years.
Changes in Earth's atmosphere.
There is considerable evidence that over the very recent period of the last 100–1000 years, the sharp increases in human activity, especially the burning of fossil fuels, has caused the parallel sharp and accelerating increase in atmospheric greenhouse gases which trap the sun's heat. The consensus theory of the scientific community is that the resulting greenhouse effect is a principal cause of the increase in global warming which has occurred over the same period, and a chief contributor to the accelerated melting of the remaining glaciers and polar ice. A 2012 investigation finds that dinosaurs released methane through digestion in a similar amount to humanity's current methane release, which "could have been a key factor" to the very warm climate 150 million years ago.
There is evidence that greenhouse gas levels fell at the start of ice ages and rose during the retreat of the ice sheets, but it is difficult to establish cause and effect (see the notes above on the role of weathering). Greenhouse gas levels may also have been affected by other factors which have been proposed as causes of ice ages, such as the movement of continents and volcanism.
The Snowball Earth hypothesis maintains that the severe freezing in the late Proterozoic was ended by an increase in CO2 levels in the atmosphere, and some supporters of Snowball Earth argue that it was caused by a reduction in atmospheric CO2. The hypothesis also warns of future Snowball Earths.
In 2009, further evidence was provided that changes in solar insolation provide the initial trigger for the earth to warm after an Ice Age, with secondary factors like increases in greenhouse gases accounting for the magnitude of the change.
William Ruddiman has proposed the early anthropocene hypothesis, according to which the anthropocene era, as some people call the most recent period in the earth's history when the activities of the human species first began to have a significant global impact on the earth's climate and ecosystems, did not begin in the 18th century with the advent of the Industrial Era, but dates back to 8,000 years ago, due to intense farming activities of our early agrarian ancestors. It was at that time that atmospheric greenhouse gas concentrations stopped following the periodic pattern of the Milankovitch cycles. In his overdue-glaciation hypothesis Ruddiman states that an incipient glacial would probably have begun several thousand years ago, but the arrival of that scheduled glacial was forestalled by the activities of early farmers.
At a meeting of the American Geophysical Union (December 17, 2008), scientists detailed evidence in support of the controversial idea that the introduction of large-scale rice agriculture in Asia, coupled with extensive deforestation in Europe began to alter world climate by pumping significant amounts of greenhouse gases into the atmosphere over the last 1,000 years. In turn, a warmer atmosphere heated the oceans making them much less efficient storehouses of carbon dioxide and reinforcing global warming, possibly forestalling the onset of a new glacial age.
Position of the continents.
The geological record appears to show that ice ages start when the continents are in positions which block or reduce the flow of warm water from the equator to the poles and thus allow ice sheets to form. The ice sheets increase Earth's reflectivity and thus reduce the absorption of solar radiation. With less radiation absorbed the atmosphere cools; the cooling allows the ice sheets to grow, which further increases reflectivity in a positive feedback loop. The ice age continues until the reduction in weathering causes an increase in the greenhouse effect.
There are three known configurations of the continents which block or reduce the flow of warm water from the equator to the poles:
Since today's Earth has a continent over the South Pole and an almost land-locked ocean over the North Pole, geologists believe that Earth will continue to experience glacial periods in the geologically near future.
Some scientists believe that the Himalayas are a major factor in the current ice age, because these mountains have increased Earth's total rainfall and therefore the rate at which carbon dioxide is washed out of the atmosphere, decreasing the greenhouse effect. The Himalayas' formation started about 70 million years ago when the Indo-Australian Plate collided with the Eurasian Plate, and the Himalayas are still rising by about 5 mm per year because the Indo-Australian plate is still moving at 67 mm/year. The history of the Himalayas broadly fits the long-term decrease in Earth's average temperature since the mid-Eocene, 40 million years ago.
Fluctuations in ocean currents.
Another important contribution to ancient climate regimes is the variation of ocean currents, which are modified by continent position, sea levels and salinity, as well as other factors. They have the ability to cool (e.g. aiding the creation of Antarctic ice) and the ability to warm (e.g. giving the British Isles a temperate as opposed to a boreal climate). The closing of the Isthmus of Panama about 3 million years ago may have ushered in the present period of strong glaciation over North America by ending the exchange of water between the tropical Atlantic and Pacific Oceans.
Analyses suggest that ocean current fluctuations can adequately account for recent glacial oscillations. During the last glacial period the sea-level has fluctuated 20–30 m as water was sequestered, primarily in the northern hemisphere ice sheets. When ice collected and the sea level dropped sufficiently, flow through the Bering Strait (the narrow strait between Siberia and Alaska is ~50 m deep today) was reduced, resulting in increased flow from the North Atlantic. This realigned the thermohaline circulation in the Atlantic, increasing heat transport into the Arctic, which melted the polar ice accumulation and reduced other continental ice sheets. The release of water raised sea levels again, restoring the ingress of colder water from the Pacific with an accompanying shift to northern hemisphere ice accumulation.
Uplift of the Tibetan plateau and surrounding mountain areas above the snowline.
Matthias Kuhle's geological theory of Ice Age development was suggested by the existence of an ice sheet covering the Tibetan plateau during the Ice Ages (Last Glacial Maximum?). According to Kuhle, the plate-tectonic uplift of Tibet past the snow-line has led to a surface of c. 2,400,000 square kilometres (930,000 sq mi) changing from bare land to ice with a 70% greater albedo. The reflection of energy into space resulted in a global cooling, triggering the Pleistocene Ice Age. Because this highland is at a subtropical latitude, with 4 to 5 times the insolation of high-latitude areas, what would be Earth's strongest heating surface has turned into a cooling surface.
Kuhle explains the interglacial periods by the 100,000-year cycle of radiation changes due to variations in Earth's orbit. This comparatively insignificant warming, when combined with the lowering of the Nordic inland ice areas and Tibet due to the weight of the superimposed ice-load, has led to the repeated complete thawing of the inland ice areas.
Variations in Earth's orbit (Milankovitch cycles).
The Milankovitch cycles are a set of cyclic variations in characteristics of the Earth's orbit around the Sun. Each cycle has a different length, so at some times their effects reinforce each other and at other times they (partially) cancel each other.
There is strong evidence that the Milankovitch cycles affect the occurrence of glacial and interglacial periods within an ice age. The present ice age is the most studied and best understood, particularly the last 400,000 years, since this is the period covered by ice cores that record atmospheric composition and proxies for temperature and ice volume. Within this period, the match of glacial/interglacial frequencies to the Milanković orbital forcing periods is so close that orbital forcing is generally accepted. The combined effects of the changing distance to the Sun, the precession of the Earth's axis, and the changing tilt of the Earth's axis redistribute the sunlight received by the Earth. Of particular importance are changes in the tilt of the Earth's axis, which affect the intensity of seasons. For example, the amount of solar influx in July at 65 degrees north latitude varies by as much as 22% (from 450 W/m² to 550 W/m²). It is widely believed that ice sheets advance when summers become too cool to melt all of the accumulated snowfall from the previous winter. Some believe that the strength of the orbital forcing is too small to trigger glaciations, but feedback mechanisms like CO2 may explain this mismatch.
While Milankovitch forcing predicts that cyclic changes in the Earth's orbital elements can be expressed in the glaciation record, additional explanations are necessary to explain which cycles are observed to be most important in the timing of glacial–interglacial periods. In particular, during the last 800,000 years, the dominant period of glacial–interglacial oscillation has been 100,000 years, which corresponds to changes in Earth's orbital eccentricity and orbital inclination. Yet this is by far the weakest of the three frequencies predicted by Milankovitch. During the period 3.0–0.8 million years ago, the dominant pattern of glaciation corresponded to the 41,000-year period of changes in Earth's obliquity (tilt of the axis). The reasons for dominance of one frequency versus another are poorly understood and an active area of current research, but the answer probably relates to some form of resonance in the Earth's climate system.
The "traditional" Milankovitch explanation struggles to explain the dominance of the 100,000-year cycle over the last 8 cycles. Richard A. Muller, Gordon J. F. MacDonald, and others have pointed out that those calculations are for a two-dimensional orbit of Earth but the three-dimensional orbit also has a 100,000-year cycle of orbital inclination. They proposed that these variations in orbital inclination lead to variations in insolation, as the Earth moves in and out of known dust bands in the solar system. Although this is a different mechanism to the traditional view, the "predicted" periods over the last 400,000 years are nearly the same. The Muller and MacDonald theory, in turn, has been challenged by Jose Antonio Rial.
Another worker, William Ruddiman, has suggested a model that explains the 100,000-year cycle by the modulating effect of eccentricity (weak 100,000-year cycle) on precession (26,000-year cycle) combined with greenhouse gas feedbacks in the 41,000- and 26,000-year cycles. Yet another theory has been advanced by Peter Huybers who argued that the 41,000-year cycle has always been dominant, but that the Earth has entered a mode of climate behavior where only the second or third cycle triggers an ice age. This would imply that the 100,000-year periodicity is really an illusion created by averaging together cycles lasting 80,000 and 120,000 years. This theory is consistent with a simple empirical multi-state model proposed by Didier Paillard. Paillard suggests that the late Pleistocene glacial cycles
can be seen as jumps between three quasi-stable climate states. The jumps are induced by the orbital forcing, while in the early Pleistocene the 41,000-year glacial cycles resulted from jumps between only two climate states. A dynamical
model explaining this behavior was proposed by Peter Ditlevsen. This is in support of the suggestion that the late Pleistocene glacial cycles are not due to the weak 100,000-year eccentricity cycle, but a non-linear response to mainly the 41,000-year obliquity cycle.
Variations in the Sun's energy output.
There are at least two types of variation in the Sun's energy output
The long-term increase in the Sun's output cannot be a cause of ice ages.
Volcanism.
Volcanic eruptions may have contributed to the inception and/or the end of ice age periods. At times during the paleoclimate, carbon dioxide levels were two or three times greater than today. Volcanoes and movements in continental plates contributed to high amounts of CO2 in the atmosphere. Carbon dioxide from volcanoes probably contributed to periods with highest overall temperatures. One suggested explanation of the Paleocene-Eocene Thermal Maximum is that undersea volcanoes released methane from clathrates and thus caused a large and rapid increase in the greenhouse effect. There appears to be no geological evidence for such eruptions at the right time, but this does not prove they did not happen.
Recent glacial and interglacial phases.
Glacial stages in North America.
The major glacial stages of the current ice age in North America are the Illinoian, Sangamonian and Wisconsin stages. The use of the Nebraskan, Afton, Kansan, and Yarmouthian (Yarmouth) stages to subdivide the ice age in North America have been discontinued by Quaternary geologists and geomorphologists. These stages have all been merged into the Pre-Illinoian Stage in the 1980s.
During the most recent North American glaciation, during the latter part of the Wisconsin Stage (26,000 to 13,300 years ago), ice sheets extended to about 45 degrees north latitude. These sheets were 3 to 4 km thick.
This Wisconsin glaciation left widespread impacts on the North American landscape. The Great Lakes and the Finger Lakes were carved by ice deepening old valleys. Most of the lakes in Minnesota and Wisconsin were gouged out by glaciers and later filled with glacial meltwaters. The old Teays River drainage system was radically altered and largely reshaped into the Ohio River drainage system. Other rivers were dammed and diverted to new channels, such as the Niagara, which formed a dramatic waterfall and gorge, when the waterflow encountered a limestone escarpment. Another similar waterfall, at the present Clark Reservation State Park near Syracuse, New York, is now dry.
The area from Long Island to Nantucket was formed from glacial till, and the plethora of lakes on the Canadian Shield in northern Canada can be almost entirely attributed to the action of the ice. As the ice retreated and the rock dust dried, winds carried the material hundreds of miles, forming beds of loess many dozens of feet thick in the Missouri Valley. Isostatic rebound continues to reshape the Great Lakes and other areas formerly under the weight of the ice sheets.
The Driftless Zone, a portion of western and southwestern Wisconsin along with parts of adjacent Minnesota, Iowa, and Illinois, was not covered by glaciers.
Last Glacial Period in the semiarid Andes around Aconcagua and Tupungato.
A specially interesting climatic change during glacial times has taken place in the semi-arid Andes. Beside the expected cooling down in comparison with the current climate, a significant precipitation is concerned here. So, researches in the presently semiarid subtropic Aconcagua-massif (6,962 m) have shown an unexpectedly extensive glacial glaciation of the type "ice stream network". The connected valley glaciers exceeding 100 km in length, flowed down on the East-side of this section of the Andes at 32–34°S and 69–71°W as far as a height of 2,060 m and on the western luff-side still clearly deeper. Where current glaciers scarcely reach 10 km in length, the snowline (ELA) runs at a height of 4,600 m and at that time was lowered to 3,200 m asl, i.e. about 1,400 m. From this follows that—beside of an annual depression of temperature about c. 8.4 °C— here was an increase in precipitation. Accordingly, at glacial times the humid climatic belt that today is situated several latitude degrees further to the S, was shifted much further to the N.
Effects of glaciation.
Although the last glacial period ended more than 8,000 years ago, its effects can still be felt today. For example, the moving ice carved out the landscape in Canada (See Canadian Arctic Archipelago), Greenland, northern Eurasia and Antarctica. The erratic boulders, till, drumlins, eskers, fjords, kettle lakes, moraines, cirques, horns, etc., are typical features left behind by the glaciers.
The weight of the ice sheets was so great that they deformed the Earth's crust and mantle. After the ice sheets melted, the ice-covered land rebounded. Due to the high viscosity of the Earth's mantle, the flow of mantle rocks which controls the rebound process is very slow—at a rate of about 1 cm/year near the center of rebound area today.
During glaciation, water was taken from the oceans to form the ice at high latitudes, thus global sea level dropped by about 110 meters, exposing the continental shelves and forming land-bridges between land-masses for animals to migrate. During deglaciation, the melted ice-water returned to the oceans, causing sea level to rise. This process can cause sudden shifts in coastlines and hydration systems resulting in newly submerged lands, emerging lands, collapsed ice dams resulting in salination of lakes, new ice dams creating vast areas of freshwater, and a general alteration in regional weather patterns on a large but temporary scale. It can even cause temporary reglaciation. This type of chaotic pattern of rapidly changing land, ice, saltwater and freshwater has been proposed as the likely model for the Baltic and Scandinavian regions, as well as much of central North America at the end of the last glacial maximum, with the present-day coastlines only being achieved in the last few millennia of prehistory. Also, the effect of elevation on Scandinavia submerged a vast continental plain that had existed under much of what is now the North Sea, connecting the British Isles to Continental Europe.
The redistribution of ice-water on the surface of the Earth and the flow of mantle rocks causes changes in the gravitational field as well as changes to the distribution of the moment of inertia of the Earth. These changes to the moment of inertia result in a change in the angular velocity, axis, and wobble of the Earth's rotation.
The weight of the redistributed surface mass loaded the lithosphere, caused it to flex and also induced stress within the Earth. The presence of the glaciers generally suppressed the movement of faults below. However, during deglaciation, the faults experience accelerated slip triggering earthquakes. Earthquakes triggered near the ice margin may in turn accelerate ice calving and may account for the Heinrich events. As more ice is removed near the ice margin, more intraplate earthquakes are induced and this positive feedback may explain the fast collapse of ice sheets.
In Europe, glacial erosion and isostatic sinking from weight of ice made the Baltic Sea, which before the Ice Age was all land drained by the Eridanos River.

</doc>
<doc id="15362" url="http://en.wikipedia.org/wiki?curid=15362" title="Irving Langmuir">
Irving Langmuir

Irving Langmuir (31 January 1881 – 16 August 1957) was an American chemist and physicist. His most noted publication was the famous 1919 article "The Arrangement of Electrons in Atoms and Molecules" in which, building on Gilbert N. Lewis's cubical atom theory and Walther Kossel's chemical bonding theory, he outlined his "concentric theory of atomic structure". Langmuir became embroiled in a priority dispute with Lewis over this work; Langmuir's presentation skills were largely responsible for the popularization of the theory, although the credit for the theory itself belongs mostly to Lewis. While at General Electric from 1909–1950, Langmuir advanced several basic fields of physics and chemistry, invented the gas-filled incandescent lamp, the hydrogen welding technique, and was awarded the 1932 Nobel Prize in Chemistry for his work in surface chemistry. The Langmuir Laboratory for Atmospheric Research near Socorro, New Mexico, was named in his honor as was the American Chemical Society journal for Surface Science, called Langmuir.
Biography.
Early years.
Irving Langmuir was born in Brooklyn, New York, on 31 January 1881. He was the third of the four children of Charles Langmuir and Sadie, née Comings. During his childhood, Langmuir's parents encouraged him to carefully observe nature and to keep a detailed record of his various observations. When Irving was eleven, it was discovered that he had poor eyesight. When this problem was corrected, details that had previously eluded him were revealed, and his interest in the complications of nature was heightened.
During his childhood, Langmuir was influenced by his older brother, Arthur Langmuir. Arthur was a research chemist who encouraged Irving to be curious about nature and how things work. Arthur helped Irving set up his first chemistry lab in the corner of his bedroom, and he was content to answer the myriad of questions that Irving would pose. Langmuir's hobbies included mountaineering, skiing, piloting his own plane, and classical music. In addition to his professional interest in the politics of atomic energy, he was concerned about wilderness conservation.
Education.
Langmuir attended his early education at various schools and institutes in America and Paris (1892–1895). Langmuir graduated high school from Chestnut Hill Academy (1898), an elite private school located in the affluent Chestnut Hill area in Philadelphia. He graduated with a Bachelor of Science degree in metallurgical engineering (Met.E.) from the Columbia University School of Mines (the first mining and metallurgy school in the U.S., established,1864 and presently known as Fu Foundation School of Engineering and Applied Science) in 1903. He earned his Ph.D. degree in 1906 under Nobel laureate Walther Nernst in Göttingen, for research done using the "Nernst glower", an electric lamp invented by Nernst. His doctoral thesis was entitled “On the Partial Recombination of Dissolved Gases During Cooling.” He later did postgraduate work in chemistry. Langmuir then taught at Stevens Institute of Technology in Hoboken, New Jersey, until 1909, when he began working at the General Electric research laboratory (Schenectady, New York).
Research.
His initial contributions to science came from his study of light bulbs (a continuation of his Ph.D. work). His first major development was the improvement of the diffusion pump, which ultimately led to the invention of the high-vacuum rectifier and amplifier tubes. A year later, he and colleague Lewi Tonks discovered that the lifetime of a tungsten filament could be greatly lengthened by filling the bulb with an inert gas, such as argon, the critical factor (overlooked by other researchers) being the need for extreme cleanliness in all stages of the process. He also discovered that twisting the filament into a tight coil improved its efficiency. These were important developments in the history of the incandescent light bulb. His work in surface chemistry began at this point, when he discovered that molecular hydrogen introduced into a tungsten-filament bulb dissociated into atomic hydrogen and formed a layer one atom thick on the surface of the bulb.
His assistant in vacuum tube research was his cousin William Comings White.
As he continued to study filaments in vacuum and different gas environments, he began to study the emission of charged particles from hot filaments (thermionic emission). He was one of the first scientists to work with plasmas and was the first to call these ionized gases by that name, because they reminded him of blood plasma. Langmuir and Tonks discovered electron density waves in plasmas that are now known as Langmuir waves.
He introduced the concept of electron temperature and in 1924 invented the diagnostic method for measuring both temperature and density with an electrostatic probe, now called a Langmuir probe and commonly used in plasma physics. The current of a biased probe tip is measured as a function of bias voltage to determine the local plasma temperature and density. He also discovered atomic hydrogen, which he put to use by inventing the atomic hydrogen welding process; the first plasma weld ever made. Plasma welding has since been developed into gas tungsten arc welding.
In 1917, he published a paper on the chemistry of oil films that later became the basis for the award of the 1932 Nobel Prize in chemistry. Langmuir theorized that oils consisting of an aliphatic chain with a hydrophilic end group (perhaps an alcohol or acid) were oriented as a film one-molecule thick upon the surface of water, with the hydrophilic group down in the water and the hydrophobic chains clumped together on the surface. The thickness of the film could be easily determined from the known volume and area of the oil, which allowed investigation of the molecular configuration before spectroscopic techniques were available.
Later years.
Following World War I Langmuir contributed to atomic theory and the understanding of atomic structure by defining the modern concept of valence shells and isotopes.
Langmuir was president of the Institute of Radio Engineers in 1923.
Based on his work at General Electric, John B. Taylor developed a detector ionizing beams of alkali metals, called nowadays the Langmuir-Taylor detector.
He joined Katharine B. Blodgett to study thin films and surface adsorption. They introduced the concept of a monolayer (a layer of material one molecule thick) and the two-dimensional physics which describe such a surface. In 1932 he received the Nobel Prize in Chemistry "for his discoveries and investigations in surface chemistry."
In 1938, Langmuir's scientific interests began to turn to atmospheric science and meteorology. One of his first ventures, although tangentially related, was a refutation of the claim of entomologist Charles H. T. Townsend that the deer botfly flew at speeds in excess of 800 miles per hour. Langmuir estimated the fly's true speed at 25 miles per hour.
After observing windrows of drifting seaweed in the Sargasso Sea he discovered a wind-driven surface circulation in the sea. It is now called the Langmuir circulation.
During World War II, Langmuir worked on improving naval sonar for submarine detection, and later to develop protective smoke screens and methods for deicing aircraft wings. This research led him to theorize that the introduction of dry ice and iodide into a sufficiently moist cloud of low temperature could induce precipitation (cloud seeding); though in frequent practice, particularly in Australia and the People's Republic of China, the efficiency of this technique remains controversial today.
In 1953 Langmuir coined the term "pathological science", describing research conducted with accordance to the scientific method, but tainted by unconscious bias or subjective effects. This is in contrast to pseudoscience, which has no pretense of following the scientific method. In his original speech, he presented ESP and flying saucers as examples of pathological science; since then, the label has been applied to polywater and cold fusion.
His house in Schenectady, was designated a National Historic Landmark in 1976.
Personal life.
Langmuir was married to Marion Mersereau (1883-1971) in 1912 with whom he adopted two children: Kenneth and Barbara. After a short illness, he died in Woods Hole, Massachusetts from a heart attack in 1957. His obituary ran on the front page of "The New York Times".
On his religious views, Langmuir was an agnostic.
In fiction.
According to author Kurt Vonnegut, Langmuir was the inspiration for his fictional scientist Dr. Felix Hoenikker in the novel "Cat's Cradle". The character's invention of ice-nine eventually destroyed the world. Langmuir had worked with Vonnegut's brother, Bernard Vonnegut.

</doc>
<doc id="15365" url="http://en.wikipedia.org/wiki?curid=15365" title="International Association of Travel Agents Network">
International Association of Travel Agents Network

International Airlines Travel Agent Network (IATAN) is an industry association in the USA designed to represent the interests of its member companies.
In addition, it (along with the IATA) is the body responsible for the standard international codes for airlines, airports, hotels, cities and car rental firms (for example, the three-digit codes that designate London Heathrow Airport as LHR). These codes provide a method to link international travel network with international suppliers.

</doc>
<doc id="15368" url="http://en.wikipedia.org/wiki?curid=15368" title="Insider trading">
Insider trading

Insider trading is the trading of a public company's stock or other securities (such as bonds or stock options) by individuals with access to nonpublic information about the company. In various countries, trading based on insider information is illegal. This is because it is seen as unfair to other investors who do not have access to the information as the investor with insider information could potentially make far larger profits that a typical investor could not make.
The authors of one study claim that illegal insider trading raises the cost of capital for securities issuers, thus decreasing overall economic growth. However, some economists have argued that insider trading should be allowed and could, in fact, benefit markets.
Trading by specific insiders, such as employees, is commonly permitted as long as it does not rely on material information not in the public domain. However, most jurisdictions require such trading be reported so that these can be monitored. In the United States and several other jurisdictions, trading conducted by corporate officers, key employees, directors, or significant shareholders must be reported to the regulator or publicly disclosed, usually within a few business days of the trade.
The rules around insider trading are complex and vary significantly from country to country and enforcement is mixed. The definition of insider can be broad and may not only cover insiders themselves but also any person related to them, such as brokers, associates and even family members. Any person who becomes aware of non-public information and trades on that basis may be guilty.
Illegal insider trading.
Rules against insider trading on material non-public information exist in most jurisdictions around the world (Bhattacharya and Daouk, 2002), but the details and the efforts to enforce them vary considerably. In the United States, Sections 16(b) and 10(b) of the Securities Exchange Act of 1934 directly and indirectly address insider trading. Congress enacted this act after the stock market crash of 1929. The United States is generally viewed as having the strictest laws against illegal insider trading, and makes the most serious efforts to enforce them. In the United Kingdom, the Financial Services and Markets Act, 2000 gives the UK's Financial Conduct Authority the responsibility to investigate and prosecute insider dealing, defined by The Criminal Justice Act 1993.
Definition of "insider".
In the United States,Canada Australia and Germany, for mandatory reporting purposes, corporate insiders are defined as a company's officers, directors and any beneficial owners of more than 10% of a class of the company's equity securities. Trades made by these types of insiders in the company's own stock, based on material non-public information, are considered fraudulent since the insiders are violating the fiduciary duty that they owe to the shareholders. The corporate insider, simply by accepting employment, has undertaken a legal obligation to the shareholders to put the shareholders' interests before their own, in matters related to the corporation. When the insider buys or sells based upon company owned information, he is violating his obligation to the shareholders.
For example, illegal insider trading would occur if the chief executive officer of Company A learned (prior to a public announcement) that Company A will be taken over and then bought shares in Company A while knowing that the share price would likely rise.
In the United States and many other jurisdictions, however, "insiders" are not just limited to corporate officials and major shareholders where illegal insider trading is concerned but can include any individual who trades shares based on material non-public information in violation of some duty of trust. This duty may be imputed; for example, in many jurisdictions, in cases of where a corporate insider "tips" a friend about non-public information likely to have an effect on the company's share price, the duty the corporate insider owes the company is now imputed to the friend and the friend violates a duty to the company if the corporate insider trades on the basis of this information.
Liability for insider trading.
Liability for inside trading violations cannot be avoided by passing on the information in an "I scratch your back; you scratch mine" or quid pro quo arrangement as long as the person receiving the information knew or should have known that the information was material non-public information. When allegations of a potential inside deal occur, all parties that may have been involved are at risk of being found guilty.
For example, if Company A's CEO did not trade on the undisclosed takeover news, but instead passed the information on to his brother-in-law who traded on it, illegal insider trading would still have occurred (albeit by proxy by passing it on to a "non-insider" so Company A's CEO wouldn't get his hands dirty).:589
Misappropriation theory.
A newer view of insider trading, the misappropriation theory, is now accepted in U.S. law. It states that anyone who misappropriates information from their employer and trades on that information in any stock (either the employer's stock or the company's competitor stocks) is guilty of insider trading.
For example, if a journalist who worked for Company B learned about the takeover of Company A while performing his work duties and bought stock in Company A, illegal insider trading might still have occurred. Even though the journalist did not violate a fiduciary duty to Company A's shareholders, he might have violated a fiduciary duty to Company B's shareholders (assuming the newspaper had a policy of not allowing reporters to trade on stories they were covering).:586–7
Proof of responsibility.
Proving that someone has been responsible for a trade can be difficult because traders may try to hide behind nominees, offshore companies, and other proxies. Nevertheless, the Securities and Exchange Commission prosecutes over 50 cases each year, with many being settled administratively out of court. The SEC and several stock exchanges actively monitor trading, looking for suspicious activity. The SEC does not have criminal enforcement authority, but can refer serious matters to the U.S. Attorney's Office for further investigation and prosecution.
Trading on information in general.
Not all trading on information is illegal insider trading, however. For example, a person in a restaurant who hears the CEO of Company A at the next table tell the CFO that the company's profits will be higher than expected and then buys the stock is not guilty of insider trading—unless they had some closer connection to the company or company officers. However, information about a tender offer (usually regarding a merger or acquisition) is held to a higher standard. If this type of information is obtained (directly or indirectly) and there is reason to believe it is nonpublic, there is a duty to disclose it or abstain from trading. The punishment for insider trading depends on a few different factors. There are three main factors, which can be identified. 
Tracking insider trades.
Since insiders are required to report their trades, others often track these traders, and there is a school of investing which follows the lead of insiders. This is, of course, subject to the risk that an insider is making a buy specifically to increase investor confidence or making a sell for reasons unrelated to the health of the company (such as a desire to diversify or pay a personal expense).
Legal insider trading.
Legal trades by insiders are common, as employees of publicly traded corporations often have stock or stock options. These trades are made public in the United States through Securities and Exchange Commission filings, mainly Form 4. From January 2014 to March 2015, there were 124803 such transactions worth $5.23 billion.
SEC Rule 10b5-1 clarified that the prohibition against insider trading does not require proof that an insider actually used material nonpublic information when conducting a trade; possession of such information alone is sufficient to violate the provision, and the SEC would infer that an insider in possession of material nonpublic information used this information when conducting a trade. However, SEC Rule 10b5-1 also created for insiders an affirmative defense if the insider can demonstrate that the trades conducted on behalf of the insider were conducted as part of a pre-existing contract or written binding plan for trading in the future.
For example, if an insider expects to retire after a specific period of time and, as part of retirement planning, the insider has adopted a written binding plan to sell a specific amount of the company's stock every month for two years and later comes into possession of material nonpublic information about the company, trades based on the original plan might not constitute prohibited insider trading.
American insider trading law.
The United States has been the leading country in prohibiting insider trading made on the basis of material non-public information. Thomas Newkirk and Melissa Robertson of the U.S. Securities and Exchange Commission (SEC) summarize the development of US insider trading laws. Insider trading has a base offense level of 8, which puts it in Zone A under the U.S. Sentencing Guidelines. This means that first-time offenders are eligible to receive probation rather than incarceration.
Statutory law.
US insider trading prohibitions are based on English and American common law prohibitions against fraud. In 1909, well before the Securities Exchange Act was passed, the United States Supreme Court ruled that a corporate director, who bought that company's stock when he knew it was about to jump up in price, committed fraud by buying but not disclosing his inside information.
Section 15 of the Securities Act of 1933 contained prohibitions of fraud in the sale of securities which were greatly strengthened by the Securities Exchange Act of 1934.
Section 16(b) of the Securities Exchange Act of 1934 prohibits short-swing profits (from any purchases and sales within any six-month period) made by corporate directors, officers, or stockholders owning more than 10% of a firm's shares. Under Section 10(b) of the 1934 Act, SEC Rule 10b-5, prohibits fraud related to securities trading.
The Insider Trading Sanctions Act of 1984 and the Insider Trading and Securities Fraud Enforcement Act of 1988 place penalties for illegal insider trading as high as three times profit gained or loss avoided from the illegal trading.
SEC regulations.
SEC regulation FD ("Fair Disclosure") requires that if a company intentionally discloses material non-public information to one person, it must simultaneously disclose that information to the public at large. In the case of an unintentional disclosure of material non-public information to one person, the company must make a public disclosure "promptly.":586
Insider trading, or similar practices, are also regulated by the SEC under its rules on takeovers and tender offers under the Williams Act.
Court decisions.
Much of the development of insider trading law has resulted from court decisions.
In "SEC v. Texas Gulf Sulphur Co.", a federal circuit court stated that anyone in possession of inside information must either disclose the information or refrain from trading. Officers of the Texas Gulf Sulphur Corporation had used inside information about the discovery of the Kidd Mine to make profits by buying shares and call options on company stock.
In 1909, the Supreme Court of the United States ruled in "Strong v. Repide" that a director who expects to act in a way that affects the value of shares cannot use that knowledge to acquire shares from those who do not know of the expected action. Even though in general, ordinary relations between directors and shareholders in a business corporation are not of such a fiduciary nature as to make it the duty of a director to disclose to a shareholder the general knowledge which he may possess regarding the value of the shares of the company before he purchases any from a shareholder, yet there are cases where, by reason of the special facts, such duty exists.
In 1984, the Supreme Court of the United States ruled in the case of "Dirks v. SEC" that tippees (receivers of second-hand information) are liable if they had reason to believe that the tipper had breached a fiduciary duty in disclosing confidential information and the tipper received any personal benefit from the disclosure. (Since Dirks disclosed the information to expose a fraud, rather than for personal gain, nobody was liable for insider trading violations in his case.)
The Dirks case also defined the concept of "constructive insiders," who are lawyers, investment bankers and others who receive confidential information from a corporation while providing services to the corporation. Constructive insiders are also liable for insider trading violations if the corporation expects the information to remain confidential, since they acquire the fiduciary duties of the true insider.
The next expansion of insider trading liability came in "SEC vs. Materia"
745 F.2d 197 (2d Cir. 1984), the case which first introduced the misappropriation theory of liability for insider trading. Materia, a financial printing firm proofreader, and clearly not an insider by any definition, was found to have determined the identity of takeover targets based on proofreading tender offer documents during his employment. After a two week trial, the district court found him liable for insider trading, and the Second Circuit Court of Appeals affirmed holding that the theft of information from an employer, and the use of that information to purchase or sell securities in another entity, constituted a fraud in connection with the purchase or sale of a securities. The misappropriation theory of insider trading was born, and liability further expanded to encompass a larger group of outsiders.
In "United States v. Carpenter" (1986) the US Supreme Court cited an earlier ruling while unanimously upholding mail and wire fraud convictions for a defendant who received his information from a journalist rather than from the company itself. The journalist R. Foster Winans was also convicted, on the grounds that he had misappropriated information belonging to his employer, the Wall Street Journal. In that widely publicized case, Winans traded in advance of "Heard on the Street" columns appearing in the Journal.
The court ruled in "Carpenter": "It is well established, as a general proposition, that a person who acquires special knowledge or information by virtue of a confidential or fiduciary relationship with another is not free to exploit that knowledge or information for his own personal benefit but must account to his principal for any profits derived therefrom."
However, in upholding the securities fraud (insider trading) convictions, the justices were evenly split.
In 1997, the U.S. Supreme Court adopted the misappropriation theory of insider trading in "United States v. O'Hagan", 521 U.S. 642, 655 (1997). O'Hagan was a partner in a law firm representing Grand Metropolitan, while it was considering a tender offer for Pillsbury Company. O'Hagan used this inside information by buying call options on Pillsbury stock, resulting in profits of over $4.3 million. O'Hagan claimed that neither he nor his firm owed a fiduciary duty to Pillsbury, so he did not commit fraud by purchasing Pillsbury options.
The Court rejected O'Hagan's arguments and upheld his conviction.
The "misappropriation theory" holds that a person commits fraud "in connection with" a securities transaction and thereby violates 10(b) and Rule 10b-5, when he misappropriates confidential information for securities trading purposes, in breach of a duty owed to the source of the information. Under this theory, a fiduciary's undisclosed, self-serving use of a principal's information to purchase or sell securities, in breach of a duty of loyalty and confidentiality, defrauds the principal of the exclusive use of the information. In lieu of premising liability on a fiduciary relationship between company insider and purchaser or seller of the company's stock, the misappropriation theory premises liability on a fiduciary-turned-trader's deception of those who entrusted him with access to confidential information.
The Court specifically recognized that a corporation's information is its property:
"A company's confidential information... qualifies as property to which the company has a right of exclusive use. The undisclosed misappropriation of such information in violation of a fiduciary duty...constitutes fraud akin to embezzlement – the fraudulent appropriation to one's own use of the money or goods entrusted to one's care by another."
In 2000, the SEC enacted SEC Rule 10b5-1, which defined trading "on the basis of" inside information as any time a person trades while aware of material nonpublic information. It is no longer a defense for one to say that one would have made the trade anyway. The rule also created an affirmative defense for pre-planned trades.
Insider trading by members of Congress.
Members of the US Congress are not exempt from the laws that ban insider trading, but as they generally do not have a confidential relationship with the source of the information they receive, they do not meet the usual definition of an "insider."
House of Representatives rules may however consider congressional insider trading unethical. A 2004 study found that stock sales and purchases by Senators outperformed the market by 12.3% per year. Peter Schweizer points out several examples of insider trading by members of Congress, including action taken by Spencer Bachus following a private, behind-the-doors meeting on the evening of September 18, 2008 when Hank Paulson and Ben Bernanke informed members of Congress about the imminent financial crisis, Bachus then shorted stocks the next morning and cashed in his profits within a week. Also attending the same meeting were Senator Dick Durbin and John Boehner; the same day (trade effective the next day), Durbin sold mutual-fund shares worth $42,696, and reinvested it all with Warren Buffett. Also the same day (trade effective the next day), Congressman Boehner cashed out of an equity mutual fund.
Insider trading with Congress-sourced information.
In 2014, federal prosecutors issued a subpoena to the House Ways and Means committee and Brian Sutter, staff director of its health-care sub-committee, relative to a price move in stocks just prior to the passage of a law favorable to the companies involved. An e-mail was sent out by a "Washington-based policy-research firm that predicted the change [in the law] for its Wall Street clients. That alert, in turn, was based in part on information provided to the firm by a former congressional health-care aide turned lobbyist, according to emails reviewed by the "[Wall Street] Journal" in 2013.
Security analysis and insider trading.
Security analysts gather and compile information, talk to corporate officers and other insiders, and issue recommendations to traders. Thus their activities may easily cross legal lines if they are not especially careful. The CFA Institute in its code of ethics states that analysts should make every effort to make all reports available to all the broker's clients on a timely basis. Analysts should never report material nonpublic information, except in an effort to make that information available to the general public. Nevertheless, analysts' reports may contain a variety of information that is "pieced together" without violating insider trading laws, under the Mosaic theory. This information may include non-material nonpublic information as well as material public information, which may increase in value when properly compiled and documented.
In May 2007, a bill entitled the "Stop Trading on Congressional Knowledge Act, or STOCK Act" was introduced that would hold congressional and federal employees liable for stock trades they made using information they gained through their jobs and also regulate analysts or "Political Intelligence" firms that research government activities. The 2012 STOCK Act was passed on April 4, 2012.
Arguments for legalizing insider trading.
Some economists and legal scholars (such as Henry Manne, Milton Friedman, Thomas Sowell, Daniel Fischel, and Frank H. Easterbrook) have argued that laws against insider trading should be repealed. They claim that insider trading based on material nonpublic information benefits investors, in general, by more quickly introducing new information into the market.
Friedman, laureate of the Nobel Memorial Prize in Economics, said: "You want more insider trading, not less. You want to give the people most likely to have knowledge about deficiencies of the company an incentive to make the public aware of that." Friedman did not believe that the trader should be required to make his trade known to the public, because the buying or selling pressure itself is information for the market.:591–7
Other critics argue that insider trading is a victimless act: a willing buyer and a willing seller agree to trade property which the seller rightfully owns, with no prior contract (according to this view) having been made between the parties to refrain from trading if there is asymmetric information. The Atlantic has described the process as "arguably the closest thing that modern finance has to a victimless crime".
Legalization advocates also question why "trading" where one party has more information than the other is legal in other markets, such as real estate, but not in the stock market. For example, if a geologist knows there is a high likelihood of the discovery of petroleum under Farmer Smith's land, he may be entitled to make Smith an offer for the land, and buy it, without first telling Farmer Smith of the geological data. Nevertheless, circumstances can occur when the geologist would be committing fraud if, because he owes a duty to the farmer, he did not disclose the information; for example, if he had been hired by Farmer Smith to assess the geology of the farm.
Advocates of legalization make free speech arguments. Punishment for communicating about a development pertinent to the next day's stock price might seem an act of censorship. If the information being conveyed is proprietary information and the corporate insider has contracted to not expose it, he has no more right to communicate it than he would to tell others about the company's confidential new product designs, formulas, or bank account passwords.
Some authors have used these arguments to propose legalizing insider trading on negative information (but not on positive information). Since negative information is often withheld from the market, trading on such information has a higher value for the market than trading on positive information.
There are very limited laws against "insider trading" in the commodities markets if, for no other reason than that the concept of an "insider" is not immediately analogous to commodities themselves (corn, wheat, steel, etc.). However, analogous activities such as front running are illegal under US commodity and futures trading laws. For example, a commodity broker can be charged with fraud by receiving a large purchase order from a client (one likely to affect the price of that commodity) and then purchasing that commodity before executing the client's order to benefit from the anticipated price increase.
Legal differences among jurisdictions.
The US and the UK vary in the way the law is interpreted and applied with regard to insider trading.
In the UK, the relevant laws are the Criminal Justice Act 1993, Part V, Schedule 1, and the Financial Services and Markets Act 2000, which defines an offence of Market Abuse. It is also illegal to fail to trade based on inside information (whereas without the inside information the trade would have taken place). The principle is that it is illegal to trade on the basis of market-sensitive information that is not generally known. No relationship to the issuer of the security is required; all that is required is that the guilty party traded (or caused trading) whilst having inside information.
Japan enacted its first law against insider trading in 1988. Roderick Seeman said, "Even today many Japanese do not understand why this is illegal. Indeed, previously it was regarded as common sense to make a profit from your knowledge."
In accordance with EU Directives, Malta enacted the Financial Markets Abuse Act in 2002, which effectively replaced the Insider Dealing and Market Abuse Act of 1994.
The "Objectives and Principles of Securities Regulation" published by the International Organization of Securities Commissions (IOSCO) in 1998 and updated in 2003 states that the three objectives of good securities market regulation are:
The discussion of these "Core Principles" state that "investor protection" in this context means "Investors should be protected from misleading, manipulative or fraudulent practices, including insider trading, front running or trading ahead of customers and the misuse of client assets." More than 85 percent of the world's securities and commodities market regulators are members of IOSCO and have signed on to these Core Principles.
The World Bank and International Monetary Fund now use the IOSCO Core Principles in reviewing the financial health of different country's regulatory systems as part of these organization's financial sector assessment program, so laws against insider trading based on non-public information are now expected by the international community. Enforcement of insider trading laws varies widely from country to country, but the vast majority of jurisdictions now outlaw the practice, at least in principle.
Larry Harris claims that differences in the effectiveness with which countries restrict insider trading help to explain the differences in executive compensation among those countries. The US, for example, has much higher CEO salaries than do Japan or Germany, where insider trading is less effectively restrained.:593
By nation.
European Union.
In 2014, the European Union (EU) adopted legislation (Criminal Sanctions for Market Abuse Directive) that harmonises criminal sanctions for insider dealing. All EU Member States agreed to introduce maximum prison sentences of at least four years for serious cases of market manipulation and insider dealing, and at least two years for improper disclosure of insider information.
Norway.
In 2009, a journalist in Nettavisen (Thomas Gulbrandsen) was sentenced to 4 months in prison for insider trading.
The longest prison sentence in a Norwegian trial where the main charge was insider trading, was for 8 years (2 of which suspended) when Alain Angelil was convicted in a district court on December 9, 2011.
United Kingdom.
Although insider trading in the UK has been illegal since 1980, it proved difficult to successfully prosecute individuals accused of insider trading. There were a number of notorious cases where individuals were able to escape prosecution. Instead the UK regulators relied on a series of fines to punish market abuses.
These fines were widely perceived as an ineffective deterrent (Cole, 2007), and there was a statement of intent by the UK regulator (the Financial Services Authority) to use its powers to enforce the legislation (specifically the Financial Services and Markets Act 2000). Between 2009–2012 the FSA secured 14 convictions in relation to insider dealing.
United States.
Rajat Gupta, who had been managing partner of McKinsey & Co. and a director at Goldman Sachs Group Inc. and Procter & Gamble Co., was convicted by a federal jury in 2012 of leaking inside information to hedge fund manager Raj Rajaratnam. The case was prosecuted by the office of United States Attorney for the Southern District of New York Preet Bharara.
With the guilty plea by Perkins Hixon in 2014 for insider trading from 2010-2013 while at Evercore Partners, Bharara said in a press release that 250 defendants whom his office had charged since August 2009 had now been convicted.
On December 10, 2014, a federal appeals court overturned the insider trading convictions of two former hedge fund traders, Todd Newman and Anthony Chiasson, based on the "erroneous" instructions given to jurors by the trial judge.
External links.
Listen to this article ()
This audio file was created from a revision of the "Insider trading" article dated 2012-03-19, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="15369" url="http://en.wikipedia.org/wiki?curid=15369" title="International Brigades">
International Brigades

The International Brigades (Spanish: "Brigadas Internacionales") were military units, made up of volunteers from different countries, who travelled to Spain, in order to fight for the Second Spanish Republic, in the Spanish Civil War, between 1936 and 1939.
The number of combatant volunteers has been estimated at between 32,000–35,000, though with no more than about 20,000 active at any one time. A further 10,000 people probably participated in non-combatant roles and about 3,000–5,000 foreigners were members of CNT or POUM. They came from a claimed "53 nations" to fight against the Spanish Falangist forces led by General Francisco Franco, who was assisted by German and Italian forces.
Formation and recruitment.
Using foreign Communist Parties to recruit volunteers for Spain was first proposed in the Soviet Union in September 1936—apparently at the suggestion of Maurice Thorez—by Willi Münzenberg, chief of Comintern propaganda for Western Europe. As a security measure, non-Communist volunteers would first be interviewed by an NKVD agent.
By the end of September, the Italian and French Communist Parties had decided to set up a column. Luigi Longo, ex-leader of the Italian Communist Youth, was charged to make the necessary arrangements with the Spanish government. The Soviet Ministry of Defense also helped, since they had experience of dealing with corps of international volunteers during the Russian Civil War. The idea was initially opposed by Largo Caballero, but after the first setbacks of the war, he changed his mind, and finally agreed to the operation on 22 October. However, the Soviet Union did not withdraw from the Non-Intervention Committee, probably to avoid diplomatic conflict with France and the United Kingdom.
The main recruitment centre was in Paris, under the supervision of Soviet colonel Karol "Walter" Świerczewski. On 17 October 1936, an open letter by Joseph Stalin to José Díaz was published in "Mundo Obrero", arguing that victory for the Spanish second republic was a matter not only for Spaniards, but also for the whole of "progressive humanity"; in a matter of days, support organisations for the Spanish Republic were founded in most countries, all more or less controlled by the Comintern.
Entry to Spain was arranged for volunteers: for instance, a Yugoslav (ethnic Croat), Josip Broz, who would become famous as Marshal Josip Broz Tito, was in Paris to provide assistance, money and passports for volunteers from Eastern Europe. Volunteers were sent by train or ship from France to Spain, and sent to the base at Albacete. However, many of them also went by themselves to Spain. The volunteers were under no contract, nor defined engagement period, which would later prove a problem.
Also many Italians, Germans, and people from other countries joined the movement, with the idea that combat in Spain was a first step to restore democracy or advance a revolutionary cause in their own country. There were also many unemployed workers (especially from France), and adventurers. Finally, some 500 communists who had been exiled to Russia were sent to Spain (among them, experienced military leaders from the First World War like "Kléber" Stern, "Gomez" Zaisser, "Lukacs" Zalka and "Gal" Galicz, who would prove invaluable in combat).
The operation was met by communists with enthusiasm, but by anarchists with skepticism, at best. At first, the anarchists who controlled the borders with France were told to refuse communist volunteers, and reluctantly allowed their passage after protests. A group of 500 volunteers (mainly French, with a few exiled Poles and Germans) arrived in Albacete on 14 October 1936. They were met by international volunteers who had already been fighting in Spain: Germans from the Thälmann Battalion, Italians from Centuria Gastone Sozzi and French from Commune de Paris Battalion. Among them was British poet John Cornford. Men were sorted according to their experience and origin, and dispatched to units.
Albacete soon became the International Brigades headquarters and its main depot. It was run by a "troika" of Comintern heavyweights: André Marty was commander; Luigi Longo ("Gallo") was Inspector-General; and Giuseppe Di Vittorio ("Nicoletti") was chief political commissar.
The French Communist Party provided uniforms for the Brigades. They were organized into mixed brigades, the basic military unit of the Republican People's Army. Discipline was extreme. For several weeks, the Brigades were locked in their base while their strict military training was under way.
Service.
First engagements: Siege of Madrid.
The Battle of Madrid was a major success for the Republic. It staved off the prospect of a rapid defeat at the hands of Francisco Franco's forces. The role of the International Brigades in this victory was generally recognised, but was exaggerated by Comintern propaganda, so that the outside world heard only of their victories, and not those of Spanish units. So successful was such propaganda that the British Ambassador, Sir Henry Chilton, declared that there were no Spaniards in the army which had defended Madrid. The International Brigade forces that fought in Madrid arrived after other successful Republican fighting. Of the 40,000 Republican troops in the city, the foreign troops numbered less than 3,000. Even though the International Brigades did not win the battle by themselves, nor significantly change the situation, they certainly did provide an example by their determined fighting, and improved the morale of the population by demonstrating the concern of other nations in the fight. Many of the older members of the International Brigades provided valuable combat experience, having fought during the First World War (Spain remained neutral in 1914–18) and the Irish War of Independence (Some fought in the IRA while others fought in the British army).
One of the strategic positions in Madrid was the Casa de Campo. There the Nationalist troops were Moroccans, commanded by General José Enrique Varela. They were stopped by III and IV Brigades of the Spanish Republican Army.
On 9 November 1936, the XI International Brigade - comprising 1,900 men from the Edgar André Battalion, the Commune de Paris Battalion and the Dabrowski Battalion, together with a British machine-gun company — took up position at the Casa de Campo. In the evening, its commander, General Kléber, launched an assault on the Nationalist positions. This lasted for the whole night and part of the next morning. At the end of the fight, the Nationalist troops had been forced to retreat, abandoning all hopes of a direct assault on Madrid by Casa de Campo, while the XIth Brigade had lost a third of its personnel.
On 13 November, the 1,550-man strong XII International Brigade, made up of the Thälmann Battalion, the Garibaldi Battalion and the André Marty Battalion, deployed. Commanded by General "Lukacs", they assaulted Nationalist positions on the high ground of Cerro de los Angeles. As a result of language and communication problems, command issues, lack of rest, poor coordination with armoured units, and insufficient artillery support, the attack failed.
On November 19, the anarchist militias were forced to retreat, and Nationalist troops — Moroccans and Spanish Foreign Legionnaires, covered by the Nazi Condor Legion — captured a foothold in the University City. The 11th Brigade was sent to drive the Nationalists out of the University City. The battle was extremely bloody, a mix of artillery and aerial bombardment, with bayonet and grenade fights, room by room. Anarchist leader Buenaventura Durruti was shot there on 19 November 1936, and died the next day. The battle in the University went on until three quarters of the University City was under Nationalist control. Both sides then started setting up trenches and fortifications. It was then clear that any assault from either side would be far too costly; the nationalist leaders had to renounce the idea of a direct assault on Madrid, and prepare for a siege of the capital.
On 13 December 1936, 18,000 nationalist troops attempted an attack to close the encirclement of Madrid at Guadarrama — an engagement known as the Battle of the Corunna Road. The Republicans sent in a Soviet armoured unit, under General Dmitry Pavlov, and both XI and XII International Brigades. Violent combat followed, and they stopped the Nationalist advance.
An attack was then launched by the Republic on the Córdoba front. The battle ended in a form of stalemate; a communique was issued, saying: "[t]oday, our advance ctontinued without loss of land". Poets Ralph Winston Fox and John Cornford were killed. Eventually, the Nationalists advanced, taking the hydroelectric station at El Campo. André Marty accused the commander of the Marseillaise Battalion, Gaston Delasalle, of espionage and treason and had him executed. (It is doubtful that Delasalle would have been a spy for Francisco Franco; he was denounced by his own second-in-command, André Heussler, who was subsequently executed for treason during World War II by the French Resistance.)
Further Nationalist attempts after Christmas to encircle Madrid met with failure, but not without extremely violent combat. On 6 January 1937, the Thälmann Battalion arrived at Las Rozas, and held its positions until it was destroyed as a fighting force. On January 9, only 10 km had been lost to the Nationalists, when the XIII International Brigade and XIV International Brigade and the 1st British Company, arrived in Madrid. Violent Republican assaults were launched in attempt to retake the land, with little success. On January 15, trenches and fortifications were built by both sides, resulting in a stalemate.
The Nationalists did not take Madrid until the very end of the war, in March 1939, when they marched in unopposed. There were some pockets of resistance during the consecutive months.
Battle of Jarama.
On 6 February 1937, following the fall of Málaga, the nationalists launched an attack on the Madrid–Andalusia road, south of Madrid. The Nationalists quickly advanced on the little town Ciempozuelos, held by the XV International Brigade, which was composed of the British Battalion (British Commonwealth and Irish), the Dimitrov Battalion (miscellaneous Balkan nationalities), the 6 Février Battalion (Belgians and French), the Canadian Mackenzie-Papineau Battalion and the Abraham Lincoln Brigade (Americans, including African-American).
An independent 80-men-strong (mainly) Irish unit, known as the Connolly Column, made up of people from both sides of the Irish border also fought. Several histories of the Irish in Spain record that they included an ex-Catholic Christian Brother and an ordained Church of Ireland (Anglican Protestant) Clergyman, fighting and dying on the same side. (These battalions were not composed entirely of one nationality or another, rather they were for the most part a mix of many)
On 11 February 1937, a Nationalist brigade launched a surprise attack on the André Marty Battalion (XIV International Brigade), stabbing its sentries and crossing the Jarama. The Garibaldi Battalion stopped the advance with heavy fire. At another point, the same tactic allowed the Nationalists to move their troops across the river.
On 12 February, the British Battalion, XV International Brigade took the brunt of the attack, remaining under heavy fire for seven hours. The position became known as "Suicide Hill". At the end of the day, only 225 of the 600 members of the British battalion remained. One company was captured by ruse, when Nationalists advanced among their ranks singing "The Internationale".
On 17 February, the Republican Army counter-attacked. On February 23 and 27, the International Brigades were engaged, but with little success. The Lincoln Battalion was put under great pressure, with no artillery support. It suffered 120 killed and 175 wounded. Amongst the dead was the Irish poet Charles Donnelly and Leo Greene.
There were heavy casualties on both sides, and although "both claimed victory ... both suffered defeats". It resulted in a stalemate, with both sides digging in, creating elaborate trench systems.
On 22 February 1937 the League of Nations Non-Intervention Committee ban on foreign volunteers went into effect.
Battle of Guadalajara.
After the failed assault on the Jarama, the Nationalists attempted another assault on Madrid, from the North-East this time. The objective was the town of Guadalajara, 50 km from Madrid. The whole Italian expeditionary corps — 35,000 men, with 80 battle tanks and 200 field artillery — was deployed, as Benito Mussolini wanted the victory to be credited to Italy. On 9 March 1937, the Italians made a breach in the Republican lines, but did not properly exploit the advance. However, the rest of the Nationalist army was advancing, and the situation appeared critical for the Republicans. A formation drawn from the best available units of the Republican army, including the XI and XII International Brigades, was quickly assembled.
At dawn on 10 March, the Nationalists closed in, and by noon, the Garibaldi Battalion counterattacked. Some confusion arose from the fact that the sides were not aware of each other's movements, and that both sides spoke Italian; this resulted in scouts from both sides exchanging information without realising they were enemies. The Republican lines advanced and made contact with XI International Brigade. Nationalist tanks were shot at and infantry patrols came into action.
On March 11, the Nationalist army broke the front of the Republican army. The Thälmann Battalion suffered heavy losses, but succeeded in holding the Trijueque–Torija road. The Garibaldi also held its positions. On March 12, Republican planes and tanks attacked. The Thälmann Battalion attacked Trijuete in a bayonet charge and re-took the town, capturing numerous prisoners.
The International Brigades also saw combat in the Battle of Teruel in January 1938. The 35th International Division suffered heavily in this battle from aerial bombardment as well as shortages of food, winter clothing and ammunition. The XIV International Brigade fought in the Battle of Ebro in July 1938, the last Republican offensive of the war.
Casualties.
Although exact figures are not available, an estimated 5,857 to 25,229 brigadiers died in Spain, of an estimated 23,670 to 59,380 who served, with estimated death rates of 16.7% to 29.2%. These high casualty rates are blamed on lack of training, poor leadership and use as shock troops.
Disbandment.
In October 1938, at the height of the Battle of the Ebro, the Non-Intervention Committee ordered the withdrawal of the International Brigades which were fighting on the Republican side. The Republican government of Juan Negrín announced the decision in the League of Nations on 21 September 1938. The disbandment was part of an ill-advised effort to get the Nationalists' foreign backers to withdraw their troops and to persuade the Western democracies such as France and Britain to end their arms embargo on the Republic.
By this time there were about an estimated 10,000 foreign volunteers still serving in Spain for the Republican side, and about 50,000 foreign conscripts for the Nationalists (excluding another 30,000 Moroccans). Perhaps half of the International Brigadists came from Nazi Germany, Fascist Italy or other countries, such as Hungary, which had authoritarian right-wing governments at the time. These men could not safely return home and some were instead given honorary Spanish citizenship and integrated into Spanish units of the Popular Army. The remainder were repatriated to their own countries. The Belgian and Dutch volunteers lost their citizenship because they had served in a foreign army.
Composition.
Overview.
The first brigades were composed mostly of French, Belgian, Italian, and German volunteers, backed by a sizeable contingent of Polish miners from Northern France and Belgium. The XIth, XIIth and XIIIth were the first brigades formed. Later, the XIVth and XVth Brigades were raised, mixing experienced soldiers with new volunteers. Smaller Brigades — the 86th, 129th and 150th - were formed in late 1937 and 1938, mostly for temporary tactical reasons.
About 32,000 people volunteered to defend the Spanish Republic. Many were veterans of World War I. Their early engagements in 1936 during the Siege of Madrid amply demonstrated their military and propaganda value.
The international volunteers were mainly socialists, communists, or under communist authority, and a high proportion were Jewish. Some were involved in the fighting in Barcelona against Republican opponents of the Communists: the Workers' Party of Marxist Unification (POUM) ("Partido Obrero de Unificación Marxista", an anti-Stalinist Marxist party) and anarchists. These more libertarian groups like the POUM fought together on the front with the anarchist federations of the CNT (CNT, Confederación Nacional del Trabajo) and the FAI (FAI, Iberian Anarchist Federation) who had large support in the area of Catalonia. However, overseas volunteers from anarchist, socialist, liberal and other political positions also served with the international brigades.
To simplify communication, the battalions usually concentrated people of the same nationality or language group. The battalions were often (formally, at least) named after inspirational people or events. From Spring 1937 onwards, many battalions contained one Spanish volunteer company (about 150 men).
Later in the war, military discipline tightened and learning Spanish became mandatory. By decree of 23 September 1937, the International Brigades formally became units of the Spanish Foreign Legion.<ref name="Beevor 2006/309"></ref> This made them subject to the Spanish Code of Military Justice. However the Spanish Foreign Legion itself sided with the Nationalists throughout the coup and the civil war. The same decree also specified that non-Spanish officers in the Brigades should not exceed Spanish ones by more than 50 per cent
MKVD created in 1937 ‘Control and Security Service’.
Status after the war.
Since the Civil War was eventually won by the Nationalists, the Brigadiers were initially on the "wrong side" of history, especially since most of their home countries had a right-wing government (in France, for instance, the Popular Front was not in power any more).
However, since most of these countries found themselves at war with the very powers which had been supporting the Nationalists, the Brigadists gained some prestige as the first guard of the democracies, having fought a prophetical combat. Retrospectively, it was clear that the war in Spain was as much a precursor of the Second World War as a Spanish civil war.
Some glory was therefore accredited to the volunteers (a great deal of the survivors also fought during World War II), but this soon faded in the fear that it would promote (by association) communism.
An exception is among groups to the left of the Communist Parties, for example anarchists. Among these groups the Brigades, or at least their leadership, are criticised for their alleged role in suppressing the Spanish Revolution. An example of a modern work which promotes this view is Ken Loach's film "Land and Freedom". A well-known contemporary account of the Spanish Civil War which also takes this view is George Orwell's book "Homage to Catalonia".
East Germany.
Germany was of course undivided until after the Second World War. At that time, the new German Democratic Republic found itself in need of a 'founding myth' going beyond the conquest of eastern Nazi Germany by the Red Army. The Spanish Civil War, and especially the role of the International Brigades, were considered ideal, and became a substantial part of East Germany's memorial rituals, because of the substantial numbers of German communists who had served in the brigades.
Canada.
Survivors of the Mackenzie-Papineau Battalion were often investigated by the Royal Canadian Mounted Police and denied employment when they returned to Canada. Some were prevented from serving in the military during the Second World War due to "political unreliability".
In 1995 a monument to veterans of the war was built near Ontario's provincial parliament.
On February 12, 2000, a bronze statue "The Spirit of the Republic" based on an original poster from the Spanish Republic, by sculptor Jack Harman, was placed on the grounds of the BC Legislature. And in 2001, the few remaining Canadian veterans of the Spanish Civil War dedicated a monument to Canadian members of the International Brigades in Ottawa's Green Park.
Switzerland.
In Switzerland, public sympathy was high for the Republican cause, but the federal government banned all fundraising and recruiting activities a month after the start of the war so as to preserve Swiss neutrality. Around 800 Swiss volunteers joined the International Brigades, among them a small number of women. Sixty percent of Swiss volunteers identified as communists, while the others included socialists, anarchists and antifascists.
Some 170 Swiss volunteers were killed in the war. The survivors were tried by military courts upon their return to Switzerland for violating the criminal prohibition on foreign military service. The courts pronounced 420 sentences which ranged from around two weeks to four years in prison, and often also stripped the convicts of their political rights. In the judgment of Swiss historian Mauro Cerutti, volunteers were punished more harshly in Switzerland than in any other democratic country.
Motions to pardon the Swiss brigadists on the account that they fought for a just cause have been repeatedly introduced in the Swiss federal parliament. A first such proposal was defeated in 1939 on neutrality grounds. In 2002, Parliament again rejected a pardon of the Swiss war volunteers, with a majority arguing that they did break a law that remains in effect to this day. In March 2009, Parliament adopted a third bill of pardon, retroactively rehabilitating Swiss brigadists, only a handful of whom were still alive.
United Kingdom.
On disbandment, 305 British volunteers left Spain. They arrived at Victoria Station on 7 December, to be met by a crowd of supporters including Clement Attlee, Stafford Cripps, Willie Gallacher, and Will Lawther.
United States.
In the United States, the returned volunteers were labeled "premature anti-fascists" by the FBI, denied promotion during service in the US military during World War II, and pursued by Congressional committees during the Red Scare of 1947-1957. However, threats of loss of citizenship were not carried out.
Recognition.
Spain.
On 26 January 1996, the Spanish government gave Spanish citizenship to the Brigadists. At the time, roughly 600 remained. At the end of 1938, Prime Minister Juan Negrín had promised Spanish citizenship to the Brigadists, which citizenship was of course not recognized by the Nationalists who were about to take over the entire country.
France.
In 1996, Jacques Chirac, then French President, granted the former French members of the International Brigades the legal status of former service personnel ("anciens combattants") following the request of two French communist Members of Parliament, Lefort and Asensi, both children of volunteers. Before 1996, the same request was turned down several times including by François Mitterrand, the former Socialist President.
Monuments.
There is a full list of British and Irish monuments on the .
Symbolism and heraldry.
The International Brigades were inheritors of a socialist aesthetic.
The flags featured the colours of the Spanish Republic: red, yellow and purple, often along with socialist symbols (red flags, hammer and sickle, fist). The emblem of the brigades themselves was the three-pointed red star, which is often featured.
Notable associated people.
Note: not all the following were International Brigade members. 

</doc>
<doc id="15373" url="http://en.wikipedia.org/wiki?curid=15373" title="Iron Duke">
Iron Duke

Iron Duke may refer to:
People:
Artistic works:
Ship names:
Locomotive names:
Miscellaneous:

</doc>
<doc id="15374" url="http://en.wikipedia.org/wiki?curid=15374" title="Food irradiation">
Food irradiation

Food irradiation is the process of exposing foodstuffs to a source of energy capable of stripping electrons from individual atoms (ionizing radiation). This treatment is used to preserve food, reduce the risk of food borne illness, prevent the spread of invasive pests, and delay or eliminate sprouting or ripening. The radiation can be emitted by a radioactive substance or generated electrically. Irradiated food does not become radioactive. Food irradiation is permitted by over 60 countries, with about 500,000 metric tons of foodstuffs annually processed worldwide. Irradiation is also used for non-food applications, such as medical devices.
Although there have been concerns about the safety of irradiated food, a large amount of independent research has confirmed it to be safe. One family of chemicals is uniquely formed by irradiation, and this product is nontoxic. When heating food, all other chemicals occur in a lower or comparable frequency. Others criticize irradiation because of confusion with radioactive contamination or because of negative impressions of the nuclear industry.
The regulations that dictate how food is to be irradiated, as well as the food allowed to be irradiated, vary greatly from country to country. In Austria, Germany, and many other countries of the European Union only dried herbs, spices, and seasonings can be processed with irradiation and only at a specific dose, while in Brazil all foods are allowed at any dose.
Uses.
Irradiation is used to reduce the pathogens in foods. Depending on the dose, some or all of the microorganisms, bacteria, and viruses present are destroyed, slowed down, or rendered incapable of reproduction. This reduces or eliminates the risk of food borne illnesses. Some foods are irradiated at sufficient doses to ensure that the product is sterilized and does not add any spoilage or pathogenic microorganisms into the final product.
Irradiation is used to delay the ripening of fruits and the sprouting of vegetables by slowing down the enzymatic action in foods. By halting or slowing down spoilage and slowing down the ripening of food, irradiation prolongs the shelf life of goods. Irradiation cannot revert spoiled or over ripened food to a fresh state. If this food was processed by irradiation, spoilage would cease and ripening would slow down, yet the irradiation would not destroy the toxins or repair the texture, color, or taste of the food.
Insect pests are sterilized using irradiation at relatively low doses of irradiation. This stops the spread of foreign invasive species across national boundaries, and allows foods to pass quickly through quarantine and avoid spoilage. Depending on the dose, some or all of the insects present are destroyed, or rendered incapable of reproduction.
Public perception and impact.
Irradiation has been approved by the FDA for over 50 years, but the only major growth area for the commercial sale of irradiated foods for human consumption is fruits and vegetables that are irradiated to kill insects for the purpose of quarantine. In the early 2000s in the US irradiated meat was common at some grocery stores, but because of lack of consumer demand it is no longer common. Because consumer demand for irradiated food is low, reducing the spoilage between manufacture and consumer purchase and reducing the risk of food borne illness is currently not sufficient incentive for most manufactures to supplement their process with irradiation.
It is widely believed that consumer perception of foods treated with irradiation is more negative than those processed by other means, although some industry studies indicate the number of consumers concerned about the safety of irradiated food has decreased in the last 10 years to levels comparable to those of people concerned about food additives and preservatives. “These irradiated foods are not less safe than others,” Dr. Tarantino said, “and the doses are effective in reducing the level of disease-causing micro-organisms.” "People think the product is radioactive," said Harlan Clemmons, president of Sadex, a food irradiation company based in Sioux City, Iowa.
Some common concerns about food irradiation include the impact of irradiation on food chemistry, as well as the indirect effects of irradiation becoming a prevalent in the food handling process. Irradiation reduces the risk of infection and spoilage, does not make food radioactive, and the food is shown to be safe, but it does cause chemical reactions that alter the food and therefore alters the chemical makeup, nutritional content, and the sensory qualities of the food. Some of the potential secondary impacts of irradiation are hypothetical, while others are demonstrated. These effects include impacts due to the reduction of food quality, the loss of bacteria, and the irradiation process. Because of these concerns and the increased cost of irradiated foods, there is not a widespread public demand for the irradiation of foods for human consumption.
Effect of irradiation on food chemistry.
The irradiation source supplies energetic particles or waves. As these waves/particles pass through a target material they collide with other particles. Around the sites of these collisions chemical bonds are broken, creating short lived radicals (e.g. the hydroxyl radical, the hydrogen atom and solvated electrons). These radicals cause further chemical changes by bonding with and or stripping particles from nearby molecules. When collisions damage DNA or RNA, effective reproduction becomes unlikely, also when collisions occur in cells, cell division is often suppressed.
Irradiated food does not become radioactive as the radioactive source is never in contact with the foodstuffs and energy of radiation is limited below the threshold of induction of radioactivity, but it does reduce the nutritional content and change the flavor (much like cooking), produce radiolytic products, and increase the number of free radicals in the food.
Irradiation causes a multitude of chemical changes including introducing radiolytic products and free radicals. A few of these products are unique, but not considered dangerous. The scale of these chemical changes is not unique. Cooking, smoking, salting, and other less novel techniques, cause the food to be altered so drastically that its original nature is almost unrecognizable, and must be called by a different name. Storage of food also causes dramatic chemical changes, ones that eventually lead to deterioration and spoilage.
Misconceptions.
A major concern is that irradiation might cause chemical changes that are harmful to the consumer. Several national expert groups and two international expert groups evaluated the available data and concluded that any food at any dose is wholesome and safe to consume as long as it remains palatable and maintains its technical properties (e.g. feel, texture, or color).
Irradiated food does not become radioactive, just as an object exposed to light does not start producing light. Radioactivity is the ability of a substance to emit high energy particles. When these particles hit the target materials they may free other highly energetic particles. This ends shortly after the end of the exposure, much like objects stop reflecting light when the source is turned off and warm objects emit heat until they cool down but do not continue to produce their own heat.
It is impossible for food irradiators to induce radiation into a product. Irradiators radiate non-alpha particles and radiation is intrinsically radiated at precisely known strengths (wavelengths). These radiated particles can never be strong enough to split the atoms found in food. Without alpha particles, radioactivity can only be induced if a radiated particle with sufficient strength hits another atom and that atom splits into two or more pieces . If this happens the resulting atom(s) may be radioactive. If the particle is not strong enough, it can never split an atom, no matter how many particles are emitted from the radioactive source. 
Only in rare materials, such as plutonium and uranium, is the energy released by splitting an atom strong enough to split other atoms, these materials are not found in foods in sufficient quantities, so there can be no chain reaction.
Food quality.
Because of the extent of the chemical reactions, changes to the foods quality after irradiation are inevitable. The nutritional content of food, as well as the sensory qualities (taste, appearance, and texture) is impacted by irradiation. Because of this food advocacy groups consider labeling irradiated food raw as misleading. However, the degradation of vitamins caused by irradiation is similar or even less than the loss caused by other food preservation processes. Other processes like chilling, freezing, drying, and heating also result in some vitamin loss.
The changes in quality and nutrition vary greatly from food to food. The changes in the flavor of fatty foods like meats, nuts and oils are sometimes noticeable, while the changes in lean products like fruits and vegetables are less so. Some studies by the irradiation industry show that for some properly treated fruits and vegetables irradiation is seen by consumers to improve the sensory qualities of the product compared to untreated fruits and vegetables.
Radiolytic products and free radicals.
The formation of new, previously unknown chemical compounds (unique radiolytic products) via irradiation is a concern. Most of the substances found in irradiated food are also found in food that has been subjected to other food processing treatments, and are therefore not unique. Furthermore, the quantities in which they occur in irradiated food are lower or similar to the quantities formed in heat treatments.
When fatty acids are irradiated, a family of compounds called 2-alkylcyclobutanones (2-ACBs) are produced. These are thought to be unique radiolytic products. Some studies show that these chemicals may be toxic, while others dispute this.
Potentially damaging compounds known as free radicals form when food is irradiated. Most of these are oxidizers (i.e., accept electrons) and some react very strongly. According to the free-radical theory of aging excessive amounts of these free radicals can lead to cell injury and cell death, which may contribute to many diseases. Though this traditional relates to the free radicals generated in the body, not the free radicals consumed by the individual, as much of these are destroyed in the digestive process.
The radiation doses to cause toxic changes are much higher than the doses needed to accomplish the benefits of irradiation, and taking into account the presence of 2-ABCs along with what is known of free radicals, these results lead to the conclusion that there is no significant risk from radiolytic products.
Side effects of irradiation.
The side effects of irradiation are the concerns and benefits of irradiation that are not directly related to the chemical changes that occur when food is irradiated, but instead are related to what would occur if food irradiation was a common process.
When food is irradiated some nutrition is lost. Therefore, if the majority of food was irradiated at high enough levels to decrease its nutritional content significantly, there could be an increase in nutritional deficiencies due to a diet composed entirely of irradiated foods. Furthermore for at least 3 studies on cats, the consumption of irradiated food was associated with a loss of tissue in the myelin sheath, leading to reversible paralysis. Researchers suspect that reduced levels of vitamin A and high levels of free radicals may be the cause. This effect is thought to be specific to cats and has not been reproduced in any other animal. To produce these effects the cats were fed solely on food that was irradiated at a dose at least five times higher than the maximum allowable dose.
If irradiation was to become common in the food handling process there would be a reduction of the prevalence of foodborne illness and potentially the eradication of specific pathogens. However, multiple studies suggest that an increased rate of pathogen growth may occur when irradiated food is cross-contaminated with a pathogen, as the competing spoilage organisms are no longer present.
The ability to remove bacterial contamination through post-processing by irradiation may reduce the fear of mishandling food which could cultivate a cavalier attitude toward hygiene and result in contaminants other than bacteria. However, concerns that the pasteurization of milk would lead to increased contamination of milk where prevalent when mandatory pasteurization was introduced, but these fears never materialized after adoption of this law. Therefore, it is unlikely for irradiation to cause an increase of illness due to non bacteria based contamination.
It may seem reasonable to assume that irradiating food might lead to radiation-tolerant strains, similar to the way that strains of bacteria have developed resistance to antibiotics. Bacteria develop a resistance to antibiotics after an individual uses antibiotics repeatedly. Much like pasteurization plants products that pass through irradiation plants are processed once, and are not processed and reprocessed. Cycles of heat treatment have been shown to produce heat tolerant bacteria, yet no problems have appeared so far in pasteurization plants. Furthermore, when the irradiation dose is chosen to target a specific species of microbe, it is calibrated to doses several times the value required to target the species. This ensures that the process randomly destroys all members of a target species. Therefore the more irradiation tolerant members of the target species are not given any evolutionary advantage. Without evolutionary advantage selection does not occur. As to the irradiation process directly producing mutations that lead to more virulent, radiation resistant, strains the European Commission's Scientific Committee on Food found that there is no evidence, on the contrary, irradiation has been found to cause loss of virulence and infectivity as mutants are usually less competitive and less adapted."
Misconceptions.
The argument is made that there is a lack of long-term studies, and therefore the safety of irradiated food is not scientifically proven in spite of the fact that hundreds of animal feeding studies of irradiated food, including multigenerational studies, have been performed since 1950. Endpoints investigated have included subchronic and chronic changes in metabolism, histopathology, and function of most systems; reproductive effects; growth; teratogenicity; and mutagenicity. A large number of studies have been performed; meta-studies have supported the safety of irradiated food.
The below experiments are cited by food irradiation opponents, but could be either not verified in later experiments, could not be clearly attributed to the radiation effect, or could be attributed to an inappropriate design of the experiment etc.
Treatment.
Up to the point where the food is processed by irradiation, the food is processed in the same way as all other food. To treat the foodstuffs, they are exposed to a radioactive source, for a set period of time to achieve a desired dose. Radiation may be emitted by a radioactive substance, or by X-ray and electron beam accelerators. Special precautions are taken to ensure the food stuffs never come in contact with the radioactive substances and that the personnel and the environment are protected from exposure radiation.
Irradiation treatments are typically classified by dose (high, medium, and low), but are sometimes classified by the effects of the treatment (radappertization, radicidation and radurization). Food irradiation is sometimes referred to as "cold pasteurization" or "electronic pasteurization" because ionizing the food does not heat the food to high temperatures during the process, and the effect is similar to heat pasteurization. The term "cold pasteurization" is controversial because the term may be used to disguise the fact the food has been irradiated and pasteurization and irradiation are fundamentally different processes.
Treatment costs vary as a function of dose and facility usage. A pallet or tote is typically exposed for several minutes to hours depending on dose. Low-dose applications such as disinfestation of fruit range between US$0.01/lbs and US$0.08/lbs while higher-dose applications can cost as much as US$0.20/lbs.
Process.
Typically, when the food is being irradiated, pallets of food are exposed a source of radiation for a specific time. Dosimeters are embedded in the pallet (at various locations) of food to determine what dose was achieved.
Most irradiated food is processed by gamma irradiation. Special precautions are taken because gamma rays are continuously emitted by the radioactive material. In most designs, to nullify the effects of radiation, the radioisotope is lowered into a water-filled storage pool, which absorbs the radiation but does not become radioactive. This allows pallets of the products to be added and removed from the irradiation chamber and other maintenance to be done. Sometimes movable shields are used to reduce radiation levels in areas of the irradiation chamber instead of submerging the source. For x ray and electron irradiation these precautions are not necessary as the source of the radiation can be turned off.
For x-ray, gamma ray and electron irradiation, shielding is required when the foodstuffs are being irradiated. This is done to protect workers and the environment outside of the chamber from radiation exposure. Typically permanent or movable shields are used. In some gamma irradiators the radioactive source is under water at all times, and the hermetically sealed product is lowered into the water. The water acts as the shield in this application. Because of the lower penetration depth of electron irradiation, treatment to entire industrial pallets or totes is not possible.
Dosimetry.
The radiation absorbed dose is the amount energy absorbed per unit weight of the target material. Dose is used because, when the same substance is given the same dose, similar changes are observed in the target material. The SI unit for dose is grays (Gy or J/kg). Dosimeters are used to measure dose, and are small components that, when exposed to ionizing radiation, change measurable physical attributes to a degree that can be correlated to the dose received. Measuring dose (dosimetry) involves exposing one or more dosimeters along with the target material.
For purposes of legislation doses are divided into low (up to 1 kGy), medium (1 kGy to 10 kGy), and high dose applications (above 10 kGy). High dose applications are above those currently permitted in the USA for commercial food items by the FDA and other regulators around the world. Though these doses are approved for non commercial applications, such as sterilizing frozen meat for NASA astronauts (doses of 44 kGy) and food for hospital patients.
Technology.
Electron irradiation uses electrons accelerated in an electric field to a velocity close to the speed of light. Electrons have a charge, and therefore do not penetrate the product beyond a few centimeters, depending on product density.
Gamma irradiation involves exposing the target material to packets of light (photons) that are highly energetic (Gamma rays). A radioactive material (radioisotopes) is used as the source for the gamma rays. Gamma irradiation is the standard because the deeper penetration of the gamma rays enables administering treatment to entire industrial pallets or totes (reducing the need for material handling) and it is significantly less expensive than using a X-ray source. Generally cobalt-60 is used as a radioactive source for gamma irradiation. Cobalt-60 is bred from cobalt-59 using neutron irradiation in specifically designed nuclear reactors. In limited applications caesium-137, a less costly alternative recovered during the processing of spent nuclear fuel, is used as a radioactive source. Insufficient quantities are available for large scale commercial use. An incident where water soluble caesium-137 leaked into the source storage pool requiring NRC intervention has led to near elimination of this radioisotope outside of military applications.
Irradiation by X-ray is similar to irradiation by gamma rays in that less energetic packets of light (X-rays) are used. X-rays are generated by colliding accelerated electrons with a dense material (this process is known as bremsstrahlung-conversion), and therefore do not necessitate the use of radioactive materials.
X-rays ability to penetrate the target is similar to gamma irradiation. X-ray machine produces better dose uniformity then Gamma irradiation but they require much more electricity as only as much as 12% of the input energy is converted into X-rays.
Cost.
The cost of food irradiation is influenced by dose requirements, the food's tolerance of radiation, handling conditions, i.e., packaging and stacking requirements, construction costs, financing arrangements, and other variables particular to the situation. Irradiation is a capital-intensive technology requiring a substantial initial investment, ranging from $1 million to $5 million. In the case of large research or contract irradiation facilities, major capital costs include a radiation source, hardware (irradiator, totes and conveyors, control systems, and other auxiliary equipment), land (1 to 1.5 acres), radiation shield, and warehouse. Operating costs include salaries (for fixed and variable labor), utilities, maintenance, taxes/insurance, cobalt-60 replenishment, general utilities, and miscellaneous operating costs.
Regulations and international standards.
The Codex Alimentarius represents the global standard for irradiation of food, in particular under the WTO-agreement. Member states are free to convert those standards into national regulations at their discretion, therefore regulations about irradiation differ from country to country.
The United Nations Food and Agricultural Organization (FAO) has passed a motion to commit member states to implement irradiation technology for their national phytosanitary programs; the General assembly of the International Atomic Energy Agency (IAEA) has urged wider use of the irradiation technology.
Labeling regulations and international standards.
The provisions of the Codex Alimentarius are that any "first generation" product must be labeled "irradiated" as any product derived directly from an irradiated raw material; for ingredients the provision is that even the last molecule of an irradiated ingredient must be listed with the ingredients even in cases where the unirradiated ingredient does not appear on the label. The RADURA-logo is optional; several countries use a graphical version that differs from the Codex-version. The suggested rules for labeling is published at CODEX-STAN – 1 (2005), and includes the usage of the Radura symbol for all products that contain irradiated foods. The Radura symbol is not a designator of quality. The amount of pathogens remaining is based upon dose and the original content and the dose applied can vary on a product by product basis.
The European Union follows the Codex's provision to label irradiated ingredients down to the last molecule of irradiated foodstuffs. The European Community does not provide for the use of the Radura logo and relies exclusively on labeling by the appropriate phrases in the respective languages of the Member States. The European Union enforces its irradiation labeling laws by requiring its member countries to perform tests on a cross section of food items in the market-place and to report to the European Commission. The results are published annually in the OJ of the European Communities.
The US defines irradiated foods as foods in which the irradiation causes a material change in the food, or a material change in the consequences that may result from the use of the food. Therefore food that is processed as an ingredient by a restaurant or food processor is exempt from the labeling requirement in the US. This definition is not consistent with the Codex Alimentarius. All irradiated foods must bear a slightly modified Radura symbol at the point of sale and use the term "irradiated" or a derivative there of, in conjunction with explicit language describing the change in the food or its conditions of use.
Food safety regulations and international standards.
In 2003, the Codex Alimentarius removed any upper dose limit for food irradiation as well as clearances for specific foods, declaring that all are safe to irradiate. Countries such as Pakistan and Brazil have adopted the Codex without any reservation or restriction. Other countries, including New Zealand, Australia, Thailand, India, and Mexico, have permitted the irradiation of fresh fruits for fruit fly quarantine purposes, amongst others.
Standards that describe calibration and operation for radiation dosimetry, as well as procedures to relate the measured dose to the effects achieved and to report and document such results, are maintained by the American Society for Testing and Materials (ASTM international) and are also available as ISO/ASTM standards.
All of the rules involved in processing foodstuffs are applied to all foods before they are irradiated.
United States clearances.
In the United States, each new food is approved separately with a guideline specifying a maximum dosage; in case of quarantine applications the minimum dose is regulated. Packaging materials containing the food processed by irradiation must also undergo approval. Food irradiation in the United States is primarily regulated by the FDA since it is considered a food additive. The United States Department of Agriculture (USDA) amends these rules for use with meat, poultry, and fresh fruit.
The United States Department of Agriculture (USDA) has approved the use of low-level irradiation as an alternative treatment to pesticides for fruits and vegetables that are considered hosts to a number of insect pests, including fruit flies and seed weevils. Under bilateral agreements that allows less-developed countries to earn income through food exports agreements are made to allow them to irradiate fruits and vegetables at low doses to kill insects, so that the food can avoid quarantine.
The U.S. Food and Drug Administration (FDA) and the USDA have approved irradiation of the following foods and purposes:
European Union clearances.
European law dictates that no foods other than dried aromatic herbs, spices and vegetable seasonings are permitted for the application of irradiation. However, any Member State is permitted to maintain previous clearances that are in categories that the EC's Scientific Committee on Food (SCF) had previously approved, or add clearance granted to other Member States. Presently, Belgium, Czech Republic, France, Italy, Netherlands, Poland, and the United Kingdom) have adopted such provisions. Before individual items in an approved class can be added to the approved list, studies into the toxicology of each of such food and for each of the proposed dose ranges are requested. It also states that irradiation shall not be used "as a substitute for hygiene or health practices or good manufacturing or agricultural practice". These regulations only govern food irradiation in consumer products to allow irradiation to be used for patients requiring sterile diets.
Because of the Single Market of the EC any food, even if irradiated, must be allowed to be marketed in any other Member State even if a general ban of food irradiation prevails, under the condition that the food has been irradiated legally in the state of origin.
Furthermore, imports into the EC are possible from third countries if the irradiation facility had been inspected and approved by the EC and the treatment is legal within the EC or some Member state.
Nuclear and employee safety regulations.
Interlocks and safeguards are mandated to minimize this risk. There have been radiation related accidents, deaths, and injury at such facilities, many of them caused by operators overriding the safety related interlocks. In a radiation processing facility, radiation specific concerns are supervised by special authorities, while "Ordinary" occupational safety regulations are handled much like other businesses.
The safety of irradiation facilities is regulated by the United Nations International Atomic Energy Agency and monitored by the different national Nuclear Regulatory Commissions. The regulators enforce a safety culture that mandates that all incidents that occur are documented and thoroughly analyzed to determine the cause and improvement potential. Such incidents are studied by personnel at multiple facilities, and improvements are mandated to retrofit existing facilities and future design.
In the US the Nuclear Regulatory Commission (NRC) regulates the safety of the processing facility, and the United States Department of Transportation (DOT) regulates the safe transport of the radioactive sources.
Irradiated food supply.
There are analytical methods available to detect the usage of irradiation on food items in the marketplace. This is used as a tool for government authorities to enforce existing labeling standards and to bolster consumer confidence. Phytosanitary irradiation of fruits and vegetables has been increasing globally. In 2010, 18446 tonnes of fruits and vegetables were irradiated in six countries for export quarantine control; the countries follow: Mexico (56.2%), United States (31.2%), Thailand (5.18%), Vietnam (4.63%), Australia (2.69%), and India (0.05%). The three types of fruits irradiated the most were guava (49.7%), sweet potato(29.3%) and sweet lime (3.27%).
In total, 103 000 tonnes of food products were irradiated on mainland United States in 2010. The three types of foods irradiated the most were spices (77.7%), fruits and vegetables (14.6%) and meat and poultry (7.77%). 17 953 tonnes of irradiated fruits and vegetables were exported to the mainland United States. Mexico, the United States' state of Hawaii, Thailand, Vietnam and India export irradiated produce to the mainland U.S. Mexico, followed by the United States' state of Hawaii, is the largest exporter of irradiated produce to the mainland U.S.
In total, 7 972 tonnes of food products were irradiated in European Union countries in 2012; mainly in three member state countries: Belgium (64.7%), the Netherlands (18.5%) and France (7.7%). The three types of foods irradiated the most were frog legs (36%), poultry (35%) and dried herbs and spices (15%). The European Union's official site gives information on the regulatory status of food irradiation, the quantities of foods irradiated at authorized facilities in European Union member states and the results of market surveillance where foods have been tested to see if they are irradiated. The Official Journal of the European Union publishes annual reports on food irradiation, the current report covers the period from 1 January 2012 to 31 December 2012 and compiles information from 27 member States.

</doc>
<doc id="15378" url="http://en.wikipedia.org/wiki?curid=15378" title="IUD with copper">
IUD with copper

A copper IUD (ATC G02) (also intrauterine device, IUD, copper-T, or coil) is a type of intrauterine device. The copper IUD is a type of long-acting reversible contraception and is one of the most effective forms of birth control.
It is also the most effective nonhormonal contraceptive device.
Medical uses.
Copper IUDs are a form of long-acting reversible contraception and are considered to be one of the most effective forms of birth control available. The type of frame and amount of copper can affect the effectiveness of different copper IUD models. The failure rates for different models vary between 0.1 and 2.2% after 1 year of use. The T-shaped models with a surface area of 380 mm² of copper have the lowest failure rates. The TCu 380A (ParaGard) has a one year failure rate of 0.8% and a cumulative 12-year failure rate of 2.2%. Over 12 years of use, the models with less surface area of copper have higher failure rates. The TCu 220A has a 12 year failure rate of 5.8%, whereas the TCu 380A has a 12 year failure rate of 2.2%. The frameless GyneFix also has a failure rate of less than 1% per year. Worldwide, older IUD models with lower effectiveness rates are no longer produced.
Unlike other forms of reversible contraception, the typical use failure rate and the perfect use failure rate for the copper IUDs are the same because the IUD does not depend on user action. A 2008 review of the available T-shaped copper IUDs recommended that the TCu 380A and the TCu 280S be used as the first choice for copper IUDs because those two models have the lowest failure rates and the longest lifespans. The effectiveness of the copper IUD is comparable to tubal sterilization, which has a first year failure rate of 0.5%. However, the effects of the copper IUD are reversible, which can be viewed as either an advantage or a disadvantage, depending on a woman's goals for contraception.
Emergency contraception.
It was first discovered in 1976 that the copper IUD could be used as a form of emergency contraception (EC).
The copper IUD is the most effective form of emergency contraception. It is more effective than the hormonal EC pills currently available.
The pregnancy rate among those using the copper IUD for EC is 0.09%. It can be used for EC up to 5 days after the act of unprotected sex and does not decrease in effectiveness during the 5 days. 
An additional advantage of using the copper IUD for emergency contraception is that it can be used as a form of birth control for 10–12 years after insertion.
Removal and return to fertility.
Removal of the copper IUD should also be performed by a qualified medical practitioner. Fertility has been shown to return to previous levels quickly after removal of the device. One study found that the median amount of time from removal to planned pregnancy was three months for those women using the TCu 380Ag.
Adverse effects.
Expulsion: Sometimes the copper IUD can be spontaneously expelled from the uterus. Expulsion rates can range from 2.2% to 11.4% of users from the first year to the 10th year. The TCu380A may have lower rates of expulsion than other models. Unusual vaginal discharge, cramping or pain, spotting between periods, postcoital (after sex) spotting, dyspareunia, or the absence or lengthening of the strings can be signs of a possible expulsion. If expulsion occurs, the woman is not protected against pregnancy. If an IUD with copper is inserted after an expulsion has occurred, the risk of re-expulsion has been estimated in one study to be approximately one third of cases after one year.
Perforation: Very rarely, the IUD can move through the wall of the uterus. Risk of perforation is mostly determined by the skill of the practitioner performing the insertion. For experienced medical practitioners, the risk of perforation is 1 per 1,000 insertions or less. If perforation does occur it can damage the internal organs, and in some cases surgery is needed to remove the IUD.
Infection: The insertion of a copper IUD poses a transient risk of pelvic inflammatory disease (PID) in the first 21 days after insertion. However, it is a small risk and is attributable to preexisting gonorrhea or chlamydia infection at the time of insertion, and not to the IUD itself. Proper infection prevention procedures have little or no effect on the course of gonorrhea or chlamydia infections, but is important in helping protect both clients and providers from infection in general. Such infection prevention practices include washing hands and then putting on gloves, cleaning the cervix and vagina, making minimal contact with non-sterile surfaces (using a "no touch insertion technique") and, after the procedure, washing hands again and then processing instruments. The device itself carries no increased risk of PID beyond the time of insertion.
Cramping: Many women feel cramping or pain during the IUD insertion process and immediately after as a result of cervix dilation during insertion. Taking NSAIDS before the procedure can reduce discomfort, as can the use of a local anaesthetic. Misoprostol 6 to 12 hrs before insertion can help with cervical dilation. Some women may have cramps for 1 to 2 weeks following insertion. The copper IUD can also increase cramps during a woman’s period. This symptom will clear up for some women in 3 to 6 months, but may not for others.
Heavier Periods: The copper IUD increases the amount of blood flow during a woman’s menstrual periods. On average, menstrual blood loss increases by 20–50% after insertion of a copper-T IUD; increased menstrual discomfort is the most common medical reason for IUD removal. This symptom may clear up for some women after 3 to 6 months, but may not for others. Despite this, anemia is not listed as a contraindication by the World Health Organization.
Irregular Bleeding and Spotting: For the first 3 to 6 months after insertion, the copper IUD can cause irregular periods and spotting between periods.
String problems: A small portion of men report that they can feel the strings during intercourse. In this case, strings can be trimmed. However, very short strings can prevent the woman from checking the strings for expulsion. Medical ultrasonography may be required in such cases to check the location of the IUD.
Pregnancy: Although rare, if pregnancy does occur with the copper IUD in place there can be serious side effects. The risk of ectopic pregnancy to a woman using an IUD is lower than the risk of ectopic pregnancy to a woman using no form of birth control. However, of pregnancies that do occur during IUD use, a higher than expected percentage (3–4%) are ectopic. If a pregnancy occurs with the IUD in place there is a higher risk of miscarriage or early delivery. If this occurs and the IUD strings are visible, the IUD should be removed immediately by a clinician. Although the Dalkon Shield IUD was associated with septic abortions (infections associated with miscarriage), other brands of IUD are not. IUDs are also "not" associated with birth defects.
Some barrier contraceptives protect against STIs. Hormonal contraceptives reduce the risk of developing pelvic inflammatory disease (PID), a serious complication of certain STIs. IUDs, by contrast, do "not" protect against STIs or PID.
Contraindications.
A category 3 condition indicates conditions where the theoretical or proven risks usually outweigh the advantages of inserting a copper IUD. A category 4 condition indicates conditions that represent an unacceptable health risk if a copper IUD is inserted.
Women should not use a copper IUD if they:
(Category 3) 
A full list of contraindications can be found in the WHO "Medical Eligibility Criteria for Contraceptive Use" and the CDC "United States Medical Eligibility Criteria for Contraceptive Use".
While nulliparous women (women who have never given birth) are somewhat more likely to have side effects, this is not a contraindication for IUD use. Overall, IUDs are safe and acceptable also in young nulliparous women. The same is likely the case for virgin women, unless there is a microperforate hymen that obstructs any insertion of the IUD.
Device description.
There are a number of models of the copper IUD available around the world. Most copper devices consist of a plastic core that is wrapped in a copper wire.
Many of the devices, including the TCu 380A (ParaGard), have a T-shape similar to the hormonal IUD. However, there are "frameless" copper IUDs available around the world as well. ParaGard is the only model currently available in the United States. At least three copper IUD models are available in Canada, two of which are a slimmer T-shape version used for women who have not had children. Early copper IUDs had copper around only the vertical stem, but more recent models have copper sleeves wrapped around the horizontal arms as well, increasing effectiveness.
Some newer models also contain a silver core instead of a plastic core to delay copper fragmentation as well as increase the lifespan of the device. The lifespan of the devices range from 3 years to 10 years; however, some studies have demonstrated that the TCu 380A may be effective through 12 years.
Insertion.
The copper IUD must be inserted by a qualified medical practitioner. A copper IUD can be inserted at any phase of the menstrual cycle, but the optimal time is right after the menstrual period, when the cervix is softest and the woman is least likely to be pregnant. The insertion process generally takes five minutes or less. The procedure can cause cramping or be painful for some women. 
Before placement of an IUD, a medical history and physical examination by a medical professional is useful to check for any contraindications or concerns. It is also recommended by some clinicians that patients be tested for gonorrhea and chlamydia, as these two infections increase the risk of contracting pelvic inflammatory disease shortly after insertion.
Immediately prior to insertion, the clinician will perform a pelvic exam to determine the position of the uterus.
After the pelvic exam, the vagina is held open with a speculum, the same device used during a pap smear. A tenaculum is used to steady the cervix and uterus. Uterine sounding may be used to measure the length and direction of the cervical canal and uterus in order to decrease the risk of uterine perforation. The IUD is placed using a narrow tube, which is inserted through the cervix into the uterus. Short monofilament plastic/nylon strings hang down from the uterus into the vagina. The clinician will trim the threads so that they only protrude 3 to 4 cm out of the cervix and remain in the upper vagina. The strings allow the patient or clinician to periodically check to ensure the IUD is still in place and to enable easy removal of the device.
The copper IUD can be inserted at any time in a woman's menstrual cycle as long as the woman is not pregnant. An IUD can also be inserted immediately postpartum and post-abortion as long as no infection has occurred. Breastfeeding is not a contraindication for the use of the copper IUD. The IUD can be inserted in women with HIV or AIDS as it does not increase the risk of transmission.
Although previously not recommended for nulliparous women (women who have not had children), the IUD is now recommended for most women who are past menarche (their first period), including adolescents.
After the insertion is finished, normal activities such as sex, exercise, and swimming can be performed as soon as it feels comfortable. Strenuous physical activity does not affect the position of the IUD.
Types.
Many different types of copper IUDs are currently manufactured worldwide, but availability varies by country. In the United States, only one type of copper IUD is approved for use, while in the United Kingdom, over ten varieties are available. One company, Mona Lisa N.V. offers generic versions of many existing IUDs.
Frameless IUDs.
Gynefix is the only frameless IUD brand currently available; it is not approved for use in the US. It consists of copper cylinders on a polypropylene thread. It is inserted through the cervix with a special applicator that sutures the thread to the fundus (top) of the uterus; the thread is then cut with a tail hanging outside of the cervix, similar to frame IUDs such as the ParaGard. When this tail is pulled, the suture comes undone and the device can be removed. This requires more force than removing a T-shaped IUD, but results in comparable discomfort during removal.
This change in design was made in order to reduce discomfort and expulsion associated with prior IUDs; without a solid frame, the frameless IUD should mold to the shape of the uterus. Studies have shown reduced expulsion and discontinuation rates compared to framed copper IUDs.
Mechanism of action.
The copper IUD's primary mechanism of action is to prevent fertilization. Copper acts as a natural spermicide within the uterus. The presence of copper increases the levels of copper ions, prostaglandins, and white blood cells within the uterine and tubal fluids.
Although not a primary mechanism of action, some experts in human reproduction believe there is sufficient evidence to suggest that IUDs with copper can disrupt implantation, especially when used for emergency contraception. Despite this, there has been no definitive evidence that IUD users have higher rates of embryonic loss than women not using contraception. Therefore, the copper IUD is considered to be a true contraceptive and not an abortifacient.
Prevalence.
Globally, the IUD is the most widely used method of reversible birth control. The most recent data indicates that there are 169 million IUD users around the world. This includes both the nonhormonal and hormonal IUDs. IUDs are most popular in Asia, where the prevalence is almost 30%. In Africa and Europe the prevalence is around 20%. As of 2009, levels of IUD use in the United States are estimated to be 5.5%. Data in the United States does not distinguish between hormonal and nonhormonal IUDs. In Europe, copper IUD prevalence ranges from under 5% in the United Kingdom to over 10% in Denmark in 2006.
History.
According to popular legend, Arab traders inserted small stones into the uteruses of their camels to prevent pregnancy during long desert treks. The story was originally a tall tale to entertain delegates at a scientific conference on family planning; although it was later repeated as truth, it has no known historical basis.
Precursors to IUDs were first marketed in 1902. Developed from stem pessaries (where the stem held the pessary in place over the cervix), the 'stem' on these devices actually extended into the uterus itself. Because they occupied both the vagina and the uterus, this type of stem pessary was also known as an "interuterine device". Use of "interuterine" devices was associated with high rates of infection; for this reason, they were condemned by the medical community.
The first intrauterine device (contained entirely in the uterus) was described in a German publication in 1909, although the author appears to have never marketed his product.
In 1929, Dr. Ernst Gräfenberg of Germany published a report on an IUD made of silk suture. He had found a 3% pregnancy rate among 1,100 women using his ring. In 1930, Dr. Gräfenberg reported a lower pregnancy rate of 1.6% among 600 women using an improved ring wrapped in silver wire. Unbeknownst to Dr. Gräfenberg, the silver wire was contaminated with 26% copper. Copper's role in increasing IUD efficacy would not be recognized until nearly 40 years later.
In 1934, Japanese physician Tenrei Ota developed a variation of the "Gräfenberg ring" that contained a supportive structure in the center. The addition of this central disc lowered the IUD's expulsion rate. These devices still had high rates of infection, and their use and development was further stifled by World War II politics: contraception was forbidden in both Nazi Germany and Axis-allied Japan. The Allies did not learn of the work by Gräfenberg and Ota until well after the war ended.
The first plastic IUD, the "Margulies Coil" or "Margulies Spiral", was introduced in 1958. This device was somewhat large, causing discomfort to a large proportion of women users, and had a hard plastic tail, causing discomfort to their male partners. The modern colloquialism "coil" is based on the coil-shaped design of early IUDs.
The "Lippes Loop", a slightly smaller device with a monofilament tail, was introduced in 1962 and gained in popularity over the Margulies device.
The stainless steel single-ring IUD was developed in the 1970s and widely used in China because of low manufacturing costs. The Chinese government banned production of steel IUDs in 1993 due to high failure rates (up to 10% per year).
Dr. Howard Tatum, in the USA, conceived the plastic T-shaped IUD in 1968. Shortly thereafter Dr. Jaime Zipper, in Chile, introduced the idea of adding copper to the devices to improve their contraceptive effectiveness. It was found that copper-containing devices could be made in smaller sizes without compromising effectiveness, resulting in fewer side effects such as pain and bleeding. T-shaped devices had lower rates of expulsion due to their greater similarity to the shape of the uterus.
The poorly designed Dalkon Shield plastic IUD (which had a multifilament tail) was manufactured by the A. H. Robins Company and sold by Robins in the United States for three and a half years from January 1971 through June 1974, before sales were suspended by Robins on June 28, 1974 at the request of the FDA because of safety concerns following reports of 110 septic spontaneous abortions in women with the Dalkon Shield in place, seven of whom had died. 
Robins stopped international sales of the Dalkon Shield in April 1975.
Dr. Tatum developed many different models of the copper IUD. He created the TCu220 C, which had copper collars as opposed to copper filament, which prevented metal loss and increased the lifespan of the device. Second-generation copper-T IUDs were also introduced in the 1970s. These devices had higher surface areas of copper, and for the first time consistently achieved effectiveness rates of greater than 99%. The last model Tatum developed was the TCu380A, the model that is most recommended today. In addition to T-shaped IUDs, there are also U-shaped IUDs (such as the Multiload) and 7-shaped Gravigard Copper 7 (with a mini version for nulliparous women introduced in the 1980s). More recently, a frameless IUD called Gynefix was introduced.

</doc>
<doc id="15379" url="http://en.wikipedia.org/wiki?curid=15379" title="Isle Royale National Park">
Isle Royale National Park

Isle Royale National Park is a U.S. National Park on Isle Royale and adjacent islands in Lake Superior, in the state of Michigan. Isle Royale National Park was established on April 3, 1940; designated as a National Wilderness Area in 1976; and made an International Biosphere Reserve in 1980. The park covers 894 sqmi, with 209 sqmi above water. At the U.S.-Canada border, it meets the borders of the future Canadian Lake Superior National Marine Conservation Area.
Geography.
Isle Royale, the largest island in Lake Superior, is over 45 mi in length and 9 mi wide at its widest point. The park is made up of Isle Royale itself and approximately 400 smaller islands, along with any submerged lands within 4.5 mi of the surrounding islands ().
History.
Prehistoric.
In older times, large quantities of copper were mined on Isle Royale and the nearby Keweenaw Peninsula by the indigenous peoples. The region is scarred by ancient mine pits and trenches up to 20 ft deep. Carbon-14 testing of wood remains found in sockets of copper artifacts indicates that they are at least 5700 years old.
In "Prehistoric Copper Mining in the Lake Superior Region", published in 1961, Drier and Du Temple estimated that over 1.5 billion pounds (630,400 t) of copper had been mined from the region. However, David Johnson and Susan Martin contend that their estimate was based on exaggerated and inaccurate assumptions.
19th & 20th centuries.
In the mid-1840s, a report by Douglass Houghton, Michigan's first state geologist, set off a copper boom in the state, and the first modern copper mines were opened on the island. Evidence of the earlier mining efforts was everywhere, in the form of many stone hammers, some copper artifacts, and places where copper had been partially worked out of the rock but left in place. The ancient pits and trenches led to the discovery of many of the copper deposits that were mined in the 19th century.
The island was once the site of a resort community. The fishing industry has declined considerably, but continues at Edisen Fishery. Because numerous small islands surround Isle Royale, ships were once guided through the area by lighthouses at Passage Island, Rock Harbor, Rock of Ages, and Isle Royale Lighthouse on Menagerie Island.
Within the waters of Isle Royale National Park are several shipwrecks. The area’s notoriously harsh weather, dramatic underwater topography, the island’s central location on historic shipping routes, and the cold, fresh water have resulted in largely intact, well preserved wrecks throughout the park. These were documented in the 1980s, with follow up occurring in 2009, by the National Park Service Submerged Resources Center.
Natural history.
Flora.
The predominant floral habitats of Isle Royal are within the Laurentian Mixed Forest Province. The area is a Temperate broadleaf and mixed forests biome transition zone between the true boreal forest to the north and Big Woods to the south, with characteristics of each. It has areas of both broadleaf and conifer forest cover, and bodies of water ranging from conifer bogs to swamps.
Conifers can include: Jack pines ("Pinus banksiana"); Black and White spruces ("Picea mariana" and "Picea glauca"); Balsam firs ("Abies balsamea"), and EasternRed junipers ("Juniperus virginiana").
Deciduous trees can include: Quaking aspens ("Populus tremuloides"), Bur oaks ("Quercus macrocarpa"), Paper birches ("Betula papyrifera"), American mountain ash ("Sorbus americana"), and Red maples ("Acer rubrum"), Sugar maples ("Acer saccharum"), and Mountain maples ("Acer spicatum").
Fauna.
Isle Royale National Park is known for its wolf and moose populations which are studied by scientists investigating predator-prey relationships in a closed environment. This is made easier because Isle Royale has been colonized by roughly just one third of the mainland mammal species, due to it being so remote. In addition, the environment is unique in that it is the only known place where wolves and moose coexist without the presence of bears.
There are usually around 25 wolves and 1000 moose on the island, but the numbers change greatly year to year. In rare years with very hard winters, animals can travel over the frozen lake from the Canadian mainland. To protect the wolves from canine diseases, dogs are not allowed in any part of the park, including the adjacent waters. In the 2006-2007 winter, 385 moose were counted, as well as 21 wolves, in three packs. In spring 2008, 23 wolves and approximately 650 moose were counted.
Geology.
Bedrock on Isle Royale is basalt or sandstone and conglomerates on the 1.1 billion year old Midcontinent Rift. Most of the island is covered with a thin layer of glacial material. A number of small native copper mines were active in the 1800s but mining was never really prosperous. Recent analyses by the USGS of both unmineralized basalt and copper-mineralized rock show that a small amount of naturally-occurring mercury is associated with mineralization.
Isle Royale greenstone (chlorastrolite, a form of pumpellyite) is found here, as well as on the Upper Peninsula of Michigan. It is the official Michigan state gemstone.
Recreation.
The Greenstone Ridge is a high ridge in the center of the island and carries the longest trail in the park, the Greenstone Ridge Trail, which runs 40 mi from one end of the island to the other. This is generally done as a 4 or 5 day hike. A boat shuttle can carry hikers back to their starting port. In total there are 165 mi of hiking trails. There are also canoe/kayak routes, many involving portages, along coastal bays and inland lakes.
Services.
The park has two developed areas:
Windigo, at the southwest end of the island (docking site for the ferries from Minnesota), with a campstore, showers, campsites, and a boat dock.
Rock Harbor on the south side of the northeast end (docking site for the ferries from Michigan), with a campstore, showers, restaurant, lodge, campsites, and a boat dock. Non-camping sleeping accommodations at the park are limited to the lodge at Rock Harbor
Camping.
The park has 36 designated wilderness campgrounds. Some campgrounds in the interior are accessible only by trail or by canoe/kayak on the island lakes. Others campgrounds are accessible only by private boat. The campsites vary in capacity but typically include a few three-sided wood shelters (the fourth wall is screened) with floors and roofs, and several individual sites suitable for pitching a small tent. Some tent sites with space for groups of up to 10 are available, and are used for overflow if all the individual sites are filled.
The only amenities at the campgrounds are pit toilets, picnic tables, and fire-rings at specific areas. Campfires are not permitted at most campgrounds; gas or alcohol camp stoves are recommended. Drinking and cooking water must be drawn from local water sources (Lake Superior and inland lakes) and filtered, treated, or boiled to avoid parasites. Hunting is not permitted, but fishing is, and edible berries (blueberries, thimbleberries) may be picked from the trail.
Access.
The park is accessible by ferries, floatplanes, and passenger ships during the summer months — from Houghton and Copper Harbor in Michigan; and Grand Portage in Minnesota. Private boats travel to the island from the coasts of Michigan, Minnesota, and Ontario. Isle Royale is quite popular with day-trippers in private boats, and day trip ferry service is provided from Copper Harbor and Grand Portage to and from the park. 
Because of the difficulty of travel and the hazards of wilderness survival during the winter months, Isle Royale National Park is the only U. S. National Park to entirely close for the full winter season. Due to the distance across Lake Superior to reach the park, and the winter closing, fewer than 20,000 people a year visit Isle Royale — which is fewer than the number of visitors to the most popular National Parks in a single day. Isle Royale had 15,973 visitors in 2007, making it the least-visited national park in the continental United States; and the fifth-least visited in the National Park Service system.
Ships.
Scheduled ferry service operates from Grand Portage, Copper Harbor and Houghton. 
The Grand Portage ferries reach the island in 1 1/2 hours, and stay 4 hours at the island, allowing time for hiking, a guided hike or program by the park staff, and picnics. 
The "Isle Royale Queen" serves park visitors out of Copper Harbor, on the northern Upper Peninsula coast of Michigan. It arrives at Rock Harbor in the park in 3 to 3 1/2 hours, spends 3 1/2 hours before returning to Copper Harbor.
The "Sea Hunter" operates round-trips and offers day trips through much of the season, less frequently in early summer and autumn. Some ferries may delay—and in some situations cancel—trips during heavy weather.
The "Ranger III" is a 165 ft ship that serves park visitors from Houghton, Michigan to Rock Harbor. It is operated by the National Park Service, and said to be the largest piece of equipment in the National Park system. It carries 125 passengers, and canoes, kayaks, and even small powerboats. It is a six-hour voyage from Houghton to the park. The ship overnights at Rock Harbor before returning the next day, making two round trips each week, June to mid-September. Briefly in the 2008 season, the Ranger III carried visitors to/from Windigo. This was not continued after 4 trips, due to low interest and long crossing times.
The "Voyageur II", out of Grand Portage, crosses up to three times a week, overnighting at Rock Harbor and providing transportation between popular lakeside campgrounds. The "Voyageur II" and other boat taxi services ferry hikers to points along the island, allowing a one-way hike back to Rock Harbor or Windigo. Visitors may land at Rock Harbor and depart from Windigo several days later, or vice versa. Hikers will frequently ride it in one direction to do a cross-island hike and be picked up at the other end when they finish.

</doc>
<doc id="15381" url="http://en.wikipedia.org/wiki?curid=15381" title="Integrated NATO Air Defense System">
Integrated NATO Air Defense System

NATO Integrated Air Defense System or NATINADS was the NATO response to the Soviet development of long range bombers in the 1970s. The need to maintain a credible deterrence when early warning and intercept times were massively reduced led to the development of an improved air defence (AD) system.
Development was approved by the NATO Military Committee in December 1955. The system was to be based on four air defense regions (ADRs) coordinated by SACEUR (Supreme Allied Commander Europe). Starting from 1956 early warning coverage was extended across Western Europe using 18 radar stations. This part of the system was completed by 1962. Linked to existing national radar sites the coordinated system was called the NATO Air Defence Ground Environment (NADGE).
From 1960 NATO countries agreed to place all their air defence forces under the command of SACEUR in the event of war. These forces included command & control (C2) systems, radar installations, and Surface-to-Air (SAM) missile units as well as interceptor aircraft.
By 1972 NADGE was converted into NATINADS consisting of 84 radar sites and associated Control Reporting Centers (CRC) and in the 1980s the Airborne Early Warning / Ground Environment Integration Segment (AEGIS) upgraded the NATINADS with the possibility to integrate the AWACS radarpicture and all of its information into its visual displays. (NOTE: This AEGIS is not to be confused with the U.S.Navy AEGIS, a shipboard fire control radar and weapons system.) AEGIS processed the information through Hughes H5118ME computers, which replaced the H3118M computers installed at NADGE sites in the late 1960s and early 1970s.
NATINADS ability to handle data increased with faster clock rates. The H5118M computer had a staggering 1 megabyte of memory and could handle 1.2 million instructions per second while the former model had a memory of only 256 kilobytes and a clock speed of 150000 instructions per seconds.
NATINADS/AEGIS were complemented, in W-Germany by the German Air Defence Ground Environment (GEADGE), an updated radar network adding the southern part of W-Germany to the European system and Coastal Radar Integration System (CRIS), adding data links from Danish coastal radars.
In order to counter the hardware obsolescence, during the mid-90's NATO started the AEGIS Site Emulator (ASE) program allowing the NATINADS/AEGIS sites to replace the proprietary hardware (the 5118ME computer and the various operator consoles IDM-2, HMD-22, IDM-80) with Commercial-Off-the-Shelf servers and workstations.
In the first years 2000, the initial ASE capability was expanded with the possibility to run, thanks to the new hardware power, multiple site emulators on the same hardware, so the system was renamed into Multi-AEGIS Site Emulator (MASE). The NATO system designed to replace MASE in the near future is the Air Command and Control System (ACCS).
Because of changing politics, NATO expanding and financial crises most European (NATO) countries are trying to cut defence budgets; as a direct result lots of obsolete and outdated NATINADS facilities are phased out earlier. Currently (2013) still operational NATO radar sites in Europe are these:
Germany
Greece
Italy
Norway
Portugal
Spain
UK
"Other (non-NATO) operational radar sites in Europe"
Austrian radar (GOLDHAUBE)
Swiss Air Force radar (FLORAKO)

</doc>
<doc id="15382" url="http://en.wikipedia.org/wiki?curid=15382" title="Invisible balance">
Invisible balance

The invisible balance or balance of trade on services is that part of the balance of trade that refers to services and other products that do not result in the transfer of physical objects. Examples include consulting services, shipping services, tourism, and patent license revenues. This figure is usually generated by tertiary industry. The term 'invisible balance' is especially common in the United Kingdom.
For countries that rely on service exports or on tourism, the invisible balance is particularly important. For instance the United Kingdom and Saudi Arabia receive significant international income from financial services, while Japan and Germany rely more on exports of manufactured goods.
Types of invisibles.
Invisibles are both international payments for services (as opposed to goods), as well as movements of money without exchange for goods or services. These invisibles are called 'transfer payments' or 'remittances' and may include money sent from one country to another by an individual, business, government or non-governmental organisations (NGO) – often charities.
An individual remittance may include money sent to a relative overseas. Business transfers may include profits sent by a foreign subsidiary to a parent company or money invested by a business in a foreign country. Bank loans to foreign countries are also included in this category, as are license fees paid for the use of patents and trademarks. Government transfers may involve loans made, or official aid given to, foreign countries while transfers made by NGO's include money used for charitable work in foreign country.
Balance of payments and invisibles.
In many countries a useful distinction is drawn between the balance of trade and the balance of payments. 'Balance of trade' refers to the trade of both tangible (physical) objects as well as the trade in services – collectively known as exports and imports (in other words, 'visibles plus services') – while the 'balance of payments' also includes transfers of Capital in the form of loans, investments in shares or direct investment in projects. 
A nation may have a visibles balance surplus but this can be offset by a larger deficit in the invisibles balance (creating a Balance of Trade deficit overall) – if, for example, there are large payments made to foreign businesses for invisibles such as shipping or tourism. On the other hand a Visibles Balance deficit can be offset by a strong surplus on the invisibles balance if, for example, foreign aid is being provided.
In a similar way, a nation may also have a surplus 'balance of trade' because it exports more than it imports but a negative (or deficit) 'balance of payments' because, it has a much greater shortfall in transfers of capital. And, just as easily, a deficit in the 'balance of trade' may be offset by a larger surplus in capital transfers from overseas to produce a balance of payments surplus overall.
Balance of payments problems and the invisible balance.
Problems with a country's balance of trade (or balance of payments) are often associated with an inappropriate valuation of its currency – that is, its country's foreign exchange rate. 
If a country's exchange rate is too high, its exports will become uncompetitive as buyers in foreign countries require more of their own currency to pay for them. In the meantime, it also becomes cheaper for the citizens of the country to buy goods from overseas (as opposed to buying locally produced goods) – because an over-valued currency makes foreign products less expensive. 
The simultaneous decline in currency inflows (due to decreased exports) and the rise in outflows (due to increased imports) sends the Balance of Trade into deficit which then needs to be paid for by a transfer of funds in some form – either invisible transfers (aid, etc.) or capital flows (loans, etc.). However, relying on funds like this, to support a trade deficit, is unsustainable, and the country may eventually be placed in a position where its currency needs to be devalued.
If, on the other hand, a currency is under-valued, its exports will become cheaper and therefore more competitive internationally. At the same time, imports will also become more costly – stimulating the production of domestic substitutes to replace them. This will result in a growth of currency flowing into the country and a decline in currency flowing out of it – resulting in an improvement in the country's balance of trade. 
Because a nation's exchange rate has a big impact on its 'balance of trade' and its 'balance of payments' many economists favour freely floating exchange rates over the older, fixed (or pegged) rates of foreign currency exchange. Floating exchange rates allow more regular adjustments in exchange rates to occur, allowing the greater opportunity for international payments to maintain equilibrium.

</doc>
<doc id="15387" url="http://en.wikipedia.org/wiki?curid=15387" title="Irreducible complexity">
Irreducible complexity

Irreducible complexity (IC) is a pseudoscientific theory promoted by advocates of intelligent design. IC postulates that certain biological systems are too complex to have evolved from simpler or "less complete" predecessors through natural selection acting upon a series of advantageous naturally occurring chance mutations. The argument is central to intelligent design and is rejected by the scientific community, which regards intelligent design as pseudoscience. Irreducible complexity is one of two main arguments used by intelligent design proponents, the other being specified complexity.
Evolutionary biologists have demonstrated how such systems could have evolved, and describe Behe's claim as an argument from incredulity. In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe gave testimony on the subject of irreducible complexity. The court found that "Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large."
Definitions.
The term "irreducible complexity" was coined by Michael Behe, who defined it as applying to:
A single system which is composed of several interacting parts that contribute to the basic function, and where the removal of any one of the parts causes the system to effectively cease functioning. ("Darwin's Black Box" p39 in the 2006 edition)
A second definition given by Behe (his "evolutionary definition") is as follows:
An irreducibly complex evolutionary pathway is one that contains one or more unselected steps (that is, one or more necessary-but-unselected mutations). The degree of irreducible complexity is the number of unselected steps in the pathway.
Intelligent design advocate William A. Dembski gives this definition:
A system performing a given basic function is irreducibly complex if it includes a set of well-matched, mutually interacting, nonarbitrarily individuated parts such that each part in the set is indispensable to maintaining the system's basic, and therefore original, function. The set of these indispensable parts is known as the irreducible core of the system.
History.
Forerunners.
The argument from irreducible complexity is a descendant of the teleological argument for God (the argument from design or from complexity). This states that because certain things in nature are very complicated, they must have been designed. William Paley famously argued, in his 1802 watchmaker analogy, that complexity in nature implies a God for the same reason that the existence of a watch implies the existence of a watchmaker. This argument has a long history, and can be traced back at least as far as Cicero's "De Natura Deorum" ii.34.
Up to the 18th century.
Galen (1st and 2nd centuries AD) wrote about the large number of parts of the body and their relationships, which observation was cited as evidence for creation. The idea that the interdependence between parts would have implications for the origins of living things was raised by writers starting with Pierre Gassendi in the mid 17th century and John Wilkins, who wrote (citing Galen), "Now to imagine, that all these things, according to their several kinds, could be brought into this regular frame and order, to which such an infinite number of Intentions are required, without the contrivance of some wise Agent, must needs be irrational in the highest degree." In the late 17th century, Thomas Burnet referred to "a multitude of pieces aptly joyn’d" to argue against the eternity of life. In the early 18th century, Nicolas Malebranche wrote "An organized body contains an infinity of parts that mutually depend upon one another in relation to particular ends, all of which must be actually formed in order to work as a whole," arguing in favor of preformation, rather than epigenesis, of the individual; and a similar argument about the origins of the individual was made by other 18th century students of natural history.
In his 1790 book, "The Critique of Judgment", Kant is said to argue that "we cannot conceive how a whole that comes into being only gradually from its parts can nevertheless be the cause of the properties of those parts"
19th century.
Chapter XV of Paley's "Natural Theology" discusses at length what he called "relations" of parts of living things as an indication of their design.
Georges Cuvier applied his principle of the "correlation of parts" to describe an animal from fragmentary remains. For Cuvier, this was related to another principle of his, the "conditions of existence", which excluded the possibility of transmutation of species.
While he did not originate the term, Charles Darwin identified the argument as a possible way to falsify a prediction of the theory of evolution at the outset. In "The Origin of Species", he wrote, "If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case." Darwin's theory of evolution challenges the teleological argument by postulating an alternative explanation to that of an intelligent designer—namely, evolution by natural selection. By showing how simple unintelligent forces can ratchet up designs of extraordinary complexity without invoking outside design, Darwin showed that an intelligent designer was not the necessary conclusion to draw from complexity in nature. The argument from irreducible complexity attempts to demonstrate that certain biological features cannot be purely the product of Darwinian evolution.
In the late 19th century, in a dispute between supporters of the adequacy of natural selection and those who held for inheritance of acquired characters, one of the arguments made repeatedly by Herbert Spencer, and followed by others, depended on what Spencer referred to as "co-adaptation" of "co-operative" parts, as in: "We come now to Professor Weismann's endeavour to disprove my second thesis — that it is impossible to explain by natural selection alone the co-adaptation of co-operative parts. It is thirty years since this was set forth in "The Principles of Biology." In §166, I instanced the enormous horns of the extinct Irish elk, and contended that in this and in kindred cases, where for the efficient use of some one enlarged part many other parts have to be simultaneously enlarged, it is out of the question to suppose that they can have all spontaneously varied in the required proportions." Darwin responded to Spencer's objections in chapter XXV of "The Variation of Animals and Plants under Domestication". The history of this concept in the dispute has been characterized: "An older and more religious tradition of idealist thinkers were committed to the explanation of complex adaptive contrivances by intelligent design. ... Another line of thinkers, unified by the recurrent publications of Herbert Spencer, also saw coadaptation as a composed, irreducible whole, but sought to explain it by the inheritance of acquired characteristics."
St. George Jackson Mivart raised the objection to natural selection that "Complex and simultaneous co-ordinations … until so far developed as to effect the requisite junctions, are useless" which "amounts to the concept of "irreducible complexity" as defined by … Michael Behe".
20th century.
Hermann Muller, in the early 20th century, discussed a concept similar to irreducible complexity. However, far from seeing this as a problem for evolution, he described the "interlocking" of biological features as a consequence to be expected of evolution, which would lead to irreversibility of some evolutionary changes. He wrote, "Being thus finally woven, as it were, into the most intimate fabric of the organism, the once novel character can no longer be withdrawn with impunity, and may have become vitally necessary."
In 1974, Young Earth Creationist Henry M. Morris introduced a similar concept in his book "Scientific Creationism" in which he wrote; "This issue can actually be attacked quantitatively, using simple principles of mathematical probability. The problem is simply whether a complex system, in which many components function unitedly together, and in which each component is uniquely necessary to the efficient functioning of the whole, could ever arise by random processes."
A book-length study of a concept similar to irreducible complexity, explained by gradual, step-wise, non-teleological evolution, was published in 1975 by Thomas H. Frazzetta. "A complex adaptation is one constructed of "several" components that must blend together operationally to make the adaptation "work". It is analogous to a machine whose performance depends upon careful cooperation among its parts. In the case of the machine, no single part can greatly be altered without changing the performance of the entire machine." The machine that he chose as an analog is the Peaucellier machine, and one biological system given extended description was the jaw apparatus of a python. The conclusion of this investigation, rather than that evolution of a complex adaptation was impossible, "awed by the adaptations of living things, to be stunned by their complexity and suitability", was "to accept the inescapable but not humiliating fact that much of mankind can be seen in a tree or a lizard."
In 1981, Ariel Roth, in defense of the creation science position in the trial "McLean v. Arkansas", said of "complex integrated structures" that "This system would not be functional until all the parts were there ... How did these parts survive during evolution ...?"
In 1985 Cairns-Smith wrote of "interlocking", "How can a complex collaboration between components evolve in small steps?" and used the analogy of the scaffolding called centering used to build an arch then removed afterwards: "Surely there was 'scaffolding'. Before the multitudinous components of present biochemistry could come to lean together "they had to lean on something else."" However, neither Muller or Cairns-Smith claimed that their ideas were evidence of something supernatural.
An essay in support of creationism published in 1994 referred to bacterial flagella as showing "multiple, integrated components", where "nothing about them works unless every one of their complexly fashioned and integrated components are in place" and asked the reader to "imagine the effects of natural selection on those organisms that fortuitously evolved the flagella ... without the concommitant ["sic"] control mechanisms".
An early concept of irreducibly complex systems comes from Ludwig von Bertalanffy, a 20th-century Austrian biologist. He believed that complex systems must be examined as complete, irreducible systems in order to fully understand how they work. He extended his work on biological complexity into a general theory of systems in a book titled "General Systems Theory".
After James Watson and Francis Crick published the structure of DNA in the early 1950s, General Systems Theory lost many of its adherents in the physical and biological sciences. However, Systems theory remained popular in the social sciences long after its demise in the physical and biological sciences.
Origins.
Michael Behe developed his ideas on the concept around 1992, in the early days of the 'wedge movement', and first presented his ideas about "irreducible complexity" in June 1993 when the "Johnson-Behe cadre of scholars" met at Pajaro Dunes in California. He set out his ideas in the second edition of "Of Pandas and People" published in 1993, extensively revising Chapter 6 "Biochemical Similarities" with new sections on the complex mechanism of blood clotting and on the origin of proteins.
He first used the term "irreducible complexity" in his 1996 book "Darwin's Black Box", to refer to certain complex biochemical cellular systems. He posits that evolutionary mechanisms cannot explain the development of such "irreducibly complex" systems. Notably, Behe credits philosopher William Paley for the original concept (alone among the predecessors) and suggests that his application of the concept to biological systems is entirely original.
Intelligent design advocates argue that irreducibly complex systems must have been deliberately engineered by some form of intelligence.
In 2001, Michael Behe wrote: "[T]here is an asymmetry between my current definition of irreducible complexity and the task facing natural selection. I hope to repair this defect in future work." Behe specifically explained that the "current definition puts the focus on removing a part from an already functioning system", but the "difficult task facing Darwinian evolution, however, would not be to remove parts from sophisticated pre-existing systems; it would be to bring together components to make a new system in the first place". In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe testified under oath that he "did not judge [the asymmetry] serious enough to [have revised the book] yet."
Behe additionally testified that the presence of irreducible complexity in organisms would not rule out the involvement of evolutionary mechanisms in the development of organic life. He further testified that he knew of no earlier "peer reviewed articles in scientific journals discussing the intelligent design of the blood clotting cascade," but that there were "probably a large number of peer reviewed articles in science journals that demonstrate that the blood clotting system is indeed a purposeful arrangement of parts of great complexity and sophistication." (The judge ruled that "intelligent design is not science and is essentially religious in nature".)
According to the theory of evolution, genetic variations occur without specific design or intent. The environment "selects" the variants that have the highest fitness, which are then passed on to the next generation of organisms. Change occurs by the gradual operation of natural forces over time, perhaps slowly, perhaps more quickly (see punctuated equilibrium). This process is able to adapt complex structures from simpler beginnings, or convert complex structures from one function to another (see spandrel). Most intelligent design advocates accept that evolution occurs through mutation and natural selection at the "micro level", such as changing the relative frequency of various beak lengths in finches, but assert that it cannot account for irreducible complexity, because none of the parts of an irreducible system would be functional or advantageous until the entire system is in place.
The mousetrap example.
Behe uses the mousetrap as an illustrative example of this concept. A mousetrap consists of five interacting pieces—the base, the catch, the spring, the hammer and the hold-down bar. All of these must be in place for the mousetrap to work, as the removal of any one piece destroys the function of the mousetrap. Likewise, he asserts that biological systems require multiple parts working together in order to function. Intelligent design advocates claim that natural selection could not create from scratch those systems for which science is currently unable to find a viable evolutionary pathway of successive, slight modifications, because the selectable function is only present when all parts are assembled.
In his 2008 book "Only A Theory", biologist Kenneth R. Miller challenges Behe's claim that the mousetrap is irreducibly complex. Miller observes that various subsets of the five components can be devised to form cooperative units, ones that have different functions from the mousetrap and so, in biological terms, could form functional spandrels before being adapted to the new function of catching mice. In an example taken from his high school experience, Miller recalls that one of his classmates...struck upon the brilliant idea of using an old, broken mousetrap as a spitball catapult, and it worked brilliantly...It had worked perfectly as something other than a mousetrap...my rowdy friend had pulled a couple of parts --probably the hold-down bar and catch-- off the trap to make it easier to conceal and more effective as a catapult...[leaving] the base, the spring, and the hammer. Not much of a mousetrap, but a helluva spitball launcher...I realized why [Behe's] mousetrap analogy had bothered me. It was wrong. The mousetrap is not irreducibly complex after all.
Other systems identified by Miller that include mousetrap components include the following:
The point of the reduction is that - in biology - most or all of the components were already at hand, by the time it became necessary to build a mousetrap.
As such it required far fewer steps to develop a mousetrap than to design all the components from scratch.
Thus the development of the mousetrap, said to consist of five different parts which had no function on their own, has been reduced to one step: the assembly from parts that are already present, performing other functions.
The Intelligent Design argument focusses on the functionality to catch mice. It skips over the case that many, if not all, parts are already available in their own right, at the time that the need for a mousetrap arises.
Consequences of irreducible complexity.
Supporters of intelligent design argue that anything less than the complete form of such a system or organ would not work at all, or would in fact be a "detriment" to the organism, and would therefore never survive the process of natural selection. Although they accept that some complex systems and organs "can" be explained by evolution, they claim that organs and biological features which are "irreducibly complex" cannot be explained by current models, and that an intelligent designer must have created life or guided its evolution. Accordingly, the debate on irreducible complexity concerns two questions: whether irreducible complexity can be found in nature, and what significance it would have if it did exist in nature.
Behe's original examples of irreducibly complex mechanisms included the bacterial flagellum of "E. coli", the blood clotting cascade, cilia, and the adaptive immune system.
Behe argues that organs and biological features which are irreducibly complex cannot be wholly explained by current models of evolution. In explicating his definition of "irreducible complexity" he notes that:
An irreducibly complex system cannot be produced directly (that is, by continuously improving the initial function, which continues to work by the same mechanism) by slight, successive modifications of a precursor system, because any precursor to an irreducibly complex system that is missing a part is by definition nonfunctional.
Irreducible complexity is not an argument that evolution does not occur, but rather an argument that it is "incomplete". In the last chapter of "Darwin's Black Box", Behe goes on to explain his view that irreducible complexity is evidence for intelligent design. Mainstream critics, however, argue that irreducible complexity, as defined by Behe, can be generated by known evolutionary mechanisms. Behe's claim that no scientific literature adequately modeled the origins of biochemical systems through evolutionary mechanisms has been challenged by TalkOrigins. The judge in the "Dover" trial wrote "By defining irreducible complexity in the way that he has, Professor Behe attempts to exclude the phenomenon of exaptation by definitional fiat, ignoring as he does so abundant evidence which refutes his argument. Notably, the NAS has rejected Professor Behe’s claim for irreducible complexity..."
Stated examples.
Behe and others have suggested a number of biological features that they believe may be irreducibly complex.
Blood clotting cascade.
The process of blood clotting or coagulation cascade in vertebrates is a complex biological pathway which is given as an example of apparent irreducible complexity.
The irreducible complexity argument assumes that the necessary parts of a system have always been necessary, and therefore could not have been added sequentially. However, in evolution, something which is at first merely advantageous can later become necessary. Natural selection can lead to complex biochemical systems being built up from simpler systems, or to existing functional systems being recombined as a new system with a different function. For example, one of the clotting factors that Behe listed as a part of the clotting cascade (Factor XII, also called Hageman factor) was later found to be absent in whales, demonstrating that it is not essential for a clotting system. Many purportedly irreducible structures can be found in other organisms as much simpler systems that utilize fewer parts. These systems, in turn, may have had even simpler precursors that are now extinct. Behe has responded to critics of his clotting cascade arguments by suggesting that homology is evidence for evolution, but not for natural selection.
The "improbability argument" also misrepresents natural selection. It is correct to say that a set of simultaneous mutations that form a complex protein structure is so unlikely as to be unfeasible, but that is not what Darwin advocated. His explanation is based on small accumulated changes that take place without a final goal. Each step must be advantageous in its own right, although biologists may not yet understand the reason behind all of them—for example, jawless fish accomplish blood clotting with just six proteins instead of the full 10.
Eye.
The eye is an example of a supposedly irreducibly complex structure, due to its many elaborate and interlocking parts, seemingly all dependent upon one another. It is frequently cited by intelligent design and creationism advocates as an example of irreducible complexity. Behe used the "development of the eye problem" as evidence for intelligent design in "Darwin's Black Box". Although Behe acknowledged that the evolution of the larger anatomical features of the eye have been well-explained, he claimed that the complexity of the minute biochemical reactions required at a molecular level for light sensitivity still defies explanation. Creationist Jonathan Sarfati has described the eye as evolutionary biologists' "greatest challenge as an example of superb 'irreducible complexity' in God's creation", specifically pointing to the supposed "vast complexity" required for transparency.
In an often misquoted passage from "On the Origin of Species", Charles Darwin appears to acknowledge the eye's development as a difficulty for his theory. However, the quote in context shows that Darwin actually had a very good understanding of the evolution of the eye (see fallacy of quoting out of context). He notes that “to suppose that the eye ... could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree”. Yet this observation was merely a rhetorical device for Darwin. He goes on to explain that if gradual evolution of the eye could be shown to be possible, “the difficulty of believing that a perfect and complex eye could be formed by natural selection ... can hardly be considered real”. He then proceeded to roughly map out a likely course for evolution using examples of gradually more complex eyes of various species.
Since Darwin's day, the eye's ancestry has become much better understood. Although learning about the construction of ancient eyes through fossil evidence is problematic due to the soft tissues leaving no imprint or remains, genetic and comparative anatomical evidence has increasingly supported the idea of a common ancestry for all eyes.
Current evidence does suggest possible evolutionary lineages for the origins of the anatomical features of the eye. One likely chain of development is that the eyes originated as simple patches of photoreceptor cells that could detect the presence or absence of light, but not its direction. When, via random mutation across the population, the photosensitive cells happened to have developed on a small depression, it endowed the organism with a better sense of the light's source. This small change gave the organism an advantage over those without the mutation. This genetic trait would then be "selected for" as those with the trait would have an increased chance of survival, and therefore progeny, over those without the trait. Individuals with deeper depressions would be able to discern changes in light over a wider field than those individuals with shallower depressions. As ever deeper depressions were advantageous to the organism, gradually, this depression would become a pit into which light would strike certain cells depending on its angle. The organism slowly gained increasingly precise visual information. And again, this gradual process continued as individuals having a slightly shrunken aperture of the eye had an advantage over those without the mutation as an aperture increases how collimated the light is at any one specific group of photoreceptors. As this trait developed, the eye became effectively a pinhole camera which allowed the organism to dimly make out shapes—the nautilus is a modern example of an animal with such an eye. Finally, via this same selection process, a protective layer of transparent cells over the aperture was differentiated into a crude lens, and the interior of the eye was filled with humours to assist in focusing images. In this way, eyes are recognized by modern biologists as actually a relatively unambiguous and simple structure to evolve, and many of the major developments of the eye's evolution are believed to have taken place over only a few million years, during the Cambrian explosion.
Behe maintains that the complexity of light sensitivity at the molecular level and the minute biochemical reactions required for those first “simple patches of photoreceptor[s]” still defies explanation. Other intelligent design proponents claim that the evolution of the entire visual system would be difficult rather than the eye alone.
Flagella.
The flagella of certain bacteria constitute a molecular motor requiring the interaction of about 40 different protein parts. Behe presents this as a prime example of an irreducibly complex structure defined as "a single system composed of several well-matched, interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning", and argues that since "an irreducibly complex system that is missing a part is by definition nonfunctional", it could not have evolved gradually through natural selection.
Scientists regard this argument as having been disproved in the light of research dating back to 1996 as well as more recent findings. They point out that the basal body of the flagella has been found to be similar to the Type III secretion system (TTSS), a needle-like structure that pathogenic germs such as "Salmonella" and "Yersinia pestis" use to inject toxins into living eucaryote cells. The needle's base has ten elements in common with the flagellum, but it is missing forty of the proteins that make a flagellum work. The TTSS system negates Behe's claim that taking away any one of the flagellum's parts would prevent the system from functioning. On this basis, Kenneth Miller notes that, "The parts of this supposedly irreducibly complex system actually have functions of their own."
Dembski has argued that phylogenetically, the TTSS is found in a narrow range of bacteria which makes it seem to him to be a late innovation, whereas flagella are widespread throughout many bacterial groups, and he argues that it was an early innovation. Against Dembski's argument, different flagella use completely different mechanisms, and publications show a plausible path in which bacterial flagella could have evolved from a secretion system.
Further studies have shown that, contrary to claims of "irreducible complexity", flagella and related protein transport mechanisms show evidence of evolution through Darwinian processes, providing case studies in how complex systems can evolve from simpler components.
Response of the scientific community.
Like intelligent design, the concept it seeks to support, irreducible complexity has failed to gain any notable acceptance within the scientific community. One science writer called it a "full-blown intellectual surrender strategy".
Reducibility of "irreducible" systems.
Researchers have proposed potentially viable evolutionary pathways for allegedly irreducibly complex systems such as blood clotting, the immune system and the flagellum - the three examples Behe proposed. John H. McDonald even showed his example of a mousetrap to be reducible. If irreducible complexity is an insurmountable obstacle to evolution, it should not be possible to conceive of such pathways.
Niall Shanks and Karl H. Joplin, both of East Tennessee State University, have shown that systems satisfying Behe's characterization of irreducible biochemical complexity can arise naturally and spontaneously as the result of self-organizing chemical processes. They also assert that what evolved biochemical and molecular systems actually exhibit is "redundant complexity"—a kind of complexity that is the product of an evolved biochemical process. They claim that Behe overestimated the significance of irreducible complexity because of his simple, linear view of biochemical reactions, resulting in his taking snapshots of selective features of biological systems, structures and processes, while ignoring the redundant complexity of the context in which those features are naturally embedded. They also criticized his over-reliance of overly simplistic metaphors, such as his mousetrap.
A computer model of the co-evolution of proteins binding to DNA in the peer-reviewed journal "Nucleic Acids Research" consisted of several parts (DNA binders and DNA binding sites) which contribute to the basic function; removal of either one leads immediately to the death of the organism. This model fits the definition of irreducible complexity exactly, yet it evolves.(The program can be run from .)
In addition, research published in the peer-reviewed journal "Nature" has shown that computer simulations of evolution demonstrate that it is possible for complex features to evolve naturally.
One can compare a mousetrap with a cat in this context. Both normally function so as to control the mouse population. The cat has many parts that can be removed leaving it still functional; for example, its tail can be bobbed, or it can lose an ear in a fight. Comparing the cat and the mousetrap, then, one sees that the mousetrap (which is not alive) offers better evidence, in terms of irreducible complexity, for intelligent design than the cat. Even looking at the mousetrap analogy, several critics have described ways in which the parts of the mousetrap could have independent uses or could develop in stages, demonstrating that it is not irreducibly complex.
Moreover, even cases where removing a certain component in an organic system will cause the system to fail do not demonstrate that the system couldn't have been formed in a step-by-step, evolutionary process. By analogy, stone arches are irreducibly complex—if you remove any stone the arch will collapse—yet humans build them easily enough, one stone at a time, by building over centering that is removed afterward. Similarly, naturally occurring arches of stone form by the weathering away of bits of stone from a large concretion that has formed previously.
Evolution can act to simplify as well as to complicate. This raises the possibility that seemingly irreducibly complex biological features may have been achieved with a period of increasing complexity, followed by a period of simplification.
A team led by Joe Thornton, assistant professor of biology at the University of Oregon's Center for Ecology and Evolutionary Biology, using techniques for resurrecting ancient genes, reconstructed the evolution of an apparently irreducibly complex molecular system. The April 7, 2006 issue of "Science" published this research.
Irreducible complexity may not actually exist in nature, and the examples given by Behe and others may not in fact represent irreducible complexity, but can be explained in terms of simpler precursors. The theory of facilitated variation challenges irreducible complexity. Marc W. Kirschner, a professor and chair of Department of Systems Biology at Harvard Medical School, and John C. Gerhart, a professor in Molecular and Cell Biology, University of California, Berkeley, presented this theory in 2005. They describe how certain mutation and changes can cause apparent irreducible complexity. Thus, seemingly irreducibly complex structures are merely "very complex", or they are simply misunderstood or misrepresented.
Gradual adaptation to new functions.
The precursors of complex systems, when they are not useful in themselves, may be useful to perform other, unrelated functions. Evolutionary biologists argue that evolution often works in this kind of blind, haphazard manner in which the function of an early form is not necessarily the same as the function of the later form. The term used for this process is exaptation. The mammalian middle ear (derived from a jawbone) and the panda's thumb (derived from a wrist bone spur) provide classic examples. A 2006 article in "Nature" demonstrates intermediate states leading toward the development of the ear in a Devonian fish (about 360 million years ago). Furthermore, recent research shows that viruses play a heretofore unexpected role in evolution by mixing and matching genes from various hosts.
Arguments for irreducibility often assume that things started out the same way they ended up—as we see them now. However, that may not necessarily be the case. In the "Dover" trial an expert witness for the plaintiffs, Ken Miller, demonstrated this possibility using Behe's mousetrap analogy. By removing several parts, Miller made the object unusable as a mousetrap, but he pointed out that it was now a perfectly functional, if unstylish, tie clip.
Methods by which irreducible complexity may evolve.
Irreducible complexity can be seen as equivalent to crossing a "valley" in a fitness landscape. A number of mathematical models of evolution have explored the circumstances under which this can happen.
Falsifiability and experimental evidence.
Some critics, such as Jerry Coyne (professor of evolutionary biology at the University of Chicago) and Eugenie Scott (a physical anthropologist and former executive director of the National Center for Science Education) have argued that the concept of irreducible complexity, and more generally, intelligent design is not falsifiable, and therefore, not scientific.
Behe argues that the theory that irreducibly complex systems could not have evolved can be falsified by an experiment where such systems are evolved. For example, he posits taking bacteria with no flagellum and imposing a selective pressure for mobility. If, after a few thousand generations, the bacteria evolved the bacterial flagellum, then Behe believes that this would refute his theory.
Other critics take a different approach, pointing to experimental evidence that they believe falsifies the argument for Intelligent Design from irreducible complexity. For example, Kenneth Miller cites the lab work of Barry G. Hall on E. coli, which he asserts is evidence that "Behe is wrong".
Other evidence that irreducible complexity is not a problem for evolution comes from the field of computer science, which routinely uses computer analogues of the processes of evolution in order to automatically design complex solutions to problems. The results of such Genetic Algorithms are frequently irreducibly complex since the process, like evolution, both removes non-essential components over time as well as adding new components. The removal of unused components with no essential function, like the natural process where rock underneath a natural arch is removed, can produce irreducibly complex structures without requiring the intervention of a designer. Researchers applying these algorithms automatically produce human-competitive designs—but no human designer is required.
Argument from ignorance.
Intelligent design proponents attribute to an intelligent designer those biological structures they believe are irreducibly complex and whereof they say a natural explanation is insufficient to account for them. However, critics view irreducible complexity as a special case of the "complexity indicates design" claim, and thus see it as an argument from ignorance and as a God-of-the-gaps argument.
Eugenie Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent-design proponents are arguments from ignorance. Behe has been accused of using an "argument by lack of imagination", and Behe himself acknowledges that a failure of current science to explain how an "irreducibly complex" organism did or could evolve does not automatically prove the impossibility of such an evolution.
False dilemma.
Irreducible complexity is at its core an argument against evolution. If truly irreducible systems are found, the argument goes, then intelligent design must be the correct explanation for their existence. However, this conclusion is based on the assumption that current evolutionary theory and intelligent design are the only two valid models to explain life, a false dilemma.
Irreducible complexity in the Dover trial.
While testifying during the 2005 Kitzmiller v. Dover Area School District trial, Behe conceded that there are no peer-reviewed papers supporting his claims that complex molecular systems, like the bacterial flagellum, the blood-clotting cascade, and the immune system, were intelligently designed nor are there any peer-reviewed articles supporting his argument that certain complex molecular structures are "irreducibly complex."
In the final ruling of "Kitzmiller v. Dover Area School District", Judge Jones specifically singled out Behe and irreducible complexity:

</doc>
<doc id="15388" url="http://en.wikipedia.org/wiki?curid=15388" title="Arabian mythology">
Arabian mythology

Arabian mythology is the set of ancient, pre-Islamic beliefs held by the Arab people. Prior to Islam, the "Kaaba" of Mecca was covered in symbols representing the myriad demons, djinn, demigods, or simply tribal gods and other assorted deities which represented the polytheistic culture of pre-Islamic Arabia. The shrine was dedicated to the god Hubal and also contained images the three chief goddesses Al-lāt, Al-‘Uzzá, and Manāt. It has been inferred from this plurality that this mythology flourished in an exceptionally broad context. Many of the physical descriptions of the pre-Islamic gods are traced to idols, especially near the Kaaba, which is believed to have contained up to 360 of them.
Gods.
Allah and associated goddesses.
In pre-Islamic Arabia, Allah was used by polythiestic Arabs including the Meccans as a reference to a creator god, possibly a supreme deity of their pantheon. In the Meccan religion Allah was considered the creator of the world and the giver of rain, but in contrast to Islam, Allah was not considered the sole divinity. The notion of the term may have been vague in the Meccan religion. The Meccans possibly associated angels with Allah. Allah was invoked in times of distress. Muhammad's father's name was ʿAbd-Allāh meaning "the slave of Allāh".
The three daughters of Allah and chief goddesses of Meccan Arabian mythology were Al-lāt, Al-‘Uzzá, and Manāt. Each is associated with certain domains and had shrines with idols located near Taif which have been destroyed. Allāt (Arabic: اللات‎) or Al-lāt is the goddess associated with the underworld. Al-‘Uzzá (Arabic: العزى‎) "The Mightiest One" or "The Strong" was an Arabian fertility goddess. She was called upon for protection and victory before war. Manāt (Arabic: مناة‎) was the goddess of fate; the Book of Idols describes her as the most ancient of all these idols. An idol of Manāt was erected on the seashore in the vicinity of al-Mushallal in Qudayd, between Medina and Mecca. The Aws and the Khazraj, as well as the inhabitants of Medina and Mecca and their vicinities, venerated Manāt and performed sacrifices before her idol, including offering their children. Pilgrimages of some Arabs, including the Aws, Khazraj, Yathrib and others, were not considered completed until they visited Manāt and shaved their heads.

</doc>
<doc id="15392" url="http://en.wikipedia.org/wiki?curid=15392" title="Imperial Conference">
Imperial Conference

Imperial Conferences (Colonial Conferences before 1907) were periodic gatherings of government leaders from the self-governing colonies and dominions of the British Empire between 1887 and 1937, before the establishment of regular Meetings of Commonwealth Prime Ministers in 1944. They were held in 1887, 1894, 1897, 1902, 1907, 1911, 1921, 1923, 1926, 1930, 1932 and 1937.
All the conferences were held in London, the United Kingdom, the seat of the Empire, except for the 1894 and 1932 conferences which were held in Ottawa, the capital of the most senior dominion. The 1907 conference changed the name of the meetings to Imperial Conferences and agreed that the meetings should henceforth be regular rather than taking place while overseas statesmen were visiting London for royal occasions (e.g. jubilees and coronations).
Notable meetings.
Originally instituted to emphasise imperial unity, as time went on, the conferences became a key forum for dominion governments to assert the desire for removing the remaining vestiges of their colonial status. The conference of 1926 agreed the Balfour Declaration, which acknowledged that the dominions would henceforth rank as equals to the United Kingdom, as members of the 'British Commonwealth of Nations'.
The conference of 1930 decided to abolish the legislative supremacy of the British Parliament as it was expressed through the Colonial Laws Validity Act and other Imperial Acts. The statesmen recommended that a declaratory enactment of the Parliament, which became the Statute of Westminster 1931, be passed with the consent of the dominions, but some dominions did not ratify the statute until some years afterwards. The 1930 conference was notable, too, for the attendance of Southern Rhodesia, despite it being a self-governing colony, not a dominion.
The 1932 British Empire Economic Conference held in Ottawa discussed the Great Depression, and the governments agreed to institute 'Imperial Preference': a system of protectionist tariffs on imports from non-imperial countries.
Towards Commonwealth meetings.
As World War II drew to a close, Imperial Conferences were replaced by Commonwealth Prime Ministers' Conferences, with 17 such meetings occurring from 1944 until 1969, all but one of the meetings occurred in London. The gatherings were renamed Commonwealth Heads of Government Meetings (CHOGM) in 1971 and were henceforth held every two years with hosting duties rotating around the Commonwealth.

</doc>
<doc id="15395" url="http://en.wikipedia.org/wiki?curid=15395" title="International Refugee Organisation">
International Refugee Organisation

The International Refugee Organization (IRO) was founded on April 20, 1946 to deal with the massive refugee problem created by World War II. A Preparatory Commission began operations fourteen months previously. It was a United Nations specialized agency and took over many of the functions of the earlier United Nations Relief and Rehabilitation Administration. In 1952, its operations ceased, and it was replaced by the Office of the United Nations High Commissioner for Refugees (UNHCR).
The "'Constitution of the International Refugee Organization", adopted by the United Nations General Assembly on December 15, 1946, specified the agency's field of operations. Controversially, this defined "persons of German ethnic origin" who had been expelled, or were to be expelled from their countries of birth into the postwar Germany, as individuals who would "not be the concern of the Organization." This excluded from its purview a group that exceeded in number all the other European displaced persons put together. Also, because of disagreements between the Western allies and the Soviet Union, the IRO only worked in areas controlled by Western armies of occupation.
Twenty-six states became members of the IRO and it formally came into existence in 1948: Argentina, Australia, Belgium, Bolivia, Brazil, Canada, Republic of China, Denmark, the Dominican Republic, France, Guatemala, Honduras, Iceland, Italy, Liberia, Luxembourg, the Netherlands, New Zealand, Norway, Panama, Peru, the Philippines, Switzerland, the United Kingdom, the United States, and Venezuela. The U.S. provided about 40% of the IRO's $155 million annual budget. The total contribution by the members for the five years of operation was around $400 million. It had rehabilitated around 10 million people during this time, out of 15 million people who were stranded in Europe. The IRO's first Director General was William Hallam Tuck, succeeded by J. Donald Kingsley on July 31, 1949.

</doc>
<doc id="15396" url="http://en.wikipedia.org/wiki?curid=15396" title="IRO">
IRO

IRO may refer to:

</doc>
<doc id="15401" url="http://en.wikipedia.org/wiki?curid=15401" title="Isabella d'Este">
Isabella d'Este

Isabella d'Este (18 May 1474 – 13 February 1539) was "Marchesa" of Mantua and one of the leading women of the Italian Renaissance as a major cultural and political figure. She was a patron of the arts as well as a leader of fashion, whose innovative style of dressing was copied by women throughout Italy and at the French court. The poet Ariosto labeled her as the "liberal and magnanimous Isabella", while author Matteo Bandello described her as having been "supreme among women". Diplomat Niccolò da Correggio went even further by hailing her as "The First Lady of the world".
She served as the regent of Mantua during the absence of her husband, Francesco II Gonzaga, Marquess of Mantua and the minority of her son, Federico, Duke of Mantua. In 1500 she met King Louis XII of France in Milan on a diplomatic mission to persuade him not to send his troops against Mantua.
She was a prolific letter-writer, and maintained a lifelong correspondence with her sister-in-law Elisabetta Gonzaga. Lucrezia Borgia was another sister-in-law; she later became the mistress of Isabella's husband.
Early life.
Isabella d'Este grew up in a cultured family in the city-state of Ferrara. She received a fine classical education and, as a girl, met many famous humanist scholars and artists. 
Due to the vast amount of extant correspondence between Isabella and her family and friends, her life is unusually well-documented. She was born on Tuesday 19 May 1474 at nine o'clock in the evening in Ferrara, to Ercole I d'Este, Duke of Ferrara and Eleanor of Naples. Eleanor was the daughter of Ferdinand I, the Aragonese King of Naples, and Isabella of Clermont.
One year later on 29 June 1475 her sister Beatrice d'Este was born, and in 1476 and 1477 two brothers, Alfonso and Ippolito arrived. In 1479 and 1480 two more brothers were born; they were Ferrante and Sigismondo. Of all the children Isabella was considered to have been the favourite.
In the year of Ferrante's birth, Isabella travelled to Naples with her mother. When her mother returned to Ferrara, Isabella accompanied her, while the other children stayed behind with their grandfather for eight years. It was during the journey with her mother, that Isabella acquired the art of diplomacy and statecraft.
Education.
Isabella, being naturally gifted and intellectually precocious in her youth, received an excellent education. As a child she studied Roman history, and rapidly learned to translate Greek and Latin (the former would become her favourite language) [citation needed]. Because of her outstanding intellect, she often discussed the classics and the affairs of state with ambassadors. Moreover, she was personally acquainted with the painters, musicians, writers, and scholars, who lived in and around the court. Besides her knowledge of history and languages, she could also recite Virgil and Terence by heart. Isabella was also a talented singer and musician, and was taught to play the lute by Giovanni Angelo Testagrossa In addition to all these admirable accomplishments, she also was an innovator of new dances, having been instructed in the art by Ambrogio, a Jewish dancing master.
She was described as having been physically attractive, albeit slightly plump; however, she also possessed "lively eyes" and was "of lively grace".
In 1480, at the age of six, Isabella was betrothed to Gianfrancesco, the heir to the Marquis of Mantua. Although he was not handsome, Isabella admired him for his strength and bravery; she also regarded him as a gentleman. After their first few encounters, she found that she enjoyed his company and spent the next few years getting to know him and preparing herself to be the Marchesa of Mantua. During their courtship, Isabella treasured the letters, poems, and sonnets he sent her as gifts.
Marriage.
Ten years later on 11 February 1490, at age 16, she married Francesco Gonzaga, who had by then succeeded to the marquisate. Isabella became his wife and Marchesa amid a spectacular outpouring of popular acclamation. Besides Marquis, Francesco was also Captain General of the armies of the Republic of Venice. She brought as her marriage portion, the sum of 3,000 ducats besides valuable jewellery, dishes, and a silver service. Prior to the magnificent banquet which followed the wedding ceremony, Isabella rode through the main streets of Ferrara astride a horse draped in gems and gold.
As the couple had known and admired one another for many years, their mutual attraction deepened into love; marriage to Francesco allegedly caused Isabella to "bloom". At the time of her wedding, Isabella was said to have been pretty, slim, graceful and well-dressed. Her long, fine hair was dyed pale blonde, and her eyes, "brown as fir cones in autumn, scattered laughter".
Francesco, in his capacity of Captain General of the Venetian armies, was often required to go to Venice for conferences which left Isabella in Mantua on her own at "La Reggia" the ancient palace which was the family seat of the Gonzagas. She did not lack company, however, as she passed the time with her mother and sister, Beatrice; and upon meeting Elisabetta Gonzaga, her 18-year-old sister-in-law, the two women became close friends. They enjoyed reading books, playing cards, and travelling about the countryside together. Once they journeyed as far as Lake Garda during one of Francesco's absences, and later travelled to Venice. They maintained a steady correspondence until Elisabetta's death in 1526.
Almost four years after her marriage in December 1493, Isabella gave birth to her first child out of an eventual total of eight; it was a daughter, Eleonora, whom they called Leonora for short.
Children.
Together Isabella and Francesco had eight children:
Lucrezia Borgia.
A year after her marriage to Isabella's brother, Alfonso in 1502, the notorious Lucrezia Borgia became the mistress of Francesco. Isabella had given birth to a daughter, Ippolita at about the same time, and she continued to bear him children throughout Francesco and Lucrezia's long, passionate affair, which was more sexual than romantic. Lucrezia had previously made overtures of friendship to Isabella which the latter had coldly and disdainfully ignored. From the time Lucrezia had first arrived in Ferrara as Alfonso's intended bride, Isabella, despite having acted as hostess during the wedding festivities, had regarded Lucrezia as a rival, whom she sought to outdo at every opportunity. Francesco's affair with Lucrezia, whose beauty was renowned, caused Isabella much jealous suffering and emotional pain. Their liaison ended when he contracted syphilis as a result of encounters with prostitutes.
Regency.
Isabella played an important role in Mantua during the city's troubled times. When her husband was captured in 1509 and held hostage in Venice, she took control of Mantua's military forces and held off their invaders until his release in 1512. In the same year 1512, she was the hostess at the Congress of Mantua, which was held to settle questions concerning Florence and Milan. As a ruler, she appeared to have been much more assertive and competent than her husband. When apprised of this fact upon his return, Francesco was furious and humiliated at being upstaged by his wife's superior political ability. This caused their marriage to break down irrevocably. As a result, Isabella began to travel freely and live independently from her husband until his death on 19 March 1519.
After the death of her husband, Isabella ruled Mantua as regent for her son, Federico. She commenced to play an increasingly important role in Italian politics, steadily advancing Mantua's position. She was instrumental in promoting Mantua to a Duchy, which was obtained by wise diplomatic use of her son's marriage contracts. She also succeeded in obtaining a cardinalate for her son Ercole. She further displayed a shrewd political acumen in her negotiations with Cesare Borgia, who had dispossessed Guidobaldo da Montefeltro, duke of Urbino, the husband of her sister-in-law and good friend Elisabetta Gonzaga in 1502.
Cultural pursuits and diplomatic missions.
Throughout her marriage and during her regency, when she was not conducting affairs of state, Isabella preferred to spend her free time engaged in cultural pursuits. She read books, wrote letters, and played the lute (see Bartolomeo Tromboncino). She enjoyed the latter so much that she soon wished to experiment with all the new musical instruments that were being made available. She was an important patron of music, and was instrumental in the development of the frottola. Unusually, she employed women as professional singers at the court, including Giovanna Moreschi, the wife of Marchetto Cara.
In addition to playing music, she collected art, and sponsored philosophers, poets, and painters, such as Titian, Raphael, Giovanni Bellini, and Leonardo da Vinci. She repeatedly requested that Leonardo should paint her. For many centuries, only a drawing was known to have been made, but the final painting was eventually discovered in a private collection. She complained in a letter to Leonardo that her husband had given the sketch away and requested another, which she never received. Her requests for a painting of any other subject were apparently also ignored.
Being a leader of fashion, she ordered the finest clothing, including furs as well as the newest distillations of perfume, which she concocted herself and sent as presents. Her style of dressing in simple, boyish caps contrasting with gowns that were richly embroidered with plunging décolletage that revealed the nipples, was imitated throughout Italy and at the French court. Anne of Brittany, Queen consort of Louis XII often copied Isabella, who had a fashion doll made in her likeness.
Isabella had met the French king in Milan in 1500 on a successful diplomatic mission which she had undertaken to protect Mantua from French invasion. Louis had been impressed by her alluring personality and keen intelligence. It was while she was being entertained by Louis, whose troops occupied Milan, that she offered asylum to Milanese refugees including Cecilia Gallerani, the refined mistress of her sister Beatrice's husband, Ludovico Sforza, Duke of Milan, who had been forced to leave his duchy in the wake of French occupation. Isabella presented Cecilia to King Louis, describing her as a "lady of rare gifts and charm".
Isabella was also an extreme example of the Renaissance European tendency to treat black African slaves in her household as exotic accessories. Isabella's fascination with black child servants is extensively documented. On 1 May 1491 Isabella asked Giorgio Brognolo, her agent in Venice, to procure a young black girl ('una moreta') between the ages of one-and-a-half and four, and twice in early June reminded him of the request, emphasizing that the girl should be 'as black as possible'. Isabella’s household and financial records reflect she already had a significantly older black girl in her service when she inquired after a younger black child. Records also reflect that she obtained a little black girl from a Venetian orphanage, opened negotiations with a Venetian patrician household for the sale of a little black boy and purchased an enslaved little black girl from her sister. The commission for the purchase of a little girl "as black as possible" could be construed as a wish for the maximum exoticism, from the southernmost reaches of the Land of the Blacks.
Widowhood.
"Devoted head of state".
As a widow, Isabella at the age of 45 became a "devoted head of state". Her position as a Marchesa required her serious attention, therefore she was required to study the problems faced by a ruler of a city-state. To improve the well-being of her subjects she studied architecture, agriculture, and industry, and followed the principles that Niccolò Machiavelli had set forth for rulers in his book "The Prince". In return, the people of Mantua respected and loved her.
Isabella left Mantua for Rome in 1527. She was present during the catastrophic Sack of Rome, when she converted her house into an asylum for about 2000 people fleeing the Imperial soldiers. Isabella's house was one of the very few which was not attacked, due to the fact that her son was a member of the invading army. When she left, she managed to acquire safe passage for all the refugees who had sought refuge in her home.
Later years and death.
After Rome became stabilised following the sacking, she left the city, and returned to Mantua. She made it a centre of culture, started a school for girls, and turned her ducal apartments into a museum containing the finest art treasures. This was not enough to satisfy Isabella, already in her mid-60s, so she returned to political life and ruled Solarolo, in Romagna until her death on 13 February 1539.
Legacy.
During her lifetime and after her death, poets, popes, and statesmen paid tribute to Isabella. Pope Leo X invited her to treat him with "as much friendliness as you would your brother". The latter's secretary Pietro Bembo described her as "one of the wisest and most fortunate of women; while the poet Ariosto deemed her the "liberal and magnanimous Isabella". Author Matteo Bandello wrote that she was "supreme among women", and the diplomat Niccolò da Correggio entitled her "The First Lady of the world".
Depiction in media.
Isabella d'Este is portrayed by Belgian actress Alexandra Oppo in the TV show Borgia.
Further reading.
George, L., , "Clio History Journal", 2009.

</doc>
<doc id="15402" url="http://en.wikipedia.org/wiki?curid=15402" title="International standard">
International standard

International standards are standards developed by international standards organizations. International standards are available for consideration and use worldwide. One prominent organization is the International Organization for Standardization.
Purpose.
International standards may be used either by direct application or by a process of modifying an international standard to suit local conditions. The adoption of international standards results in the creation of equivalent, national standards that are substantially the same as international standards in technical content, but may have (i) editorial differences as to appearance, use of symbols and measurement units, substitution of a point for a comma as the decimal marker, and (ii) differences resulting from conflicts in governmental regulations or industry-specific requirements caused by fundamental climatic, geographical, technological, or infrastructural factors, or the stringency of safety requirements that a given standard authority considers appropriate.
International standards are one way of overcoming technical barriers in international commerce caused by differences among technical regulations and standards developed independently and separately by each nation, national standards organization, or company. Technical barriers arise when different groups come together, each with a large user base, doing some well established thing that between them is mutually incompatible. Establishing international standards is one way of preventing or overcoming this problem.
History.
Standardization.
The implementation of standards in industry and commerce became highly important with the onset of the Industrial Revolution and the need for high-precision machine tools and interchangeable parts. Henry Maudslay developed the first industrially practical screw-cutting lathe in 1800, which allowed for the standardisation of screw thread sizes for the first time.
Maudslay's work, as well as the contributions of other engineers, accomplished a modest amount of industry standardization; some companies' in-house standards spread a bit within their industries. Joseph Whitworth's screw thread measurements were adopted as the first (unofficial) national standard by companies around the country in 1841. It came to be known as the British Standard Whitworth, and was widely adopted in other countries.
By the end of the 19th century, differences in standards between companies, was making trade increasingly difficult and strained. The Engineering Standards Committee was established in London in 1901 as the world's first national standards body. After the First World War, similar national bodies were established in other countries. The Deutsches Institut für Normung was set up in Germany in 1917, followed by its counterparts, the American National Standard Institute and the French Commission Permanente de Standardisation, both in 1918.
International organizations.
By the mid to late 19th century, efforts were being made to standardize electrical measurement. An important figure was R. E. B. Crompton, who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in the early 20th century. Many companies had entered the market in the 1890s and all chose their own settings for voltage, frequency, current and even the symbols used on circuit diagrams. Adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies. Crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering.
In 1904, Crompton represented Britain at the Louisiana Purchase Exposition in Saint Louis as part of a delegation by the Institute of Electrical Engineers. He presented a paper on standardisation, which was so well received that he was asked to look into the formation of a commission to oversee the process. By 1906 his work was complete and he drew up a permanent constitution for the first international standards organization, the International Electrotechnical Commission. The body held its first meeting that year in London, with representatives from 14 countries. In honour of his contribution to electrical standardisation, Lord Kelvin was elected as the body's first President.
The International Federation of the National Standardizing Associations (ISA) was founded in 1926 with a broader remit to enhance international cooperation for all technical standards and specifications. The body was suspended in 1942 during World War II.
After the war, ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization (ISO); the new organization officially began operations in February 1947.

</doc>
<doc id="15403" url="http://en.wikipedia.org/wiki?curid=15403" title="ISO 4217">
ISO 4217

ISO 4217 is a standard published by the International Organization for Standardization, which delineates currency designators, country codes (alpha and numeric), and references to minor units in three tables:
The tables, history and ongoing discussion are maintained by SIX Interbank Clearing on behalf of ISO and the Swiss Association for Standardization.
The ISO 4217 code list is used in banking and business globally. In many countries the ISO codes for the more common currencies are so well known publicly that exchange rates published in newspapers or posted in banks use only these to delineate the different currencies, instead of translated currency names or ambiguous currency symbols. ISO 4217 codes are used on airline tickets and international train tickets to remove any ambiguity about the price.
Code formation.
The first two letters of the code are the two letters of ISO 3166-1 alpha-2 country codes (which are also used as the basis for national top-level domains on the Internet) and the third is usually the initial of the currency itself. So Japan's currency code is JPY—JP for Japan and Y for yen. This eliminates the problem caused by the names "dollar, franc" and "pound" being used in dozens of different countries, each having significantly differing values. Also, if a currency is revalued, the currency code's last letter is changed to distinguish it from the old currency. In some cases, the third letter is the initial for "new" in that country's language, to distinguish it from an older currency that was revalued; the code sometimes outlasts the usage of the term "new" itself (for example, the code for the Mexican peso is MXN). Other changes can be seen, however; the Russian ruble, for example, changed from RUR to RUB, where the B comes from the third letter in the word "ruble".
X currencies.
In addition to codes for most active national currencies ISO 4217 provides codes for "supranational" currencies, procedural purposes, and several things which are "similar to" currencies:
The use of an initial letter "X" for these purposes is facilitated by the ISO 3166 rule that no official country code beginning with X will ever be assigned. Because of this rule ISO 4217 can use X codes without risk of clashing with a future country code. ISO 3166 country codes beginning with "X" are used for private custom use (reserved), never for official codes. For instance, the ISO 3166-based NATO country codes (STANAG 1059, 9th edition) use "X" codes for imaginary exercise countries ranging from XXB for "Brownland" to XXR for "Redland", as well as for major commands such as XXE for SHAPE or XXS for SACLANT. Consequently, ISO 4217 can use "X" codes for non-country-specific currencies without risk of clashing with future country codes.
The inclusion of EU (denoting the European Union) in the ISO 3166-1 reserved codes list, allows the euro to be coded as EUR rather than assigned a code beginning with X even though it is a supranational currency.
Treatment of minor currency units (the "exponent").
The ISO 4217 standard includes a crude mechanism for expressing the relationship between a major currency unit and its corresponding minor currency unit. This mechanism is called the currency "exponent" and assumes a base of 10. For example, USD (the United States Dollar) is equal to 100 of its minor currency unit the "Cent". So the USD has exponent 2 (10 to the power 2 is 100 which is the number of Cents in a Dollar). The code JPY is given the exponent 0, because its minor unit, the Sen, although nominally valued at 100th of a Yen is of such negligible value that it is no longer used. Usually, as with the USD, the minor currency unit has a value that is 100th of the major unit, but in some cases (including most varieties of the dinar) 1000th is used, and sometimes ratios apply which are not integer powers of 10. Mauritania does not use a decimal division of units, setting 1 ouguiya (UM) = 5 khoums, and Madagascar has 1 ariary = 5 iraimbilanja. Some currencies do not have any minor currency unit at all and these are given an exponent of 0, as with currencies whose minor units are unused due to negligible value.
Currency numbers.
There is also a three-digit code number assigned to each currency, in the same manner as there is also a three-digit code number assigned to each country as part of ISO 3166. This numeric code is usually the same as the ISO 3166-1 numeric code. For example, USD (United States dollar) has code 840 which is also the numeric code for the US (United States).
Position of ISO 4217 code in amounts.
The ISO standard does not regulate either the spacing, prefixing or suffixing in usage of currency codes. According however to the European Union's Publication Office, in English, Irish, Latvian and Maltese texts, the ISO 4217 code is to be followed by a fixed space and the amount:
In Bulgarian, Croatian, Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Lithuanian, Polish, Portuguese, Romanian, Slovak, Slovene, Spanish and Swedish the order is reversed; the amount is followed by a fixed space and the ISO 4217 code:
History.
In 1973, the ISO Technical Committee 68 decided to develop codes for the representation of currencies and funds for use in any application of trade, commerce or banking. At the 17th session (February 1978), the related UN/ECE Group of Experts agreed that the three-letter alphabetic codes for International Standard ISO 4217, "Codes for the representation of currencies and funds", would be suitable for use in international trade.
Over time, new currencies are created and old currencies are discontinued. Frequently, these changes are due to the formation of new governments, treaties between countries standardizing on a shared currency, or revaluation of an existing currency due to excessive inflation. As a result, the list of codes must be updated from time to time. The ISO 4217 maintenance agency (MA), , is responsible for maintaining the list of codes.
Active codes.
The following is a list of active codes of official ISO 4217 currency names.
USD/USS/USN, three currency codes belonging to the US.
The US dollar has two codes assigned: USD and USN (next day). The USS (same day) code is not in use any longer, and was removed from the list of active ISO 4217 codes in March 2014.
According to UN/CEFACT recommendation 9, paragraphs 8–9 ECE/TRADE/203, 1996, :
Non ISO 4217 currencies.
Currencies without ISO 4217 currency codes.
A number of currencies are not included in ISO 4217, because these currencies are: (a) minor currencies pegged 1:1 to a larger currency, even if independently regulated (b) a legal tender only issued as commemorative banknotes or coinage, or (c) a currency of an unrecognized or partially recognized state. These currencies include:
See for a list of all currently pegged currencies.
Unofficial currency codes.
Despite the lack of an ISO code, the following non-ISO codes are sometimes used commercially.
In addition, GBX is sometimes used (for example on the London Stock Exchange) to denote Penny sterling, a subdivision of pound sterling, the currency for the United Kingdom.
Historical currency codes.
A number of currencies were official ISO 4217 currency codes and currency names until their replacement by the euro or other currencies. The table below shows the ISO currency codes of former currencies and their common names (which do not always match the ISO 4217 names). These codes were introduced in 1989 after a request from the reinsurance sector in 1988 was accepted.

</doc>
