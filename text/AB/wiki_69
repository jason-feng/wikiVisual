<doc id="22087" url="http://en.wikipedia.org/wiki?curid=22087" title="Nicaragua Canal">
Nicaragua Canal

The Nicaragua Canal (Spanish: "Canal de Nicaragua"), formally the Nicaraguan Canal and Development Project (also referred to as the Nicaragua Grand Canal, or the Nicaragua Interoceanic Grand Canal) is a shipping route under construction through Nicaragua to connect the Caribbean Sea (and therefore the Atlantic Ocean) with the Pacific Ocean. In June 2013, Nicaragua's National Assembly approved a bill to grant a 50-year concession to finance and manage the project to the private Hong Kong Nicaragua Canal Development Investment Company (HKND Group) headed by Wang Jing, a Chinese billionaire. The concession can be extended for another 50 years once the waterway is operational. Construction of the canal, estimated to cost $40 to $50 billion, began in December 2014, with completion due within five years. However, the Nicaraguan government has since failed to present reliable information about whether or not the project can be financed, thus casting doubt over whether or not it can be completed. Scientists are concerned about the environmental impact of the project, as Lake Nicaragua is Central America's key freshwater reservoir.
Construction of a canal using the San Juan River as an access route to Lake Nicaragua was first proposed in the early colonial era. The United States abandoned plans to construct a waterway in Nicaragua in the early 20th century after it purchased the French interests in the Panama Canal. 
History.
The idea of constructing a manmade waterway through Central America is old. The colonial administration of New Spain conducted preliminary surveys. The routes suggested usually ran across Nicaragua, Panama, or the Isthmus of Tehuantepec in Mexico.
The history of attempts to build a Nicaragua canal connecting the Caribbean Sea and thus the Atlantic Ocean and the Pacific Ocean goes back at least to 1825 when the Federal Republic of Central America hired surveyors to study a route via Lake Nicaragua. Many other proposals have followed. Despite the operation of the Panama Canal that opened in 1914, interest in a Nicaragua canal has continued. With emergence of globalization, an increase in commerce and the cost of fuel, and the limitations of the Panama canal, the concept of a second canal across the American land bridge became more attractive, and in 2006 the president of Nicaragua, Enrique Bolaños, announced the intent of Nicaragua to proceed with such an project. 
On September 26, 2012, the Nicaraguan Government and the newly formed Hong Kong Nicaragua Canal Development Group (HKND) signed a memorandum of understanding that committed HKND to financing and building the "Nicaraguan Canal and Development Project". The Nicaraguan Government subsequently approved the "Master Concession Agreement" with HKND on June 13, 2013 thereby granting the company "the sole rights to the HKND Group to plan, design, construct and thereafter to operate and manage the Nicaragua Grand Canal and other related projects, including ports, a free trade zone, an international airport and other infrastructure development projects." The agreement will last for 50 years and is renewable for another 50 years. HKND will pay the Government of Nicaragua US$10m annually for 10 years, and thereafter a portion of the revenue starting at 1% and increasing later. Stratfor indicated that after 10 years periodically ownership shares will be handed over to Nicaragua, so that after 50 years Nicaragua would be the majority shareholder.
On September 26, 2012, the Nicaraguan government and the newly formed Hong Kong Nicaragua Canal Development Group signed a memorandum of understanding that committed HKND Group to financing and building the Nicaraguan Canal and Development Project. HKND Group is a private enterprise. HKND Group has now entered the study phase of development to assess the technological and economic feasibility of constructing a canal in Nicaragua, as well as the potential environmental, social, and regional implications of various routes. The canal and other associated projects would be financed by investors throughout the world and would generate jobs for Nicaragua and other Central American countries.
Initial findings of the commercial analysis conducted by HKND Group indicate that the combined impact of growth in east–west trade and in ship sizes could provide a compelling argument for the construction of a second canal, substantially larger than the expanded Panama Canal, across Central America. Within 10–15 years, growth in global maritime trade is expected to cause congestion and delays in transit through the Panama Canal without a complementary route through the isthmus. Additionally, by 2030, the volume of trade that a Nicaragua Canal could serve will have grown by 240% from today.
On June 10, 2013, The Associated Press reported that the National Assembly's Infrastructure Committee unanimously voted in favor of the project, with four members abstaining. On June 13, 2013, Nicaragua's legislature passed the legislation granting the concession. On June 15, Nicaraguan President Daniel Ortega and the chairman of HKND Group, Wang Jing, signed the concession agreement giving HKND Group the rights to construct and manage the canal and associated projects for 50 years. An HKND Group press release read, "HKND Group Successfully Obtains Exclusive Right to Develop and Manage Nicaragua Grand Canal for 100 Years". Under the exclusive contract, Wang can skip building the canal (and making any payments to Nicaragua) and instead simply operate lucrative tax-free side projects.
Wang Jing, a Chinese billionaire who leads and wholly owns HKND Group, announced at a press briefing in June 2013 that he had successfully attracted global investors to the $40 billion project. In January 2014 Wang Jing and President Ortega issued a statement that construction of the project would begin in December 2014, and that it will be completed in 2019.
On July 7, 2014, a 278 km (172.7 mi) route for the Nicaragua Canal was approved. The route starts from the mouth of the Brito river on the Pacific side, passes through Lake Nicaragua, and ends in the Punta Gorda river on the Caribbean. The proposed canal would be between 230 meters and 520 meters (754.6 feet and 1,706 feet) wide and 27.6 meters (90.6 feet) deep. The "Toronto Star" noted that Chinese engineer Dong Yung Song said the canal's design called for the creation of a 400 sqkm artificial lake. The water to fill the canal's giant locks would come from the artificial lake, not from Lake Nicaragua.
The Moscow Times has reported that Russia will take part in the building of the Nicaragua Canal, viewing the project in part as an opportunity to pursue strategic interests in the region. Construction was to begin on December 29, 2014, and officially started a week earlier. However, due to Nicaragua’s volatile climate and seismic activity, feasibility concerns have emerged over the project’s future.
Construction.
On December 22, 2014, Monday, the China-backed canal announced construction started in Rivas, Nicaragua. HKND Group Chairman Wang Jing spoke during the starting ceremony of the first works of the Interoceanic Grand Canal in Brito town. Construction of the new waterway will be run by HKND Group—Hong Kong-based HK Nicaragua Canal Development Investment Co Ltd., which is controlled by Wang Jing.
According to the plans of HKND the project entails the development and building of the canal as well as a supporting infrastructure as follows (status January 2015, modifications may still be made)
The Nicaragua Grand Canal.
The 259.4 km long canal has three segments. The West Canal runs from Brito at the Pacific Ocean up the Rio Brito valley, crosses the continental divide, and after passing through the Rio Las Lajas valley enters Lake Nicaragua; its length is 25.9 km. The Nicaragua Lake section measures 106.8 km and runs from 4 km south of San Jorge to 8 km south of San Miguelito. The Eastern Canal is the longest section with 126.7 km and will be built along the Rio Tule valley through the Caribbean highland to the Rio Punta Gorda valley to meet the Caribbean Sea.
Both the West Canal and the East Canal have each one lock with 3 consecutive chambers to raise ships to the level of Lake Nicaragua that has an average water elevation of 31.3 m, range 30.2-33.0 m. The western Brito Lock is 14.5 km inland from the Pacific, and the eastern Camilo Lock is 13.7 km inland from the Caribbean Sea. The dimensions of each of the chambers of the locks are 520 m long, 75 m wide, and 27.6 m threshold depth. As locks generally define the limit of the size of ships that can be handled, the Nicaragua Canal is being designed to allow passage for larger ships than those that pass through the Panama Canal. For comparison, the new third set of locks in the Panama expansion will only be 427 m long, 55 m wide, and 18.3 m deep. 
No water from Lake Nicaragua is planned to be used to flood the locks; water will come from local rivers and recycling using water saving basins. The Camilo lock is built adjacent to a new dam of the upper Punta Gorda river that creates a reservoir. This Atlanta Reservoir (or Lake Atlanta) will have a surface area of 395 km2. West of the Atlanta reservoir the Rio Agua Zarca will be dammed to create a second reservoir. This reservoir would have a surface area of 48.5 km2 and hold 1,100 gigalitres. A hydropower facility will be built at the dam and is expected to generate over 10 megawatts of power to be used for operations of the Camilo Lock. Both locks would also be connected to the country’s power grid and have back-up generator facilities. It is estimated that each lock uses about 9 megawatts of power.
Four lighthouses will be constructed at the entrances to the East and West Canals. In addition, the channel entrance on sea will be marked on both sides with a large sailing buoy about 2 miles offshore and 2 light buoys will mark the passage through Lake Nicaragua. Navigation and lock control centers will be established.
Oceanic access and ports.
At each oceanic entrance of the canal breakwaters and port facilities will be constructed. The Pacific port will be named Brito Port and the Caribbean one Aguila Port. Initially these two ports will help during construction and later become international ports. Their design capacity is 1.68 million TEU/year and 2.5 million TEU/year, respectively.
Existing port facilities at Corinto and Bluefields will be improved to allow for shipment of material to the entry ports under construction.
Dredging and excavation.
HKND describes the project as the largest civil earthmoving operation in history. Most of this will consists of dry excavation to form the canal with an estimate of 4,019 Mm3 material of rock and soil. There will be 739 Mm3 freshwater dredging (Lake Nicaragua) and 241 Mm3 marine dredging. Marine dredging of the oceanic access canal will be required on the Pacific side for 1.7 km and on the Caribbean Sea for 14.4 km.
Disposal of excavation material will be done usually along the canal in designated disposal areas typically within 3 km of the canal.
Infrastructure.
Two concrete plants and a steel factory are planned to support the project. While cement will be probably imported, construction aggregate will come from local quarries near the two locks.
Appropriate road improvements are planned. Most important will be these changes:
The Pan-American Highway would cross the canal via a bridge. Nicaragua Route 25 (Acoyapa-San Carlos) on the eastern side of Lake Nicaragua would get a ferry service. Both ports would get public road connections. HKND plans to construct a private gravel maintenance road on both sides of the canal.
The management offices will be rented or purchased near Rivas and nine construction camps will be erected at various sites. Camps would provide food and shelter as well as health care and security. 
Fuel storage sites will be placed at the two port sites.
Free trade zone and hotels.
A free trade zone with commercial facilities as well as tourist hotels and an international airport at Rivas are planned to be built when canal construction is advanced.
Workforce.
HKND estimates that about 50,000 people will be employed during the 5-year construction, about half of them from Nicaragua, 25% from China, and the remainder from various countries. 1,400 workers will be in office or administrative positions and the others are in the field. Workers live in one of the nine camps. These are “closed” camps, that is workers cannot leave the camp unless part of an organized activity. The work schedule calls for 12 hour shifts for seven days a week. Domestic workers work two weeks and get one week off, while foreign workers are 6 weeks on and get 2 weeks off (management) or 22 weeks on, 4 weeks off (blue collar workers).
The estimate for the workforce in 2020 when the project is completed is 3,700 people, and 12,700 in 2050 when traffic has increased.
Financing.
The estimated costs are US$40bn or US$50bn.
Beside private money provided by Wang Jing at the start-up, further influx of financial support is expected from investors. An IPO was reported to be in preparation by the end on 2014. XCMG, a state-owned Chinese construction company will provide machinery and take 1.5-3% of HKND shares in return.
As of the end of 2014 no major investors have been named. There has been speculation that the Chinese government provides financial backing for the project, but China as well as Wang Jing have denied this.
Projections.
Transit time is about 30 hours. It is projected that by 2020 3,576 ships will pass the canal annually. The transit rate is expected to increase to 4,138 by 2030, and to 5,097 by 2050. For comparison, the Panama Canal handled 12,855 transits in 2009.
Potential benefits for Nicaragua.
Daniel Ortega whose government approved the agreement within one week in June of 2013 sees the canal as the second phase of the Nicaraguan Revolution predicting that it will pull Nicaragua out of poverty and lead to the creation of 250,000 jobs.
Rivalry.
The Nicaragua canal project has seen business rivalry greatly intensify in late 2014. China Harbor Engineering Company, an experienced construction company, has offered to design, construct, and finance a fourth set of locks, in Panama, where it opened a regional headquarters, which if built to the width of the proposed Nicaragua Canal, would cut across far fewer kilometers, and still cost only $10 billion according to the firm. Panama is in a much better financial situation than Nicaragua to afford taking on such debt, and already has a stream of income from its existing canals. Furthermore, the Suez Canal, with financing already complete, has put pressure on Panama, by initiating an expansion in August 2014 to double transit capacity, and due to be finished by September 2015, which is before the 3rd set of locks in Panama is completed.
Risks and opposition.
Wang Jing admitted that the project has financial, political and engineering risks. With the high cost of the project that independently has been estimated to be about $100 billion it remains to be seen if it will be fully funded. The project is supposed to be completed in 2020, but Stratfor, an analyst agency, believes this target to be an “unrealistic goal”. As the Panama Canal still has capacity and is undergoing a renovation, projections for the traffic of the Nicaragua canal may optimistic. Further, a coast-to-coast railway line may be built by China in Honduras and could affect utilization of the canal. Also, North American land bridges in Mexico and the United States will compete in the traffic between Asia and the east coast of the United States. Thus, competition may undermine the economic viability of the Canal.
A major environmental concern is the impact of the project on Lake Nicaragua, the largest source of freshwater in Nicaragua. An oil spill would have serious and lasting consequences. Other problems would be the impact of dredging bringing up toxic sediments, the disruption of migration patterns of animal species, and the potential to introduce invasive species to the Lake. Environmental studies had not been released by HKND when the project officially started in December 2014.
The project faces opposition within Nicaragua. Fabio Gadea, Ortega’s political opponent, indicated that the government violated democratic principles when making the agreement with HKND and that the agreement curtails Nicaragua’s sovereignty. Opposition leader Eliseo Nuñez called the deal “part of one of the biggest international scams in the world.” Legal challenges that the deal violates constitutional rights were rejected by the Supreme Court and a retrospective rewriting of the constitution placed HKND beyond legal challenge.
HKND has been granted the right to expropriate landowners within 5 km on each side of the canal and pay only cadastral value, not market value for property. Wang Jing, however, promised to pay fair market value. The estimates of the number of people who will be displaced range from 29,000 to more than 100,000. There are indications of local opposition to intended expropriations. Thus, according to an activist leader an unrest in Rivas in December 2014 in opposition to the canal left two protesters dead. Studies of the social impact regarding displacement of people had not been released when the project officially started.
The apparent lack of experience of Wang Jing and his HKND Group in large-scale engineering has been cited as a risk. While the reserves of the Nicaraguan National Bank serve as collateral on part of Nicaragua, HKND has so such potential liability. 
There is concern that the project might not get finished and the country’s situation would deteriorate. Due to Nicaragua's volatile climate and seismic activity, it has also been questioned whether or not the canal is feasible. 
Protests against the construction of the canal erupted shortly after the official ceremony marking the beginning of the construction. Farmers protested against the possible eviction from their lands due the construction of the canal.
The protesters also fear that the canal will bring massive environmental destruction to Lake Nicaragua and the Atlantic Autonomous Regions.

</doc>
<doc id="22090" url="http://en.wikipedia.org/wiki?curid=22090" title="Nu metal">
Nu metal

Nu metal (also known as new metal, neo-metal, nü-metal, or aggro-metal) is a subgenre of alternative metal that fuses elements of heavy metal music with those of multiple other genres, most notably ones like hip hop, funk, and grunge.
Bands associated with nu metal have derived influence from a variety of diverse styles, including multiple subgenres of heavy metal. Nu metal music is largely syncopated and based on guitar riffs, although guitar solos are rare. Many nu metal bands use seven-string guitars with a low "B" to create a heavier sound. DJs are also sometimes used for rhythmic scratching and electronic backgrounds. Nu metal vocal styles range between singing, rapping, screaming and growling.
In 1997, nu metal was beginning to rise in popularity. 1998 is generally recognized as the year nu metal broke into the mainstream. In the late 1990s, some bands were blending nu metal with other genres (e.g., industrial metal, like Static-X or Dope). In 2002, critics began claiming that nu metal's mainstream popularity was declining, but some bands still had commercial success. By the mid 2000s, metalcore was one of the most popular genres within the New Wave of American Heavy Metal. During this period, many nu metal bands experimented with other genres and sounds.
Predecessors.
Alternative metal, funk metal, rap metal, grunge and industrial metal bands of the 1980s and 1990s including Faith No More, Mr. Bungle, Red Hot Chili Peppers, Jane's Addiction, Tool, Primus, Rage Against the Machine, Helmet, Soundgarden, Alice in Chains, Godflesh, Nine Inch Nails and Ministry have been identified as laying groundwork for the development of nu metal, such as combining aggressive riffs with pop structures and drawing influence from a variety of genres within and outside of heavy metal music.
Groove metal and thrash metal bands of the same era such as Pantera, Sepultura, Metallica and Anthrax have also been cited as influential to nu metal. Anthrax pioneered the rap metal sound by fusing hip hop with metal on their EP "I'm the Man".
Characteristics.
Bands associated with nu metal have derived influence from a variety of diverse styles, including electronic music, funk, glam metal, gothic rock, hardcore punk, hip hop, new wave music, industrial metal, jazz, post-punk, symphonic rock and synthpop. Nu metal also derives influences from multiple subgenres of heavy metal including rap metal, funk metal, alternative metal and thrash metal.
Nu metal music is largely syncopated and based on guitar riffs. Mid-song bridges and a general lack of guitar solos contrasts it with other genres that are part of heavy metal, in which guitar solos play a major role. Another contrast with other metal genres is its emphasis on rhythm, rather than complexity or mood, tending towards groove metal in rhythm. Nu metal bassists and drummers often draw influence from funk and hip hop break beats, respectively, helping add to the rhythmic nature of the genre. Similarities with many heavy metal subgenres include its use of common time, distorted guitars, power chords and note structures primarily revolving around Dorian, Aeolian or Phrygian modes. 
Many nu metal bands use seven-string guitars (which are sometimes downtuned to increase heaviness) and rarely eight-string guitars (Deftones) over traditional six-string guitars. This results in bass guitarists using five-string and six string instruments. DJs are also sometimes used for additional rhythmic instrumentation such as music sampling, turntable scratching and electronic backgrounds.
Nu metal is also sometimes noted for participation of women in the genre in contrast to some other metal genres, including bands such as Coal Chamber, Otep and the all-female band Kittie.
Nu metal vocal styles range between singing, rapping, screaming and death growling, sometimes using multiple of these styles within one song. The lyrics of many nu metal bands focus on pain and personal alienation, similar to that of grunge, rather than the themes of other metal subgenres. Nu metal uses the traditional pop structure of verses, choruses and bridges, contrasting it with other metal genres such as thrash and death metal.
Trevor Baker of "The Guardian" wrote "Bands such as Linkin Park, Korn and even the much reviled Limp Bizkit also, incidentally, did far more to break down the artificial barriers between "urban music" and rock than any of their more critically acceptable counterparts. Their concerts also drew huge numbers of women, which is much more than you could say for any old-metal band." Nu metal fashion can include baggy shirts, sports jerseys and jackets, basketball singlets and shorts, hoodies, cargo pants, sweatpants, dreadlocks, spiky hair, buzz cut, body piercings, tattoos, long hair, shaved heads, goatee, jumpsuits and sweatsuits. Like the music itself, nu metal fashion is influenced by other fashions such as goth and hip hop.
History.
Early development (early-mid 1990s).
The origins of the term are often attributed to the work of producer Ross Robinson, sometimes called "The Godfather of Nu Metal". Many of the first nu metal bands came from California, such as Korn, who are identified as the ones who pioneered the nu metal sound with the release of their "Neidermayer's Mind" demo album in 1993, and the Deftones. Other influential bands are Staind from Massachusetts, Limp Bizkit from Florida, and Slipknot from Iowa. The aggressive riffs of Korn, the rapping of Limp Bizkit, and the acoustic ballads of Staind created the sonic template for nu metal.
In 1994, Korn's debut single "Blind"'s music video received airplay on MTV, exposing nu metal to a wider audience in a time when grunge dominated. Nu metal continued to achieve recognition through MTV and Ozzy Osbourne's 1995 introduction of Ozzfest, which led the media to talk of a resurgence of heavy metal. Ozzfest was integral to launching the careers of several nu metal bands, including Limp Bizkit in 1998. The band only had experienced underground fame, as their debut album peaked at number 72 on the "Billboard" 200. Nu metal didn't have plenty of bands playing the genre until 1997 where multiple nu metal bands like Coal Chamber, Limp Bizkit, Papa Roach and Sevendust all released their debut albums.
Mainstream popularity (late 1990s and early 2000s).
In 1997, nu metal was beginning to rise in popularity when Korn released their single, "A.D.I.D.A.S." off their album "Life is Peachy". "Life is Peachy" peaked at number 3 on the "Billboard" 200 while the song A.D.I.D.A.S peaked at number 13 on the Billboard Bubbling Under Hot 100 Singles.
1998 is generally recognized as the year nu metal broke into the mainstream, with Korn's third album, "Follow the Leader", which peaked at number 1 on the "Billboard" 200 and became a multi-platinum hit, and paved the way for other nu metal bands. By this point most nu metal bands were playing a combination of heavy metal, hip hop, industrial, grunge and hardcore punk. Established artists such as Sepultura, Slayer, Vanilla Ice, Primus, Ice Cube Fear Factory and Machine Head released albums that drew from the style. In "Sound of the Beast: The Complete Headbanging History of Heavy Metal", Ian Christie wrote that the genre demonstrated that "pancultural metal could pay off". Korn's "Issues" album also peaked at number 1 on the "Billboard" 200 and the band had "Billboard" Hot 100 singles such as "Falling Away From Me". By the late 1990s and early 2000s, many nu metal bands including Korn appeared constantly on MTV's Total Request Live. Woodstock also held a 1999 festival which featured many nu metal artists like Korn, Kid Rock, Limp Bizkit and Sevendust.
Max Cavalera, former frontman of the band Sepultura, formed his new band Soulfly. Their debut album, "Soulfly", was released on April 21, 1998 to positive reviews and was soon certified gold in the United States. It continued the Nu metal sound his former band started on the album "Roots", a raw and extremely heavy, aggressive sound. Deftones, a band from California, released their album "Around the Fur", which peaked at #29 on The Billboard 200, remaining there for seventeen weeks and sold 43,000 copies in its first week of release. "Around the Fur" as well as the band's "Adrenaline" album both were certified gold in the summer of 1999.The album was RIAA certified gold on July 7, 1999 in recognition of 500,000 units sold. The nu metal band Coal Chamber's self-titled debut peaked at #10 on The Billboard 200 and the band's single covering Peter Gabriel titled "Shock the Monkey" from their album "Chamber Music" peaked #26 on the Hot Mainstream Rock Tracks chart. Their self-titled debut has been certified Gold by the RIAA, with an excess of 500,000 copies in the United States.
Orgy also became extremely popular in 1999 and 2000 with albums like "Candyass", which was certified platinum by the RIAA and the band had "Billboard" Hot 100 singles such as "Blue Monday" and "Opticon," which both peaked at number 56 on the Billboard Hot 100.
In May 1999, nu metal musician Kid Rock had sales for his Devil Without a Cause album taking off with the third single "Bawitdaba" and by April 1999, where "Devil Without a Cause" had achieved a gold disc. The following month, Devil Without a Cause, as he predicted, went platinum.
In 1999, Slipknot, a nu metal band from Iowa, emerged with an extremely heavy sound, releasing their debut album, which has gone on to sell over 2 million copies in the United States alone, with Rick Anderson of AllMusic writing "You thought Limp Bizkit was hard? They're the Osmonds. These guys are something else entirely." Limp Bizkit's second album "Significant Other", released in 1999, reached number 1 on the "Billboard" 200, selling 643,874 copies in its first week of release. In its second week, the album sold 335,000 copies.
In 1999, the nu metal band Staind's second album "Dysfunction" had popular singles such as "Mudshovel", and has been certified 2 times platinum by the RIAA by selling 2 million copies in the United States alone. In 2000, Limp Bizkit's follow-up album "Chocolate Starfish and the Hot Dog Flavored Water", set a record for highest week-one sales of a rock album with over one million copies sold in the U.S. in its first week of release, with 400,000 of those sales coming on its first day, making it the fastest-selling rock album ever, breaking the world record held for seven years by Pearl Jam's "Vs." That same year, Papa Roach's major label debut "Infest", and Disturbed's "The Sickness" became platinum hits.
Late in 2000, Linkin Park released their debut album "Hybrid Theory", which remains both the best-selling debut album by any artist in the 21st century, and the best-selling nu metal album of all time. The album was also the best-selling album in all genres in 2001, offsetting sales by prominent pop acts like the Backstreet Boys and NSYNC, earning the band a Grammy Award for their second single "Crawling", with the fourth single, "In the End", released late in 2001, becoming one of the most recognized songs in the first decade of the 21st century. During the same year, the prog-influenced band Mudvayne's debut "L.D. 50" received critical acclaim ."CMJ" called the Mudvayne album "A vivid cross section of nu-metal styles." 
Staind's 2001 album "Break The Cycle" debuted at number 1 on the "Billboard" 200 with first week sales of 716,003 copies. 
That same year, Slipknot released their second album "Iowa", which peaked at number 3 on the "Billboard" 200, going on to sell over a million copies in the United States, critic John Mulvey proclaimed the album as the "absolute triumph of nu metal". The band P.O.D.'s Satellite album went triple platinum and peaked at #6 on the "Billboard" 200 chart. The band Drowning Pool released a nu metal album titled "Sinner", which featured the hit single "Bodies". It went platinum after six weeks of its release and its song "Bodies" became one of the most frequently played videos on MTV for new bands. The nu metal band Alien Ant Farm's album "Anthology" peaked at no. 1 on the Top Heatseekers chart and included a popular cover of the Michael Jackson song Smooth Criminal.
In 2002, critics began claiming that nu metal's mainstream popularity was declining, citing the fact that Korn's long awaited fifth album "Untouchables", and Papa Roach's third album "Lovehatetragedy", did not sell as well as their previous releases, nu metal bands were played less frequently on radio stations and MTV began focusing on pop punk, metalcore and emo.
Evanescence's debut album "Fallen", was also released on March 2003. Many critics noted the nu metal sound of the album, whose Grammy Award-winning lead single "Bring Me to Life" was compared favorably to Linkin Park's style. By the end of 2003, Linkin Park's "Meteora" and Evanescence's "Fallen" ranked third and fourth respectively in the best-selling albums of 2003, and would go on to sell nearly 35 million copies between them as of 2012. Both bands released high-charting singles throughout 2003 to mid-2004. Also in 2003, Korn and Limp Bizkit released their new albums "Take a Look in the Mirror" and "Results May Vary", both of which sold considerably less than their previous efforts. Korn went on to admit "Take a Look in the Mirror" was rushed, while readers of Guitar World magazine named Limp Bizkit, along with the post-grunge band Creed "worst band of 2003". In 2005, Linkin Park's "Hybrid Theory" received a diamond certification by the RIAA for shipment of ten million copies. Jane's Addiction returned in 2003 with their album Strays, which contained nu metal elements.
Decline in popularity (mid 2000s).
By the mid 2000s, metalcore (a fusion of extreme metal and hardcore punk) had become the most popular genre within the New Wave of American Heavy Metal, in both the mainstream and within "core" audiences. After a period of massive success of Linkin Park and Evanescence, nu metal had declined in popularity. Regarding his band's decline in popularity, Fred Durst said "Here's the deal: say in 2000, there were 35 million people who connected to this band. Twelve years later, lots of those people have moved on. We were a moment in time and it's over."
Many nu metal bands experimented with other genres and sounds. While Deftones retained several of their nu metal traits, they had overall moved on to a more alternative metal style, with their subsequent releases having eliminated rapping in almost all of their songs. Linkin Park's third studio album "Minutes to Midnight", released in 2007, was noted for its complete departure from the band's signature nu metal sound. Other nu metal bands such as Disturbed, Drowning Pool, and Slipknot moved onto a more standard heavy metal sound, while others, such as Staind and Papa Roach went for lighter sounds, such as post-grunge and hard rock. Nu metal bands Korn and Mudvayne still managed to experience popularity during the mid-2000s while also not completely abandoning the genre. Korn had popular hits such as "Coming Undone" and "Twisted Transistor", although the band added slight industrial influences and moved onto a more commercially acceptable sound, with pop producers The Matrix helping produce the band's 2005 album "See You on the Other Side". Some critics cited Mudvayne's 2005 album Lost and Found as a change in the band's musical style.
2010s and slight revival.
Despite the lack of radio play and popularity, some nu metal bands still gain critical and commercial success. Korn's 9th studio album "", sold 63,000 copies during its first week in the US, landing at number two on the "Billboard" 200. As of December 6, 2011, the album has sold 185,000 units in the U.S. and received positive reviews. In 2011, Limp Bizkit's long awaited sixth studio album "Gold Cobra", sold 27,000 copies during its first week in the United States and peaking at number 16 on the "Billboard" 200 and the album has received mostly positive reviews. Also in 2011, Staind's self-titled album was a return to their early days of nu metal. The album debuted at number 5 on the "Billboard" 200, with first sales week of 47,000 copies, making the fifth consecutive top-five album for Staind.
Evanescence's self-titled album debuted at number 1 on the Billboard 200 and other U.S charts and sold over 127,000 copies in the first week. On 2 December 2011, Korn released "The Path of Totality" selling 55,000 copies in its first week. Many cited this album as a new direction for nu metal, with the band taking influence from electronic music, most notably dubstep. Artists collaborating on the album included Skrillex, 12th Planet, Kill the Noise, Jon Gooch and Excision. The album received mostly positive reviews, winning a Revolver Golden God award for best album. This has led to some talk within the media of a possible nu metal revival.
Nu metal-influenced metalcore and deathcore bands such as Emmure, Here Comes the Kraken, Suicide Silence, Of Mice & Men and Issues gained moderate popularity in the 2010s. Recently, Linkin Park released their sixth record The Hunting Party which featured a return to their heavier style of nu metal and rap metal. The album peaked at number three on the "Billboard" 200 chart behind Lana Del Rey's "Ultraviolence" and Sam Smith's "In the Lonely Hour", with first-week sales of 110,000 copies in the United States. Their song "Until It's Gone" was nominated for the Best Rock Video category on the 2014 MTV Video Music Awards, but lost to Lorde's Royals. MTV also held a chance for fans to meet the band as well. Slipknot released "" in 2014, which featured a return to their nu metal roots from their earlier albums while still keeping elements of their newer material. It debuted at #1 on the "Billboard" 200 with 132,000 copies sold.
Criticism.
Nu metal has been often criticized by fans of some other metal genres, often being labelled derogatory terms such as "mallcore". Gregory Heaney of AllMusic has described the genre as "one of metal's more unfortunate pushes into the mainstream." Jonathan Davis, the frontman of the pioneering nu metal band Korn, was in an interview and said 
Some bands considered influential to nu metal have tried to distance themselves from the genre. Regarding his band's influence on nu metal, Faith No More and Mr. Bungle singer Mike Patton said "I feel no responsibility for that, it's their mothers' fault, not mine." While Helmet frontman Page Hamilton has stated "It's frustrating that people write [us] off because we're affiliated with or credited with or discredited with creating nu-metal and rap metal or whatever the fuck it is, which we sound nothing like." In response to reports that Fred Durst, lead singer of nu metal band Limp Bizkit is a big fan of his band, Tool's lead singer Maynard James Keenan said "If the lunch-lady in high school hits on you, you appreciate the compliment, but you’re not really gonna start dating the lunch-lady, are ya?" Although Trent Reznor of Nine Inch Nails defended some of the members of Korn during a MTV interview, he has also criticized the genre, saying in an interview with "Kerrang!" 
As the band had abandoned the nu metal sound often featured on their early work, Deftones' frontman Chino Moreno began to criticize the genre, especially Korn's 2002 release "Untouchables" saying "As Korn go on, it's the same things—bad childhoods and mean moms. It gets too old after a while. How old is Jonathan? Thirty? How long has it been since he lived with his parents?" Korn's frontman Jonathan Davis responded to it in an interview saying "Obviously, Chino hasn't listened to the words on the rest of my albums because they're nothing about my parents or my childhood. He's bitter and pissed off. I haven't talked to him because that's some straight fucked up shit that he said. When we first came out it was cool and we were homies. Then as we came up they became bitter because we were getting more attention or some shit. It's retarded how it got like that.

</doc>
<doc id="22091" url="http://en.wikipedia.org/wiki?curid=22091" title="Ncurses">
Ncurses

ncurses (new curses) is a programming library that provides an API which allows the programmer to write text-based user interfaces in a terminal-independent manner. It is a toolkit for developing "GUI-like" application software that runs under a terminal emulator. It also optimizes screen changes, in order to reduce the latency experienced when using remote shells.
History.
The "N" in ncurses comes from the word "new". This is because ncurses is a free software emulation (clone) of the System V Release 4.0 (SVr4) curses, which was itself an enhancement over the discontinued classic 4.4 BSD curses. The XSI Curses standard issued by X/Open is explicitly and closely modeled on System V.
curses.
The first curses library was developed at the University of California at Berkeley, for a BSD operating system, around 1980 to support a screen-oriented game. It originally used the termcap library, which was used in other programs, such as the vi editor.
The success of the BSD curses library prompted Bell Labs to release an enhanced curses library in their System V Release 2 Unix systems. This library was more powerful and instead of using termcap, it used terminfo. However, due to AT&T policy regarding source-code distribution, this improved curses library did not have much acceptance in the BSD community.
pcurses.
Around 1982, Pavel Curtis started work on a freeware clone of the Bell Labs curses, named pcurses, which was maintained by various people through 1986.
ncurses.
The pcurses library was further improved when Zeyd Ben-Halim took over the development effort in late 1991. The new library was released as ncurses in November 1993, with version 1.8.1 as the first major release. Subsequent work, through version 1.8.8 (1995), was driven by Eric S. Raymond, who added the form and menu libraries written by Juergen Pfeifer. Since 1996, it has been maintained by Thomas E. Dickey.
Most ncurses calls can be easily ported to the old curses. System V curses implementations can support BSD curses programs with just a recompilation. However, a few areas are problematic, such as handling terminal resizing, since no counterpart exists in the old curses.
Terminal database.
Ncurses can use either terminfo (with extensible data) or termcap. Other implementations of curses generally use terminfo; a minority use termcap. Few (mytinfo was an older exception) use both.
License.
Ncurses is a part of the GNU Project. It is one of the few GNU files not distributed under the GNU GPL or LGPL; it is distributed under a permissive free software licence, similar to the MIT License. This is due to the agreement made with the Free Software Foundation at the time the developers assigned their copyright.
When the agreement was made to pass on the rights to the FSF, there was a clause that stated
The Foundation promises that all distribution of the Package, or of any work "based on the Package", that takes place under the control of the Foundation or its agents or assignees, shall be on terms that explicitly and perpetually permit anyone possessing a copy of the work to which the terms apply, and possessing accurate notice of these terms, to redistribute copies of the work to anyone on the same terms.
According to the maintainer Thomas E. Dickey, this precludes relicensing to the GPL in any version, since it would place restrictions on the programs that will be able to link to the libraries.
Programs using ncurses.
There are hundreds of programs which use ncurses. Some, such as GNU Screen and w3m, use only the termcap interface, performing screen management within the application. Others, such as GNU Midnight Commander and YaST, use the curses programming interface.

</doc>
<doc id="22092" url="http://en.wikipedia.org/wiki?curid=22092" title="NBA (disambiguation)">
NBA (disambiguation)

NBA commonly refers to the National Basketball Association, the pre-eminent men's professional basketball league in North America.
NBA may also refer to:

</doc>
<doc id="22093" url="http://en.wikipedia.org/wiki?curid=22093" title="National Basketball Association">
National Basketball Association

The National Basketball Association (NBA) is the pre-eminent men's professional basketball league in North America, and is widely considered to be the premier men's professional basketball league in the world. It has 30 franchised member clubs (29 in the United States and 1 in Canada), and is an active member of USA Basketball (USAB), which is recognized by FIBA (also known as the International Basketball Federation) as the national governing body for basketball in the United States. The NBA is one of the four major North American professional sports leagues. NBA players are the world's best paid sportsmen, by average annual salary per player.
The league was founded in New York City on June 6, 1946, as the Basketball Association of America (BAA). The league adopted the name National Basketball Association on August 3, 1949, after merging with its rival National Basketball League (NBL). The league's several international as well as individual team offices are directed out of its head offices located in the Olympic Tower at 645 Fifth Avenue in New York City. NBA Entertainment and NBA TV studios are directed out of offices located in Secaucus, New Jersey.
History.
Creation and merger.
The Basketball Association of America was founded in 1946 by owners of the major ice hockey arenas in the Northeastern and Midwestern United States and Canada. On November 1, 1946, in Toronto, Ontario, Canada, the Toronto Huskies hosted the New York Knickerbockers at Maple Leaf Gardens, in a game the NBA now regards as the first played in its history. The first basket was made by Ossie Schectman of the Knickerbockers. Although there had been earlier attempts at professional basketball leagues, including the American Basketball League and the NBL, the BAA was the first league to attempt to play primarily in large arenas in major cities. During its early years, the quality of play in the BAA was not significantly better than in competing leagues or among leading independent clubs such as the Harlem Globetrotters. For instance, the 1948 ABL finalist Baltimore Bullets moved to the BAA and won that league's 1948 title, and the 1948 NBL champion Minneapolis Lakers won the 1949 BAA title. Prior to the 1948-49 season, however, NBL teams from Fort Wayne, Indianapolis, Minneapolis, and Rochester jumped to the BAA, which established the BAA as the league of choice for collegians looking to turn professional.
Following the 1948-49 season, the BAA took in the remainder of the NBL: Syracuse, Anderson, Tri-Cities, Sheboygan, Denver, and Waterloo. In deference to the merger and to avoid possible legal complications, the league name was changed from the BAA to the National Basketball Association in spite of having the same BAA governing body including Podoloff. The new league had seventeen franchises located in a mix of large and small cities, as well as large arenas and smaller gymnasiums and armories. In 1950, the NBA consolidated to eleven franchises, a process that continued until 1953–54, when the league reached its smallest size of eight franchises, all of which are still in the league (the New York Knicks, Boston Celtics, Golden State Warriors, Los Angeles Lakers, Royals/Kings, Detroit Pistons, Atlanta Hawks, and Nationals/76ers). The process of contraction saw the league's smaller-city franchises move to larger cities. The Hawks shifted from the "Tri-Cities" (the area now known as the Quad Cities), to Milwaukee in 1951, and then to St. Louis, Missouri in 1955; the Royals from Rochester, New York to Cincinnati in 1957; and the Pistons from Fort Wayne, Indiana to Detroit in 1957.
Japanese-American Wataru Misaka broke the NBA color barrier in the 1947–48 season when he played for the New York Knicks. He remained the only non-white player in league history prior to the first African-American, Harold Hunter, signing with the Washington Capitols in 1950. Hunter was cut from the team during training camp, but several African-American players did play in the league later that year, including Chuck Cooper with the Celtics, Nathaniel "Sweetwater" Clifton with the Knicks, and Earl Lloyd with the Washington Capitols. During this period, the Minneapolis Lakers, led by center George Mikan, won five NBA Championships and established themselves as the league's first dynasty. To encourage shooting and discourage stalling, the league introduced the 24-second shot clock in 1954. If a team does not attempt to score a field goal (or the ball fails to make contact with the rim) within 24 seconds of obtaining the ball, play is stopped and the ball given to its opponent.
Celtics' dominance, league expansion, and competition.
In 1957, rookie center Bill Russell joined the Boston Celtics, who already featured guard Bob Cousy and coach Red Auerbach, and went on to lead the club to eleven NBA titles in thirteen seasons. Center Wilt Chamberlain entered the league with the Warriors in 1959 and became a dominant individual star of the 1960s, setting new single game records in scoring (100) and rebounding (55). Russell's rivalry with Chamberlain became one of the greatest rivalries in the history of American team sports.
The 1960s were dominated by the Celtics. Led by Russell, Bob Cousy and coach Red Auerbach, Boston won eight straight championships in the NBA from the 1959–66. This championship streak is the longest in NBA history. They did not win the title in 1966-67, but regained it in the 1967-68 season and repeated in 1969. The domination totaled nine of the ten championship banners of the 1960s.
Through this period, the NBA continued to strengthen with the shift of the Minneapolis Lakers to Los Angeles, the Philadelphia Warriors to San Francisco, the Syracuse Nationals to Philadelphia to become the Philadelphia 76ers, and the St. Louis Hawks moving to Atlanta, as well as the addition of its first expansion franchises. The Chicago Packers (now Washington Wizards) became the ninth NBA team in 1961. From 1966 to 1968, the league expanded from 9 to 14 teams, introducing the Chicago Bulls, Seattle SuperSonics (now Oklahoma City Thunder), San Diego Rockets (who relocated to Houston four years later), Milwaukee Bucks, and Phoenix Suns.
In 1967, the league faced a new external threat with the formation of the American Basketball Association (ABA). The leagues engaged in a bidding war. The NBA landed the most important college star of the era, Kareem Abdul-Jabbar (then known as Lew Alcindor). However, the NBA's leading scorer, Rick Barry, jumped to the ABA, as did four veteran referees—Norm Drucker, Earl Strom, John Vanak, and Joe Gushue.
In 1969, Alan Siegel, who oversaw the design of Jerry Dior's Major League Baseball logo a year prior, created the modern NBA logo inspired by the MLB's. It incorporates the silhouette of the legendary Jerry West based on a photo by Wen Roberts, although NBA officials denied a particular player as being its influence because, according to Siegel, "They want to institutionalize it rather than individualize it. It's become such a ubiquitous, classic symbol and focal point of their identity and their licensing program that they don't necessarily want to identify it with one player." The iconic logo debuted in 1971 and would remain a fixture of the NBA brand.
The ABA succeeded in signing a number of major stars in the 1970s, including Julius Erving of the Virginia Squires, in part because it allowed teams to sign college undergraduates. The NBA expanded rapidly during this period, one purpose being to tie up the most viable cities. From 1966 to 1974, the NBA grew from nine franchises to 18. In 1970, the Portland Trail Blazers, Cleveland Cavaliers, and Buffalo Braves (now the Los Angeles Clippers) all made their debuts expanding the league to 17. The New Orleans Jazz (now in Utah) came aboard in 1974 bringing the total to 18. Following the 1976 season, the leagues reached a settlement that provided for the addition of four ABA franchises to the NBA, raising the number of franchises in the league at that time to 22. The franchises added were the San Antonio Spurs, Denver Nuggets, Indiana Pacers, and New York Nets (now the Brooklyn Nets). Some of the biggest stars of this era were Kareem Abdul-Jabbar, Rick Barry, Dave Cowens, Julius Erving, Elvin Hayes, Walt Frazier, Moses Malone, Artis Gilmore, George Gervin, Dan Issel, and Pete Maravich. The end of the decade, however, saw declining TV ratings, low attendance and drug-related player issues – both perceived and real – that threatened to derail the NBA.
Surging popularity.
The league added the ABA's innovative three-point field goal beginning in 1979 to open up the game. That same year, rookies Larry Bird and Magic Johnson joined the Boston Celtics and Los Angeles Lakers respectively, initiating a period of significant growth in fan interest in the NBA throughout the country and the world. In 1984 they played against each other for the first time in the NBA Finals. Johnson went on to lead the Lakers to five titles, and Bird went on to lead the Celtics to three. Also in the early 1980s, the NBA added one more expansion franchise, the Dallas Mavericks, bringing the total to 23 teams. Later on, Larry Bird won the first three three-point shooting contests. Former league commissioner David Stern who took office on February 1, 1984 before retiring February 1, 2014, oversaw the expansion and growth of the NBA to a global commodity.
Michael Jordan entered the league in 1984 with the Chicago Bulls, providing an even more popular star to support growing interest in the league. This resulted in more cities demanding teams of their own. In 1988 and 1989, four cities got their wishes as the Charlotte Hornets, Miami Heat, Orlando Magic, and Minnesota Timberwolves made their NBA debuts, bringing the total to 27 teams. In the first year of the 1990s, the Detroit Pistons would win the second of their back-to-back titles, led by coach Chuck Daly and guard Isiah Thomas. Jordan and Scottie Pippen would lead the Bulls to two three-peats in eight years during the 1991–98 seasons. Hakeem Olajuwon won back-to-back titles with the Houston Rockets in 1994 and 1995.
The 1992 Olympic basketball Dream Team, the first to use current NBA stars, featured Michael Jordan as the anchor, along with Bird, Johnson, David Robinson, Patrick Ewing, Scottie Pippen, Clyde Drexler, Karl Malone, John Stockton, Chris Mullin, Charles Barkley, and Christian Laettner. Eleven players on the Dream Team have been inducted individually into the Basketball Hall of Fame.
In 1995, the NBA expanded to Canada with the addition of the Vancouver Grizzlies and the Toronto Raptors. In 2001, the Vancouver Grizzlies relocated to Memphis, which left the Raptors as the only Canadian team in the NBA.
In 1996, the NBA created a women's league, the Women's National Basketball Association (WNBA).
In 1998, the NBA owners began a lockout which lasted 191 days and was settled on January 18, 1999. As a result of this lockout the 1998–99 NBA season was reduced from 82 to 50 games (61% of a normal season), and the All-Star Game was cancelled. The San Antonio Spurs won their first championship, and first by a former ABA team, by beating the New York Knicks, who were the first, and to this date, the only, eighth seed to ever make it to the NBA Finals.
Modern era.
Since the breakup of the Chicago Bulls championship roster in the summer of 1998, the Western Conference has dominated, with the Los Angeles Lakers and San Antonio Spurs combining to win the title in nine of fourteen years. Tim Duncan and David Robinson won the 1999 championship with the Spurs, and Shaquille O'Neal and Kobe Bryant started the 2000s with three consecutive championships for the Lakers. The Spurs reclaimed the title in 2003 against the Nets. In 2004, the Lakers returned to the Finals, only to fall in five games to the Detroit Pistons.
After the Spurs took home the Larry O'Brien Championship Trophy in 2005, the 2006 Finals featured two franchises making their inaugural Finals appearances. The Miami Heat, led by their star shooting guard, Dwyane Wade, and Shaquille O'Neal, who had been traded from the Lakers during the 2004 summer, won the series over the Dallas Mavericks in six after losing the first two games. The Lakers/Spurs dominance continued in 2007 with a four-game sweep by the Spurs over the Cleveland Cavaliers, who were led by LeBron James. The 2008 Finals saw a rematch of the league's highest profile rivalry, the Boston Celtics and Los Angeles Lakers, with the Celtics winning, for their 17th championship, thanks to their new big three of Paul Pierce, Ray Allen, and Kevin Garnett.
In 2009, the Lakers with Kobe Bryant returned to the Finals, this time defeating the Dwight Howard-led Orlando Magic. Kobe Bryant won his first Bill Russell NBA Finals Most Valuable Player Award in his 13th season after leading the Lakers to their first NBA championship since the departure of Shaquille O'Neal.
The 2010 NBA All-Star Game was held at Cowboys Stadium in front of the largest crowd ever, 108,713. At the end of that season, the Celtics and the Lakers renewed their rivalry from 2008 when they met again in the NBA Finals for a record 12th time. The Lakers won the title in Game 7, 83–79. Before the start of the 2010–11 season the NBA had an exciting summer with one of the most anticipated free agent classes of all time. Two of which signed, and one resigned, with the Miami Heat, leading to a season that was heavily centered on their eventual success or failure at taking home the championship. The Heat, led by LeBron James, Dwyane Wade, and Chris Bosh, did in fact make the Finals against the Dallas Mavericks, in a rematch for the franchises of the 2006 Finals. The Mavericks, led by Dirk Nowitzki (the eventual NBA Finals MVP), took the series in six games. This was the Mavericks' first title. Other veterans like Shawn Marion, Jason Kidd, Jason Terry, and Peja Stojaković also won their first titles with Nowitzki.
On July 1, 2011, at 12:01 am, the NBA announced another lockout. After the first few weeks of the season were canceled, the players and owners ratified a new collective bargaining agreement on December 8, 2011, setting up a shortened 66-game season. Following the shortened season, the Miami Heat made a return to the Finals with the trio of Dwyane Wade, Lebron James and Chris Bosh against Oklahoma City's Kevin Durant, Russell Westbrook and James Harden. The Heat went on to defeat the Thunder in five games, capturing their second NBA title in six years.
International influence.
Following pioneers like Vlade Divac (Serbia) and Dražen Petrović (Croatia) who joined the NBA in the late 1980s, an increasing number of international players have moved directly from playing elsewhere in the world to starring in the NBA. Below is a short list of foreign players who have won NBA awards or have been otherwise recognized for their contributions to basketball, either currently or formerly active in the league:
On some occasions, young players, most but not all from the English-speaking world, have attended U.S. colleges before playing in the NBA. Notable examples are:
Since 2006, the NBA has faced Euroleague teams in exhibition matches in the NBA Europe Live Tour and since 2009 in the Euroleague American Tour.
The 2013–14 season opened with a record 92 international players on the opening night rosters, representing 39 countries and comprising over 20% of the league The NBA defines "international" players as those born outside the 50 United States and Washington, D.C. This means that:
Other developments.
In 2001, an affiliated minor league, the National Basketball Development League, now called the NBA Development League (or D-League) was created. Before the league was started, there were strong rumors that the NBA would purchase the Continental Basketball Association, and call it its developmental league.
In 2004, two years after the Hornets' relocation to New Orleans, the NBA returned to North Carolina as the Charlotte Bobcats were formed as an expansion team.
In 2005, the Hornets relocated to Oklahoma City for two seasons because of damage caused by Hurricane Katrina. In 2007, the Hornets returned to New Orleans.
On June 28, 2006, a new official game ball was introduced for the 2006–07 season, marking the first change to the ball in over 35 years and only the second ball in 60 seasons. Manufactured by Spalding, the new ball featured a new design and new synthetic material that Spalding claimed offered a better grip, feel, and consistency than the original ball. However, many players were vocal in their disdain for the new ball, saying that it was too sticky when dry, and too slippery when wet.
On December 11, 2006, Commissioner Stern announced that beginning January 1, 2007, the NBA would return to the traditional leather basketball in use prior to the 2006–07 season. The change was influenced by frequent player complaints and confirmed hand injuries (cuts) caused by the microfiber ball. The Players' Association had filed a suit in behalf of the players against the NBA over the new ball. As of 2006, the NBA team jerseys are manufactured by Adidas, which purchased the previous supplier, Reebok.
On July 19, 2007, the Federal Bureau of Investigation investigated allegations that veteran NBA referee Tim Donaghy bet on basketball games he officiated over the past two seasons and that he made calls affecting the point spread in those games. On August 15, 2007, Donaghy pleaded guilty to two federal charges related to the investigation. However, he could face additional charges if it is determined that he deliberately miscalled individual games. Donaghy claimed in 2008 that certain refs were friendly with players and "company men" for the NBA. Donaghy alleged that refs influenced the outcome of certain playoff and finals games in 2002 and 2005. NBA commissioner David Stern denied the allegations and said Donaghy was a convicted felon and a "singing, cooperating witness". Donaghy served 15 months in prison and was released in November 2009. According to an independent study by Ronald Beech of Game 6 of the NBA 2002 Western Conference Finals between the Lakers and Kings, although the refs increased the Lakers' chances of winning through foul calls during the game, there was no collusion to fix the game. On alleged "star treatment" during Game 6 by the refs toward certain players, Beech claimed, "there does seem to be issues with different standards and allowances for different players." 
On July 2, 2008, it was announced that the Seattle SuperSonics would relocate to Oklahoma City. The Oklahoma City Thunder began playing in the 2008–09 season.
On October 11, 2008, the Phoenix Suns and the Denver Nuggets played the first outdoor game in the modern era of the NBA at the Indian Wells Tennis Garden.
On September 1, 2009, the contract between the NBA and its referees expired, creating a referee lockout. On October 1, 2009, the first preseason games were played and replacement referees from the WNBA and NBA Development League were used. The last time replacement referees were used was the beginning of the 1995–96 season. The NBA and the regular referees reached a deal on October 23, 2009.
In 2011, the first official NBA league games on European ground took place. In two matchups, the New Jersey Nets faced the Toronto Raptors in London's O2 Arena in front of over 20,000 fans.
In July 2011, the NBA laid off around 114 league employees (about 11 percent of all the league office workforce) to save money.
The 2011–12 NBA season, scheduled to begin November 1, 2011, with a matchup between the defending champion Dallas Mavericks and the Chicago Bulls, was postponed due to a labor dispute. The lockout officially ended on December 8, 2011, when players and owners ratified a new collective bargaining agreement, and the season began on Christmas Day.
On April 30, 2012, the New Jersey Nets officially changed their name to the Brooklyn Nets. They began playing in the New York City borough of Brooklyn in the 2012–13 season.
In October 2012, the NBA announced that it would begin fining players for flopping.
After the 2012–13 season, the New Orleans Hornets renamed themselves the Pelicans. During the 2013-14 season, Stern retired as commissioner after 30 years, and deputy commissioner Adam Silver ascended to the position of commissioner. During that season's playoffs, the Bobcats officially reclaimed the Hornets name, and by agreement with the league and the Pelicans, also received sole ownership of all history, records, and statistics from the Pelicans' time in Charlotte. As a result, the Hornets are now officially considered to have been founded in 1988, suspended operations in 2002, and resumed in 2004 as the Bobcats, while the Pelicans are officially treated as a 2002 expansion team. (This is somewhat similar to the relationship between the Cleveland Browns and Baltimore Ravens in the NFL.)
In April 2014 Donald Sterling, an NBA owner at the time, received a lifetime ban from the NBA after racist remarks he made became public.
On August 5, 2014, Becky Hammon was hired by the San Antonio Spurs as an assistant coach, becoming the second female coach in NBA history but the first full-time coach. This also makes her the first full-time female coach in any of the four major professional sports in North America.
Teams.
Raptors 
Celtics 
Knicks 
Nets 
76ers 
Bulls 
Cavaliers 
Pistons 
Pacers 
Bucks 
Hawks 
Hornets 
Heat 
Magic 
Wizards 
Mavericks 
Rockets 
Grizzlies 
Pelicans 
Spurs 
Nuggets 
Timberwolves 
Trail Blazers 
Thunder 
Jazz 
Warriors 
Kings 
Suns 
Clippers 
Lakers 
The NBA originated in 1946 with 11 teams, and through a sequence of team expansions, reductions, and relocations currently consists of 30 teams. The United States is home to 29 teams and one is located in Canada.
The current league organization divides thirty teams into two conferences of three divisions with five teams each. The current divisional alignment was introduced in the 2004–05 season. Reflecting the population distribution of the United States and Canada as a whole, most teams are in the eastern half of the country: thirteen teams are in the Eastern Time Zone, nine in the Central, three in the Mountain, and five in the Pacific.
</dl>
Regular season.
Following the summer break, teams begin training camps in late September. Training camps allow the coaching staff to evaluate players (especially rookies), scout the team's strengths and weaknesses, prepare the players for the rigorous regular season, and determine the 12-man active roster (and a 3-man inactive list) with which they will begin the regular season. Teams have the ability to assign players with less than two years of experience to the NBA development league. After training camp, a series of preseason exhibition games are held. Preseason matches are sometimes held in non-NBA cities, both in the United States and overseas. The NBA regular season begins in the last week of October.
During the regular season, each team plays 82 games, 41 each home and away. A team faces opponents in its own division four times a year (16 games). Each team plays six of the teams from the other two divisions in its conference four times (24 games), and the remaining four teams three times (12 games). Finally, each team plays all the teams in the other conference twice apiece (30 games). This asymmetrical structure means the strength of schedule will vary between teams (but not as significantly as the NFL or MLB). Over five seasons, each team will have played 80 games against their division (20 games against each opponent, 10 at home, 10 on the road), 180 games against the rest of their conference (18 games against each opponent, 9 at home, 9 on the road), and 150 games against the other conference (10 games against each team, 5 at home, 5 on the road).
The NBA is one of only two of the four major professional sports leagues in the United States and Canada in which teams play every other team during the regular season (the other being the National Hockey League). Each team hosts and visits every other team at least once every season. From 2005 to 2008, the NBA had the distinction of being the only one of the four major leagues in which all teams play every other team.
The NBA is also the only league that regularly schedules games on Christmas Day. The league has been playing games regularly on the holiday since 1947, though the first Christmas Day games weren't televised until . Games played on this day have featured some of the best teams and players. Christmas is also notable for NBA on television, as the holiday is when the first NBA games air on network television each season. Games played on this day have been some of the highest-rated games during a particular season.
In February, the regular season pauses to celebrate the annual NBA All-Star Game. Fans vote throughout the United States, Canada, and on the Internet, and the top vote-getters at each position in each conference are given a starting spot on their conference's All-Star team. Coaches vote to choose the remaining 14 All-Stars. Then, Eastern conference players face the Western conference players in the All-Star game. The player with the best performance during the game is rewarded with a Game MVP award. Other attractions of the All-Star break include the Rising Stars Challenge (originally Rookie Challenge), where the top rookies and second-year players in the NBA play in a 5-on-5 basketball game, with the current format pitting U.S. players against those from the rest of the world; the Skills Challenge, where players compete to finish an obstacle course consisting of shooting, passing, and dribbling in the fastest time; the Three Point Contest, where players compete to score the most amount of three-point field goals in a given time; and the NBA Slam Dunk Contest, where players compete to dunk the ball in the most entertaining way according to the judges. These other attractions have varying names which include the names of the various sponsors who have paid for naming rights.
Shortly after the All-Star break is the trade deadline, which is set to fall on the 16th Thursday of the season (usually in February) at 3pm Eastern Time. After this date, teams are not allowed to exchange players with each other for the remainder of the season, although they may still sign and release players. Major trades are often completed right before the trading deadline, making that day a hectic time for general managers.
Around the middle of April, the regular season ends. It is during this time that voting begins for individual awards, as well as the selection of the honorary, league-wide, post-season teams. The Sixth Man of the Year Award is given to the best player coming off the bench (must have more games coming off the bench than actual games started). The Rookie of the Year Award is awarded to the most outstanding first-year player. The Most Improved Player Award is awarded to the player who is deemed to have shown the most improvement from the previous season. The Defensive Player of the Year Award is awarded to the league's best defender. The Coach of the Year Award is awarded to the coach that has made the most positive difference to a team. The Most Valuable Player Award is given to the player deemed the most valuable for (his team) that season. Additionally, "Sporting News" awards an unofficial (but widely recognized) Executive of the Year Award to the general manager who is adjudged to have performed the best job for the benefit of his franchise.
The post-season teams are the All-NBA Team, the All-Defensive Team, and the All-Rookie Team; each consists of five players. There are three All-NBA teams, consisting of the top players at each position, with first-team status being the most desirable. There are two All-Defensive teams, consisting of the top defenders at each position. There are also two All-Rookie teams, consisting of the top first-year players regardless of position.
Playoffs.
NBA Playoffs begin in late April, with eight teams in each conference competing for the Championship. The three division winners, along with the team with the next best record from the conference are given the top four seeds. The next four teams in terms of record are given the lower four seeds.
Having a higher seed offers several advantages. Since the first seed begins the playoffs playing against the eighth seed, the second seed plays the seventh seed, the third seed plays the sixth seed, and the fourth seed plays the fifth seed, having a higher seed means a team faces a weaker team in the first round. The team in each series with the better record has home court advantage, including the First Round. This means that, for example, if the team who receives the 5 seed has a better record than the team with the 4 seed (by virtue of a divisional championship), the 5 seed would have home court advantage, even though the other team has a higher seed. Therefore, the team with the best regular season record in the league is guaranteed home court advantage in every series it plays. For example, in 2006, the Denver Nuggets won 44 games and captured the Northwest Division and the #3 seed. Their opponent was the #6 seed Los Angeles Clippers, who won 47 games and finished second in the Pacific Division. Although Denver won its much weaker division, the Clippers had home-court advantage and won the series in 5.
The playoffs follow a tournament format. Each team plays an opponent in a best-of-seven series, with the first team to win four games advancing into the next round, while the other team is eliminated from the playoffs. In the next round, the successful team plays against another advancing team of the same conference. All but one team in each conference are eliminated from the playoffs. Since the NBA does not re-seed teams, the playoff bracket in each conference uses a traditional design, with the winner of the series matching the 1st and 8th seeded teams playing the winner of the series matching the 4th and 5th seeded teams, and the winner of the series matching the 2nd and 7th seeded teams playing the winner of the series matching the 3rd and 6th seeded teams. In every round, the best-of-7 series follows a 2–2–1–1–1 home-court pattern, meaning that one team will have home court in games 1, 2, 5, and 7, while the other plays at home in games 3, 4, and 6. From 1985 to 2013, the NBA Finals followed a 2–3–2 pattern, meaning that one team had home court in games 1, 2, 6, and 7, while the other played at home in games 3, 4, and 5.
The final playoff round, a best-of-seven series between the victors of both conferences, is known as the NBA Finals, and is held annually in June. The victor in the NBA Finals wins the Larry O'Brien Championship Trophy. Each player and major contributor—including coaches and the general manager—on the winning team receive a championship ring. In addition, the league awards the Bill Russell NBA Finals Most Valuable Player Award to the best performing player of the series.
On August 2, 2006, the NBA announced the new playoff format. The new format takes the three division winners and the second-place team with the best record and rank them 1–4 by record. The other 4 slots are filled by best record other than those other 4 teams. Previously, the top three seeds went to the division winners.
League championships.
The Boston Celtics have won the most championships with 17 NBA Finals wins. The second most successful franchise is the Los Angeles Lakers, who have 16 overall championships (11 in Los Angeles, 5 in Minneapolis). Following the Lakers are the Chicago Bulls with six championships, all of them over an 8-year span during the 1990s, and the San Antonio Spurs with five championships, all since 1999.
Current teams that have no NBA Finals appearances:
International competitions.
The National Basketball Association has sporadically participated in international club competitions. From 1987 to 1999 the NBA champions played against the continental champions of the Fédération Internationale de Basketball (FIBA) in the McDonald's Championship. This tournament was won by the NBA invitee every year it was held. FIBA is organizing a new World Club Championship to begin in 2010, and currently plans to invite the NBA champions starting in 2011.
Ticket prices.
In 2012, a ticket cost from $10 to $3,000 apiece, depending on the location of the seat and the success of the teams that were playing.
Awards.
List of National Basketball Association awards

</doc>
<doc id="22094" url="http://en.wikipedia.org/wiki?curid=22094" title="Nutation">
Nutation

Nutation (from Latin "nūtātiō", "nodding, swaying") is a rocking, swaying, or nodding motion in the axis of rotation of a largely axially symmetric object, such as a gyroscope, planet, or bullet in flight, or as an intended behavior of a mechanism. In an appropriate reference frame it can be defined as a change in the second Euler angle. If it is not caused by forces external to the body, it is called "free nutation" or "Euler nutation". A "pure nutation" is a movement of a rotational axis such that the first Euler angle is constant. In spacecraft dynamics, precession (a change in the first Euler angle) is sometimes referred to as nutation.
Rigid body.
If a top is set at a tilt on a horizontal surface and spun rapidly, its rotational axis starts precessing about the vertical. After a short interval, the top settles into a motion in which each point on its rotation axis follows a circular path. The vertical force of gravity produces a horizontal torque τ about the point of contact with the surface; the top rotates in the direction of this torque with an angular velocity Ω such that at any moment
where L is the instantaneous angular momentum of the top.
Initially, however, there is no precession, and the top falls straight downward. This gives rise to an imbalance in torques that starts the precession. In falling, the top overshoots the level at which it would precess steadily and then oscillates about this level. This oscillation is called "nutation". If the motion is damped, the oscillations will die down until the motion is a steady precession.
The physics of nutation in tops and gyroscopes can be explored using the model of a "heavy symmetrical top" with its tip fixed. Initially, the effect of friction is ignored. The motion of the top can be described by three Euler angles: the tilt angle "θ" between the symmetry axis of the top and the vertical; the azimuth "φ" of the top about the vertical; and the rotation angle "ψ" of the top about its own axis. Thus, precession is the change in "φ" and nutation is the change in "θ".
If the top has mass "M" and its center of mass is at a distance "l" from the pivot point, its gravitational potential relative to the plane of the support is
In a coordinate system where the "z" axis is the axis of symmetry, the top has angular velocities "ω"1, "ω"2, "ω"3 and moments of inertia "I"1, "I"2, "I"3 about the "x", "y", and "z" axes. The kinetic energy is
In terms of the Euler angles, this is
If the Euler–Lagrange equations are solved for this system, it is found that the motion depends on two constants "a" and "b" (each related to a constant of motion). The rate of precession is related to the tilt by
The tilt is determined by a differential equation for of the form
where "f" is a cubic polynomial that depends on parameters "a" and "b" as well as constants that are related to the energy and the gravitational torque. The roots of "f" are cosines of the angles at which the rate of change of "θ" is zero. One of these is not related to a physical angle; the other two determine the upper and lower bounds on the tilt angle, between which the gyroscope oscillates.
Astronomy.
The nutation of a planet happens because of gravitational attraction of other bodies that cause the precession of the equinoxes to vary over time so that the speed of precession is not constant. The nutation of the axis of the Earth was discovered in 1728 by the British astronomer James Bradley, but this nutation was not explained in detail until 20 years later by the Swiss mathematician Leonhard Euler.
Because the dynamic motions of the planets are so well known, their nutations can be calculated to within arcseconds over periods of many decades. There is another disturbance of the Earth's rotation called polar motion that can be estimated for only a few months into the future because it is influenced by rapidly and unpredictably varying things such as ocean currents, wind systems, and hypothesised motions in the liquid nickel-iron outer core of the Earth.
Values of nutations are usually divided into components parallel and perpendicular to the ecliptic. The component that works along the ecliptic is known as the "nutation in longitude". The component perpendicular to the ecliptic is known as the "nutation in obliquity". Celestial coordinate systems are based on an "equator" and "equinox", which means a great circle in the sky that is the projection of the Earth's equator outwards, and a line, the Vernal equinox intersecting that circle, which determines the starting point for measurement of right ascension. These items are affected both by precession of the equinoxes and nutation, and thus depend on the theories applied to precession and nutation, and on the date used as a reference date for the coordinate system. In simpler terms, nutation (and precession) values are important in observation from Earth for calculating the apparent positions of astronomical objects.
Earth.
Nutation makes a small change to the angle at which the Earth tilts with respect to the Sun, changing the location of the major circles of latitude that are defined by the Earth's tilt (the tropical circles and the polar circles).
In the case of the Earth, the principal sources of tidal force are the Sun and Moon, which continuously change location relative to each other and thus cause nutation in Earth's axis. The largest component of Earth's nutation has a period of 18.6 years, the same as that of the precession of the Moon's orbital nodes. However, there are other significant periodical terms that must be calculated depending on the desired accuracy of the result. A mathematical description (set of equations) that represents nutation is called a "theory of nutation". In the theory, parameters are adjusted in a more or less "ad hoc" method to obtain the best fit to data. Simple rigid body dynamics do not give the best theory; one has to account for deformations of the Earth, including mantle inelasticity and changes in the core–mantle boundary.
The principal term of nutation is due to the regression of the Moon's nodal line and has the same period of 6798 days (18.61 years). It reaches plus or minus 17″ in longitude and 9.2″ in obliquity. All other terms are much smaller; the next-largest, with a period of 183 days (0.5 year), has amplitudes 1.3″ and 0.6″ respectively. The periods of all terms larger than 0.0001″ (about as accurately as one can measure) lie between 5.5 and 6798 days; for some reason (as with ocean tidal periods) they seem to avoid the range from 34.8 to 91 days, so it is customary to split the nutation into long-period and short-period terms. The long-period terms are calculated and mentioned in the almanacs, while the additional correction due to the short-period terms is usually taken from a table.
In popular culture.
In the 1961 movie "The Day The Earth Caught Fire", the near-simultaneous detonation of two super-hydrogen bombs near the poles causes a change in the Earth's nutation, as well as an 11 degree shift in the axis of rotation and a change in the Earth's orbit around the Sun.
The verb "to nutate" was used by MIT physicist Peter Fisher on the television show Late Night with Conan O'Brien on February 8, 2008. Fisher used the term to describe the motion of a spinning ring as it began to slow down and wobble.
Further reading.
</dl>

</doc>
<doc id="22096" url="http://en.wikipedia.org/wiki?curid=22096" title="NASCO">
NASCO

NASCO or Nasco may refer to:

</doc>
<doc id="22097" url="http://en.wikipedia.org/wiki?curid=22097" title="North Atlantic Salmon Conservation Organization">
North Atlantic Salmon Conservation Organization

The North Atlantic Salmon Conservation Organization (NASCO) is an international organization established under the Convention for the Conservation of Salmon in the North Atlantic Ocean from October 1 1983. 
The organization's mission is to contribute through consultation and cooperation to the conservation, restoration, enhancement and rational management of salmon stocks.
Its headquarters are in Edinburgh, United Kingdom.
Membership.
Current participants (since 1984): Canada, Denmark (in respect of the Faroe Islands and Greenland), the European Union, Norway, Russian Federation, and the United States of America.
Former participants:

</doc>
<doc id="22099" url="http://en.wikipedia.org/wiki?curid=22099" title="Narcissus (mythology)">
Narcissus (mythology)

In Greek mythology, Narcissus (; Greek: Νάρκισσος, "Narkissos") was a hunter from Thespiae in Boeotia who was known for his beauty. He was the son of the river god Cephissus and nymph Liriope. He was proud, in that he disdained those who loved him. Nemesis noticed this behavior and attracted Narcissus to a pool, where he saw his own reflection in the water and fell in love with it, not realizing it was merely an image. Unable to leave the beauty of his reflection, Narcissus drowned. Narcissus is the origin of the term "narcissism", a fixation with oneself and one's physical appearance.
Etymology.
The name is of uncertain etymology. According to R. S. P. Beekes, "[t]he suffix [-ισσος] clearly points to a Pre-Greek word."
Ancient sources.
Multiple versions of the myth have survived from ancient sources. The classic version is by Ovid, found in book 3 of his "Metamorphoses" (completed 8 AD); this is the story of Narcissus and Echo. One day Narcissus was walking in the woods when Echo, an Oread (mountain nymph) saw him, fell deeply in love, and followed him. Narcissus sensed he was being followed and shouted "Who's there?". Echo repeated "Who's there?". She eventually revealed her identity and attempted to embrace him. He stepped away and told her to leave him alone. She was heartbroken and spent the rest of her life in lonely glens until nothing but an echo sound remained of her. Nemesis, the goddess of revenge, learned of this story and decided to punish Narcissus. She lured him to a pool where he saw his own reflection. He didn't realize it was only an image and fell in love with it. He eventually realized that his love could not be addressed and committed suicide.
An earlier version ascribed to the poet Parthenius of Nicaea, composed around 50 BC, was recently rediscovered among the Oxyrhynchus papyri at Oxford. Like Ovid's version, it ends with Narcissus committing suicide. A version by Conon, a contemporary of Ovid, also ends in suicide ("Narrations," 24). In it, a young man named Aminias fell in love with Narcissus, who had already spurned his male suitors. Narcissus also spurned him and gave him a sword. Aminias committed suicide at Narcissus's doorstep. He had prayed to the gods to give Narcissus a lesson for all the pain he provoked. Narcissus walked by a pool of water and decided to drink some. He saw his reflection, became entranced by it, and killed himself because he could not have his object of desire. A century later the travel writer Pausanias recorded a novel variant of the story, in which Narcissus falls in love with his twin sister rather than himself ("Guide to Greece", 9.31.7).
Influence on culture.
Тhe myth of Narcissus has inspired artists for at least two thousand years, even before the Roman poet Ovid featured a version in book III of his "Metamorphoses". This was followed in more recent centuries by other poets (e.g. Keats and Alfred Edward Housman) and painters (Caravaggio, Poussin, Turner, Dalí (see "Metamorphosis of Narcissus"), and Waterhouse).
Narcissus in literature.
In Stendhal's novel "Le Rouge et le Noir" (1830), there is a classic narcissist in the character of Mathilde. Says Prince Korasoff to Julien Sorel, the protagonist, with respect to his beloved girl:
She looks at herself instead of looking at you, and so doesn't know you.
During the two or three little outbursts of passion she has allowed herself in your favor, she has, by a great effort of imagination, seen in you the hero of her dreams, and not yourself as you really are.<br>
(Page 401, 1953 Penguin Edition, trans. Margaret R.B. Shaw).
The myth had a decided influence on English Victorian homoerotic culture, via André Gide's study of the myth, "Le Traité du Narcisse" ('The Treatise of the Narcissus', 1891), and the only novel by Oscar Wilde, "The Picture of Dorian Gray".
Paulo Coelho's "The Alchemist" also starts with a story about Narcissus, found (we are told) by the alchemist in a book brought by someone in the caravan. The alchemist's (and Coelho's) source was very probably Hesketh Pearson's "The Life of Oscar Wilde" (1946) in which this story is recorded (Penguin edition, p. 217) as one of Wilde's inspired inventions. This version of the Narcissus story is based on Wilde's "The Disciple" from his "Poems in Prose (Wilde) ".
Author and poet Rainer Maria Rilke visits the character and symbolism of Narcissus in several of his poems.
Seamus Heaney references Narcissus in his poem "Personal Helicon" from his first collection "Death of a Naturalist":"To stare, big-eyed Narcissus, into some spring<br>
Is beneath all adult dignity."
In Rick Riordan's "Heroes of Olympus series," Narcissus appears as a minor antagonist in the third book "The Mark of Athena".
In the fantasy series "Harry Potter", Narcissa Malfoy, a minor antagonist, is named for Narcissus.
William Faulkner's character "Narcissa" in "Sanctuary", sister of Horace Benbow, was also named after Narcissus. Throughout the novel, she allows the arrogant, pompous pressures of high-class society to overrule the unconditional love that she should have for her brother.
Hermann Hesse's character "Narcissus" in "Narcissus and Goldmund" shares several of mythycal narcissus' traits, although his narcissism is based on his intellect rather than his physical beauty.
A. E. Housman refers to the 'Greek Lad', Narcissus, in his poem "Look not in my Eyes" from "A Shropshire Lad" set to music by several English composers including George Butterworth. At the end of the poem stands a jonquil, a variety of daffodil, Narcissus Jonquilla, which like Narcissus looks sadly down into the water.
Herman Melville references the myth of Narcissus in his novel Moby-Dick, in which Ishmael explains the myth as "the key to it all," referring to the greater theme of finding the essence of Truth through the physical world.
On Sophia de Mello Breyner Andresen's A Fada Oriana, the eponymous protagonist is punished with mortality for abandoning her duties in order to stare at herself in the surface of a river.
Narcissus on film.
In the TV series Boardwalk Empire, a Dr. Narcisse (Valentin Narcisse) who is introduced as a condescending intellectual.
Scottish-Canadian animator Norman McLaren finished his career with a short film named "Narcissus", re-telling the Greek legend through ballet.
Narcissus appears in the Disney adaptation of "Hercules". In the film, he is portrayed as an Olympian god with purple skin.
In the film Bab'Aziz, directed by Nacer Khemir, a Narcissus like character was portrayed by an ancient prince who sat by a pond for days after days and looked at the reflection of his own soul. He was referred to as 'The prince who contemplated his soul'.
"Pink Narcissus" is an artistic film by James Bidgood about the fantasies of a hustler.
The escape craft Ripley boards in the 1979 Ridley Scott film Alien is called the Narcissus.
In the 2011 film "Seeing Heaven", Narcissus is depicted in a painting - the character of the film also replicates the myth of Narcissus gazing at his own reflection. The film delves deeply into the main character (Paul) and the theme is loosely based on the myth of Narcissus, as all who look at Paul are transfixed by his beauty - just as all those who gazed upon Narcissus were transifixed with his beauty.
In music.
National Medal Of Arts recipient Morten Lauridsen wrote a choral work entitled "Dirait-on" based on the poem by Rainer Maria Rilke.
"Supper's Ready" by Genesis (ca. 1972), a near-23-minute epic song laden with religious and mythological imagery, refers to the myth of Narcissus as follows: "A young figure sits still by the pool / He's been stamped "Human Bacon" by some butchery tool / (He is you) / Social Security took care of this lad. / We watch in reverence, as Narcissus is turned to a flower. / A flower?". The movement is titled "How Dare I Be So Beautiful?".
American rock band Tool made a subtle reference in their song "Reflection" from their third studio album Lateralus. Not only is the whole song a metaphor of the myth, but it also makes an explicit reference: "And as I pull my head out I am without one doubt/ Don't want to be down here feeding my narcissism/ I must crucify the ego before it's far too late/ I pray the light lifts me out".
The song combines elements of self-analysis and finding the right path, versus self-infatuation and absorption.
Progressive metal band Threshold referenced the myth with an 11-minute epic titled "Narcissus", the closing track on their album "Hypothetical". Greek metal band Septic Flesh recorded a song about Narcissus (called "Narcissus") on their album "Communion".
"Narcissus in a Red Dress" by The Like was released on "The Like EP" and their album "Release Me". The Canadian band Hedley has written a song about Narcissus (called "Narcissist"). One line goes "He falls in love with his reflection in the glass / He can't resist who's staring back"
Composer Nikolai Tcherepnin wrote his ballet "Narcisse et Echo, Op. 40 in 1911 for Sergei Diaghilev's Ballets Russes and was danced by Nijinski. Uruguayan band El Cuarteto de Nos wrote the song "Me Amo" (I Love Myself) in which the chorus sings "como Narciso soy" (I am like Narcissus). In 2010, Swedish electronic artist pacific! released "Narcissus" an album and ballet staged in Gothenburg. In 1994, composer Mark Applebaum composed Narcissus: Strata/Panacea for marimba solo. This work comprised one movement of the larger Janus Cycle, for mixed instrumentation. In 1987, Thea Musgrave was commissioned by a consortium of four flutists for a solo work. She composed Narcissus for flute and digital delay.

</doc>
<doc id="22102" url="http://en.wikipedia.org/wiki?curid=22102" title="Naval mine">
Naval mine

A naval mine is a self-contained explosive device placed in water to damage or destroy surface ships or submarines. Unlike depth charges, mines are deposited and left to wait until they are triggered by the approach of, or contact with, an enemy vessel. Naval mines can be used offensively—to hamper enemy shipping movements or lock vessels into a harbour; or defensively—to protect friendly vessels and create "safe" zones.
Description.
Mines can be laid in many ways: by purpose-built minelayers, refitted ships, submarines, or aircraft—and even by dropping them into a harbour by hand. They can be inexpensive: some variants can cost as little as US$1000, though more sophisticated mines can cost millions of dollars, be equipped with several kinds of sensors, and deliver a warhead by rocket or torpedo.
Their flexibility and cost-effectiveness make mines attractive to the less powerful belligerent in asymmetric warfare. The cost of producing and laying a mine is usually anywhere from 0.5% to 10% of the cost of removing it, and it can take up to 200 times as long to clear a minefield as to lay it. Parts of some World War II naval minefields still exist because they are too extensive and expensive to clear. It is possible for some of these 1940s-era mines to remain dangerous for many years to come.
Mines have been employed as offensive or defensive weapons in rivers, lakes, estuaries, seas, and oceans, but they can also be used as tools of psychological warfare. Offensive mines are placed in enemy waters, outside harbours and across important shipping routes with the aim of sinking both merchant and military vessels. Defensive minefields safeguard key stretches of coast from enemy ships and submarines, forcing them into more easily defended areas, or keeping them away from sensitive ones.
Minefields designed for psychological effect are usually placed on trade routes and are used to stop shipping from reaching an enemy nation. They are often spread thinly, to create an impression of minefields existing across large areas. A single mine inserted strategically on a shipping route can stop maritime movements for days while the entire area is swept.
International law requires nations to declare when they mine an area, in order to make it easier for civil shipping to avoid the mines. The warnings do not have to be specific; for example, during World War II, Britain declared simply that it had mined the English Channel, North Sea, and French coast.
History.
Early use.
The precursor to naval mines was first described by the early Ming dynasty Chinese artillery officer Jiao Yu, in his 14th century military treatise known as the "Huolongjing". Chinese records tell of naval explosives in the 16th century, used to fight against Japanese pirates ("wokou"). This kind of naval mine was loaded in a wooden box, sealed with putty. General Qi Jiguang made several timed, drifting explosives, to harass Japanese pirate ships. However, in the "Tiangong Kaiwu" ("The Exploitation of the Works of Nature") treatise, written by Song Yingxing in 1637 AD, it describes naval mines with a rip cord pulled by hidden ambushers located on the nearby shore who rotated a steel wheellock flint mechanism to produce sparks and ignite the fuse of the naval mine. Although this is the rotating steel wheellock's first use with naval mines, Jiao Yu had described their use for land mines back in the 14th century.
The first plan for a sea mine in the West was by Ralph Rabbards, who presented his design to Queen Elizabeth I of England in 1574. The Dutch inventor Cornelius Drebbel was employed in the Office of Ordnance by King Charles I of England to make weapons, including a "floating petard" which proved a failure. Weapons of this type were apparently tried by the English at the Siege of La Rochelle in 1627.
American David Bushnell invented the first practical mine, for use against the British in the American War of Independence. It was a watertight keg filled with gunpowder that was floated toward the enemy, detonated by a sparking mechanism if it struck a ship. It was used on the Delaware River as a drift mine, and was regarded as unethical.
19th century.
In 1812 Russian engineer Pavel Shilling exploded an underwater mine using an electrical circuit. In 1854, during the unsuccessful attempt of the Anglo-French fleet to seize the Kronstadt fortress, British steamships HMS "Merlin" (9 June 1855, the first successful mining in history), HMS "Vulture" and HMS "Firefly" were damaged by underwater explosions of Russian naval mines. More than 1500 naval mines, or "infernal machines", designed by Moritz von Jacobi and Alfred Nobel were set by Russian naval specialists in the Gulf of Finland during the Crimean War. The mining of "Vulcan" led to the world's first minesweeping operation. During the next 72 hours, 33 mines were swept.
The American Civil War also saw the successful use of mines. The first ship sunk by a mine was the USS "Cairo" in 1862 in the Yazoo River. Rear Admiral David Farragut's famous statement, "Damn the torpedoes, full speed ahead!" refers to a minefield laid at Mobile, Alabama.
In the 19th century, mines were called torpedoes, a name probably conferred by Dennis Fletcher after the torpedo fish, which gives powerful electric shocks. A spar torpedo was a mine attached to a long pole and detonated when the ship carrying it rammed another one and withdrew a safe distance. The submarine "H. L. Hunley" used one to sink USS "Housatonic" on February 17, 1864. A Harvey torpedo was a type of floating mine towed alongside a ship, and was briefly in service in the Royal Navy in the 1870s. Other "torpedoes" attached to ships or propelled themselves. One such weapon, called the Whitehead torpedo after its inventor, caused the word "torpedo" to be used for self-propelled underwater missiles as well as static devices. These mobile devices were also known as "fish torpedos."
Following the civil war, the United States adopted the mine as its primary weapon for coast defense. In the decade following 1868, Major Henry Larcom Abbot carried out a lengthy set of experiments to design and test moored mines that could be exploded on contact or be detonated at will as enemy shipping passed near them. This initial development of mines in the United States took place under the purview of the U.S. Army Corps of Engineers, which trained officers and men in their use at the Engineer School of Application at Willets Point, New York (later the site of Fort Totten).
The Imperial Russian Navy was a pioneer in mine warfare, and successfully deployed mines against the Ottoman Navy during both the Crimean War and the Russo-Turkish War (1877-1878).
During the Battle of Tamsui, in the Keelung Campaign, in the Sino-French War, Chinese forces on Taiwan under Liu Mingchuan took measures to reinforce Tamsui against the French, in the river, nine torpedo mines were planted and the entrance was blocked with ballast boats filled with stone which were sunk on September 3, matchlock armed "Hakka hill people" were used to reinforce the mainland Chinese battalion, and around the British consulate and customs house at the Red Fort hilltop, Shanghai Arsenal manufactured Krupp guns were used to form an additional battery.
Early 20th century.
During the Boxer Rebellion, Imperial Chinese forces deployed a command-detonated mine field at the mouth of the Peiho river before the Dagu forts, to prevent the western Allied forces from sending ships to attack.
The next major use of mines was during the Russo-Japanese War of 1904-1905. They proved their worth as weapons in this conflict. For instance, two mines blew up when the "Petropavlovsk" struck them near Port Arthur, sending the holed vessel to the bottom and killing the fleet commander, Admiral Stepan Makarov, and most of his crew in the process. The toll inflicted by mines was not confined to the Russians, however. The Japanese Navy lost two battleships, four cruisers, two destroyers and a torpedo-boat to offensively laid mines during the war. Most famously, on May 15, 1904, the Russian minelayer "Amur" planted a 50-mine minefield off Port Arthur and succeeded in sinking the Japanese battleships "Hatsuse" and "Yashima".
Following the end of the Russo-Japanese War, several nations attempted to have mines banned as weapons of war at the Hague Peace Conference (1907).
Many early mines were fragile and dangerous to handle, as they contained glass containers filled with nitroglycerin or mechanical devices that activated a blast upon tipping. Several mine-laying ships were destroyed when their cargo exploded.
Beginning around the start of the 20th century, submarine mines played a major role in the defense of U.S. harbors against enemy attack. The mines employed were controlled mines, anchored to the bottoms of the harbors and detonated under control from large mine casemates on shore.
During World War I, mines were used extensively to defend coasts, coastal shipping, ports and naval bases around the globe. The Germans laid mines in shipping lanes to sink merchant and naval vessels serving Britain. The Allies targeted the German U-boats in the Strait of Dover and the Hebrides. In an attempt to seal up the northern exits of the North Sea, the Allies developed the North Sea Mine Barrage. During a period of five months from June 1918 almost 70,000 mines were laid spanning the North Sea's northern exits. The total number of mines laid in the North Sea, the British East Coast, Straits of Dover, and Heligoland Bight is estimated at 190,000 and the total number during the whole of WWI was 235,000 sea mines. Clearing the barrage after the war took 82 ships and five months, working around the clock.
World War II.
During World War II, the U-boat fleet, which dominated much of the battle of the Atlantic, was small at the beginning of the war and much of the early action by German forces involved mining convoy routes and ports around Britain. German submarines also operated in the Mediterranean Sea, in the Caribbean Sea, and along the U.S. coast.
Initially, contact mines—requiring a ship physically strike a mine to detonate it—were employed, usually tethered at the end of a cable just below the surface of the water. Contact mines usually blew a hole in ships' hulls. By the beginning of World War II, most nations had developed mines that could be dropped from aircraft and floated on the surface, making it possible to lay them in enemy harbours. The use of dredging and nets was effective against this type of mine, but this consumed valuable time and resources, and required harbours to be closed.
Later, some ships survived mine blasts, limping into port with buckled plates and broken backs. This appeared to be due to a new type of mine, detecting ships magnetically and detonating at a distance, causing damage with the shock wave of the explosion. Ships that had successfully run the gauntlet of the Atlantic crossing were sometimes destroyed entering freshly cleared British harbours. More shipping was being lost than could be replaced, and Churchill ordered the intact recovery of one of these new mines to be of the highest priority.
The British experienced a stroke of luck in November 1939. A German mine was dropped from an aircraft onto the mud flats off Shoeburyness during low tide. As if this was not sufficiently good fortune, the land belonged to the army, and a base with men and workshops was at hand. Experts were dispatched from London to investigate the mine. They had some idea that the mines used magnetic sensors, so everyone removed all metal, including their buttons, and made tools of non-magnetic brass. They disarmed the mine and rushed it to labs at Portsmouth, where scientists discovered a new type of arming mechanism. A large ferrous object passing through the Earth's magnetic field will concentrate the field through it; the mine's detector was designed to trigger after the ship's midpoint passed over, when its magnetic field began to diminish in strength as measured by the mine. The mechanism had an adjustable sensitivity, calibrated in milligauss. (As it turned out, the German firing mechanism was overly sensitive, making sweeping easier.) The U.S. began adding delay counters to their magnetic mines in June 1945.
From this data, methods were developed to clear the mines. Early methods included the use of large electromagnets dragged behind ships or below low-flying aircraft (a number of older bombers like the Vickers Wellington were used for this). Both of these methods had the disadvantage of "sweeping" only a small strip. A better solution was found in the "Double-L Sweep" using electrical cables dragged behind ships that passed large pulses of current through the seawater. This induced a large magnetic field and swept the entire area between the two ships. The older methods continued to be used in smaller areas. The Suez Canal continued to be swept by aircraft, for instance. Wartime Japanese sweep methods, by contrast, never advanced much past 1930s standards, and failed entirely to keep up with new American mines, clearing no more than 15% of all the mines laid in Japan's coastal waters. Moreover, IJN's minesweeping force was far too small with 350 ships, and 20,000 men.
While these methods were useful for clearing mines from local ports, they were of little or no use for enemy-controlled areas. These were typically visited by warships, and the majority of the fleet then underwent a massive degaussing process, where their hulls had a slight "south" bias induced into them which offset the concentration effect almost to zero.
Initially, major warships and large troopships had a copper "degaussing coil" fitted around the perimeter of the hull, energized by the ship's electrical system whenever in suspected magnetic-mined waters. Some of the first to be so-fitted were the carrier HMS "Ark Royal" and the liners and , which were used as troopships. This was felt to be impracticable for the myriad of smaller warships and merchant vessels, not least due to the amount of copper that would be required. It was found that "wiping" a current-carrying cable up and down a ship's hull temporarily cancelled the ships' magnetic signature sufficiently to nullify the threat. This started in late 1939, and by 1940 merchant vessels and the smaller British warships were largely immune for a few months at a time until they once again built up a field. Many of the boats that sailed to Dunkirk were degaussed in a marathon four-day effort by degaussing stations.
The Allies deployed acoustic mines, against which even wooden-hulled ships (in particular minesweepers) remained vulnerable. Japan developed sonic generators to sweep these; the gear was not ready by war's end. The primary method Japan used was small air-delivered bombs. This was profligate and ineffectual; used against acoustic mines at Penang, 200 bombs were needed to detonate just 13 mines.
The Germans had also developed a pressure-activated mine and planned to deploy it as well, but they saved it for later use when it became clear the British had defeated the magnetic system. The U.S. also deployed these, adding "counters" which would allow a variable number of ships to pass unharmed before detonating. This made them a great deal harder to sweep. Japan's antiquated sweep methods, lifting mines in nets, accidentally proved useful against these mines; it remained too slow and hazardous to be truly effective, especially in light of the high numbers being laid.
Mining campaigns could have devastating consequences. The U.S. effort against Japan, for instance, closed major ports, such as Hiroshima, for days, and by the end of the Pacific War had cut the amount of freight passing through Kobe–Yokohama by 90%.
When the war ended, more than 25,000 U.S.-laid mines were still in place, and the Navy proved unable to sweep them all, limiting efforts to critical areas. After sweeping for almost a year, in May 1946, the Navy abandoned the effort with 13,000 mines still unswept. Over the next thirty years, more than 500 minesweepers (of a variety of types) were damaged or sunk in continuing clearance efforts.
Cold War era.
Since World War II, mines have damaged 14 United States Navy ships, whereas air and missile attacks have damaged four. During the Korean War, mines laid by North Korean forces caused 70% of the casualties suffered by U.S. naval vessels and caused 4 sinkings.
During the Iran–Iraq War from 1980 to 1988, the belligerents mined several areas of the Persian Gulf and nearby waters. On April 14, 1988, USS "Samuel B. Roberts" struck an Iranian M-08/39 mine in the central Persian Gulf shipping lane, wounding 10 sailors.
In the summer of 1984, magnetic sea mines damaged at least 19 ships in the Red Sea. The U.S. concluded Libya was probably responsible for the minelaying. In response the U.S., Britain, France, and three other nations launched Operation Intense Look, a minesweeping operation in the Red Sea involving more than 46 ships.
On the orders of the Reagan administration, the CIA mined Nicaragua's Sandino port in 1984 in support of the Contra guerrilla group. A Soviet tanker was among the ships damaged by these mines. In 1986, in the case of "Nicaragua v. United States", the International Court of Justice ruled that this mining was a violation of international law.
During the Gulf War, Iraqi naval mines severely damaged USS "Princeton" and USS "Tripoli". When the war concluded, eight countries conducted clearance operations.
Types.
Naval mines may be classified into three major groups; contact, remote and influence mines.
Contact mines.
The earliest mines were usually of this type. They are still used today, as they are extremely low cost compared to any other anti-ship weapon and are effective, both as a terror weapon and to sink enemy ships. Contact mines need to be touched by the target before they detonate, limiting the damage to the direct effects of the explosion and usually affecting only the vessel that triggers them.
Early mines had mechanical mechanisms to detonate them, but these were superseded in the 1870s by the "Hertz horn" (or "chemical horn"), which was found to work reliably even after the mine had been in the sea for several years. The mine's upper half is studded with hollow lead protuberances, each containing a glass vial filled with sulfuric acid. When a ship's hull crushes the metal horn, it cracks the vial inside it, allowing the acid to run down a tube and into a lead–acid battery which until then contained no acid electrolyte. This energizes the battery, which detonates the explosive.
Earlier forms of the detonator employed a vial of sulfuric acid surrounded by a mixture of potassium perchlorate and sugar. When the vial was crushed, the acid ignited the perchlorate-sugar mix, and the resulting flame ignited the gunpowder charge.
During the initial period of World War I, the British Navy used contact mines in the English Channel and later in large areas of the North Sea to hinder patrols by German submarines. Later, the American antenna mine was widely used because submarines could be at any depth from the surface to the seabed. This type of mine had a copper wire attached to a buoy that floated above the explosive charge which was weighted to the seabed with a steel cable. If a submarine's steel hull touched the copper wire, the slight voltage change caused by contact between two dissimilar metals was amplified and detonated the explosives.
Limpet mines.
Limpet mines are a special form of contact mine that are manually attached to the target by magnets and left, and are so named because of the superficial similarity to the limpet, a mollusk.
Moored contact mines.
Generally, this mine type is set to float just below the surface of the water or as deep as five meters. A steel cable connecting the mine to an anchor on the seabed prevents it from drifting away. The explosive and detonating mechanism is contained in a buoyant metal or plastic shell. The depth below the surface at which the mine floats can be set so that only deep draft vessels such as aircraft carriers, battleships or large cargo ships are at risk, saving the mine from being used on a less valuable target. In littoral waters it is important to ensure that the mine does not become visible when the sea level falls at low tide, so the cable length is adjusted to take account of tides. Even during the Second World War, there were mines that could be moored in 300m-deep water (Example: The ).
Floating mines typically have a mass of around 200 kg, including 80 kg of explosives e.g. TNT, minol or amatol.
During WWII, mine traps were used for blocking port entrances. Two floating mines were anchored some distance apart on either side of a shipping channel, linked by a chain. When a deep draft vessel passed through the trap it would pull the chain along with it, dragging the mines onto the sides of the ship; the resulting double explosion often sank it. This system was not used extensively, but proved effective in blocking ports.
Drifting contact mines.
Drifting mines were occasionally used during World War I and World War II. However, they were more feared than effective. Sometimes floating mines break from their moorings and become drifting mines; modern mines are designed to deactivate in this event. After several years at sea, the deactivation mechanism might not function as intended and the mines may remain live. Admiral Jellicoe's British fleet did not pursue and destroy the outnumbered German High Seas Fleet when it turned away at the Battle of Jutland because he thought they were leading him into a trap: he believed it possible that the Germans were either leaving floating mines in their wake, or were drawing him towards submarines, although neither of these was the case.
Churchill promoted "Operation Royal Marine" in 1940 and again in 1944 where floating mines were put into the Rhine in France to float down the river, becoming active after a time calculated to be long enough to reach German territory.
After World War I the drifting contact mine was banned, but was occasionally used during World War II. The drifting mines were much harder to remove than tethered mines after the war, and they caused about the same damage to both sides.
Bottom contact mines.
A bottom contact mine is the simplest form of mine. It is merely an explosive charge with some form of fuze fitted lying on the seafloor. They have been used against submarines, as submarines sometimes lie on the seafloor to reduce their acoustic signature. They are also used to prevent landing craft from reaching the shore and were a major obstacle during the D-Day landings. The Germans used antitank mines here with minor modifications to make them more reliable underwater, attaching the mines to the front of many of the obstacles seen in photos of the landing.
These mines usually weighed 2 to 50 kg, including 1 to 40 kg of explosives (TNT or hexatonal).
Remotely controlled mines.
Frequently used in combination with coastal artillery and hydrophones, controlled mines (or command detonation mines) can be in place in peacetime, which is a huge advantage in blocking important shipping routes. The mines can usually be turned into "normal" mines with a switch (which prevents the enemy from simply capturing the controlling station and deactivating the mines), detonated on a signal or be allowed to detonate on their own. The earliest ones were developed around 1812 by Robert Fulton. The first remotely controlled mines were moored mines used in the American Civil War, detonated electrically from shore. They were considered superior to contact mines because they did not put friendly shipping at risk.
Modern examples usually weigh 200 kg, including 80 kg of explosives (TNT or hexatonal).
Influence mines.
These mines are triggered by the influence of a ship or submarine, rather than direct contact. Such mines incorporate electronic sensors designed to detect the presence of a vessel and detonate when it comes within the blast range of the warhead. The fuzes on such mines may incorporate one or more of the following sensors: magnetic, passive acoustic or water pressure displacement caused by the proximity of a vessel.
First used during the First World War, their use became more general in the Second World War. The sophistication of influence mine fuzes has increased considerably over the years as first transistors and then microprocessors have been incorporated into designs. Simple magnetic sensors have been superseded by total-field magnetometers. Whereas early magnetic mine fuzes would respond only to changes in a single component of a target vessel's magnetic field, a total field magnetometer responds to changes in the magnitude of the total background field (thus enabling it to better detect even degaussed ships). Similarly, the original broadband hydrophones of 1940s acoustic mines (which operate on the integrated volume of all frequencies) have been replaced by narrow-band sensors which are much more sensitive and selective. Mines can now be programmed to listen for highly specific acoustic signatures (e.g. a gas turbine powerplant or cavitation sounds from a particular design of propeller) and ignore all others. The sophistication of modern electronic mine fuzes incorporating these digital signal processing capabilities makes it much more difficult to detonate the mine with electronic countermeasures because several sensors working together (e.g. magnetic, passive acoustic and water pressure) allow it to ignore signals which are not recognised as being the unique signature of an intended target vessel.
Modern influence mines such as the BAE Stonefish are computerised, with all the programmability that this implies e.g. the ability to quickly load new acoustic signatures into fuzes, or program them to detect a single, highly distinctive target signature. In this way, a mine with a passive acoustic fuze can be programmed to ignore all friendly vessels and small enemy vessels, only detonating when a very large enemy target passes over it. Alternatively, the mine can be programmed specifically to ignore all surface vessels regardless of size and exclusively target submarines.
Even as far back as the Second World War it was possible to incorporate a "ship counter" facility into mine fuzes e.g. set the mine to ignore the first two ships to pass over it (which could be minesweepers deliberately trying to trigger mines) but detonate when the third ship passes overhead—which could be a high-value target such as an aircraft carrier or oil tanker. Even though modern mines are generally powered by a long life lithium battery, it is important to conserve power because they may need to remain active for months or even years. For this reason, most influence mines are designed to remain in a semi-dormant state until an unpowered (e.g. deflection of a mu-metal needle) or low-powered sensor detects the possible presence of a vessel, at which point the mine fuze powers up fully and the passive acoustic sensors will begin to operate for some minutes. It is possible to program computerised mines to delay activation for days or weeks after being laid; similarly, they can be programmed to self-destruct or render themselves safe after a preset period of time. Generally, the more sophisticated the mine design, the more likely it is to have some form of anti-handling device fitted in order to hinder clearance by divers or remotely piloted submersibles.
Moored mines.
The moored mine is the backbone of modern mine systems. They are deployed where water is too deep for bottom mines. They can use several kinds of instruments to detect an enemy, usually a combination of acoustic, magnetic and pressure sensors, or more sophisticated optical shadows or electro potential sensors. These cost many times more than contact mines. Moored mines are effective against most kinds of ships. As they are cheaper than other anti-ship weapons they can be deployed in large numbers, making them useful area denial or "channelizing" weapons.
Moored mines usually have lifetimes of more than 10 years, and some almost unlimited. These mines usually weigh 200 kg, including 80 kg of explosives (RDX). In excess of 150 kg of explosives the mine becomes inefficient, as it becomes too large to handle and the extra explosives add little to the mine's effectiveness.
Bottom mines.
Bottom mines are used when the water is no more than 60 m deep or when mining for submarines down to around 200 m. They are much harder to detect and sweep, and can carry a much larger warhead than a moored mine. Bottom mines commonly utilize multiple types of sensors, which are less sensitive to sweeping.
These mines usually weigh between 150 and, including between 125 and of explosives.
Unusual mines.
Several specialized mines have been developed for other purposes than the common minefield.
Bouquet mine.
The bouquet mine is a single anchor attached to several floating mines. It is designed so that when one mine is swept or detonated, another takes its place. It is a very sensitive construction and lacks reliability.
Anti-sweep mine.
The anti-sweep mine is a very small mine (40 kg warhead) with as small a floating device as possible. When the wire of a mine sweep hits the mine, it sinks, letting the sweep wire drag along the anchoring wire of the mine until the sweep hits the mine. That detonates the mine and cuts the sweeping wire. They are very cheap and usually used in combination with other mines in a minefield to make sweeping more difficult. One type is the Mark 23 used by the United States during World War II.
Oscillating mine.
The mine is hydrostatically controlled to maintain a pre-set depth below the water's surface independently of the rise and fall of the tide.
Ascending mine.
The ascending mine is a floating distance mine that may cut its mooring or in some other way float higher when it detects a target. It lets a single floating mine cover a much larger depth range.
Homing mines.
These are mines containing a moving weapon as a warhead, either a torpedo or a rocket.
Rocket mine: a Russian invention, the rocket mine is a bottom distance mine that fires a homing high-speed rocket (not torpedo) upwards towards the target. It is intended to allow a bottom mine to attack surface ships as well as submarines from a greater depth. One type is the Te-1 rocket propelled mine.
Torpedo mine: the torpedo mine is a self-propelled variety, able to lie in wait for a target and then pursue it e.g. the Mark 60 CAPTOR. Other designs such as the Mk 67 submarine launched mobile mine (which is based on a Mark 37 torpedo) are capable of travelling as far as 10 miles through or into a channel, harbor, shallow water area and other zones which would normally be inaccessible to craft laying the device. After reaching the target area they sink to the sea bed and act like conventionally laid influence mines. Generally, torpedo mines incorporate computerised acoustic and magnetic fuzes.
The U.S. Mark 24 "mine", code-named Fido, was actually an ASW homing torpedo. The mine designation was disinformation to conceal its function.
Mobile mine.
The mine is propelled to its intended position by propulsion equipment such as a torpedo. After reaching its destination, it sinks to the seabed and operates like a standard mine. It differs from the homing mine in that its mobile stage is before it lays in wait, rather than as part of the attacking phase.
Nuclear mine.
During the Cold War a test was conducted with naval mine fitted with tactical nuclear warheads for the "Baker" shot of Operation Crossroads. This weapon was experimental and never went into production. There have been some reports that North Korea may be developing a nuclear mine The Seabed Arms Control Treaty prohibits the placement of nuclear weapons on the seabed beyond a 12-mile coast zone.
Daisy-chained mine.
This comprises two moored, floating contact mines which are tethered together by a length of steel cable or chain. Typically, each mine is situated approximately 60 ft away from its neighbour, and each floats a few metres below the surface of the ocean. When the target ship hits the steel cable, the mines on either side are drawn down the side of the ship's hull, exploding on contact. In this manner it is almost impossible for target ships to pass safely between two individually moored mines. Daisy-chained mines are a very simple concept which was used during World War II.
Dummy mine.
Plastic drums filled with sand or concrete are periodically rolled off the side of ships as real mines are laid in large mine-fields. These inexpensive false targets (designed to be of a similar shape and size as genuine mines) are intended to slow down the process of mine clearance: a mine-hunter is forced to investigate each suspicious sonar contact on the sea bed, whether it is real or not. Often a maker of naval mines will provide both training and dummy versions of their mines.
Mine laying.
Historically several methods were used to lay mines. During the First and Second world wars, the Germans used U-boats to lay mines around the UK. In the Second World War, aircraft came into favour for mine laying with one of the largest such examples being the mining of the Japanese sea routes in Operation Starvation.
Laying a minefield is a relatively fast process with specialized ships, which is still today the most common method. These minelayers can carry several thousand mines and manoeuvre with high precision. The mines are dropped at predefined intervals into the water behind the ship. Each mine is recorded for later clearing, but it is not unusual for these recordings to be lost together with the ships. Therefore many countries demand that all mining operations be planned on land and records kept so the mines can later be recovered more easily.
Other methods to lay minefields include:
In some cases, mines are automatically activated upon contact with the water. In others, a safety lanyard is pulled (e.g. one end attached to the rail of a ship, aircraft or torpedo tube) which starts an automatic timer countdown before the arming process is complete. Typically, the automatic safety-arming process takes some minutes to complete. This is in order to give the people laying the mines sufficient time to move out of its activation and blast zones.
Aerial mining in World War II.
Germany.
In the 1930s, Germany had experimented with the laying of mines by aircraft; it became a crucial element in their overall mining strategy. Aircraft had the advantage of speed, and they would never get caught in their own minefields. German mines held a large 1000 lb explosive charge. From April to June 1940, the Luftwaffe laid 1,000 mines in British waters. Soviet ports were mined, as was the Arctic convoy route to Murmansk. The Heinkel He 115 could carry two medium or one large mine while the Heinkel He 59, Dornier Do 18, Junkers Ju 88 and Heinkel He 111 could carry more.
Soviet Union.
The USSR was relatively ineffective in its use of naval mines in WWII in comparison with its record in previous wars. Small mines were developed for use in rivers and lakes, and special mines for shallow water. A very large chemical mine was designed to sink through ice with the aid of a melting compound. Special aerial mine designs finally arrived in 1943–1944, the AMD-500 and AMD-1000. Various Soviet Naval Aviation torpedo bombers were pressed into the role of aerial mining in the Baltic Sea and the Black Sea, including Ilyushin DB-3s, Il-4s and Lend Lease Douglas Boston IIIs.
United Kingdom.
In September 1939, the UK announced the placement of extensive defensive minefields in waters surrounding the Home Islands. Offensive aerial mining operations began in April 1940 when 38 mines were laid at each of these locations: the Elbe River, the port of Lübeck and the German naval base at Kiel. In the next 20 months, mines delivered by aircraft sank or damaged 164 Axis ships with the loss of 94 aircraft. By comparison, direct aerial attacks on Axis shipping had sunk or damaged 105 vessels at a cost of 373 aircraft lost. The advantage of aerial mining became clear, and the United Kingdom geared up for it. A total of 48,000 aerial mines were laid by the Royal Air Force (RAF) in the European Theatre during World War II.
United States.
The United States' early aerial mining efforts used smaller aircraft unable to carry many mines. Using TBF Avenger torpedo bombers, the US Navy mounted a direct aerial mining attack on enemy shipping in Palau on 30 March 1944 in concert with simultaneous conventional bombing and strafing attacks. The dropping of 78 mines stopped 32 Japanese ships from escaping Koror harbor; the combined operation sank or damaged 36 ships. Two Avengers were lost; and their crews were recovered. The mines brought port usage to a halt for 20 days; further mine-laying in the area contributed to the Japanese abandoning Palau as a base.
As early as 1942, American mining experts such as Naval Ordnance Laboratory scientist Dr. Ellis A. Johnson, commander, naval reserve, suggested massive aerial mining operations against Japan's "outer zone" (Korea and northern China) as well as the "inner zone", their home islands. First, aerial mines would have to be developed further and manufactured in large numbers. Second, laying the mines would require a sizable air group. The US Army Air Force had the carrying capacity but considered mining to be the navy's job. The US Navy lacked suitable aircraft. Johnson set about convincing General Curtis LeMay of the efficacy of very heavy bombers laying aerial mines.
In the meantime, B-24 Liberators, PBY Catalinas and other available bomber aircraft took part in localized mining operations in the Southwest Pacific and the China Burma India (CBI) theaters, beginning with a very successful attack on the Yangon River in February 1943. Aerial minelaying operations involved a coalition of British, Australian and American aircrews, with the RAF and the Royal Australian Air Force (RAAF) carrying out 60% of the sorties and the USAAF and US Navy covering 40%. Both British and American mines were used. Japanese merchant shipping suffered tremendous losses, while Japanese mine sweeping forces were spread too thin attending to far-flung ports and extensive coastlines. Admiral Thomas C. Kinkaid, who directed nearly all RAAF mining operations in CBI, heartily endorsed aerial mining, writing in July 1944 that "aerial mining operations were of the order of 100 times as destructive to the enemy as an equal number of bombing missions against land targets."
In March 1945, Operation Starvation began in earnest, using 160 of LeMay's B-29 Superfortress bombers to attack Japan's inner zone. Almost half of the mines were the US-built Mark 25 model, carrying 1250 lbs of explosives and weighing about 2,000 lbs. Other mines used included the smaller 1,000 lb Mark 26. Fifteen B-29s were lost while 293 Japanese merchant ships were sunk or damaged. Twelve thousand aerial mines were laid, a significant barrier to Japan's access to outside resources. Prince Fumimaro Konoe said after the war that the aerial mining by B-29s had been "equally as effective as the B-29 attacks on Japanese industry at the closing stages of the war when all food supplies and critical material were prevented from reaching the Japanese home islands." The United States Strategic Bombing Survey (Pacific War) concluded that it would have been more efficient to combine the United States's effective anti-shipping submarine effort with land- and carrier-based air power to strike harder against merchant shipping and begin a more extensive aerial mining campaign earlier in the war. Survey analysts projected that this would have starved Japan, forcing an earlier end to the war. After the war, Dr. Johnson looked at the Japan inner zone shipping results, comparing the total economic cost of submarine-delivered mines versus air-dropped mines and found that, though 1 in 12 submarine mines connected with the enemy as opposed to 1 in 21 for aircraft mines, the aerial mining operation was about ten times less expensive per enemy ton sunk.
Clearing WWII aerial mines.
Between 600,000 and 1,000,000 naval mines of all types were laid in World War II. Advancing military forces worked to clear mines from newly taken areas, but extensive minefields remained in place after the war. Air-dropped mines had an additional problem for mine sweeping operations: they were not meticulously charted. In Japan, much of the B-29 mine-laying work had been performed at high altitude, with the drifting on the wind of mines carried by parachute adding a randomizing factor to their placement. Generalized danger areas were identified, with only the quantity of mines given in detail. Mines used in Operation Starvation were supposed to be self-sterilizing, but the circuit did not always work. Clearing the mines from Japanese waters took so many years that the task was eventually given to the Japan Maritime Self-Defense Force.
For the purpose of clearing all types of naval mines, the Royal Navy employed German crews and minesweepers from June 1945 to January 1948, organised in the German Mine Sweeping Administration (GMSA), which consisted of 27,000 members of the former "Kriegsmarine" and 300 vessels. Mine clearing was not always successful: a number of ships were damaged or sunk by mines after the war. Two such examples were the liberty ships "Pierre Gibault" which was scrapped after hitting a mine in a previously cleared area off the Greek island of Kythira in June 1945, and "Nathaniel Bacon" which hit a minefield off Civitavecchia, Italy in December 1945, caught fire, was beached, and broke in two.
Damage.
The damage that may be caused by a mine depends on the "shock factor value", a combination of the initial strength of the explosion and of the distance between the target and the detonation. When taken in reference to ship hull plating, the term "Hull Shock Factor" (HSF) is used, while keel damage is termed "Keel Shock Factor" (KSF). If the explosion is directly underneath the keel, then HSF is equal to KSF, but explosions that are not directly underneath the ship will have a lower value of KSF.
Direct damage.
Usually only created by contact mines, direct damage is a hole blown in the ship. Among the crew, fragmentation wounds are the most common form of damage. Flooding typically occurs in one or two main watertight compartments which can sink smaller ships or disable larger ones. Contact mine damage often occurs at or close to the waterline near the bow, but depending on circumstances a ship could be hit anywhere on its outer hull surface (the USS "Samuel B. Roberts" mine attack being a good example of a contact mine detonating amidships and underneath the ship).
Bubble jet effect.
The bubble jet effect occurs when a mine detonates in the water a short distance away from the ship. The explosion creates a bubble in the water, and due to the difference in pressure, the bubble will collapse from the bottom. The bubble is buoyant and so it rises towards the surface. If the bubble reaches the surface as it collapses it can create a pillar of water that can go over a hundred meters into the air (a "columnar plume"). If conditions are right and the bubble collapses onto the ship's hull the damage to the ship can be extremely serious; the collapsing bubble forms a high energy jet that can break a meter wide hole straight through the ship, flooding one or more compartments, and is capable of breaking smaller ships apart. The crew in the areas hit by the pillar are usually killed instantly. Other damage is usually limited.
The Baengnyeong incident, in which the ROKS "Cheonan" broke in half and sank off the coast South Korea in 2010, was caused by the bubble jet effect according to an international investigation.
Shock effect.
If the mine detonates at a distance from the ship, the change in water pressure causes the ship to resonate. This is frequently the most deadly type of explosion, if it is strong enough. The whole ship is dangerously shaken and everything onboard is tossed around. Engines rip from their beds, cables from their holders, etc. A badly shaken ship usually sinks quickly, with hundreds, or even thousands of small leaks all over the ship and no way to power the pumps. The crew fare no better, as the violent shaking tosses them around. This shaking is powerful enough to cause disabling injury to knees and other joints in the body, particularly if the affected person stands on surfaces connected directly to the hull (such as steel decks).
The resulting gas cavitation and shock-front-differential over the width of the human body is sufficient to stun or kill divers.
Countermeasures.
Weapons are frequently a few steps ahead of countermeasures, and mines are no exception. In this field the British, with their large seagoing navy, have had the bulk of world experience, and most anti-mine developments, such as degaussing and the double-L sweep were British inventions. When on operational missions, such as the recent invasion of Iraq, the US still relies on British and Canadian minesweeping services. The US has worked on some innovative mine hunting countermeasures, such as the use of military dolphins to detect and flag mines. However, they are of questionable effectiveness.
Passive countermeasures.
Ships can be designed to be difficult for mines to detect, to avoid detonating them. This is especially true for minesweepers and mine hunters that work in minefields, where a minimal signature outweighs the need for armour and speed. These ships have hulls of glass fibre or wood instead of steel to avoid magnetic signatures, they use special propulsion systems, such as Voith-Schneider propellers, to limit the acoustic signature. They are built with hulls that produce a minimal pressure signature. These measures create other problems. They are expensive, slow, and vulnerable to enemy fire. Therefore, they need protection. Many modern ships have a mine-warning sonar—a simple sonar looking forward and warning the crew if it detects possible mines ahead. It is only effective when the ship is moving slowly.<br> 
A steel-hulled ship can be "degaussed" (more correctly, de-oerstedted or depermed) using a special degaussing station that contains many large coils and induces a magnetic field in the hull with alternating current to demagnetize the hull. This is a rather problematic solution, as magnetic compasses need recalibration and all metal objects must be kept in exactly the same place. Ships slowly regain their magnetic field as they travel through the Earth's magnetic field, so the process has to be repeated every six months.
A simpler variation of this technique, called "wiping", was developed by Charles F. Goodeve which saved time and resources.
Between 1941 and 1943 the US Naval Gun factory (a division of the Naval Ordnance Laboratory) in Washington D.C. built physical models of all US Naval ships. Three kinds of steel were used in shipbuilding: mild steel for bulkheads, a mixture of mild steel and high tensile steel for the hull, and special treatment steel for armor plate. The models were placed within coils which could simulate the Earth's magnetic field at any location. The magnetic signatures were measured with degaussing coils. The objective was to reduce the vertical component of the combination of the Earth's field and the ship's field at the usual depth of German mines. From the measurements, coils were placed and coil currents determined to minimize the chance of detonation for any ship at any heading at any latitude.
Some ships are built with magnetic inductors, large coils placed along the ship to counter the ship's magnetic field. Using magnetic probes in strategic parts of the ship, the strength of the current in the coils can be adjusted to minimize the total magnetic field. This is a heavy and clumsy solution, suited only to small-to-medium-sized ships. Boats typically lack the generators and space for the solution, while the amount of power needed to overcome the magnetic field of a large ship is impractical.
Active countermeasures.
Active countermeasures are ways to clear a path through a minefield or remove it completely. This is one of the most important tasks of any mine warfare flotilla.
Mine sweeping.
A sweep is either a contact sweep, a wire dragged through the water by one or two ships to cut the mooring wire of floating mines, or a distance sweep that mimics a ship to detonate the mines. The sweeps are dragged by minesweepers, either purpose-built military ships or converted trawlers. Each run covers between one and two hundred meters, and the ships must move slowly in a straight line, making them vulnerable to enemy fire. This was exploited by the Turkish army in the Battle of Gallipoli in 1915, when mobile howitzer batteries prevented the British and French from clearing a way through minefields.
If a contact sweep hits a mine, the wire of the sweep rubs against the mooring wire until it is cut. Sometimes "cutters", explosive devices to cut the mine's wire, are used to lessen the strain on the sweeping wire. Mines cut free are recorded and collected for research or shot with a deck gun.
Minesweepers protect themselves with an oropesa or paravane instead of a second minesweeper. These are torpedo-shaped towed bodies, similar in shape to a Harvey Torpedo, that are streamed from the sweeping vessel thus keeping the sweep at a determined depth and position. Some large warships were routinely equipped with paravane sweeps near the bows in case they inadvertently sailed into minefields—the mine would be deflected towards the paravane by the wire instead of towards the ship by its wake. More recently, heavy-lift helicopters have dragged minesweeping sleds, as in the 1991 Persian Gulf War.
The distance sweep mimics the sound and magnetism of a ship and is pulled behind the sweeper. It has floating coils and large underwater "drums". It is the only sweep effective against bottom mines.
During the Second World War, RAF Coastal Command used Vickers Wellington bombers Wellington DW.Mk I fitted with degaussing coils to trigger magnetic mines.
Modern influence mines are designed to discriminate against false inputs and are therefore much harder to sweep. They often contain inherent anti-sweeping mechanisms. For example, they may be programmed to respond to the unique noise of a particular ship-type, its associated magnetic signature and the typical pressure displacement of such a vessel. As a result, a mine-sweeper must accurately guess and mimic the required target signature in order to trigger detonation. The task is complicated by the fact that an influence mine may have one or more of a hundred different potential target signatures programmed into it.
Another anti-sweeping mechanism is a ship-counter in the mine fuze. When enabled, this allows detonation only after the mine fuze has been triggered a pre-set number of times. To further complicate matters, influence mines may be programmed to arm themselves (or disarm automatically—known as "self-sterilization") after a pre-set time. During the pre-set arming delay (which could last days or even weeks) the mine would remain dormant and ignore any target stimulus, whether genuine or faked.
When influence mines are laid in an ocean minefield, they may have various combinations of fuze settings configured. For example, some mines (with the acoustic sensor enabled) may become active within three hours of being laid, others (with the acoustic and magnetic sensors enabled) may become active after two weeks but have the ship-counter mechanism set to ignore the first two trigger events, and still others in the same minefield (with the magnetic and pressure sensors enabled) may not become armed until three weeks have passed. Groups of mines within this mine-field may have different target signatures which may or may not overlap. The fuzes on influence mines allow many different permutations, which complicates the clearance process.
Mines with ship-counters, arming delays and highly specific target signatures in mine fuzes can falsely convince a belligerent that a particular area is clear of mines or has been swept effectively because a succession of vessels have already passed through safely.
Mine hunting.
As naval mines have become more sophisticated, and able to discriminate between targets, so they have become more difficult to deal with by conventional sweeping. This has given rise to the practice of mine-hunting.
Mine hunting is very different from sweeping, although some minehunters can do both tasks. Mines are hunted using sonar, then inspected and destroyed either by divers or ROVs (remote controlled unmanned mini submarines). It is slow, but also the most reliable way to remove mines. Mine hunting started during the Second World War, but it was only after the war that it became truly effective.
Sea mammals (mainly the Bottlenose Dolphin) have been trained to hunt and mark mines, most famously by the U.S. Navy Marine Mammal Program. Mine-clearance dolphins were deployed in the Persian Gulf during the Iraq War in 2003. The US Navy claims that these dolphins were effective in helping to clear more than 100 antiship mines and underwater booby traps from Umm Qasr Port.
French naval officer Jacques Yves Cousteau's Undersea Research Group was once involved in mine-hunting operations: They removed or detonated a variety of German mines, but one particularly defusion-resistant batch—equipped with acutely sensitive pressure, magnetic, and acoustic sensors and wired together so that one explosion would trigger the rest—was simply left undisturbed for years until corrosion would (hopefully) disable the mines.#redirect 
Mine running.
A more drastic method is simply to run ship through the minefield, letting other ship to be protected follow the same path. An early example of this was Farragut's actions at Mobile Bay during the American Civil War. However as mine warfare became more developed this method became uneconomical.
This method was revived by the German "Kriegsmarine" during WWII. Left with a surfeit of idle ships due to the Allied blockade, the "Kriegsmarine" introduced a ship known as "Sperrbrecher" ("barrage breaker"). Typically an old cargo ship, loaded with cargo that made her less vulnerable to sinking (wood for example), the "Sperrbrecher" was run ahead of the ship to be protected, detonating any mines that might be in their path. The use of "Sperrbrecher" obviated the need to continuous and painstaking sweeping, but the cost was high. Over half the 100 or so ships used as "Sperrbrecher" were sunk during the war. Alternatively, a shallow draught vessel can be steamed through the minefield at high speed to generate a pressure wave sufficient to trigger mines, with the minesweeper moving fast enough to be sufficiently clear of the pressure wave so that triggered mines do not destroy the ship itself. These techniques are the only publicly known to be employed way to sweep pressure mines. The technique can be simply countered by use of a ship-counter, set to allow a certain number of passes before the mine is actually triggered. Modern doctrine calls for ground mines to be hunted rather than swept. A new system is being introduced for sweeping pressure mines, however counters are going to remain a problem.
An updated form of this method is the use of small unmanned ROVs (such as the "Seehund" drone) that simulate the acoustic and magnetic signatures of larger ships and are built to survive exploding mines. Repeated sweeps would be required in case one or more of the mines had its "ship counter" facility enabled i.e. were programmed to ignore the first 2, 3, or even 6 target activations.
National arsenals.
US mines.
The United States Navy MK56 ASW mine (the oldest still in use by the US) was developed in 1966. More advanced mines include the MK60 CAPTOR (short for "encapsulated torpedo"), the MK62 and MK63 Quickstrike and the MK67 SLMM (Submarine Launched Mobile Mine). Today, most U.S. naval mines are delivered by aircraft.
MK67 SLMM Submarine Launched Mobile Mine<br>
The SLMM was developed by the United States as a submarine deployed mine for use in areas inaccessible for other mine deployment techniques or for covert mining of hostile environments. The SLMM is a shallow-water mine and is basically a modified Mark 37 torpedo.
General characteristics
MK65 Quickstrike<br>
The Quickstrike is a family of shallow-water aircraft-laid mines used by the United States, primarily against surface craft. The MK65 is a 2,000-lb (900 kg) dedicated, purpose-built mine. However, other Quickstrike versions (MK62, MK63, and MK64) are converted general-purpose bombs. These latter three mines are actually a single type of electronic fuze fitted to Mk82, Mk83 and Mk84 air-dropped bombs. Because this latter type of Quickstrike fuze only takes up a small amount of storage space compared to a dedicated sea mine, the air-dropped bomb casings have dual purpose i.e. can be fitted with conventional contact fuzes and dropped on land targets, or have a Quickstrike fuze fitted which converts them into sea mines.
General characteristics
MK56<br>
General characteristics
Royal Navy.
According to a statement made to the UK Parliament in 2002:
However, a British company (BAE Systems) does manufacture the Stonefish influence mine for export to friendly countries such as Australia, which has both war stock and training versions of Stonefish, in addition to stocks of smaller Italian MN103 Manta mines. The computerised fuze on a Stonefish mine contains acoustic, magnetic and water pressure displacement target detection sensors. Stonefish can be deployed by fixed-wing aircraft, helicopters, surface vessels and submarines. An optional kit is available to allow Stonefish to be air-dropped, comprising an aerodynamic tail-fin section and parachute pack to retard the weapon's descent. The operating depth of Stonefish ranges between 30 and 200 metres. The mine weighs 990 kilograms and contains a 600 kilogram aluminised PBX explosive warhead.

</doc>
<doc id="22104" url="http://en.wikipedia.org/wiki?curid=22104" title="Nawal El Moutawakel">
Nawal El Moutawakel


</doc>
<doc id="22106" url="http://en.wikipedia.org/wiki?curid=22106" title="North Melbourne Football Club">
North Melbourne Football Club

The North Melbourne Football Club, nicknamed The Kangaroos or less formally "The Roos", is the fourth oldest Australian rules football club in the Australian Football League (AFL) and is one of the oldest sporting clubs in Australia and the world. It is based at the Arden Street Oval in the inner Melbourne suburb of North Melbourne, but plays its home matches at the nearby Docklands Stadium.
The club's mascot is a grey or red kangaroo, and its use dates from the middle of the 20th century. The club is also unofficially known as "The Shinboners", a term which dates back to its 19th-century abattoir-worker origins. The club's motto is "Victoria amat curam", Latin for "Victory Demands Dedication".
Club history.
Formative years.
North Melbourne Football Club originated in the year 1869, when a football team was formed for local cricketers desiring to keep fit over the winter months. One thought is that the club was connected to the St Mary's Church of England Cricket Club, now the St Mary's Anglican Church North Melbourne, whose colours – blue and white – are reflected in the North Melbourne's colours today. The association between the St Mary's Church of England Cricket Club and the establishment of the North Melbourne Football Club is believed to have been an informal gathering to play some competitive sport. Information on the club's first ever match is limited, but it is known that it took place in Royal Park, which also served as the club's home ground until 1882. The ball used in the match was purchased by a local resident called Tom Jacks, who sold some roofing iron to pay for it. James Henry Gardiner is considered the founder of the club. He continued an active role with North Melbourne until his death in 1921.
Regular premiership matches of Australian Football commenced in Victoria in 1870. Although North Melbourne was a part of this, it was classed as a "junior club". "The Australasian" noted them as being "one of the best of many junior clubs".
The club continued to develop, graduating to senior ranks in 1874 finishing 4th. Along with the promotion, the club adopted its first uniform of blue and white horizontal stripes.
In 1876 North Melbourne amalgamated with Albert-park to form "North Melbourne Cum Albert Park", but by 1877 the local community had raised sufficient funds to reestablish the club, under the new name of "Hotham".
Association years.
Football took a giant step forward in 1877, with the formation of Victoria's first colonial football league, the VFA. Hotham were prime movers in establishing this league and were afforded a place in light of their previous contributions to Australian Football.
The 1880s marked the emergence of the modern identity we now associate with North today. In 1882, the club amalgamated with the Hotham Cricket Club and moved into the North Melbourne Recreation Reserve (Arden St Oval), which remains the home of the club today. The joint venture was aimed at affecting improvements at the Hotham Cricket Ground, which was the name of the Reserve at the time. Four years later the club adopted the traditional uniform of blue and white vertical stripes at the insistence of the VFA, who wanted a visible contrast between Geelong's and Hotham's uniforms. The third significant development occurred in 1888 with the club returning to its original name of the North Melbourne Football Club. This followed the name of the local area reverting from Hotham to North Melbourne.
The 1880s saw the club develop a penchant for inter-colonial travel with trips to Tasmania (1881/1887) and South Australia (1889). Hotham also found itself well represented at the first ever inter-colonial representative game in 1879 with four players from the club gaining selection for Victoria.
Disregarded by the VFL.
The VFA grew to 13 senior clubs in the 1890s. Led by Geelong and Essendon, the largest clubs of the VFA formed their own break away league, the Victorian Football League (VFL), in 1896. Despite finishing 6th in 1896, North Melbourne was not invited to the breakaway competition. The main reasons for being excluded were:
North continued on in the depleted VFA, emerging as a powerhouse, finishing 2nd in 1897, 1898 and 1899. In 1903, after 34 years of competing, the club won its first premiership, defeating Richmond in the final. The club became back to back premiers in 1904 after Richmond forfeited the grand final due to the appointment of an umpire whose performance when the two teams met earlier in the year was severely criticised by Richmond players and officials.
North merged with fellow VFA football club West Melbourne in 1907, which at the time had lost its home ground. The joint venture saw a chance of promotion, and the club applied for admission to the more prestigious VFL in 1908, but Richmond and University were admitted instead. North was kicked out of the VFA during the 1907/08 offseason as a result of applying to join the VFL, before the local community reestablished the North Melbourne Football Club under a new committee, successfully enabling the club to play in the VFA in the 1908 season.
"The Invincibles".
The reformation of the Club necessitated a massive clean out of the team, leaving only two players remaining from the previous season. The 1910 season was marked by one of the most sensational transfers in Victorian football history, when Andy Curran masterminded the clearance of Carlton's famed "Big Four" of 'Mallee' Johnson, Fred Jinks, Charlie Hammond and Frank 'Silver' Caine to North Melbourne. These signings secured the Northerners' third premiership in 1910.
The 1912 finals series was one of the most amazing ever, with the semi-final having to be played three times, after North and Brunswick drew twice. North was eventually victorious and moved on to the final, but lost the game by a mere four points with the last kick of the day.
The next few years were punctuated by "The Invincibles". In the Northerners' most illustrious period ever, the club went undefeated from 1914 to 1919, collecting premierships in 1914, 1915 and 1918 – the league was in recess in 1916 and 1917 due to World War I. As well as this, the club won the championship in both 1915 and 1918 for finishing on top of the ladder, and accounted for VFL side St Kilda comfortably. During this period the club won 58 consecutive matches including 49 successive premiership matches, a record that has remained unmatched in Association or League history since.
Despite being rejected from the VFL in both 1896 and 1907, North persisted in trying to gain admission into the League. On 30 June 1921, North told its players it would disband and try to gain entry to the VFL by the ‘back-door’. Essendon League Football Club had lost its playing ground at East Melbourne and had decided to acquire the North Melbourne Recreation Reserve as a new playing ground. North accepted their proposal in the idea that the clubs would amalgamate. All of North's players were urged to join the Essendon League Club to help facilitate the amalgamation. The amalgamation was foiled when some members of the VFA launched a successful legal challenge. As a result the Essendon League Club moved instead to the Essendon Oval, replacing the ground's original occupants, Essendon Association.
North was now without a playing team and the Essendon Association Club was now without a ground, so as a matter of convenience the two clubs amalgamated so they could compete in the 1922 season. As it had after the merger with West Melbourne, North once again managed to avert its destruction.
Entering the VFL.
After three attempts, 29 years of waiting and numerous other applications to enter the VFL, finally North was rewarded for its persistence with admittance to the League in 1925, along with Footscray and Hawthorn. Even then, the opportunity was almost lost as the League delegates debated into the early hours of the morning on which clubs should be invited to join the intake. It was only after much deliberation that North Melbourne's name was eventually substituted for Prahran's making North "the lucky side" of the invitees that included Footscray and Hawthorn. North Melbourne was forced to change its uniform to avoid a clash when it joined the VFL.
North Melbourne were cellar dwellers for its first twenty-five years of VFL membership, but by the late 1940s had developed a strong list and significant supporter base. In 1949 North secured the VFL Minor Premiership, finishing top of the ladder at the end of the home-and-away season with 14 wins and 5 losses. They failed to make the Grand Final that year (eventually won by Essendon), but in 1950 they did reach the final, defeated by a more efficient Essendon. It was in this year that the club adopted the "Kangaroos" mascot.
In February 1965, North Melbourne moved its playing and training base from the Arden Street Oval to Coburg Oval, signing a seven year lease with the City of Coburg after initially negotiating long-term leases for up to 40 years. The club came to an arrangement to merge with the VFA's Coburg Football Club, whom it was displacing from the ground; fourteen Coburg committeemen joined the North Melbourne committee, but the merger was never completed after Coburg established a rival committee which remained loyal to the VFA. The lease at Coburg lasted only eight months; the Coburg council was hesitant to build a new grandstand without the security of a long-term lease, and neither party made the returns they expected, so it was terminated by mutual agreement in September 1965 and North Melbourne returned to the Arden Street Oval.
Onfield, the 1950s and 1960s were lean years for North Melbourne, though the club did secure two consecutive Night Premierships in 1965 and 1966. Allen Aylett was a brilliant player in the late 1950s and early 1960s (and captain between 1961 and 1964), as was Noel Teasdale, who lost the Brownlow Medal on a countback in 1965 (he was later awarded a retrospective medal when the counting system was amended).
Golden era.
In the late 1960s, under the leadership of Allen Aylett, North Melbourne began its climb to supremacy. As part of a major recruitment drive North secured the services of several big name stars including Barry Davis from Essendon and Doug Wade (Geelong), John Rantall (South Melbourne), Barry Cable (Perth). In a major coup, the great Ron Barassi was appointed coach in 1973. Barrassi reversed the club's playing fortunes, taking an unremarkable team that was once regarded as the traditional cellar dwellers of the competition, through a golden era of success that transformed North into one of the powerhouses of the VFL. Barassi took North to a Grand Final (defeated by Richmond) in 1974 and brought success in his 1975 and 1977 seasons. North made five consecutive Grand Finals from 1974–1978):209 and defeated Norwood in the 1975 national championship to be declared Champions of Australia.
In 1973 and 1974, North's wingman Keith Greig won consecutive Brownlow Medals; forward Malcolm Blight then won the award in 1978. Doug Wade won the Coleman medal in 1974 with his 103 goals for the season.
Barassi remained team coach until 1980, but only a Night Premiership in that year was to result from his last years at Arden Street. North then entered another period of decline, though Malcolm Blight kicked 103 goals to take out the Coleman medal in 1982, and another Brownlow win came through the talented Ross Glendinning in 1983. In that year, North Melbourne won a third Minor Premiership with 16 wins and 6 losses for the season, but failed to make the Grand Final.
Team of the 1990s.
The capable coaching of John Kennedy aside, the 1980s and early 1990s were lean years for the Kangaroos. However, the rebuilding of the club was taking place. The Krakouer brothers (Jim and Phil) brought a spark into the side and lifted many hopes for North supporters and the excitement to the general football public. The innovative idea of night games was instigated by the club and meeting the challenges, the club survived. One major highlight was the recruitment of forward John Longmire in 1989, who topped the club goalkicking over five consecutive seasons (1990–1994) and won the Coleman medal in 1990 with 98 goals. At the beginning of the 1993 season, in a dramatic and controversial move, the board of the club sacked coach and long-time player Wayne Schimmelbusch, and appointed Denis Pagan in his place. Results were immediate, as North reached the finals for the first time in nearly a decade.
Pagan was instrumental in appointing young centre half-forward Wayne Carey as the club's youngest-ever captain. Carey had been recruited at the same time as Longmire, but taken longer to develop as a player. Over the next nine seasons, Carey came to be regarded as the standout player in the league, and was known as 'the King'.
North Melbourne became a powerhouse through the 1990s under Pagan and Carey, and finished in the top four from 1994 until 2000. After being eliminated in the preliminary finals in 1994 and 1995, North went on to defeat the Sydney Swans in the 1996 Grand Final to take out the club's third premiership, and the gold centenary AFL cup; Glenn Archer won the Norm Smith Medal. The club was again eliminated in the preliminary final in 1997. In 1998, as the club won both the pre-season Ansett Cup and topped the ladder with 16 wins and 6 losses, but went on to lose the 1998 Grand Final to Adelaide, not helped by an inaccurate goalkicking performance of 8.22 (70) to Adelaide's 15.15 (105). In 1999, the Kangaroos finished in second position on the ladder, and went on to defeat Carlton in the Grand Final, winning the club's fourth VFL/AFL premiership; former Sydney midfielder Shannon Grant taking out the Norm Smith Medal. The club was eliminated in the preliminary finals in 2000 against Melbourne.
In 1996, the club was in advanced talks with the Fitzroy Football Club, which was in a terminal financial condition, to a merger between the two clubs; however, Fitzroy ultimately merged with the Brisbane Bears instead.
Seeking new markets and greater financial security in an increasingly corporatized AFL environment, the title "North Melbourne" was officially dropped from the logo in 1999, from which time the team played only as the "Kangaroos". During the successful 1999 season, North Melbourne played home games in Sydney with a view of becoming a second team in New South Wales; however, the experiment was not successful, with crowds averaging only 12,000.
21st century.
The 21st century did not begin well for North Melbourne. Its decade-long onfield potency was in decline, questions were raised about its financial position and long-term sustainability. Furthermore, three of the people most important to the club's success in the 1990s left the club under acrimonious circumstances: CEO Greg Miller left the club, captain Wayne Carey left prior to the 2002 season following an extramarital affair with the wife of team-mate and vice captain Anthony Stevens, coach Denis Pagan was lured to Carlton at the end of 2002. Pagan was replaced by 1996 premiership player Dean Laidley, who had previously been an Assistant Coach at Collingwood from 1999 until the end of season 2002.
On a post-season holiday, several players were caught in the 2002 Bali bombing terrorist attack. Forward Jason McCartney suffered severe burns in the incident. He memorably played one final game for the club, on 6 June 2003 against Richmond, and he set up the winning goal with seconds remaining. He retired immediately after the game.
Onfield, the club reached the elimination finals in 2002 and 2005, but otherwise failed to reach the finals from 2001 until 2006.
After two seasons of finals, North Melbourne dropped to 13th in 2009, and coach Dean Laidley, was replaced by ex-Brisbane Lions premiership player and Collingwood assistant coach Brad Scott. A $15M redevelopment of the Arden Street, which had started in 2006, was completed in 2009, giving the club top-class training facilities.
North Melbourne struggled in its first two years under Scott, finishing 9th in both 2010 and 2011. In 2012, the club returned to the finals for the first time since 2008, finishing the season in 8th place but would go down to the West Coast Eagles by 96 points in the elimination final. In 2012, the club began a three-year deal to play two games each year at Blundstone Arena in Hobart, Tasmania. The club finished 10th in 2013 in a season full of close losses.
Nick Dal Santo signed with the club at the end of the 2013 season as a restricted free agent.
In 2014, North Melbourne finished 6th at the end of the home and away season and reached 40,000 members for the first time in the club's history. 
In September, North Melbourne went on to defeat Essendon by 12 points in the 2nd Elimination Final, only taking the lead in the last quarter. The following week, North Melbourne beat Geelong in the 2nd Semi Final by 6 points advancing them through to their first Preliminary Final since 2007. They lost to Sydney by 71 points.
Club symbols and identity.
Name and mascot.
The club was widely known as the 'Shinboners' for much of their early history. The origins of this nickname are unknown but it may have had something to do with the club's reputation for targeting the shinbones of opposition players, or to do with local butchers who showed their support for North by dressing up beef leg-bones in the club colours. By 1926, the club was known as the 'Blue Birds' but this nickname did not last. It was Phonse Tobin, North president from 1953–56, who oversaw the club adopting the Kangaroos emblem in 1954; Tobin found the image of a shinbone unsavoury and wanted the club to have a mascot it could show with pride. In selecting a new name, he wanted something characteristically Australian and got inspiration from a giant Kangaroo he saw on display outside a city store.
The official name of the club is North Melbourne, but the club has gone under several other aliases over the years. The club was originally founded as the 'North Melbourne Football Club', but changed to 'North Melbourne cum Albert Park' after merging with Albert Park in 1876. Following the reformation of the club in 1877, it was known as the 'Hotham Football Club' but later retook the name 'North Melbourne' in 1888. In 1998 the club proposed changing its name to the 'Northern Kangaroos', but it was rejected by the AFL. Between 1999 and 2007, the club traded without much success as 'The Kangaroos' in a bid to increase its appeal nationally; this decision was reversed at the end of 2007, and the club has once again reverted to the name 'North Melbourne'.
Guernsey.
North Melbourne's guernsey since entering the VFL in 1925 has consisted of white and royal blue vertical stripes. The guernsey is predominantly white.
The current clash guernsey is a reversed version of the home strip, with blue stripes where the white stripes traditionally are placed and vice versa; as such, the clash strip is predominantly royal blue, rather than predominantly white, creating a much darker design.
Club song.
"Join in the Chorus" is the official anthem of the North Melbourne Football Club. It is sung to the tune of a Scottish folk song "A Wee Deoch an Doris", from around 1911.
The famed song is generally sung, in accordance to common football tradition, after a victory. It is also played before every match.
"Join in the Chorus" is believed to be the oldest club anthem of any AFL club, and has been associated with North from its early VFA days. The preamble of the song originates from a score of a Theatre Musical called 'Australia: Heart to Heart and Hand to Hand" written by Toso Taylor in the 1890s in pre-federation Australia. The second verse is unknown in origin and was presumably added later by members of the North Melbourne Football Club when the song was chosen as the club theme. The chorus was appropriated from a song written and performed by Scottish musician Harry Lauder. The recording currently used by the club was performed by the Fable Singers in April 1972 and only includes the choruses.
The song has a strong Victorian heritage, and has been traditionally sung by the Victorian State Football and Victorian Cricket teams respectively. The lyrics have occasionally been changed, including updating the year in the song ("e.g." "North Melbourne will be premiers in 1993"), or to remove the words "North Melbourne" during the period when the club was competing as 'Kangaroos'.
For the 2015 premiership season, "You Am I" lead singer Tim Rogers, a fan of North Melbourne, announced he will assist in an updated version of the club song including the 2 versus. This version is only played at North home games as they run out onto the ground.
Shinboner spirit.
The term "Shinboner spirit" is often used to refer to camaraderie and determination of players or members of the North Melbourne Football Club. The term persists to the modern day, despite North Melbourne having switched its official nickname from Shinboners to Kangaroos in the 1950s.
Because it relates to the club's original nickname, Shinboner spirit is often associated with the complete history of the club. In 2005, to celebrate the club's 80th anniversary of senior competition in the VFL and the thirtieth anniversary of the first VFL premiership, the Kangaroos held a "Shinboner Spirit" gala event, attended by almost the entire surviving playing list. In the awards ceremony, the key Shinboners of the past eighty years were acknowledged, and Glenn Archer was named the "Shinboner of the Century".
Corporate.
Ownership.
The North Melbourne Football Club is non a profit organisation limited by guarantee. Members of the club serve as the guarantees of capital, and have full voting rights at AGMs to elect directors to the club's board.
The club's board compromises nine directors with each director serving a 3-year term before their position is put up for re-election at an AGM. Only one-third of the board is contested at each AGM due to the rolling structure of the terms of the directors. This structure safeguards the entire board from being ousted at a single AGM, and has made North Melbourne immune to a lot of the in house fighting witnessed at other AFL football clubs. The board governs the club as well as selecting a chairman to head the club through a majority vote of directors.
North Melbourne is unique in its structure, because from 1986 to 2006 the club was privately owned and limited by shares. The club was floated in 1986 through a membership vote led by then chairman Bob Ansett. At the meeting, members were encouraged to buy into the club by purchasing shares. The float ended up raising over $3 million and helped to keep the club solvent through the next decade.
In 1991, the John Elliott-led Carlton Football Club attempted a hostile take over North Melbourne by purchasing a large parcel of shares formerly owned by Bob Ansett. The Blues acquired 20 per cent of the capital but that stake was eventually bought back by John Magowan, the former head of Merrill Lynch Australia, in 2001. The resulting melodrama saw the formation of B-Class shareholders who had the effective power of veto over any attempt to merge or relocate the club.
Further takeover attempts were made in the first decade of the 21st century by the Southport Sharks. Then chairman Allan Aylett knocked back a proposal from the Sharks that would have seen them gain a majority stake in the club in exchange for an injection of capital. In early 2006, another proposal from Sharks to underwrite Kangaroos games on the Gold Coast, in exchange for a slice of the shareholder structure at the club was knocked back after AFL intervention.
Due to an Australian Tax Office ruling in 2006, the club proposed a shareholder restructure that would have seen the B Class shareholders power reduced significantly and some voting rights returned to members. This was done to avoid extraordinary taxes being placed on the club, but the move was blocked in December by Bob Ansett and his proxies who feared that the restructure would make the club vulnerable to further takeover bids.
On 28 February 2007, another meeting was called to resolve the shareholder issue, and a motion was passed that would return see some voting rights return to members and stop any future tax increments.
In April 2007 it was revealed the AFL was attempting to buy out the shareholders of the club in a bid to gain full ownership, and force a relocation of the club to the Gold Coast.
During October 2007, a group called We Are North Melbourne emerged and launched a public campaign, calling for ordinary members to be given the final say on the relocation issue. While the group became synonymous with the push to keep the club in Melbourne, its first priority was to see the club's shareholder structure wound-up and control returned to ordinary members.
North Melbourne reverted to public company in November 2008. A moratorium was passed at an extraordinary general meeting that will allow James Brayshaw's board to serve unopposed until 2010, so as to allow his ticket the maximum time to enact their policies to make the North Melbourne Football Club financially viable.
Membership base.
In 2014 The North Melbourne Football club achieved a record membership of 40,000 members including 4,145 from Tasmania. The figure is a 78% increase since 2007 when the club faced possible relocation.
Season 2015 is shaping as a club defying year, with the club set to break another membership record and forever bury the "team who have no fans" statement.
Reputation.
Night football.
In 1985, North Melbourne pioneered the concept of playing football on Friday nights. Since then, North Melbourne has played the most Friday night games of any AFL club.
Friday night matches later became the most lucrative timeslot for televised games, and North Melbourne's relatively low supporter base resulted in less Friday night matches. Since 2010, North Melbourne has hosted an annual Friday night match against Carlton in recognition of its pioneering role in the concept.
Indigenous players.
North Melbourne has a strong history of supporting Aboriginal footballers and fostering Aboriginal talent in the VFL and AFL. The first indigenous footballer to play for the club was Percy Johnson in the 1950s, and was followed by other fan favorites like Bertie Johnson, Barry Cable and the Krakouer brothers in the following decades.
The following is a list of Indigenous footballers to have played senior football at the club:
Killed in action.
The following footballers who were killed in action during the World Wars played senior football for North Melbourne.
Rivalries.
Major.
 Essendon (main) – North and Essendon have a chequered history that dates back to the late 19th century; firstly in 1896, Essendon had North excluded from the VFL because both clubs drew supporters from the same area. North supporters have long been bitter with Essendon for excluding them from the VFL, and have blamed that for their small supporter base in comparison to Essendon's. North's first VFL Grand Final was against Essendon in 1950. The rivalry was reignited in the 90s; in 1998, following comments by Essendon coach labelling Kangaroos executives Greg Miller and Mark Dawson "marshmallows", a reference to their softness, North supporters threw marshmallows at Sheedy after the opening Qualifying Final.
 Hawthorn – North and Hawthorn have a fierce rivalry that dates back to the 1970s when they played off against each other in three Grand Finals in the space of four years. From 1974 to 1978 the two clubs played against each other in ten finals, and took each other on for the Australian Championship in Adelaide in 1976. During the 1980s Hawthorn dominated North, and during the 90s the results were reversed with North dominating Hawthorn.
Minor.
 Collingwood – North and Collingwood have a small kind of rivalry between them. As they drew in the 1977 Grand Final, which North went on to win in the replay by 27 points.
 Carlton – Although not known for their on-field rivalry, North Melbourne and Carlton have endured a tumultuous off-field relationship. In 1965, North Melbourne moved to the Coburg City Oval, which Carlton opposed as an invasion of its own territory: Coburg was located within Carlton's recruiting zone, and was home to about of quarter of Carlton's members at the time. Later, after North Melbourne was listed on the stock market in 1987, Carlton bought 20% of the shares to become part-owner of the rival club. In 1988 there was a merger proposal put to North Melbourne, that was rejected.
 Port Adelaide- Port Adelaide often refer to North Melbourne as the "bogey side" and as of 2014, North lead 21-8. Their most recent was in round 3 2014, where North came from behind to win by 7 points. This was Port's only loss in the first 12 rounds of 2014.
Port Adelaide lost their first finals appearance to North Melbourne in 1999 in the qualifying final, and in Port's premiership year of 2004, North beat Port by 92 points, however, Port beat North Melbourne in the 2007 Preliminary Final by 87 points.
Club honour board.
North Melbourne Team of the Century.
At a special function in August 2001 the North Melbourne Team of the Century was announced. There was no minimum number of games set for selection. Wayne Carey was named as captain and Denis Pagan as coach. The selection panel was Geoff Poulter (journalist), Father Gerard Dowling (club historian), Keith McKenzie (former coach), Lloyd Holyoak (former president), Max Ritchie (former player and chairman of selectors) and Greg Miller (chief executive).
Shinboner of the Century.
On 18 March 2005, the North Melbourne football club held a special gala dinner entitled the "North Story" to celebrate the 80th anniversary of North's admission to the VFL, and the 30th anniversary of the club's first VFL premiership. Over 3500 people attended the historic event held at the Royal Exhibition Building, including almost all surviving North Melbourne players. Glenn Archer was voted the Shinboner of the Century by his peers as the player who most represents the 'Shinboner Spirit'. The following players were voted 'Shinboners' of their era:

</doc>
<doc id="22107" url="http://en.wikipedia.org/wiki?curid=22107" title="Treaty on the Non-Proliferation of Nuclear Weapons">
Treaty on the Non-Proliferation of Nuclear Weapons

The Treaty on the Non-Proliferation of Nuclear Weapons, commonly known as the Non-Proliferation Treaty or NPT, is an international treaty whose objective is to prevent the spread of nuclear weapons and weapons technology, to promote cooperation in the peaceful uses of nuclear energy, and to further the goal of achieving nuclear disarmament and general and complete disarmament.
Opened for signature in 1968, the Treaty entered into force in 1970. On 11 May 1995, the Treaty was extended indefinitely. More countries have adhered to the NPT than any other arms limitation and disarmament agreement, a testament to the Treaty's significance. A total of 191 states have joined the Treaty, though North Korea, which acceded to the NPT in 1985 but never came into compliance, announced its withdrawal in 2003. Four UN member states have never joined the NPT: India, Israel, Pakistan and South Sudan.
The treaty recognizes five states as nuclear-weapon states: the United States, Russia, the United Kingdom, France, and China (also the five permanent members of the United Nations Security Council). Four other states are known or believed to possess nuclear weapons: India, Pakistan and North Korea have openly tested and declared that they possess nuclear weapons, while Israel has had a policy of opacity regarding its nuclear weapons program.
The NPT consists of a preamble and eleven articles. Although the concept of "pillars" is not expressed anywhere in the NPT, the treaty is nevertheless sometimes interpreted as a "three-pillar" system, with an implicit balance among them:
The NPT is often seen to be based on a central bargain: “the NPT non-nuclear-weapon states agree never to acquire nuclear weapons and the NPT nuclear-weapon states in exchange agree to share the benefits of peaceful nuclear technology and to pursue nuclear disarmament aimed at the ultimate elimination of their nuclear arsenals”. The treaty is reviewed every five years in meetings called Review Conferences of the Parties to the Treaty of Non-Proliferation of Nuclear Weapons. Even though the treaty was originally conceived with a limited duration of 25 years, the signing parties decided, by consensus, to extend the treaty indefinitely and without conditions during the Review Conference in New York City on 11 May 1995, culminating successful U.S. government efforts led by Ambassador Thomas Graham Jr..
At the time the NPT was proposed, there were predictions of 25–30 nuclear weapon states within 20 years. Instead, over forty years later, five states are not parties to the NPT, and they include the only four additional states believed to possess nuclear weapons. Several additional measures have been adopted to strengthen the NPT and the broader nuclear nonproliferation regime and make it difficult for states to acquire the capability to produce nuclear weapons, including the export controls of the Nuclear Suppliers Group and the enhanced verification measures of the IAEA Additional Protocol.
Critics argue that the NPT cannot stop the proliferation of nuclear weapons or the motivation to acquire them. They express disappointment with the limited progress on nuclear disarmament, where the five authorized nuclear weapons states still have 22,000 warheads in their combined stockpile and have shown a reluctance to disarm further. Several high-ranking officials within the United Nations have said that they can do little to stop states using nuclear reactors to produce nuclear weapons.
Treaty "pillars".
The NPT is commonly described as having three main "pillars": non-proliferation, disarmament, and peaceful use. This "pillars" concept has been questioned by some who believe that the NPT is, as its name suggests, principally about nonproliferation, and who worry that "three pillars" language misleadingly implies that the three elements have equivalent importance.
First pillar: non-proliferation.
Five states are recognized by the Non-Proliferation Treaty as nuclear weapon states (NWS): China (signed 1992), France (1992), the Soviet Union (1968; obligations and rights now assumed by the Russian Federation), the United Kingdom (1968), and the United States (1968) (The United States, UK, and the Soviet Union – the World War II's “Big Three” — were the only states openly possessing such weapons among the original ratifiers of the treaty, which entered into force in 1970). These five nations are also the five permanent members of the United Nations Security Council.
These five NWS agree not to transfer "nuclear weapons or other nuclear explosive devices" and "not in any way to assist, encourage, or induce" a non-nuclear weapon state (NNWS) to acquire nuclear weapons (Article I). NNWS parties to the NPT agree not to "receive," "manufacture" or "acquire" nuclear weapons or to "seek or receive any assistance in the manufacture of nuclear weapons" (Article II). NNWS parties also agree to accept safeguards by the International Atomic Energy Agency (IAEA) to verify that they are not diverting nuclear energy from peaceful uses to nuclear weapons or other nuclear explosive devices (Article III).
The five NWS parties have made undertakings not to use their nuclear weapons against a non-NWS party except in response to a nuclear attack, or a conventional attack in alliance with a Nuclear Weapons State. However, these undertakings have not been incorporated formally into the treaty, and the exact details have varied over time. The U.S. also had nuclear warheads targeted at North Korea, a non-NWS, from 1959 until 1991. The previous United Kingdom Secretary of State for Defence, Geoff Hoon, has also explicitly invoked the possibility of the use of the country's nuclear weapons in response to a non-conventional attack by "rogue states". In January 2006, President Jacques Chirac of France indicated that an incident of state-sponsored terrorism on France could trigger a small-scale nuclear retaliation aimed at destroying the "rogue state's" power centers.
Second pillar: disarmament.
Article VI of the NPT represents the only binding commitment in a multilateral treaty to the goal of disarmament by the nuclear-weapon States. The NPT's preamble contains language affirming the desire of treaty signatories to ease international tension and strengthen international trust so as to create someday the conditions for a halt to the production of nuclear weapons, and treaty on general and complete disarmament that liquidates, in particular, nuclear weapons and their delivery vehicles from national arsenals.
The wording of the NPT's Article VI arguably imposes only a vague obligation on all NPT signatories to move in the general direction of nuclear and total disarmament, saying, "Each of the Parties to the Treaty undertakes to pursue negotiations in good faith on effective measures relating to cessation of the nuclear arms race at an early date and to nuclear disarmament, and on a treaty on general and complete disarmament." Under this interpretation, Article VI does not strictly require all signatories to actually conclude a disarmament treaty. Rather, it only requires them "to negotiate in good faith."
On the other hand, some governments, especially non-nuclear-weapon states belonging to the Non-Aligned Movement, have interpreted Article VI's language as being anything but vague. In their view, Article VI constitutes a formal and specific obligation on the NPT-recognized nuclear-weapon states to disarm themselves of nuclear weapons, and argue that these states have failed to meet their obligation. The International Court of Justice (ICJ), in its advisory opinion on the Legality of the Threat or Use of Nuclear Weapons, issued 8 July 1996, unanimously interprets the text of Article VI as implying that
"There exists an obligation to pursue in good faith and bring to a conclusion negotiations leading to nuclear disarmament in all its aspects under strict and effective international control."
The ICJ opinion notes that this obligation involves all NPT parties (not just the nuclear weapon states) and does not suggest a specific time frame for nuclear disarmament.
Critics of the NPT-recognized nuclear-weapon states (the United States, Russia, China, France, and the United Kingdom) sometimes argue that what they view as the failure of the NPT-recognized nuclear weapon states to disarm themselves of nuclear weapons, especially in the post–Cold War era, has angered some non-nuclear-weapon NPT signatories of the NPT. Such failure, these critics add, provides justification for the non-nuclear-weapon signatories to quit the NPT and develop their own nuclear arsenals.
Other observers have suggested that the linkage between proliferation and disarmament may also work the other way, i.e., that the failure to resolve proliferation threats in Iran and North Korea, for instance, will cripple the prospects for disarmament. No current nuclear weapons state, the argument goes, would seriously consider eliminating its last nuclear weapons without high confidence that other countries would not acquire them. Some observers have even suggested that the very progress of disarmament by the superpowers—which has led to the elimination of thousands of weapons and delivery systems—could eventually make the possession of nuclear weapons more attractive by increasing the perceived strategic value of a small arsenal. As one U.S. official and NPT expert warned in 2007, "logic suggests that as the number of nuclear weapons decreases, the 'marginal utility' of a nuclear weapon as an instrument of military power increases. At the extreme, which it is precisely disarmament's hope to create, the strategic utility of even one or two nuclear weapons would be huge."
Third pillar: peaceful use of nuclear energy.
The third pillar allows for and agrees upon the transfer of nuclear technology and materials to NPT signatory countries for the development of civilian nuclear energy programs in those countries, as long as they can demonstrate that their nuclear programs are not being used for the development of nuclear weapons.
Since very few of the states with nuclear energy programs are willing to abandon the use of nuclear energy, the third pillar of the NPT under Article IV provides other states with the possibility to do the same, but under conditions intended to make it difficult to develop nuclear weapons.
The treaty recognizes the inalienable right of sovereign states to use nuclear energy for peaceful purposes, but restricts this right for NPT parties to be exercised "in conformity with Articles I and II" (the basic nonproliferation obligations that constitute the "first pillar" of the Treaty). As the commercially popular light water reactor nuclear power station uses enriched uranium fuel, it follows that states must be able either to enrich uranium or purchase it on an international market. Mohamed ElBaradei, then Director General of the International Atomic Energy Agency, has called the spread of enrichment and reprocessing capabilities the "Achilles' heel" of the nuclear nonproliferation regime. As of 2007 13 states have an enrichment capability.
Because the availability of fissile material has long been considered the principal obstacle to, and "pacing element" for, a country's nuclear weapons development effort, it was declared a major emphasis of U.S. policy in 2004 to prevent the further spread of uranium enrichment and plutonium reprocessing (a.k.a. "ENR") technology. Countries possessing ENR capabilities, it is feared, have what is in effect the option of using this capability to produce fissile material for weapons use on demand, thus giving them what has been termed a "virtual" nuclear weapons program. The degree to which NPT members have a "right" to ENR technology notwithstanding its potentially grave proliferation implications, therefore, is at the cutting edge of policy and legal debates surrounding the meaning of Article IV and its relation to Articles I, II, and III of the Treaty.
Countries that have signed the treaty as Non-Nuclear Weapons States and maintained that status have an unbroken record of not building nuclear weapons. However, Iraq was cited by the IAEA with punitive sanctions enacted against it by the UN Security Council for violating its NPT safeguards obligations; North Korea never came into compliance with its NPT safeguards agreement and was cited repeatedly for these violations, and later withdrew from the NPT and tested multiple nuclear devices; Iran was found in non-compliance with its NPT safeguards obligations in an unusual non-consensus decision because it "failed in a number of instances over an extended period of time" to report aspects of its enrichment program;<ref name="IAEA-GOV/2003/75"></ref><ref name="IAEA-GOV/2005/77"></ref> and Libya pursued a clandestine nuclear weapons program before abandoning it in December 2003.
In 1991 Romania reported previously undeclared nuclear activities by the former regime and the IAEA reported this non-compliance to the Security Council for information only. In some regions, the fact that all neighbors are verifiably free of nuclear weapons reduces any pressure individual states might feel to build those weapons themselves, even if neighbors are known to have peaceful nuclear energy programs that might otherwise be suspicious. In this, the treaty works as designed.
In 2004, Mohamed ElBaradei said that by some estimates thirty-five to forty states could have the knowledge to develop nuclear weapons.
Key articles.
"Article I": Each nuclear-weapons state (NWS) undertakes not to transfer, to any recipient, nuclear weapons, or other nuclear explosive devices, and not to assist any non-nuclear weapon state to manufacture or acquire such weapons or devices.
"Article II": Each non-NWS party undertakes not to receive, from any source, nuclear weapons, or other nuclear explosive devices; not to manufacture or acquire such weapons or devices; and not to receive any assistance in their manufacture.
"Article III": Each non-NWS party undertakes to conclude an agreement with the IAEA for the application of its safeguards to all nuclear material in all of the state's peaceful nuclear activities and to prevent diversion of such material to nuclear weapons or other nuclear explosive devices.
"Article IV": 1. Nothing in this Treaty shall be interpreted as affecting the inalienable right of all the Parties to the Treaty to develop research, production and use of nuclear energy for peaceful purposes without discrimination and in conformity with Articles I and II of this Treaty.
2. All the Parties to the Treaty undertake to facilitate, and have the right to participate in, the fullest possible exchange of equipment, materials and scientific and technological information for the peaceful uses of nuclear energy. Parties to the Treaty in a position to do so shall also co-operate in contributing alone or together with other States or international organizations to the further development of the applications of nuclear energy for peaceful purposes, especially in the territories of non-nuclear-weapon States Party to the Treaty, with due consideration for the needs of the developing areas of the world.
"Article VI": Each party "undertakes to pursue negotiations in good faith on effective measures relating to cessation of the nuclear arms race at an early date and to nuclear disarmament, and on a Treaty on general and complete disarmament under strict and effective international control".
"Article X". Establishes the right to withdraw from the Treaty giving 3 months' notice. It also establishes the duration of the Treaty (25 years before 1995 Extension Initiative).
History.
The impetus behind the NPT was concern for the safety of a world with many nuclear weapon states. It was recognized that the cold war deterrent relationship between just the United States and Soviet Union was fragile. Having more nuclear-weapon states would reduce security for all, multiplying the risks of miscalculation, accidents, unauthorized use of weapons, or from escalation in tensions, nuclear conflict.
The NPT process was launched by Frank Aiken, Irish Minister for External Affairs, in 1958. It was opened for signature in 1968, with Finland the first State to sign. Accession became nearly universal after the end of the Cold War and of South African apartheid.
In 1992 China and France acceded to the NPT, the last of the five nuclear powers recognized by the treaty to do so.
In 1995 the treaty was extended indefinitely. After Brazil acceded to the NPT in 1998 the only remaining non-nuclear-weapons state which had not signed was Cuba, which joined NPT (and the Treaty of Tlatelolco NWFZ) in 2002.
Several NPT signatories have given up nuclear weapons or nuclear weapons programs.
South Africa undertook a nuclear weapons program, but has since renounced its nuclear program and signed the treaty in 1991 after destroying its small nuclear arsenal; after this, the remaining African countries signed the treaty. The former Soviet Republics where nuclear weapons had been based, namely Ukraine, Belarus and Kazakhstan, transferred those weapons to Russia and joined NPT by 1994.
Successor states from the breakups of Yugoslavia and Czechoslovakia also joined the treaty soon after their independence. Montenegro and East Timor were the last countries to sign the treaty on their independence in 2006 and 2003; the only other country to sign in the 21st century was Cuba in 2002. The three Micronesian countries in Compact of Free Association with the USA joined NPT in 1995, along with Vanuatu.
Major South American countries Argentina, Chile, and Brazil joined in 1995 and 1998. Arabian Peninsula countries included Saudi Arabia and Bahrain in 1988, Qatar and Kuwait in 1989, UAE in 1995, and Oman in 1997. The tiny European states of Monaco and Andorra joined in 1995-6. Also signing in the 1990s were Myanmar in 1992 and Guyana in 1993.
United States-NATO nuclear weapons sharing.
At the time the treaty was being negotiated, NATO had in place secret nuclear weapons sharing agreements whereby the United States provided nuclear weapons to be deployed by, and stored in, other NATO states. Some argue this is an act of proliferation violating Articles I and II of the treaty. A counter-argument is that the U.S. controlled the weapons in storage within the NATO states, and that no transfer of the weapons or control over them was intended "unless and until a decision were made to go to war, at which the treaty would no longer be controlling", so there is no breach of the NPT. These agreements were disclosed to a few of the states, including the Soviet Union, negotiating the treaty, but most of the states that signed the NPT in 1968 would not have known about these agreements and interpretations at that time.
As of 2005, it is estimated that the United States still provides about 180 tactical B61 nuclear bombs for use by Belgium, Germany, Italy, the Netherlands and Turkey under these NATO agreements. Many states, and the Non-Aligned Movement, now argue this violates Articles I and II of the treaty, and are applying diplomatic pressure to terminate these agreements. They point out that the pilots and other staff of the "non-nuclear" NATO states practice handling and delivering the U.S. nuclear bombs, and non-U.S. warplanes have been adapted to deliver U.S. nuclear bombs which must have involved the transfer of some technical nuclear weapons information. NATO believes its "nuclear forces continue to play an essential role in war prevention, but their role is now more fundamentally political".
U.S. nuclear sharing policies were originally designed to help prevent the proliferation of nuclear weapons—not least by persuading the then West Germany not to develop an independent nuclear capability by assuring it that West Germany would be able, in the event of war with the Warsaw Pact, to wield (U.S.) nuclear weapons in self-defense. (Until that point of all-out war, however, the weapons themselves would remain in U.S. hands.) The point was to limit the spread of countries having their own nuclear weapons programs, helping ensure that NATO allies would not choose to go down the proliferation route. (West Germany was discussed in U.S. intelligence estimates for a number of years as being a country with the potential to develop nuclear weapons capabilities of its own if officials in Bonn were not convinced that their defense against the Soviet Union and its allies could otherwise be met.)
India, Israel, and Pakistan.
Three states—India, Israel, and Pakistan—have never signed the treaty. India and Pakistan are confirmed nuclear powers, and Israel has a long-standing policy of deliberate ambiguity (see List of countries with nuclear weapons). India argues that the NPT creates a club of "nuclear haves" and a larger group of "nuclear have-nots" by restricting the legal possession of nuclear weapons to those states that tested them before 1967, but the treaty never explains on what ethical grounds such a distinction is valid. India's then External Affairs Minister Pranab Mukherjee said during a visit to Tokyo in 2007: "If India did not sign the NPT, it is not because of its lack of commitment for non-proliferation, but because we consider NPT as a flawed treaty and it did not recognize the need for universal, non-discriminatory verification and treatment."
India and Pakistan have publicly announced possession of nuclear weapons and have detonated nuclear devices in tests, India having first done so in 1974 and Pakistan following suit in 1998 in response to another Indian test. India is estimated to have enough fissile material for more than 150 warheads. Pakistan reportedly has between 80 and 120 warheads according to the former head of its strategic arms division. India was among the few countries to have a no first use policy, a pledge not to use nuclear weapons unless first attacked by an adversary using nuclear weapons, however India's NSA Shivshankar Menon signaled "a significant shift from "no first use" to "no first use against non-nuclear weapon states"" in a speech on the occasion of Golden Jubilee celebrations of the National Defence College in New Delhi on 21 October 2010, a doctrine Menon said reflected India's "strategic culture, with its emphasis on minimal deterrence". 
According to leaked intelligence, Israel has been developing nuclear weapons at its Dimona site in the Negev since 1958, and many nonproliferation analysts like David Albright estimate that Israel may have stockpiled between 100 to 200 warheads using the plutonium reprocessed from Dimona. The Israeli government refuses to confirm or deny possession of nuclear weapons, although this is now regarded as an open secret after Israeli low level nuclear technician Mordechai Vanunu—subsequently arrested and sentenced for treason by Israel—published evidence about the program to the British "Sunday Times" in 1986.
On 18 September 2009 the General Conference of the International Atomic Energy Agency called on Israel to open its nuclear facilities to IAEA inspection and adhere to the non-proliferation treaty as part of a resolution on "Israeli nuclear capabilities," which passed by a narrow margin of 49–45 with 16 abstentions. The chief Israeli delegate stated that "Israel will not co-operate in any matter with this resolution."
In early March 2006, India and the United States finalized an agreement, in the face of criticism in both countries, to restart cooperation on civilian nuclear technology. Under the deal India has committed to classify 14 of its 22 nuclear power plants as being for civilian use and to place them under IAEA safeguards. Mohamed ElBaradei, then Director General of the IAEA, welcomed the deal by calling India "an important partner in the non-proliferation regime."
In December 2006, United States Congress approved the United States-India Peaceful Atomic Energy Cooperation Act that was cemented during President Bush's visit to India earlier in the year. The legislation allows for the transfer of civilian nuclear material to India. Despite its status outside the Nuclear Non-Proliferation Treaty, India was granted these transactions on the basis of its clean non-proliferation record, and India's need for energy fueled by its rapid industrialization and a billion-plus population.
On 1 August 2008, the IAEA approved the India Safeguards Agreement and on 6 September 2008, India was granted the waiver at the Nuclear Suppliers Group (NSG) meeting held in Vienna, Austria. The consensus was arrived after overcoming misgivings expressed by Austria, Ireland and New Zealand and is an unprecedented step in giving exemption to a country, which has not signed the NPT and the Comprehensive Test Ban Treaty (CTBT). While India could commence nuclear trade with other willing countries. The U.S. Congress approved this agreement and President Bush signed it on 8 October 2008.
The NSG Guidelines currently rule out nuclear exports by all major suppliers to Pakistan and Israel, with very narrow exceptions, since neither has full-scope IAEA safeguards (i.e. safeguards on all its nuclear activities). Attempts by Pakistan to reach a similar agreement have been rebuffed by the United States and other NSG members. The argument put forth is that not only does Pakistan lack the same energy requirements but that the track record of Pakistan as a nuclear proliferator makes it impossible for it to have any sort of nuclear deal in the near future.
By 2010, China reportedly signed a civil nuclear deal with Pakistan claiming that the deal was "peaceful."
The British government looked askance at the deal purporting that 'the time is not yet right for a civil nuclear deal with Pakistan'.
In the Pakistan-China case, the parties did not seek formal approval from the nuclear suppliers group, and China preferred to "grandfather" the reactor.
Exponents of arms control denounced the China-Pakistan deal as they did in case of U.S.-India deal claiming that both the deals violate the NPT by facilitating nuclear programmes in states which are not parties to the NPT.
s of January 2011[ [update]], Australia, a top three producer and home to worlds largest known reserves, had continued its refusal to export Uranium to India despite diplomatic pressure from India.
In November 2011 the Australian Prime Minister announced a desire to allow exports to India, a policy change which was authorized by her party's national conference in December. On 4 December 2011, Prime Minister Julia Gillard overturned Australia's long-standing ban on exporting uranium to India. She further said "We should take a decision in the national interest, a decision about strengthening our strategic partnership with India in this the Asian century," and said that any agreement to sell uranium to India would include strict safeguards to ensure it would only be used for civilian purposes, and not end up in nuclear weapons. On Sep 5, 2014; Australian Prime Minister Tony Abbott sealed a civil nuclear deal to sell uranium to India. "We signed a nuclear cooperation agreement because Australia trusts India to do the right thing in this area, as it has been doing in other areas," Abbott told reporters after he and Indian Prime Minister Narendra Modi signed a pact to sell uranium for peaceful power generation.
North Korea.
North Korea ratified the treaty on 12 December 1985, but gave notice of withdrawal from the treaty on 10 January 2003 following U.S. allegations that it had started an illegal enriched uranium weapons program, and the U.S. subsequently stopping fuel oil shipments under the Agreed Framework which had resolved plutonium weapons issues in 1994. The withdrawal became effective 10 April 2003 making North Korea the first state ever to withdraw from the treaty. North Korea had once before announced withdrawal, on 12 March 1993, but suspended that notice before it came into effect.
On 10 February 2005, North Korea publicly declared that it possessed nuclear weapons and pulled out of the six-party talks hosted by China to find a diplomatic solution to the issue. "We had already taken the resolute action of pulling out of the Nuclear Non-Proliferation Treaty and have manufactured nuclear arms for self-defence to cope with the Bush administration's evermore undisguised policy to isolate and stifle the DPRK [Democratic People's Republic of Korea]," a North Korean Foreign Ministry statement said regarding the issue. Six-party talks resumed in July 2005.
On 19 September 2005, North Korea announced that it would agree to a preliminary accord. Under the accord, North Korea would scrap all of its existing nuclear weapons and nuclear production facilities, rejoin the NPT, and readmit IAEA inspectors. The difficult issue of the supply of light water reactors to replace North Korea's indigenous nuclear power plant program, as per the 1994 Agreed Framework, was left to be resolved in future discussions. On the next day North Korea reiterated its known view that until it is supplied with a light water reactor it will not dismantle its nuclear arsenal or rejoin the NPT.
On 2 October 2006, the North Korean foreign minister announced that his country was planning to conduct a nuclear test "in the future", although it did not state when. On Monday, 9 October 2006 at 01:35:28 (UTC) the United States Geological Survey detected a magnitude 4.3 seismic event 70 km north of Kimchaek, North Korea indicating a nuclear test. The North Korean government announced shortly afterward that they had completed a successful underground test of a nuclear fission device.
In 2007, reports from Washington suggested that the 2002 CIA reports stating that North Korea was developing an enriched uranium weapons program, which led to North Korea leaving the NPT, had overstated or misread the intelligence. On the other hand, even apart from these press allegations—which some critics worry could have been planted in order to justify the United States giving up trying to verify the dismantlement of Pyongyang's uranium program in the face of North Korean intransigence—there remains some information in the public record indicating the existence of a uranium effort. Quite apart from the fact that North Korean First Vice Minister Kang Sok Ju at one point admitted the existence of a uranium enrichment program, Pakistan's then-President Musharraf revealed that the A.Q. Khan proliferation network had provided North Korea with a number of gas centrifuges designed for uranium enrichment. Additionally, press reports have cited U.S. officials to the effect that evidence obtained in dismantling Libya's WMD programs points toward North Korea as the source for Libya's uranium hexafluoride (UF6) – which, if true, would mean that North Korea has a uranium conversion facility for producing feedstock for centrifuge enrichment.
Iran.
Iran is a party to the NPT but was found in non-compliance with its NPT safeguards agreement and the status of its nuclear program remains in dispute. In November 2003 IAEA Director General Mohamed ElBaradei reported that Iran had repeatedly and over an extended period failed to meet its safeguards obligations, including by failing to declare its uranium enrichment program. After about two years of EU3-led diplomatic efforts and Iran temporarily suspending its enrichment program, the IAEA Board of Governors, acting under Article XII.C of the IAEA Statute, found in a rare non-consensus decision with 12 abstentions that these failures constituted non-compliance with the IAEA safeguards agreement. This was reported to the UN Security Council in 2006, after which the Security Council passed a resolution demanding that Iran suspend its enrichment.
Instead, Iran resumed its enrichment program.
The IAEA has been able to verify the non-diversion of declared nuclear material in Iran, and is continuing its work on verifying the absence of undeclared activities. In February 2008, the IAEA also reported that it was working to address "alleged studies" of weaponization, based on documents provided by certain Member States, which those states claimed originated from Iran. Iran rejected the allegations as "baseless" and the documents as "fabrications." In June 2009, the IAEA reported that Iran had not “cooperated with the Agency in connection with the remaining issues ... which need to be clarified to exclude the possibility of military dimensions to Iran's nuclear program.”
The United States concluded that Iran violated its Article III NPT safeguards obligations, and further argued based on circumstantial evidence that Iran's enrichment program was for weapons purposes and therefore violated Iran's Article II nonproliferation obligations. The November 2007 US National Intelligence Estimate (NIE) later concluded that Iran had halted an active nuclear weapons program in the fall of 2003 and that it had remained halted as of mid-2007. The NIE's "Key Judgments," however, also made clear that what Iran had actually stopped in 2003 was only "nuclear weapon design and weaponization work and covert uranium conversion-related and uranium enrichment-related work"-namely, those aspects of Iran's nuclear weapons effort that had not by that point already been leaked to the press and become the subject of IAEA investigations.
Since Iran's uranium enrichment program at Natanz—and its continuing work on a heavy water reactor at Arak that would be ideal for plutonium production—began secretly years before in conjunction with the very weaponization work the NIE discussed and for the purpose of developing nuclear weapons, many observers find Iran's continued development of fissile material production capabilities distinctly worrying. Particularly because fissile material availability has long been understood to be the principal obstacle to nuclear weapons development and the primary "pacing element" for a weapons program, the fact that Iran has reportedly suspended weaponization work may not mean very much. As U.S. Director of National Intelligence Mike McConnell has put it, the aspects of its work that Iran allegedly suspended were thus "probably the least significant part of the program."
Iran states it has a legal right to enrich uranium for peaceful purposes under the NPT, and further says that it "has constantly complied with its obligations under the NPT and the Statute of the International Atomic Energy Agency". Iran also states that its enrichment program is part of its civilian nuclear energy program, which is allowed under Article IV of the NPT. The Non-Aligned Movement has welcomed the continuing cooperation of Iran with the IAEA and reaffirmed Iran's right to the peaceful uses of nuclear technology. UN Secretary General Ban Ki-moon has welcomed the continued dialogue between Iran and the IAEA, and has called for a peaceful resolution to the issue.
In April 2010, during the signing of the U.S.-Russia New START Treaty, President Obama said that the United States, Russia, and other nations are demanding that Iran face consequences for failing to fulfill their obligations under the Nuclear Non-Proliferation Treaty, and that "we will not tolerate actions that flout the NPT, risk an arms race in a vital region, and threaten the credibility of the international community and our collective security."
South Africa.
South Africa is the only country that developed nuclear weapons by itself and later dismantled them – unlike the former Soviet states Ukraine, Belarus and Kazakhstan, which inherited nuclear weapons from the former USSR and also acceded to the NPT as non-nuclear weapon states.
During the days of apartheid, the South African government developed a deep fear of both a black uprising and the threat of communism. This led to the development of a secret nuclear weapons program as an ultimate deterrent. South Africa has a large supply of uranium, which is mined in the country's gold mines. The government built a nuclear research facility at Pelindaba near Pretoria where uranium was enriched to fuel grade for the Koeberg Nuclear Power Station as well as weapon grade for bomb production.
In 1991, after international pressure and when a change of government was imminent, South African Ambassador to the United States Harry Schwarz signed the Nuclear Non-Proliferation Treaty. In 1993, the then president Frederik Willem de Klerk openly admitted that the country had developed a limited nuclear weapon capability. These weapons were subsequently dismantled before South Africa acceded to the NPT and opened itself up to IAEA inspection. In 1994 the IAEA completed its work and declared that the country had fully dismantled its nuclear weapons program.
Libya.
Libya had signed and ratified the Nuclear Non-Proliferation Treaty and was subject to IAEA nuclear safeguards inspections, but undertook a secret nuclear weapons development program in violation of its NPT obligations, using material and technology provided by the A.Q. Khan proliferation network—including actual nuclear weapons designs allegedly originating in China. Libya began secret negotiations with the United States and the United Kingdom in March 2003 over potentially eliminating its WMD programs. In October 2003, Libya was embarrassed by the interdiction of a shipment of Pakistani-designed centrifuge parts sent from Malaysia, also as part of A. Q. Khan's proliferation ring.
In December 2003, Libya announced that it had agreed to eliminate all its WMD programs, and permitted U.S. and British teams (as well as IAEA inspectors) into the country to assist this process and verify its completion. The nuclear weapons designs, gas centrifuges for uranium enrichment, and other equipment—including prototypes for improved SCUD ballistic missiles—were removed from Libya by the United States. (Libyan chemical weapons stocks and chemical bombs were also destroyed on site with international verification, with Libya joining the Chemical Weapons Convention.) Libya's non-compliance with its IAEA safeguards was reported to the U.N. Security Council, but with no action taken, as Libya's return to compliance with safeguards and Article II of the NPT was welcomed.
Leaving the treaty.
Article X allows a state to leave the treaty if "extraordinary events, related to the subject matter of this Treaty, have jeopardized the supreme interests of its country", giving three months' (ninety days') notice. The state is required to give reasons for leaving the NPT in this notice.
NATO states argue that when there is a state of "general war" the treaty no longer applies, effectively allowing the states involved to leave the treaty with no notice. This is a necessary argument to support the NATO nuclear weapons sharing policy, but a troubling one for the logic of the treaty. NATO's argument is based on the phrase "the consequent need to make every effort to avert the danger of such a war" in the treaty preamble, inserted at the behest of U.S. diplomats, arguing that the treaty would at that point have failed to fulfill its function of prohibiting a general war and thus no longer be binding. Many states do not accept this argument. See United States-NATO nuclear weapons sharing above.
North Korea has also caused an uproar by its use of this provision of the treaty. Article X.1 only requires a state to give three months' notice in total, and does not provide for other states to question a state's interpretation of "supreme interests of its country". In 1993, North Korea gave notice to withdraw from the NPT. However, after 89 days, North Korea reached agreement with the United States to freeze its nuclear program under the Agreed Framework and "suspended" its withdrawal notice. In October 2002, the United States accused North Korea of violating the Agreed Framework by pursuing a secret uranium enrichment program, and suspended shipments of heavy fuel oil under that agreement. In response, North Korea expelled IAEA inspectors, disabled IAEA equipment, and, on 10 January 2003, announced that it was ending the suspension of its previous NPT withdrawal notification. North Korea said that only one more day's notice was sufficient for withdrawal from the NPT, as it had given 89 days before.
The IAEA Board of Governors rejected this interpretation. Most countries held that a new three-months withdrawal notice was required, and some questioned whether North Korea's notification met the "extraordinary events" and "supreme interests" requirements of the Treaty. The Joint Statement of 19 September 2005 at the end of the Fourth Round of the Six-Party Talks called for North Korea to "return" to the NPT, implicitly acknowledging that it had withdrawn.
Recent and coming events.
The main outcome of the 2000 Conference was the adoption by consensus of a comprehensive Final Document, which included among other things "practical steps for the systematic and progressive efforts" to implement the disarmament provisions of the NPT, commonly referred to as the Thirteen Steps.
On 18 July 2005, US President George W. Bush met Indian Prime Minister Manmohan Singh and declared that he would work to change US law and international rules to permit trade in US civilian nuclear technology with India. Some, such as British columnist George Monbiot, argue that the U.S.-India nuclear deal, in combination with US attempts to deny Iran (an NPT signatory) civilian nuclear fuel-making technology, may destroy the NPT regime, while others contend that such a move will likely bring India, an NPT non-signatory, under closer international scrutiny.
In the first half of 2010, it was strongly believed that China had signed a civilian nuclear deal with Pakistan claiming that the deal was "peaceful".
Arms control advocates criticised the reported China-Pakistan deal as they did in case of U.S.-India deal claiming that both the deals violate the NPT by facilitating nuclear programmes in states which are not parties to the NPT. Some reports asserted that the deal was a strategic move by China to balance US influence in South-Asia.
According to a report published by U.S. Department of Defense in 2001, China had provided Pakistan with nuclear materials and has given critical technological assistance in the construction of Pakistan's nuclear weapons development facilities, in violation of the Nuclear Non-Proliferation Treaty, of which China even then was a signatory.
At the , there were stark differences between the United States, which wanted the conference to focus on non-proliferation, especially on its allegations against Iran, and most other countries, who emphasized the lack of serious nuclear disarmament by the nuclear powers. The non-aligned countries reiterated their position emphasizing the need for nuclear disarmament.
The 2010 Review Conference was held in May 2010 in New York City, and adopted a final document that included a summary by the Review Conference President, Ambassador Libran Capactulan of the Philippines, and an Action Plan that was adopted by consensus. The 2010 conference was generally considered a success because it reached consensus where the previous Review Conference in 2005 ended in disarray, a fact that many attributed to the U.S. President Barack Obama's commitment to nuclear nonproliferation and disarmament. Some have warned that this success raised unrealistically high expectations that could lead to failure at the next Review Conference in 2015.
The "Global Summit on Nuclear Security" took place 12–13 April 2010. The summit was proposed by President Obama in Prague and was intended to strengthen the Nuclear Non-Proliferation Treaty in conjunction with the Proliferation Security Initiative and the Global Initiative to Combat Nuclear Terrorism. Forty seven states and three international organizations took part in the summit, which issued a communiqué and a work plan. For further information see 2010 Nuclear Security Summit.
In a major policy speech at the Brandenburg Gate in Berlin on 19 June 2013, United States President Barack Obama outlined plans to further reduce the number of warheads in the U.S. nuclear arsenal. According to "Foreign Policy", Obama proposed a "one-third reduction in strategic nuclear warheads - on top of the cuts already required by the New START treaty - bringing the number of deployed warheads to about 1,000." Obama is seeking to "negotiate these reductions with Russia to continue to move beyond Cold War nuclear postures," according to briefing documents provided to "Foreign Policy". In the same speech, Obama emphasized his administration's efforts to isolate any nuclear weapons capabilities emanating from Iran and North Korea. He also called for a renewed bipartisan effort in the United States Congress to ratify the Comprehensive Nuclear-Test-Ban Treaty and called on countries to negotiate a new treaty to end the production of fissile material for nuclear weapons.
In 24 April 2014 it was announced that the nation of the Marshall Islands has brought suit in The Hague against the United States, the former Soviet Union, the United Kingdom, France, China, India, Pakistan, North Korea and Israel seeking to have the disarmament provisions of the NNPT enforced.
Criticism and responses.
Over the years the NPT has come to be seen by many Third World states as “a conspiracy of the nuclear 'haves' to keep the nuclear ‘have-nots’ in their place”. This argument has roots in Article VI of the treaty which “obligates the nuclear weapons states to liquidate their nuclear stockpiles and pursue complete disarmament. The non-nuclear states see no signs of this happening”. Some argue that the NWS have not fully complied with their disarmament obligations under Article VI of the NPT. Some countries such as India have criticized the NPT, because it "discriminated against states not possessing nuclear weapons on January 1, 1967," while Iran and numerous Arab states and Iran have criticized Israel for not signing the NPT. There has been disappointment with the limited progress on nuclear disarmament, where the five authorized nuclear weapons states still have 22,000 warheads between them and have shown a reluctance to disarm further.
As noted , the International Court of Justice, in its advisory opinion on the Legality of the Threat or Use of Nuclear Weapons, stated that "there exists an obligation to pursue in good faith and bring to a conclusion negotiations leading to nuclear disarmament in all its aspects under strict and effective international control. Such an obligation requires that states actively pursue measures to reduce the numbers of nuclear weapons and the importance of their role in military force structures. Some critics of the nuclear-weapons states contend that they have failed to comply with Article VI by failing to make disarmament the driving force in national planning and policy with respect to nuclear weapons, even while they ask other states to plan for their security without nuclear weapons.
The United States responds to criticism of its disarmament record by pointing out that since the end of the Cold War it has eliminated over 13,000 nuclear weapons and eliminated over 80% of its deployed strategic warheads and 90% of non-strategic warheads deployed to NATO, in the process eliminating whole categories of warheads and delivery systems and reducing its reliance on nuclear weapons. U.S. officials have also pointed out the ongoing U.S. work to dismantle nuclear warheads. When current accelerated dismantlement efforts ordered by President George W. Bush have been completed, the U.S. arsenal will be less than a quarter of its size at the end of the Cold War, and smaller than it has been at any point since the Eisenhower administration, well before the drafting of the NPT.
The United States has also purchased many thousands of weapons' worth of uranium formerly in Soviet nuclear weapons for conversion into reactor fuel. As a consequence of this latter effort, it has been estimated that the equivalent of one lightbulb in every ten in the United States is powered by nuclear fuel removed from warheads previously targeted at the United States and its allies during the Cold War.
The U.S. Special Representative for Nuclear Nonproliferation agreed that nonproliferation and disarmament are linked, noting that they can be mutually reinforcing but also that growing proliferation risks create an environment that makes disarmament more difficult. The United Kingdom, France and Russia likewise defend their nuclear disarmament records, and the five NPT NWS issued a joint statement in 2008 reaffirming their Article VI disarmament commitments.
According to Thomas Reed and Danny Stillman, the “NPT has one giant loophole”: Article IV gives each non-nuclear weapon state the 'inalienable right' to pursue nuclear energy for the generation of power. A "number of high-ranking officials, even within the United Nations, have argued that they can do little to stop states using nuclear reactors to produce nuclear weapons". A 2009 United Nations report said that:
The revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.
According to critics, those states which possess nuclear weapons, but are not authorized to do so under the NPT, have not paid a significant price for their pursuit of weapons capabilities. Also, the NPT has been explicitly weakened by a number of bilateral deals made by NPT signatories, notably the United States.

</doc>
<doc id="22109" url="http://en.wikipedia.org/wiki?curid=22109" title="Nikolai Bukharin">
Nikolai Bukharin

Nikolai Ivanovich Bukharin (Russian: Никола́й Ива́нович Буха́рин; 9 October [O.S. 27 September] 1888 – 15 March 1938) was a Russian Bolshevik revolutionary, Soviet politician and prolific author on revolutionary theory. 
As a young man, he spent six years in exile, working closely with fellow exiles Lenin and Trotsky. After the revolution of February 1917, he returned to Moscow, where his Bolshevik credentials earned him a high rank in the party, and after the October Revolution, he became editor of the party newspaper "Pravda."
Within the bitterly divided Bolsheviks, his gradual move to the right, as a defender of the New Economic Policy (NEP), positioned him favourably as Stalin’s chief ally, and together they ousted Trotsky, Zinoviev and Kamenev from the party leadership. From 1926 to 1929, Bukharin enjoyed great power as General Secretary of Comintern's executive committee. But Stalin’s decision to proceed with collectivisation drove the two men apart, and Bukharin was expelled from the Politburo.
When the Great Purge began in 1936, Stalin looked for any pretext to liquidate his former allies and rivals for power, and some of Bukharin’s letters, conversations and tapped phone-calls indicated disloyalty. Arrested in February 1937, he was charged with conspiring to overthrow the Soviet state and executed in March 1938, after a trial that alienated many Western communist sympathisers.
Before 1917.
Nikolai Bukharin was born on September 27 (October 9, new style), 1888 in Moscow. He was the second son of two schoolteachers, Ivan Gavrilovich and Liubov Ivanovna Bukharin. His childhood is vividly recounted in his mostly autobiographic novel "How It All Began".
Bukharin's political life began at the age of sixteen with his lifelong friend Ilya Ehrenburg when he participated in student activities at Moscow University related to the Russian Revolution of 1905. He joined the Russian Social Democratic Labour Party in 1906, becoming a member of the Bolshevik faction. With Grigori Sokolnikov, he convened the 1907 national youth conference in Moscow, which was later considered the founding of Komsomol.
By age twenty, he was a member of the Moscow Committee of the party. The committee was heavily infiltrated by the Tsarist secret police, the Okhrana. As one of its leaders, Bukharin quickly became a person of interest to them. During this time, he became closely associated with N. Osinskii and Vladimir Smirnov, and also met his future first wife, Nadezhda Mikhailovna Lukina, his cousin and the sister of Nikolai Lukin, who was also a member of the party. They married soon after their exile, in 1911.
In 1911, after a brief imprisonment, Bukharin was exiled to Onega in Arkhangelsk, but soon escaped to Hanover, where he stayed for a year before visiting Kraków in 1912 to meet Vladimir Lenin for the first time. During the exile, he continued his education and wrote several books that established him as a major Bolshevik theorist in his 20's. His work, "Imperialism and World Economy" influenced Lenin, who freely borrowed from it in his larger and better known work, "Imperialism, the Highest Stage of Capitalism". Nevertheless, he and Lenin often had hot disputes on theoretical issues and Bukharin's closeness with the European Left and his anti-statist tendencies. Bukharin developed an interest in the works of Austrian Marxists and non-Marxist economic theorists, such as Aleksandr Bogdanov, who deviated from Leninist positions. Also while in Vienna in 1913, he helped the Georgian Bolshevik Joseph Stalin write an article, "Marxism and the National Question," at Lenin's request.
In October 1916, while based in New York City, he edited the newspaper "Novy Mir" ("New World") with Leon Trotsky and Alexandra Kollontai. When Trotsky arrived in New York in January 1917, Bukharin was the first to greet him (as Trotsky's wife recalled, "with a bear hug and immediately began to tell them about a public library which stayed open late at night and which he proposed to show us at once" dragging the tired Trotskys across town "to admire his great discovery").
1917 to 1923.
At the news of Russian Revolution of February 1917, exiled revolutionaries from around the world began to flock back to the homeland. Trotsky left New York on March 27, 1917,sailing for St. Petersburg. Bukharin left New York in early April and returned to Russia by way of Japan arriving in Moscow in early May, 1917. Politically, the Bolsheviks in Moscow remained a definite minority to the Mensheviks and Socialist Revolutionaries. However, as the Russian soldiers and workers began to realize that only the Bolsheviks would bring peace by withdrawing from the war and the peasants realized that only the Bolsheviks would give them their own land, membership in the Bolsheviks faction began to sky rocket—from 24,000 members in February 1917 to 200,000 members in October 1917. Upon his return to Moscow, Bukharin resumed his seat on the Moscow City Committee and also became a member of the Moscow Regional Bureau of the Party.
Initially, the Bolshevik position in Moscow was that of a minority position to the stronger Mensheviks and Socialist Revolutionaries. To make matters worse, the Bolsheviks themselves were divided into a right wing and a left wing. The right wing of the Bolsheviks, including Aleksei Rykov and Viktor Nogin, controlled the Moscow Committee, while the younger left wing Bolsheviks including Vladimir Smirnov, Valerian Obolensky, Georgii Lomov, Nikolay Yakovlev, Ivan Kizelshtein and Ivan Stukov, were members of the Moscow Regional Bureau. On October 10, 1917, Bukharin, along with two other Moscow Bolsheviks—A. S. Bubnov and G. Iu. Sokolnikov were elected to the Central Committee. This strong representation on the Central Committee was a direct recognition of the fact that Moscow Bureau had grown in importance. Whereas, the Bolsheviks had been a minority in Moscow behind the Mensheviks and the Socialist Revolutionaries, by September 1917, the Bolsheviks were in the majority in Moscow. Furthermore the Moscow Regional Bureau was formally responsible for the party organizations in each of the thirteen (13) central provinces around Moscow—which accounted for 37% of the whole population of Russia and 20% of the Bolshevik membership.
While no one dominated revolutionary politics in Moscow during the October Revolution, as Trotsky did in St. Petersburg, Bukharin certainly was the most prominent leader in Moscow. During the October Revolution, Bukharin drafted, introduced, and defended the revolutionary decrees of the Moscow Soviet. Bukharin then represented the Moscow Soviet in their report to the revolutionary government in Petrograd. Following the October Revolution, Bukharin became the editor of the party's newspaper, "Pravda".
Bukharin believed passionately in the promise of world revolution. In the Russian turmoil near the end of World War I, when a negotiated peace with the Central Powers was looming, he demanded a continuance of the war, fully expecting to incite all the foreign proletarian classes to arms. Even as he was uncompromising toward Russia's battlefield enemies, he also rejected any fraternization with the capitalist Allied powers: he reportedly wept when he learned of official negotiations for assistance. 
Bukharin emerged as the leader of the Left Communists in bitter opposition to Lenin's decision to sign the Treaty of Brest-Litovsk. In this wartime power struggle, he was urged by some of his more fiery allies to have Lenin arrested. He rejected this idea immediately, but the issue would later become the basis of Stalinist charges against him, culminating in the show trial of 1938.
After the ratification of the treaty, Bukharin resumed his responsibilities within the party. In March 1919, he became a member of the Comintern's executive committee and a candidate member of Politburo. During the Civil War period, he published several theoretical economic works, including the popular primer "The ABC of Communism" (with Yevgeni Preobrazhensky, 1919), and the more academic (1920) and (1921).
By 1921, he changed his position and accepted Lenin's emphasis on the survival and strengthening of the Soviet state as the bastion of the future world revolution. He became the foremost supporter of the New Economic Policy (NEP), to which he was to tie his political fortunes. Considered by the Left Communists as a retreat from socialist policies, NEP reintroduced money, allowed private ownership and capitalistic practices in agriculture, retail trade, and light industry while the state retained the control of heavy industry. While some have criticized Bukharin for this apparent U-turn, his change of emphasis can be partially explained by the necessity for peace and stability following seven years of war in Russia, and the failure of Communist Revolutions in Central and Eastern Europe, which ended the prospect of worldwide revolution.
Power struggle.
After Lenin's death in 1924, Bukharin became a full member of the Politburo. In the subsequent power struggle among Leon Trotsky, Grigory Zinoviev, Lev Kamenev, and Stalin, Bukharin allied himself with Stalin, who positioned himself as centrist of the Party and supported NEP against the Left Opposition, which wanted more rapid industrialization, escalation of class struggle against the kulaks (wealthier peasants), and agitation for world revolution. It was Bukharin who formulated the thesis of "Socialism in One Country" put forth by Stalin in 1924, which argued that socialism (in Marxist theory, the transitional stage from capitalism to communism) could be developed in a single country, even one as underdeveloped as Russia. This new theory stated that revolution need no longer be encouraged in the capitalist countries since Russia could and should achieve socialism alone. The thesis would become a hallmark of Stalinism.
Trotsky, the prime force behind the Left Opposition, was defeated by a triumvirate formed by Stalin, Zinoviev and Kamenev, with the support of Bukharin. By 1926, the Stalin-Bukharin alliance ousted Zinoviev and Kamenev from the Party leadership, and Bukharin enjoyed the highest degree of power during the 1926–1928 period. He emerged as the leader of the Party's right wing, which included two other Politburo members Alexei Rykov, Lenin's successor as Chairman of the Council of People's Commissars and Mikhail Tomsky, head of trade unions, and he became General Secretary of Comintern's executive committee in 1926. However, prompted by grain shortage in 1928, Stalin reversed himself and proposed a program of rapid industrialization and forced collectivization because he believed that the NEP was not working fast enough. Stalin felt that in the new situation the policies of his former foes – Trotsky, Zinoviev, and Kamenev—was the right one.
Bukharin was worried by the prospect of Stalin's plan, which he feared would lead to “military-feudal exploitation” of the peasantry. Bukharin did want the Soviet Union to achieve industrialization but he preferred the more moderate approach of offering the peasants the opportunity to become prosperous, which would lead to greater grain production for sale abroad. Bukharin pressed his views throughout 1928 in meetings of the Politburo and at the Party Congress, insisting that enforced grain requisition would be counter-productive, as War Communism had been a decade earlier.
Fall from power.
Bukharin's support of continuation of NEP was not popular with higher Party cadres, and his slogan to peasants, “Enrich yourselves!” and proposal to achieve socialism “at snail's pace” left him vulnerable to attacks first by Zinoviev and later by Stalin. Stalin attacked Bukharin's views, portraying them as capitalist deviation and declaring that the revolution would be at risk without a strong policy that encouraged rapid industrialization.
Having helped Stalin achieve unchecked power against the Left Opposition, Bukharin found himself easily outmaneuvered by Stalin. Yet Bukharin played to Stalin's strength by maintaining the appearance of unity within the Party leadership. Meanwhile, Stalin used his control of the Party machine to replace Bukharin's supporters in the Rightist power base in Moscow, trade unions, and the Comintern.
Bukharin attempted to gain support from earlier foes including Kamenev and Zinoviev who had fallen from power and held mid-level positions within the Communist party. The details of his meeting with Kamenev, to whom he confided that Stalin was “Genghis Khan” and changed policies to get rid of rivals, were leaked by the Trotskyist press and subjected him to accusations of factionalism. Eventually, Bukharin lost his position in the Comintern and the editorship of "Pravda" in April 1929 and he was expelled from the Politburo on 17 November of that year.
Bukharin was forced to renounce his views under pressure. He wrote letters to Stalin pleading for forgiveness and rehabilitation, but through wiretaps of Bukharin's private conversations with Stalin's enemies, Stalin knew Bukharin's repentance was insincere.
International supporters of Bukharin, Jay Lovestone of the Communist Party USA among them, were also expelled from the Comintern. They formed an international alliance to promote their views, calling it the "International Communist Opposition", though it became better known as the Right Opposition, after a term used by the Trotskyist Left Opposition in the Soviet Union to refer to Bukharin and his supporters there.
Great purge.
Stalin's collectivization policy proved to be as disastrous as Bukharin predicted, but Stalin had by then achieved unchallenged authority in the party leadership. However, there were signs that moderates among Stalin's supporters sought to end official terror and bring a general change in policy, now that mass collectivization was largely completed and the worst was over. Although Bukharin had not challenged Stalin since 1929, his former supporters, including Martemyan Ryutin, drafted and clandestinely circulated an anti-Stalin platform, which called Stalin the “evil genius of the Russian Revolution”.
In the brief period of thaw in 1934–1936, Bukharin was politically rehabilitated and was made editor of "Izvestia" in 1934. There, he consistently highlighted the dangers of fascist regimes in Europe and the need for "proletarian humanism".
However, Kirov was assassinated in Leningrad in December 1934, and his death was used by Stalin as a pretext to launch the Great Purge, in which about a million people were to perish as Stalin eliminated all past and potential opposition to his authority. Some historians now believe that Kirov's assassination in 1934 was arranged by Stalin himself or at least that there is sufficient evidence to plausibly posit such a conclusion. After Kirov's assassination, the NKVD charged an ever-growing group of former oppositionists with Kirov's murder and other acts of treason, terrorism, sabotage, and espionage.
Tightening noose.
In February 1936, shortly before the purge started in earnest, Bukharin was sent to Paris by Stalin to negotiate the purchase of the Marx and Engels archives, held by the German Social Democratic Party (SPD) before its dissolution by Hitler. He was joined by his young wife Anna Larina, which therefore opened the possibility of exile, but he decided against it, saying that he could not live outside the Soviet Union.
Bukharin, who had been forced to follow the Party line since 1929, confided to his old friends and former opponents his real view of Stalin and his policy. His conversations with Boris Nicolaevsky, a Menshevik leader who held the manuscripts on behalf of the SPD, formed the basis of "Letter of an Old Bolshevik", which was very influential in contemporary understanding of the period (especially the Ryutin Affair and the Kirov murder) although there are doubts about its authenticity.
According to Nicolaevsky, Bukharin spoke of "the mass annihilation of completely defenseless men, with women and children" under forced collectivization and liquidation of kulaks as a class that dehumanized the Party members with "the profound psychological change in those communists who took part in the campaign. Instead of going mad, they accepted terror as a normal administrative method and regarded obedience to all orders from above as a supreme virtue... They are no longer human beings. They have truly become the cogs in a terrible machine."
Yet to another Menshevik leader, Fyodor Dan, he confided that Stalin became "the man to whom the Party granted its confidence" and "is a sort of a symbol of the Party" even though he "is not a man, but a devil." In Dan's account, Bukharin’s acceptance of the Soviet Union’s new direction was thus a result of his utter commitment to Party solidarity.
To André Malraux, he also confided, "Now he is going to kill me". To his boyhood friend, Ilya Ehrenburg, he expressed the suspicion that the whole trip was a trap set up by Stalin. Indeed, his contacts with Mensheviks during this trip were to feature prominently in his trial.
The trial.
Following the trial and execution of Zinoviev, Kamenev, and other leftist Old Bolsheviks in 1936, Bukharin and Rykov were arrested on 27 February 1937 following a plenum of the Central Committee and were charged with conspiring to overthrow the Soviet state.
Bukharin was tried in the Trial of the Twenty One on 2–13 March 1938 during the Great Purges, along with ex-premier Alexei Rykov, Christian Rakovsky, Nikolai Krestinsky, Genrikh Yagoda, and 16 other defendants alleged to belong to the so-called "Bloc of Rightists and Trotskyites". Meant to be the culmination of previous show trials, it was now alleged that Bukharin and others sought to assassinate Lenin and Stalin from 1918, murder Maxim Gorky by poison, partition the Soviet Union and hand out her territories to Germany, Japan, and Great Britain.
Even more than earlier Moscow show trials, Bukharin's trial horrified many previously sympathetic observers as they watched allegations become more absurd than ever and the purge expand to include almost every living Old Bolshevik leader except Stalin. For some prominent communists such as Bertram Wolfe, Jay Lovestone, Arthur Koestler, and Heinrich Brandler, the Bukharin trial marked their final break with communism and even turned the first three into fervent anti-Communists eventually.
While Anastas Mikoyan and Vyacheslav Molotov later claimed that Bukharin was never tortured and his letters from prison do not give the suggestion that he was tortured, it is also known that his interrogators were instructed with the order: "beating permitted". Bukharin held out for three months, but threats to his young wife and infant son, combined with "methods of physical influence" wore him down. But when he read his confession amended and corrected personally by Stalin, he withdrew his whole confession. The examination started all over again, with a double team of interrogators.
Bukharin's confession and his motivation became subject of much debate among Western observers, inspiring Koestler's acclaimed novel "Darkness at Noon" and a philosophical essay by Maurice Merleau-Ponty in "Humanism and Terror." His confessions were somewhat different from others in that while he pled guilty to the "sum total of crimes," he denied knowledge when it came to specific crimes. Some astute observers noted that he would allow only what was in the written confession and refuse to go any further.
There are several interpretations of Bukharin's motivations (beside being coerced) in the trial. Koestler and others viewed it as a true believer's last service to the Party (while preserving the little amount of personal honor left) whereas Bukharin biographer Stephen Cohen and Robert Tucker saw traces of Aesopian language, with which Bukharin sought to turn the table into an anti-trial of Stalinism (while keeping his part of the bargain to save his family). While his letters to Stalin – he wrote 34 very emotional and desperate letters tearfully protesting his innocence and professing his loyalty – suggest a complete capitulation and acceptance of his role in the trial, it contrasts with his actual conduct in the trial.
Bukharin himself speaks of his "peculiar duality of mind" in his last plea, which led to "semi-paralysis of the will" and Hegelian "unhappy consciousness", which likely stemmed not only from his knowledge of the ruinous reality of Stalinism (although he could not of course say so in the trial) but also of the impending threat of fascism.
The result was a curious mix of fulsome confessions (of being a "degenerate fascist" working for the "restoration of capitalism") and subtle criticisms of the trial. After disproving several charges against him (one observer noted that he "proceeded to demolish or rather showed he could very easily demolish the whole case.") and saying that "the confession of accused is not essential. The confession of the accused is a medieval principle of jurisprudence" in a trial that was solely based on confessions, he finished his last plea with the words: 
"the monstrousness of my crime is immeasurable especially in the new stage of struggle of the U.S.S.R. May this trial be the last severe lesson, and may the great might of the U.S.S.R become clear to all."
While in prison, he wrote at least four book-length manuscripts including a lyrical autobiographical novel, "How It All Began", philosophical treatise "Philosophical Arabesques", a collection of poems, and "Socialism and Its Culture" – all of which were found in Stalin's archive and published in the 1990s.
Execution.
Among other intercessors, the French author and Nobel laureate Romain Rolland wrote to Stalin seeking clemency, arguing that "an intellect like that of Bukharin is a treasure for his country." He compared Bukharin's situation to that of the great chemist Antoine Lavoisier who was guillotined during the French Revolution: "We in France, the most ardent revolutionaries... still profoundly grieve and regret what we did... I beg you to show clemency." He had earlier written to Stalin in 1937, "For the sake of Gorky I am asking you for mercy, even if he may be guilty of something," to which Stalin noted: "We must not respond." Bukharin was executed on 15 March 1938, but the announcement of his death was overshadowed by the Nazi Anschluss of Austria.
Bukharin's last message to Stalin: 'Koba, why do you need me to die?' (Russian: "Коба, зачем тебе нужна моя смерть?") was written in a note to Stalin just before his execution. ("Koba" was Stalin's revolutionary pseudonym, and Bukharin's use of it was a sign of how close the two had once been. The note was found still in Stalin's desk after his death in 1953).
Despite the promise to spare his family, Bukharin's wife, Anna Larina, was sent to a labor camp, but she survived to see her husband officially rehabilitated by the Soviet state under Mikhail Gorbachev in 1988.
Political stature and achievements.
Bukharin was immensely popular within the party throughout the twenties and thirties, even after his fall from power. In his testament, Lenin portrayed him as the "Golden Boy" of the party, writing:
Speaking of the young C.C. members, I wish to say a few words about Bukharin and Pyatakov. They are, in my opinion, the most outstanding figures (among the youngest ones), and the following must be borne in mind about them: Bukharin is not only a most valuable and major theorist of the Party; he is also rightly considered the favourite of the whole Party, but his theoretical views can be classified as fully Marxist only with great reserve, for there is something scholastic about him (he has never made a study of the dialectics, and, I think, never fully understood it)... Both of these remarks, of course, are made only for the present, on the assumption that both these outstanding and devoted Party workers fail to find an occasion to enhance their knowledge and amend their one-sidedness.
Bukharin made several notable contributions to Marxist–Leninist thought, most notably "The Economics of the Transition Period" (1920) and his prison writings, "Philosophical Arabesques", (which clearly reveal Bukharin had corrected the 'one-sidedness' of his thought), as well as being a founding member of the Soviet Academy of Arts and Sciences, and a keen botanist. His primary contributions to economics were his critique of marginal utility theory, his analysis of imperialism, and his writings on the transition to communism in the Soviet Union.
His ideas, especially in economy and question of market-socialism, later became basic idea of Chinese market-socialism and Deng Xiao Ping reforms.
Works.
Cartoons.
Nikolai Bukharin was a cartoonist who left many cartoons of contemporary Soviet politicians. The renowned artist Konstantin Yuon once told him: “Forget about politics. There is no future in politics for you. Painting is your real calling."
His cartoons are sometimes used to illustrate biographies of Soviet officials. Russian historian Yury Zhukov stated that Nikolai Bukarin's portraits of Joseph Stalin were the only ones drawn from the original, not from a photograph.

</doc>
<doc id="22110" url="http://en.wikipedia.org/wiki?curid=22110" title="Nasal consonant">
Nasal consonant

In phonetics, a nasal, also called a nasal occlusive, nasal stop in contrast with a nasal fricative, or nasal continuant, is an occlusive consonant produced with a lowered velum, allowing air to escape freely through the nose. Examples of nasals in English are [n] and [m], in words such as "nose" and "mouth". Nasal occlusives are nearly universal in human languages. There are also other kinds of nasal consonants in some languages.
Definition.
Nearly all nasal consonants are nasal occlusives, where air escapes through the nose but not through the mouth, as it is blocked (occluded) by the lips or tongue. The oral cavity still acts as a resonance chamber for the sound. Rarely, non-occlusive consonants may be nasalized.
Most nasals are voiced, and in fact the nasal sounds [n] and [m] are among the most common sounds cross-linguistically. Voiceless nasals do occur in a few languages, such as Burmese, Welsh, Icelandic and Guaraní. (Compare oral stops, which block off the air completely, and fricatives, which obstruct the air with a narrow channel. Both stops and fricatives are more commonly voiceless than voiced, and are known as obstruents.)
In terms of acoustics, nasals are sonorants, meaning that they do not significantly restrict the escape of air (as it can freely escape out the nose). However, nasals are also obstruents in their articulation because the flow of air through the mouth is blocked. This duality, a sonorant airflow through the nose along with an obstruction in the mouth, means that nasal occlusives behave both like sonorants and like obstruents. For example, nasals tend to pattern with other sonorants such as [r] and [l], but in many languages they may develop from or into stops.
Acoustically, nasals have bands of energy at around 200 and 2,000 Hz.
1. ^ The symbol ⟨n⟩ is commonly used to represent the dental nasal as well, rather than ⟨n̪⟩, as it is rarely distinguished from the alveolar nasal.
Examples of languages containing nasal occlusives:
The voiced retroflex nasal is [ɳ] is a common sound in Languages of India.
The voiced palatal nasal [ɲ] is a common sound in European languages, such as: Spanish ⟨ñ⟩, French and Italian ⟨gn⟩, Catalan and Hungarian ⟨ny⟩, Czech and Slovak ⟨ň⟩, Polish ⟨ń⟩, Occitan and Portuguese ⟨nh⟩, Serbo-Croatian ⟨nj⟩, and (before a vowel) Modern Greek ⟨νι⟩.
Many Germanic languages, including German, Dutch, English and Swedish, as well as Sinitic languages such as Mandarin and Cantonese, have [m], [n] and [ŋ]. Tamil has a six-fold distinction between [m], [n̪], [n], [ɳ], [ɲ] and [ŋ] (ம,ந,ன,ண,ஞ,ங).
Catalan, Occitan, Spanish, and Italian have [m], [n], [ɲ] as phonemes, and [ɱ] and [ŋ] as allophones. Nevertheless, in several American dialects of Spanish, there is no palatal nasal but only a palatalized nasal, [nʲ], as in English "canyon".
In Brazilian Portuguese and Angolan Portuguese [ɲ], written ⟨nh⟩, is typically pronounced as [ȷ̃], that is, as a nasal palatal approximant, a nasal glide (in Polish this feature is also possible as an allophone). Semivowels in Portuguese often nasalize before and always after nasal vowels, resulting in [ȷ̃] and [w͂]. What would be coda nasal occlusives in other West Iberian languages is only slightly pronounced before dental consonants, outside this environment the nasality is spread over the vowel or become a nasal diphthong (e.g. "mambembe" [mɐ̃ˈbẽjbi], outside the final, only in Brazil, and "mantém" [mɐ̃ˈtẽj ~ mɐ̃ˈtɐ̃j] in all Portuguese dialects).
The term 'nasal occlusive' (or 'nasal stop') is generally abbreviated to "nasal". However, there are also nasalized fricatives, nasalized flaps, nasal glides, and nasal vowels, as in French, Portuguese, and Polish. In the , nasal vowels and nasalized consonants are indicated by placing a tilde (~) over the vowel or consonant in question: French "sang" [sɑ̃], Portuguese "bom" [bõ].
Voiceless nasals.
A few languages have phonemic voiceless nasal occlusives. Among them are Icelandic, Burmese, Jalapa Mazatec, Kildin Sami, Welsh, and Central Alaskan Yup'ik. Iaai of New Caledonia has an unusually large number of them, with /m̥ m̥ʷ n̪̥ ɳ̊  ɲ̊ ŋ̊/.
Other kinds of nasal consonant.
Ladefoged and Maddieson (1996) distinguish purely nasal consonants, the nasal occlusives such as "m n ng" in which the airflow is purely nasal, from partial nasal consonants such as prenasalized stops and prestopped nasals, which are nasal for only part of their duration, as well as from nasalized consonants, which have simultaneous oral and nasal airflow. In some languages, such as Portuguese, a nasal consonant may have occlusive and non-occlusive allophones. In general, therefore, a nasal consonant may be:
Languages without nasals.
Few languages, perhaps 2%, contain no phonemically distinctive nasals. This led Ferguson (1963) to assume that all languages have at least one primary nasal occlusive. When a language is claimed to lack nasals altogether, as with several Niger–Congo languages or the Pirahã language of the Amazon, nasal and non-nasal or prenasalized consonants usually alternate allophonically, and it is a theoretical claim on the part of the individual linguist that the nasal is not the basic form of the consonant. In the case of some Niger–Congo languages, for example, nasals occur before only nasal vowels. Since nasal vowels are phonemic, it simplifies the picture somewhat to assume that nasalization in occlusives is allophonic. There is then a second step in claiming that nasal vowels nasalize oral occlusives, rather than oral vowels denasalizing nasal occlusives, that is, whether [mã, mba] are phonemically /mbã, mba/ without full nasals, or /mã, ma/ without prenasalized stops. Postulating underlying oral or prenasalized stops rather than true nasals helps to explain the apparent instability of nasal correspondences throughout Niger–Congo compared with, for example, Indo-European.
However, this comes at the expense, in some languages, of postulating either a single nasal consonant that can only be syllabic, or a larger set of nasal vowels than oral vowels, both typologically odd situations. The way such a situation could develop is illustrated by a Jukunoid language, Wukari. Wukari allows oral vowels in syllables like "ba, mba" and nasal vowels in "bã, mã", suggesting that nasals become prenasalized stops before oral vowels. Historically, however, *mb became **mm before nasal vowels, and then reduced to *m, leaving the current asymmetric distribution.
In older speakers of the Tlingit language, [l] and [n] are allophones. Tlingit is usually described as having an unusual, perhaps unique lack of /l/ despite having six lateral obstruents; the older generation could be argued to have /l/ but at the expense of having no nasals.
However, several of the Chimakuan, Salish, and Wakashan languages surrounding Puget Sound, such as Quileute, Lushootseed, and Makah, are truly without any nasalization at all, in consonants or vowels, except in special speech registers such as baby talk or the archaic speech of mythological figures (and perhaps not even that in the case of Quileute). This is an areal feature, only a few hundred years old, where nasals became voiced stops ([m] became [b], etc.). The only other places in the world where this occurs is in the central dialect of the Rotokas language of Papua New Guinea, where nasals are used only when imitating foreign accents (a second dialect has nasals).
The unconditioned loss of nasals is considered unusual. However, currently in Korean, word-initial /m/ and /n/ are shifting to [b] and [d]. This started out in nonstandard dialects and was restricted to the beginning of prosodic units (a common position for fortition), but has expanded to many speakers of the standard language to the beginnings of common words even within prosodic units.

</doc>
<doc id="22111" url="http://en.wikipedia.org/wiki?curid=22111" title="Nuvistor">
Nuvistor

The nuvistor is a type of vacuum tube announced by RCA in 1959. Most nuvistors are basically thimble-shaped, but somewhat smaller than a thimble, and much smaller than conventional tubes of the day. Triodes and a few tetrodes were made. The tube is made entirely of metal and ceramic. Making nuvistors requires special equipment, since there is no intubation to pump gases out of the envelope. Instead, the entire structure is assembled, inserted into its metal envelope, sealed and processed in a large vacuum chamber with simple robotic devices.
Nuvistors are among the highest performing small signal receiving tubes. They feature excellent VHF and UHF performance plus low noise figures, and were widely used throughout the 1960s in television sets (beginning with RCA's "New Vista" line of color sets in 1961 with the CTC-11 chassis) and radio equipment and high-fidelity equipment, primarily in RF sections. They competed with the solid state revolution, and along with GE's Compactron, probably held it at bay for a few years. RCA discontinued their use in television tuners for its product line in late 1971. One famous application was in the Ampex MR-70, a costly studio tape recorder whose entire electronics section was based on nuvistors. Another limited application of this very small tube was in studio-grade microphones from that era, the AKG/Norelco C12a, which employed the 7586, being a good example. It was also later found that, with minor circuit modification, the nuvistor made a sufficient replacement for the obsolete Telefunken VF14 tube, used in the famed Neumann U 47 studio microphone.

</doc>
<doc id="22113" url="http://en.wikipedia.org/wiki?curid=22113" title="No Logo">
No Logo

No Logo: Taking Aim at the Brand Bullies is a book by the Canadian author Naomi Klein. First published by Knopf Canada and Picador in December 1999, shortly after the 1999 WTO Ministerial Conference protests in Seattle had generated media attention around such issues, it became one of the most influential books about the alter-globalization movement and an international bestseller.
Focus.
The book focuses on branding and often makes connections with the alter-globalization movement. Throughout the four parts ("No Space", "No Choice", "No Jobs", and "No Logo"), Klein writes about issues such as sweatshops in the Americas and Asia, culture jamming, corporate censorship, and Reclaim the Streets. She pays special attention to the deeds and misdeeds of Nike, The Gap, McDonald's, Shell, and Microsoft – and of their lawyers, contractors, and advertising agencies. Many of the ideas in Klein's book derive from the influence of the Situationists, an art/political group founded in the late 1950s.
However, while globalization appears frequently as a recurring theme, Klein rarely addresses the topic of globalization itself, and when she does, it is usually indirectly. She goes on to discuss globalization in much greater detail in her book, "Fences and Windows" (2002).
Summary.
The book comprises four sections: "No Space", "No Choice", "No Jobs", and "No Logo". The first three deal with the negative effects of brand-oriented corporate activity, while the fourth discusses various methods people have taken in order to fight back.
"No Space".
The book begins by tracing the history of brands. Klein argues that there has been a shift in the usage of branding and gives examples of this shift to "anti-brand" branding. Early examples of brands were often used to put a recognizable face on factory-produced products. These slowly gave way to the idea of selling lifestyles. According to Klein, in response to an economic crash in the 1980s (due to the Latin American debt crisis, Black Monday (1987), the savings and loan crisis, and the Japanese asset price bubble), corporations began to seriously rethink their approach to marketing and to target the youth demographic, as opposed to the baby boomers, who had previously been considered a much more valuable segment.
The book discusses how brand names such as Nike or Pepsi expanded beyond the mere products which bore their names, and how these names and logos began to appear everywhere. As this happened, the brands' obsession with the youth market drove them to further associate themselves with whatever the youth considered "cool". Along the way, the brands attempted to associate their names with everything from movie stars and athletes to grassroots social movements.
Klein argues that large multinational corporations consider the marketing of a brand name to be more important than the actual manufacture of products; this theme recurs in the book, and Klein suggests that it helps explain the shift to production in Third World countries in such industries as clothing, footwear, and computer hardware.
This section also looks at ways in which brands have "muscled" their presence into the school system, and how in doing so, they have pipelined advertisements into the schools and used their position to gather information about the students. Klein argues that this is part of a trend toward targeting younger and younger consumers.
"No Choice".
In the second section, Klein discusses how brands use their size and clout to limit the number of choices available to the public – whether through market dominance (e.g., Wal-Mart) or through aggressive invasion of a region (e.g., Starbucks). Klein argues that each company's goal is to become the dominant force in its respective field. Meanwhile, other corporations, such as Sony or Disney, simply open their own chains of stores, preventing the competition from even putting their products on the shelves.
This section also discusses the way that corporations merge with one another in order to add to their ubiquity and provide greater control over their image. ABC News, for instance, is allegedly under pressure not to air any stories that are overly critical of Disney, its parent company. Other chains, such as Wal-Mart, often threaten to pull various products off their shelves, forcing manufacturers and publishers to comply with their demands. This might mean driving down manufacturing costs or changing the artwork or content of products like magazines or albums so they better fit with Wal-Mart's image of family friendliness.
Also discussed is the way that corporations abuse copyright laws in order to silence anyone who might attempt to criticize their brand.
"No Jobs".
In this section, the book takes a darker tone and looks at the way in which manufacturing jobs move from local factories to foreign countries, and particularly to places known as export processing zones. Such zones often have no labor laws, leading to dire working conditions.
The book then shifts back to North America, where the lack of manufacturing jobs has led to an influx of work in the service sector, where most of the jobs are for minimum wage and offer no benefits. The term "McJob" is introduced, defined as a job with poor compensation that does not keep pace with inflation, inflexible or undesirable hours, little chance of advancement, and high levels of stress. Meanwhile, the public is being sold the perception that these jobs are temporary employment for students and recent graduates, and therefore need not offer living wages or benefits.
All of this is set against a backdrop of massive profits and wealth being produced within the corporate sector. The result is a new generation of employees who have come to resent the success of the companies they work for. This resentment, along with rising unemployment, labour abuses abroad, disregard for the environment, and the ever-increasing presence of advertising breeds a new disdain for corporations.
"No Logo".
The final section of the book discusses various movements that have sprung up during the 1990s. These include "Adbusters" magazine and the culture-jamming movement, as well as Reclaim the Streets and the McLibel trial. Less radical protests are also discussed, such as the various movements aimed at putting an end to sweatshop labour.
Klein concludes by contrasting consumerism and citizenship, opting for the latter. "When I started this book," she writes, "I honestly didn't know whether I was covering marginal atomized scenes of resistance or the birth of a potentially broad-based movement. But as time went on, what I clearly saw was a movement forming before my eyes."
Criticism.
After the book's release, Klein was heavily criticized by the news magazine "The Economist", leading to a broadcast debate with Klein and the magazine's writers, dubbed "No Logo vs. Pro Logo".
The 2004 book "The Rebel Sell" (published as "Nation of Rebels" in the United States) specifically criticised "No Logo", stating that turning the improving quality of life in the working class into a fundamentally anti-market ideology is shallow. 
Awards.
In 2000, "No Logo" was short-listed for the "Guardian" First Book Award in 2000.
In 2001, the book won the following awards:
Editions.
Several imprints of "No Logo" exist, including a hardcover first edition a subsequent hardcover edition, and a paperback. A 10th anniversary edition was published by Fourth Estate that includes a new introduction by the author. Translations from the original English into several other languages have also been published. The subtitle, "Taking Aim at the Brand Bullies", was dropped in some later editions.
Video.
Naomi Klein explains her ideas in the 40-minute video "No Logo – Brands, Globalization & Resistance" (2003), directed by Sut Jhally.

</doc>
<doc id="22115" url="http://en.wikipedia.org/wiki?curid=22115" title="National War College">
National War College

The National War College (NWC) of the United States is a school in the National Defense University. It is housed in Roosevelt Hall on Fort Lesley J. McNair, Washington, D.C., the third-oldest Army post still active.
History.
The National War College ("NWC") was officially established on July 1, 1946, as an upgraded replacement for the Army-Navy Staff College, which operated from June 1943 to July 1946. The college was one of James Forrestal's favorite causes.
According to Lt. Gen. Leonard T. Gerow, President of the Board which recommended its formation, "The College is concerned with grand strategy and the utilization of the national resources necessary to implement that strategy... Its graduates will exercise a great influence on the formulation of national and foreign policy in both peace and war..."
Mid-level and senior military officers who are likely to be promoted to the most senior ranks are selected to study at the War College in preparation for higher staff and command positions. About 75 percent of the student body is composed of equal representation from the land, air, and sea (including Marine and Coast Guard) Services. The remaining 25 percent are drawn from the Department of State and other federal departments and agencies. In addition, international fellows from a number of countries join the student body. The curriculum is based upon critical analysis of strategic problem solving with emphasis on strategic leadership. Starting with the 2014-2015 academic year, the curriculum will be based upon a core standard throughout National Defense University.
Because of NWC's privileged location close to the White House, the Supreme Court, and Capitol Hill, it's been able throughout its history to call upon an extraordinarily well-connected array of speakers to animate its discussions. All lectures at the National War College are conducted under a strict "no quotation nor attribution" policy which has facilitated discussion on some of the most difficult issues of the day.
Alumni and influence.
Graduates of the National War College include numerous current and former flag officers, general officers, and U.S. ambassadors. Notable graduates include former U.S. Secretary of State and Chairman of the Joint Chiefs of Staff Colin Powell; U.S. Senator John McCain; former NATO Supreme Allied Commander Europe Wesley Clark; former Chairmen of the Joint Chiefs of Staff Peter Pace and Hugh Shelton; former National Security Advisor and NATO Supreme Allied Commander Europe James L. Jones; former U.S Army Chief of Staff Eric Shinseki; former U.S. Chief of Naval Operations Elmo Zumwalt; Commandant of the Marine Corps Robert H. Barrow; retired Air Force General Arnold W. Braswell; U.S. Ambassador to Russia John Beyrle; World War II submarine officer and best-selling novelist Edward L. Beach, Jr.; former military aide to President John F. Kennedy, Godfrey McHugh; the late U.S. Ambassador to Libya J. Christopher Stevens; and former U.S. Air Force Chief of Staff Norton A. Schwartz; one of the initial nuclear theorists Bernard Brodie was a founding faculty member.
Roosevelt Hall.
Roosevelt Hall (built 1903-07) is a Beaux Arts-style building housing the NWC since its inception in 1946. Designed by the New York architectural firm McKim, Mead and White, it is now designated a National Historical Landmark and is listed on the National Register of Historic Places.

</doc>
<doc id="22117" url="http://en.wikipedia.org/wiki?curid=22117" title="Neelin, Manitoba">
Neelin, Manitoba

Neelin is a small community in the Canadian province of Manitoba. It is located on Manitoba Provincial Highway 5 in the Rural Municipality of Argyle, about 29 km east of Killarney, or about 200 km southwest of Winnipeg.

</doc>
<doc id="22118" url="http://en.wikipedia.org/wiki?curid=22118" title="Norn language">
Norn language

Norn, an extinct North Germanic language, was spoken in the Northern Isles (Orkney and Shetland) off the north coast of mainland Scotland and in Caithness in the far north of the Scottish mainland. After Orkney and Shetland were pledged to Scotland by Norway in 1468/69, it was gradually replaced by Scots.
History.
Norse settlement in the islands probably began in the early 9th century. These settlers are believed to have arrived in very substantial numbers and like those who migrated to Iceland and the Faroe Islands it is probable that most came from the west coast of Norway. Shetland toponymy bears some resemblance to that of northwest Norway, while Norn vocabulary implies links with more southerly Norwegian regions.
Orkney and Shetland were pledged to James III in 1468 and 1469 respectively, and it is with these pledges that the replacement of Norn with Scots is most associated. However, the decline of Norse speech in Orkney probably began in 1379 when the earldom passed into the hands of the Sinclairs, and Scots had superseded Norse as the language of prestige on the island by the early 15th century. In Shetland the transition began later, but by the end of the 15th century both islands were bilingual. Despite this, the process by which Scots overtook Norn as the primary spoken language on the islands was not a swift one, and most natives of Orkney and Shetland likely spoke Norn as a first language until the late 16th and early-to-mid 17th centuries respectively.
Death.
It is not known exactly when Norn became extinct. Sources from the 17th and 18th centuries speak of Norn (sometimes identified as "Norse", "Norwegian" or "Danish") as being in a state of decline and generally indicate that the language remained stronger in Shetland than in Orkney. A source from 1670 states that there are "only three or four parishes" in Orkney where people speak "Noords or rude Danish" and that they do so "chiefly when they are at their own houses". Another from 1701 indicates that there were still a few monoglot "Norse" speakers who were capable of speaking "no other thing", and notes that there were more speakers of the language in Shetland than in Orkney. It was said in 1703 that the people of Shetland generally spoke English, but that "many among them retain the ancient Danish Language"; while in 1750 Orkney-born James Mackenzie wrote that Norn was not yet entirely extinct, being "retained by old people", who still spoke it among each other.
The last reports of Norn speakers are claimed to be from the 19th century, but it is more likely that the language was dying out in the late 18th century. The isolated islands of Foula and Unst are variously claimed as the last refuges of the language in Shetland, where there were people "who could repeat sentences in Norn, probably passages from folk songs or poems, as late as 1893. Walter Sutherland from Skaw in Unst, who died about 1850, has been cited as the last native speaker of the Norn language. However, fragments of vocabulary survived the death of the main language and remain to this day, mainly in place-names and terms referring to plants, animals, weather, mood, and fishing vocabulary.
Norn had also been a spoken language in Caithness but had probably become extinct there by the 15th century, replaced by Scots. Hence, some scholars also speak about "Caithness Norn", but others avoid this. Even less is known about "Caithness Norn" than about Orkney and Shetland Norn. Almost no written Norn has survived, but what little remains includes a version of the Lord's Prayer and a ballad. Michael P Barnes, professor of Scandinavian Studies at University College London, has published a study, "The Norn Language of Orkney and Shetland".
Classification.
Norn is an Indo-European language belonging to the North Germanic branch of the Germanic languages. Together with Faroese, Icelandic and Norwegian, it belongs to the West Scandinavian group, separating it from the East Scandinavian group consisting of Swedish, Danish and Gutnish. While this classification is based on the differences between the North Germanic languages at the time they split, their present-day characteristics justify another classification, dividing them into "Insular Scandinavian" and "Mainland Scandinavian" language groups, based on mutual intelligibility. Under this system, Norwegian is grouped together with Danish and Swedish, because the last millennium has seen all three undergo important changes, especially in grammar and lexis, which have set them apart from Faroese and Icelandic. Norn is generally considered to have been fairly similar to Faroese, sharing many phonological and grammatical traits, and might even have been mutually intelligible with it; thus, it can be considered an Insular Scandinavian language.
Few written texts remain. It is distinct from the present day dialect of Shetland, termed by linguists Shetlandic.
Sounds.
The phonology of Norn can never be determined with much precision due to the lack of source material, but the general aspects can be extrapolated from the few written sources that do exist. Norn shared many traits with the dialects of south-west Norway. This includes a voicing of /p, t, k/ to [b, d, ɡ] before or between vowels and (in the Shetland dialect, but only partially in the Orkney dialect) a conversion of /θ/ and /ð/ ("thing" and "that" respectively) to [t] and [d] respectively.
Grammar.
The features of Norn grammar were very similar to the other Scandinavian languages. There were two numbers, three genders and four cases (nominative, accusative, genitive and dative). The two main conjugations of verbs in present and past tense were also present and like all other North Germanic languages, it used a suffix instead of a prepositioned article to indicate definiteness as in modern Scandinavian: "man(n)" ("man"); "mannen" ("the man"). Though it is difficult to be certain of many of the aspects of Norn grammar, documents indicate that it may have featured subjectless clauses, which were common in the West Scandinavian languages.
Sample text.
The following are Norn and Old Norse versions of the Lord's Prayer:
A Shetland "guddick" (riddle) in Norn, which Jakob Jakobsen heard told on Unst, the northernmost island in Shetland, in the 1890s. The same riddle is also known from the Faroe Islands, from Iceland, and a variation also occurs in England.
The answer is a cow: four teats hang, four legs walk, two horns and two ears stand skyward, two eyes show the way to the field and one tail comes shaking (dangling) behind.
Modern use.
Most of the use of Norn/Norse in modern day Shetland and Orkney is purely ceremonial, and mostly in Old Norse, for example the Shetland motto, which is "Með lögum skal land byggja" ("with law shall land be built") which is the same motto used by the Icelandic police force and inspired by the Danish Codex Holmiensis.
Another example of the use of Norse/Norn in the Northern Isles can be found in the names of ferries:
Norn words are still used to describe many of the colour and pattern variations in the native sheep of Shetland and Orkney, which survive as the Shetland and North Ronaldsay breeds. Icelandic uses similar words for many of the same colour variations in Icelandic sheep.
There are some enthusiasts who are engaged in developing and disseminating a modern form called Nynorn ("New Norn"), based upon linguistic analysis of the known records and Norse linguistics in general.

</doc>
<doc id="22120" url="http://en.wikipedia.org/wiki?curid=22120" title="Nuoro">
Nuoro

Nuoro (], Sardinian: "Nùgoro" ]) is a city and "comune" (municipality) in central-eastern Sardinia, Italy, situated on the slopes of the Monte Ortobene. It is the capital of the province of Nuoro. With a population of 36,347 (2011), it is the sixth-largest city in Sardinia.
Birthplace of several renowned artists, including writers, poets, painters, and sculptors, Nuoro hosts some of the most important museums in Sardinia. It is considered an important cultural center of the region and it has been referred as the "Atene sarda" (Sardinian Athens). Nuoro is the hometown of Grazia Deledda, the first and only Italian woman to win (1926) the Nobel Prize in Literature.
History.
The earliest traces of human settlement in the Nuoro area (called " the Nuorese") are the so-called Domus de janas, rock-cut tombs dated at the third millennium BC. However, fragments of ceramics of the Ozieri culture have also been discovered and dated at c. 3500 BC.
The Nuorese was a centre of the Nuragic civilization (which developed in Sardinia from c. 1500 BC to c. 250 BC), as attested by more than 30 Nuragic sites, such has the village discovered in the countryside of Tanca Manna, just outside Nuoro, which was made of about 800 huts.
The Nuorese was crossed by a Roman road which connected Karalis (Cagliari) to Ulbia (Olbia). The legacy of the Roman colonization can especially be found in the variety of the Sardinian language which is still spoken today in Nuoro: "Sardu nugoresu" is considered the most conservative language of the Romance family.
After the fall of the Western Roman Empire, Sardinia was held first by the Vandals and then by the Byzantines. According to the letters of Pope Gregory I, a Romanized and Christianized culture (that of the "provinciales") co-existed with several Pagan cultures (those of the "Gens Barbaricina", i.e. "Barbarian People") mainly located in the island's interior. As the Byzantine control waned, the Giudicati appeared. A small village known as Nugor appears on a medieval map from 1147. In the two following centuries it grew to more than 1000 inhabitants. Nuoro remained a town of average importance under the Aragonese and Spanish domination of Sardinia, until famine and plague struck it in the late 17th century.
After the annexation to the Kingdom of Sardinia, the town became the administrative center of the area, obtaining the title of city in 1836.
Culture.
Language.
Sardo Logudorese is also spoken in Nuoro, in its variant called Nugoresu.
Transportation.
Nuoro is served by the SS 131 DCN (Olbia-Abbasanta), the SS 129 (Orosei-Macomer), and the SS 389 (Monti-Lanusei). It is connected by train (FdS) to Macomer and by bus (ARST, Azienda Regionale Sarda Trasporti) to Cagliari, Sassari, Olbia, and to several minor centres in the province and the region. ATP Nuoro's bus system provides service within the city.

</doc>
<doc id="22122" url="http://en.wikipedia.org/wiki?curid=22122" title="Nürburgring">
Nürburgring

Nürburgring is a 150,000-capacity motorsports complex around the village of Nürburg, Rhineland-Palatinate, Germany. It is located about 70 km south of Cologne, and 120 km northwest of Frankfurt. It features a Grand Prix race track built in 1984, and a much longer old "North loop" track which was built in the 1920s around the village and medieval castle of Nürburg in the Eifel mountains. The north loop is 20.8 km long and has more than 300 metres (1,000 feet) of elevation change from its lowest to highest points. Jackie Stewart nicknamed the old track "The Green Hell".
Originally, the track featured four configurations: the 28.265 km-long "Gesamtstrecke" ("Whole Course"), which in turn consisted of the 22.810 km "Nordschleife" ("North Loop"), and the 7.747 km "Südschleife" ("South Loop"). There also was a 2.281 km warm-up loop called "Zielschleife" ("Finish Loop") or "Betonschleife" ("Concrete Loop"), around the pit area.
Between 1982 and 1983 the start/finish area was demolished to create a new "GP-Strecke", and this is used for all major and international racing events. However, the shortened "Nordschleife" is still in use for racing, testing and public access.
History.
1925–1939: The beginning of the "Nürburg-Ring".
In the early 1920s, ADAC Eifelrennen races were held on public roads in the Eifel mountains. This was soon recognised as impractical and dangerous. The construction of a dedicated race track was proposed, following the examples of Italy's Monza and Targa Florio courses, and Berlin's AVUS, yet with a different character. The layout of the circuit in the mountains was similar to the Targa Florio event, one of the most important motor races at that time. The original Nürburgring was to be a showcase for German automotive engineering and racing talent. Construction of the track, designed by the "Eichler Architekturbüro" from Ravensburg (led by architect Gustav Eichler), began in September 1925.
The track was completed in spring of 1927, and the ADAC Eifelrennen races were continued there. The first races to take place on 18 June 1927 showed motorcycles and sidecars. The first motorcycle race was won by Toni Ulmen on an English 350 cc Velocette. The cars followed a day later, and Rudolf Caracciola was the winner of the over 5000 cc class in a Mercedes Compressor. In addition, the track was opened to the public in the evenings and on weekends, as a one-way toll road. The whole track consisted of 174 bends (prior to 1971 changes), and averaged 8 to in width. The fastest time ever around the full "Gesamtstrecke" was by Louis Chiron, at an average speed of 112.31 km/h (72 mph) in his Bugatti.
In 1929 the full Nürburgring was used for the last time in major racing events, as future Grands Prix would be held only on the "Nordschleife". Motorcycles and minor races primarily used the shorter and safer "Südschleife". Memorable pre-war races at the circuit featured the talents of early "Ringmeister" (Ringmasters) such as Rudolf Caracciola, Tazio Nuvolari and Bernd Rosemeyer.
1947–1970: The Green Hell.
After World War II, racing resumed in 1947 and in 1951, the "Nordschleife" of the Nürburgring again became the main venue for the German Grand Prix as part of the Formula One World Championship (with the exception of 1959, when it was held on the AVUS in Berlin). A new group of "Ringmeister" arose to dominate the race – Alberto Ascari, Juan Manuel Fangio, Stirling Moss, Jim Clark, John Surtees, Jackie Stewart and Jacky Ickx.
On 5 August 1961, during practice for the 1961 German Grand Prix, Phil Hill became the first person to complete a lap of the "Nordschleife" in under 9 minutes, with a lap of 8 minutes 55.2 seconds (153.4 km/h or 95.3 mph) in the Ferrari 156 "Sharknose" Formula One car. Over half a century later, the highest-performing road cars have difficulty breaking 8 minutes without a professional race driver or one very familiar with the track. Also, several rounds of the German motorcycle Grand Prix were held, mostly on the 7.7 km "Südschleife", but the Hockenheimring and the Solitudering were the main sites for Grand Prix motorcycle racing.
In 1953, the ADAC 1000 km Nürburgring race was introduced, an Endurance race and Sports car racing event that counted towards the World Sportscar Championship for decades. The 24 Hours Nürburgring for touring car racing was added in 1970.
By the late 1960s, the "Nordschleife" and many other tracks were becoming increasingly dangerous for the latest generation of F1 cars. In 1967, a chicane was added before the start/finish straight, called "Hohenrain", in order to reduce speeds at the pit lane entry. This made the track 25 m longer. Even this change, however, was not enough to keep Stewart from nicknaming it "The Green Hell" following his victory in the 1968 German Grand Prix amid a driving rainstorm and thick fog. In 1970, after the fatal crash of Piers Courage at Zandvoort, the F1 drivers decided at the French Grand Prix to boycott the Nürburgring unless major changes were made, as they did at Spa the year before. The changes were not possible on short notice, and the German GP was moved to the Hockenheimring, which had already been modified.
1971–1983: Changes.
In accordance with the demands of the F1 drivers the "Nordschleife" was reconstructed by taking out some bumps, smoothing out some sudden jumps (particularly at Brünnchen), and installing Armco safety barriers. The track was made straighter, following the race line, which reduced the number of corners. The German GP could be hosted at the Nürburgring again, and was for another six years from 1971 to 1976.
In 1973 the entrance into the dangerous and bumpy Kallenhard corner was made slower by adding another left-hand corner after the fast Metzgesfeld sweeping corner. Safety was improved again later on, e.g. by removing the jumps on the long main straight and widening it, and taking away the bushes right next to the track at the main straight, which made that section of the Nürburgring dangerously narrow. A second series of three more F1 races was held until 1976. However, primarily due to its length of over 22 km, and the lack of space due to its situation on the sides of the mountains, increasing demands by the F1 drivers and the FIA's CSI commission were too expensive or impossible to meet. For instance, by the 1970s the German Grand Prix required five times the marshals and medical staff as a typical F1 race, something the German organizers were unwilling to provide. Additionally, even with the 1971 modifications it was still possible for cars to become airborne off the track. The Nürburgring was also unsuitable for the burgeoning television market; its vast expanse made it almost impossible to effectively cover a race there. As a result, early in the season it was decided that the 1976 race would be the last to be held on the old circuit.
Niki Lauda, the reigning world champion and only person ever to lap the full 22835 m "Nordschleife" in under 7 minutes (6:58.6, 1975), proposed to the other drivers that they boycott the circuit in 1976. Lauda was not only concerned about the safety arrangements and the lack of marshals around the circuit, but did not like the prospect of running the race in another rainstorm- usually when that happened, some parts of the circuit were wet and other parts were dry, which is what the conditions of the circuit were for that race. The other drivers voted against the idea and the race went ahead. Lauda crashed in his Ferrari coming out of the left-hand kink before Bergwerk, for causes that were never established. He was badly burned as his car was still loaded with fuel in lap 2. Lauda was saved by the combined actions of fellow drivers Arturo Merzario, Guy Edwards, Brett Lunger, and Harald Ertl, rather than by the ill-equipped track marshals.
The crash also showed that the track's distances were too long for regular fire engines and ambulances, even though the "ONS-Staffel" was equipped with a Porsche 911 rescue car, marked (R). The old Nürburgring never hosted another F1 race again, as the German Grand Prix was moved to the Hockenheimring for 1977. The German motorcycle Grand Prix was held for the last time on the old Nürburgring in 1980, also permanently moving to Hockenheim.
By its very nature, the "Nordschleife" was impossible to make safe in its old configuration. It soon became apparent that it would have to be completely overhauled if there was any prospect of Formula One returning there. With this in mind, in 1981 work began on a 4.5 km-long new circuit, which was built on and around the old pit area.
At the same time, a bypass shortened the "Nordschleife" to 20832 m, and with an additional small pit lane, this version was used for races in 1983, e.g. the 1000km Nürburgring endurance race, while construction work was going on nearby. In training for that race, the late Stefan Bellof set the all-time lap record for the 20.8 km "Nordschleife" in his Porsche 956, which is still unbeaten at 6:11.13, or over 200 km/h on average (partially because no major racing has taken place there since 1984).
Meanwhile, more run-off areas were added at corners like Aremberg and Brünnchen, where originally there were just embankments protected by Armco barriers. The track surface was made safer in some spots where there had been nasty bumps and jumps. Racing line markers were added to the corners all around the track as well. Also, bushes and hedges at the edges of corners were taken out and replaced with Armco and grass.
The former "Südschleife" had not been modified in 1970/71 and was abandoned a few years later in favour of the improved "Nordschleife". It is now mostly gone (in part due to the construction of the new circuit) or converted to a normal public road, but since 2005 a vintage car event has been hosted on the old track layout, including part of the parking area.
1984: The new Grand Prix track.
The new track was completed in 1984 and named "GP-Strecke" (German: "Großer Preis-Strecke": literally, "Grand Prix Course"). It was built to meet the highest safety standards. 
However, it was considered in character a mere shadow of its older sibling. Some fans, who had to sit much farther away from the track, called it "Eifelring", "Ersatzring", "Grünering" or similar nicknames, believing it did not deserve to be called Nürburgring. Like many circuits of the time, it offered few overtaking opportunities.
Prior to the 2013 German Grand Prix both Mark Webber and Lewis Hamilton said they like the track. Webber described the layout as "an old school track" before adding, "It’s a beautiful little circuit for us to still drive on so I think all the guys enjoy driving here." While Hamilton said "It’s a fantastic circuit, one of the classics and it hasn’t lost that feel of an old classic circuit."
To celebrate its opening, an exhibition race was held, on 12 May, featuring an array of notable drivers. Driving identical Mercedes 190E 2.3–16's, the line-up was Elio de Angelis, Jack Brabham (Formula 1 World Champion 1959, 1960, 1966), Phil Hill (1961), Denis Hulme (1967), James Hunt (1976), Alan Jones (1980), Jacques Laffite, Niki Lauda (1975, 1977)*, Stirling Moss, Alain Prost*, Carlos Reutemann, Keke Rosberg (1982), Jody Scheckter (1979), Ayrton Senna*, John Surtees (1964) and John Watson. Senna won ahead of Lauda, Reutemann, Rosberg, Watson, Hulme and Jody Scheckter, being the only one to resist Lauda's overwhelming performance who – having missed the qualifying – had to start from the last row and overtook all the others except Senna.
The asterisk ( * ) in the previous paragraph indicate that titles which were not yet won at the time of the race are not mentioned here, so there were nine former and two future Formula 1 World Champions competing, in a field of 20 cars with 16 Formula 1 drivers; the other four were local drivers: Klaus Ludwig, Manfred Schurti, Udo Schütz and Hans Herrmann.
Besides other major international events, the Nürburgring has seen the brief return of Formula One racing, as the 1984 European Grand Prix was held at the track, followed by the 1985 German Grand Prix. As F1 did not stay, other events were the highlights at the new Nürburgring, including the 1000km Nürburgring, DTM, motorcycles, and newer types of events, like truck racing, vintage car racing at the AvD "Oldtimer Grand Prix", and even the "Rock am Ring" concerts.
Following the success and first world championship of Michael Schumacher, a second German F1 race was held at the Nürburgring between 1995 and 2006, called the European Grand Prix, or in 1997 and 1998, the Luxembourg Grand Prix.
For 2002, the track was changed, by replacing the former "Castrol-chicane" at the end of the start/finish straight with a sharp right-hander (nicknamed "Haug-Hook"), in order to create an overtaking opportunity. Also, a slow Omega-shaped section was inserted, on the site of the former kart track. This extended the GP track from 4500 to, while at the same time, the Hockenheimring was shortened from 6800 to.
Both the Nürburgring and the Hockenheimring events have been losing money due to high and rising license fees charged by Bernie Ecclestone and low attendance due to high ticket prices; starting with the 2007 Formula One season, Hockenheim and Nürburgring will alternate for hosting of the German GP.
In Formula One, Ralf Schumacher collided with his brother at the start of the 1997 race, which may have cost Michael the championship. In 1999, in changing conditions, Johnny Herbert managed to score the only win for the team of former "Ringmeister" Jackie Stewart. One of the highlights of the 2005 season was Kimi Räikkönen's spectacular exit while in the last lap of the race, when his suspension gave way after being rattled lap after lap by a flat-spotted tire that was not changed due to the short-lived rule.
Prior to the 2007 European Grand Prix, the "Audi S" (turns 8 and 9) was renamed "Michael Schumacher S" after Michael Schumacher. Schumacher had retired from Formula One the year before, but returned in 2010, and in 2011 became the second Formula One driver to drive through a turn named after them (after Ayrton Senna driving his "S for Senna" at Autódromo José Carlos Pace).
Alternation with Hockenheim.
In 2007, the FIA announced that Hockenheimring and Nürburgring would alternate with the German Grand Prix with Nürburgring hosting in 2007. Due to name-licensing problems, it was held as the European Grand Prix that year. However, in 2008 the European Grand Prix was held at Valencia Street Circuit, Eastern Spain.
Fatal accidents.
While it is unusual for deaths to occur during sanctioned races, there are many accidents and several deaths each year during public sessions. It is common for the track to be closed several times a day for cleanup, repair, and medical intervention. While track management does not publish any official figures, several regular visitors to the track have used police reports to estimate the number of fatalities at somewhere between 3 and 12 in a full year. Jeremy Clarkson noted in "Top Gear" in 2004 that "over the years this track has claimed over 200 lives".
"Nordschleife" racing today.
Several touring car series still compete on the "Nordschleife", using either only the simple 20.8 km version with its separate small pit lane, or a combined 24.4 km-long track that uses a part of the original modern F1 track (without the Mercedes Arena section, which is often used for support pits) plus its huge pit facilities. Entry-level competition requires a regularity test (GLP) for street-legal cars. Two racing series (RCN/CHC and VLN) compete on 15 Saturdays each year, for several hours.
The annual highlight is the 24 Hours Nürburgring weekend, held usually in mid-May, featuring 220 cars (from small 100 hp cars to 700 hp Turbo Porsches or 500 hp factory race cars built by BMW, Opel, Audi, Mercedes-Benz), over 700 drivers (amateurs and professionals), and up to 290,000 spectators.
In 2015 the World Touring Car Championship is scheduled to host the WTCC Race of Germany at the Nordschleife as a support category to the 24h.
Automotive media outlets and manufacturers use the "Nordschleife" as a standard to publish their lap times achieved with production vehicles.
BMW Sauber’s Nick Heidfeld made history on 28 April 2007 as the first driver in over 30 years to tackle the Nürburgring "Nordschleife" track in a contemporary Formula One car. Heidfeld’s three demonstration laps round the German circuit in an F1.06 were the highlight of festivities celebrating BMW’s contribution to motorsport. About 45,000 spectators showed up for the main event, the third four-hour VLN race of the season, and the subsequent show by Heidfeld. Conceived largely as a photo opportunity, the lap times were not as fast as the car was capable of, BMW instead choosing to run the chassis at a particularly high ride height to allow for the "Nordschleife"'s abrupt gradient changes and to limit maximum speeds accordingly. Former F1 driver Hans-Joachim Stuck was injured during the race when he crashed his BMW Z4.
As part of the festivities before the 2013 Nürburgring 24 Hour race, Michael Schumacher and other Mercedes-Benz drivers took part in a promotional event which saw Schumacher complete a demonstration lap of the Nordschleife at the wheel of a 2011 Mercedes W02. As with Heidfeld's lap, and also partly due to F1's strict in-season testing bans, the lap left many motorsport fans underwhelmed.
"Nordschleife" public access.
Since its opening in 1927, the track has been used by the public for the so-called ""Touristenfahrten"," i.e. anyone with a road-legal car or motorcycle, as well as tour buses, motor homes, or cars with trailers. It is opened mainly on Sundays, but also many Saturdays and weekday evenings. The track may be closed for weeks during the winter months, depending on weather conditions and maintenance work. Passing on the right is prohibited, and some sections have speed limits.
This Nürburgring is a popular attraction for many driving enthusiasts and riders from all over the world, partly because of its history and the challenge it provides. The lack of oncoming traffic and intersections sets it apart from regular roads, and the absence of a blanket speed limit is a further attraction.
Normal ticket buyers on these tourist days cannot quite complete a full lap of the 20.8 km "Nordschleife", which bypasses the modern "GP-Strecke", as they are required to slow down and pass through a 200 m "pit lane" section where the toll gates are installed. On busier days, a mobile ticket barrier is installed on the main straight in order to reduce the length of queues at the fixed barriers. This is open to all ticket holders. On rare occasions, it is possible to drive both the "Nordschleife" and the Grand Prix circuit combined.
Drivers interested in lap times often time themselves from the first bridge after the barriers to the last gantry (aka Bridge-to-Gantry or BTG time) before the exit. However, the track's general conditions state that any form of racing, including speed record attempts, is forbidden. The driver's insurance coverage may consequently be voided, leaving the driver fully liable for damage. Normal, non-racing, non-timed driving accidents might be covered by driver's insurance, but it is increasingly common for UK insurers especially to insert exclusion clauses that mean drivers and riders on the Nürburgring only have third-party cover only or are not covered at all.
Commercial aspects.
One of the original purposes of the "Nordschleife" was as a test track for auto manufacturers, and its demanding layout had been traditionally used as a proving ground. Weekdays are often booked for so-called "Industriefahrten" for auto makers and the media. With the advent of the Internet, awareness of the "Nordschleife" has risen in Germany and abroad, in addition to publicity in print media. In 1999, Porsche reported that their new 996 GT3 had lapped the Nürburgring in under eight minutes, and in subsequent years, manufacturers from overseas also showed up to test cars. Some high-performance models are promoted with videotaped laps published on the web, and the claimed lap times are generating discussion. Few of these "supercars" are actually entered in racing where the claims could be backed up.
The TV Series "Top Gear" has also used the "Nordschleife" for its challenges, often involving Sabine Schmitz. In addition, during series 17 (summer 2011) of Top Gear, James May was very critical of the ride quality of cars whose development processes included testing on the "Nordschleife", saying that cars which were tested at Nordschleife got ruined.
Other pastimes are hosted at the Nürburgring, such as the "Rock am Ring", Germany's biggest rock festival, attracting close to 100,000 rock fans each year since 1985. Since 1978, the "Nordschleife" is also the venue of a major running event (Nürburgring-Lauf/Run am Ring). In 2003, a major cycling event (Rad am Ring) was added and it became the multi-sports event "Rad & Run am Ring".
In 2009, new commercial areas opened, including a hotel and shopping mall. In the summer of 2009, ETF Ride Systems opened a new interactive dark ride application called "Motor Mania" at the racetrack, in collaboration with Lagotronics B.V. The roller coaster "ring°racer" was scheduled to open in 2011 but never started its operations due to technical failures.
In 2012, the track was preparing to file for bankruptcy as a result of nearly $500 million in debts and the inability to secure financing. On 1 August 2012, the government of Rheinland-Pfalz guaranteed $312 million to allow the track to meet its debt obligations.
In 2013, the Nürburgring was for sale for US$165 million (€127.3 million). The sale process was by sealed-bid auction with an expected completion date of "Late Summer". This meant there was to be a new owner in 2013, unencumbered by the debts of the previous operation, with the circuit expected to return to profitability.
On March 11, 2014 it was reported that the Nürburgring was sold for 77 million euros ($106.8 million). Düsseldorf-based Capricorn Development was the buyer. The company was to take full ownership of the Nürburgring on January 1, 2015.
In May 2015, the Nürburgring was set to hold the first "Grüne Hölle Rock" festival as a replacement for the "Rock am Ring" festival, but it fell through. "Grüne Hölle Rock" has changed their name to "Rock im Revier" and will be held in the Schalke area.
"Nordschleife" map.
Locations of note.
"Flugplatz" (lit. "Flying Place", but means "air field", a small airport).
The "Nordschleife" was formerly known for its abundance of sharp crests, causing fast-moving, firmly-sprung racing cars to jump clear off the track surface at many locations. Although by no means the most fearsome, "Flugplatz" is perhaps the most aptly (although coincidentally) named and widely remembered. The name of this part of the track comes from a small airfield, which in the early years was located close to the track in this area. The track features a very short straight that climbs sharply uphill for a short time, then suddenly drops slightly downhill, and this is immediately followed by two very fast right-hand kinks. Chris Irwin's career was ended following a massive accident at "Flugplatz", in a Ford 3L GT sports car in 1968. Manfred Winkelhock flipped his March F2 car at the same corner in 1980. The Flugplatz is one of the most important parts of the Nürburgring because after the two very fast right handers comes what is possibly the fastest part of the track: a downhill straight called "Kottenborn", into a very fast curve called "Schwedenkreuz" (Swedish Cross). Drivers are flat for some time here.
Right before Flugplatz is Quiddelbacher Höhe (Peak, as in "Mountain Summit"), where the track crosses a bridge over the Bundesstraße 257.
"Fuchsröhre" ("Fox Hole").
The "Fuchsrohre" is soon after the very fast downhill section succeeding the Flugplatz. After negotiating a long right hand corner called Aremberg (which is after Schwedenkreuz) the road goes slightly uphill, under a bridge and then it plunges downhill, and the road switches back left and right and finding a point of reference for the racing line is difficult. This whole sequence is flat out and then, the road climbs sharply uphill. The road then turns left and levels out at the same time; this is one of the many jumps of the Nürburgring where the car goes airborne. This leads to the Adenauer Forst (Forest) turns. The Fuchsrohre is one of the fastest and most dangerous parts of the Nürburgring because of the extremely high speeds in such a tight and confined place; this part of the Nürburgring goes right through a forest and there is only about 7–8 feet of grass separating the track from Armco barrier, and beyond the barriers is a wall of trees.
"Bergwerk" ("Mine").
Perhaps the most notorious corner on the long circuit, "Bergwerk" has been responsible for some serious and sometimes fatal accidents. A tight right-hand corner, coming just after a long, fast section and a left-hand kink on a small crest, was where Carel Godin de Beaufort fatally crashed. The fast kink was also the scene of Niki Lauda's infamous fiery accident during the 1976 German Grand Prix. This left kink is often referred to as the Lauda Links (Lauda left). The Bergwerk, along with the Breidscheid/Adenauer Bridge corners before it, are one of the series of corners that make or break one's lap time around the Nürburgring because of the fast, lengthy uphill section called "Kesselchen" (Little Valley) that comes after the Bergwerk.
Caracciola "Karussell" ("Carousel").
Although being one of the slower corners on the "Nordschleife", the "Karussell" is perhaps its most famous and one of its most iconic- it is one of two berm-style, banked corners on the track. Soon after the driver has negotiated the long uphill section after Bergwerk and gone through a section called "Klostertal" (Monastery Valley), the driver turns right through a long hairpin, past an abandoned section called "Steilstrecke" (Steep Route) and then goes up another hill towards the Karrusell. The entrance to the corner is blind, although Juan Manuel Fangio is reputed to have advised a young driver to "aim for the tallest tree," a feature that was also built into the rendering of the circuit in the Gran Turismo 4 and Grand Prix Legends video games. Once the driver has reached the top of the hill, the road then becomes sharply banked on one side and level on the other- this banking drops off, rather than climbing up like most bankings on circuits. The sharply banked side has a concrete surface, and there is a foot-wide tarmac surface on the bottom of the banking for cars to get extra grip through the very rough concrete banking. Cars drop into the concrete banking, and keep the car in the corner (which is 210 degrees, much like a hairpin bend) until the road levels out and the concrete surface becomes tarmac again. This corner is very hard on the driver's wrists and hands because of the prolonged bumpy cornering the driver must do while in the Karrusell. Usually cars come out of the top of the end of the banking to hit the apex that comes right after the end of the Karrusell.
The combination of a recognisable corner, slow-moving cars, and the variation in viewing angle as cars rotate around the banking, means that this is one of the circuit's most popular locations for photographers. It is named after German pre-WWII racing driver Rudolf Caracciola, who reportedly made the corner his own by hooking the inside tires into a drainage ditch to help his car "hug" the curve. As more concrete was uncovered and more competitors copied him, the trend took hold. At a later reconstruction, the corner was remade with real concrete banking, as it remains to this day.
Shortly after the Karussell is a steep section, with gradients in excess of 16%, leading to a right-hander called Hohe Acht, which is some 300 m higher in altitude than Breidscheid.
"Brünnchen" ("Small Fountain").
A favourite spectator vantage point, the Brünnchen section is composed of two right-hand corners and a very short straight. The first corner goes sharply downhill and the next, after the very short downhill straight, goes uphill slightly. This is a section of the track where on public days, accidents happen particularly at the blind uphill right-hand corner. Like almost every corner at the Nürburgring, both right-handers are blind. The short straight used to have a steep and sudden drop-off that caused cars to take off and a bridge that went over a pathway; these were taken out and smoothed over when the circuit was rebuilt in 1970 and 1971.
"Pflanzgarten" ("Planting Garden") and "Stefan Bellof S" ("Stefan Bellof Esses").
The Pflanzgarten, which is soon after the Brunnchen, is one of the fastest, trickiest and most difficult sections of the Nürburgring. It is full of jumps, including 2 huge ones, one of which is called "Sprunghugel" (Hill Jump). This very complex section is unique in that it is made up of 2 different sections- getting the entire Pflanzgarten right is crucial to a good lap time around the Nürburgring. This section was the scene of Briton Peter Collins's fatal accident during the German Grand Prix in 1958, and the scene of a number of career ending accidents in Formula One in the 1970s – Britons Mike Hailwood and Ian Ashley were 2 victims of the Pflanzgarten.
Pflanzgarten 1 is made up of a slightly banked, downhill left hander which then suddenly switches back left, then right. Then immediately, giving the driver nearly no time to react (knowledge of this section is key) the road drops away twice- the first jump is only slight, then right after (somewhat like a staircase) the road drops away very sharply which usually causes almost all cars to go airborne at this jump; the drop is so sudden. Then, immediately after the road levels out very shortly after the jump and the car touches the ground again, the road immediately and suddenly goes right very quickly and then right again; this is what makes up the end of the first Pflanzgarten- a very fast multiple apex sequence of right hand corners.
The road then goes slightly uphill and then through another jump; the road suddenly drops away and levels out and at the same time, the road turns through a flat-out left hander. Then, the road suddenly drops away again very suddenly, which is the 2nd huge jump of the Pflanzgarten known as the Sprunghugel. The road then goes downhill then quickly levels out, then it goes through a flat-out right hander and this starts the Stefan Bellof S, which was known as Pflanzgarten 2 prior to 2013. The Stefan Bellof S is very tricky because the road quickly switches back left and right- a car is going so fast through here that it is like walking on a tightrope. It is very difficult to find the racing line here because the curves come up so quickly, so it is hard to find any point of reference. Then, after a jump at the end of the switchback section, it goes through a flat-out, top gear right hander and into a short straight that leads into two very fast curves called the Schwalbenschwanz (Swallow's Tail).
The room for error on every part of the consistently high-speed Pflanzgarten and Bellof Esses is virtually non-existent (much like the entire track itself)- this is why it is such a difficult part of the circuit. The road and the surface of the Pflanzgarten and Bellof moves around unpredictably; knowledge of this section is key to getting through cleanly.
"Schwalbenschwanz/Kleines Karussell" ("Swallow's Tail"/"Little Carousel").
The Schwalbenschwanz is a sequence of very fast sweepers located after the second Pflanzgarten. After a short straight, there is a very fast right hand sweeper that progressively goes uphill, and this leads into a blind left-hander that is a bit slower (but still rather fast). The apex is completely blind, and the corner then changes gradient a bit; it goes up then down, which leads into a short straight that ends at the Kleines Karussell. Originally, this part had a bridge that went over a stream and it was very bumpy; this bridge was taken out and replaced with a culvert (large industrial pipe) so that the road could be smoothed over.
The Kleines Karussell is similar to its bigger brother, except that it is a 90 degree corner instead of 210 degrees, and is faster and slightly less banked. Once this part of the track is dealt with, the drivers are near the end of the lap; with 2 more corners to negotiate before the 2.135 km long Döttinger Höhe straight.
Lap times.
Lap times recorded on the Nürburgring "Nordschleife" are published by several manufacturers. They are published and discussed in print media, and online.

</doc>
<doc id="22126" url="http://en.wikipedia.org/wiki?curid=22126" title="Northern Hemisphere">
Northern Hemisphere

The Northern Hemisphere of Earth is the half that is north of the equator. For other planets in the Solar System, north is defined as being in the same celestial hemisphere relative to the invariable plane of the solar system as Earth's North pole.
Due to the Earth's axial tilt, winter in the Northern Hemisphere lasts from the winter solstice (typically December 21) to the March Equinox (typically March 20), while summer lasts from the summer solstice (typically June 21) through to the autumnal equinox (typically September 23).
Geography and climate.
The Arctic is the region north of the Arctic Circle. Its climate is characterized by cold winters and cool summers. Precipitation mostly comes in the form of snow. The Arctic experiences some days in summer when the Sun never sets, and some days during the winter when it never rises. The duration of these phases varies from one day for locations right on the Arctic Circle to several months near the North Pole.
Between the Arctic Circle and the Tropic of Cancer lies the Northern Temperate Zone. The changes in these regions between summer and winter are generally mild, rather than extreme hot or cold. However, a temperate climate can have very unpredictable weather.
Tropical regions (between the Tropic of Cancer and the equator) are generally hot all year round and tend to experience a rainy season during the summer months, and a dry season during the winter months.
In the Northern Hemisphere, objects moving across or above the surface of the Earth tend to turn to the right because of the coriolis effect. As a result, large-scale horizontal flows of air or water tend to form clockwise-turning gyres. These are best seen in ocean circulation patterns in the North Atlantic and North Pacific oceans.
For the same reason, flows of air down toward the northern surface of the Earth tend to spread across the surface in a clockwise pattern. Thus, clockwise air circulation is characteristic of high pressure weather cells in the Northern Hemisphere. Conversely, air rising from the northern surface of the Earth (creating a region of low pressure) tends to draw air toward it in a counterclockwise pattern. Hurricanes and tropical storms (massive low-pressure systems) spin counter-clockwise in the Northern Hemisphere.
The shadow of a sundial moves clockwise in the Northern Hemisphere (opposite of the Southern Hemisphere). During the day, the Sun tends to raise to its maximum at a southerly position.
Also, the Moon appears "upside down" compared to a view from the Southern Hemisphere and the view of the stars is very different. The North Pole faces away from the galactic centre of the Milky Way. This results in there being fewer and less bright visible stars in the Northern Hemisphere compared to the Southern Hemisphere, making the Northern Hemisphere more suitable for deep-space observation, as it is not "blinded" by the Milky Way.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="22130" url="http://en.wikipedia.org/wiki?curid=22130" title="Noun class">
Noun class

In linguistics, a noun class is a particular category of nouns. A noun may belong to a given class because of characteristic features of its referent, such as sex, animacy, shape, but counting a given noun among nouns of such or another class is often clearly conventional. Some authors use the term "grammatical gender" as a synonym of "noun class", but others use different definitions for each (see below). Noun classes should not be confused with noun classifiers.
Notion.
In general, there are three main ways by which natural languages categorize nouns into noun classes:
Usually, a combination of the three types of criteria is used, though one is more prevalent.
Noun classes form a system of grammatical agreement. The fact that a noun belongs to a given class may imply the presence of:
Modern English expresses noun classes through the third person singular personal pronouns "he" (male person), "she" (female person), and "it" (object, abstraction, or animal), and their other inflected forms. The choice between the relative pronoun "who" (persons) and "which" (non-persons) may also be considered a way of categorizing nouns into noun classes. A few nouns also exhibit vestigial noun classes, such as "stewardess", where the suffix "-ess" added to "steward" denotes a female person. This type of noun affixation is not very frequent in English, but quite common in languages which have the true grammatical gender, including most of the Indo-European family, to which English belongs. 
When noun class is expressed on other parts of speech, besides nouns and pronouns, the language is said to have grammatical gender.
In languages without inflectional noun classes, nouns may still be extensively categorized by independent particles called noun classifiers.
Common criteria for noun classes.
Common criteria that define noun classes include:
See Swahili for the semantic motivations for an elaborate noun-class system.
Language families.
Algonquian languages.
The Ojibwe language and other members of the Algonquian languages distinguish between animate and inanimate classes. Some sources argue that the distinction is between things which are powerful and things which are not. All living things, as well as sacred things and things connected to the Earth are considered powerful and belong to the animate class. Still, the assignment is somewhat arbitrary, as "raspberry" is animate, but "strawberry" is inanimate.
Athabaskan languages.
In Navajo (Southern Athabaskan) nouns are classified according to their animacy, shape, and consistency. Morphologically, however, the distinctions are not expressed on the nouns themselves, but on the verbs of which the nouns are the subject or direct object. For example, in the sentence "Shi’éé’ tsásk’eh bikáa’gi dah siłtsooz" "My shirt is lying on the bed", the verb siłtsooz "lies" is used because the subject "shi’éé’" "my shirt" is a flat, flexible object. In the sentence Siziiz tsásk’eh bikáa’gi dah silá "My belt is lying on the bed", the verb "silá" "lies" is used because the subject "siziiz" "my belt" is a slender, flexible object. See Navajo language: Classificatory Verbs for more discussion.
Koyukon (Northern Athabaskan) has a more intricate system of classification. Like Navajo, it has classificatory verb stems that classify nouns according to animacy, shape, and consistency. However, in addition to these verb stems, Koyukon verbs have what are called "gender prefixes" that further classify nouns. That is, Koyukon has two different systems that classify nouns: (a) a classificatory verb system and (b) a gender system. To illustrate, the verb stem "-tonh" is used for enclosed objects. When "-tonh" is combined with different gender prefixes, it can result in "daaltonh" which refers to objects enclosed in boxes or "etltonh" which refers to objects enclosed in bags.
Australian Aboriginal languages.
The Dyirbal language is well known for its system of four noun classes, which tend to be divided along the following semantic lines:
The class usually labeled "feminine", for instance, includes the word for fire and nouns relating to fire, as well as all dangerous creatures and phenomena. (This inspired the title of the George Lakoff book "Women, Fire, and Dangerous Things".)
The Ngangikurrunggurr language has noun classes reserved for canines and hunting weapons. The Anindilyakwa language has a noun class for things that reflect light. The Diyari language distinguishes only between female and other objects. Perhaps the most noun classes in any Australian language are found in 
Yanyuwa, which has 16 noun classes, including nouns associated with food, trees and abstractions, in addition to separate classes for men and masculine things, women and feminine things. In the men's dialect, the classes for men and for masculine things have simplified to a single class, marked the same way as the women's dialect marker reserved exclusively for men.
Basque.
In Basque there are two classes, animate and inanimate; however, the only difference is in the declension of locative cases (inessive, locative genitive, adlative, terminal adlative, ablative and directional ablative). There are a few words with both masculine and feminine forms, generally words for relatives (cousin: lehengusu (m)/lehengusina (f)) or words borrowed from Latin ("king": "errege", from the Latin word "rex"; "queen": "erregina", from "regina"). In names for familiar relatives, where both genders are taken into account, either the words for each gender are put together ("son": "seme"; "daughter": "alaba"; "children"(meaning son(s) and daughter(s)): "seme-alaba(k)") or there is a noun that includes both: "father": "aita"; "mother": "ama"; "parent": "guraso".
Caucasian languages.
Some members of the Northwest Caucasian family, and almost all of the Northeast Caucasian languages, manifest noun class. In the Northeast Caucasian family, only Lezgian, Udi, and Aghul do not have noun classes. Some languages have only two classes, whereas Bats has eight. The most widespread system, however, has four classes: male, female, animate beings and certain objects, and finally a class for the remaining nouns. The Andi language has a noun class reserved for insects.
Among Northwest Caucasian languages, only Abkhaz and Abaza have noun class, making use of a human male/human female/non-human distinction.
In all Caucasian languages that manifest class, it is not marked on the noun itself but on the dependent verbs, adjectives, pronouns and prepositions.
Niger–Congo languages.
Niger–Congo languages can have ten or more noun classes, defined according to non-sexual criteria. Certain nominal classes are reserved for humans. The Fula language has about 26 noun classes (exact number varies slightly by dialect). According to Steven Pinker, the Kivunjo language has 16 noun classes including classes for precise locations and for general locales, classes for clusters or pairs of objects and classes for the objects that come in pairs or clusters, and classes for abstract qualities.
Bantu languages.
According to Carl Meinhof, the Bantu languages have a total of 22 noun classes called nominal classes (this notion was introduced by W.H.J. Bleek). While no single language is known to express all of them, most of them have at least 10 noun classes. For example, by Meinhof's numbering, Shona has 20 classes, Swahili has 15, Sotho has 18 and Ganda has 17.
Additionally, there are polyplural noun classes. A polyplural noun class is a the plural class for more than one singular class. For example, Proto-Bantu class 10 contains plurals of class 9 nouns and class 11 nouns, while class 6 contains plurals of class 5 nouns and class 15 nouns. Classes 6 and 10 are inherited as polyplural classes by most surviving Bantu languages, but many languages have developed new polyplural classes that are not widely shared by other languages.
Specialists in Bantu emphasize that there is a clear difference between genders (such as known from Afro-Asiatic and Indo-European) and nominal classes (such as known from Niger–Congo). Languages with nominal classes divide nouns formally on the base of hyperonymic meanings. The category of nominal class replaces not only the category of gender, but also the categories of number and case.
Critics of the Meinhof's approach notice that his numbering system of nominal classes counts singular and plural numbers of the same noun as belonging to separate classes. This seems to them to be inconsistent with the way other languages are traditionally considered, where number is orthogonal to gender (according to the critics, a Meinhof-style analysis would give Ancient Greek 9 genders). If one follows broader linguistic tradition and counts singular and plural as belonging to the same class, then Swahili has 8 or 9 noun classes, Sotho has 11 and Ganda has 10.
The Meinhof numbering tends to be used in scientific works dealing with comparisons of different Bantu languages. For instance, in Swahili the word "rafiki" ‘friend’ belongs to the class 9 and its "plural form" is "marafiki" of the class 6, even if most nouns of the 9 class have the plural of the class 10. For this reason, noun classes are often referred to by combining their singular and plural forms, e.g., "rafiki" would be classified as "9/6", indicating that it takes class 9 in the singular, and class 6 in the plural.
However not all Bantu languages have these exceptions. In Ganda each singular class has a corresponding plural class (apart from one class which has no singular–plural distinction; also some plural classes correspond to more than one singular class) and there are no exceptions as there are in Swahili. For this reason Ganda linguists use the orthogonal numbering system when discussing Ganda grammar (other than in the context of Bantu comparative linguistics), giving the 10 traditional noun classes of that language.
The distinction between genders and nominal classes is blurred still further by Indo-European languages that have nouns that behave like Swahili's "rafiki". Italian, for example, has a group of nouns deriving from Latin neuter nouns that acts as masculine in the singular but feminine in the plural: "il braccio"/"le braccia"; "l'uovo"/"le uova". (These nouns are still placed in a neuter gender of their own by some grammarians.)
Here is a complete list of nominal classes in Swahili:
"Ø-" means no prefix. Note also that some classes are homonymous (esp. 9 and 10). The Proto-Bantu class 12 disappeared in Swahili, class 13 merged with 7, and 14 with 11.
Class prefixes appear also on adjectives and verbs, e.g.:
The class markers which appear on the adjectives and verbs may differ from the noun prefixes: 
In this example, the verbal prefix a- and the pronominal prefix wa- are in concordance with the noun prefix m-: they all express class 1 despite of their different forms.
Zande.
The Zande language distinguishes four noun classes:
There are about 80 inanimate nouns which are in the animate class, including nouns denoting heavenly objects (moon, rainbow), metal objects (hammer, ring), edible plants (sweet potato, pea), and non-metallic objects (whistle, ball). Many of the exceptions have a round shape, and some can be explained by the role they play in Zande mythology.
Noun classes versus grammatical genders.
The term gender, as used by some linguists, refers to a noun-class system composed with 2, 3, or 4 classes, particularly if the classification is semantically based on a distinction between masculine and feminine. Genders are then considered a sub-class of noun classes. Not all linguists recognize a distinction between noun-classes and genders, however, and instead use either the term "gender" or "noun class" for both.
Noun classes versus noun classifiers.
Some languages, such as Japanese, Chinese and the Tai languages, have elaborate systems of particles that nouns based on shape and function, but are free morphemes rather than affixes. Because the classes defined by these classifying words are not generally distinguished in other contexts, many if not most linguists take the view that they do not create noun classes.

</doc>
<doc id="22131" url="http://en.wikipedia.org/wiki?curid=22131" title="Natural gas">
Natural gas

Natural gas is a fossil fuel formed when layers of buried plants, gases, and animals are exposed to intense heat and pressure over thousands of years. The energy that the plants originally obtained from the sun is stored in the form of chemical bonds in natural gas. Natural gas is a nonrenewable resource because it cannot be replenished on a human time frame. Natural gas is a hydrocarbon gas mixture consisting primarily of methane, but commonly includes varying amounts of other higher alkanes and sometimes a usually lesser percentage of carbon dioxide, nitrogen, and/or hydrogen sulfide. Natural gas is an energy source often used for heating, cooking, and electricity generation. It is also used as fuel for vehicles and as a chemical feedstock in the manufacture of plastics and other commercially important organic chemicals.
Natural gas is found in deep underground rock formations or associated with other hydrocarbon reservoirs in coal beds and as methane clathrates. Petroleum is another resource and fossil fuel found in close proximity to, and with natural gas. Most natural gas was created over time by two mechanisms: biogenic and thermogenic. Biogenic gas is created by methanogenic organisms in marshes, bogs, landfills, and shallow sediments. Deeper in the earth, at greater temperature and pressure, thermogenic gas is created from buried organic material.
Before natural gas can be used as a fuel, it must be processed to remove impurities, including water, to meet the specifications of marketable natural gas. The by-products of this processing include: ethane, propane, butanes, pentanes, and higher molecular weight hydrocarbons, hydrogen sulfide (which may be converted into pure sulfur), carbon dioxide, water vapor, and sometimes helium and nitrogen.
Natural gas is often informally referred to simply as "gas", especially when compared to other energy sources such as oil or coal. However, it is not to be confused with gasoline, especially in North America, where the term gasoline is often shortened in colloquial usage to "gas".
Natural gas was used by the Chinese in about 500 BC. They discovered a way to transport gas seeping from the ground in crude pipelines of bamboo to where it was used to boil salt water to extract the salt, in the Ziliujing District of Sichuan. The world's first industrial extraction of natural gas started at Fredonia, New York, USA in 1825. By 2009, 66 trillion cubic meters (or 8%) had been used out of the total 850 trillion cubic meters of estimated remaining recoverable reserves of natural gas. Based on an estimated 2015 world consumption rate of about 3.4 trillion cubic meters of gas per year, the total estimated remaining economically recoverable reserves of natural gas would last 250 years at current consumption rates. An annual increase in usage of 2–3% could result in currently recoverable reserves lasting significantly less, perhaps as few as 80 to 100 years.
Sources.
Natural gas.
In the 19th century, natural gas was usually obtained as a by-product of producing oil, since the small, light gas carbon chains came out of solution as the extracted fluids underwent pressure reduction from the reservoir to the surface, similar to uncapping a soft drink bottle where the carbon dioxide effervesces. Unwanted natural gas was a disposal problem in the active oil fields. If there was not a market for natural gas near the wellhead it was virtually valueless since it had to be piped to the end user.
In the 19th century and early 20th century, such unwanted gas was usually burned off at oil fields. Today, unwanted gas (or stranded gas without a market) associated with oil extraction often is returned to the reservoir with 'injection' wells while awaiting a possible future market or to repressurize the formation, which can enhance extraction rates from other wells. In regions with a high natural gas demand (such as the US), pipelines are constructed when it is economically feasible to transport gas from a wellsite to an end consumer.
In addition to transporting gas via pipelines for use in power generation, other end uses for natural gas include export as liquefied natural gas (LNG) or conversion of natural gas into other liquid products via gas-to-liquids (GTL) technologies. GTL technologies can convert natural gas into liquids products such as gasoline, diesel or jet fuel. A variety of GTL technologies have been developed, including Fischer–Tropsch (F–T), methanol to gasoline (MTG) and STG+. F–T produces a synthetic crude that can be further refined into finished products, while MTG can produce synthetic gasoline from natural gas. STG+ can produce drop-in gasoline, diesel, jet fuel and aromatic chemicals directly from natural gas via a single-loop process. In 2011, Royal Dutch Shell’s 140,000 barrel per day F–T plant went into operation in Qatar.
Natural gas can be "associated" (found in oil fields), or "non-associated" (isolated in natural gas fields), and is also found in coal beds (as coalbed methane). It sometimes contains a significant amount of ethane, propane, butane, and pentane—heavier hydrocarbons removed for commercial use prior to the methane being sold as a consumer fuel or chemical plant feedstock. Non-hydrocarbons such as carbon dioxide, nitrogen, helium (rarely), and hydrogen sulfide must also be removed before the natural gas can be transported.
Natural gas extracted from oil wells is called casinghead gas (whether or not truly produced up the annulus and through a casinghead outlet) or associated gas. The natural gas industry is extracting an increasing quantity of gas from challenging resource types: sour gas, tight gas, shale gas, and coalbed methane.
There is some disagreement on which country has the largest proven gas reserves. Sources that consider that Russia has by far the largest proven reserves include the US CIA (47.6 trillion cubic meters), the US Energy Information Administration (47.8 tcm), and OPEC (48.7 tcm). However, BP credits Russia with only 32.9 tcm, which would place it in second place, slightly behind Iran (33.1 to 33.8 tcm, depending on the source). With Gazprom, Russia is frequently the world's largest natural gas extractor. Major proven resources (in billion cubic meters) are world 187,300 (2013), Iran 33,600 (2013), Russia 32,900 (2013), Qatar 25,100 (2013), Turkmenistan 17,500 (2013) and the United States 8,500 (2013).
It is estimated that there are about 900 trillion cubic meters of "unconventional" gas such as shale gas, of which 180 trillion may be recoverable. In turn, many studies from MIT, Black & Veatch and the DOE predict that natural gas will account for a larger portion of electricity generation and heat in the future.
The world's largest gas field is the offshore South Pars / North Dome Gas-Condensate field, shared between Iran and Qatar. It is estimated to have 51 trillion cubic meters of natural gas and 50 billion barrels of natural gas condensates.
Because natural gas is not a pure product, as the reservoir pressure drops when non-associated gas is extracted from a field under supercritical (pressure/temperature) conditions, the higher molecular weight components may partially condense upon isothermic depressurizing—an effect called retrograde condensation. The liquid thus formed may get trapped as the pores of the gas reservoir get depleted. One method to deal with this problem is to re-inject dried gas free of condensate to maintain the underground pressure and to allow re-evaporation and extraction of condensates. More frequently, the liquid condenses at the surface, and one of the tasks of the gas plant is to collect this condensate. The resulting liquid is called natural gas liquid (NGL) and has commercial value.
Shale gas.
Shale gas is natural gas produced from shale. Because shale has matrix permeability too low to allow gas to flow in economical quantities, shale gas wells depend on fractures to allow the gas to flow. Early shale gas wells depended on natural fractures through which gas flowed; almost all shale gas wells today require fractures artificially created by hydraulic fracturing. Since 2000, shale gas has become a major source of natural gas in the United States and Canada. Following the success in the United States, shale gas exploration is beginning in countries such as Poland, China, and South Africa. With the increase of shale production it has caused the United States to become the number one natural gas producer in the world
Town gas.
Town gas is a flammable gaseous fuel made by the destructive distillation of coal and contains a variety of calorific gases including hydrogen, carbon monoxide, methane, and other volatile hydrocarbons, together with small quantities of non-calorific gases such as carbon dioxide and nitrogen, and is used in a similar way to natural gas. This is a historical technology, not usually economically competitive with other sources of fuel gas today. But there are still some specific cases where it is the best option and it may be so into the future.
Most town "gashouses" located in the eastern US in the late 19th and early 20th centuries were simple by-product coke ovens that heated bituminous coal in air-tight chambers. The gas driven off from the coal was collected and distributed through networks of pipes to residences and other buildings where it was used for cooking and lighting. (Gas heating did not come into widespread use until the last half of the 20th century.) The coal tar (or asphalt) that collected in the bottoms of the gashouse ovens was often used for roofing and other waterproofing purposes, and when mixed with sand and gravel was used for paving streets.
Biogas.
Methanogenic archaea are responsible for all biological sources of methane. Some live in symbiotic relationships with other life forms, including termites, ruminants, and cultivated crops. Other sources of methane, the principal component of natural gas, include landfill gas, biogas, and methane hydrate. When methane-rich gases are produced by the anaerobic decay of non-fossil organic matter (biomass), these are referred to as biogas (or natural biogas). Sources of biogas include swamps, marshes, and landfills (see landfill gas), as well as agricultural waste materials such as sewage sludge and manure by way of anaerobic digesters, in addition to enteric fermentation, particularly in cattle. Landfill gas is created by decomposition of waste in landfill sites. Excluding water vapor, about half of landfill gas is methane and most of the rest is carbon dioxide, with small amounts of nitrogen, oxygen, and hydrogen, and variable trace amounts of hydrogen sulfide and siloxanes. If the gas is not removed, the pressure may get so high that it works its way to the surface, causing damage to the landfill structure, unpleasant odor, vegetation die-off, and an explosion hazard. The gas can be vented to the atmosphere, flared or burned to produce electricity or heat. Biogas can also be produced by separating organic materials from waste that otherwise goes to landfills. This method is more efficient than just capturing the landfill gas it produces. Anaerobic lagoons produce biogas from manure, while biogas reactors can be used for manure or plant parts. Like landfill gas, biogas is mostly methane and carbon dioxide, with small amounts of nitrogen, oxygen and hydrogen. However, with the exception of pesticides, there are usually lower levels of contaminants.
Landfill gas cannot be distributed through utility natural gas pipelines unless it is cleaned up to less than 3 per cent CO2, and a few parts per million H2S, because CO2 and H2S corrode the pipelines. The presence of CO2 will lower the energy level of the gas below requirements for the pipeline. Siloxanes in the gas will form deposits in gas burners and need to be removed prior to entry into any gas distribution or transmission system. Consequently it may be more economical to burn the gas on site or within a short distance of the landfill using a dedicated pipeline. Water vapor is often removed, even if the gas is burned on site. If low temperatures condense water out of the gas, siloxanes can be lowered as well because they tend to condense out with the water vapor. Other non-methane components may also be removed to meet emission standards, to prevent fouling of the equipment or for environmental considerations. Co-firing landfill gas with natural gas improves combustion, which lowers emissions.
Biogas, and especially landfill gas, are already used in some areas, but their use could be greatly expanded. Experimental systems were being proposed for use in parts of Hertfordshire, UK, and Lyon in France. Using materials that would otherwise generate no income, or even cost money to get rid of, improves the profitability and energy balance of biogas production. Gas generated in sewage treatment plants is commonly used to generate electricity. For example, the Hyperion sewage plant in Los Angeles burns 8 e6cuft of gas per day to generate power New York City utilizes gas to run equipment in the sewage plants, to generate electricity, and in boilers. Using sewage gas to make electricity is not limited to large cities. The city of Bakersfield, California, uses cogeneration at its sewer plants. California has 242 sewage wastewater treatment plants, 74 of which have installed anaerobic digesters. The total biopower generation from the 74 plants is about 66 MW.
Crystallized natural gas — hydrates.
Huge quantities of natural gas (primarily methane) exist in the form of hydrates under sediment on offshore continental shelves and on land in arctic regions that experience permafrost, such as those in Siberia. Hydrates require a combination of high pressure and low temperature to form.
In 2010, the cost of extracting natural gas from crystallized natural gas was estimated to 100–200 per cent the cost of extracting natural gas from conventional sources, and even higher from offshore deposits.
In 2013, Japan Oil, Gas and Metals National Corporation (JOGMEC) announced that they had recovered commercially relevant quantities of natural gas from methane hydrate.
Natural gas processing.
The image below is a schematic block flow diagram of a typical natural gas processing plant. It shows the various unit processes used to convert raw natural gas into sales gas pipelined to the end user markets.
The block flow diagram also shows how processing of the raw natural gas yields byproduct sulfur, byproduct ethane, and natural gas liquids (NGL) propane, butanes and natural gasoline (denoted as pentanes +).
Depletion.
Natural gas production in the U.S. reached a peak in 1973, and went over a second lower peak in 2001, but recently has peaked again and is continuing to rise.
Uses.
Mid-stream natural gas.
Natural gas flowing in the distribution lines and at the natural gas well head are often used to power natural gas powered engines. These engines rotate compressors to facilitate the natural gas transmission. These compressors are required in the mid-stream line to pressurize and to re-pressurize the natural gas in the transmission line as the gas travels. The natural gas transmission lines extend to the natural gas processing plant or unit which removes the higher molecular weighted natural gas hydrocarbons to produce a British thermal unit (BTU) value between 950 and 1050 BTUs. The processed natural gas may then be used for residential, commercial and industrial uses.
Often mid-stream and well head gases require removal of many of the various hydrocarbon species contained within the natural gas. Some of these gases include heptane, pentane, propane and other hydrocarbons with molecular weights above Methane (CH4) to produce a natural gas fuel which is used to operate the natural gas engines for further pressurized transmission. Typically, natural gas compressors require 950 to 1050 BTU per cubic foot to operate at the natural gas engines rotational name plate specifications.
Several methods are used to remove these higher molecular weighted gases for use at the natural gas engine. A few technologies are as follows:
Power generation.
Natural gas is a major source of electricity generation through the use of cogeneration, gas turbines and steam turbines. Natural gas is also well suited for a combined use in association with renewable energy sources such as wind or solar and for alimenting peak-load power stations functioning in tandem with hydroelectric plants. Most grid peaking power plants and some off-grid engine-generators use natural gas. Particularly high efficiencies can be achieved through combining gas turbines with a steam turbine in combined cycle mode. Natural gas burns more cleanly than other hydrocarbon fuels, such as oil and coal, and produces less carbon dioxide per unit of energy released. For an equivalent amount of heat, burning natural gas produces about 30 per cent less carbon dioxide than burning petroleum and about 45 per cent less than burning coal. The Energy Information Administration reports the following emissions in million metric tons of carbon dioxide in the world:
For 2012 as the official energy statistics of the US Government.
Coal-fired electric power generation emits around 2,000 pounds of carbon dioxide for every megawatt hour generated, which is almost double the carbon dioxide released by a natural gas-fired electric plant per megawatt hour generated. Because of this higher carbon efficiency of natural gas generation, as the fuel mix in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions have unexpectedly fallen. Those measured in the first quarter of 2012 were the lowest of any recorded for the first quarter of any year since 1992.
Combined cycle power generation using natural gas is currently the cleanest available source of power using hydrocarbon fuels, and this technology is widely and increasingly used as natural gas can be obtained at increasingly reasonable costs. Fuel cell technology may eventually provide cleaner options for converting natural gas into electricity, but as yet it is not price-competitive. Locally produced electricity and heat using natural gas powered Combined Heat and Power plant (CHP or Cogeneration plant) is considered energy efficient and a rapid way to cut carbon emissions.
Domestic use.
Natural gas dispensed from a simple stovetop can generate temperatures in excess of 1100 °C (2000 °F) making it a powerful domestic cooking and heating fuel. In much of the developed world it is supplied through pipes to homes, where it is used for many purposes including ranges and ovens, gas-heated clothes dryers, heating/cooling, and central heating. Heaters in homes and other buildings may include boilers, furnaces, and water heaters.
Compressed natural gas (CNG) is used in rural homes without connections to piped-in public utility services, or with portable grills. Natural gas is also supplied by independent natural gas suppliers through Natural Gas Choice programs throughout the United States. However, as CNG costs more than LPG, LPG (propane) is the dominant source of rural gas.
Transportation.
CNG is a cleaner and also cheaper alternative to other automobile fuels such as gasoline (petrol) and diesel. By the end of 2012 there were 17.25 million natural gas vehicles worldwide, led by Iran (3.3 million), Pakistan (3.1 million), Argentina (2.18 million), Brazil (1.73 million), India (1.5 million), and China (1.5 million). The energy efficiency is generally equal to that of gasoline engines, but lower compared with modern diesel engines. Gasoline/petrol vehicles converted to run on natural gas suffer because of the low compression ratio of their engines, resulting in a cropping of delivered power while running on natural gas (10%–15%). CNG-specific engines, however, use a higher compression ratio due to this fuel's higher octane number of 120–130.
Fertilizers.
Natural gas is a major feedstock for the production of ammonia, via the Haber process, for use in fertilizer production.
Aviation.
Russian aircraft manufacturer Tupolev is currently running a development program to produce LNG- and hydrogen-powered aircraft. The program has been running since the mid-1970s, and seeks to develop LNG and hydrogen variants of the Tu-204 and Tu-334 passenger aircraft, and also the Tu-330 cargo aircraft. It claims that at current market prices, an LNG-powered aircraft would cost 5,000 roubles (~ US$218/ £112) less to operate per ton, roughly equivalent to 60 per cent, with considerable reductions to carbon monoxide, hydrocarbon and nitrogen oxide emissions.
The advantages of liquid methane as a jet engine fuel are that it has more specific energy than the standard kerosene mixes do and that its low temperature can help cool the air which the engine compresses for greater volumetric efficiency, in effect replacing an intercooler. Alternatively, it can be used to lower the temperature of the exhaust.
Hydrogen.
Natural gas can be used to produce hydrogen, with one common method being the hydrogen reformer. Hydrogen has many applications: it is a primary feedstock for the chemical industry, a hydrogenating agent, an important commodity for oil refineries, and the fuel source in hydrogen vehicles.
Other.
Natural gas is also used in the manufacture of fabrics, glass, steel, plastics, paint, and other products.
Storage and transport.
Because of its low density, it is not easy to store natural gas or to transport it by vehicle. Natural gas pipelines are impractical across oceans. Many existing pipelines in America are close to reaching their capacity, prompting some politicians representing northern states to speak of potential shortages. In Western Europe, the gas pipeline network is already dense. New pipelines are planned or under construction in Eastern Europe and between gas fields in Russia, Near East and Northern Africa and Western Europe. See also List of natural gas pipelines.
LNG carriers transport liquefied natural gas (LNG) across oceans, while tank trucks can carry liquefied or compressed natural gas (CNG) over shorter distances. Sea transport using CNG carrier ships that are now under development may be competitive with LNG transport in specific conditions.
Gas is turned into liquid at a liquefaction plant, and is returned to gas form at regasification plant at the terminal. Shipborne regasification equipment is also used. LNG is the preferred form for long distance, high volume transportation of natural gas, whereas pipeline is preferred for transport for distances up to 4000 km over land and approximately half that distance offshore.
CNG is transported at high pressure, typically above 200 bars. Compressors and decompression equipment are less capital intensive and may be economical in smaller unit sizes than liquefaction/regasification plants. Natural gas trucks and carriers may transport natural gas directly to end-users, or to distribution points such as pipelines.
In the past, the natural gas which was recovered in the course of recovering petroleum could not be profitably sold, and was simply burned at the oil field in a process known as flaring. Flaring is now illegal in many countries. Additionally, higher demand in the last 20–30 years has made production of gas associated with oil economically viable. As a further option, the gas is now sometimes re-ed into the formation for enhanced oil recovery by pressure maintenance as well as miscible or immiscible flooding. Conservation, re-injection, or flaring of natural gas associated with oil is primarily dependent on proximity to markets (pipelines), and regulatory restrictions.
A "master gas system" was invented in Saudi Arabia in the late 1970s, ending any necessity for flaring. Satellite observation, however, shows that flaring and venting are still practiced in some gas-extracting countries.
Natural gas is used to generate electricity and heat for desalination. Similarly, some landfills that also discharge methane gases have been set up to capture the methane and generate electricity.
Natural gas is often stored underground inside depleted gas reservoirs from previous gas wells, salt domes, or in tanks as liquefied natural gas. The gas is injected in a time of low demand and extracted when demand picks up. Storage nearby end users helps to meet volatile demands, but such storage may not always be practicable.
With 15 countries accounting for 84 per cent of the worldwide extraction, access to natural gas has become an important issue in international politics, and countries vie for control of pipelines. In the first decade of the 21st century, Gazprom, the state-owned energy company in Russia, engaged in disputes with Ukraine and Belarus over the price of natural gas, which have created concerns that gas deliveries to parts of Europe could be cut off for political reasons. The United States is preparing to export natural gas.
Floating Liquefied Natural Gas (FLNG) is an innovative technology designed to enable the development of offshore gas resources that would otherwise remain untapped because due to environmental or economic factors it is nonviable to develop them via a land-based LNG operation. FLNG technology also provides a number of environmental and economic advantages:
Many gas and oil companies are considering the economic and environmental benefits of Floating Liquefied Natural Gas (FLNG). However, for the time being, the only FLNG facility now in development is being built by Shell, due for completion around 2017.
Environmental effects.
Effect of natural gas release.
Natural gas is mainly composed of methane. After release to the atmosphere it is removed by gradual oxidation to carbon dioxide and water by hydroxyl radicals (·OH) formed in the troposphere or stratosphere, giving the overall chemical reaction CH4 + 2O2→ CO2 + 2H2O. While the lifetime of atmospheric methane is relatively short when compared to carbon dioxide, with a half-life of about 7 years, it is more efficient at trapping heat in the atmosphere, so that a given quantity of methane has 84 times the global-warming potential of carbon dioxide over a 20-year period and 28 times over a 100-year period. Natural gas is thus a more potent greenhouse gas than carbon dioxide due to the greater global-warming potential of methane. Current estimates by the EPA place global emissions of methane at 85 e9m3 annually, or 3.2 per cent of global production. Direct emissions of methane represented 14.3 per cent of all global anthropogenic greenhouse gas emissions in 2004.
During extraction, storage, transportation, and distribution, natural gas is known to leak into the atmosphere, particularly during the extraction process. A Cornell University study in 2011 demonstrated that the leak rate of methane may be high enough to jeopardize its global warming advantage over coal.
This study was criticized later for its high assumption of methane leakage values. These values were later shown to be close to the findings of the Scientists at the National Oceanic and Atmospheric Administration.
Natural gas extraction also releases an isotope of Radon, ranging from 5 to 200,000 Becquerels per cubic meter.
CO2 emissions.
Natural gas is often described as the cleanest fossil fuel. It produces about 29% and 44% less carbon dioxide per joule delivered than oil and coal respectively, and potentially fewer pollutants than other hydrocarbon fuels. However, in absolute terms, it comprises a substantial percentage of human carbon emissions, and this contribution is projected to grow. According to the IPCC Fourth Assessment Report, in 2004, natural gas produced about 5.3 billion tons a year of CO2 emissions, while coal and oil produced 10.6 and 10.2 billion tons respectively. According to an updated version of the Special Report on Emissions Scenario by 2030, natural gas would be the source of 11 billion tons a year, with coal and oil now 8.4 and 17.2 billion respectively because demand is increasing 1.9 percent a year.
Other pollutants.
Natural gas produces far lower amounts of sulfur dioxide and nitrous oxides than any other hydrocarbon fuels.
The other pollutants due to natural gas combustion are listed below in parts per million (ppm):
Safety concerns.
Production.
In mines, where methane seeping from rock formations has no odor, sensors are used, and mining apparatus such as the Davy lamp has been specifically developed to avoid ignition sources.
Some gas fields yield sour gas containing hydrogen sulfide (H2S). This untreated gas is toxic. Amine gas treating, an industrial scale process which removes acidic gaseous components, is often used to remove hydrogen sulfide from natural gas.
Extraction of natural gas (or oil) leads to decrease in pressure in the reservoir. Such decrease in pressure in turn may result in subsidence, sinking of the ground above. Subsidence may affect ecosystems, waterways, sewer and water supply systems, foundations, and so on.
Another ecosystem effect results from the noise of the process. This can change the composition of animal life in the area, and have consequences for plants as well in that animals disperse seeds and pollen.
Releasing the gas from low-permeability reservoirs is accomplished by a process called hydraulic fracturing or "hydrofracking". To allow the natural gas to flow out of the shale, oil operators force 1 to 9 e6USgal of water mixed with a variety of chemicals through the wellbore casing into the shale. The high pressure water breaks up or "fracks" the shale, which releases the trapped gas. Sand is added to the water as a proppant to keep the fractures in the shale open, thus enabling the gas to flow into the casing and then to the surface. The chemicals are added to the frack fluid to reduce friction and combat corrosion. During the extracting life of a gas well, other low concentrations of other chemical substances may be used, such as biocides to eliminate fouling, scale and corrosion inhibitors, oxygen scavengers to remove a source of corrosion, and acids to clean the perforations in the pipe.
Dealing with fracking fluid can be a challenge. Along with the gas, 30 percent to 70 percent of the chemically laced frack fluid, or flow back, returns to the surface. Additionally, a significant amount of brine, containing salts and other minerals, may be produced with the gas.
Use.
In order to assist in detecting leaks, a minute amount of odorant is added to the otherwise colorless and almost odorless gas used by consumers. The odor has been compared to the smell of rotten eggs, due to the added tert-Butylthiol (t-butyl mercaptan). Sometimes a related compound, thiophane, may be used in the mixture. Situations in which an odorant that is added to natural gas can be detected by analytical instrumentation, but cannot be properly detected by an observer with a normal sense of smell, have occurred in the natural gas industry. This is caused by odor masking, when one odorant overpowers the sensation of another. As of 2011, the industry is conducting research on the causes of odor masking.
Explosions caused by natural gas leaks occur a few times each year. Individual homes, small businesses and other structures are most frequently affected when an internal leak builds up gas inside the structure. Frequently, the blast is powerful enough to significantly damage a building but leave it standing. In these cases, the people inside tend to have minor to moderate injuries. Occasionally, the gas can collect in high enough quantities to cause a deadly explosion, disintegrating one or more buildings in the process. The gas usually dissipates readily outdoors, but can sometimes collect in dangerous quantities if flow rates are high enough. However, considering the tens of millions of structures that use the fuel, the individual risk of using natural gas is very low.
Natural gas heating systems are a minor source of carbon monoxide deaths in the United States. According to the US Consumer Product Safety Commission (2008), 56 per cent of unintentional deaths from non-fire CO poisoning were associated with engine-driven tools like gas-powered generators and lawnmowers. Natural gas heating systems accounted for 4 per cent of these deaths. Improvements in natural gas furnace designs have greatly reduced CO poisoning concerns. Detectors are also available that warn of carbon monoxide and/or explosive gas (methane, propane, etc.).
Energy content, statistics, and pricing.
Quantities of natural gas are measured in normal cubic meters (corresponding to 0 °C at 101.325 kPa) or in standard cubic feet (corresponding to 60 °F and 14.73 psia). The gross heat of combustion of 1 m3 of commercial quality natural gas is around 39 MJ (≈10.8 kWh), but this can vary by several percent. This comes to about 49 MJ (≈13.5 kWh) for 1 kg of natural gas (assuming a density of 0.8 kg m−3, an approximate value).
The price of natural gas varies greatly depending on location and type of consumer. In 2007, a price of $7 per 1000 cubic feet (about 25 cents per m3) was typical in the United States. The typical caloric value of natural gas is roughly 1,000 British thermal units (BTU) per cubic foot, depending on gas composition. This corresponds to around $7 per million BTU, or around $7 per gigajoule. In April 2008, the wholesale price was $10 per 1000 cuft ($10/MMBTU). The residential price varies from 50% to 300% more than the wholesale price. At the end of 2007, this was $12–$16 per 1000 cubic feet (about 50 cents per m3). Natural gas in the United States is traded as a futures contract on the New York Mercantile Exchange. Each contract is for 10,000 MMBTU (~10,550 gigajoules), or 10 billion BTU. Thus, if the price of gas is $10 per million BTUs on the NYMEX, the contract is worth $100,000.
European Union.
Gas prices for end users vary greatly across the EU. A single European energy market, one of the key objectives of the European Union, should level the prices of gas in all EU member states. Moreover, it would help to resolve supply and global warming issues.
United States.
In US units, one standard cubic foot 1 cuft of natural gas produces around 1028 btu. The actual heating value when the water formed does not condense is the net heat of combustion and can be as much as 10% less.
In the United States, retail sales are often in units of therms (th); 1 therm = 100,000 BTU. Gas meters measure the volume of gas used, and this is converted to therms by multiplying the volume by the energy content of the gas used during that period, which varies slightly over time. Wholesale transactions are generally done in decatherms (Dth), or in thousand decatherms (MDth), or in million decatherms (MMDth). A million decatherms is roughly a billion cubic feet of natural gas. Gas sales to domestic consumers may be in units of 100 standard cubic feet (scf). The typical annual consumption of a single family residence is 1,000 therms or one RCE.
Canada.
Canada uses metric measure for internal trade of petrochemical products. Consequently, natural gas is sold by the Gigajoule, cubic metre (m3) or thousand cubic metres (E3m3). Distribution infrastructure and meters almost always meter volume (cubic foot or cubic meter). Some jurisdictions, such as Saskatchewan, sell gas by volume only. Other jurisdictions, such as Alberta, gas is sold by the energy content (GJ). In these areas, almost all meters for residential and small commercial customers measure volume (m3 or ft3), and billing statements include a multiplier to convert the volume to energy content of the local gas supply.
A gigajoule (GJ) is a measure approximately equal to half a barrel (250 lbs) of oil, or 1 million BTUs, or 1000 cu ft of gas, or 28 m3 of gas. The energy content of gas supply in Canada can vary from 37 to 43 MJ per m3 depending on gas supply and processing between the wellhead and the customer.
Elsewhere.
In the rest of the world, natural gas is sold in Gigajoule retail units. LNG (liquefied natural gas) and LPG (liquefied petroleum gas) are traded in metric tons or MMBTU as spot deliveries. Long term natural gas distribution contracts are signed in cubic metres, and LNG contracts are in metric tonnes (1,000 kg). The LNG and LPG is transported by specialized transport ships, as the gas is liquified at cryogenic temperatures. The specification of each LNG/LPG cargo will usually contain the energy content, but this information is in general not available to the public.
In the Russian Federation, Gazprom sold approximately 250 billion cubic metres of natural gas in 2008. In 2013 the Group produced 487.4 billion cubic meters of natural and associated gas. Gazprom supplied Europe with 161.5 billion cubic meters of gas in 2013.
Natural gas as an asset class for institutional investors.
Research conducted by the suggests that large US and Canadian pension funds and Asian and MENA area SWF investors have become particularly active in the fields of natural gas and natural gas infrastructure, a trend started in 2005 by the formation of Scotia Gas Networks in the UK by OMERS and Ontario Teachers' Pension Plan.
Adsorbed natural gas (ANG).
Another way to store natural gas is adsorbing it to the porous solids called sorbents. The best condition for methane storage is at room temperature and atmospheric pressure. The used pressure can be up to 4 MPa (about 40 times atmospheric pressure) for having more storage capacity. The most common sorbent used for ANG is activated carbon (AC). Three main types of activated carbons for ANG are: Activated Carbon Fiber (ACF), Powdered Activated Carbon (PAC), activated carbon monolith.
See also.
General:

</doc>
<doc id="22133" url="http://en.wikipedia.org/wiki?curid=22133" title="Nuclear chain reaction">
Nuclear chain reaction

A nuclear chain reaction occurs when one single nuclear reaction causes an average of one or more subsequent nuclear reactions, thus leading to the possibility of a self-propagating series of these reactions. The specific nuclear reaction may be the fission of heavy isotopes (e.g. 235U). The nuclear chain reaction releases several million times more energy per reaction than any chemical reaction.
History.
Chemical chain reactions were first proposed by German chemist Max Bodenstein in 1913, and were reasonably well understood before nuclear chain reactions were proposed. It was understood that chemical chain reactions were responsible for exponentially increasing rates in reactions, such as produced chemical explosions.
The concept of a nuclear chain reaction was first hypothesized by Hungarian scientist Leó Szilárd on Tuesday, September 12, 1933. The neutron had been discovered in 1932, shortly before. Szilárd realized that if a nuclear reaction produced neutrons, which then caused further nuclear reactions, the process might be self-perpetuating. Szilárd, however, did not propose fission as the mechanism for his chain reaction, since the fission reaction was not yet discovered or even suspected. Instead, Szilárd proposed using mixtures of lighter known isotopes which produced neutrons in copious amounts. He filed a patent for his idea of a simple nuclear reactor the following year.
In 1936, Szilárd attempted to create a chain reaction using beryllium and indium, but was unsuccessful. After nuclear fission was discovered and proved by Otto Hahn and Fritz Strassmann in December 1938, Szilárd and Enrico Fermi in 1939 searched for, and discovered, neutron multiplication in uranium, proving that a nuclear chain reaction by this mechanism was indeed possible. This discovery prompted the letter from Szilárd and signed by Albert Einstein to President Franklin D. Roosevelt warning of the possibility that Nazi Germany might be attempting to build an atomic bomb.
Enrico Fermi and Leo Szilárd created the first artificial self-sustaining nuclear chain reaction, called Chicago Pile-1 (CP-1), in a racquets court below the bleachers of Stagg Field at the University of Chicago on December 2, 1942. Fermi's experiments at the University of Chicago were part of Arthur H. Compton's Metallurgical Laboratory, part of the Manhattan Project; the lab was later moved outside Chicago, renamed Argonne National Laboratory, and tasked with conducting research in harnessing fission for nuclear energy.
In 1956, Paul Kuroda of the University of Arkansas postulated that a natural fission reactor may have once existed. Since nuclear chain reactions only require natural materials (such as water and uranium), it is possible to have these chain reactions occur where there is the right combination of materials within the Earth's crust. Kuroda's prediction was verified with the discovery of evidence of natural self-sustaining nuclear chain reactions in the past at Oklo in Gabon, Africa in September 1972.
Fission chain reaction.
Fission chain reactions occur because of interactions between neutrons and fissile isotopes (such as 235U). The chain reaction requires both the release of neutrons from fissile isotopes undergoing nuclear fission and the subsequent absorption of some of these neutrons in fissile isotopes. When an atom undergoes nuclear fission, a few neutrons (the exact number depends on several factors) are ejected from the reaction. These free neutrons will then interact with the surrounding medium, and if more fissile fuel is present, some may be absorbed and cause more fissions. Thus, the cycle repeats to give a reaction that is self-sustaining.
Nuclear power plants operate by precisely controlling the rate at which nuclear reactions occur, and that control is maintained through the use of several redundant layers of safety measures. Moreover, the materials in a nuclear reactor core and the uranium enrichment level make a nuclear explosion impossible, even if all safety measures failed. On the other hand, nuclear weapons are specifically engineered to produce a reaction that is so fast and intense it cannot be controlled after it has started. When properly designed, this uncontrolled reaction can lead to an explosive energy release.
Nuclear fission fuel.
Nuclear weapons employ high quality, highly enriched fuel exceeding the critical size and geometry (critical mass) necessary in order to obtain an explosive chain reaction. The fuel for energy purposes, such as in a nuclear fission reactor, is very different, usually consisting of a low-enriched oxide material (e.g. UO2).
Fission reaction products.
When a heavy atom undergoes nuclear fission it breaks into two or more fission fragments. Also, several free neutrons, gamma rays, and neutrinos are emitted, and a large amount of energy is released. The sum of the rest masses of the fission fragments and ejected neutrons is less than the sum of the rest masses of the original atom and incident neutron (of course the fission fragments are not at rest). The mass difference is accounted for in the release of energy according to the equation E=Δmc²:
mass of released energy = formula_1
Due to the extremely large value of the speed of light, c, a small decrease in mass is associated with a tremendous release of active energy (for example, the kinetic energy of the fission fragments). This energy (in the form of radiation and heat) carries the missing mass, when it leaves the reaction system (total mass, like total energy, is always conserved). While typical chemical reactions release energies on the order of a few eVs (e.g. the binding energy of the electron to hydrogen is 13.6 eV), nuclear fission reactions typically release energies on the order of hundreds of millions of eVs.
Two typical fission reactions are shown below with average values of energy released and number of neutrons ejected:
Note that these equations are for fissions caused by slow-moving (thermal) neutrons. The average energy released and number of neutrons ejected is a function of the incident neutron speed. Also, note that these equations exclude energy from neutrinos since these subatomic particles are extremely non-reactive and, therefore, rarely deposit their energy in the system.
Timescales of nuclear chain reactions.
Prompt neutron lifetime.
The prompt neutron lifetime, "l", is the average time between the emission of neutrons and either their absorption in the system or their escape from the system. The term lifetime is used because the emission of a neutron is often considered its "birth," and the subsequent absorption is considered its "death." For thermal (slow-neutron) fission reactors, the typical prompt neutron lifetime is on the order of 10−4 seconds, and for fast fission reactors, the prompt neutron lifetime is on the order of 10−7 seconds. These extremely short lifetimes mean that in 1 second, 10,000 to 10,000,000 neutron lifetimes can pass. The "average" (also referred to as the "adjoint unweighted") prompt neutron lifetime takes into account all prompt neutrons regardless of their importance in the reactor core; the "effective" prompt neutron lifetime (referred to as the "adjoint weighted" over space, energy, and angle) refers to a neutron with average importance.
Mean generation time.
The mean generation time, Λ, is the average time from a neutron emission to a capture that results in fission. The mean generation time is different from the prompt neutron lifetime because the mean generation time only includes neutron absorptions that lead to fission reactions (not other absorption reactions). The two times are related by the following formula:
In this formula, k is the effective neutron multiplication factor, described below.
Effective neutron multiplication factor.
The effective neutron multiplication factor, "k", is the average number of neutrons from one fission that cause another fission. The remaining neutrons either are absorbed in non-fission reactions or leave the system without being absorbed. The value of "k" determines how a nuclear chain reaction proceeds:
When describing kinetics and dynamics of nuclear reactors, and also in the practice of reactor operation, the concept of reactivity is used, which characterizes the deflection of reactor from the critical state. ρ=(k-1)/k. InHour is a unit of reactivity of a nuclear reactor.
In a nuclear reactor, "k" will actually oscillate from slightly less than 1 to slightly more than 1, due primarily to thermal effects (as more power is produced, the fuel rods warm and thus expand, lowering their capture ratio, and thus driving "k" lower). This leaves the average value of "k" at exactly 1. Delayed neutrons play an important role in the timing of these oscillations.
In an infinite medium, the multiplication factor may be described by the four factor formula; in a non-infinite medium, the multiplication factor may be described by the six factor formula.
Prompt and delayed supercriticality.
Not all neutrons are emitted as a direct product of fission; some are instead due to the radioactive decay of some of the fission fragments. The neutrons that occur directly from fission are called "prompt neutrons," and the ones that are a result of radioactive decay of fission fragments are called "delayed neutrons." The fraction of neutrons that are delayed is called β, and this fraction is typically less than 1% of all the neutrons in the chain reaction.
The delayed neutrons allow a nuclear reactor to respond several orders of magnitude more slowly than just prompt neutrons would alone. Without delayed neutrons, changes in reaction rates in nuclear reactors would occur at speeds that are too fast for humans to control.
The region of supercriticality between k = 1 and k = 1/(1-β) is known as delayed supercriticality (or delayed criticality). It is in this region that all nuclear power reactors operate. The region of supercriticality for k > 1/(1-β) is known as prompt supercriticality (or prompt criticality), which is the region in which nuclear weapons operate.
The change in k needed to go from critical to prompt critical is defined as a dollar.
Nuclear weapons application of neutron multiplication.
Nuclear fission weapons require a mass of fissile fuel that is prompt supercritical.
For a given mass of fissile material the value of k can be increased by increasing the density. Since the probability per distance traveled for a neutron to collide with a nucleus is proportional to the material density, increasing the density of a fissile material can increase k. This concept is utilized in the implosion method for nuclear weapons. In these devices, the nuclear chain reaction begins after increasing the density of the fissile material with a conventional explosive.
In the gun-type fission weapon two subcritical pieces of fuel are rapidly brought together. The value of k for a combination of two masses is always greater than that of its components. The magnitude of the difference depends on distance, as well as the physical orientation.
The value of k can also be increased by using a neutron reflector surrounding the fissile material
Once the mass of fuel is prompt supercritical, the power increases exponentially. However, the exponential power increase cannot continue for long since k decreases when the amount of fission material that is left decreases (i.e. it is consumed by fissions). Also, the geometry and density are expected to change during detonation since the remaining fission material is torn apart from the explosion.
Predetonation.
Detonation of a nuclear weapon involves bringing fissile material into its optimal supercritical state very rapidly. During part of this process, the assembly is supercritical, but not yet in an optimal state for a chain reaction. Free neutrons, in particular from spontaneous fissions, can cause the device to undergo a preliminary chain reaction that destroys the fissile material before it is ready to produce a large explosion, which is known as predetonation. To keep the probability of predetonation low, the duration of the non-optimal assembly period is minimized and fissile and other materials are used which have low spontaneous fission rates. In fact, the combination of materials has to be such that it is unlikely that there is even a single spontaneous fission during the period of supercritical assembly. In particular, the gun method cannot be used with plutonium (see nuclear weapon design).
Nuclear power plants and control of chain reactions.
Chain reactions naturally give rise to reaction rates that grow (or shrink) exponentially, whereas a nuclear power reactor needs to be able to hold the reaction rate reasonably constant. To maintain this control, the chain reaction criticality must have a slow enough time-scale to permit intervention by additional effects (e.g., mechanical control rods or thermal expansion). Consequently, all nuclear power reactors (even fast-neutron reactors) rely on delayed neutrons for their criticality. An operating nuclear power reactor fluctuates between being slightly subcritical and slightly delayed-supercritical, but must always remain below prompt-critical.
It is impossible for a nuclear power plant to undergo a nuclear chain reaction that results in an explosion of power comparable with a nuclear weapon, but even low-powered explosions due to uncontrolled chain reactions, that would be considered "fizzles" in a bomb, may still cause considerable damage and meltdown in a reactor. For example, the Chernobyl disaster involved a runaway chain reaction but the result was a low-powered steam explosion from the relatively small release of heat, as compared with a bomb. However, the reactor complex was destroyed by the heat, as well as by ordinary burning of the graphite exposed to air. Such steam explosions would be typical of the very diffuse assembly of materials in a nuclear reactor, even under the worst conditions.
In addition, other steps can be taken for safety. For example, power plants licensed in the United States require a negative void coefficient of reactivity (this means that if water is removed from the reactor core, the nuclear reaction will tend to shut down, not increase). This eliminates the possibility of the type of accident that occurred at Chernobyl (which was due to a positive void coefficient). However, nuclear reactors are still capable of causing smaller explosions even after complete shutdown, such as was the case of the Fukushima Daiichi nuclear disaster. In such cases, residual decay heat from the core may cause high temperatures if there is loss of coolant flow, even a day after the chain reaction has been shut down (see SCRAM). This may cause a chemical reaction between water and fuel that produces hydrogen gas which can explode after mixing with air, with severe contamination consequences, since fuel rod material may still be exposed to the atmosphere from this process. However, such explosions do not happen during a chain reaction, but rather as a result of energy from radioactive beta decay, after the fission chain reaction has been stopped.

</doc>
<doc id="22135" url="http://en.wikipedia.org/wiki?curid=22135" title="Nichiren">
Nichiren

Nichiren (日蓮; April 6, 1222 – November 21, 1282) was a Buddhist monk who lived during the Kamakura period (1185–1333) in Japan. Nichiren taught devotion to the "Lotus Sutra", which claims to contain Gautama Buddha's teachings towards the end of his life, as the exclusive means to attain enlightenment.
Nichiren believed that this sutra contained the essence of all of Gautama Buddha's teachings related to the laws of causality, karma, and leading all people without distinction to enlightenment. This devotion to the sutra entails the chanting of "Namu Myōhō Renge Kyō" "homage to the "Lotus Sutra"", a phrase referred to as the "daimoku", as the essential practice of the teaching.
Nichiren Buddhism includes various schools such as Nichiren Shōshū, the Nichiren Shū confederation of schools, and lay movements such as Risshō Kōsei Kai or Soka Gakkai, each claiming to be the only true follower of their founder, with their own interpretations of Nichiren's teachings. However, despite the differences between schools, all Nichiren sects share the fundamental practice of chanting daimoku. While all Nichiren Buddhist schools regard him as a reincarnation of the "Lotus Sutra"'s Visistacaritra or "Jōgyō Bosatsu" (上行菩薩), some schools of Nichiren Buddhism's Nikkō lineages regard him as the actual Buddha of this age, or the Buddha of the Latter day of the Law and for all eternity.
In the 20th century, Nichirenism was a fascist movement led by Nichiren Buddhists who attempted a wave of assassinations in an attempt to further ultranationalist goals in 1932. The result is known as the League of Blood Incident. Two months after, in the May 15 Incident, Japanese naval officers, including some associated with the League of Blood, assassinated Prime Minister Inukai Tsuyoshi.
Life.
Birth.
Nichiren was born on February 16, 1222 in the village of Kominato (today part of the city of Kamogawa), Nagase District, Awa Province (within present-day Chiba Prefecture). Nichiren's father, a fisherman, was Mikuni-no-Tayu Shigetada, also known as Nukina Shigetada Jiro (d. 1258) and his mother was Umegiku-nyo (d. 1267). On his birth, his parents named him "Zennichimaro" (善日麿) which has variously been translated into English as "Splendid Sun" and "Virtuous Sun Boy" among others. The exact site of Nichiren's birth is believed to be submerged off the shore from present-day Kominato-zan Tanjō-ji (小湊山　誕生寺), a temple in Kominato that commemorates Nichiren's birth.
In his own words, Nichiren stated that he was "the son of a chandala family who lived near the sea in Tojo in Awa Province, in the remote countryside of the eastern part of Japan."
Education.
Nichiren began his Buddhist study at a nearby temple of the Tendai school, Seichō-ji (清澄寺, also called Kiyosumi-dera), at age 11. He was formally ordained at 16 and took the Buddhist name Zeshō-bō Renchō (Rencho meaning Lotus Growth). He left Seichō-ji shortly thereafter to study in Kamakura and several years later traveled to western Japan for more in-depth study in the Kyoto–Nara area, where Japan's major centers of Buddhist learning were located. In 1233 he went to Kamakura, where he studied Amidism, a pietistic school that stressed salvation through the invocation of Amitābha (Japanese "Amida"), the Buddha of infinite compassion, under the guidance of a renowned master. After having persuaded himself that Amidism was not the true Buddhist doctrine, he passed to the study of Zen, which had become popular in Kamakura and Kyōto. He then went to Mount Hiei, the cradle of Tendai, where he felt the original purity of the Tendai doctrine corrupted by the introduction and acceptance of other doctrines, especially Amidism and esoteric Buddhism. To eliminate any possible doubts, Nichiren decided to spend some time at Mount Kōya, the centre of Shingon Buddhism, and also in Nara, Japan's ancient capital, where he studied the Risshū, which emphasized strict adherence to the Vinaya, the code of monastic discipline and ordination. During this time, he became convinced of the pre-eminence of the "Lotus Sutra" and in 1253, returned to Seichō-ji.
Initial teaching.
On April 28, 1253, he expounded the "daimoku" teachings for the first time, marking his "Sho Tempōrin" (初転法輪: "first turning the wheel of the Law"). With this, he proclaimed that devotion and practice based on the "Lotus Sutra" was the correct form of Buddhism for the current time. At the same time he changed his name to Nichiren, "nichi" (日) meaning "sun" and "ren" (蓮) meaning "lotus". This choice, as Nichiren himself explained, was rooted in passages from the "Lotus Sutra".
After making his declaration, which all schools of Nichiren Buddhism regard as marking their foundation (立宗: "risshū"), Nichiren began propagating his teachings in Kamakura, then Japan's de facto capital since it was where the shikken or regent for the shogun and the shogun himelf lived and the government was established. He gained a fairly large following there, consisting of both priests and laity. Many of his lay believers came from among the samurai class.
It is claimed that in 1253 Nichiren predicted the Mongol invasions of Japan: a prediction which was validated in 1274. Nichiren viewed his teachings as a method of efficaciously preventing this and other disasters: that the best countermeasure against the degeneracy of the times and its associated disasters was through the activation of Buddha-nature by chanting and the other practices which he advocated.
Treatise (first remonstrance).
Nichiren then engaged in writing, publishing various works including his "Risshō Ankoku Ron" (立正安国論): "Treatise On Establishing the Correct Teaching for the Peace of the Land", his first major treatise and the first of three remonstrations with government authorities. He felt that it was imperative for "the sovereign to recognize and accept the singly true and correct form of Buddhism" (i.e., 立正: "risshō") as the only way to "achieve peace and prosperity for the land and its people and end their suffering" (i.e., 安国: "ankoku"). This "true and correct form of Buddhism", as Nichiren saw it, entailed regarding the Lotus Sutra as the fullest expression of the Buddha's teachings and putting those teachings into practice. Nichiren thought this could be achieved in Japan by withdrawing lay support so that the deviant monks would be forced to change their ways or revert to laymen to prevent starving.
Based on prophecies made in several sutras, Nichiren attributed the occurrence of the famines, disease, and natural disasters (especially drought, typhoons, and earthquakes) of his day to teachings of Buddhism no longer appropriate for the time.
Nichiren submitted his treatise in July 1260. Though it drew no official response, it prompted a severe backlash, especially from among priests of other Buddhist schools. Nichiren was harassed frequently, several times with force, and often had to change dwellings.
Nichiren was exiled to the Izu Peninsula in 1261, and pardoned in 1263. He was ambushed and nearly killed at Komatsubara in Awa Province in November 1264 by forces led by Lord Tōjō Kagenobu.
Failed execution attempt.
The following several years were marked by successful propagation activities in eastern Japan that generated more resentment among rival priests and government authorities. After one exchange with the influential priest, Ryōkan (良観), Nichiren was summoned for questioning by the authorities in September 1271. He used this as an opportunity to make his second government remonstration, this time to Hei no Saemon (平の左衛門, also called 平頼綱: Taira no Yoritsuna), a powerful police and military figure who issued the summons.
Two days later, on September 12, Hei no Saemon and a group of soldiers abducted Nichiren from his hut at Matsubagayatsu, Kamakura. Their intent was to arrest and behead him. According to Nichiren's account, an astronomical phenomenon — "a brilliant orb as bright as the moon" — over the seaside Tatsunokuchi execution grounds terrified Nichiren's executioners into inaction. The incident is known as the Tatsunokuchi Persecution and regarded as a turning point in Nichiren's lifetime called "Hosshaku kenpon" (発迹顕本), translated as "casting off the transient and revealing the true," or "Outgrowing the provisional and revealing the essential".
Second exile.
Unsure of what to do with Nichiren, Hei no Saemon decided to banish him to Sado, an island in the Sea of Japan known for its particularly severe winters and a place of harsh exile.
This exile, Nichiren's second, lasted about three years and, though harsh and in the long term detrimental to his health, represents one of the most important and productive segments of his life. While on Sado, he won many devoted converts and wrote two of his most important doctrinal treatises, the "Kaimoku Shō" (開目抄: "On the Opening of the Eyes") and the "Kanjin no Honzon Shō" (観心本尊抄: "The Object of Devotion for Observing the Mind") as well as numerous letters and minor treatises whose content containing critical components of his teaching.
Gohonzon.
It was also during his exile on Sado, in 1272, that he inscribed the first "Gohonzon" (御本尊). This mandala is a visual representation, in Chinese characters, of the Ceremony in the Air. This ceremony is described in the 11th (Treasure Tower) to 22nd (Entrustment) chapters of the Lotus Sutra. Within these chapters it is revealed that all persons can attain Buddhahood in this lifetime and Shakyamuni transfers the essence of the sutra to the Bodhisattvas of the Earth led by Bodhisattva Superior Practices (Jogyo), entrusting them with the propagation of the essence of the sutra in the Latter Day of the Law. For Nichiren, the Gohonzon embodies the eternal and intrinsic Law of Nam-Myoho-Renge-Kyo, which he identified as the ultimate Law permeating life and the universe.
Return to Kamakura.
Nichiren was pardoned in February 1274 and returned to Kamakura in late March. He was again interviewed by Hei no Saemon, who now was interested in Nichiren's prediction of an invasion by the Mongols. Mongol messengers demanding Japan's fealty had frightened the authorities into believing that Nichiren's prophecy of foreign invasion would materialize (which it later did in October of that year; see Mongol Invasions of Japan). Nichiren, however, used the audience as yet another opportunity to remonstrate with the government.
Retirement to Mount Minobu.
His third remonstration also went unheeded, and Nichiren—following a Chinese adage that if a wise man remonstrates three times but is ignored, he should leave the country—decided to go into voluntary exile at Mount Minobu (身延山) in 1274.
With the exception of a few short journeys, Nichiren spent the rest of his life at Minobu, where he and his disciples erected a temple, Kuon-ji (久遠寺), and he continued writing and training his disciples. Two of his works from this period are the "Senji Shō" (撰時抄: "The Selection of the Time") and the "Hōon Shō" (報恩抄: "On Repaying Debts of Gratitude"), which, along with his "Risshō Ankoku Ron" (立正安国論: "On Establishing the Correct Teaching for the Peace of the Land"), "Kaimoku Shō" ("The Opening of the Eyes"), and "Kanjin no Honzon Shō" ("The Object of Devotion for Observing the Mind"), constitute his Five Major Writings. He also inscribed numerous Gohonzon for bestowal upon specific disciples and lay believers. Many of these survive today in the repositories of Nichiren temples such as Taiseki-ji (大石寺) in Fujinomiya, Shizuoka Prefecture, which has a particularly large collection that is publicly aired once a year in April.
Death.
Nichiren spent his final years writing, inscribing Gohonzon for his disciples and believers, and delivering sermons. In failing health, he was encouraged to travel to hot springs for their medicinal benefits. He left Minobu in the company of several disciples on September 8, 1282.
He arrived ten days later at the residence of Ikegami Munenaka, a lay believer who lived in what is now Ikegami, the site is marked by Ikegami Honmon-ji. On September 25 he delivered his last sermon on the "Risshō Ankoku Ron", and on October 8 he appointed six senior disciples—Nisshō (日昭), Nichirō (日朗), Nikkō (日興), Nikō (日向), Nichiji (日持), and Nitchō (日頂)—to continue leading propagation of his teachings after his death. Nichiren Shoshu believe that Nichiren designated five senior priests, and one successor, Nikko.
On October 13, 1282, Nichiren died in the presence of many disciples and lay believers. His funeral and cremation took place the following day. His disciple Nikkō left Ikegami with Nichiren's ashes on October 21, reaching Minobu on October 25. Nichiren's original tomb is sited, as per his request, at Kuonji on Mt. Minobu.
Development of Nichiren's teachings.
The Kamakura period of 13th century Japan, in which Nichiren was born - was characterised by natural disasters, internal strife and confusion within Mahayana schools about whether: "...the world had further entered a period of decline" referring to the Latter Day of the Law. Nichiren attributed the turmoil in society to the invalid teachings of the Buddhist schools of his time, including the Tendai sect in which he was ordained: "It is better to be a leper who chants Nam-myōhō-renge-kyō than be a chief abbot of the Tendai school". Examinations of such breaks and continuities have been useful in illuminating the sources of Nichiren's ideas and to what extent Nichiren's thought is original or derivative of his parent tradition. Setting out to declare his own teachings of Buddhism, Nichiren started at the age of 32 by denouncing all Mahayana schools of his time and by declaring the correct teaching as the Universal Dharma (Namu-Myōhō-Renge-Kyō) and chanting as the only path for personal and social salvation. At the age of 51, Nichiren inscribed the Object of Veneration in Buddhism, the Gohonzon,"never before known" as he described it. Other contributions to Buddhism were the teaching of The Five Guides of Propagation, The doctrine of the Three Great Secret Dharmas and the teaching of The Three Proofs for verification of the validity of Buddhist doctrines. There is a difference between Nichiren teachings and almost all schools of Mahayana Buddhism regarding the understanding of the Latter day of the Law, Mappō. Nichiren, on the other hand, believed that the teachings of the Lotus Sutra will flourish for all eternity, and that the Bodhisattvas of the Earth will propagate Buddhism in the future.
Nichiren criticized other Buddhist schools for their manipulations of the populace for political and religious control. Citing Buddhist sutras and commentaries, Nichiren argued that the Buddhist teachings were being distorted for their own gain. Nichiren stated his criticism clearly, in his "Risshō Ankoku Ron" (立正安国論): "Treatise On Establishing the Correct Teaching for the Peace of the Land", his first major treatise and the first of three remonstrations with government authorities.
After Nichiren's death, his teachings were interpreted in different ways. As a result, Nichiren Buddhism encompasses several major branches and schools, each with its own doctrine and set of interpretations of Nichiren's teachings. See Nichiren Buddhism.
Writings.
Some Nichiren schools refer to the entirety of Nichiren's Buddhism as his "lifetime of teaching". Many of his writings still exist in his original hand, some as complete writings and some as fragments. Others survive as copies made by his immediate disciples. His existing works number over 700, including transcriptions of orally delivered lectures, letters of remonstration and illustrations. Today's Nichiren schools can not agree however, which of his writings can be deemed authentic and which are apocryphal. Nichiren declared that women could attain enlightenment, therefore a great number of letters were addressed to female believers. Some schools within Nichiren Buddhism consider this to be a unique feature of Nichiren's teachings and have published separate volumes of those writings.
In addition to treatises written in "kanbun" (漢文), a formal writing style modeled on classical Chinese that was the language of government and learning in contemporary Japan, Nichiren also wrote expositories and letters to disciples and lay followers in mixed-kanji–kana vernacular as well as letters in simple kana for believers who could not read the more-formal styles, particularly children. He is also known for his "kanbun", many of his writings preserved in the libraries of the empire had been lost at the end of the Boshin War.
Some of Nichiren's "kanbun" works, especially the "Risshō Ankoku Ron", are considered exemplary of the "kanbun" style, while many of his letters show unusual empathy and understanding for the down-trodden of his day. Many of his most famous letters were to women believers, whom he often complimented for their in-depth questions about Buddhism while encouraging them in their efforts to attain enlightenment in this lifetime.
Important writings.
The Five Major Writings that are common to all Nichiren Schools are:
Nichiren Shoshu and Soka Gakkai International (SGI) revere Ten Major Writings of Nichiren. These are the five listed above and:
Posthumous titles and status in major lineages.
In his writings, Nichiren refers to his identity in a variety of ways, nevertheless always related to the Lotus Sutra, for example: "I, Nichiren, am the foremost votary of the Lotus Sutra". Of the many figures appearing in the Lotus Sutra, Nichiren chose his spiritual identity as that of Bodhisattva Superior Practices, and identified his goal as attaining Buddhahood: "From the beginning… I wanted to master Buddhism and attain Buddhahood". In his post Tatsunokuchi's persecution writings, Nichiren referred to his person as parent, teacher and sovereign. While some schools regard this as features attributed to Shakyamuni Buddha others underline that he identifies himself as a votary of the Lotus Sutra:"Shakyamuni Buddha is the father and mother, teacher and sovereign to all living being..." and similarly mentioning in his letter 'The Opening of the Eyes':"I, Nichiren, am sovereign, teacher, and father and mother to all the people...".
After his death, Nichiren has been known by several posthumous names intended to express respect toward him or to represent his position in the history of Buddhism. Most common among these are "Shōnin" 日蓮聖人 Saint or Sage, and "Daishōnin" 日蓮大聖人 "Great Sage".
"Shōnin" is commonly used within Nichiren Shū, who regard Nichiren as a Buddhist reformer and embodiment of Bodhisattva Superior Practices.
"Daishōnin" is the title used by followers of most, but not all, of the schools and temples derived from the Nikkō lineage, most notably Nichiren Shōshū and Sōka Gakkai, who regard Nichiren as 'The Buddha of the Latter Day of the Law'. Shakyamuni is seen as 'The Buddha of True Effect' as he only revealed the 'effect' of Buddhahood. 
The Japanese imperial court also awarded Nichiren the honorific designations "Nichiren Daibosatsu" 日蓮大菩薩 "Great Bodhisattva Nichiren", and "Risshō Daishi" 立正大師 "Great Teacher "Risshō"; the former title was granted in 1358, and the latter in 1922.

</doc>
<doc id="22137" url="http://en.wikipedia.org/wiki?curid=22137" title="Nichiren Buddhism">
Nichiren Buddhism

Nichiren Buddhism (Japanese: 法華系仏教 "Hokke-kei Bukkyo") is a branch of Mahayana Buddhism based on the teachings of the 13th century Japanese monk Nichiren (1222–1282) and belongs to the schools of so-called "Kamakura Buddhism". Nichiren Buddhism is generally noted for its focus on the Lotus Sutra and an attendant belief that all people have an innate Buddha nature and are therefore inherently capable of attaining enlightenment in their current form and present lifetime. It is also noted for its hardline opposition to any other form of Buddhism, which Nichiren saw as deviating from the Buddhist truth he had discovered. Nichiren Buddhism is a comprehensive term covering several major schools and many sub-schools, as well as several of Japan's new religions. Its many denominations have in common a focus on the chanting and recital of the Lotus Sutra, which is thought to hold "extraordinary power".
Founder.
From the age of 16 until 32, Nichiren, originally a monk of Tendai Buddhism, studied in numerous temples in Japan, especially Mt. Hiei (Enryaku-ji) and Mt. Kōya, in his day the major centers of Buddhist study, in the Kyoto–Nara area. He eventually concluded that the highest teachings of Shakyamuni Buddha (563?–483?BC) were to be found in the Lotus Sutra. The mantra he expounded on 28 April 1253, known as the "Daimoku" or "Odaimoku", Nam(u)-Myōhō-Renge-Kyō, expresses his devotion to that body of teachings. During his lifetime, Nichiren stridently maintained that the contemporary teachings of Buddhism taught by other sects, (particularly the Nembutsu, Zen, Shingon, and Ritsu sects) were, to his mind, mistaken in their interpretations of the correct path to enlightenment, and therefore refuted them publicly and vociferously. In doing so, he provoked the ire of the country's rulers and of the priests of the sects he criticized; he was subjected to persecution which included an attempted beheading and at least two exiles.
Some Nichiren schools see the attempted beheading incident as marking a turning point in Nichiren's teaching, since Nichiren began inscribing the Gohonzon and wrote a number of major doctrinal treatises during his subsequent three-year exile on Sado Island in the Japan Sea. After a pardon and his return from exile, Nichiren moved to Mt. Minobu in today's Yamanashi Prefecture, where he and his disciples built a temple, Kuon-ji. Nichiren spent most of the rest of his life here training disciples.
Basic teachings.
Nichiren Buddhism is based on the Lotus Sutra. Common to most lineages of Nichiren Buddhism is the chanting of "Namu Myōhō Renge Kyō" and veneration of the "Gohonzon". The definition of "Gohonzon" varies between the Nichiren schools.
Nichiren Buddhism expounds the doctrine of the Ten Worlds of life, the Ten Factors of existence, the principle of The Three Thousand Realms in a single moment of life and the teachings of The Three Proofs for verification of the validity of teachings. Most of these teachings are shared and identical in most schools and groups of Nichiren Buddhism. However, different interpretations are found for the doctrine of the "Three Great Secret Dharmas", called also "The Three Great Secret Laws", and Three Jewels.
Nichiren's writings.
Nichiren was a prolific writer. His personal communications and writings to his followers as well as numerous treatises detail his view of the correct form of practice for the "Latter Day of the Law" ("mappō"); lay out his views on other Buddhist schools, particularly those of influence during his lifetime; and elucidate his interpretations of Buddhist teachings that preceded his. These writings are collectively known as "Gosho" ("go" is an honorific prefix designating respect) or "Goibun". Which of these writings, including the "Ongi Kuden" (orally transmitted teachings), are deemed authentic or apocryphal is a matter of debate within the various schools of today's Nichiren Buddhism. One of his most important writings the "Rissho Ankoku Ron", preserved at Shochuzan Hokekyo-ji, is one of the National Treasures of Japan.
Development of Nichiren Buddhism and its major lineages.
Nichiren Buddhism is not a single denomination (see following lists). Nichiren was originally an ordained Tendai priest and is not known to have established a separate Buddhist school. Nevertheless, his teachings led to the formation of different schools within several years after his passing. Before his death Nichiren had named "six senior priests" ("rokurōsō") whom he wanted to transmit his teachings to future generations: Nisshō (日昭), Nichirō (日朗), Nikō (日向), Nitchō (日頂), Nichiji (日持), and Nikkō (日興). Each started a lineage of schools, but Nichiji eventually travelled to the Asian continent (ca. 1295) and was never heard from again, and Nitchō later in life (1302) rejoined and became a follower of Nikkō.
Schools and lineages.
Different interpretations of Nichiren's teachings had led to the establishment of various temples and schools, which however have in common reverence to the two basic doctrines of the chanting and the object of devotion. Although the former five disciples remained loosely affiliated to varying degrees, the last—Nikkō—made a clean break by leaving Kuon-ji in 1289. He had come to the conclusion that Nikō and the others were embarking on paths to heresy that he could not stem.
After the passing of Nichiren differences between the various Nichiren schools were relatively minor; nevertheless, the following schools formed around Nichiren's disciples:
In the years following Nichiren's death, his and the temples founded by his disciples remained to a varying degree affiliated. By the 14th century a certain split within the Nichiren Schools occurred though. One differentiates between the so-called Ichi lineage (meaning unity or harmony) and Shoretsu lineage (a contraction of two words meaning superior/inferior).
"The Itchi–Soretsu controversy was of no interest to outsiders, but it kept Nichiren theologians on their toes and forced them to define their positions with more clarity. It did result in the formation of new sub-sects, but these gave impetus to missionary enterprises which expanded Nichiren Buddhism and helped spread it throughout the country." The number of adherents to Nichiren's teachings grew steadily during the 14th and 15th century to the extent that whole communities became followers. By 1400, and only being outnumbered by Zen, Nichiren temples had been founded all over Kyoto and although the various sects of Nichiren Buddhism were administratively independent they met in a council to resolve common problems.
By the 16th century Nichiren Buddhism was no longer on the fringe of religious life and a vast number of Kyoto's inhabitants adhered to Nichiren's teachings. The anarchy resulting from the conflict between the shoguns and the emperor resulted in the attacks by the so-called warrior monks from Mount Hiei. In its aftermath "twenty-one Nichiren temples were destroyed by fire … It was estimated that tens of thousands of Nichiren Buddhists lost their lives"
Some researchers compare early Nichiren Buddhism with early Christianity: "Tamura finds Nichiren’s Buddhism to be broadly comparable with Christianity 'as a religion of prophecy, in its spirit of martyrdom, in its apostolic consciousness, and additionally, in its emphasis upon history'".
Based on the tradition set by Nichiren the relationship between the government, other major Buddhist schools and Nichiren temples remained ambiguous though. The adherents of Nichiren Buddhism who made this aspect of Nichiren teachings a central pillar of their belief were the followers of the so-called Fuju-fuse lineage. Their services were partly held in secret and culminated in the persecution and partly even the execution of its believers in 1668. The majority of official Nichiren temples were "tamed" during the Edo period to the effect that they were subsumed "into a nationwide Buddhist parish system designed to ensure religious peace and eradicate the common enemy, Christianity". In this process, also known as the Danka system, Buddhist temples were generally not only a centre of Buddhist practice and learning, but were forced carry out administrative functions thereby also being controlled by the government taming any missionary activities.
During the Meiji Restoration from 1868 onwards and in an attempt to eradicate Buddhism Nichiren temples were forced, just like any other Buddhist school, to focus on funeral and memorial services as their main activity. Therefore Nichiren-Buddhism remained mainly temple-based. Most Nichiren schools, referring to their establishment, state the founding of their respective head or main temple, for example, Nichiren Shū the year 1281, Nichiren Shōshū the year 1288 and Kempon Hokke Shu the year 1384. However, most of today's Nichren schools did not form until the late 19th and early 20th century as, also legal, religious bodies. A last wave of merges took place in the 1950s. Following the above-mentioned divide between the Ichi lineage and Shoretsu lineage, the most notable division is the one between Nichiren Shū and Nichiren Shōshū. Documents first mentioned and discovered by Taiseki-ji priest Nikkyo in 1488 claimed that Nichiren passed full authority "to Nikkō alone. The original documents have disappeared, but 'true copies' are preserved at Taiseki-ji. Other Nichiren bodies ignore them as forgeries."
At the time the documents may have served to underline Taiseki-ji's supposed superiority amongst Nikkō temples, especially in respect to Ikegami Honmon-ji the site of Nikkō's tomb. In the later context of developments the above-mentioned claims served as a reason on which, what would later become, Nichiren Shōshū based its orthodoxy on Nichiren-Buddhism in general. Even though there had been efforts by temples of the Nikkō lineage in the late 19th century to unify into one single separate Nichiren school the "Kommon-ha", today's Nichiren Shōshū comprises only the Taiseki-ji temple and its dependent temples. It is not identical to the historical Nikkō or Fuji lineage. Parts of the "Kommon-ha", the "Honmon-Shu", eventually became part of Nichren Shu in the 1950s. New religions like Sōka Gakkai, Shōshinkai, and Kenshōkai trace their origins to the Nichiren Shōshū school, most notably amongst those is Sōka Gakkai which due to its steady growth is regarded today as Japan's largest lay Buddhist organization.
Kuon-ji eventually became the head temple of today's Nichiren Shū, today the largest branch amongst traditional schools, encompassing the schools and temples tracing their origins to Nikō, Nisshō, Nichirō, Nichiji and also Nikkō. The Reiyūkai, Risshō Kōsei Kai, and Nipponzan-Myōhōji-Daisanga stem, in one form or another, from the Kuon-ji lineage.
The Fuji-lineage.
In contrast to some views the Fuji-lineage is not limited to the Nichiren Shoshu or organisations formally affiliated with it. 
Major Nichiren Buddhist schools and organisations.
The following lists are based on the Japanese Wikipedia article on .
Major Nichiren Buddhist schools and their head temples.
In alphabetical order (Japanese characters preceded by "ja:" link to articles in the Japanese Wikipedia). 
Major Nichiren Buddhist based movements and lay organisations.
In alphabetical order (Japanese characters preceded by "ja:" link to articles in the Japanese Wikipedia): 
Nationalistic interpretations.
Both Nichiren and his followers have been associated with fervent Japanese nationalism known as Nichirenism not least between the Meiji period and the conclusion of World War II.
The nationalistic interpretation of Nichiren's teachings are to be found mainly within lay Buddhist movements like Kenshōkai, most notable in this context however are the May 15 Incident, the League of Blood Incident and Tanaka Chigaku's Kokuchūkai.

</doc>
<doc id="22141" url="http://en.wikipedia.org/wiki?curid=22141" title="Newport News Shipbuilding">
Newport News Shipbuilding

Newport News Shipbuilding (NNS), originally "Newport News Shipbuilding and Drydock Company" (NNS&DD), was the largest privately owned shipyard in the United States prior to being purchased by Northrop Grumman in 2001. Formerly known as "Northrop Grumman Newport News" (NGNN), and later "Northrop Grumman Shipbuilding Newport News" (NGSB-NN), the company is located in Newport News, Virginia, and often participates in projects with the Norfolk Naval Shipyard in Portsmouth, Virginia, also located adjacent to Hampton Roads. In March 2011 Newport News Shipbuilding, along with the shipbuilding sector of Northrop Grumman spun off to form a new company called Huntington Ingalls Industries.
The shipyard is a major employer (largest industrial employer in the state of Virginia) not only for the lower Virginia Peninsula, but also portions of Hampton Roads south of the James River and the harbor, portions of the Middle Peninsula region, and even some northeastern counties of North Carolina.
As of December 2014, the shipyard was building the aircraft carriers USS "Gerald R. Ford" (CVN 78) and the USS "John F. Kennedy" (CVN 79).
History.
Industrialist Collis P. Huntington (1821–1900) provided crucial funding to complete the Chesapeake and Ohio Railroad (C&O) from Richmond, Virginia to the Ohio River in the early 1870s. Although originally built for general commerce, this C&O rail link to the midwest was soon also being used to transport bituminous coal from the previously isolated coalfields, adjacent to the New River and the Kanawha River in West Virginia. In 1881, the Peninsula Extension of the C&O was built from Richmond down the Virginia Peninsula to reach a new coal pier on Hampton Roads in Warwick County near the small unincorporated community of Newport News Point. However, building the railroad and coal pier was only the first part of Huntington's dreams for Newport News.
The shipyard's early years.
In 1886 he built a shipyard to repair ships servicing this transportation hub. In 1891 Newport News Shipbuilding and Drydock Company delivered its first ship, the tugboat "Dorothy". By 1897 NNS had built three warships for the US Navy: USS "Nashville", "Wilmington" and "Helena".
When Collis died in 1900, his nephew Henry E. Huntington inherited much of his uncle's fortune. He also married Collis' widow Arabella Huntington, and assumed Collis's leadership role with Newport News Shipbuilding and Drydock Company. Under Henry Huntington's leadership, growth continued. 
In 1906 the revolutionary HMS "Dreadnought" launched a great naval race worldwide. Between 1907 and 1923, Newport News built six of the US Navy's total of 22 dreadnoughts – USS "Delaware", "Texas", "Pennsylvania", "Mississippi", "Maryland" and "West Virginia". All but the first were in active service in World War II. In 1907 President Theodore Roosevelt sent the Great White Fleet on its round-the-world voyage. NNS had built seven of its 16 battleships.
In 1914 NNS built SS "Medina" for the Mallory Steamship Company; as she was until 2009 the world's oldest active ocean-faring passenger ship.
Newport News and the shipyard.
In the early years, leaders of the Newport News community and those of the shipyard were virtually interchangeable. Shipyard president Walter A. Post served from March 9, 1911 to Feb. 12, 1912, when he died. Earlier, he had come to the area as one of the builders of the C&O Railway's terminals, and had served as the first mayor of Newport News after it became an independent city in 1896. It was on March 14, 1914 that Albert L. Hopkins, a young New Yorker trained in engineering, succeeded Post as President of the company. In May 1915 while traveling to England on shipyard business, aboard , Albert L. Hopkins tenure and life ended prematurely when that ship was torpedoed and sunk by a German U-boat off Queenstown on the Irish coast. His assistant Fred Gauntlett, was also on board, but was able to swim to safety. Homer Lenoir Ferguson was a manager when Hopkins died, and assumed the presidency the following July. He saw the company through both world wars, became a noted community leader, and was a co-founder of the Mariners' Museum with Archer Huntington. He served until July 31, 1946, after the second World War had ended on both the European and Pacific fronts.
Just northwest of the shipyard, Hilton Village, one of the first planned communities in the country, was built by the federal government to house shipyard workers in 1918. The planners met with the wives of shipyard workers. Based on their input 14 house plans were designed for the projected 500 English-village-style homes. After the war, in 1922, Henry Huntington acquired it from the government, and helped facilitate the sale of the homes to shipyard employees and other local residents. Three streets there were named after Post, Hopkins, and Ferguson.
Navy orders during and after the First World War.
The "Lusitania" incident was among the events that brought the United States into World War I. Between 1918 and 1920 NNS delivered 25 destroyers, and after the war it began building aircraft carriers. was delivered in 1934, and NNS went on to build "Yorktown" and "Enterprise".
Ocean liners.
After the First World War NNS completed a major reconditioning and refurbishment of the ocean liner . Before the war she had been the German liner "Vaterland", but the start of hostilities found her laid up in New York Harbor and she had been seized by the US Government in 1917 and converted into a troopship. War duty and age meant that all wiring, plumbing, and interior layouts were stripped and redesigned while her hull was strengthened and her boilers converted from coal to oil while being refurbished. Virtually a new ship emerged from NNS in 1923, and the SS Leviathan became the flagship of United States Lines.
In 1927 NNS launched the World's first significant turbo-electric ocean liner: Panama Pacific Line's  GRT SS "California". At the time she was also the largest merchant ship yet built in the USA, although she was a modest size compared with the biggest European liners of her era. NNS launched "California"‍ '​s sister ships "Virginia" in 1928 and "Pennsylvania" in 1929. NNS followed them by launching two even larger turbo-electric liners for Dollar Steamship Company: the  GRT in 1930, followed by her sister "President Coolidge" in 1931. The SS America was launched in 1939 and entered service with United States lines shortly before World War II but soon returned to the shipyard for conversion to a troopship, USS West Point.
Navy orders before and during the Second World War.
By 1940 the Navy had ordered a battleship, seven more aircraft carriers and four cruisers. During World War II, NNS built ships as part of the U.S. Government's Emergency Shipbuilding Program, and swiftly filled requests for "Liberty ships" that were needed during the war. It founded the North Carolina Shipbuilding Company, an emergency yard on the banks of the Cape Fear River and launched its first Liberty ship before the end of 1941, building 243 ships in all, including 186 Liberties. For its contributions during the war, the Navy awarded the company its "E" pennant for excellence in shipbuilding. NNS ranked 23rd among United States corporations in the value of wartime production contracts.
Post-war ships.
In the post-war years NNS built the famous passenger liner , which set a transatlantic speed record that still stands today. In 1954 NNS, Westinghouse and the Navy developed and built a prototype nuclear reactor for a carrier propulsion system. NNS designed the USS "Enterprise" in 1960. In 1959 NNS launched its first nuclear-powered submarine, USS "Shark" as well as the ballistic missile submarine USS "Robert E. Lee".
In the 1970s, NNS launched two of the largest tankers ever built in the western hemisphere and also constructed three liquefied natural gas carriers – at over 390,000 deadweight tons, the largest ever built in the United States. NNS and Westinghouse Electric Company jointly form Offshore Power Systems to build floating nuclear power plants for Public Service Electric and Gas Company. 
In the 1980s, NNS produced a variety of Navy products, including "Nimitz"-class aircraft carrier nuclear aircraft carriers and "Los Angeles"-class submarine nuclear attack submarines. Since 1999 the shipyard has produced only warships for the Navy.
Submarine building problems.
In 2007, the US Navy found that workers had used incorrect metal to fuse together pipes and joints on submarines under construction and this could have led to cracking and leaks. In 2009 it was found that bolts and fasteners in weapons-handling systems on four Navy submarines, including , "North Carolina" (SSN-777), "Missouri" (SSN-780), and "California" (SSN-781), were installed incorrectly, delaying the launching of the boats whilst the problems were corrected.
Mergers, realignment, and spin-off.
In 1968, Newport News merged with Tenneco Corporation. In 1996, Tenneco initiated a spinoff of Newport News into an independent company (Newport News Shipbuilding). 
On 7 November 2001, Northrop Grumman entered an agreement to purchase Newport News Shipbuilding for a total of $2.6 billion. This acquisition created a $4 billion shipyard called Northrop Grumman Newport News. 
On 28 January 2008, Northrop Grumman Corporation realigned its two shipbuilding sectors, Northrop Grumman Newport News and Northrop Grumman Ship Systems, into a single sector called "Northrop Grumman Shipbuilding". On March 15, 2011 Northrop Grumman announced the spin-off of this sector into a separate company, Huntington Ingalls Industries, Inc.
Ships built.
Ships built at the Newport News yard include:

</doc>
<doc id="22145" url="http://en.wikipedia.org/wiki?curid=22145" title="Newton's method">
Newton's method

In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.
The Newton–Raphson method in one variable is implemented as follows:
Given a function "ƒ" defined over the reals "x", and its derivative "ƒ"', we begin with a first guess "x"0 for a root of the function "f". Provided the function satisfies all the assumptions made in the derivation of the formula, a better approximation "x"1 is
Geometrically, ("x"1, 0) is the intersection with the "x"-axis of the tangent to the graph of "f" at ("x"0, "f" ("x"0)).
The process is repeated as
until a sufficiently accurate value is reached.
This algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.
Description.
The idea of the method is as follows: one starts with an initial guess which is reasonably close to the true root, then the function is approximated by its tangent line (which can be computed using the tools of calculus), and one computes the "x"-intercept of this tangent line (which is easily done with elementary algebra). This "x"-intercept will typically be a better approximation to the function's root than the original guess, and the method can be iterated.
Suppose "ƒ" : ["a", "b"] → R is a differentiable function defined on the interval ["a", "b"] with values in the real numbers R. The formula for converging on the root can be easily derived. Suppose we have some current approximation "x""n". Then we can derive the formula for a better approximation, "x""n"+1 by referring to the diagram on the right. The equation of the tangent line to the curve "y" = "ƒ"("x") at the point "x=x""n" is 
where, "ƒ"' denotes the derivative of the function "ƒ".
The "x"-intercept of this line (the value of "x" such that "y"=0) is then used as the next approximation to the root, "x""n"+1. In other words, setting "y" to zero and "x" to "x""n"+1 gives 
Solving for "x""n"+1 gives
We start the process off with some arbitrary initial value "x"0. (The closer to the zero, the better. But, in the absence of any intuition about where the zero might lie, a "guess and check" method might narrow the possibilities to a reasonably small interval by appealing to the intermediate value theorem.) The method will usually converge, provided this initial guess is close enough to the unknown zero, and that "ƒ"'("x"0) ≠ 0. Furthermore, for a zero of multiplicity 1, the convergence is at least quadratic (see rate of convergence) in a neighbourhood of the zero, which intuitively means that the number of correct digits roughly at least doubles in every step. More details can be found in the analysis section below.
The Householder's methods are similar but have higher order for even faster convergence.
However, the extra computations required for each step can slow down the overall performance relative to Newton's method, particularly if "f" or its derivatives are computationally expensive to evaluate.
History.
The name "Newton's method" is derived from Isaac Newton's description of a special case of the method in "De analysi per aequationes numero terminorum infinitas" (written in 1669, published in 1711 by William Jones) and in "De metodis fluxionum et serierum infinitarum" (written in 1671, translated and published as "Method of Fluxions" in 1736 by John Colson). However, his method differs substantially from the modern method given above: Newton applies the method only to polynomials. He does not compute the successive approximations formula_7, but computes a sequence of polynomials, and only at the end arrives at an approximation for the root "x". Finally, Newton views the method as purely algebraic and makes no mention of the connection with calculus. Newton may have derived his method from a similar but less precise method by Vieta. The essence of Vieta's method can be found in the work of the Persian mathematician Sharaf al-Din al-Tusi, while his successor Jamshīd al-Kāshī used a form of Newton's method to solve formula_8 to find roots of "N" (Ypma 1995). A special case of Newton's method for calculating square roots was known much earlier and is often called the Babylonian method.
Newton's method was used by 17th-century Japanese mathematician Seki Kōwa to solve single-variable equations, though the connection with calculus was missing.
Newton's method was first published in 1685 in "A Treatise of Algebra both Historical and Practical" by John Wallis. In 1690, Joseph Raphson published a simplified description in "Analysis aequationum universalis". Raphson again viewed Newton's method purely as an algebraic method and restricted its use to polynomials, but he describes the method in terms of the successive approximations "x""n" instead of the more complicated sequence of polynomials used by Newton. Finally, in 1740, Thomas Simpson described Newton's method as an iterative method for solving general nonlinear equations using calculus, essentially giving the description above. In the same publication, Simpson also gives the generalization to systems of two equations and notes that Newton's method can be used for solving optimization problems by setting the gradient to zero.
Arthur Cayley in 1879 in "The Newton-Fourier imaginary problem" was the first to notice the difficulties in generalizing Newton's method to complex roots of polynomials with degree greater than 2 and complex initial values. This opened the way to the study of the theory of iterations of rational functions.
Practical considerations.
Newton's method is an extremely powerful technique—in general the convergence is quadratic: as the method converges on the root, the difference between the root and the approximation is squared (the number of accurate digits roughly doubles) at each step. However, there are some difficulties with the method.
Difficulty in calculating derivative of a function.
Newton's method requires that the derivative be calculated directly. An analytical expression for the derivative may not be easily obtainable and could be expensive to evaluate. In these situations, it may be appropriate to approximate the derivative by using the slope of a line through two nearby points on the function. Using this approximation would result in something like the secant method whose convergence is slower than that of Newton's method.
Failure of the method to converge to the root.
It is important to review the proof of quadratic convergence of Newton's Method before implementing it. Specifically, one should review the assumptions made in the proof. For situations where the method fails to converge, it is because the assumptions made in this proof are not met.
Overshoot.
If the first derivative is not well behaved in the neighborhood of a particular root, the method may overshoot, and diverge from that root. An example of a function with one root, for which the derivative is not well behaved in the neighborhood of the root, is
for which the root will be overshot and the sequence of x will diverge. For a = 1/2, the root will still be overshot, but the sequence will oscillate between two values. For 1/2 < a < 1, the root will still be overshot but the sequence will converge, and for a ≥ 1 the root will not be overshot at all.
In some cases, Newton's method can be stabilized by using successive over-relaxation, or the speed of convergence can be increased by using the same method.
Stationary point.
If a stationary point of the function is encountered, the derivative is zero and the method will terminate due to division by zero.
Poor initial estimate.
A large error in the initial estimate can contribute to non-convergence of the algorithm.
Mitigation of non-convergence.
In a robust implementation of Newton's method, it is common to place limits on the number of iterations, bound the solution to an interval known to contain the root, and combine the method with a more robust root finding method.
Slow convergence for roots of multiplicity > 1.
If the root being sought has multiplicity greater than one, the convergence rate is merely linear (errors reduced by a constant factor at each step) unless special steps are taken. When there are two or more roots that are close together then it may take many iterations before the iterates get close enough to one of them for the quadratic convergence to be apparent. However, if the multiplicity formula_10 of the root is known, one can use the following modified algorithm that preserves the quadratic convergence rate:
This is equivalent to using successive over-relaxation. On the other hand, if the multiplicity formula_10 of the root is not known, it is possible to estimate formula_10 after carrying out one or two iterations, and then use that value to increase the rate of convergence.
Analysis.
Suppose that the function "ƒ" has a zero at α, i.e., "ƒ"(α) = 0, and "ƒ" is differentiable in a neighborhood of α.
If "f"  is continuously differentiable and its derivative is nonzero at α, then there exists a neighborhood of α such that for all starting values "x"0 in that neighborhood, the sequence {"x""n"} will converge to α.
If the function is continuously differentiable and its derivative is not 0 at α and it has a second derivative at α then the convergence is quadratic or faster. If the second derivative is not 0 at α then the convergence is merely quadratic. If the third derivative exists and is bounded in a neighborhood of α, then:
where formula_15
If the derivative is 0 at α, then the convergence is usually only linear. Specifically, if "ƒ" is twice continuously differentiable, "ƒ" '("α") = 0 and "ƒ" "("α") ≠ 0, then there exists a neighborhood of α such that for all starting values "x"0 in that neighborhood, the sequence of iterates converges linearly, with rate log10 2 (Süli & Mayers, Exercise 1.6). Alternatively if "ƒ" '("α") = 0 and "ƒ" '("x") ≠ 0 for "x" ≠ α, "x" in a neighborhood "U" of α, α being a zero of multiplicity "r", and if "ƒ" ∈ "C""r"("U") then there exists a neighborhood of α such that for all starting values "x"0 in that neighborhood, the sequence of iterates converges linearly.
However, even linear convergence is not guaranteed in pathological situations.
In practice these results are local, and the neighborhood of convergence is not known in advance. But there are also some results on global convergence: for instance, given a right neighborhood "U+" of α, if "f" is twice differentiable in "U+" and if formula_16, formula_17 in "U+", then, for each "x"0 in "U"+ the sequence "xk" is monotonically decreasing to α.
Proof of quadratic convergence for Newton's iterative method.
According to Taylor's theorem, any function "f"("x") which has a continuous second derivative can be represented by an expansion about a point that is close to a root of f(x). Suppose this root is formula_18 Then the expansion of f(α) about "x""n" is:
where the Lagrange form of the Taylor series expansion remainder is
where ξ"n" is in between "x""n" and formula_18
Since formula_21 is the root, (1) becomes:
Dividing equation (2) by formula_22 and rearranging gives
Remembering that "x""n"+1 is defined by
one finds that
That is,
Taking absolute value of both sides gives
Equation (6) shows that the rate of convergence is quadratic if the following conditions are satisfied:
The term "sufficiently" close in this context means the following:
(a) Taylor approximation is accurate enough such that we can ignore higher order terms,
(b) formula_28
(c) formula_29
Finally, (6) can be expressed in the following way:
The initial point formula_31 has to be chosen such that conditions 1 through 3 are satisfied, where the third condition requires that formula_32
Basins of attraction.
The basins of attraction—the regions of the real number line such that within each region iteration from any point leads to one particular root—can be infinite in number and arbitrarily small. For example, for the function formula_33, the following initial conditions are in successive basins of attraction:
Failure analysis.
Newton's method is only guaranteed to converge if certain conditions are satisfied. If the assumptions made in the proof of quadratic convergence are met, the method will converge. For the following subsections, failure of the method to converge indicates that the assumptions made in the proof were not met.
Bad starting points.
In some cases the conditions on the function that are necessary for convergence are satisfied, but the point chosen as the initial point is not in the interval where the method converges. This can happen, for example, if the function whose root is sought approaches zero asymptotically as "x" goes to formula_34 or formula_35. In such cases a different method, such as bisection, should be used to obtain a better estimate for the zero to use as an initial point.
Iteration point is stationary.
Consider the function:
It has a maximum at "x" = 0 and solutions of "f"("x") = 0 at "x" = ±1. If we start iterating from the stationary point "x"0 = 0 (where the derivative is zero), "x"1 will be undefined, since the tangent at (0,1) is parallel to the "x"-axis:
The same issue occurs if, instead of the starting point, any iteration point is stationary. Even if the derivative is small but not zero, the next iteration will be a far worse approximation.
Starting point enters a cycle.
For some functions, some starting points may enter an infinite cycle, preventing convergence. Let
and take 0 as the starting point. The first iteration produces 1 and the second iteration returns to 0 so the sequence will alternate between the two without converging to a root. In fact, this 2-cycle is stable: there are neighborhoods around 0 and around 1 from which all points iterate asymptotically to the 2-cycle (and hence not to the root of the function). In general, the behavior of the sequence can be very complex (see Newton fractal).
Derivative issues.
If the function is not continuously differentiable in a neighborhood of the root then it is possible that Newton's method will always diverge and fail, unless the solution is guessed on the first try.
Derivative does not exist at root.
A simple example of a function where Newton's method diverges is the cube root, which is continuous and infinitely differentiable, except for "x" = 0, where its derivative is undefined (this, however, does not affect the algorithm, since it will never require the derivative if the solution is already found):
For any iteration point "xn", the next iteration point will be:
The algorithm overshoots the solution and lands on the other side of the "y"-axis, farther away than it initially was; applying Newton's method actually doubles the distances from the solution at each iteration.
In fact, the iterations diverge to infinity for every formula_41, where formula_42. In the limiting case of formula_43 (square root), the iterations will alternate indefinitely between points "x"0 and −"x"0, so they do not converge in this case either.
Discontinuous derivative.
If the derivative is not continuous at the root, then convergence may fail to occur in any neighborhood of the root. Consider the function
Its derivative is:
Within any neighborhood of the root, this derivative keeps changing sign as "x" approaches 0 from the right (or from the left) while "f"("x") ≥ "x" − "x"2 > 0 for 0 < "x" < 1.
So "f"("x")/"f"'("x") is unbounded near the root, and Newton's method will diverge almost everywhere in any neighborhood of it, even though:
Non-quadratic convergence.
In some cases the iterates converge but do not converge as quickly as promised. In these cases simpler methods converge just as quickly as Newton's method.
Zero derivative.
If the first derivative is zero at the root, then convergence will not be quadratic. Indeed, let
then formula_47 and consequently formula_48. So convergence is not quadratic, even though the function is infinitely differentiable everywhere.
Similar problems occur even when the root is only "nearly" double. For example, let
Then the first few iterates starting at "x"0 = 1 are
1, 0.500250376, 0.251062828, 0.127507934, 0.067671976, 0.041224176, 0.032741218, 0.031642362; it takes six iterations to reach a point where the convergence appears to be quadratic.
No second derivative.
If there is no second derivative at the root, then convergence may fail to be quadratic. Indeed, let
Then
And
except when formula_53 where it is undefined. Given formula_54,
which has approximately 4/3 times as many bits of precision as formula_54 has. This is less than the 2 times as many which would be required for quadratic convergence. So the convergence of Newton's method (in this case) is not quadratic, even though: the function is continuously differentiable everywhere; the derivative is not zero at the root; and formula_57 is infinitely differentiable except at the desired root.
Generalizations.
Complex functions.
When dealing with complex functions, Newton's method can be directly applied to find their zeroes. Each zero has a basin of attraction in the complex plane, the set of all starting values that cause the method to converge to that particular zero. These sets can be mapped as in the image shown. For many complex functions, the boundaries of the basins of attraction are fractals.
In some cases there are regions in the complex plane which are not in any of these basins of attraction, meaning the iterates do not converge. For example, if one uses a real initial condition to seek a root of formula_58, all subsequent iterates will be real numbers and so the iterations cannot converge to either root, since both roots are non-real. In this case almost all real initial conditions lead to chaotic behavior, while some initial conditions iterate either to infinity or to repeating cycles of any finite length.
Nonlinear systems of equations.
k variables, k functions.
One may also use Newton's method to solve systems of "k" (non-linear) equations, which amounts to finding the zeroes of continuously differentiable functions "F" : R"k" → R"k". In the formulation given above, one then has to left multiply with the inverse of the "k"-by-"k" Jacobian matrix "J""F"("x""n") instead of dividing by "f" '("x""n").
Rather than actually computing the inverse of this matrix, one can save time by solving the system of linear equations
for the unknown "x""n"+1 − "x""n".
k variables, m equations, with m > k.
The k-dimensional Newton's method can be used to solve systems of ">k" (non-linear) equations as well if the algorithm uses the generalized inverse of the non-square Jacobian matrix J+ = ((JTJ)−1)JT instead of the inverse of J. If the nonlinear system has no solution, the method attempts to find a solution in the non-linear least squares sense. See Gauss–Newton algorithm for more information.
Nonlinear equations in a Banach space.
Another generalization is Newton's method to find a root of a functional "F" defined in a Banach space. In this case the formulation is
where formula_61 is the Fréchet derivative computed at formula_62. One needs the Fréchet derivative to be boundedly invertible at each formula_62 in order for the method to be applicable. A condition for existence of and convergence to a root is given by the Newton–Kantorovich theorem.
Nonlinear equations over "p"-adic numbers.
In "p"-adic analysis, the standard method to show a polynomial equation in one variable has a "p"-adic root is Hensel's lemma, which uses the recursion from Newton's method on the "p"-adic numbers. Because of the more stable behavior of addition and multiplication in the "p"-adic numbers compared to the real numbers (specifically, the unit ball in the "p"-adics is a ring), convergence in Hensel's lemma can be guaranteed under much simpler hypotheses than in the classical Newton's method on the real line.
Newton-Fourier method.
The Newton-Fourier method is Joseph Fourier's extension of Newton's method to provide bounds on the absolute error of the root approximation, while still providing quadratic convergence.
Assume that formula_64 is twice continuously differentiable on formula_65 and that formula_66 contains a root in this interval. Assume that formula_67 on this interval (this is the case for instance if formula_68, formula_69, and formula_70, and formula_71 on this interval). This guarantees that there is a unique root on this interval, call it formula_72. If it is concave down instead of concave up then replace formula_64 by formula_74 since they have the same roots.
Let formula_75 be the right endpoint of the interval and let formula_76 be the left endpoint of the interval. Given formula_7, define formula_78, which is just Newton's method as before. Then define formula_79 and note that the denominator has formula_80 and not formula_81. The iterates formula_7 will be strictly decreasing to the root while the iterates formula_83 will be strictly increasing to the root. Also, formula_84 so that distance between formula_7 and formula_83 decreases quadratically.
Quasi-Newton methods.
When the Jacobian is unavailable or too expensive to compute at every iteration, a Quasi-Newton method can be used.
Applications.
Minimization and maximization problems.
Newton's method can be used to find a minimum or maximum of a function. The derivative is zero at a minimum or maximum, so minima and maxima can be found by applying Newton's method to the derivative. The iteration becomes:
Multiplicative inverses of numbers and power series.
An important application is Newton–Raphson division, which can be used to quickly find the reciprocal of a number, using only multiplication and subtraction.
Finding the reciprocal of "a" amounts to finding the root of the function
Newton's iteration is 
Therefore, Newton's iteration needs only two multiplications and one subtraction.
This method is also very efficient to compute the multiplicative inverse of a power series.
Solving transcendental equations.
Many transcendental equations can be solved using Newton's method. Given the equation
with "g(x)" and/or "h(x)" a transcendental function, one writes
The values of "x" that solves the original equation are then the roots of "f(x)", which may be found via Newton's method.
Examples.
Square root of a number.
Consider the problem of finding the square root of a number. Newton's method is one of many methods of computing square roots.
For example, if one wishes to find the square root of 612, this is equivalent to finding the solution to
The function to use in Newton's method is then,
with derivative,
With an initial guess of 10, the sequence given by Newton's method is
where the correct digits are underlined. With only a few iterations one can obtain a solution accurate to many decimal places.
Solution of cos("x") = "x"3.
Consider the problem of finding the positive number "x" with cos("x") = "x"3. We can rephrase that as finding the zero of "f"("x") = cos("x") − "x"3. We have "f"'("x") = −sin("x") − 3"x"2. Since cos("x") ≤ 1 for all "x" and "x"3 > 1 for "x" > 1, we know that our solution lies between 0 and 1. We try a starting value of "x"0 = 0.5. (Note that a starting value of 0 will lead to an undefined result, showing the importance of using a starting point that is close to the solution.)
The correct digits are underlined in the above example. In particular, "x"6 is correct to the number of decimal places given. We see that the number of correct digits after the decimal point increases from 2 (for "x"3) to 5 and 10, illustrating the quadratic convergence.
Pseudocode.
The following is an example of using the Newton's Method to help find a root of a function codice_1 which has derivative codice_2. 
The initial guess will be formula_97 and the function will be formula_98 so that formula_99.
Each new iterative of Newton's method will be denoted by codice_3. We will check during the computation whether the denominator (codice_4) becomes too small (smaller than codice_5), which would be the case if formula_100, since otherwise a large amount of error could be introduced. 

</doc>
<doc id="22146" url="http://en.wikipedia.org/wiki?curid=22146" title="New Order">
New Order

New Order are an English rock band formed in 1980 and currently consisting of Bernard Sumner, Stephen Morris, Gillian Gilbert, Phil Cunningham and Tom Chapman. The band was formed in 1980 by Sumner (vocals, guitars, keyboards and synthesisers), Peter Hook (bass and vocals) and Morris (drums, electronic drums, keyboards and synthesisers) – the remaining members of Joy Division, following the suicide of vocalist Ian Curtis – with the addition of Gilbert (keyboards, synthesisers and guitars).
By combining post-punk and electronic dance music, New Order became one of the most critically acclaimed and influential bands of the 1980s. Though the band's early years were shadowed by the legacy and basic sound of Joy Division, their experience of the early 1980s New York City club scene increased their knowledge of dance music and helped them incorporate elements of that style into their work. The band's 1983 hit "Blue Monday", the best-selling 12-inch single of all time, is one example of how the band transformed their sound.
New Order were the flagship band for Manchester-based independent record label Factory Records. Their minimalist album sleeves and "non-image" (the band rarely gave interviews and were known for performing short concert sets with no encores) reflected the label's aesthetic of doing whatever the relevant parties wanted to do, including an aversion to including singles as album tracks.
In 1993 the band worked on their own individual projects and reunited as a band in 1998. In 2001, Cunningham (guitars, keyboards and synthesisers) replaced Gilbert, who took a sabbatical from the band because of family commitments. In 2007, Peter Hook left the band. After Hook's departure, Bernard, Phil and Steve worked on Bad Lieutenant and the band reunited in 2011 without Hook, with Gilbert's return and Chapman replacing Hook on bass. During the band's career and in between lengthy breaks, band members have been involved in several solo projects, such as Sumner's Electronic and Bad Lieutenant; Hook's Monaco and Revenge and Gilbert and Morris' The Other Two. Cunningham was previously a member of Marion and with Sumner and Chapman was a member of Bad Lieutenant.
History.
Origins and formation: 1977–1980.
Between 1977 and 1980, Ian Curtis, Peter Hook, Stephen Morris, and Bernard Sumner were members of the post-punk band Joy Division, often featuring heavy production input from producer Martin Hannett. Curtis committed suicide on 18 May 1980, the day before Joy Division were scheduled to depart for their first American tour, and prior to release of the band's second album, "Closer". The rest of the band decided soon after Curtis's death that they would carry on. Prior to his death, the members of Joy Division had agreed not to continue under the Joy Division name should any one member leave. On 29 July 1980, the still unnamed trio debuted live at Manchester's Beach Club. Rob Gretton, the band's manager for over twenty years, is credited for having found the name "New Order" in an article in "The Guardian" entitled "The People's New Order of Kampuchea". The band adopted this name, despite its previous use for ex-Stooge Ron Asheton's band The New Order. The group states that the name New Order (as was also the case with "Joy Division") does not draw a direct line to Nazism or Fascism.
The band rehearsed with each member taking turns on vocals. Sumner ultimately took the role, as he could sing when he wasn't playing his guitar. Wanting to complete the line-up with someone they knew well and whose musical skill and style was compatible with their own, New Order invited Morris's girlfriend, Gillian Gilbert from Macclesfield, to join the band in early October 1980, as keyboardist and guitarist. Gilbert's membership was suggested by Gretton. Gilbert's first live performance with New Order occurred at The Squat in Manchester on 25 October 1980.
"Movement": 1981–1982.
The initial release as New Order was the single "Ceremony", backed with "In a Lonely Place". These two songs were written in the weeks before Curtis took his own life. With the release of "Movement" in November 1981, New Order initially started on a similar route as their previous incarnation, performing dark, melodic songs, albeit with an increased use of synthesisers. The band viewed the period as a low point, as they were still reeling from Curtis' death. Hook commented that the only positive thing to come out of the "Movement" sessions was that producer Martin Hannett had showed the band how to use a mixing board, which allowed them to produce records by themselves from then on. More recently, Hook indicated a change of heart: "I think "Movement" gets a raw deal in general really – for me, when you consider the circumstances in which it was written, it is a fantastic record."
New Order visited New York City again in 1981, where the band were introduced to post-disco, Latin freestyle, and electro. The band had taken to listening to Italian disco to cheer themselves up, while Morris taught himself drum programming. The singles that followed, "Everything's Gone Green" and "Temptation", saw a change in direction toward dance music.
The Haçienda, Factory Records' own nightclub (largely funded by New Order) opened in May 1982 in Manchester and was even issued a Factory catalogue number: FAC51. The opening of UK's first ever superclub was marked by a nearly 23-minute instrumental piece originally entitled "Prime 5 8 6", but released 15 years later as "Video 5 8 6". Composed primarily by Sumner and Morris, "Prime 5 8 6"/"Video 5 8 6" was an early version of "5 8 6" that contained rhythm elements that would later surface on "Blue Monday" and "Ultraviolence".
"Power, Corruption & Lies": 1983–1984.
"Power, Corruption & Lies", released in May 1983, was a synthesiser-based outing and a dramatic change in sound from Joy Division and the preceding album, although the band had been hinting at the increased use of technology during the music-making process for a number of years then, including their work as Joy Division. Starting from what earlier singles had hinted, this was where the band had found their footing, mixing early techno music with their earlier guitar-based sound and showing the strong influence of acts like Kraftwerk and Giorgio Moroder. Even further in this direction was the electronically sequenced, four-on-the-floor single "Blue Monday". Inspired by Klein & MBO's "Dirty Talk" and Sylvester's disco classic, "You Make Me Feel (Mighty Real)", "Blue Monday" became the best-selling independent 12" single of all time in the UK; however, (much to the chagrin of the buying public) it was not on the track list of "Power, Corruption & Lies". This resulted in a sticker being applied to unsold copies of "Power, Corruption & Lies" album saying, "DOES NOT CONTAIN BLUE MONDAY". (It was included on the cassette format in some countries, such as Australia and New Zealand.) "Blue Monday" is now included on the 2008 collector's edition of "Power, Corruption & Lies".
The 1983 single "Confusion" firmly established the group as a dance music force, inspiring many musicians in subsequent years. In 1984 they followed the largely synthesised single "Thieves Like Us" with the heavy guitar-drum-bass rumble of "Murder", a not-too-distant cousin of "Ecstasy" from the "Power, Corruption & Lies" album.
"Low-Life", "Brotherhood", and "Substance": 1985–1987.
1985's "Low-Life" refined and sometimes mixed the two styles, brandishing "The Perfect Kiss"—the video for which was filmed by Jonathan Demme—and "Sub-culture". In February 1986, the soundtrack album to "Pretty in Pink" featuring "Shellshock" was released on A&M Records. An instrumental version of "Thieves Like Us" and the instrumental "Elegia" appeared in the film but were not on the soundtrack album. Later that summer, New Order headlined a line-up that included the Smiths, the Fall, and A Certain Ratio during the Festival of the Tenth Summer at Manchester's G-Mex.
"Brotherhood" (1986) divided the two approaches onto separate album sides. The album notably featured "Bizarre Love Triangle" and "Angel Dust" (of which a remixed instrumental version is available on the UK "True Faith" CD video single, under the title "Evil Dust"), a track which marries a synth break beat with "Low-Life"-era guitar effects. While New Order toured North America with friends Echo & the Bunnymen, the summer of 1987 saw the release of the compilation "Substance", which featured the new single "True Faith". "Substance" was an important album in collecting the group's 12-inch singles onto CD for the first time and featured new versions of "Temptation" and "Confusion"—referred to as "Temptation '87" and "Confusion '87". A second disc featured several of the B-sides from the singles on the first disc, as well as additional A-sides "Procession" and "Murder". The single, "True Faith", with its surreal video, became a hit on MTV and the band's first American top 40 hit. The single's B-side, "1963"—originally planned on being the A-side until the group's label convinced them to release "True Faith" instead—would later be released as a single in its own right several years later, with two new versions.
In December 1987, the band released a further single, "Touched by the Hand of God", with a Katherine Bigelow-directed video parodying glam-metal. The single reached number 20 on the UK Singles Chart and number 1 in the UK Independent Singles chart, but would not appear on album until the 1994 compilation "The Best of New Order".
"Technique", "Republic" and first break-up: 1988–1993.
By this time, the group was heavily influenced by the Balearic sounds of Ibiza making their way into the Hacienda. Partly recorded at Mediterranean Sound studios on Ibiza, "Technique" was released in February 1989. The album entered the charts at number one in the UK and contained a mix of the acid house influence (as on "Fine Time", which is the opening track on the album) and a more traditional rock sound on others (such as the single "Run 2"). The album is a blend of occasionally upbeat, accessible music coupled with blunt, poignant lyrics. During the summer of 1989, New Order supported "Technique" by touring with Public Image Ltd, Throwing Muses and The Sugarcubes across the United States and Canada in what was the press dubbed the "Monsters of Alternative Rock" tour. Also New Order recorded the official song of the England national football team's 1990 World Cup campaign, "World in Motion", under the ad-hoc band name EnglandNewOrder. The song, co-written with comedian Keith Allen, was a number one UK hit.
New Order never had a formal contract with their label Factory Records, which is unusual for any major group. (This was in fact the label's standard practice until the mid-1980s. According to Factory's co-founder Tony Wilson, "All our bands are free to fuck off whenever they please.") Because of this, the group (rather than Factory Records) legally owned all their own recorded material. This has often been cited, not least by Wilson himself, as the main reason London Records' 1992 offer to buy the ailing label fell through.
"Republic", released around the world in 1993, was the band's first album release since parting company with the now-defunct Factory Records. The release spawned the singles "Regret"—their highest-charting single in the US—"Ruined in a Day", "World", and "Spooky".
Following the release of "Republic", the band put New Order on hold while each member continued on with their own side-projects. In 1994, a second singles collection was released, entitled "The Best of New Order". It featured all of the band's singles since "Substance" as well as a few extra tracks: "Vanishing Point" (from 1989's "Technique"), "The Perfect Kiss", "Thieves Like Us", "Shellshock", and new recordings of "True Faith", "Bizarre Love Triangle", "1963" and "Round & Round". The new versions of "True Faith" and "1963" – the latter with a yet newer, more guitar-oriented version produced by Arthur Baker – were released as singles to promote the album. In the US, the track listing was altered to set it apart from "Substance" as well as the UK release of "The Best of New Order" which had been available months prior. This collection was followed by a remix album, "The Rest of New Order", featuring a selection of old remixes and newly commissioned mixes of classic New Order tracks. Some versions contained an extra disc/cassette composed entirely of remixes of "Blue Monday". "Blue Monday" was released as a single for a third time to promote the collection.
Reformation and "Get Ready": 1998–2003.
The group reconvened in 1998 at the suggestion of Rob Gretton. Nearly five years had passed since they had last seen each other. Sumner said, "We decided before we agreed to doing any gig, to have a meeting, and if anyone had any grudges to bear, to iron them out." By the second meeting everyone agreed to continue playing, scheduling their reunion gig for the Phoenix Festival that same year. In addition to rarer songs, New Order also decided to begin playing Joy Division songs again. When the Phoenix Festival was cancelled due to low ticket sales, New Order instead played the last night of that year's Reading Festival.
Their 2001 release "Get Ready" largely departed from their more electronic style and focused on the more guitar oriented music. According to Sumner, ""Get Ready" was guitar-heavy simply because we felt that we'd left that instrument alone for a long time." Longtime fan Billy Corgan of The Smashing Pumpkins played guitar and sang back-up on the track "Turn My Way", and in 2001 toured with the band on dates in the UK, US, and Japan for a short period of time. Phil Cunningham (formerly of Marion) joined the band in a live capacity, deputising for Gilbert who declined to tour in favour of caring for her and Morris' children. Primal Scream's Bobby Gillespie provided vocals on the track "Rock the Shack". Singles from the album included "Crystal", "60 Miles an Hour", and "Someone Like You".
In 2002, "Q" featured New Order on their list of the "50 Bands to See Before You Die", although this was as part of a sub-list of "5 Bands That Could Go Either Way". Both New Order and Joy Division were portrayed in the Michael Winterbottom film "24 Hour Party People", depicting the rise and fall of Factory Records as seen through the eyes of label founder Tony Wilson. Cameos by Wilson himself, along with Mark E. Smith of The Fall and former members of Happy Mondays and Inspiral Carpets, lent a degree of legitimacy to the proceedings. The film touched on some of Factory's other artists, including Happy Mondays and The Durutti Column. The soundtrack featured "Get Ready"-era Chemical Brothers-produced "Here to Stay", which was released as a single. The DVD release of the single highlighted scenes taken from the film.
"Waiting for the Sirens' Call", "Singles" and second break-up: 2004–2007.
The band released a new album on 27 March 2005, entitled "Waiting for the Sirens' Call", their first with new member Phil Cunningham. Cunningham replaced Gilbert (now married to Morris) so she could look after their children. Singles from this album were "Krafty", "Jetstream" (which features guest vocals by Ana Matronic from Scissor Sisters), and the title track. At the 2005 NME Awards, New Order and Joy Division received the award for "Godlike Geniuses" (for lifetime achievement). Previous winners include Ozzy Osbourne, The Clash, and Happy Mondays. In 2006 the album track "Guilt Is a Useless Emotion" was nominated for a Grammy Award in the category of Best Dance Recording.
In the autumn of 2005, the group released another greatest hits compilation, in the form of "Singles". The two-disc release was an updated version of the "Substance" collection and contained every single released from their 1981 debut all the way through to "Waiting for the Sirens' Call". However, unlike "Substance", which focused almost exclusively on the 12" versions of the group's singles, "Singles" collected the 7" versions, many of which (like "Ceremony", "Temptation" and "Confusion") had never been released on CD. The album was accompanied by a two-disc DVD set, entitled "Item", that collected the extended UK version of "NewOrderStory" with a DVD of all New Order music videos as well as two newly commissioned videos for "Temptation '87" and "Ceremony".
The "New Order: Live in Glasgow" DVD was recorded at the Glasgow Academy in 2006 and features 18 tracks, including 4 Joy Division songs. Next to that, the release also contains a bonus disc of footage from the band's personal archive including 1980s footage from Glastonbury, Rome, Cork, Rotterdam and Toronto.
In 2006, the band played several one-off live dates as well as short tours in the UK, Brazil and Argentina. At the end of the Buenos Aires show in November 2006, Peter Hook suggested that the band should stop touring. In early May 2007, bassist Peter Hook was interviewed by British radio station XFM – originally to talk about his contribution to the debut album of former Jane's Addiction singer Perry Farrell's new band Satellite Party saying that "[...]me and Bernard [Sumner] aren't working together." Further complicating the news, NewOrderOnline, a website with support from New Order management, reported that, according to "a source close to the band," "the news about the split is false... New Order still exists despite what [Hook] said [...] Peter Hook can leave the band, but this doesn't mean the end of New Order." The band however dissolved and Sumner revealed in 2009 that he no longer wished to make music as New Order.
Reunion with new line-up, "Lost Sirens": 2011–present.
In September 2011, the band announced that they would perform for the first time since 2006, at the Ancienne Belgique, Brussels on 17 October and at the Bataclan, Paris on 18 October. The band's line-up included keyboardist Gillian Gilbert, who returned to the band after a ten-year break, and Bad Lieutenant bassist Tom Chapman in place of Peter Hook. They played subsequent shows in London and South America in December.
In December 2011, New Order released "Live at the London Troxy", a live album from their performance of 10 December 2011 at The Troxy in London. This release featured the new lineup and their first show in London in over five years.
They continued to tour throughout 2012, including a short tour of New Zealand and Australia in February/March. They played at the 'T in the Park' festival in Scotland on 3 and 4 July 2012 and at the EXIT Festival in Novi Sad Serbia on 13 July 2012. New Order performed at Hyde Park with Blur and The Specials to celebrate the 2012 Summer Olympics closing ceremony.
In December 2012 it was announced that "Lost Sirens" would be released in the United Kingdom on 14 January 2013. "Lost Sirens" is an eight-track album of tracks left out of "Waiting for the Sirens' Call". The album was discussed by Gillian Gilbert in a Brazilian interview to promote the band’s appearance in São Paulo. She acknowledged issues with former member Peter Hook, and stated there was "a lot going on behind the scenes on the copyright" delaying the release.
The band debuted their first newly-written song since the "Waiting for the Sirens' Call" sessions during Lollapalooza Chile in March 2014. The song was initially performed without an official name, but now has a confirmed title of "Singularity." In July, the group toured North America, where they debuted the song "Plastic". On 2 September it was announced that the band decided to release their new album through Mute Records. The New Order catalogue remains with Warner Music.
Other projects.
In 1988, Bernard Sumner teamed up with former Smiths guitarist Johnny Marr for the Electronic project (also enlisting the help of Neil Tennant and Chris Lowe of the Pet Shop Boys). Sumner worked with Marr in 1996 for second time in Electronic for "Raise the Pressure". Karl Bartos (formerly of Kraftwerk) also assisted with this record. The project's third album "Twisted Tenderness" was released in 1999 after which the band dissolved.
In June 2009, Bernard Sumner formed a new band called Bad Lieutenant with Phil Cunningham (guitar) and Jake Evans (guitar and vocals), that completed an album, "Never Cry Another Tear", which was released on 5 October 2009. In addition to Cunningham and Evans the album also features appearances by Stephen Morris (drums), Jack Mitchell (drums), Tom Chapman (bass) and Alex James (bass). The live band includes Morris on drums and Tom Chapman on bass.
Hook has been involved with several other projects. In the 1990s, Hook recorded with Killing Joke with a view to joining the band. However, original bassist Martin 'Youth' Glover instead returned to the band. In 1995 he toured with The Durutti Column. He has recorded one album with the band Revenge and two with Monaco (both as bassist, keyboardist and lead vocalist) with David Potts, the latter of which scored a club and alternative radio hit "What Do You Want From Me?" in 1997. He also worked on a new band project called Freebass with bass players Mani (The Stone Roses) and Andy Rourke (ex-The Smiths), active from 2007 to 2010. He also contributed to Perry Farrell's Satellite Party. Currently, Hook's new band The Light, is touring playing Joy Division's albums and New Order first 2 albums.
In 1990 Gilbert and Morris formed their own band, The Other Two. The Other Two released its first single "Tasty Fish" in 1991 and released two albums, "The Other Two & You" in 1993 and "Super Highways" 1999. In 2007, Gilbert and Morris remixed two tracks for the Nine Inch Nails remixes album "Year Zero Remixed".
BeMusic.
"BeMusic" was a name the band used for their publishing company (and the LP label for "Movement" says "B Music" in large letters, though using an italic ß for the letter B). All four members of the band used the name for production work for other artists' recordings between 1982 and 1985.
The first BeMusic credit was for Peter Hook producing Stockholm Monsters in 1982. Other artists with producer or musician credit for "BeMusic" were 52nd Street, Section 25, Marcel King, Quando Quango, Paul Haig, Thick Pigeon, Nyam Nyam and Life.
Their production work as BeMusic was collected on two LTM Recordings compilation CDs, "Cool As Ice: The BeMusic Productions" and "Twice As Nice" (which also included production work by Donald Johnson, of A Certain Ratio, and Arthur Baker).
Sound.
Both New Order and Joy Division were among the most successful artists on the Factory Records label, run by Granada television personality Tony Wilson, and partnered with Factory in the financing of the Manchester club The Haçienda. The band rarely gave interviews in the 1980s, later ascribing this to not wanting to discuss Curtis. This, along with the Peter Saville sleeve designs and the tendency to give short performances with no encores, gave New Order a reputation as standoffish. The band became more open in the '90s; for example, the aforementioned "NewOrderStory" (and in particular the longer UK version) featured extensive personal interviews.
Their music mixes rock with dance music, as can be seen on signature tracks such as "True Faith" and "Temptation". This synthesis laid down the groundwork for dance-rock groups of today. The group's album art earned them the status of icons in the alternative community, and has shown considerable longevity. The band are also regarded as the first alternative dance music group with their fusion of "used icy, gloomy post-punk with Kraftwerk-style synth-pop" and were also labeled as synthpop, post-punk, new wave, and dance-rock.
They have heavily influenced techno, rock, and pop musicians including Pet Shop Boys, The Killers and Moby, and were themselves influenced by the likes of David Bowie, Neu!, Kraftwerk, Cabaret Voltaire and Giorgio Moroder. They have also significantly influenced electro, freestyle and house. The Kraftwerk influence was acknowledged by their single "Krafty", which had cover art referencing "Autobahn". Bassist Peter Hook contributed to New Order's sound by developing an idiosyncratic bass guitar technique. He often used the bass as a lead instrument, playing melodies on the high strings with a signature heavy chorus effect, leaving the "actual" basslines to keyboards or sequencers. This has often been cited as the defining characteristic of the New Order sound.
Drummer Stephen Morris regularly played a mixture of acoustic and electronic drums, and in many cases played along seamlessly with sequenced parts. All the band members could and did switch instruments throughout gigs, as evidenced on Jonathan Demme's video for "The Perfect Kiss" and the fairly common "Taras Shevchenko" and "Pumped Full of Drugs" concert videos. In particular, Gilbert, Morris, and Sumner could be seen playing keyboards at times, while Hook played electronic drums variously. ("Taras Shevchenko" is especially notable for the fact all four members of the group have left the stage before the final song ("Temptation") comes to an end.)
Album covers.
Almost all New Order recordings bore the minimalist packaging of Peter Saville. The group's record sleeves bucked the 1980s trend by rarely showing the band members (the "Low-Life" album was the exception) or even providing basic information such as the band name or the title of the release. Song names were often hidden within the shrink wrapped package, either on the disc itself (such as the "Blue Monday" single) or on an inconspicuous part of an inner sleeve ("The Perfect Kiss" single), or a cryptic colour code invented by Saville ("Power, Corruption & Lies"). Saville said his intention was to sell the band as a "mass-produced secret" of sorts, and that the minimalist style was enough to allow fans to identify the band's products without explicit labelling. Saville frequently sent the artwork straight to the printer, unreviewed by either the band or the label.
Band members.
<br>

</doc>
<doc id="22148" url="http://en.wikipedia.org/wiki?curid=22148" title="Niccolò Fontana Tartaglia">
Niccolò Fontana Tartaglia

Niccolò Fontana Tartaglia (]; 1499/1500, Brescia – 13 December 1557, Venice) was an Italian mathematician, engineer (designing fortifications), a surveyor (of topography, seeking the best means of defense or offense) and a bookkeeper from the then-Republic of Venice (now part of Italy). He published many books, including the first Italian translations of Archimedes and Euclid, and an acclaimed compilation of mathematics. Tartaglia was the first to apply mathematics to the investigation of the paths of cannonballs, known as ballistics, in his Nova Scientia, “A New Science;” his work was later partially validated and partially superseded by Galileo's studies on falling bodies. He also published a treatise on retrieving sunken ships.
Personal life.
Niccolò Fontana was the son of Michele Fontana, a dispatch rider who travelled to neighboring towns to deliver mail. But in 1506, Michele was murdered by robbers, and Niccolo, his two siblings, and his mother were left impoverished. Niccolò experienced further tragedy in 1512 when the King Louis XII's troops invaded Brescia during the War of the League of Cambrai against Venice. The militia of Brescia defended their city for seven days. When the French finally broke through, they took their revenge by massacring the inhabitants of Brescia. By the end of battle, over 45,000 residents were killed. During the massacre, Niccolò and his family sought sanctuary in the local cathedral. But the French entered and a soldier sliced Niccolò's jaw and palate with a saber and left him for dead. His mother nursed him back to health but the young boy would never recover the power of speech, prompting the nickname "Tartaglia" ("stammerer"). After this he would never shave, and grew a beard to camouflage his scars.
There is a story that Tartaglia learned only half the alphabet from a private tutor before funds ran out, and he had to learn the rest by himself. Be that as it may, he was essentially self-taught. He and his contemporaries, working outside the academies, were responsible for the spread of classical works in modern languages among the educated middle class.
Major works.
His edition of Euclid in 1543, the first translation of the "Elements" into any modern European language, was especially significant. For two centuries Euclid had been taught from two Latin translations taken from an Arabic source; these contained errors in Book V, the Eudoxian theory of proportion, which rendered it unusable. Tartaglia's edition was based on Zamberti's Latin translation of an uncorrupted Greek text, and rendered Book V correctly. He also wrote the first modern and useful commentary on the theory. Later, the theory was an essential tool for Galileo, just as it had been for Archimedes.
However, his best known work is his treatise "General Trattato di numeri, et misure" published in Venice 1556–1560. This has been called the "best" treatise on arithmetic that appeared in the sixteenth century. Not only does Tartaglia have complete discussions of numerical operations and the commercial rules used by Italian arithmeticians in this work, but he also discusses the life of the people, the customs of merchants and the efforts made to improve arithmetic in the 16th century.
Solution to cubic equations.
Tartaglia is perhaps best known today for his conflicts with Gerolamo Cardano. Cardano cajoled Tartaglia into revealing his solution to the cubic equations, by promising not to publish them. Tartaglia divulged the secrets of the solutions of three different forms of the cubic equation in verse. Several years later, Cardano happened to see unpublished work by Scipione del Ferro who independently came up with the same solution as Tartaglia. As the unpublished work was dated before Tartaglia's, Cardano decided his promise could be broken and included Tartaglia's solution in his next publication. Even though Cardano credited his discovery, Tartaglia was extremely upset. He responded by publicly insulting Cardano. Mathematical historians now credit both with the paternity of the formula to solve cubic equations, referring to it as the "Cardano-Tartaglia Formula".
Volume of a tetrahedron.
Tartaglia is also known for having given an expression (Tartaglia's formula) for the volume of a tetrahedron (including any irregular tetrahedra) as the Cayley–Menger determinant of the distance values measured pairwise between its four corners:
where "d" "ij" is the distance between vertices "i" and "j". This is a generalization of Heron's formula for the area of a triangle.

</doc>
<doc id="22149" url="http://en.wikipedia.org/wiki?curid=22149" title="Nagarjuna">
Nagarjuna

Nāgārjuna (Sanskrit: नागार्जुन, Telugu: నాగార్జునుడు, Tibetan: ཀླུ་སྒྲུབ་, Wylie: "klu sgrub"
 , 龍樹 ("Ryūju"), Sinhalese: නාගර්ජුන, c. 150 – c. 250 CE) is widely considered one of the most important Buddhist philosophers after Gautama Buddha. Along with his disciple Āryadeva, he is considered to be the founder of the Madhyamaka school of Mahāyāna Buddhism. Nāgārjuna is also credited with developing the philosophy of the Prajñāpāramitā sūtras and, in some sources, with having revealed these scriptures in the world, having recovered them from the nāgas (snake-people). Furthermore, he is traditionally supposed to have written several treatises on rasayana as well as serving a term as the head of Nālandā.
History.
Very little is reliably known of the life of Nāgārjuna, since the surviving accounts were written in Chinese and Tibetan centuries after his death. According to some accounts, Nāgārjuna was originally from South India. Some scholars believe that Nāgārjuna was an advisor to a king of the Sātavāhana dynasty. Archaeological evidence at Amarāvatī indicates that if this is true, the king may have been Yajña Śrī Śātakarṇi, who ruled between 167 and 196 CE. On the basis of this association, Nāgārjuna is conventionally placed at around 150–250 CE.
According to a 4th/5th-century biography translated by Kumārajīva, Nāgārjuna was born into a Brahmin family in Vidarbha (a region of Maharashtra) and later became a Buddhist.
Some sources claim that in his later years, Nāgārjuna lived on the mountain of Śrīparvata near the city that would later be called Nāgārjunakoṇḍa ("Hill of Nāgārjuna"). The ruins of Nāgārjunakoṇḍa are located in Guntur district, Andhra Pradesh. The Caitika and Bahuśrutīya nikāyas are known to have had monasteries in Nāgārjunakoṇḍa.
Writings.
There exist a number of influential texts attributed to Nāgārjuna though, as there are many pseudepigrapha attributed to him, lively controversy exists over which are his authentic works. The only work that all scholars agree is Nagarjuna's is the "Mūlamadhyamakakārikā" (Fundamental Verses on the Middle Way), which contains the essentials of his thought in twenty-seven chapters.
According to one view, that of Christian Lindtner, the works definitely written by Nagarjuna are:
In addition to works mentioned above, several others are attributed to
Nāgārjuna. There is an ongoing, lively controversy over which of those
works are authentic. Contemporary research suggest that these works belong
to a significantly later period, either to late 8th or early 9th century CE,
and hence can not be authentic works of Nāgārjuna.
However, several works considered important in esoteric Buddhism are
attributed to Nāgārjuna and his disciples by traditional historians
like Tāranātha from 17th century Tibet. These historians try to account
for chronological difficulties with various theories. For example a
propagation of later writings via mystical revelation. For a useful
summary of this tradition, see Wedemeyer 2007.
Lindtner considers that the "Māhaprajñāparamitopadeśa" "Commentary on the Great Perfection of Wisdom" is not a genuine work of Nāgārjuna. This work is only attested in a Chinese translation by Kumārajīva.There is much discussion as to whether this is a work of Nāgārjuna, or someone else. Étienne Lamotte, who translated one third of the work into French, felt that it was the work of a North Indian bhikṣu of the Sarvāstivāda school who later became a convert to the Mahayana. The Chinese scholar-monk Yin Shun felt that it was the work of a South Indian and that Nāgārjuna was quite possibly the author. These two views are not necessarily in opposition and a South Indian Nāgārjuna could well have studied the northern Sarvāstivāda. Neither of the two felt that it was composed by Kumārajīva, which others have suggested.
Philosophy.
From studying his writings, it is clear that Nāgārjuna was conversant with many of the Śrāvaka philosophies and with the Mahāyāna tradition. However, determining Nāgārjuna's affiliation with a specific nikāya is difficult, considering much of this material has been lost. If the most commonly accepted attribution of texts (that of Christian Lindtner) holds, then he was clearly a Māhayānist, but his philosophy holds assiduously to the Śrāvaka "Tripiṭaka", and while he does make explicit references to Mahāyāna texts, he is always careful to stay within the parameters set out by the Śrāvaka canon.
Nagarjuna may have arrived at his positions from a desire to achieve a consistent exegesis of the Buddha's doctrine as recorded in the āgamas. In the eyes of Nagarjuna, the Buddha was not merely a forerunner, but the very founder of the Madhyamaka system. David Kalupahana sees Nagarjuna as a successor to Moggaliputta-Tissa in being a champion of the middle-way and a reviver of the original philosophical ideals of the Buddha.
Sunyata.
Nāgārjuna's primary contribution to Buddhist philosophy is in the use of the concept of śūnyatā, or "emptiness," which brings together other key Buddhist doctrines, particularly anātman "not-self" and pratītyasamutpāda "dependent origination", to refute the metaphysics of the Sarvastivāda and Sautrāntika (extinct non-Mahayana schools). For Nāgārjuna, as for the Buddha in the early texts, it is not merely sentient beings that are "selfless" or non-substantial; all phenomena are without any svabhāva, literally "own-being", "self-nature", or "inherent existence" and thus without any underlying essence. They are "empty" of being independently existent; thus the heterodox theories of svabhāva circulating at the time were refuted on the basis of the doctrines of early Buddhism. This is so because all things arise always dependently: not by their own power, but by depending on conditions leading to their coming into existence, as opposed to being. As part of his analysis of the emptiness of phenomena in the Mūlamadhyamakakārikā, Nagarjuna critiques svabhāva in several different concepts. He discusses the problems of positing any sort of inherent essence to causation, movement, change and personal identity. Nagarjuna makes use of the Indian logical tool of the tetralemma to attack any essentialist conceptions. Nagarjuna’s logical analysis is based on four basic propositions:
To say that all things are 'empty' is to deny any kind of ontological foundation, therefore Nagarjuna's view is often seen as a kind of ontological anti-foundationalism or a metaphysical anti-realism.
Understanding the nature of the emptiness of phenomena is simply a means to an end, which is nirvana. Thus Nagarjuna's philosophical project is ultimately a soteriological one meant to correct our everyday cognitive processes which mistakenly posits svabhāva on the flow of experience.
Two truths.
Nāgārjuna was also instrumental in the development of the two truths doctrine, which claims that there are two levels of truth or reality in Buddhist teaching, the ultimate reality ("paramārtha satya") and the conventionally or superficial reality ("saṃvṛtisatya"). The ultimate truth to Nagarjuna is the truth that everything is empty of essence, this includes emptiness itself ('the emptiness of emptiness'). While some (Murti, 1955) have interpreted this by positing Nagarjuna as a Neo-Kantian and thus making ultimate truth a metaphysical noumenon or an "ineffable ultimate that transcends the capacities of discursive reason", others such as Mark Siderits and Jay Garfield have argued that Nagarjuna's view is that "the ultimate truth is that there is no ultimate truth" (Siderits) and that Nagarjuna is a "semantic anti-dualist" who posits that there are only conventional truths. Hence according to Garfield:
Suppose that we take a conventional entity, such as a table. We analyze it to demonstrate its emptiness, finding that there is no table apart from its parts […]. So we conclude that it is empty. But now let us analyze that emptiness […]. What do we find? Nothing at all but the table’s lack of inherent existence. […]. To see the table as empty […] is to see the table as conventional, as dependent.
In articulating this notion in the "Mūlamadhyamakakārikā", Nāgārjuna drew on an early source in the "Kaccānagotta Sutta", which distinguishes definitive meaning ("nītārtha") from interpretable meaning ("neyārtha"):
By and large, Kaccayana, this world is supported by a polarity, that of existence and non-existence. But when one reads the origination of the world as it actually is with right discernment, "non-existence" with reference to the world does not occur to one. When one reads the cessation of the world as it actually is with right discernment, "existence" with reference to the world does not occur to one.
By and large, Kaccayana, this world is in bondage to attachments, clingings (sustenances), and biases. But one such as this does not get involved with or cling to these attachments, clingings, fixations of awareness, biases, or obsessions; nor is he resolved on "my self". He has no uncertainty or doubt that just stress, when arising, is arising; stress, when passing away, is passing away. In this, his knowledge is independent of others. It's to this extent, Kaccayana, that there is right view.
"Everything exists": That is one extreme. "Everything doesn't exist": That is a second extreme. Avoiding these two extremes, the Tathagata teaches the Dhamma via the middle...
Relativity.
Nagarjuna also taught the idea of relativity; in the Ratnāvalī, he gives the example that shortness exists only in relation to the idea of length. The determination of a thing or object is only possible in relation to other things or objects, especially by way of contrast. He held that the relationship between the ideas of "short" and "long" is not due to intrinsic nature (svabhāva). This idea is also found in the Pali Nikāyas and Chinese Āgamas, in which the idea of relativity is expressed similarly: "That which is the element of light ... is seen to exist on account of [in relation to] darkness; that which is the element of good is seen to exist on account of bad; that which is the element of space is seen to exist on account of form."
Nagarjuna as Ayurvedic physician.
According to Frank John Ninivaggi, Nagarjuna was also a practitioner of Ayurveda. First described in the Sanskrit medical treatise "Sushruta Samhita", of which he was the compiler of the redaction, many of his conceptualisations, such as his descriptions of the circulatory system and blood tissue (described as rakta dhātu) and his pioneering work on the therapeutic value of specially treated minerals knowns as "bhasmas", which earned him the title of the "father of iatrochemistry".
Influence.
According to Jay Garfield, Nagarjuna is a 'titanic figure' in the history of Mahayana Buddhism:
...his inﬂuence in the Mahayana Buddhist world is not only unparalleled in that tradition but exceeds in that tradition the inﬂuence of any single Western philosopher. The degree to which he is taken seriously by so many eminent Indian, Chinese, Tibetan, Korean, Japanese, and Vietnamese philosophers, and lately by so many Western philosophers, alone justifies attention to his corpus.
Also Gadjin M. Nagao writes: 
Nagarjuna who lived around the second or third C.E., was a great philosopher and monk-scholar second only to the Buddha. It was owing to him that Mahāyāna Buddhism got a firm philosophical foundation and almost all forms of Mahāyāna schools of later times regard and accept him as their founder.
In contrast, Richard P. Hayes writes:
Nagarjuna's writings had relatively little effect on the course of subsequent Indian Buddhist philosophy. Despite his apparent attempts to discredit some of the most fundamental concepts of abhidharma, abhidharma continued to ﬂourish for centuries, without any appreciable attempt on the part of abhidharmikas to defend their methods of analysis against Nagarjuna's criticisms. And despite Nagarjuna's radical critique of the very possibility of having grounded knowledge (pramana), the epistemological school of Dignaga and Dharmakirti dominated Indian Buddhist intellectual circles, again without any explicit attempt to answer Nagarjuna's criticisms of their agenda. Aside from a few commentators on Nagarjuna's works, who identiﬁed themselves as Madhyamikas, Indian Buddhist intellectual life continued almost as if Nagarjuna had never existed.
Iconography.
Nāgārjuna is often depicted in composite form comprising human and naga characteristics. Often the naga aspect forms a canopy crowning and shielding his human head. The notion of the naga is found throughout Indian religious culture, and typically signifies an intelligent serpent or dragon, who is responsible for the rains, lakes and other bodies of water. In Buddhism, it is a synonym for a realised arhat, or wise person in general. The term also means "elephant".

</doc>
<doc id="22151" url="http://en.wikipedia.org/wiki?curid=22151" title="Nuclear reactor">
Nuclear reactor

A nuclear reactor, formerly known as atomic pile, is a device used to initiate and control a sustained nuclear chain reaction. Nuclear reactors are used at nuclear power plants for electricity generation and in propulsion of ships. Heat from nuclear fission is passed to a working fluid (water or gas), which runs through turbines. These either drive a ship's propellers or turn electrical generators. Nuclear generated steam in principle can be used for industrial process heat or for district heating. Some reactors are used to produce isotopes for medical and industrial use, or for production of weapons-grade plutonium. Some are run only for research. Today there are about 450 nuclear power reactors that are used to generate electricity in about 30 countries around the world.
Mechanism.
Just as conventional power-stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear reactors convert the energy released by controlled nuclear fission into thermal energy for further conversion to mechanical or electrical forms.
Fission.
When a large fissile atomic nucleus such as uranium-235 or plutonium-239 absorbs a neutron, it may undergo nuclear fission. The heavy nucleus splits into two or more lighter nuclei, (the fission products), releasing kinetic energy, gamma radiation, and free neutrons. A portion of these neutrons may later be absorbed by other fissile atoms and trigger further fission events, which release more neutrons, and so on. This is known as a nuclear chain reaction.
To control such a nuclear chain reaction, neutron poisons and neutron moderators can change the portion of neutrons that will go on to cause more fission. Nuclear reactors generally have automatic and manual systems to shut the fission reaction down if monitoring detects unsafe conditions.
Commonly-used moderators include regular (light) water (in 74.8% of the world's reactors), solid graphite (20% of reactors) and heavy water (5% of reactors). Some experimental types of reactor have used beryllium, and hydrocarbons have been suggested as another possibility.
Heat generation.
The reactor core generates heat in a number of ways:
A kilogram of uranium-235 (U-235) converted via nuclear processes releases approximately three million times more energy than a kilogram of coal burned conventionally (7.2 × 1013 joules per kilogram of uranium-235 versus 2.4 × 107 joules per kilogram of coal).
Cooling.
A nuclear reactor coolant — usually water but sometimes a gas or a liquid metal (like liquid sodium) or molten salt — is circulated past the reactor core to absorb the heat that it generates. The heat is carried away from the reactor and is then used to generate steam. Most reactor systems employ a cooling system that is physically separated from the water that will be boiled to produce pressurized steam for the turbines, like the pressurized water reactor. However, in some reactors the water for the steam turbines is boiled directly by the reactor core; for example the boiling water reactor.
Reactivity control.
The power output of the reactor is adjusted by controlling how many neutrons are able to create more fissions.
Control rods that are made of a neutron poison are used to absorb neutrons. Absorbing more neutrons in a control rod means that there are fewer neutrons available to cause fission, so pushing the control rod deeper into the reactor will reduce its power output, and extracting the control rod will increase it.
At the first level of control in all nuclear reactors, a process of delayed neutron emission by a number of neutron-rich fission isotopes is an important physical process. These delayed neutrons account for about 0.65% of the total neutrons produced in fission, with the remainder (termed "prompt neutrons") released immediately upon fission. The fission products which produce delayed neutrons have half lives for their decay by neutron emission that range from milliseconds to as long as several minutes, and so considerable time is required to determine exactly when a reactor reaches the critical point. Keeping the reactor in the zone of chain-reactivity where delayed neutrons are "necessary" to achieve a critical mass state allows mechanical devices or human operators to control a chain reaction in "real time"; otherwise the time between achievement of criticality and nuclear meltdown as a result of an exponential power surge from the normal nuclear chain reaction, would be too short to allow for intervention. This last stage, where delayed neutrons are no longer required to maintain criticality, is known as the prompt critical point. There is a scale for describing criticality in numerical form, in which bare criticality is known as "zero dollars" and the prompt critical point is "one dollar", and other points in the process interpolated in cents.
In some reactors, the coolant also acts as a neutron moderator. A moderator increases the power of the reactor by causing the fast neutrons that are released from fission to lose energy and become thermal neutrons. Thermal neutrons are more likely than fast neutrons to cause fission. If the coolant is a moderator, then temperature changes can affect the density of the coolant/moderator and therefore change power output. A higher temperature coolant would be less dense, and therefore a less effective moderator.
In other reactors the coolant acts as a poison by absorbing neutrons in the same way that the control rods do. In these reactors power output can be increased by heating the coolant, which makes it a less dense poison. Nuclear reactors generally have automatic and manual systems to scram the reactor in an emergency shut down. These systems insert large amounts of poison (often boron in the form of boric acid) into the reactor to shut the fission reaction down if unsafe conditions are detected or anticipated.
Most types of reactors are sensitive to a process variously known as xenon poisoning, or the iodine pit. The common fission product Xenon-135 produced in the fission process acts as a "neutron poison" that absorbs neutrons and therefore tends to shut the reactor down. Xenon-135 accumulation can be controlled by keeping power levels high enough to destroy it by neutron absorption as fast as it is produced. Fission also produces iodine-135, which in turn decays (with a half-life of 6.57 hours) to new xenon-135. When the reactor is shut down, iodine-135 continues to decay to xenon-135, making restarting the reactor more difficult for a day or two, as the xenon-135 decays into cesium-135, which is not nearly as poisonous as xenon-135, with a half-life of 9.2 hours. This temporary state is the "iodine pit." If the reactor has sufficient extra reactivity capacity, it can be restarted. As the extra xenon-135 is transmuted to xenon-136, which is much less a neutron poison, within a few hours the reactor experiences a "xenon burnoff (power) transient". Control rods must be further inserted to replace the neutron absorption of the lost xenon-135. Failure to properly follow such a procedure was a key step in the Chernobyl disaster.
Reactors used in nuclear marine propulsion (especially nuclear submarines) often cannot be run at continuous power around the clock in the same way that land-based power reactors are normally run, and in addition often need to have a very long core life without refueling. For this reason many designs use highly enriched uranium but incorporate burnable neutron poison in the fuel rods. This allows the reactor to be constructed with an excess of fissionable material, which is nevertheless made relatively safe early in the reactor's fuel burn-cycle by the presence of the neutron-absorbing material which is later replaced by normally produced long-lived neutron poisons (far longer-lived than xenon-135) which gradually accumulate over the fuel load's operating life.
Electrical power generation.
The energy released in the fission process generates heat, some of which can be converted into usable energy. A common method of harnessing this thermal energy is to use it to boil water to produce pressurized steam which will then drive a steam turbine that turns an alternator and generates electricity.
Early reactors.
The neutron was discovered in 1932. The concept of a nuclear chain reaction brought about by nuclear reactions mediated by neutrons was first realized shortly thereafter, by Hungarian scientist Leó Szilárd, in 1933. He filed a patent for his idea of a simple nuclear reactor the following year while working at the Admiralty in London. However, Szilárd's idea did not incorporate the idea of nuclear fission as a neutron source, since that process was not yet discovered. Szilárd's ideas for nuclear reactors using neutron-mediated nuclear chain reactions in light elements proved unworkable.
Inspiration for a new type of reactor using uranium came from the discovery by Lise Meitner, Fritz Strassmann and Otto Hahn in 1938 that bombardment of uranium with neutrons (provided by an alpha-on-beryllium fusion reaction, a "neutron howitzer") produced a barium residue, which they reasoned was created by the fissioning of the uranium nuclei. Subsequent studies in early 1939 (one of them by Szilárd and Fermi) revealed that several neutrons were also released during the fissioning, making available the opportunity for the nuclear chain reaction that Szilárd had envisioned six years previously.
On 2 August 1939 Albert Einstein signed a letter to President Franklin D. Roosevelt (written by Szilárd) suggesting that the discovery of uranium's fission could lead to the development of "extremely powerful bombs of a new type", giving impetus to the study of reactors and fission. Szilárd and Einstein knew each other well and had worked together years previously, but Einstein had never thought about this possibility for nuclear energy until Szilard reported it to him, at the beginning of his quest to produce the Einstein-Szilárd letter to alert the U.S. government.
Shortly after, Hitler's Germany invaded Poland in 1939, starting World War II in Europe. The U.S. was not yet officially at war, but in October, when the Einstein-Szilárd letter was delivered to him, Roosevelt commented that the purpose of doing the research was to make sure "the Nazis don't blow us up." The U.S. nuclear project followed, although with some delay as there remained skepticism (some of it from Fermi) and also little action from the small number of officials in the government who were initially charged with moving the project forward.
The following year the U.S. Government received the Frisch–Peierls memorandum from the UK, which stated that the amount of uranium needed for a chain reaction was far lower than had previously been thought. The memorandum was a product of the MAUD Committee, which was working on the UK atomic bomb project, known as Tube Alloys, later to be subsumed within the Manhattan Project.
Eventually, the first artificial nuclear reactor, Chicago Pile-1, was constructed at the University of Chicago, by a team led by Enrico Fermi, in late 1942. By this time, the program had been pressured for a year by U.S. entry into the war. The Chicago Pile achieved criticality on 2 December 1942 at 3:25 PM. The reactor support structure was made of wood, which supported a pile (hence the name) of graphite blocks, embedded in which was natural uranium-oxide 'pseudospheres' or 'briquettes'.
Soon after the Chicago Pile, the U.S. military developed a number of nuclear reactors for the Manhattan Project starting in 1943. The primary purpose for the largest reactors (located at the Hanford Site in Washington state), was the mass production of plutonium for nuclear weapons. Fermi and Szilard applied for a patent on reactors on 19 December 1944. Its issuance was delayed for 10 years because of wartime secrecy.
"World's first nuclear power plant" is the claim made by signs at the site of the EBR-I, which is now a museum near Arco, Idaho. Originally called "Chicago Pile-4", it was carried out under the direction of Walter Zinn for Argonne National Laboratory. This experimental LMFBR operated by the U.S. Atomic Energy Commission produced 0.8 kW in a test on 20 December 1951 and 100 kW (electrical) the following day, having a design output of 200 kW (electrical).
Besides the military uses of nuclear reactors, there were political reasons to pursue civilian use of atomic energy. U.S. President Dwight Eisenhower made his famous Atoms for Peace speech to the UN General Assembly on 8 December 1953. This diplomacy led to the dissemination of reactor technology to U.S. institutions and worldwide.
The first nuclear power plant built for civil purposes was the AM-1 Obninsk Nuclear Power Plant, launched on 27 June 1954 in the Soviet Union. It produced around 5 MW (electrical).
After World War II, the U.S. military sought other uses for nuclear reactor technology. Research by the Army and the Air Force never came to fruition; however, the U.S. Navy succeeded when they steamed the USS "Nautilus" (SSN-571) on nuclear power 17 January 1955.
The first commercial nuclear power station, Calder Hall in Sellafield, England was opened in 1956 with an initial capacity of 50 MW (later 200 MW).
The first portable nuclear reactor "Alco PM-2A" used to generate electrical power (2 MW) for Camp Century from 1960.
Components.
The key components common to most types of nuclear power plants are:
Reactor types.
Classifications.
Nuclear Reactors are classified by several methods; a brief outline of these classification methods is provided.
Classification by moderator material.
Used by thermal reactors:
Classification by generation.
The "Gen IV"-term was dubbed by the United States Department of Energy (DOE) for developing new plant types in 2000. In 2003, the French Commissariat à l'Énergie Atomique (CEA) was the first to refer to Gen II types in Nucleonics Week; first mentioning of Gen III was also in 2000 in conjunction with the launch of the Generation IV International Forum (GIF) plans.
Future and developing technologies.
Advanced reactors.
More than a dozen advanced reactor designs are in various stages of development. Some are evolutionary from the PWR, BWR and PHWR designs above, some are more radical departures. The former include the advanced boiling water reactor (ABWR), two of which are now operating with others under construction, and the planned passively safe Economic Simplified Boiling Water Reactor (ESBWR) and AP1000 units (see Nuclear Power 2010 Program).
Generation IV reactors.
Generation IV reactors are a set of theoretical nuclear reactor designs currently being researched. These designs are generally not expected to be available for commercial construction before 2030. Current reactors in operation around the world are generally considered second- or third-generation systems, with the first-generation systems having been retired some time ago. Research into these reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals. The primary goals being to improve nuclear safety, improve proliferation resistance, minimize waste and natural resource utilization, and to decrease the cost to build and run such plants.
Generation V+ reactors.
Generation V reactors are designs which are theoretically possible, but which are not being actively considered or researched at present. Though such reactors could be built with current or near term technology, they trigger little interest for reasons of economics, practicality, or safety.
Fusion reactors.
Controlled nuclear fusion could in principle be used in fusion power plants to produce power without the complexities of handling actinides, but significant scientific and technical obstacles remain. Several fusion reactors have been built, but only recently reactors have been able to release more energy than the amount of energy used in the process. Despite research having started in the 1950s, no commercial fusion reactor is expected before 2050. The ITER project is currently leading the effort to harness fusion power.
Nuclear fuel cycle.
Thermal reactors generally depend on refined and enriched uranium. Some nuclear reactors can operate with a mixture of plutonium and uranium (see MOX). The process by which uranium ore is mined, processed, enriched, used, possibly reprocessed and disposed of is known as the nuclear fuel cycle.
Under 1% of the uranium found in nature is the easily fissionable U-235 isotope and as a result most reactor designs require enriched fuel.
Enrichment involves increasing the percentage of U-235 and is usually done by means of gaseous diffusion or gas centrifuge. The enriched result is then converted into uranium dioxide powder, which is pressed and fired into pellet form. These pellets are stacked into tubes which are then sealed and called fuel rods. Many of these fuel rods are used in each nuclear reactor.
Most BWR and PWR commercial reactors use uranium enriched to about 4% U-235, and some commercial reactors with a high neutron economy do not require the fuel to be enriched at all (that is, they can use natural uranium). According to the International Atomic Energy Agency there are at least 100 research reactors in the world fueled by highly enriched (weapons-grade/90% enrichment uranium). Theft risk of this fuel (potentially used in the production of a nuclear weapon) has led to campaigns advocating conversion of this type of reactor to low-enrichment uranium (which poses less threat of proliferation).
Fissile U-235 and non-fissile but fissionable and fertile U-238 are both used in the fission process. U-235 is fissionable by thermal (i.e. slow-moving) neutrons. A thermal neutron is one which is moving about the same speed as the atoms around it. Since all atoms vibrate proportionally to their absolute temperature, a thermal neutron has the best opportunity to fission U-235 when it is moving at this same vibrational speed. On the other hand, U-238 is more likely to capture a neutron when the neutron is moving very fast. This U-239 atom will soon decay into plutonium-239, which is another fuel. Pu-239 is a viable fuel and must be accounted for even when a highly enriched uranium fuel is used. Plutonium fissions will dominate the U-235 fissions in some reactors, especially after the initial loading of U-235 is spent. Plutonium is fissionable with both fast and thermal neutrons, which make it ideal for either nuclear reactors or nuclear bombs.
Most reactor designs in existence are thermal reactors and typically use water as a neutron moderator (moderator means that it slows down the neutron to a thermal speed) and as a coolant. But in a fast breeder reactor, some other kind of coolant is used which will not moderate or slow the neutrons down much. This enables fast neutrons to dominate, which can effectively be used to constantly replenish the fuel supply. By merely placing cheap unenriched uranium into such a core, the non-fissionable U-238 will be turned into Pu-239, "breeding" fuel.
In thorium fuel cycle thorium-232 absorbs a neutron in either a fast or thermal reactor. The thorium-233 beta decays to protactinium-233 and then to uranium-233, which in turn is used as fuel. Hence, like uranium-238, thorium-232 is a fertile material.
Fueling of nuclear reactors.
The amount of energy in the reservoir of nuclear fuel is frequently expressed in terms of "full-power days," which is the number of 24-hour periods (days) a reactor is scheduled for operation at full power output for the generation of heat energy. The number of full-power days in a reactor's operating cycle (between refueling outage times) is related to the amount of fissile uranium-235 (U-235) contained in the fuel assemblies at the beginning of the cycle. A higher percentage of U-235 in the core at the beginning of a cycle will permit the reactor to be run for a greater number of full-power days.
At the end of the operating cycle, the fuel in some of the assemblies is "spent" and is discharged and replaced with new (fresh) fuel assemblies, although in practice it is the buildup of reaction poisons in nuclear fuel that determines the lifetime of nuclear fuel in a reactor. Long before all possible fission has taken place, the buildup of long-lived neutron absorbing fission byproducts impedes the chain reaction. The fraction of the reactor's fuel core replaced during refueling is typically one-fourth for a boiling-water reactor and one-third for a pressurized-water reactor. The disposition and storage of this spent fuel is one of the most challenging aspects of the operation of a commercial nuclear power plant. This nuclear waste is highly radioactive and its toxicity presents a danger for thousands of years.
Not all reactors need to be shut down for refueling; for example, pebble bed reactors, RBMK reactors, molten salt reactors, Magnox, AGR and CANDU reactors allow fuel to be shifted through the reactor while it is running. In a CANDU reactor, this also allows individual fuel elements to be situated within the reactor core that are best suited to the amount of U-235 in the fuel element.
The amount of energy extracted from nuclear fuel is called its burnup, which is expressed in terms of the heat energy produced per initial unit of fuel weight. Burn up is commonly expressed as megawatt days thermal per metric ton of initial heavy metal.
Safety.
Nuclear safety covers the actions taken to prevent nuclear and radiation accidents or to limit their consequences. The nuclear power industry has improved the safety and performance of reactors, and has proposed new safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly. Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable. An interdisciplinary team from MIT have estimated that given the expected growth of nuclear power from 2005–2055, at least four serious nuclear accidents would be expected in that period.
Accidents.
Some serious nuclear and radiation accidents have occurred. Nuclear power plant accidents include the Chernobyl disaster (1986), Fukushima Daiichi nuclear disaster (2011), the Three Mile Island accident (1979), and SL-1 accident (1961). Nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985).
Nuclear reactors have been launched into Earth orbit at least 34 times. A number of incidents connected with the unmanned nuclear-reactor-powered Soviet RORSAT radar satellite program resulted in spent nuclear fuel re-entering the Earth's atmosphere from orbit.
Natural nuclear reactors.
Although nuclear fission reactors are often thought of as being solely a product of modern technology, the first nuclear fission reactors were in fact naturally occurring. A natural nuclear fission reactor can occur under certain circumstances that mimic the conditions in a constructed reactor. Fifteen natural fission reactors have so far been found in three separate ore deposits at the Oklo uranium mine in Gabon, West Africa. First discovered in 1972 by French physicist Francis Perrin, they are collectively known as the Oklo Fossil Reactors. Self-sustaining nuclear fission reactions took place in these reactors approximately 1.5 billion years ago, and ran for a few hundred thousand years, averaging 100 kW of power output during that time. The concept of a natural nuclear reactor was theorized as early as 1956 by Paul Kuroda at the University of Arkansas.
Such reactors can no longer form on Earth: radioactive decay over this immense time span has reduced the proportion of U-235 in naturally occurring uranium to below the amount required to sustain a chain reaction.
The natural nuclear reactors formed when a uranium-rich mineral deposit became inundated with groundwater that acted as a neutron moderator, and a strong chain reaction took place. The water moderator would boil away as the reaction increased, slowing it back down again and preventing a meltdown. The fission reaction was sustained for hundreds of thousands of years.
These natural reactors are extensively studied by scientists interested in geologic radioactive waste disposal. They offer a case study of how radioactive isotopes migrate through the Earth's crust. This is a significant area of controversy as opponents of geologic waste disposal fear that isotopes from stored waste could end up in water supplies or be carried into the environment.

</doc>
<doc id="22153" url="http://en.wikipedia.org/wiki?curid=22153" title="Nuclear power">
Nuclear power

Nuclear power is the use of nuclear reactors to release nuclear energy, and thereby generate electricity. The term includes nuclear fission, nuclear decay and nuclear fusion. Presently, the nuclear fission of elements in the actinide series of the periodic table produce the vast majority of nuclear energy in the direct service of humankind, with nuclear decay processes, primarily in the form of geothermal energy, and radioisotope thermoelectric generators, in niche uses making up the rest. Nuclear (fission) power stations, excluding the contribution from naval nuclear fission reactors, provided 13% of the world's electricity in 2012. The share of the world's primary energy supply, which refers to the
heat production without the conversion efficiency of about 33 %, was about 5.7%. Its share of the global final energy consumption (actually useful energy, i.e. electric power) is below 2.5 %. 
In 2013, the IAEA report that there are 437 operational nuclear power reactors, in 31 countries, although not every reactor is producing electricity. In addition, there are approximately 140 naval vessels using nuclear propulsion in operation, powered by some 180 reactors. As of 2013, attaining a net energy gain from sustained nuclear fusion reactions, excluding natural fusion power sources such as the Sun, remains an ongoing area of international physics and engineering research. More than 60 years after the first attempts, commercial fusion power production remains unlikely before 2050.
There is an ongoing debate about nuclear power. Proponents, such as the World Nuclear Association, the IAEA and Environmentalists for Nuclear Energy contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. Opponents, such as Greenpeace International and NIRS, contend that nuclear power poses many threats to people and the environment.
Nuclear power plant accidents include the Chernobyl disaster (1986), Fukushima Daiichi nuclear disaster (2011), and the Three Mile Island accident (1979). There have also been some nuclear submarine accidents. In terms of lives lost per unit of energy generated, analysis has determined that nuclear power has caused less fatalities per unit of energy generated than the other major sources of energy generation. Energy production from coal, petroleum, natural gas and hydropower has caused a greater number of fatalities per unit of energy generated due to air pollution and energy accident effects. However, the economic costs of nuclear power accidents is high, and meltdowns can render areas uninhabitable for very long periods. The human costs of evacuations of affected populations and lost livelihoods is also significant.
Along with other sustainable energy sources, nuclear power is a low carbon power generation method of producing electricity, with an analysis of the literature on its total life cycle emission intensity finding that it is similar to other renewable sources in a comparison of greenhouse gas(GHG) emissions per unit of energy generated. With this translating into, from the beginning of nuclear power station commercialization in the 1970s, having prevented the emission of approximately 64 gigatonnes of carbon dioxide equivalent(GtCO2-eq) greenhouse gases, gases that would have otherwise resulted from the burning of fossil fuels in thermal power stations.
As of 2012, according to the IAEA, worldwide there were 68 civil nuclear power reactors under construction in 15 countries, approximately 28 of which in the Peoples Republic of China (PRC), with the most recent nuclear power reactor, as of May 2013, to be connected to the electrical grid, occurring on February 17, 2013 in Hongyanhe Nuclear Power Plant in the PRC. In the USA, two new Generation III reactors are under construction at Vogtle. U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed.
Japan's 2011 Fukushima Daiichi nuclear disaster, which occurred in a reactor design from the 1960s, prompted a re-examination of nuclear safety and nuclear energy policy in many countries. Germany decided to close all its reactors by 2022, and Italy has banned nuclear power. Following Fukushima, in 2011 the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035.
Use.
In 2011 nuclear power provided 10% of the world's electricity In 2007, the IAEA reported there were 439 nuclear power reactors in operation in the world, operating in 31 countries. However, many have now ceased operation in the wake of the Fukushima nuclear disaster while they are assessed for safety. In 2011 worldwide nuclear output fell by 4.3%, the largest decline on record, on the back of sharp declines in Japan (-44.3%) and Germany (-23.2%).
Since commercial nuclear energy began in the mid-1950s, 2008 was the first year that no new nuclear power plant was connected to the grid, although two were connected in 2009.
Annual generation of nuclear power has been on a slight downward trend since 2007, decreasing 1.8% in 2009 to 2558 TWh with nuclear power meeting 13–14% of the world's electricity demand. One factor in the nuclear power percentage decrease since 2007 has been the prolonged shutdown of large reactors at the Kashiwazaki-Kariwa Nuclear Power Plant in Japan following the Niigata-Chuetsu-Oki earthquake.
The United States produces the most nuclear energy, with nuclear power providing 19% of the electricity it consumes, while France produces the highest percentage of its electrical energy from nuclear reactors—80% as of 2006. In the European Union as a whole, nuclear energy provides 30% of the electricity. Nuclear energy policy differs among European Union countries, and some, such as Austria, Estonia, Ireland and Italy, have no active nuclear power stations. In comparison, France has a large number of these plants, with 16 multi-unit stations in current use.
In the US, while the coal and gas electricity industry is projected to be worth $85 billion by 2013, nuclear power generators are forecast to be worth $18 billion.
Many military and some civilian (such as some icebreaker) ships use nuclear marine propulsion, a form of nuclear propulsion. A few space vehicles have been launched using full-fledged nuclear reactors: 33 reactors belong to the Soviet RORSAT series and one was the American SNAP-10A.
International research is continuing into safety improvements such as passively safe plants, the use of nuclear fusion, and additional uses of process heat such as hydrogen production (in support of a hydrogen economy), for desalinating sea water, and for use in district heating systems.
Use in space.
Both fission and fusion appear promising for space propulsion applications, generating higher mission velocities with less reaction mass. This is due to the much higher energy density of nuclear reactions: some 7 orders of magnitude (10,000,000 times) more energetic than the chemical reactions which power the current generation of rockets.
Radioactive decay has been used on a relatively small scale (few kW), mostly to power space missions and experiments by using radioisotope thermoelectric generators such as those developed at Idaho National Laboratory.
History.
Origins.
The pursuit of nuclear energy for electricity generation began soon after the discovery in the early 20th century that radioactive elements, such as radium, released immense amounts of energy, according to the principle of mass–energy equivalence. However, means of harnessing such energy was impractical, because intensely radioactive elements were, by their very nature, short-lived (high energy release is correlated with short half-lives). However, the dream of harnessing "atomic energy" was quite strong, even though it was dismissed by such fathers of nuclear physics like Ernest Rutherford as "moonshine." This situation, however, changed in the late 1930s, with the discovery of nuclear fission.
In 1932, James Chadwick discovered the neutron, which was immediately recognized as a potential tool for nuclear experimentation because of its lack of an electric charge. Experimentation with bombardment of materials with neutrons led Frédéric and Irène Joliot-Curie to discover induced radioactivity in 1934, which allowed the creation of radium-like elements at much less the price of natural radium. Further work by Enrico Fermi in the 1930s focused on using slow neutrons to increase the effectiveness of induced radioactivity. Experiments bombarding uranium with neutrons led Fermi to believe he had created a new, transuranic element, which was dubbed hesperium.
But in 1938, German chemists Otto Hahn and Fritz Strassmann, along with Austrian physicist Lise Meitner and Meitner's nephew, Otto Robert Frisch, conducted experiments with the products of neutron-bombarded uranium, as a means of further investigating Fermi's claims. They determined that the relatively tiny neutron split the nucleus of the massive uranium atoms into two roughly equal pieces, contradicting Fermi. This was an extremely surprising result: all other forms of nuclear decay involved only small changes to the mass of the nucleus, whereas this process—dubbed "fission" as a reference to biology—involved a complete rupture of the nucleus. Numerous scientists, including Leó Szilárd, who was one of the first, recognized that if fission reactions released additional neutrons, a self-sustaining nuclear chain reaction could result. Once this was experimentally confirmed and announced by Frédéric Joliot-Curie in 1939, scientists in many countries (including the United States, the United Kingdom, France, Germany, and the Soviet Union) petitioned their governments for support of nuclear fission research, just on the cusp of World War II, for the development of a nuclear weapon.
In the United States, where Fermi and Szilárd had both emigrated, this led to the creation of the first man-made reactor, known as Chicago Pile-1, which achieved criticality on December 2, 1942. This work became part of the Manhattan Project, which made enriched uranium and built large reactors to breed plutonium for use in the first nuclear weapons, which were used on the cities of Hiroshima and Nagasaki.
Unexpectedly high costs in the U.S. nuclear weapons program, along with competition with the Soviet Union and a desire to spread democracy through the world, created "...pressure on federal officials to develop a civilian nuclear power industry that could help justify the government's considerable expenditures." In 1945, the pocketbook "The Atomic Age" heralded the untapped atomic power in everyday objects and depicted a future where fossil fuels would go unused. One science writer, David Dietz, wrote that instead of filling the gas tank of your car two or three times a week, you will travel for a year on a pellet of atomic energy the size of a vitamin pill. Glenn Seaborg, who chaired the Atomic Energy Commission, wrote "there will be nuclear powered earth-to-moon shuttles, nuclear powered artificial hearts, plutonium heated swimming pools for SCUBA divers, and much more". These overly optimistic predications remain unfulfilled.
United Kingdom, Canada, and USSR proceeded over the course of the late 1940s and early 1950s. Electricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW. Work was also strongly researched in the US on nuclear marine propulsion, with a test reactor being developed by 1953 (eventually, the USS Nautilus, the first nuclear-powered submarine, would launch in 1955). In 1953, US President Dwight Eisenhower gave his "Atoms for Peace" speech at the United Nations, emphasizing the need to develop "peaceful" uses of nuclear power quickly. This was followed by the 1954 Amendments to the Atomic Energy Act which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector. This involved a significant learning phase, with many early partial core meltdowns and accidents at experimental reactors and research facilities.
Early years.
On June 27, 1954, the USSR's Obninsk Nuclear Power Plant became the world's first nuclear power plant to generate electricity for a power grid, and produced around 5 megawatts of electric power.
Later in 1954, Lewis Strauss, then chairman of the United States Atomic Energy Commission (U.S. AEC, forerunner of the U.S. Nuclear Regulatory Commission and the United States Department of Energy) spoke of electricity in the future being "too cheap to meter". Strauss was very likely referring to hydrogen fusion —which was secretly being developed as part of Project Sherwood at the time—but Strauss's statement was interpreted as a promise of very cheap energy from nuclear fission. The U.S. AEC itself had issued far more realistic testimony regarding nuclear fission to the U.S. Congress only months before, projecting that "costs can be brought down... [to]... about the same as the cost of electricity from conventional sources..." Significant disappointment would develop later on, when the new nuclear plants did not provide energy "too cheap to meter."
In 1955 the United Nations' "First Geneva Conference", then the world's largest gathering of scientists and engineers, met to explore the technology. In 1957 EURATOM was launched alongside the European Economic Community (the latter is now the European Union). The same year also saw the launch of the International Atomic Energy Agency (IAEA).
The world's first commercial nuclear power station, Calder Hall at Windscale, England, was opened in 1956 with an initial capacity of 50 MW (later 200 MW). The first commercial nuclear generator to become operational in the United States was the Shippingport Reactor (Pennsylvania, December 1957).
One of the first organizations to develop nuclear power was the U.S. Navy, for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, , was put to sea in December 1954. Two U.S. nuclear submarines, USS "Scorpion" and USS "Thresher", have been lost at sea. Eight Soviet and Russian nuclear submarines have been lost at sea. This includes the Soviet submarine K-19 reactor accident in 1961 which resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The Soviet submarine K-27 reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. Moreover, Soviet submarine K-429 sank twice, but was raised after each incident. Several serious nuclear and radiation accidents have involved nuclear submarine mishaps.
The U.S. Army also had a nuclear power program, beginning in 1954. The SM-1 Nuclear Power Plant, at Fort Belvoir, Virginia, was the first power reactor in the U.S. to supply electrical energy to a commercial grid (VEPCO), in April 1957, before Shippingport. The SL-1 was a U.S. Army experimental nuclear power reactor at the National Reactor Testing Station in eastern Idaho. It underwent a steam explosion and meltdown in January 1961, which killed its three operators. In Soviet Union in The Mayak Production Association there were a number of accidents including an explosion that released 50-100 tonnes of high-level radioactive waste, contaminating a huge territory in the eastern Urals and causing numerous deaths and injuries. The Soviet regime kept this accident secret for about 30 years. The event was eventually rated at 6 on the seven-level INES scale (third in severity only to the disasters at Chernobyl and Fukushima).
Development.
Installed nuclear capacity initially rose relatively quickly, rising from less than 1 gigawatt (GW) in 1960 to 100 GW in the late 1970s, and 300 GW in the late 1980s. Since the late 1980s worldwide capacity has risen much more slowly, reaching 366 GW in 2005. Between around 1970 and 1990, more than 50 GW of capacity was under construction (peaking at over 150 GW in the late 1970s and early 1980s) — in 2005, around 25 GW of new capacity was planned. More than two-thirds of all nuclear plants ordered after January 1970 were eventually cancelled. A total of 63 nuclear units were canceled in the USA between 1975 and 1980.
During the 1970s and 1980s rising economic costs (related to extended construction times largely due to regulatory changes and pressure-group litigation) and falling fossil fuel prices made nuclear power plants then under construction less attractive. In the 1980s (U.S.) and 1990s (Europe), flat load growth and electricity liberalization also made the addition of large new baseload capacity unattractive.
The 1973 oil crisis had a significant effect on countries, such as France and Japan, which had relied more heavily on oil for electric generation (39% and 73% respectively) to invest in nuclear power.
Some local opposition to nuclear power emerged in the early 1960s, and in the late 1960s some members of the scientific community began to express their concerns. These concerns related to nuclear accidents, nuclear proliferation, high cost of nuclear power plants, nuclear terrorism and radioactive waste disposal. In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975 and anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America. By the mid-1970s anti-nuclear activism had moved beyond local protests and politics to gain a wider appeal and influence, and nuclear power became an issue of major public protest. Although it lacked a single co-ordinating organization, and did not have uniform goals, the movement's efforts gained a great deal of attention. In some countries, the nuclear power conflict "reached an intensity unprecedented in the history of technology controversies". 
In France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn. In May 1979, an estimated 70,000 people, including then governor of California Jerry Brown, attended a march and rally against nuclear power in Washington, D.C. Anti-nuclear power groups emerged in every country that has had a nuclear power programme. Some of these anti-nuclear power organisations are reported to have developed considerable expertise on nuclear power and energy issues.
Health and safety concerns, the 1979 accident at Three Mile Island, and the 1986 Chernobyl disaster played a part in stopping new plant construction in many countries, although the public policy organization, the Brookings Institution states that new nuclear units, at the time of publishing in 2006, had not been built in the U.S. because of soft demand for electricity, and cost overruns on nuclear plants due to regulatory issues and construction delays. By the end of the 1970s it became clear that nuclear power would not grow nearly as dramatically as once believed. Eventually, more than 120 reactor orders in the U.S. were ultimately cancelled and the construction of new reactors ground to a halt. A cover story in the February 11, 1985, issue of "Forbes" magazine commented on the overall failure of the U.S. nuclear power program, saying it “ranks as the largest managerial disaster in business history”.
Unlike the Three Mile Island accident, the much more serious Chernobyl accident did not increase regulations affecting Western reactors since the Chernobyl reactors were of the problematic RBMK design only used in the Soviet Union, for example lacking "robust" containment buildings. Many of these RBMK reactors are still in use today. However, changes were made in both the reactors themselves (use of a safer enrichment of uranium) and in the control system (prevention of disabling safety systems), amongst other things, to reduce the possibility of a duplicate accident.
An international organization to promote safety awareness and professional development on operators in nuclear facilities was created: WANO; World Association of Nuclear Operators.
Opposition in Ireland and Poland prevented nuclear programs there, while Austria (1978), Sweden (1980) and Italy (1987) (influenced by Chernobyl) voted in referendums to oppose or phase out nuclear power. In July 2009, the Italian Parliament passed a law that cancelled the results of an earlier referendum and allowed the immediate start of the Italian nuclear program. After the Fukushima Daiichi nuclear disaster a one-year moratorium was placed on nuclear power development, followed by a referendum in which over 94% of voters (turnout 57%) rejected plans for new nuclear power.
Nuclear power plant.
Just as many conventional thermal power stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear power plants convert the energy released from the nucleus of an atom via nuclear fission that takes place in a nuclear reactor. The heat is removed from the reactor core by a cooling system that uses the heat to generate steam, which drives a steam turbine connected to a generator producing electricity.
Life cycle.
A nuclear reactor is only part of the life-cycle for nuclear power. The process starts with mining (see "Uranium mining"). Uranium mines are underground, open-pit, or in-situ leach mines. In any case, the uranium ore is extracted, usually converted into a stable and compact form such as yellowcake, and then transported to a processing facility. Here, the yellowcake is converted to uranium hexafluoride, which is then enriched using various techniques. At this point, the enriched uranium, containing more than the natural 0.7% U-235, is used to make rods of the proper composition and geometry for the particular reactor that the fuel is destined for. The fuel rods will spend about 3 operational cycles (typically 6 years total now) inside the reactor, generally until about 3% of their uranium has been fissioned, then they will be moved to a spent fuel pool where the short lived isotopes generated by fission can decay away. After about 5 years in a spent fuel pool the spent fuel is radioactively and thermally cool enough to handle, and it can be moved to dry storage casks or reprocessed.
Conventional fuel resources.
Uranium is a fairly common element in the Earth's crust. Uranium is approximately as common as tin or germanium in the Earth's crust, and is about 40 times more common than silver. Uranium is a constituent of most rocks, dirt, and of the oceans. The fact that uranium is so spread out is a problem because mining uranium is only economically feasible where there is a large concentration. Still, the world's present measured resources of uranium, economically recoverable at a price of 130 USD/kg, are enough to last for between 70 and 100 years.
According to the OECD in 2006, there is an expected 85 years worth of uranium in identified resources, when that uranium is used in present reactor technology, with 670 years of economically recoverable uranium in total conventional resources and phosphate ores, while also using present reactor technology, a resource that is recoverable from between 60-100 US$/kg of Uranium. The OECD have noted that: Even if the nuclear industry expands significantly, sufficient fuel is available for centuries. If advanced breeder reactors could be designed in the future to efficiently utilize recycled or depleted uranium and all actinides, then the resource utilization efficiency would be further improved by an additional factor of eight.
 For example, the OECD have determined that with a pure fast reactor fuel cycle with a burn up of, and recycling of, all the Uranium and actinides, actinides which presently make up the most hazardous substances in nuclear waste, there is 160,000 years worth of Uranium in total conventional resources and phosphate ore. According to the OECD's red book in 2011, due to increased exploration, known uranium resources have grown by 12.5% since 2008, with this increase translating into greater than a century of uranium available if the metals usage rate were to continue at the 2011 level.
Current light water reactors make relatively inefficient use of nuclear fuel, fissioning only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable, and more efficient reactor designs, such as the currently under construction Generation III reactors achieve a higher efficiency burn up of the available resources, than the current vintage generation II reactors, which make up the vast majority of reactors worldwide.
Breeding.
As opposed to current light water reactors which use uranium-235 (0.7% of all natural uranium), fast breeder reactors use uranium-238 (99.3% of all natural uranium). It has been estimated that there is up to five billion years' worth of uranium-238 for use in these power plants.
Breeder technology has been used in several reactors, but the high cost of reprocessing fuel safely, at 2006 technological levels, requires uranium prices of more than 200 USD/kg before becoming justified economically. Breeder reactors are still however being pursued as they have the potential to burn up all of the actinides in the present inventory of nuclear waste while also producing power and creating additional quantities of fuel for more reactors via the breeding process. In 2005, there were two breeder reactors producing power: the Phénix in France, which has since powered down in 2009 after 36 years of operation, and the BN-600 reactor, a reactor constructed in 1980 Beloyarsk, Russia which is still operational as of 2013. The electricity output of BN-600 is 600 MW — Russia plans to expand the nation's use of breeder reactors with the BN-800 reactor, scheduled to become operational in 2014, and the technical design of a yet larger breeder, the BN-1200 reactor scheduled to be finalized in 2013, with construction slated for 2015. Japan's Monju breeder reactor restarted (having been shut down in 1995) in 2010 for 3 months, but shut down again after equipment fell into the reactor during reactor checkups, it is planned to become re-operational in late 2013. Both China and India are building breeder reactors. With the Indian 500 MWe Prototype Fast Breeder Reactor scheduled to become operational in 2014, with plans to build five more by 2020. The China Experimental Fast Reactor began producing power in 2011.
Another alternative to fast breeders is thermal breeder reactors that use uranium-233 bred from thorium as fission fuel in the thorium fuel cycle. Thorium is about 3.5 times more common than uranium in the Earth's crust, and has different geographic characteristics. This would extend the total practical fissionable resource base by 450%. India's three-stage nuclear power programme features the use of a thorium fuel cycle in the third stage, as it has abundant thorium reserves but little uranium.
Solid waste.
The most important waste stream from nuclear power plants is spent nuclear fuel. It is primarily composed of unconverted uranium as well as significant quantities of transuranic actinides (plutonium and curium, mostly). In addition, about 3% of it is fission products from nuclear reactions. The actinides (uranium, plutonium, and curium) are responsible for the bulk of the long-term radioactivity, whereas the fission products are responsible for the bulk of the short-term radioactivity.
High-level radioactive waste.
High-level radioactive waste management concerns management and disposal of highly radioactive materials created during production of nuclear power. The technical issues in accomplishing this are daunting, due to the extremely long periods radioactive wastes remain deadly to living organisms. Of particular concern are two long-lived fission products, Technetium-99 (half-life 220,000 years) and Iodine-129 (half-life 15.7 million years), which dominate spent nuclear fuel radioactivity after a few thousand years. The most troublesome transuranic elements in spent fuel are Neptunium-237 (half-life two million years) and Plutonium-239 (half-life 24,000 years). Consequently, high-level radioactive waste requires sophisticated treatment and management to successfully isolate it from the biosphere. This usually necessitates treatment, followed by a long-term management strategy involving permanent storage, disposal or transformation of the waste into a non-toxic form.
Governments around the world are considering a range of waste management and disposal options, usually involving deep-geologic placement, although there has been limited progress toward implementing long-term waste management solutions. This is partly because the timeframes in question when dealing with radioactive waste range from 10,000 to millions of years, according to studies based on the effect of estimated radiation doses.
Some proposed nuclear reactor designs however such as the American Integral Fast Reactor and the Molten salt reactor can use the nuclear waste from light water reactors as a fuel, transmutating it to isotopes that would be safe after hundreds, instead of tens of thousands of years. This offers a potentially more attractive alternative to deep geological disposal.
Another possibility is the use of thorium in a reactor especially designed for thorium (rather than mixing in thorium with uranium and plutonium (i.e. in existing reactors). Used thorium fuel remains only a few hundreds of years radioactive, instead of tens of thousands of years.
Since the fraction of a radioisotope's atoms decaying per unit of time is inversely proportional to its half-life, the relative radioactivity of a quantity of buried human radioactive waste would diminish over time compared to natural radioisotopes (such as the decay chains of 120 trillion tons of thorium and 40 trillion tons of uranium which are at relatively trace concentrations of parts per million each over the crust's 3 * 1019 ton mass). For instance, over a timeframe of thousands of years, after the most active short half-life radioisotopes decayed, burying U.S. nuclear waste would increase the radioactivity in the top 2000 feet of rock and soil in the United States (10 million km2) by ≈ 1 part in 10 million over the cumulative amount of natural radioisotopes in such a volume, although the vicinity of the site would have a far higher concentration of artificial radioisotopes underground than such an average.
Low-level radioactive waste.
The nuclear industry also produces a large volume of low-level radioactive waste in the form of contaminated items like clothing, hand tools, water purifier resins, and (upon decommissioning) the materials of which the reactor itself is built. In the US, the Nuclear Regulatory Commission has repeatedly attempted to allow low-level materials to be handled as normal waste: landfilled, recycled into consumer items, etcetera.
Comparing radioactive waste to industrial toxic waste.
In countries with nuclear power, radioactive wastes comprise less than 1% of total industrial toxic wastes, much of which remains hazardous for long periods. Overall, nuclear power produces far less waste material by volume than fossil-fuel based power plants. Coal-burning plants are particularly noted for producing large amounts of toxic and mildly radioactive ash due to concentrating naturally occurring metals and mildly radioactive material from the coal. A 2008 report from Oak Ridge National Laboratory concluded that coal power actually results in more radioactivity being released into the environment than nuclear power operation, and that the population effective dose equivalent, or dose to the public from radiation from coal plants is 100 times as much as from the ideal operation of nuclear plants. Indeed, coal ash is much less radioactive than spent nuclear fuel on a weight per weight basis, but coal ash is produced in much higher quantities per unit of energy generated, and this is released directly into the environment as fly ash, whereas nuclear plants use shielding to protect the environment from radioactive materials, for example, in dry cask storage vessels.
Waste disposal.
Disposal of nuclear waste is often said to be the Achilles' heel of the industry. Presently, waste is mainly stored at individual reactor sites and there are over 430 locations around the world where radioactive material continues to accumulate. Some experts suggest that centralized underground repositories which are well-managed, guarded, and monitored, would be a vast improvement. There is an "international consensus on the advisability of storing nuclear waste in deep geological repositories", with the lack of movement of nuclear waste in the 2 billion year old natural nuclear fission reactors in Oklo, Gabon being cited as "a source of essential information today."
As of 2009 there were no commercial scale purpose built underground repositories in operation. The Waste Isolation Pilot Plant in New Mexico has been taking nuclear waste since 1999 from production reactors, but as the name suggests is a research and development facility.
Reprocessing.
Reprocessing can potentially recover up to 95% of the remaining uranium and plutonium in spent nuclear fuel, putting it into new mixed oxide fuel. This produces a reduction in long term radioactivity within the remaining waste, since this is largely short-lived fission products, and reduces its volume by over 90%. Reprocessing of civilian fuel from power reactors is currently done in Britain, France and (formerly) Russia, soon will be done in China and perhaps India, and is being done on an expanding scale in Japan. The full potential of reprocessing has not been achieved because it requires breeder reactors, which are not commercially available. France is generally cited as the most successful reprocessor, but it presently only recycles 28% (by mass) of the yearly fuel use, 7% within France and another 21% in Russia.
Reprocessing is not allowed in the U.S. The Obama administration has disallowed reprocessing of nuclear waste, citing nuclear proliferation concerns. In the U.S., spent nuclear fuel is currently all treated as waste.
Depleted uranium.
Uranium enrichment produces many tons of depleted uranium (DU) which consists of U-238 with most of the easily fissile U-235 isotope removed. U-238 is a tough metal with several commercial uses—for example, aircraft production, radiation shielding, and armor—as it has a higher density than lead. Depleted uranium is also controversially used in munitions; DU penetrators (bullets or APFSDS tips) "self sharpen", due to uranium's tendency to fracture along shear bands.
Economics.
Internationally the price of nuclear plants rose 15% annually in 1970-1990. Total costs rose tenfold. The nuclear plant construction time doubled. According to Al Gore if intended plan does not hold, the delay cost a billion dollars a year.
The economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multi-billion dollar investments ride on the choice of an energy source. Nuclear power plants typically have high capital costs for building the plant, but low fuel costs. Therefore, comparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants as well as the future costs of fossil fuels and renewables as well as for energy storage solutions for intermittent power sources. Cost estimates also need to take into account plant decommissioning and nuclear waste storage costs. On the other hand measures to mitigate global warming, such as a carbon tax or carbon emissions trading, may favor the economics of nuclear power.
In recent years there has been a slowdown of electricity demand growth and financing has become more difficult, which has an impact on large projects such as nuclear reactors, with very large upfront costs and long project cycles which carry a large variety of risks. In Eastern Europe, a number of long-established projects are struggling to find finance, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where the electricity market is competitive, cheap natural gas is available, and its future supply relatively secure, this also poses a major problem for nuclear projects and existing plants.
Analysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, accident liability and other factors were borne by consumers rather than suppliers. In addition, because the potential liability from a nuclear accident is so great, the full cost of liability insurance is generally limited/capped by the government, which the U.S. Nuclear Regulatory Commission concluded constituted a significant subsidy. Many countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.
Following the 2011 Fukushima Daiichi nuclear disaster, costs are expected to increase for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats.
Accidents and safety, the human and financial costs.
Some serious nuclear and radiation accidents have occurred. Benjamin K. Sovacool has reported that worldwide there have been 99 accidents at nuclear power plants. Fifty-seven accidents have occurred since the Chernobyl disaster, and 57% (56 out of 99) of all nuclear-related accidents have occurred in the USA.
Nuclear power plant accidents include the Chernobyl accident (1986) with approximately 60 deaths so far attributed to the accident and a predicted, eventual total death toll, of from 4000 to 25,000 latent cancers deaths. The Fukushima Daiichi nuclear disaster (2011), has not caused any radiation related deaths, with a predicted, eventual total death toll, of from 0 to 1000, and the Three Mile Island accident (1979), no causal deaths, cancer or otherwise, have been found in follow up studies of this accident. Nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985). International research is continuing into safety improvements such as passively safe plants, and the possible future use of nuclear fusion.
In terms of lives lost per unit of energy generated, nuclear power has caused fewer accidental deaths per unit of energy generated than all other major sources of energy generation. Energy produced by coal, petroleum, natural gas and hydropower has caused more deaths per unit of energy generated, from air pollution and energy accidents. This is found in the following comparisons, when the immediate nuclear related deaths from accidents are compared to the immediate deaths from these other energy sources, when the latent, or predicted, indirect cancer deaths from nuclear energy accidents are compared to the immediate deaths from the above energy sources, and when the combined immediate and indirect fatalities from nuclear power and all fossil fuels are compared, fatalities resulting from the mining of the necessary natural resources to power generation and to air pollution. With these data, the use of nuclear power has been calculated to have prevented a considerable number of fatalities, by reducing the proportion of energy that would otherwise have been generated by fossil fuels, and is projected to continue to do so.
Nuclear power plant accidents, according to Benjamin K. Sovacool, rank first in terms of their economic cost, accounting for 41 percent of all property damage attributed to energy accidents. However analysis presented in the international Journal, "Human and Ecological Risk Assessment" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.
Following the 2011 Japanese Fukushima nuclear disaster, authorities shut down the nation's 54 nuclear power plants, but it has been estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life. As of 2013, the Fukushima site remains highly radioactive, with some 160,000 evacuees still living in temporary housing, and some land will be unfarmable for centuries. The difficult Fukushima disaster cleanup will take 40 or more years, and cost tens of billions of dollars.
In August 2014, EDF Energy announced it had shut down 4 of its 8 reactors for a period of eight weeks due to "cracking." The UK nuclear regulator, ONR confirmed there was no release of radioactive material and no persons injured.
Forced evacuation from a nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. A comprehensive 2005 study concluded that "the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date". Frank N. von Hippel, a U.S. scientist, commented on the 2011 Fukushima nuclear disaster, saying that "fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas".
Nuclear proliferation.
Many technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that they can be used to make nuclear weapons if a country chooses to do so. When this happens a nuclear power program can become a route leading to a nuclear weapon or a public annex to a "secret" weapons program. The concern over Iran's nuclear activities is a case in point.
A fundamental goal for American and global security is to minimize the nuclear proliferation risks associated with the expansion of nuclear power. If this development is "poorly managed or efforts to contain risks are unsuccessful, the nuclear future will be dangerous". The Global Nuclear Energy Partnership is one such international effort to create a distribution network in which developing countries in need of energy, would receive nuclear fuel at a discounted rate, in exchange for that nation agreeing to forgo their own indigenous develop of a uranium enrichment program.
According to Benjamin K. Sovacool, a "number of high-ranking officials, even within the United Nations, have argued that they can do little to stop states using nuclear reactors to produce nuclear weapons". A 2009 United Nations report said that:
the revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.
On the other hand, one factor influencing the support of power reactors is due to the appeal that these reactors have at reducing nuclear weapons arsenals through the Megatons to Megawatts Program, a program which has thus far eliminated 425 metric tons of highly enriched uranium, the equivalent of 17,000 nuclear warheads, by converting it into fuel for commercial nuclear reactors, and it is the single most successful non-proliferation program to date.
The Megatons to Megawatts Program has been hailed as a major success by anti-nuclear weapon advocates as it has largely been the driving force behind the sharp reduction in the quantity of nuclear weapons worldwide since the cold war ended. However without an increase in nuclear reactors and greater demand for fissile fuel, the cost of dismantling and down blending has dissuaded Russia from continuing their disarmament.
Currently, according to Harvard professor Matthew Bunn: "The Russians are not remotely interested in extending the program beyond 2013. We've managed to set it up in a way that costs them more and profits them less than them just making new low-enriched uranium for reactors from scratch. But there are other ways to set it up that would be very profitable for them and would also serve some of their strategic interests in boosting their nuclear exports."
In the Megatons to Megawatts Program approximately $8 billion of weapons grade uranium is being converted to reactor grade uranium in the elimination of 10,000 nuclear weapons.
In April 2012 there were thirty one countries that have civil nuclear power plants. In 2013, Mark Diesendorf says that governments of France, India, North Korea, Pakistan, UK, and South Africa have used nuclear power and/or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors.
Environmental issues.
Life cycle analysis (LCA) of carbon dioxide emissions show nuclear power as comparable to renewable energy sources. Emissions from burning fossil fuels are many times higher.
According to the United Nations (UNSCEAR), regular nuclear power plant operation including the nuclear fuel cycle causes radioisotope releases into the environment amounting to 0.0002 mSv (milli-Sievert) per year of public exposure as a global average. (Such is small compared to variation in natural background radiation, which averages 2.4 mSv/a globally but frequently varies between 1 mSv/a and 13 mSv/a depending on a person's location as determined by UNSCEAR). As of a 2008 report, the remaining legacy of the worst nuclear power plant accident (Chernobyl) is 0.002 mSv/a in global average exposure (a figure which was 0.04 mSv per person averaged over the entire populace of the Northern Hemisphere in the year of the accident in 1986, although far higher among the most affected local populations and recovery workers).
Climate change.
Climate change causing weather extremes such as heat waves, reduced precipitation levels and droughts can have a significant impact on nuclear energy infrastructure. Seawater is corrosive and so nuclear energy supply is likely to be negatively affected by the fresh water shortage. This generic problem may become increasingly significant over time. This can force nuclear reactors to be shut down, as happened in France during the 2003 and 2006 heat waves. Nuclear power supply was severely diminished by low river ﬂow rates and droughts, which meant rivers had reached the maximum temperatures for cooling reactors. During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power and in 2009 a similar situation created a 8GW shortage and forced the French government to import electricity. Other cases have been reported from Germany, where extreme temperatures have reduced nuclear power production 9 times due to high temperatures between 1979 and 2007. In particular:
Similar events have happened elsewhere in Europe during those same hot summers. If global warming continues, this disruption is likely to increase.
Nuclear decommissioning.
The price of energy inputs and the environmental costs of every nuclear power plant continue long after the facility has finished generating its last useful electricity. Both nuclear reactors and uranium enrichment facilities must be decommissioned, returning the facility and its parts to a safe enough level to be entrusted for other uses. After a cooling-off period that may last as long as a century, reactors must be dismantled and cut into small pieces to be packed in containers for final disposal. The process is very expensive, time-consuming, dangerous for workers, hazardous to the natural environment, and presents new opportunities for human error, accidents or sabotage.
The total energy required for decommissioning can be as much as 50% more than the energy needed for the original construction. In most cases, the decommissioning process costs between US $300 million to US$5.6 billion. Decommissioning at nuclear sites which have experienced a serious accident are the most expensive and time-consuming. In the U.S. there are 13 reactors that have permanently shut down and are in some phase of decommissioning, and none of them have completed the process.
Current UK plants are expected to exceed £73bn in decommissioning costs.
Debate on nuclear power.
The nuclear power debate concerns the controversy which has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes. The debate about nuclear power peaked during the 1970s and 1980s, when it "reached an intensity unprecedented in the history of technology controversies", in some countries.
Proponents of nuclear energy contend that nuclear power is a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on imported energy sources. Proponents claim that nuclear power produces virtually no conventional air pollution, such as greenhouse gases and smog, in contrast to the chief viable alternative of fossil fuel. Nuclear power can produce base-load power unlike many renewables which are intermittent energy sources lacking large-scale and cheap ways of storing energy. M. King Hubbert saw oil as a resource that would run out, and proposed nuclear energy as a replacement energy source. Proponents claim that the risks of storing waste are small and can be further reduced by using the latest technology in newer reactors, and the operational safety record in the Western world is excellent when compared to the other major kinds of power plants.
Opponents believe that nuclear power poses many threats to people and the environment. These threats include the problems of processing, transport and storage of radioactive nuclear waste, the risk of nuclear weapons proliferation and terrorism, as well as health risks and environmental damage from uranium mining. They also contend that reactors themselves are enormously complex machines where many things can and do go wrong; and there have been serious nuclear accidents. Critics do not believe that the risks of using nuclear fission as a power source can be fully offset through the development of new technology. They also argue that when all the energy-intensive stages of the nuclear fuel chain are considered, from uranium mining to nuclear decommissioning, nuclear power is neither a low-carbon nor an economical electricity source.
Arguments of economics and safety are used by both sides of the debate.
Comparison with renewable energy.
As of 2013, the World Nuclear Association has said "There is unprecedented interest in renewable energy, particularly solar and wind energy, which provide electricity without giving rise to any carbon dioxide emission. Harnessing these for electricity depends on the cost and efficiency of the technology, which is constantly improving, thus reducing costs per peak kilowatt".
Renewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being intermittent or variable. However, the International Energy Agency concluded that deployment of renewable technologies (RETs), when it increases the diversity of electricity sources, contributes to the flexibility of the system. However, the report also concluded (p. 29): "At high levels of grid penetration by RETs the consequences of unmatched demand and supply can pose challenges for grid management. This characteristic may affect how, and the degree to which, RETs can displace fossil fuels and nuclear capacities in power generation."
Renewable electricity supply in the 20-50+% range has already been implemented in several European systems, albeit in the context of an integrated European grid system. In 2012, the share of electricity generated by renewable sources in Germany was 21.9%, compared to 16.0% for nuclear power after Germany shut down 7-8 of its 18 nuclear reactors in 2011. In the United Kingdom, the amount of energy produced from renewable energy is expected to exceed that from nuclear power by 2018, and Scotland plans to obtain all electricity from renewable energy by 2020. The majority of installed renewable energy across the world is in the form of hydro power.
The IPCC has said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.
The cost of nuclear power has followed an increasing trend whereas the cost of electricity is declining for wind power. In about 2011, wind power became as inexpensive as natural gas, and anti-nuclear groups have suggested that in 2010 solar power became cheaper than nuclear power. Data from the EIA in 2011 estimated that in 2016, solar will have a levelized cost of electricity almost twice that of nuclear (21¢/kWh for solar, 11.39¢/kWh for nuclear), and wind somewhat less (9.7¢/kWh). However, the US EIA has also cautioned that levelized costs of intermittent sources such as wind and solar are not directly comparable to costs of “dispatchable” sources (those that can be adjusted to meet demand).
From a safety stand point, nuclear power, in terms of lives lost per unit of electricity delivered, is comparable to and in some cases, lower than many renewable energy sources. There is however no radioactive spent fuel that needs to be stored or reprocessed with conventional renewable energy sources. A nuclear plant needs to be disassembled and removed. Much of the disassembled nuclear plant needs to be stored as low level nuclear waste.
Nuclear renaissance.
Since about 2001 the term "nuclear renaissance" has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. However, the World Nuclear Association has reported that nuclear electricity generation in 2012 was at its lowest level since 1999.
In March 2011 the nuclear emergencies at Japan's Fukushima I Nuclear Power Plant and shutdowns at other nuclear facilities raised questions among some commentators over the future of the renaissance. Platts has reported that "the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world". In 2011 Siemens exited the nuclear power sector following the Fukushima disaster and subsequent changes to German energy policy, and supported the German government's planned energy transition to renewable energy technologies. China, Germany, Switzerland, Israel, Malaysia, Thailand, United Kingdom, Italy and the Philippines have reviewed their nuclear power programs. Indonesia and Vietnam still plan to build nuclear power plants. Countries such as Australia, Austria, Denmark, Greece, Ireland, Latvia, Liechtenstein, Luxembourg, Portugal, Israel, Malaysia, New Zealand, and Norway remain opposed to nuclear power. Following the Fukushima I nuclear accidents, the International Energy Agency halved its estimate of additional nuclear generating capacity built by 2035.
The World Nuclear Association has said that “nuclear power generation suffered its biggest ever one-year fall through 2012 as the bulk of the Japanese fleet remained offline for a full calendar year”. Data from the International Atomic Energy Agency showed that nuclear power plants globally produced 2346 TWh of electricity in 2012 – seven per cent less than in 2011. The figures illustrate the effects of a full year of 48 Japanese power reactors producing no power during the year. The permanent closure of eight reactor units in Germany was also a factor. Problems at Crystal River, Fort Calhoun and the two San Onofre units in the USA meant they produced no power for the full year, while in Belgium Doel 3 and Tihange 2 were out of action for six months. Compared to 2010, the nuclear industry produced 11% less electricity in 2012.
Future of the industry.
As already noted, the nuclear power industry in western nations has a history of construction delays, cost overruns, plant cancellations, and nuclear safety issues despite significant government subsidies and support. In December 2013, "Forbes" magazine reported that, in developed countries, “reactors are not a viable source of new power”. Even in developed nations where they make economic sense, they are not feasible because nuclear’s “enormous costs, political and popular opposition, and regulatory uncertainty”. This view echoes the statement of former Exelon CEO John Rowe, who said in 2012 that new nuclear plants “don’t make any sense right now” and won’t be economically viable in the foreseeable future. John Quiggin, economics professor, also says the main problem with the nuclear option is that it is not economically-viable. Quiggin says that we need more efficient energy use and more renewable energy commercialization. Former NRC member Peter Bradford and Professor Ian Lowe have recently made similar statements. However, some “nuclear cheerleaders” and lobbyists in the West continue to champion reactors, often with proposed new but largely untested designs, as a source of new power.
Much more new build activity is occurring in developing countries like South Korea, India and China. China has 25 reactors under construction, with plans to build more, However, according to a government research unit, China must not build "too many nuclear power reactors too quickly", in order to avoid a shortfall of fuel, equipment and qualified plant workers.
In the USA, licenses of almost half its reactors have been extended to 60 years, Two new Generation III reactors are under construction at Vogtle, a dual construction project which marks the end of a 34-year period of stagnation in the US construction of civil nuclear power reactors. The station operator licenses of almost half the present 104 power reactors in the US, as of 2008, have been given extensions to 60 years. As of 2012, U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed. Relevant state legislatures are trying to close Vermont Yankee and Indian Point Nuclear Power Plant.
The U.S. NRC and the U.S. Department of Energy have initiated research into Light water reactor sustainability which is hoped will lead to allowing extensions of reactor licenses beyond 60 years, provided that safety can be maintained, as the loss in non-CO2-emitting generation capacity by retiring reactors "may serve to challenge U.S. energy security, potentially resulting in increased greenhouse gas emissions, and contributing to an imbalance between electric supply and demand."
There is a possible impediment to production of nuclear power plants as only a few companies worldwide have the capacity to forge single-piece reactor pressure vessels, which are necessary in the most common reactor designs. Utilities across the world are submitting orders years in advance of any actual need for these vessels. Other manufacturers are examining various options, including making the component themselves, or finding ways to make a similar item using alternate methods.
According to the World Nuclear Association, globally during the 1980s one new nuclear reactor started up every 17 days on average, and by the year 2015 this rate could increase to one every 5 days. As of 2007, Watts Bar 1 in Tennessee, which came on-line on February 7, 1996, was the last U.S. commercial nuclear reactor to go on-line. This is often quoted as evidence of a successful worldwide campaign for nuclear power phase-out. Electricity shortages, fossil fuel price increases, global warming, and heavy metal emissions from fossil fuel use, new technology such as passively safe plants, and national energy security may renew the demand for nuclear power plants.
Nuclear phase out.
Following the Fukushima Daiichi nuclear disaster, the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035. Platts has reported that "the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world". In 2011, "The Economist" reported that nuclear power "looks dangerous, unpopular, expensive and risky", and that "it is replaceable with relative ease and could be forgone with no huge structural shifts in the way the world works".
In early April 2011, analysts at Swiss-based investment bank UBS said: "At Fukushima, four reactors have been out of control for weeks, casting doubt on whether even an advanced economy can master nuclear safety . . .. We believe the Fukushima accident was the most serious ever for the credibility of nuclear power".
In 2011, Deutsche Bank analysts concluded that "the global impact of the Fukushima accident is a fundamental shift in public perception with regard to how a nation prioritizes and values its populations health, safety, security, and natural environment when determining its current and future energy pathways". As a consequence, "renewable energy will be a clear long-term winner in most energy systems, a conclusion supported by many voter surveys conducted over the past few weeks. At the same time, we consider natural gas to be, at the very least, an important transition fuel, especially in those regions where it is considered secure".
In September 2011, German engineering giant Siemens announced it will withdraw entirely from the nuclear industry, as a response to the Fukushima nuclear disaster in Japan, and said that it would no longer build nuclear power plants anywhere in the world. The company’s chairman, Peter Löscher, said that "Siemens was ending plans to cooperate with Rosatom, the Russian state-controlled nuclear power company, in the construction of dozens of nuclear plants throughout Russia over the coming two decades". Also in September 2011, IAEA Director General Yukiya Amano said the Japanese nuclear disaster "caused deep public anxiety throughout the world and damaged confidence in nuclear power".
In February 2012, the United States Nuclear Regulatory Commission approved the construction of two additional reactors at the Vogtle Electric Generating Plant, the first reactors to be approved in over 30 years since the Three Mile Island accident, but NRC Chairman Gregory Jaczko cast a dissenting vote citing safety concerns stemming from Japan's 2011 Fukushima nuclear disaster, and saying "I cannot support issuing this license as if Fukushima never happened". One week after Southern received the license to begin major construction on the two new reactors, a dozen environmental and anti-nuclear groups are suing to stop the Plant Vogtle expansion project, saying "public safety and environmental problems since Japan's Fukushima Daiichi nuclear reactor accident have not been taken into account".
Countries such as Australia, Austria, Denmark, Greece, Ireland, Italy, Latvia, Liechtenstein, Luxembourg, Malta, Portugal, Israel, Malaysia, New Zealand, and Norway have no nuclear power reactors and remain opposed to nuclear power. However, by contrast, some countries remain in favor, and financially support nuclear fusion research, including EU wide funding of the ITER project.
Worldwide wind power has been increasing at 26%/year, and solar power at 58%/year, from 2006 to 2011, as a replacement for thermal generation of electricity.
Advanced concepts.
Current fission reactors in operation around the world are second or third generation systems, with most of the first-generation systems having been retired some time ago. Research into advanced generation IV reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals, including to improve nuclear safety, improve proliferation resistance, minimize waste, improve natural resource utilization, the ability to consume existing nuclear waste in the production of electricity, and decrease the cost to build and run such plants. Most of these reactors differ significantly from current operating light water reactors, and are generally not expected to be available for commercial construction before 2030.
The nuclear reactors to be built at Vogtle are new AP1000 third generation reactors, which are said to have safety improvements over older power reactors. However, John Ma, a senior structural engineer at the NRC, is concerned that some parts of the AP1000 steel skin are so brittle that the "impact energy" from a plane strike or storm driven projectile could shatter the wall. Edwin Lyman, a senior staff scientist at the Union of Concerned Scientists, is concerned about the strength of the steel containment vessel and the concrete shield building around the AP1000.
The Union of Concerned Scientists has referred to the EPR (nuclear reactor), currently under construction in China, Finland and France, as the only new reactor design under consideration in the United States that "...appears to have the potential to be significantly safer and more secure against attack than today's reactors."
One disadvantage of any new reactor technology is that safety risks may be greater initially as reactor operators have little experience with the new design. Nuclear engineer David Lochbaum has explained that almost all serious nuclear accidents have occurred with what was at the time the most recent technology. He argues that "the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes". As one director of a U.S. research laboratory put it, "fabrication, construction, operation, and maintenance of new reactors will face a steep learning curve: advanced technologies will have a heightened risk of accidents and mistakes. The technology may be proven, but people are not".
Hybrid nuclear fusion-fission.
Hybrid nuclear power is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to delays in the realization of pure fusion. When a sustained nuclear fusion power plant is built, it has the potential to be capable of extracting all the fission energy that remains in spent fission fuel, reducing the volume of nuclear waste by orders of magnitude, and more importantly, eliminating all actinides present in the spent fuel, substances which cause security concerns.
Nuclear fusion.
Nuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission. These reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. Fusion power has been under theoretical and experimental investigation since the 1950s.
Construction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated. A follow on commercial nuclear fusion power station, DEMO, has been proposed. There is also suggestions for a power plant based upon a different fusion approach, that of a Inertial fusion power plant.
Fusion powered electricity generation was initially believed to be readily achievable, as fission power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050.
Nuclear power organizations.
There are multiple organizations which have taken a position on nuclear power – some are proponents, and some are opponents.

</doc>
<doc id="22156" url="http://en.wikipedia.org/wiki?curid=22156" title="BI Norwegian Business School">
BI Norwegian Business School

BI Norwegian Business School (Norwegian: "Handelshøyskolen BI") is the largest business school in Norway and the second largest in all of Europe. BI has in total 5 campuses with the main one located in Oslo. The university has 831 employees consisting of an academic staff of 419 people and 412 administrative staff. In 2013, BI Norwegian Business School had 19 649 students, of which 10 889 were full-time students. BI Norwegian Business School is a private foundation and is accredited by NOKUT as a specialised university institution.
History.
BI Norwegian Business School was founded in 1943 by Finn Øien as "Bedriftøkonomisk Institut" (English: Institute of Managerial Economics), hence the abbreviation "BI". The current president is professor Inge Jan Henjesand. Past presidents include Jørgen Randers, co-author of the Club of Rome Report Limits to Growth; Peter Lorange, former president of IMD in Lausanne, Switzerland; Leif Frode Onarheim, a former member of the Norwegian Parliament and current CEO of leading Norwegian fish farming company Marine Harvest.
Current activities.
 As of 2013, the school has 19,649 students made up of 52% women and 55.6% full-time students Thereby, the Bachelor of Science programs have 15,570 students and the Master of Science programs have 3,968 students. 1,300 are international students. There are 831 employees, of whom 419 are faculty and 412 are administrative.
BI offers a full set of programs for bachelor, master, and doctoral degrees, as well as executive education and tailor-made programs for businesses. The teaching languages are English (BBA and graduate programs) and Norwegian (majority of undergraduate programs and custom programs for local businesses). The school currently participates in exchange programs with 170 foreign institutions in 45 countries.
The internationally award winning main campus in Nydalen (Oslo) was designed by Niels Torp, who also designed Gardermoen Airport.
Norsk Kundebarometer.
Norsk Kundebarometer (NKB) (English: Norwegian customer barometer) is a research program run by BI, with a focus on relations between customers and businesses. Based on an annual survey of Norwegian households, it collects data that may be used for comparison between businesses, comparisons between various industries, and comparisons over time.
Activities abroad.
BI has educated roughly 1700 students in China through its close relationship with Fudan University in Shanghai, and is also the majority shareholder of the ISM University of Management and Economics (previously known as International School of Management) with around 1800 students located in Vilnius and Kaunas in Lithuania.
Degree programs.
Undergraduate (All taught in Norwegian except Business Administration), 
"Bachelor in":
Graduate (Only available in Oslo; all taught in English except MSc in Professional Accountancy)
The current Executive Masters of Business Administration (EMBA) course at the Norwegian business school is fantastic. The new structure and the combination of the general EMBA courses with other renowned business schools in Europe, Asia, and the United States gives the program an additional edge. The new structure exposes participants to insights on business structure in Europe, Latin America, Asia and North America; hence, providing participants with a global business exposure.
Student organizations.
The school has two student organizations, one for the main campus in Oslo and one for the other campuses. The Oslo student organization is called (SBIO) (English: The Student Association at BI in Oslo). This union was formed in 2005 after the relocation of the three locations in Oslo into one—Nydalen Campus. The three previous unions were called Bedriftøkonomisk Studentersamfund (BS), BISON and MØSS. BS was the oldest union, formed in 1964. The union for the other campuses is "BI Studentsamfunn" (BIS) (English: BI Student Community). This union was founded on 7 February 1987 and is today the largest student union of a private school in Norway.
The student newspaper is named "INSIDE", and its circulation is 11,000.
The all-male student choir is named UFDA The Choir Boys and was established in 1986.
Quality accreditations.
BI is accredited as a specialised university institution by the Norwegian Agency for Quality Assurance in Education (NOKUT).
BI has also received the following recognitions from private institutions:
External links.
 

</doc>
<doc id="22158" url="http://en.wikipedia.org/wiki?curid=22158" title="Nuclear proliferation">
Nuclear proliferation

Nuclear proliferation is the spread of nuclear weapons, fissionable material, and weapons-applicable nuclear technology and information to nations not recognized as "Nuclear Weapon States" by the "Treaty on the Nonproliferation of Nuclear Weapons", also known as the Nuclear Nonproliferation Treaty or NPT. Leading experts on nuclear proliferation, such as Etel Solingen of the University of California, Irvine, suggest that states' decisions to build nuclear weapons is largely determined by the interests of their governing domestic coalitions.
Proliferation has been opposed by many nations with and without nuclear weapons, the governments of which fear that more countries with nuclear weapons may increase the possibility of nuclear warfare (up to and including the so-called "countervalue" targeting of civilians with nuclear weapons), de-stabilize international or regional relations, or infringe upon the national sovereignty of states.
Four countries besides the five recognized Nuclear Weapons States have acquired, or are presumed to have acquired, nuclear weapons: India, Pakistan, North Korea, and Israel. None of these four is a party to the NPT, although North Korea acceded to the NPT in 1985, then withdrew in 2003 and conducted announced nuclear tests in 2006, 2009, and 2013. One critique of the NPT is that it is discriminatory in recognizing as nuclear weapon states only those countries that tested nuclear weapons before 1968 and requiring all other states joining the treaty to forswear nuclear weapons.
Research into the development of nuclear weapons was undertaken during World War II by the United States (in cooperation with the United Kingdom and Canada), Germany, Japan, and the USSR. The United States was the first and is the only country to have used a nuclear weapon in war, when it used two bombs against Japan in August 1945. With their loss during the war, Germany and Japan ceased to be involved in any nuclear weapon research. In August 1949, the USSR tested a nuclear weapon. The United Kingdom tested a nuclear weapon in October 1952. France developed a nuclear weapon in 1960. The People's Republic of China detonated a nuclear weapon in 1964. India exploded a nuclear device in 1974, and Pakistan tested a weapon in 1998. In 2006, North Korea conducted a nuclear test.
Non-proliferation efforts.
Early efforts to prevent nuclear proliferation involved intense government secrecy, the wartime acquisition of known uranium stores (the Combined Development Trust), and at times even outright sabotage—such as the bombing of a heavy-water facility thought to be used for a German nuclear program. None of these efforts were explicitly public, because the weapon developments themselves were kept secret until the bombing of Hiroshima.
Earnest international efforts to promote nuclear non-proliferation began soon after World War II, when the Truman Administration proposed the Baruch Plan of 1946, named after Bernard Baruch, America's first representative to the United Nations Atomic Energy Commission. The Baruch Plan, which drew heavily from the Acheson–Lilienthal Report of 1946, proposed the verifiable dismantlement and destruction of the U.S. nuclear arsenal (which, at that time, was the only nuclear arsenal in the world) after all governments had cooperated successfully to accomplish two things: (1) the establishment of an "international atomic development authority," which would actually own and control all military-applicable nuclear materials and activities, and (2) the creation of a system of automatic sanctions, which not even the U.N. Security Council could veto, and which would proportionately punish states attempting to acquire the capability to make nuclear weapons or fissile material.
Baruch's plea for the destruction of nuclear weapons invoked basic moral and religious intuitions. In one part of his address to the UN, Baruch said, "Behind the black portent of the new atomic age lies a hope which, seized upon with faith, can work out our salvation. If we fail, then we have damned every man to be the slave of Fear. Let us not deceive ourselves. We must elect World Peace or World Destruction... We must answer the world's longing for peace and security." With this remark, Baruch helped launch the field of nuclear ethics, to which many policy experts and scholars have contributed.
Although the Baruch Plan enjoyed wide international support, it failed to emerge from the UNAEC because the Soviet Union planned to veto it in the Security Council. Still, it remained official American policy until 1953, when President Eisenhower made his "Atoms for Peace" proposal before the U.N. General Assembly. Eisenhower's proposal led eventually to the creation of the International Atomic Energy Agency (IAEA) in 1957. Under the "Atoms for Peace" program thousands of scientists from around the world were educated in nuclear science and then dispatched home, where many later pursued secret weapons programs in their home country.
Efforts to conclude an international agreement to limit the spread of nuclear weapons did not begin until the early 1960s, after four nations (the United States, the Soviet Union, the United Kingdom and France) had acquired nuclear weapons (see List of states with nuclear weapons for more information). Although these efforts stalled in the early 1960s, they renewed once again in 1964, after China detonated a nuclear weapon. In 1968, governments represented at the Eighteen Nation Disarmament Committee (ENDC) finished negotiations on the text of the NPT. In June 1968, the U.N. General Assembly endorsed the NPT with General Assembly Resolution 2373 (XXII), and in July 1968, the NPT opened for signature in Washington, DC, London and Moscow. The NPT entered into force in March 1970.
Since the mid-1970s, the primary focus of non-proliferation efforts has been to maintain, and even increase, international control over the fissile material and specialized technologies necessary to build such devices because these are the most difficult and expensive parts of a nuclear weapons program. The main materials whose generation and distribution is controlled are highly enriched uranium and plutonium. Other than the acquisition of these special materials, the scientific and technical means for weapons construction to develop rudimentary, but working, nuclear explosive devices are considered to be within the reach of industrialized nations.
Since its founding by the United Nations in 1957, the International Atomic Energy Agency (IAEA) has promoted two, sometimes contradictory, missions: on the one hand, the Agency seeks to promote and spread internationally the use of civilian nuclear energy; on the other hand, it seeks to prevent, or at least detect, the diversion of civilian nuclear energy to nuclear weapons, nuclear explosive devices or purposes unknown. The IAEA now operates a safeguards system as specified under Article III of the Nuclear Non-Proliferation Treaty (NPT) of 1968, which aims to ensure that civil stocks of uranium, plutonium, as well as facilities and technologies associated with these nuclear materials, are used only for peaceful purposes and do not contribute in any way to proliferation or nuclear weapons programs. It is often argued that proliferation of nuclear weapons to many other states has been prevented by the extension of assurances and mutual defence treaties to these states by nuclear powers, but other factors, such as national prestige, or specific historical experiences, also play a part in hastening or stopping nuclear proliferation.
Dual use technology.
Dual-use technology refers to the possibility of military use of civilian nuclear power technology. Many technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that several stages of the nuclear fuel cycle allow diversion of nuclear materials for nuclear weapons. When this happens a nuclear power program can become a route leading to the atomic bomb or a public annex to a secret bomb program. The crisis over Iran’s nuclear activities is a case in point.
Many UN and US agencies warn that building more nuclear reactors unavoidably increases nuclear proliferation risks. A fundamental goal for American and global security is to minimize the proliferation risks associated with the
expansion of nuclear power. If this development is "poorly managed or efforts to contain risks are unsuccessful, the nuclear future will be dangerous". For nuclear power programs to be developed and managed safely and securely, it is important that countries have domestic “good governance” characteristics that will encourage proper nuclear operations and management:
These characteristics include low degrees of corruption (to avoid officials selling materials and technology for their own personal gain as occurred with the A.Q. Khan smuggling network in Pakistan), high degrees of political stability (defined by the World Bank as “likelihood that the government will be destabilized or overthrown by unconstitutional or violent means, including politically-motivated violence and terrorism”), high governmental effectiveness scores (a World Bank aggregate measure of “the quality of the civil service and the degree of its independence from political pressures [and] the quality of policy formulation and implementation”), and a strong degree of regulatory competence.
International cooperation.
Nuclear Non-Proliferation Treaty.
At present, 189 countries are States Parties to the "Treaty on the Nonproliferation of Nuclear Weapons", more commonly known as the Nuclear Nonproliferation Treaty or NPT. These include the five Nuclear Weapons States (NWS) recognized by the NPT: the People's Republic of China, France, Russian Federation, the UK, and the United States.
Notable non-signatories to the NPT are Israel, Pakistan, and India (the latter two have since tested nuclear weapons, while Israel is considered by most to be an unacknowledged nuclear weapons state). North Korea was once a signatory but withdrew in January 2003. The legality of North Korea's withdrawal is debatable but as of 9 October 2006, North Korea clearly possesses the capability to make a nuclear explosive device.
International Atomic Energy Agency.
The IAEA was established on 29 July 1957 to help nations develop nuclear energy for peaceful purposes. Allied to this role is the administration of safeguards arrangements to provide assurance to the international community that individual countries are honoring their commitments under the treaty. Though established under its own international treaty, the IAEA reports to both the United Nations General Assembly and the Security Council.
The IAEA regularly inspects civil nuclear facilities to verify the accuracy of documentation supplied to it. The agency checks inventories, and samples and analyzes materials. Safeguards are designed to deter diversion of nuclear material by increasing the risk of early detection. They are complemented by controls on the export of sensitive technology from countries such as UK and United States through voluntary bodies such as the Nuclear Suppliers Group. The main concern of the IAEA is that uranium not be enriched beyond what is necessary for commercial civil plants, and that plutonium which is produced by nuclear reactors not be refined into a form that would be suitable for bomb production.
Scope of safeguards.
Traditional safeguards are arrangements to account for and control the use of nuclear materials. This verification is a key element in the international system which ensures that uranium in particular is used only for peaceful purposes.
Parties to the NPT agree to accept technical safeguard measures applied by the IAEA. These require that operators of nuclear facilities maintain and declare detailed accounting records of all movements and transactions involving nuclear material. Over 550 facilities and several hundred other locations are subject to regular inspection, and their records and the nuclear material being audited. Inspections by the IAEA are complemented by other measures such as surveillance cameras and instrumentation.
The inspections act as an alert system providing a warning of the possible diversion of nuclear material from peaceful activities. The system relies on;
All NPT non-weapons states must accept these full-scope safeguards. In the five weapons states plus the non-NPT states (India, Pakistan and Israel), facility-specific safeguards apply. IAEA inspectors regularly visit these facilities to verify completeness and accuracy of records.
The terms of the NPT cannot be enforced by the IAEA itself, nor can nations be forced to sign the treaty. In reality, as shown in Iraq and North Korea, safeguards can be backed up by diplomatic, political and economic measures.
While traditional safeguards easily verified the correctness of formal declarations by suspect states, in the 1990s attention turned to what might not have been declared. While accepting safeguards at declared facilities, Iraq had set up elaborate equipment elsewhere in an attempt to enrich uranium to weapons grade. North Korea attempted to use research reactors (not commercial electricity-generating reactors) and a reprocessing plant to produce some weapons-grade plutonium.
The weakness of the NPT regime lay in the fact that no obvious diversion of material was involved. The uranium used as fuel probably came from indigenous sources, and the nuclear facilities were built by the countries themselves without being declared or placed under safeguards. Iraq, as an NPT party, was obliged to declare all facilities but did not do so. Nevertheless, the activities were detected and brought under control using international diplomacy. In Iraq, a military defeat assisted this process.
In North Korea, the activities concerned took place before the conclusion of its NPT safeguards agreement. With North Korea, the promised provision of commercial power reactors appeared to resolve the situation for a time, but it later withdrew from the NPT and declared it had nuclear weapons.
Additional Protocol.
In 1993 a program was initiated to strengthen and extend the classical safeguards system, and a model protocol was agreed by the IAEA Board of Governors 1997. The measures boosted the IAEA's ability to detect undeclared nuclear activities, including those with no connection to the civil fuel cycle.
Innovations were of two kinds. Some could be implemented on the basis of IAEA's existing legal authority through safeguards agreements and inspections. Others required further legal authority to be conferred through an Additional Protocol. This must be agreed by each non-weapons state with IAEA, as a supplement to any existing comprehensive safeguards agreement. Weapons states have agreed to accept the principles of the model additional protocol.
Key elements of the model Additional Protocol:
As of 20 December 2010, 139 countries have signed Additional Protocols, 104 have brought them into force, and one (Iraq) is implementing its protocol provisionally. The IAEA is also applying the measures of the Additional Protocol in Taiwan. Among the leading countries that have not signed the Additional Protocol are Egypt, which says it will not sign until Israel accepts comprehensive IAEA safeguards, and Brazil, which opposes making the protocol a requirement for international cooperation on enrichment and reprocessing, but has not ruled out signing.
Limitations of Safeguards.
The greatest risk from nuclear weapons proliferation comes from countries which have not joined the NPT and which have significant unsafeguarded nuclear activities; India, Pakistan, and Israel fall within this category. While safeguards apply to some of their activities, others remain beyond scrutiny.
A further concern is that countries may develop various sensitive nuclear fuel cycle facilities and research reactors under full safeguards and then subsequently opt out of the NPT. Bilateral agreements, such as insisted upon by Australia and Canada for sale of uranium, address this by including fallback provisions, but many countries are outside the scope of these agreements. If a nuclear-capable country does leave the NPT, it is likely to be reported by the IAEA to the UN Security Council, just as if it were in breach of its safeguards agreement. Trade sanctions would then be likely.
IAEA safeguards can help ensure that uranium supplied as nuclear fuel and other nuclear supplies do not contribute to nuclear weapons proliferation. In fact, the worldwide application of those safeguards and the substantial world trade in uranium for nuclear electricity make the proliferation of nuclear weapons much less likely.
The Additional Protocol, once it is widely in force, will provide credible assurance that there are no undeclared nuclear materials or activities in the states concerned. This will be a major step forward in preventing nuclear proliferation.
Other developments.
The Nuclear Suppliers Group communicated its guidelines, essentially a set of export rules, to the IAEA in 1978. These were to ensure that transfers of nuclear material or equipment would not be diverted to unsafeguarded nuclear fuel cycle or nuclear explosive activities, and formal government assurances to this effect were required from recipients. The Guidelines also recognised the need for physical protection measures in the transfer of sensitive facilities, technology and weapons-usable materials, and strengthened retransfer provisions. The group began with seven members – the United States, the former USSR, the UK, France, Germany, Canada and Japan – but now includes 46 countries including all five nuclear weapons states.
The International Framework for Nuclear Energy Cooperation is an international project involving 25 partner countries, 28 observer and candidate partner countries, and the International Atomic Energy Agency, the Generation IV International Forum, and the European Commission. It´s goal is to "[..] provide competitive, commercially-based services as an alternative to a state’s development of costly, proliferation-sensitive facilities, and address other issues associated with the safe and secure management of used fuel and radioactive waste."
According to Kenneth D. Bergeron's "Tritium on Ice: The Dangerous New Alliance of Nuclear Weapons and Nuclear Power", tritium is not classified as a 'special nuclear material' but rather as a 'by-product'. It is seen as an important litmus test on the seriousness of the United States' intention to nuclear disarm. This radioactive super-heavy hydrogen isotope is used to boost the efficiency of fissile materials in nuclear weapons. The United States resumed tritium production in 2003 for the first time in 15 years. This could indicate that there is a potential nuclear arm stockpile replacement since the isotope naturally decays.
In May 1995, NPT parties reaffirmed their commitment to a Fissile Materials Cut-off Treaty to prohibit the production of any further fissile material for weapons. This aims to complement the Comprehensive Test Ban Treaty of 1996 (not entered into force as of 2011) and to codify commitments made by the United States, the UK, France and Russia to cease production of weapons material, as well as putting a similar ban on China. This treaty will also put more pressure on Israel, India and Pakistan to agree to international verification.
On 9 August 2005, Ayatollah Ali Khamenei issued a fatwa forbidding the production, stockpiling and use of nuclear weapons. Khamenei's official statement was made at the meeting of the International Atomic Energy Agency (IAEA) in Vienna. As of February 2006 Iran formally announced that uranium enrichment within their borders has continued. Iran claims it is for peaceful purposes but the United Kingdom, France, Germany, and the United States claim the purpose is for nuclear weapons research and construction.
Unsanctioned nuclear activity.
NPT Non Signatories.
India, Pakistan and Israel have been "threshold" countries in terms of the international non-proliferation regime. They possess or are quickly capable of assembling one or more nuclear weapons. They have remained outside the 1970 NPT. They are thus largely excluded from trade in nuclear plant or materials, except for safety-related devices for a few safeguarded facilities.
In May 1998 India and Pakistan each exploded several nuclear devices underground. This heightened concerns regarding an arms race between them, with Pakistan involving the People's Republic of China, an acknowledged nuclear weapons state. Both countries are opposed to the NPT as it stands, and India has consistently attacked the Treaty since its inception in 1970 labeling it as a lopsided treaty in favor of the nuclear powers.
Relations between the two countries are tense and hostile, and the risks of nuclear conflict between them have long been considered quite high. Kashmir is a prime cause of bilateral tension, its sovereignty being in dispute since 1948. There is persistent low level military conflict due to Pakistan backing an insurgency there and the disputed status of Kashmir.
Both engaged in a conventional arms race in the 1980s, including sophisticated technology and equipment capable of delivering nuclear weapons. In the 1990s the arms race quickened. In 1994 India reversed a four-year trend of reduced allocations for defence, and despite its much smaller economy, Pakistan was expected to push its own expenditures yet higher. Both have lost their patrons: India, the former USSR, and Pakistan, the United States.
But it is the growth and modernization of China's nuclear arsenal and its assistance with Pakistan's nuclear power programme and, reportedly, with missile technology, which exacerbate Indian concerns. In particular, Pakistan is aided by China's People's Liberation Army, which operates somewhat autonomously within that country as an exporter of military material.
India.
Nuclear power for civil use is well established in India. Its civil nuclear strategy has been directed towards complete independence in the nuclear fuel cycle, necessary because of its outspoken rejection of the NPT. This self-sufficiency extends from uranium exploration and mining through fuel fabrication, heavy water production, reactor design and construction, to reprocessing and waste management. It has a small fast breeder reactor and is planning a much larger one. It is also developing technology to utilise its abundant resources of thorium as a nuclear fuel.
India has 14 small nuclear power reactors in commercial operation, two larger ones under construction, and ten more planned. The 14 operating ones (2548 MWe total) comprise:
The two under construction and two of the planned ones are 450 MWe versions of these 200 MWe domestic products. Construction has been seriously delayed by financial and technical problems. In 2001 a final agreement was signed with Russia for the country's first large nuclear power plant, comprising two VVER-1000 reactors, under a Russian-financed US$3 billion contract. The first unit is due to be commissioned in 2007. A further two Russian units are under consideration for the site. Nuclear power supplied 3.1% of India's electricity in 2000.
Its weapons material appears to come from a Canadian-designed 40MW "research" reactor which started up in 1960, well before the NPT, and a 100MW indigenous unit in operation since 1985. Both use local uranium, as India does not import any nuclear fuel. It is estimated that India may have built up enough weapons-grade plutonium for a hundred nuclear warheads.
It is widely believed that the nuclear programs of India and Pakistan used CANDU reactors to produce fissionable materials for their weapons; however, this is not accurate. Both Canada (by supplying the 40 MW research reactor) and the United States (by supplying 21 tons of heavy water) supplied India with the technology necessary to create a nuclear weapons program, dubbed CIRUS (Canada-India Reactor, United States). Canada sold India the reactor on the condition that the reactor and any by-products would be . Similarly, the United States sold India heavy water for use in the reactor . India, in violation of these agreements, used the Canadian-supplied reactor and American-supplied heavy water to produce plutonium for their first nuclear explosion, Smiling Buddha. The Indian government controversially justified this, however, by claiming that Smiling Buddha was a "peaceful nuclear explosion."
The country has at least three other research reactors including the tiny one which is exploring the use of thorium as a nuclear fuel, by breeding fissile U-233. In addition, an advanced heavy-water thorium cycle is under development.
India exploded a nuclear device in 1974, the so-called Smiling Buddha test, which it has consistently claimed was for peaceful purposes. Others saw it as a response to China's nuclear weapons capability. It was then universally perceived, notwithstanding official denials, to possess, or to be able to quickly assemble, nuclear weapons. In 1999 it deployed its own medium-range missile and has developed an intermediate-range missile capable of reaching targets in China's industrial heartland.
In 1995 the United States quietly intervened to head off a proposed nuclear test. However, in 1998 there were five more tests in Operation Shakti. These were unambiguously military, including one claimed to be of a sophisticated thermonuclear device, and their declared purpose was "to help in the design of nuclear weapons of different yields and different delivery systems".
Indian security policies are driven by:
It perceives nuclear weapons as a cost-effective political counter to China's nuclear and conventional weaponry, and the effects of its nuclear weapons policy in provoking Pakistan is, by some accounts, considered incidental.
India has had an unhappy relationship with China. After an uneasy ceasefire ended the 1962 war, relations between the two nations were frozen until 1998. Since then a degree of high-level contact has been established and a few elementary confidence-building measures put in place. China still occupies some territory which it captured during the aforementioned war, claimed by India, and India still occupies some territory claimed by China. Its nuclear weapon and missile support for Pakistan is a major bone of contention.
American President George W. Bush met with India Prime Minister Manmohan Singh to discuss India's involvement with nuclear weapons. The two countries agreed that the United States would give nuclear power assistance to India.
Pakistan.
Over the several years, the Nuclear power infrastructure has been well established by Pakistan which is dedicated for the industrial and economic development of the country. Its current nuclear policy is directed and aimed to promote the socio-economic development of the people as a "foremost priority"; and to fulfill the energy, economic, and industrial needs from the nuclear sources. Currently, there are three operational mega-commercial nuclear power plants while three larger ones are under construction. The nuclear power supplies 787MW (roughly ~3.6%) of electricity as of 2012, and the country has projected to produce 8800MW electricity by 2030. Infrastructure established by the IAEA and the U.S. in the 1950s–1960s were based on peaceful research and development and economic prosperity of the country.
Although the civil-sector nuclear power was established in the 1950s, the country has an active nuclear weapons program which was started in the 1970s. The bomb program has its roots after East-Pakistan gained its independence as Bangladesh after India's successful intervention led to a decisive victory on Pakistan in 1971. This large-scale but clandestine atomic bomb project was directed towards the development of ingenious development of reactor and military-grade plutonium. In 1974, when India surprised the outer world with its successful detonation of its own bomb, codename "Smiling Buddha", it became "imperative for Pakistan" to pursue the weapons research. According to leading scientist in the program, it became clear once India detonated the bomb, "Newton's third law" came into "operation", from then on it was a classic case of "action and reaction". Earlier efforts were directed towards mastering the plutonium technology from France, but plutonium route was partially slowed down when the plan was failed after the U.S. intervention to cancel the project. Contrary to popular perception, Pakistan did not forego the "plutonium" route and covertly continued its indegenious research under Munir Khan and it succeeded with plutonium route in the early 1980s. Reacting on India's nuclear test (Smiling Buddha), Bhutto and the country's elite political and military science circle sensed this test as final and dangerous anticipation to Pakistan's "moral and physical existence." With Aziz Ahmed on his side, Bhutto launched a serious diplomatic offense and aggressively maintained at the session of the United Nations Security Council:
Pakistan was exposed to a kind of "nuclear threat and blackmail" unparalleled elsewhere... (...)... If the world's community failed to provide political insurance to Pakistan and other countries against the nuclear blackmail, these countries would be constraint to launch atomic bomb programs of their own!... [A]ssurances provided by the United Nations were not "Enough!"... —Zulfikar Ali Bhutto, , source
After 1974, Bhutto's government redoubled its effort, this time equally focused on uranium and plutonium. Pakistan had established science directorates in almost all of her embassies in the important countries of the world, with theoretical physicist S.A. Butt being the director. Abdul Qadeer Khan then established a network through Dubai to smuggle URENCO technology to Engineering Research Laboratories. Earlier, he worked with "Physics Dynamics Research Laboratories" (FDO), a subsidiary of the Dutch firm VMF-Stork based in Amsterdam. Later after joining, the Urenco, he had access through photographs and documents of the technology. Against the popular perception, the technology that A.Q. Khan had brought from Urenco was based on first generation civil rector technology, filled with many serious technical errors, though it was authentic and vital link for centrifuge project of the country. After the British Government stopped the British subsidiary of the American Emerson Electric Co. from shipping the components to Pakistan, he describes his frustration with a supplier from Germany as: "That man from the German team was unethical. When he did not get the order from us, he wrote a letter to a Labour Party member and questions were asked in [British] Parliament." By 1978, his efforts were paid off and made him into a national hero. In 1981, as a tribute, President General Muhammad Zia-ul-Haq, renamed the research institute after his name.
In early 1996, Prime minister Benazir Bhutto made it clear that "if India conducts a nuclear test, Pakistan could be forced to "follow suit". In 1997, her statement was echoed by Prime minister Nawaz Sharif who maintained to the fact that: "Since 1972, [P]akistan had progressed significantly, and we have left that stage (developmental) far behind. Pakistan will not be made a "hostage" to India by signing the CTBT, before (India).!" In May 1998, within weeks of India's nuclear tests, Pakistan announced that it had conducted six underground tests in the Chagai Hills, five on the 28th and one on the 30th of that month. Seismic events consistent with these claims were recorded.
In 2004, the revelation of A.Q. Khan's efforts led the exposure of many defunct European consortium who defied export restrictions in the 1970s, and many of defunct Dutch companies exported thousands of centrifuges to Pakistan as early as 1976. Many centrifuge components were apparently manufactured in Malaysian Scomi Precision Engineering with the assistance of South Asian and German companies, and used a UAE-based computer company as a false front.
It was widely believed to have direct involvement of the government of Pakistan. This claim could not be verified due to the refusal of the government of Pakistan to allow IAEA to interview the alleged head of the nuclear black market, who happened to be no other than A.Q. Khan. Confessing his crimes later a month on national television, he bailed out the government by taking full responsibility. Independent investigation conducted by IISS confirmed that he had control over the import-export deals, and his acquisition activities were largely unsupervised by Pakistan governmental authorities. All of his activities went undetected for several years. He duly confessed of running the atomic proliferation ring from Pakistan to Iran and North Korea. He was immediately given presidential immunity. Exact nature of the involvement at the governmental level is still unclear, but the manner in which the government acted cast doubt on the sincerity of Pakistan.
North Korea.
The Democratic Peoples Republic of Korea (or better known as North Korea), joined the NPT in 1985 and had subsequently signed a safeguards agreement with the IAEA. However, it was believed that North Korea was diverting plutonium extracted from the fuel of its reactor at Yongbyon, for use in nuclear weapons. The subsequent confrontation with IAEA on the issue of inspections and suspected violations, resulted in North Korea threatening to withdraw from the NPT in 1993. This eventually led to negotiations with the United States resulting in the Agreed Framework of 1994, which provided for IAEA safeguards being applied to its reactors and spent fuel rods. These spent fuel rods were sealed in canisters by the United States to prevent North Korea from extracting plutonium from them. North Korea had to therefore freeze its plutonium programme.
During this period, Pakistan-North Korea cooperation in missile technology transfer was being established. A high level delegation of Pakistan military visited North Korea in August–September 1992, reportedly to discuss the supply of missile technology to Pakistan. In 1993, PM Benazir Bhutto repeatedly traveled to China, and the paid state visit to North Korea. The visits are believed to be related to the subsequent acquisition technology to developed its Ghauri system by Pakistan. During the period 1992–1994, A.Q. Khan was reported to have visited North Korea thirteen times. The missile cooperation program with North Korea was under Dr. A. Q. Khan Research Laboratories. At this time China was under U.S. pressure not to supply the M Dongfeng series of missiles to Pakistan. It is believed by experts that possibly with Chinese connivance and facilitation, the latter was forced to approach North Korea for missile transfers. Reports indicate that North Korea was willing to supply missile sub-systems including rocket motors, inertial guidance systems, control and testing equipment for US$50 million.
It is not clear what North Korea got in return. Joseph S. Bermudez Jr. in "Jane's Defence Weekly" (27 November 2002) reports that Western analysts had begun to question what North Korea received in payment for the missiles; many suspected it was the nuclear technology. The KRL was in charge of both uranium program and also of the missile program with North Korea. It is therefore likely during this period that cooperation in nuclear technology between Pakistan and North Korea was initiated. Western intelligence agencies began to notice exchange of personnel, technology and components between KRL and entities of the North Korean 2nd Economic Committee (responsible for weapons production).
A "New York Times" report on 18 October 2002 quoted U.S. intelligence officials having stated that Pakistan was a major supplier of critical equipment to North Korea. The report added that equipment such as gas centrifuges appeared to have been "part of a barter deal" in which North Korea supplied Pakistan with missiles. Separate reports indicate ("The Washington Times", 22 November 2002) that U.S. intelligence had as early as 1999 picked up signs that North Korea was continuing to develop nuclear arms. Other reports also indicate that North Korea had been working covertly to develop an enrichment capability for nuclear weapons for at least five years and had used technology obtained from Pakistan (Washington Times, 18 October 2002).
Israel.
Israel is also thought to possess an arsenal of potentially up to several hundred nuclear warheads based on estimates of the amount of fissile material produced by Israel. This has never been openly confirmed or denied however, due to Israel's policy of deliberate ambiguity.
An Israeli nuclear installation is located about ten kilometers to the south of Dimona, the Negev Nuclear Research Center. Its construction commenced in 1958, with French assistance. The official reason given by the Israeli and French governments was to build a nuclear reactor to power a "desalination plant", in order to "green the Negev". The purpose of the Dimona plant is widely assumed to be the manufacturing of nuclear weapons, and the majority of defense experts have concluded that it does in fact do that. However, the Israeli government refuses to confirm or deny this publicly, a policy it refers to as "ambiguity".
Norway sold 20 tonnes of heavy water needed for the reactor to Israel in 1959 and 1960 in a secret deal. There were no "safeguards" required in this deal to prevent usage of the heavy water for non-peaceful purposes. The British newspaper "Daily Express" accused Israel of working on a bomb in 1960.
When the United States intelligence community discovered the purpose of the Dimona plant in the early 1960s, it demanded that Israel agree to international inspections. Israel agreed, but on a condition that U.S., rather than IAEA, inspectors were used, and that Israel would receive advanced notice of all inspections.
Some claim that because Israel knew the schedule of the inspectors' visits, it was able to hide the alleged purpose of the site from the inspectors by installing temporary false walls and other devices before each inspection. The inspectors eventually informed the U.S. government that their inspections were useless due to Israeli restrictions on what areas of the facility they could inspect. In 1969, the United States terminated the inspections.
In 1986, Mordechai Vanunu, a former technician at the Dimona plant, revealed to the media some evidence of Israel's nuclear program. Israeli agents arrested him from Italy, drugged him and transported him to Israel, and an Israeli court then tried him in secret on charges of treason and espionage, and sentenced him to eighteen years imprisonment. He was freed on 21 April 2004, but was severely limited by the Israeli government. He was arrested again on 11 November 2004, though formal charges were not immediately filed.
Comments on photographs taken by Mordechai Vanunu inside the Negev Nuclear Research Center have been made by prominent scientists. British nuclear weapons scientist Frank Barnaby, who questioned Vanunu over several days, estimated Israel had enough plutonium for about 150 weapons. Ted Taylor, a bomb designer employed by the United States of America has confirmed the several hundred warhead estimate based on Vanunu's photographs.
According to Lieutenant Colonel Warner D. Farr in a report to the USAF Counterproliferation Center while France was previously a leader in nuclear research "Israel and France were at a similar level of expertise after the war, and Israeli scientists could make significant contributions to the French effort." In 1986 Francis Perrin, French high-commissioner for atomic energy from 1951 to 1970 stated that in 1949 Israeli scientists were invited to the Saclay nuclear research facility, this cooperation leading to a joint effort including sharing of knowledge between French and Israeli scientists especially those with knowledge from the Manhattan Project.
Nuclear arms control in South Asia.
The public stance of the two states on non-proliferation differs markedly. Pakistan has initiated a series of regional security proposals. It has repeatedly proposed a nuclear free zone in South Asia and has proclaimed its willingness to engage in nuclear disarmament and to sign the Non-Proliferation Treaty if India would do so. It has endorsed a United States proposal for a regional five power conference to consider non-proliferation in South Asia.
India has taken the view that solutions to regional security issues should be found at the international rather than the regional level, since its chief concern is with China. It therefore rejects Pakistan's proposals.
Instead, the 'Gandhi Plan', put forward in 1988, proposed the revision of the Non-Proliferation Treaty, which it regards as inherently discriminatory in favor of the nuclear-weapon States, and a timetable for complete nuclear weapons disarmament. It endorsed early proposals for a Comprehensive Test Ban Treaty and for an international convention to ban the production of highly enriched uranium and plutonium for weapons purposes, known as the 'cut-off' convention.
The United States for some years, especially under the Clinton administration, pursued a variety of initiatives to persuade India and Pakistan to abandon their nuclear weapons programs and to accept comprehensive international safeguards on all their nuclear activities. To this end, the Clinton administration proposed a conference of the five nuclear-weapon states, Japan, Germany, India and Pakistan.
India refused this and similar previous proposals, and countered with demands that other potential weapons states, such as Iran and North Korea, should be invited, and that regional limitations would only be acceptable if they were accepted equally by China. The United States would not accept the participation of Iran and North Korea and these initiatives have lapsed.
Another, more recent approach, centers on 'capping' the production of fissile material for weapons purposes, which would hopefully be followed by 'roll back'. To this end, India and the United States jointly sponsored a UN General Assembly resolution in 1993 calling for negotiations for a 'cut-off' convention. Should India and Pakistan join such a convention, they would have to agree to halt the production of fissile materials for weapons and to accept international verification on their relevant nuclear facilities (enrichment and reprocessing plants). It appears that India is now prepared to join negotiations regarding such a Cut-off Treaty, under the UN Conference on Disarmament.
Bilateral confidence-building measures between India and Pakistan to reduce the prospects of confrontation have been limited. In 1990 each side ratified a treaty not to attack the other's nuclear installations, and at the end of 1991 they provided one another with a list showing the location of all their nuclear plants, even though the respective lists were regarded as not being wholly accurate. Early in 1994 India proposed a bilateral agreement for a 'no first use' of nuclear weapons and an extension of the 'no attack' treaty to cover civilian and industrial targets as well as nuclear installations.
Having promoted the Comprehensive Test Ban Treaty since 1954, India dropped its support in 1995 and in 1996 attempted to block the Treaty. Following the 1998 tests the question has been reopened and both Pakistan and India have indicated their intention to sign the CTBT. Indian ratification may be conditional upon the five weapons states agreeing to specific reductions in nuclear arsenals. The UN Conference on Disarmament has also called upon both countries "to accede without delay to the Non-Proliferation Treaty", presumably as non-weapons states.
NPT signatories.
Egypt.
In 2004 and 2005, Egypt disclosed past undeclared nuclear activities and material to the IAEA. In 2007 and 2008, high enriched and low enriched uranium particles were found in environmental samples taken in Egypt. In 2008, the IAEA states Egypt's statements were consistent with its own findings. In May 2009, "Reuters" reported that the IAEA was conducting further investigation in Egypt.
Iran.
In 2003, the IAEA reported that Iran had been in breach of its obligations to comply with provisions of its safeguard agreement. In 2005, the IAEA Board of Governors voted in a rare non-consensus decision to find Iran in non-compliance with its NPT Safeguards Agreement and to report that non-compliance to the UN Security Council. In response, the UN Security Council passed a series of resolutions citing concerns about the program. Iran's representative to the UN argues sanctions compel Iran to abandon its rights under the Nuclear Nonproliferation Treaty to peaceful nuclear technology. Iran says its uranium enrichment program is exclusively for peaceful purposes and has enriched uranium to "less than 5 percent," consistent with fuel for a nuclear power plant and significantly below the purity of WEU (around 90%) typically used in a weapons program. The director general of the International Atomic Energy Agency, Yukiya Amano, said in 2009 he had not seen any evidence in IAEA official documents that Iran was developing nuclear weapons.
Iraq.
Up to the late 1980s it was generally assumed that any undeclared nuclear activities would have to be based on the diversion of nuclear material from safeguards. States acknowledged the possibility of nuclear activities entirely separate from those covered by safeguards, but it was assumed they would be detected by national intelligence activities. There was no particular effort by IAEA to attempt to detect them.
Iraq had been making efforts to secure a nuclear potential since the 1960s. In the late 1970s a specialised plant, Osiraq, was constructed near Baghdad. The plant was attacked during the Iran–Iraq War and was destroyed by Israeli bombers in June 1981.
Not until the 1990 NPT Review Conference did some states raise the possibility of making more use of (for example) provisions for "special inspections" in existing NPT Safeguards Agreements. Special inspections can be undertaken at locations other than those where safeguards routinely apply, if there is reason to believe there may be undeclared material or activities.
After inspections in Iraq following the UN Gulf War cease-fire resolution showed the extent of Iraq's clandestine nuclear weapons program, it became clear that the IAEA would have to broaden the scope of its activities. Iraq was an NPT Party, and had thus agreed to place all its nuclear material under IAEA safeguards. But the inspections revealed that it had been pursuing an extensive clandestine uranium enrichment programme, as well as a nuclear weapons design programme.
The main thrust of Iraq's uranium enrichment program was the development of technology for electromagnetic isotope separation (EMIS) of indigenous uranium. This uses the same principles as a mass spectrometer (albeit on a much larger scale). Ions of uranium-238 and uranium-235 are separated because they describe arcs of different radii when they move through a magnetic field. This process was used in the Manhattan Project to make the highly enriched uranium used in the Hiroshima bomb, but was abandoned soon afterwards.
The Iraqis did the basic research work at their nuclear research establishment at Tuwaitha, near Baghdad, and were building two full-scale facilities at Tarmiya and Ash Sharqat, north of Baghdad. However, when the war broke out, only a few separators had been installed at Tarmiya, and none at Ash Sharqat.
The Iraqis were also very interested in centrifuge enrichment, and had been able to acquire some components including some carbon-fibre rotors, which they were at an early stage of testing. In May 1998, Newsweek reported that Abdul Qadeer Khan had sent Iraq centrifuge designs, which were apparently confiscated by the UNMOVIC officials. Iraqi officials said "the documents were authentic but that they had not agreed to work with A. Q. Khan, fearing an ISI sting operation, due to strained relations between two countries. The Government of Pakistan and A. Q. Khan strongly denied this allegation whilst the government declared the evidence to be "fraudulent".
They were clearly in violation of their NPT and safeguards obligations, and the IAEA Board of Governors ruled to that effect. The UN Security Council then ordered the IAEA to remove, destroy or render harmless Iraq's nuclear weapons capability. This was done by mid-1998, but Iraq then ceased all cooperation with the UN, so the IAEA withdrew from this work.
The revelations from Iraq provided the impetus for a very far-reaching reconsideration of what safeguards are intended to achieve.
Libya.
Libya possesses ballistic missiles and previously pursued nuclear weapons under the leadership of Muammar Gaddafi. On 19 December 2003, Gaddafi announced that Libya would voluntarily eliminate all materials, equipment and programs that could lead to internationally proscribed weapons, including weapons of mass destruction and long-range ballistic missiles. Libya signed the Nuclear Non-Proliferation Treaty (NPT) in 1968 and ratified it in 1975, and concluded a safeguards agreement with the International Atomic Energy Agency (IAEA) in 1980. In March 2004, the IAEA Board of Governors welcomed Libya's decision to eliminate its formerly undeclared nuclear program, which it found had violated Libya's safeguards agreement, and approved Libya's Additional Protocol. The United States and the United Kingdom assisted Libya in removing equipment and material from its nuclear weapons program, with independent verification by the IAEA.
Myanmar.
A report in the "Sydney Morning Herald" and "Searchina", a Japanese newspaper, report that two Myanmarese defectors saying that the Myanmar junta was secretly building a nuclear reactor and plutonium extraction facility with North Korea's help, with the aim of acquiring its first nuclear bomb in five years. According to the report, "The secret complex, much of it in caves tunnelled into a mountain at Naung Laing in northern Burma, runs parallel to a civilian reactor being built at another site by Russia that both the Russians and Burmese say will be put under international safeguards." In 2002, Myanmar had notified IAEA of its intention to pursue a civilian nuclear programme. Later, Russia announced that it would build a nuclear reactor in Myanmar. There have also been reports that two Pakistani scientists, from the AQ Khan stable, had been dispatched to Myanmar where they had settled down, to help Myanmar's project. Recently, the David Albright-led Institute for Science and International Security (ISIS) rang alarm bells about Myanmar attempting a nuclear project with North Korean help. If true, the full weight of international pressure will be brought against Myanmar, said officials familiar with developments. But equally, the information that has been peddled by the defectors is also "preliminary" and could be used by the west to turn the screws on Myanmar—on democracy and human rights issues—in the run-up to the elections in the country in 2010. During an ASEAN meeting in Thailand in July 2009, US secretary of state Hillary Clinton highlighted concerns of the North Korean link. "We know there are also growing concerns about military cooperation between North Korea and Burma which we take very seriously," Clinton said. However, in 2012, after contact with the American president, Barack Obama, the Burmese leader, Thein Sein, renounced military ties with DPRK (North Korea).
North Korea.
The Democratic People's Republic of Korea (DPRK) acceded to the NPT in 1985 as a condition for the supply of a nuclear power station by the USSR. However, it delayed concluding its NPT Safeguards Agreement with the IAEA, a process which should take only 18 months, until April 1992.
During that period, it brought into operation a small gas-cooled, graphite-moderated, natural-uranium (metal) fuelled "Experimental Power Reactor" of about 25 MWt (5 MWe), based on the UK Magnox design. While this was a well-suited design to start a wholly indigenous nuclear reactor development, it also exhibited all the features of a small plutonium production reactor for weapons purposes. North Korea also made substantial progress in the construction of two larger reactors designed on the same principles, a prototype of about 200 MWt (50 MWe), and a full-scale version of about 800 MWt (200 MWe). They made only slow progress; construction halted on both in 1994 and has not resumed. Both reactors have degraded considerably since that time and would take significant efforts to refurbish.
In addition it completed and commissioned a reprocessing plant that makes the Magnox spent nuclear fuel safe, recovering uranium and plutonium. That plutonium, if the fuel was only irradiated to a very low burn-up, would have been in a form very suitable for weapons. Although all these facilities at Yongbyon were to be under safeguards, there was always the risk that at some stage, the DPRK would withdraw from the NPT and use the plutonium for weapons.
One of the first steps in applying NPT safeguards is for the IAEA to verify the initial stocks of uranium and plutonium to ensure that all the nuclear materials in the country have been declared for safeguards purposes. While undertaking this work in 1992, IAEA inspectors found discrepancies which indicated that the reprocessing plant had been used more often than the DPRK had declared, which suggested that the DPRK could have weapons-grade plutonium which it had not declared to the IAEA. Information passed to the IAEA by a Member State (as required by the IAEA) supported that suggestion by indicating that the DPRK had two undeclared waste or other storage sites.
In February 1993 the IAEA called on the DPRK to allow special inspections of the two sites so that the initial stocks of nuclear material could be verified. The DPRK refused, and on 12 March announced its intention to withdraw from the NPT (three months' notice is required). In April 1993 the IAEA Board concluded that the DPRK was in non-compliance with its safeguards obligations and reported the matter to the UN Security Council. In June 1993 the DPRK announced that it had "suspended" its withdrawal from the NPT, but subsequently claimed a "special status" with respect to its safeguards obligations. This was rejected by IAEA.
Once the DPRK's non-compliance had been reported to the UN Security Council, the essential part of the IAEA's mission had been completed. Inspections in the DPRK continued, although inspectors were increasingly hampered in what they were permitted to do by the DPRK's claim of a "special status". However, some 8,000 corroding fuel rods associated with the experimental reactor have remained under close surveillance.
Following bilateral negotiations between the United States and the DPRK, and the conclusion of the Agreed Framework in October 1994, the IAEA has been given additional responsibilities. The agreement requires a freeze on the operation and construction of the DPRK's plutonium production reactors and their related facilities, and the IAEA is responsible for monitoring the freeze until the facilities are eventually dismantled. The DPRK remains uncooperative with the IAEA verification work and has yet to comply with its safeguards agreement.
While Iraq was defeated in a war, allowing the UN the opportunity to seek out and destroy its nuclear weapons programme as part of the cease-fire conditions, the DPRK was not defeated, nor was it vulnerable to other measures, such as trade sanctions. It can scarcely afford to import anything, and sanctions on vital commodities, such as oil, would either be ineffective or risk provoking war.
Ultimately, the DPRK was persuaded to stop what appeared to be its nuclear weapons programme in exchange, under the agreed framework, for about US$5 billion in energy-related assistance. This included two 1000 MWe light water nuclear power reactors based on an advanced U.S. System-80 design.
In January 2003 the DPRK withdrew from the NPT. In response, a series of discussions among the DPRK, the United States, and China, a series of six-party talks (the parties being the DPRK, the ROK, China, Japan, the United States and Russia) were held in Beijing; the first beginning in April 2004 concerning North Korea's weapons program.
On 10 January 2005, North Korea declared that it was in the possession of nuclear weapons. On 19 September 2005, the fourth round of the Six-Party Talks ended with a joint statement in which North Korea agreed to end its nuclear programs and return to the NPT in exchange for diplomatic, energy and economic assistance. However, by the end of 2005 the DPRK had halted all six-party talks because the United States froze certain DPRK international financial assets such as those in a bank in Macau.
On 9 October 2006, North Korea announced that it has performed its first-ever nuclear weapon test. On 18 December 2006, the six-party talks finally resumed. On 13 February 2007, the parties announced "Initial Actions" to implement the 2005 joint statement including shutdown and disablement of North Korean nuclear facilities in exchange for energy assistance. Reacting to UN sanctions imposed after missile tests in April 2009, North Korea withdrew from the six-party talks, restarted its nuclear facilities and conducted a second nuclear test on 25 May 2009.
On 12 February 2013, North Korea conducted an underground nuclear explosion with an estimated yield of 6 to 7 kilotonnes. The detonation registered a magnitude 4.9 disturbance in the area around the epicenter.
"See also: North Korea and weapons of mass destruction and Six-party talks"
Russia.
Security of nuclear weapons in Russia remains a matter of concern. According to high-ranking Russian SVR defector Tretyakov, he had a meeting with two Russian businessman representing a state-created "C-W" corporation in 1991. They came up with a project of destroying large quantities of chemical wastes collected from Western countries at the island of Novaya Zemlya (a test place for Soviet nuclear weapons) using an underground nuclear blast. The project was rejected by Canadian representatives, but one of the businessmen told Tretyakov that he keeps his own nuclear bomb at his dacha outside Moscow. Tretyakov thought that man was insane, but the "businessmen" (Vladimir K. Dmitriev) replied: "Do not be so naive. With economic conditions the way they are in Russia today, anyone with enough money can buy a nuclear bomb. It's no big deal really".
South Africa.
In 1991, South Africa acceded to the NPT, concluded a comprehensive safeguards agreement with the IAEA, and submitted a report on its nuclear material subject to safeguards. At the time, the state had a nuclear power programme producing nearly 10% of the country's electricity, whereas Iraq and North Korea only had research reactors.
The IAEA's initial verification task was complicated by South Africa's announcement that between 1979 and 1989 it built and then dismantled a number of nuclear weapons. South Africa asked the IAEA to verify the conclusion of its weapons programme. In 1995 the IAEA declared that it was satisfied all materials were accounted for and the weapons programme had been terminated and dismantled.
South Africa has signed the NPT, and now holds the distinction of being the only known state to have indigenously produced nuclear weapons, and then verifiably dismantled them.
Syria.
On 6 September 2007, Israel bombed an officially unidentified site in Syria which it later asserted was a nuclear reactor under construction ("see Operation Orchard"). The alleged reactor was not asserted to be operational and it was not asserted that nuclear material had been introduced into it. Syria said the site was a military site and was not involved in any nuclear activities. The IAEA requested Syria to provide further access to the site and any other locations where the debris and equipment from the building had been stored. Syria denounced what it called the Western "fabrication and forging of facts" in regards to the incident. IAEA Director General Mohamed ElBaradei criticized the strikes and deplored that information regarding the matter had not been shared with his agency earlier.
United States cooperation on nuclear weapons with the United Kingdom.
The United States has given the UK considerable assistance with nuclear weapon design and construction since the 1958 US–UK Mutual Defence Agreement. In 1974 a CIA proliferation assessment noted that "In many cases [the UK's sensitive technology in nuclear and missile fields] is based on technology received from the United States and could not legitimately be passed on without U.S. permission."
The U.S. President authorized the transfer of "nuclear weapon parts" to the UK between at least the years 1975 to 1996. The UK National Audit Office noted that most of the UK Trident warhead development and production expenditure was incurred in the United States, which would supply "certain warhead-related components". Some of the fissile materials for the UK Trident warhead were purchased from the United States. Declassified U.S. Department of Energy documents indicate the UK Trident warhead system was involved in non-nuclear design activities alongside the U.S. W76 nuclear warhead fitted in some U.S. Navy Trident missiles, leading the Federation of American Scientists to speculate that the UK warhead may share design information from the W76.
Under the Mutual Defence Agreement 5.37 tonnes of UK-produced plutonium was sent to the United States in return for 6.7 kg of tritium and 7.5 tonnes of highly enriched uranium over the period 1960–1979. A further 0.47 tonne of plutonium was swapped between the UK and United States for reasons that remain classified. Some of the UK produced plutonium was used in 1962 by the United States for a nuclear weapon test of reactor-grade plutonium.
The United States has supplied nuclear weapon delivery systems to support the UK nuclear forces since before the signing of the NPT. The renewal of this agreement is due to take place through the second decade of the 21st century.
Breakout capability.
For a state that does not possess nuclear weapons, the capability to produce one or more weapons quickly and with little warning is called a breakout capability.
Arguments for and against proliferation.
There has been much debate in the academic study of International Security as to the advisability of proliferation. In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles DeGaulle, argued in books like "The Balance of Terror: Strategy for the Nuclear Age" (1961) that mere possession of a nuclear arsenal, what the French called the "force de frappe", was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability.
Some very prominent neo-realist scholars, such as Kenneth Waltz, Emeritus Professor of Political Science at UC Berkeley and Adjunct Senior Research Scholar at Columbia University, and John Mearsheimer, R. Wendell Harrison Distinguished Service Professor of Political Science at the University of Chicago, continue to argue along the lines of Gallois (though these scholars rarely acknowledge their intellectual debt to Gallois and his contemporaries). Specifically, these scholars advocate some forms of nuclear proliferation, arguing that it will decrease the likelihood of war, especially in troubled regions of the world. Aside from the majority opinion which opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favor selective proliferation, and those such as Waltz, who advocate a laissez-faire attitude to programs like North Korea's.
Total proliferation.
In embryo, Waltz argues that the logic of mutually assured destruction (MAD) should work in all security environments, regardless of historical tensions or recent hostility. He sees the Cold War as the ultimate proof of MAD logic – the only occasion when enmity between two Great Powers did not result in military conflict. This was, he argues, because nuclear weapons promote caution in decision-makers. Neither Washington nor Moscow would risk a nuclear apocalypse to advance territorial or power goals, hence a peaceful stalemate ensued (Waltz and Sagan (2003), p. 24). Waltz believes there to be no reason why this effect would not occur in all circumstances.
Selective proliferation.
John Mearsheimer would not support Waltz's optimism in the majority of potential instances; however, he has argued for nuclear proliferation as policy in certain places, such as post–Cold War Europe. In two famous articles, Professor Mearsheimer opines that Europe is bound to return to its pre–Cold War environment of regular conflagration and suspicion at some point in the future. He advocates arming both Germany and Ukraine with nuclear weaponry in order to achieve a balance of power between these states in the east and France/UK in the west. If this does not occur, he is certain that war will eventually break out on the European continent (Mearsheimer (1990), pp. 5–56 and (1993), pp. 50–66).
Another separate argument against Waltz's open proliferation and in favor of Mearsheimer's selective distribution is the possibility of nuclear terrorism. Some countries included in the aforementioned laissez-faire distribution could predispose the transfer of nuclear materials or a bomb falling into the hands of groups not affiliated with any governments. Such countries would not have the political will or ability to safeguard attempts at devices being transferred to a third party. Not being deterred by self-annihilation, terrorism groups could push forth their own nuclear agendas or be used as shadow fronts to carry out the attack plans by mentioned unstable governments.
Arguments against both positions.
There are numerous arguments presented against both selective and total proliferation, generally targeting the very neorealist assumptions (such as the primacy of military security in state agendas, the weakness of international institutions, and the long-run unimportance of economic integration and globalization to state strategy) its proponents tend to make. With respect to Mearsheimer's specific example of Europe, many economists and neoliberals argue that the economic integration of Europe through the development of the European Union has made war in most of the European continent so disastrous economically so as to serve as an effective deterrent. Constructivists take this one step further, frequently arguing that the development of EU political institutions has led or will lead to the development of a nascent European identity, which most states on the European continent wish to partake in to some degree or another, and which makes all states within or aspiring to be within the EU regard war between them as unthinkable.
As for Waltz, the general opinion is that most states are not in a position to safely guard against nuclear use, that he underestimates the long-standing antipathy in many regions, and that weak states will be unable to prevent – or will actively provide for – the disastrous possibility of nuclear terrorism. Waltz has dealt with all of these objections at some point in his work; though to many, he has not adequately responded (Betts (2000)).
The Learning Channel documentary Doomsday: "On The Brink" illustrated 40 years of U.S. and Soviet nuclear weapons accidents. Even the 1995 Norwegian rocket incident demonstrated a potential scenario in which Russian democratization and military downsizing at the end of the Cold War did not eliminate the danger of accidental nuclear war through command and control errors. After asking: might a future Russian ruler or renegade Russian general be tempted to use nuclear weapons to make foreign policy? The documentary writers revealed a greater danger of Russian security over its nuclear stocks, but especially the ultimate danger of human nature to want the ultimate weapon of mass destruction to exercise political and military power. Future world leaders might not understand how close the Soviets, Russians, and Americans were to doomsday, how easy it all seemed because apocalypse was avoided for a mere 40 years between rivals, politicians not terrorists, who loved their children and did not want to die, against 30,000 years of human prehistory. History and military experts agree that proliferation can be slowed, but never stopped (technology cannot be uninvented).
Proliferation begets proliferation.
Proliferation begets proliferation is a concept described by Scott Sagan in his article, "Why Do States Build Nuclear Weapons?". This concept can be described as a strategic chain reaction. If one state produces a nuclear weapon it creates almost a domino effect within the region. States in the region will seek to acquire nuclear weapons to balance or eliminate the security threat. Sagan describes this reaction best in his article when he states, “Every time one state develops nuclear weapons to balance against its main rival, it also creates a nuclear threat to another region, which then has to initiate its own nuclear weapons program to maintain its national security” (Sagan, pg. 70). Going back through history we can see how this has taken place. When the United States demonstrated that it had nuclear power capabilities after the bombing of Hiroshima and Nagasaki, the Russians started to develop their program in preparation for the Cold War. With the Russian military buildup, France and the United Kingdom perceived this as a security threat and therefore they pursued nuclear weapons (Sagan, pg 71).
Iran.
Former Iranian President Mahmoud Ahmadinejad has been a frequent critic of the concept of nuclear apartheid as it has been put into practice by several countries, particularly the United States. In an interview with CNN's Christiane Amanpour, Ahmadinejad said that Iran was "against 'nuclear apartheid,' which means some have the right to possess it, use the fuel, and then sell it to another country for 10 times its value. We're against that. We say clean energy is the right of all countries. But also it is the duty and the responsibility of all countries, including ours, to set up frameworks to stop the proliferation of it." Hours after that interview, he spoke passionately in favor of Iran's right to develop nuclear technology, claiming the nation should have the same liberties.
Iran is a signatory of the Nuclear Non-Proliferation Treaty and claims that any work done in regards to nuclear technology is related only to civilian uses, which is acceptable under the treaty. Iran violated the treaty by performing uranium-enrichment in secret, after which the United Nations Security Council ordered Iran to stop all uranium-enrichment.
India.
India has also been discussed in the context of nuclear apartheid. India has consistently attempted to pass measures that would call for full international disarmament, however they have not succeeded due to protests from those states that already have nuclear weapons. In light of this, India viewed nuclear weapons as a necessary right for all nations as long as certain states were still in possession of nuclear weapons. India stated that nuclear issues were directly related to national security.
Years before India's first underground nuclear test in 1998, the Comprehensive Nuclear-Test-Ban Treaty was passed. Some have argued that coercive language was used in an attempt to persuade India to sign the treaty, which was pushed for heavily by neighboring China. India viewed the treaty as a means for countries that already had nuclear weapons, primarily the five nations of the United Nations Security Council, to keep their weapons while ensuring that no other nations could develop them.
External links and references.
Organizations

</doc>
<doc id="22159" url="http://en.wikipedia.org/wiki?curid=22159" title="NPT">
NPT

NPT may refer to:

</doc>
<doc id="22161" url="http://en.wikipedia.org/wiki?curid=22161" title="Nuclear energy">
Nuclear energy

Nuclear energy may refer to:

</doc>
<doc id="22164" url="http://en.wikipedia.org/wiki?curid=22164" title="Netlist">
Netlist

The word netlist can be used in several different contexts, but perhaps the most popular is in the field of electronic design. In this context, a "netlist" describes the connectivity of an electronic design, that is, a single netlist is a list of all the component terminals that should be electrically connected together for the circuit to work. 
Netlists usually convey connectivity information and provide nothing more than instances, nets, and perhaps some attributes. If they express much more than this, they are usually considered to be a hardware description language such as Verilog, VHDL, or any one of several specific languages designed for input to simulators.
Netlists can be either "physical" or "logical"; either "instance-based" or "net-based"; and "flat" or "hierarchical". The latter can be either "folded" or "unfolded".
Contents and structure of a netlist.
Most netlists either contain or refer to descriptions of the parts or devices used.
Each time a part is used in a netlist, this is called an "instance."
Thus, each instance has a "master", or "definition".
These definitions will usually list the connections that can be made to that kind of device, and some basic properties of that device.
These connection points are called "ports" or "pins", among several other names.
An "instance" could be anything from a MOSFET transistor or a bipolar transistor, to a resistor, capacitor, or integrated circuit chip.
Instances have "ports". In the case of a vacuum cleaner, these ports would be the three metal prongs in the plug. Each port has a name, and in continuing the vacuum cleaner example, they might be "Neutral", "Live" and "Ground". Usually, each instance will have a unique name, so that if you have two instances of vacuum cleaners, one might be "vac1" and the other "vac2". Besides their names, they might otherwise be identical.
Nets are the "wires" that connect things together in the circuit. There may or may not be any special attributes associated with the nets in a design, depending on the particular language the netlist is written in, and that language's features.
Instance based netlists usually provide a list of the instances used in a design.
Along with each instance, either an ordered list of net names is provided, or a list of pairs provided, of an instance port name, along with the net name to which that port is connected.
In this kind of description, the list of nets can be gathered from the connection lists, and there is no place to associate particular attributes with the nets themselves.
SPICE is an example of instance-based netlists.
Net-based netlists usually describe all the instances and their attributes, then describe each net, and say which port they are connected on each instance.
This allows for attributes to be associated with nets.
EDIF is probably the most famous of the net-based netlists.
Hierarchy.
In large designs, it is a common practice to split the design into pieces, each piece becoming a "definition" which can be used as instances in the design. In the vacuum cleaner analogy, one might have a vacuum cleaner definition with its ports, but now this definition would also include a full description of the machine's internal components and how they connect (motors, switches, etc.), like a wiring diagram does. 
A definition which includes no instances is called a "primitive" (or a "leaf", or other names); whereas a definition which includes instances is "hierarchical".
A "folded" hierarchy allows a single definition to be represented several times by instances. An "unfolded" hierarchy does not allow a definition to be used more than once in the hierarchy. 
Folded hierarchies can be extremely compact. A small netlist of just a few instances can describe designs with a very large number of instances. For example, suppose definition A is a simple primitive, like a memory cell. Then suppose definition B contains 32 instances of A; C contains 32 instances of B; D contains 32 instances of C; and E contains 32 instances of D. The design now contains 5 definitions (A through E) and 128 instances. Yet, E describes a circuit that contains over a million memory cells.
Unfolding.
In a "flat" design, only primitives are instanced. Hierarchical designs can be recursively "exploded" ("flattened") by creating a new copy (with a new name) of each definition each time it is used. If the design is highly folded, expanding it like this will result in a much larger netlist database, but preserves the hierarchy dependencies. Given a hierarchical netlist, the list of instance names in a path from the root definition to a primitive instance specifies the single unique path to that primitive. The paths to every primitive, taken together, comprise a large but flat netlist that is exactly equivalent to the compact hierarchical version.
Backannotation.
Backannotation are data that could be added to a hierarchical netlist. Usually they are kept separate from the netlist, because several such alternate sets of data could be applied to a single netlist. These data may have been extracted from a physical design, and might provide extra information for more accurate simulations. Usually the data are composed of a hierarchical path and a piece of data for that primitive or finding the values of RC delay due to interconnection.
Inheritance.
Another concept often used in netlists is that of inheritance. Suppose a definition of a capacitor has an associated attribute called "Capacitance", corresponding to the physical property of the same name, with a default value of "100 pF" (100 picofarads). Each instance of this capacitor might also have such an attribute, only with a different value of capacitance. And other instances might not associate any capacitance at all. In the case where no capacitance is specified for an instance, the instance will "inherit" the 100 pF value from its definition. A value specified will "override" the value on the definition. If a great number of attributes end up being the same as on the definition, a great amount of information can be "inherited", and not have to be redundantly specified in the netlist, saving space, and making the design easier to read by both machines and people.

</doc>
<doc id="22165" url="http://en.wikipedia.org/wiki?curid=22165" title="Nuclear disarmament">
Nuclear disarmament

Nuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-weapon-free world, in which nuclear weapons are completely eliminated.
Nuclear disarmament groups include the Campaign for Nuclear Disarmament, Greenpeace, International Physicians for the Prevention of Nuclear War, Mayors for Peace, Global Zero, and the International Campaign to Abolish Nuclear Weapons. There have been many large anti-nuclear demonstrations and protests. On June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history.
In recent years, some U.S. elder statesmen have also advocated nuclear disarmament. Sam Nunn, William Perry, Henry Kissinger, and George Shultz have called upon governments to embrace the vision of a world free of nuclear weapons, and in various oped columns have proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Organisations such as Global Zero, an international non-partisan group of 300 world leaders dedicated to achieving nuclear disarmament, have also been established.
Proponents of nuclear disarmament say that it would lessen the probability of nuclear war occurring, especially accidentally. Critics of nuclear disarmament say that it would undermine deterrence.
History.
In 1945 in the New Mexico desert, American scientists conducted “Trinity,” the first nuclear weapons test, marking the beginning of the atomic age. Even before the Trinity test, national leaders debated the impact of nuclear weapons on domestic and foreign policy. Also involved in the debate about nuclear weapons policy was the scientific community, through professional associations such as the Federation of Atomic Scientists and the Pugwash Conference on Science and World Affairs.
On August 6, 1945, towards the end of World War II, the Little Boy device was detonated over the Japanese city of Hiroshima. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings (including the headquarters of the 2nd General Army and Fifth Division) and killed approximately 75,000 people, among them 20,000 Japanese soldiers and 20,000 Koreans. Detonation of the Fat Man device exploded over the Japanese city of Nagasaki three days later on 9 August 1945, destroying 60% of the city and killing approximately 35,000 people, among them 23,200-28,200 Japanese civilian munitions workers and 150 Japanese soldiers. Subsequently, the world’s nuclear weapons stockpiles grew.
Operation Crossroads was a series of nuclear weapon tests conducted by the United States at Bikini Atoll in the Pacific Ocean in the summer of 1946. Its purpose was to test the effect of nuclear weapons on naval ships. Pressure to cancel Operation Crossroads came from scientists and diplomats. Manhattan Project scientists argued that further nuclear testing was unnecessary and environmentally dangerous. A Los Alamos study warned "the water near a recent surface explosion will be a witch's brew" of radioactivity. To prepare the atoll for the nuclear tests, Bikini's native residents were evicted from their homes and resettled on smaller, uninhabited islands where they were unable to sustain themselves.
Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when a Hydrogen bomb test in the Pacific contaminated the crew of the Japanese fishing boat "Lucky Dragon". One of the fishermen died in Japan seven months later. The incident caused widespread concern around the world and "provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries". The anti-nuclear weapons movement grew rapidly because for many people the atomic bomb "encapsulated the very worst direction in which society was moving".
Nuclear disarmament movement.
Peace movements emerged in Japan and in 1954 they converged to form a unified "Japanese Council Against Atomic and Hydrogen Bombs". Japanese opposition to the Pacific nuclear weapons tests was widespread, and "an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons". In the United Kingdom, the first Aldermaston March organised by the Direct Action Committee and supported by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. CND organised Aldermaston marches into the late 1960s when tens of thousands of people took part in the four-day events.
On November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.
In 1958, Linus Pauling and his wife presented the United Nations with the petition signed by more than 11,000 scientists calling for an end to nuclear-weapon testing. The "Baby Tooth Survey," headed by Dr Louise Reiss, demonstrated conclusively in 1961 that above-ground nuclear testing posed significant public health risks in the form of radioactive fallout spread primarily via milk from cows that had ingested contaminated grass. Public pressure and the research results subsequently led to a moratorium on above-ground nuclear weapons testing, followed by the Partial Test Ban Treaty, signed in 1963 by John F. Kennedy and Nikita Khrushchev. On the day that the treaty went into force, the Nobel Prize Committee awarded Pauling the Nobel Peace Prize, describing him as "Linus Carl Pauling, who ever since 1946 has campaigned ceaselessly, not only against nuclear weapons tests, not only against the spread of these armaments, not only against their very use, but against all warfare as a means of solving international conflicts." Pauling started the International League of Humanists in 1974. He was president of the scientific advisory board of the World Union for Protection of Life and also one of the signatories of the Dubrovnik-Philadelphia Statement.
In the 1980s, a popular movement for nuclear disarmament again gained strength in the light of the weapons build-up and aggressive rhetoric of US President Ronald Reagan. Reagan had "a world free of nuclear weapons" as his personal mission, and was largely scorned for this in Europe. His officials tried to stop such talks but Reagan was able to start discussions on nuclear disarmament with Soviet Union. He changed the name "SALT" (Strategic Arms Limitation Talks) to "START" (Strategic Arms Reduction Talks).
On June 3, 1981, Thomas launched the White House Peace Vigil in Washington, D.C.. He was later joined on the vigil by anti-nuclear activists Concepcion Picciotto and Ellen Benjamin.
On June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history. International Day of Nuclear Disarmament protests were held on June 20, 1983 at 50 sites across the United States. In 1986, hundreds of people walked from Los Angeles to Washington DC in the Great Peace March for Global Nuclear Disarmament. There were many Nevada Desert Experience protests and peace camps at the Nevada Test Site during the 1980s and 1990s.
On May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades. In 2008, 2009, and 2010, there have been protests about, and campaigns against, several new nuclear reactor proposals in the United States.
There is an annual protest against U.S. nuclear weapons research at Lawrence Livermore National Laboratory in California and in the 2007 protest, 64 people were arrested. There have been a series of protests at the Nevada Test Site and in the April 2007 Nevada Desert Experience protest, 39 people were cited by police. There have been anti-nuclear protests at Naval Base Kitsap for many years, and several in 2008.
World Peace Council.
One of the earliest peace organisations to emerge after the Second World War was the World Peace Council, which was directed by the Communist Party of the Soviet Union through the Soviet Peace Committee. Its origins lay in the Communist Information Bureau's (Cominform) doctrine, put forward 1947, that the world was divided between peace-loving progressive forces led by the Soviet Union and warmongering capitalist countries led by the United States. In 1949, Cominform directed that peace "should now become the pivot of the entire activity of the Communist Parties", and most western Communist parties followed this policy. Lawrence Wittner, a historian of the post-war peace movement, argues that the Soviet Union devoted great efforts to the promotion of the WPC in the early post-war years because it feared an American attack and American superiority of arms at a time when the USA possessed the atom bomb but the Soviet Union had not yet developed it.
In 1950, the WPC launched its Stockholm Appeal calling for the absolute prohibition of nuclear weapons. The campaign won popular support, collecting, it is said, 560 million signatures in Europe, most from socialist countries, including 10 million in France (including that of the young Jacques Chirac), and 155 million signatures in the Soviet Union – the entire adult population. Several non-aligned peace groups who had distanced themselves from the WPC advised their supporters not to sign the Appeal.
The WPC had uneasy relations with the non-aligned peace movement and has been described as being caught in contradictions as "it sought to become a broad world movement while being instrumentalized increasingly to serve foreign policy in the Soviet Union and nominally socialist countries." From the 1950s until the late 1980s it tried to use non-aligned peace organizations to spread the Soviet point of view. At first there was limited co-operation between such groups and the WPC, but western delegates who tried to criticize the Soviet Union or the WPC's silence about Russian armaments were often shouted down at WPC conferences and by the early 1960s they had dissociated themselves from the WPC.
Arms reduction treaties.
After the 1986 Reykjavik Summit between U.S. President Ronald Reagan and the new Soviet General Secretary Mikhail Gorbachev, the United States and the Soviet Union concluded two important nuclear arms reduction treaties: the INF Treaty (1987) and START I (1991). After the end of the Cold War, the United States and the Russian Federation concluded the Strategic Offensive Reductions Treaty (2003) and the New START Treaty (2010).
In the Soviet Union (USSR), voices against Soviet nuclear weapons were few and far between since there was no widespread Freedom of speech and Freedom of the press as political factors. Certain citizens who had become prominent enough to safely criticize the Soviet government, such as Andrei Sakharov, did speak out against nuclear weapons, but that was to little effect. Dissident movements that emerged in the Soviet bloc in the 1980s drew attention to Soviet armaments, some demonstrating at congresses of the World Peace Council, but they were suppressed.
When the extreme danger intrinsic to nuclear war and the possession of nuclear weapons became apparent to all sides during the Cold War, a series of disarmament and nonproliferation treaties were agreed upon between the United States, the Soviet Union, and several other states throughout the world. Many of these treaties involved years of negotiations, and seemed to result in important steps in arms reductions and reducing the risk of nuclear war.
Key treaties
Only one country has been known to ever dismantle their nuclear arsenal completely—the apartheid government of South Africa apparently developed half a dozen crude fission weapons during the 1980s, but they were dismantled in the early 1990s.
United Nations.
In its landmark resolution 1653 of 1961, “Declaration on the prohibition of the use of nuclear and thermo-nuclear weapons,” the UN General Assembly stated that use of nuclear weaponry “would exceed even the scope of war and cause indiscriminate suffering and destruction to mankind and civilization and, as such, is contrary to the rules of international law and to the laws of humanity”.
The UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.
Its goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.
Following the retirement of Sergio Duarte in February 2012, Angela Kane was appointed as the new High Representative for Disarmament Affairs.
U.S. nuclear policy.
Despite a general trend toward disarmament in the early 1990s, the George W. Bush administration repeatedly pushed to fund policies that would allegedly make nuclear weapons more usable in the post–Cold War environment , . To date the U.S. Congress has refused to fund many of these policies. However, some feel that even considering such programs harms the credibility of the United States as a proponent of nonproliferation.
Recent controversial U.S. nuclear policies.
Former U.S. officials Henry Kissinger, George Shultz, Bill Perry, and Sam Nunn (aka 'The Gang of Four' on Nuclear Deterrence)." proposed in January 2007 that the United States rededicate itself to the goal of eliminating nuclear weapons, concluding: "We endorse setting the goal of a world free of nuclear weapons and working energetically on the actions required to achieve that goal." Arguing a year later that "with nuclear weapons more widely available, deterrence is decreasingly effective and increasingly hazardous," the authors concluded that although "it is tempting and easy to say we can't get there from here, . . . we must chart a course” toward that goal." During his Presidential campaign, U.S. President Elect Barack Obama pledged to "set a goal of a world without nuclear weapons, and pursue it."
U.S. policy options for nuclear terrorism.
The United States has taken the lead in ensuring that nuclear materials globally are properly safeguarded. A popular program that has received bipartisan domestic support for over a decade is the Cooperative Threat Reduction Program (CTR). While this program has been deemed a success, many believe that its funding levels need to be increased so as to ensure that all dangerous nuclear materials are secured in the most expeditious manner possible. The CTR program has led to several other innovative and important nonproliferation programs that need to continue to be a budget priority in order to ensure that nuclear weapons do not spread to actors hostile to the United States.
Key programs:
Other states.
While the vast majority of states have adhered to the stipulations of the Nuclear Nonproliferation Treaty, a few states have either refused to sign the treaty or have pursued nuclear weapons programs while not being members of the treaty. Many view the pursuit of nuclear weapons by these states as a threat to nonproliferation and world peace, and therefore seek policies to discourage the spread of nuclear weapons to these states, a few of which are often described by the US as "rogue states".
Recent developments.
Eliminating nuclear weapons has long been an aim of the pacifist left. But now many mainstream politicians, academic analysts, and retired military leaders also advocate nuclear disarmament. Sam Nunn, William Perry, Henry Kissinger, and George Shultz have called upon governments to embrace the vision of a world free of nuclear weapons, and in three "Wall Street Journal" opeds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Nunn reinforced that agenda during a speech at the Harvard Kennedy School on October 21, 2008, saying, "I’m much more concerned about a terrorist without a return address that cannot be deterred than I am about deliberate war between nuclear powers. You can’t deter a group who is willing to commit suicide. We are in a different era. You have to understand the world has changed." In 2010, the four were featured in a documentary film entitled "Nuclear Tipping Point". The film is a visual and historical depiction of the ideas laid forth in the Wall Street Journal op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.
Global Zero is an international non-partisan group of 300 world leaders dedicated to achieving nuclear disarmament. The initiative, launched in December 2008, promotes a phased withdrawal and verification for the destruction of all devices held by official and unofficial members of the nuclear club. The Global Zero campaign works toward building an international consensus and a sustained global movement of leaders and citizens for the elimination of nuclear weapons. Goals include the initiation of United States-Russia bilateral negotiations for reductions to 1,000 total warheads each and commitments from the other key nuclear weapons countries to participate in multilateral negotiations for phased reductions of nuclear arsenals. Global Zero works to expand the diplomatic dialogue with key governments and continue to develop policy proposals on the critical issues related to the elimination of nuclear weapons.
The International Conference on Nuclear Disarmament took place in Oslo in February, 2008, and was organized by The Government of Norway, the Nuclear Threat Initiative and the Hoover Institute. The Conference was entitled "Achieving the Vision of a World Free of Nuclear Weapons" and had the purpose of building consensus between nuclear weapon states and non-nuclear weapon states in relation to the Nuclear Non-proliferation Treaty.
The Tehran International Conference on Disarmament and Non-Proliferation took place in Tehran in April 2010. The conference was held shortly after the signing of the New START, and resulted in a call of action toward eliminating all nuclear weapons. Representatives from 60 countries were invited to the conference. Non-governmental organizations were also present.
Among the prominent figures who have called for the abolition of nuclear weapons are "the philosopher Bertrand Russell, the entertainer Steve Allen, CNN’s Ted Turner, former Senator Claiborne Pell, Notre Dame president Theodore Hesburg, South African Bishop Desmond Tutu and the Dalai Lama".
Others have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability–instability paradox, including in south Asia. Kenneth Waltz has argued that nuclear weapons have created a nuclear peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common prior to their invention at the end of World War II. In the July 2012 issue of Foreign Affairs Waltz took issue with the view of most U.S., European, and Israeli, commentators and policymakers that a nuclear-armed Iran would be unacceptable. Instead Waltz argues that it would probably be the best possible outcome, as it would restore stability to the Middle East by balancing Israel's regional monopoly on nuclear weapons. Professor John Mueller of Ohio State University, the author of "Atomic Obsession", has also dismissed the need to interfere with Iran's nuclear program and expressed that arms control measures are counterproductive. During a 2010 lecture at the University of Missouri, which was broadcast by C-SPAN, Dr. Mueller has also argued that the threat from nuclear weapons, especially nuclear terrorism, has been exaggerated, both in the popular media and by officials.
Former Secretary Kissinger says there is a new danger, which cannot be addressed by deterrence: "The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn’t operate in any comparable way". George Shultz has said, "If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable".

</doc>
<doc id="22170" url="http://en.wikipedia.org/wiki?curid=22170" title="Net (mathematics)">
Net (mathematics)

In mathematics, more specifically in general topology and related branches, a net or Moore–Smith sequence is a generalization of the notion of a sequence. In essence, a sequence is a function with domain the natural numbers, and in the context of topology, the codomain of this function is usually any topological space. However, in the context of topology, sequences do not fully encode all information about a function between topological spaces. In particular, the following two conditions are not equivalent in general for a map "f" between topological spaces "X" and "Y":
It is true, however, that condition 1 implies condition 2 in the context of all spaces. The difficulty encountered when attempting to prove that condition 2 implies condition 1 lies in the fact that topological spaces are, in general, not first-countable.
If the first-countability axiom were imposed on the topological spaces in question, the two above conditions would be equivalent. In particular, the two conditions are equivalent for metric spaces.
The purpose of the concept of a net, first introduced by E. H. Moore and H. L. Smith in 1922, is to generalize the notion of a sequence so as to confirm the equivalence of the conditions (with "sequence" being replaced by "net" in condition 2). In particular, rather than being defined on a countable linearly ordered set, a net is defined on an arbitrary directed set. In particular, this allows theorems similar to that asserting the equivalence of condition 1 and condition 2, to hold in the context of topological spaces that do not necessarily have a countable or linearly ordered neighbourhood basis around a point. Therefore, while sequences do not encode sufficient information about functions between topological spaces, nets do because collections of open sets in topological spaces are much like directed sets in behaviour. The term "net" was coined by Kelley.
Nets are one of the many tools used in topology to generalize certain concepts that may only be general enough in the context of metric spaces. A related notion, that of the filter, was developed in 1937 by Henri Cartan.
Definition.
If "X" is a topological space, a "net" in "X" is a function from some directed set "A" to "X".
If "A" is a directed set, we often write a net from "A" to "X" in the form ("x"α), which expresses the fact that the element α in "A" is mapped to the element "x"α in "X".
Examples of nets.
Every non-empty totally ordered set is directed. Therefore every function on such a set is a net. In particular, the natural numbers with the usual order form such a set, and a sequence is a function on the natural numbers, so every sequence is a net.
Another important example is as follows. Given a point "x" in a topological space, let "N""x" denote the set of all neighbourhoods containing "x". Then "N""x" is a directed set, where the direction is given by reverse inclusion, so that "S" ≥ "T" if and only if "S" is contained in "T". For "S" in "N""x", let "x""S" be a point in "S". Then ("x""S") is a net. As "S" increases with respect to ≥, the points "x""S" in the net are constrained to lie in decreasing neighbourhoods of "x", so intuitively speaking, we are led to the idea that "x""S" must tend towards "x" in some sense. We can make this limiting concept precise.
Limits of nets.
If ("x"α) is a net from a directed set "A" into "X", and if "Y" is a subset of "X", then we say that ("x"α) is eventually in "Y (or residually in "Y) if there exists an α in "A" so that for every β in "A" with β ≥ α, the point "x"β lies in "Y".
If ("x"α) is a net in the topological space "X", and "x" is an element of "X", we say that the net converges towards "x or has limit "x and write
if and only if
Intuitively, this means that the values "x"α come and stay as close as we want to "x" for large enough α.
Note that the example net given above on the neighborhood system of a point "x" does indeed converge to "x" according to this definition.
Given a base for the topology, in order to prove convergence of a net it is necessary and sufficient to prove that there exists some point "x", such that ("x"α) is eventually in all members of the base containing this putative limit.
Supplementary definitions.
Let φ be a net on "X" based on the directed set "D" and let "A" be a subset of "X", then φ is said to be frequently in (or cofinally in) "A" if for every α in "D" there exists some β ≥ α, β in "D", so that φ(β) is in "A".
A point "x" in "X" is said to be an accumulation point or cluster point of a net if (and only if) for every neighborhood "U" of "x", the net is frequently in "U".
A net φ on set "X" is called universal, or an ultranet if for every subset "A" of "X", either φ is eventually in "A" or φ is eventually in "X" − "A".
Examples.
Sequence in a topological space:
A sequence ("a"1, "a"2, ...) in a topological space "V" can be considered a net in "V" defined on N.
The net is eventually in a subset "Y" of "V" if there exists an N in N such that for every "n" ≥ "N", the point "a""n" is in "Y".
We have lim"n" "a""n" = "L" if and only if for every neighborhood "Y" of "L", the net is eventually in "Y".
The net is frequently in a subset "Y" of "V" if and only if for every "N" in N there exists some "n" ≥ "N" such that "a""n" is in "Y", that is, if and only if infinitely many elements of the sequence are in "Y". Thus a point "y" in "V" is a cluster point of the net if and only if every neighborhood "Y" of "y" contains infinitely many elements of the sequence.
Function from a metric space to a topological space:
Consider a function from a metric space "M" to a topological space "V", and a point "c" of "M". We direct the set "M"\{"c"} reversely according to distance from "c", that is, the relation is "has at least the same distance to "c" as", so that "large enough" with respect to the relation means "close enough to "c"". The function ƒ is a net in "V" defined on "M"\{"c"}.
The net ƒ is eventually in a subset "Y" of "V" if there exists an "a" in "M"\{"c"} such that for every "x" in "M"\{"c"} with d("x","c") ≤ d("a","c"), the point f("x") is in "Y".
We have lim"x" → "c" ƒ("x") = "L" if and only if for every neighborhood "Y" of "L", ƒ is eventually in "Y".
The net ƒ is frequently in a subset "Y" of "V" if and only if for every "a" in "M"\{"c"} there exists some "x" in "M"\{"c"} with d("x","c") ≤ d("a","c") such that "f(x)" is in "Y".
A point "y" in "V" is a cluster point of the net ƒ if and only if for every neighborhood "Y" of "y", the net is frequently in "Y".
Function from a well-ordered set to a topological space:
Consider a well-ordered set [0, "c"] with limit point "c", and a function ƒ from [0, "c") to a topological space "V". This function is a net on [0, "c").
It is eventually in a subset "Y" of "V" if there exists an "a" in [0, "c") such that for every "x" ≥ "a", the point f("x") is in "Y".
We have lim"x" → "c" ƒ("x") = "L" if and only if for every neighborhood "Y" of "L", ƒ is eventually in "Y".
The net ƒ is frequently in a subset "Y" of "V" if and only if for every "a" in [0, "c") there exists some "x" in ["a", "c") such that "f(x)" is in "Y".
A point "y" in "V" is a cluster point of the net ƒ if and only if for every neighborhood "Y" of "y", the net is frequently in "Y".
The first example is a special case of this with "c" = ω.
See also ordinal-indexed sequence.
Properties.
Virtually all concepts of topology can be rephrased in the language of nets and limits. This may be useful to guide the intuition since the notion of limit of a net is very similar to that of limit of a sequence. The following set of theorems and lemmas help cement that similarity:
Cauchy nets.
In a metric space or uniform space, one can speak of "Cauchy nets" in much the same way as Cauchy sequences.
The concept even generalises to Cauchy spaces.
Relation to filters.
A filter is another idea in topology that allows for a general definition for convergence in general topological spaces. The two ideas are equivalent in the sense that they give the same concept of convergence. More specifically, for every filter base an "associated net" can be constructed, and convergence of the filter base implies convergence of the associated net—and the other way around (for every net there is a filter base, and convergence of the net implies convergence of the filter base). Therefore, any theorems that can be proven with one concept can be proven in the other. For instance, continuity of a function from one topological space to the other can be characterized either by the convergence of a net in the domain implying the convergence of the corresponding net in the codomain, or by the same statement with filter bases.
Robert G. Bartle argues that despite their equivalence, it is useful to have both concepts. He argues that nets are enough like sequences to make natural proofs and definitions in analogy to sequences, especially ones using sequential elements, such as is common in analysis, while filters are most useful in algebraic topology. In any case, he shows how the two can be used in combination to prove various theorems in general topology.
Limit superior.
Limit superior and limit inferior of a net of real numbers can be defined in a similar manner as for sequences. Some authors work even with more general structures than the real line, like complete lattices.
For a net formula_2 we put
Limit superior of a net of real numbers has many properties analogous to the case of sequences, e.g.
where equality holds whenever one of the nets is convergent.

</doc>
<doc id="22171" url="http://en.wikipedia.org/wiki?curid=22171" title="Nuclear winter">
Nuclear winter

Nuclear winter (also known as atomic winter) is a hypothetical climatic effect, most often considered a potential threat following a countervalue, or city-targeted, nuclear war. Climate models suggest that the ignition of a hundred or more firestorms that are comparable in intensity to that observed in Hiroshima in 1945 would produce a small nuclear winter. The burning of these firestorms would result in the injection of soot into the Earth's stratosphere, producing an anti-greenhouse effect. The models conclude that the magnitude of this effect from the cumulative products of 100 firestorms would reach sufficient extent to unmistakably alter the global climate, resulting in agricultural losses from the colder weather, and lasting for a period of years, whereas an all-out US-Russia war would cause catastrophic summer cooling by about 20 °C in core agricultural regions of the US, Europe and China, and by as much as 35 °C in Russia.
On the fundamental level, it is known that firestorms can inject sooty smoke into the stratosphere, as each natural occurrence of a wildfire firestorm has been found to "surprisingly frequently" generate minor "nuclear winter" effects. This is somewhat analogous to the frequent volcanic eruptions that inject sulfates into the stratosphere and thereby produce minor volcanic winter effects.
A suite of satellite- and aircraft-based firestorm-soot-monitoring instruments are at the forefront of attempts to accurately determine the lifespan, quantity, injection height, and optical properties of this smoke. Information regarding all of these properties is necessary to ascertain both the length and depth of the cooling effect of firestorms.
Mechanism.
The nuclear winter scenario assumes that 100 or more city firestorms are ignited by the nuclear explosions of a nuclear war, and the firestorms lift large enough amounts of sooty smoke into the upper troposphere and lower stratosphere, soot lifted by the movement offered by the pyrocumulonimbus clouds that form during a firestorm. At 10 - above the Earth's surface, the absorption of sunlight could further heat the soot in the smoke, lifting some or all of it into the stratosphere, where the smoke could persist for years, if there is no rain to wash it out. This aerosol of particles could heat the stratosphere and block out a portion of the sun's light from reaching the surface, causing surface temperatures to drop drastically, and with that, it is predicted surface air temperatures would be akin to, or colder than, a given region's winter for months to years on end.
The modeled stable inversion layer of hot soot between the troposphere and high stratosphere that produces the anti-greenhouse effect was dubbed the "Smokeosphere" by Stephen Schneider et al. in their 1988 paper.
Although it is common in the climate models for the city firestorms to be ignited by nuclear explosions, they need not be ignited by nuclear devices; more conventional ignition sources can instead be the spark of the firestorms. As prior to the previously mentioned solar heating effect, the soot's injection height is controlled by the rate of energy release from the firestorm's fuel, not the size, or lack thereof, of an initial nuclear explosion. For example, the mushroom cloud from the bomb dropped on Hiroshima reached a height of six kilometers ("middle" troposphere) within a few minutes and then dissipated due to winds, while the individual fires within the city took almost three hours to form into a firestorm and produce a "pyrocumulus" cloud, a cloud that is assumed to have reached "upper" tropospheric heights, as over its multiple hours of burning, the firestorm released an estimated 1000 times the energy of the bomb.
While the firestorm of Dresden and Hiroshima and the mass fires of Tokyo and Nagasaki occurred with mere months separating them in 1945, the more intense and conventionally lit Hamburg firestorm occurred in 1943. Despite this, these five fires potentially placed five percent as much smoke into the stratosphere as the hypothetical 100 nuclear-ignited fires of modern models. While it is believed that the effects of the mass of soot emitted by 100 firestorms (one to five teragrams) would have been detectable with technical instruments in WWII, only five percent of that would not have been possible to observe at that time.
Aerosol removal timescale.
The exact timescale for how long this smoke remains, and thus how severely this smoke affects the climate once it reaches the stratosphere, is dependent on both chemical and physical removal processes.
The most important physical removal mechanism is "rainout", both during the "fire-driven convective column" phase—which produces "black rain" near the fire site—and rainout after the convective plume's dispersal, where the smoke is no longer concentrated and thus "wet removal" is believed to be "very efficient." However these efficient removal mechanisms in the troposphere are avoided in the Robock 2007 study, where solar heating is modeled to quickly "loft" the soot into the stratosphere, "detraining" or separating the darker soot particles from the fire clouds' whiter water condensation.
Once in the stratosphere, the physical removal mechanisms having an impact on the timescale of the soot particles' residence are how quickly the aerosol of soot coagulates with other particles, and falls out of the atmosphere via gravity-driven dry deposition, and, to a slower degree, the time it takes for solar radiation pressure to force the particles to a lower level in the atmosphere. Whether by coagulation or radiation pressure, once the aerosol of smoke particles are at this lower atmospheric level, cloud seeding can begin, permitting precipitation to wash the smoke aerosol out of the atmosphere by the wet deposition mechanism.
The chemical processes that affect the removal are dependent on the ability of atmospheric chemistry to oxidize the carbonaceous component of the smoke, via reactions with oxidative species such as ozone and nitrogen oxides, both of which are found at all levels of the atmosphere, and which also occur at greater concentrations when air is heated to high temperatures, which will be discussed later.
Historical data on residence times of aerosols, albeit a different mixture of aerosols, in this case stratospheric sulfur aerosols and volcanic ash from megavolcano eruptions, appear to be in the one-to-two-year time scale.
The satellite tracking of wildfire smoke from pyrocumulonimbus-cloud-injected aerosols indicates that the aerosols are removed in a time span under approximately two months.
Aerosol–atmosphere interactions are still poorly understood.
Soot properties.
Sooty aerosols can have a wide range of properties, as well as complex shapes, making it difficult to determine their evolving optical properties. The conditions present during the creation of the soot are believed to be considerably important as to their final properties, with soot generated on the more efficient spectrum of burning efficiency considered almost "elemental carbon black," while on the more inefficient end of the burning spectrum, greater quantities of partially burnt/oxidized fuel are present. These partially burnt "organics" as they are known, often form "tar balls" and "brown carbon" during common lower-intensity wildfires, and can also coat the purer carbon black particles. However, as the soot of greatest importance is that which is injected to the highest altitudes by the pyroconvection of the firestorm—a fire being fed with storm-force winds of air—it is estimated that the majority of the soot under these conditions is of the more oxidized carbon black nature.
Consequences.
Climatic effects.
A study presented at the annual meeting of the American Geophysical Union in December 2006 found that even a small-scale, regional nuclear war could disrupt the global climate for a decade or more. In a regional nuclear conflict scenario where two opposing nations in the subtropics would each use 50 Hiroshima-sized nuclear weapons (about 15 kiloton each) on major populated centres, the researchers estimated as much as five million tons of soot would be released, which would produce a cooling of several degrees over large areas of North America and Eurasia, including most of the grain-growing regions. The cooling would last for years, and according to the research could be "catastrophic".
Ozone depletion.
A 2008 study published in the Proceedings of the National Academy of Science found that a nuclear weapons exchange between Pakistan and India using their current arsenals could create a near-global ozone hole, triggering human health problems and causing environmental damage for at least a decade. The computer-modeling study looked at a nuclear war between the two countries involving 50 Hiroshima-sized nuclear devices on each side, producing massive urban fires and lofting as much as five million metric tons of soot about 50 mi into the mesosphere. The soot would absorb enough solar radiation to heat surrounding gases, causing a series of chemical reactions that would break down the stratospheric ozone layer protecting Earth from harmful ultraviolet radiation.
Nuclear summer.
A "nuclear summer" is a hypothesized scenario in which, after a nuclear winter has abated, a greenhouse effect then occurs due to CO2 released by combustion and methane released from decay of dead organic matter.
History.
Early work.
In 1952, a few weeks prior to the Ivy Mike(10.4 megaton) test on Elugelab island, there was a concern that the "small particles"/aerosols lifted by the explosion might cool the Earth. Major Norair Lulejian, USAF, and astronomer Natarajan Visvanathan, studied this possibility reporting their findings in 
"Effects of Superweapons Upon the Climate of the World". According to a document by the Defense Threat Reduction Agency, this report was the initial study of the "nuclear winter" concept that was popularized by others decades later. It indicated no appreciable chance of explosion-induced climate change.
Following numerous surface bursts of high yield "Hydrogen bomb" explosions on Pacific Proving Ground islands such as those of Ivy Mike in the year 1952 and Castle Bravo(15 megaton) in 1954, "The Effects of Nuclear Weapons" by Samuel Glasstone was published in 1957 which contained a section entitled "Nuclear Bombs and the Weather" (pages 69–71), which states: "The dust raised in severe volcanic eruptions, such as that at Krakatoa in 1883, is known to cause a noticeable reduction in the sunlight reaching the earth ... The amount of debris remaining in the atmosphere after the explosion of even the largest nuclear weapons is probably not more than about 1 percent or so of that raised by the Krakatoa eruption. Further, solar radiation records reveal that none of the nuclear explosions to date has resulted in any detectable change in the direct sunlight recorded on the ground."
The potential cooling from soil dust was again looked at in 1992, in a US National Academy of Sciences (NAS) report on geoengineering, which estimated that about 1010 kg of stratospheric injected soil dust with particulate grain dimensions of 0.1 to 1 micrometer would be required to mitigate the warming from a doubling of atmospheric CO2, that is, to produce ~ 2 degree celsius of cooling.
In 1969, Paul Crutzen discovered that NOx (oxides of nitrogen) could be an efficient
catalyst for the destruction of the ozone layer/stratospheric ozone. With studies on the potential effects of NOx generated by engine heat in stratosphere flying Supersonic Transport(SST) airplanes in the 1970s serving as a backdrop, John Hampson in 1974 suggested in the journal "Nature" that due to the nuclear fireballs creation of atmospheric NOx, a full-scale nuclear exchange could result in depletion of the ozone shield, possibly subjecting the earth to ultraviolet radiation for a year or more. Hampson's hypothesis "led directly", in 1975, to the United States National Research Council (NRC) reporting on the models of ozone depletion following nuclear war in the book "Long-Term Worldwide Effects of Multiple Nuclear-Weapons Detonations". In this 1975 book it states that a nuclear war involving 4000Mt (megaton) from "present arsenals" would probably deposit much less dust in the stratosphere than the Krakatoa eruption, judging that the effect of dust and oxides of nitrogen would probably be slight climatic cooling which "would probably lie within normal global climatic variability, but the possibility of climatic changes of a more dramatic nature cannot be ruled out".
A study published in 1976 on the experimental measurements of an earlier atmospheric nuclear test as it affected the ozone layer found that nuclear detonations are tentatively exonerated in depleting ozone, after initially discouraging model calculations. In total about 500 megatons were atmospherically detonated between 1945 and 1971, with a peak occurring in 1961-62, when 340 megatons were detonated in the atmosphere by the United States and Soviet Union. During this 1-2 year peak, counting only the multi-megaton range detonations in the two nations nuclear test series, a total yield estimated at 300 megatons of energy was released, due to this, 3 x 10^34 additional molecules of nitric oxide(about 5000 tons per megaton) are believed to have entered the stratosphere, and while ozone depletion of 2.2 percent was noted in 1963, the decline had started prior to 1961 and is believed to have been caused by other meteorological effects, thus the 1985 book "The Effects on the Atmosphere of a Major Nuclear Exchange" states: "one can not draw definite conclusions about the effects of nuclear explosions on stratospheric ozone".
Science Fiction.
The first published suggestion that a cooling of climate or winter could be an effect of a nuclear war, appears to have been originally put forth by Poul Anderson and F.N Waldrop in their post war story "Tomorrow's Children", which appeared in the March 1947 issue of the "Astounding Science Fiction" magazine, the story which is primarily about a team of scientists hunting down mutants, warns of a "Fimbulwinter" caused by dust that blocked sunlight after the recent fictitious nuclear war and speculates that this may even trigger a new ice age. Anderson went on to publish a novel based partly on this story in 1961 titling it; "Twilight World".
1982.
In 1981, William J. Moran began discussions and research in the NRC on the dust effects of a large exchange of nuclear warheads. An NRC study panel on the topic met in December 1981 and April 1982 in preparation of the release of "The Effects on the Atmosphere of a Major Nuclear Exchange" in 1985.
As part of a study on the creation of oxidizing species such as NOx and ozone in the troposphere after a nuclear war, launched in 1980 by "Ambio", a journal of the Royal Swedish Academy of Sciences, Paul Crutzen and John Birks circulated a draft paper in early 1982 with the first quantitative evidence of alterations in short-term climate after a nuclear war. In 1982, a special issue of "Ambio" devoted to the possible environmental consequences of nuclear war by Crutzen and Birks titled "Twilight at Noon" anticipating the nuclear winter scenario. The paper which looked into fires and their climatic effect as "an afterthought" discussed particulate matter from large fires, nitrogen oxide, ozone depletion and the effect of nuclear twilight on agriculture. Crutzen and Birks' calculations suggested that smoke particulates injected into the atmosphere by fires in cities, forests and petroleum reserves could prevent up to 99% of sunlight from reaching the Earth's surface, with major climatic consequences: "The normal dynamic and temperature structure of the atmosphere would therefore change considerably over a large fraction of the Northern Hemisphere, which will probably lead to important changes in land surface temperatures and wind systems." An important implication of their work was that a "first strike" nuclear attack would have severe consequences for the perpetrator.
1983.
Interest in nuclear war environmental effects also arose in the USSR. After becoming aware of the papers by N.P.Bochkov and E.I.Chazov, Russian atmospheric scientist Georgy Golitsyn applied his research on dust-storms to the situation following a large nuclear war. His suggestion that the atmosphere would be heated and that the surface of the planet would cool appeared in "The Herald of the Academy of Sciences" in September 1983.
In 1982, the so-called TTAPS team (Richard P. Turco, Owen Toon, Thomas P. Ackerman, James B. Pollack and Carl Sagan) undertook a computational modeling study of the atmospheric consequences of nuclear war, publishing their results in "Science" in December 1983. The phrase "nuclear winter" was coined by Turco just prior to publication. In this early work, TTAPS carried out the first estimates of the total smoke and dust emissions that would result from a major nuclear exchange, and determined quantitatively the subsequent effects on the atmospheric radiation balance and temperature structure. To compute dust and smoke impacts, they employed a one-dimensional microphysics/radiative-transfer model of the Earth's lower atmosphere (to the mesopause), which defined only the vertical characteristics of the global climate perturbation.
Upon learning of the TTAPS scenarios, Vladimir Alexandrov and G. I. Stenchikov also published a report in 1983 on the climatic consequences of nuclear war based on simulations with a three-dimensional global circulation model. Two years later Vladimir Alexandrov disappeared under mysterious circumstances. Richard Turco and Starley L. Thompson were critical of the Soviet model, Turco claimed it was "a primitive rendition of an obsolete US model".
1986.
In 1984 the WMO commissioned Georgy Golitsyn and N. A. Phillips to review the state of the science. They found that studies generally assumed a scenario that half of the world's nuclear weapons would be used, ~5000 Mt, destroying approximately 1,000 cities, and creating large quantities of carbonaceous smoke - 1– being mostly likely, with a range of 0.2– (NAS; TTAPS assumed ). The smoke resulting would be largely opaque to solar radiation but transparent to infra-red, thus cooling by blocking sunlight but not causing warming from enhancing the greenhouse effect. The optical depth of the smoke can be much greater than unity. Forest fires resulting from non-urban targets could increase aerosol production further. Dust from near-surface explosions against hardened targets also contributes; each Mt-equivalent of explosion could release up to 5 million tons of dust, but most would quickly fall out; high altitude dust is estimated at 0.1-1 million tons per Mt-equivalent of explosion. Burning of crude oil could also contribute substantially.
The 1-D radiative-convective models used in these studies produced a range of results, with coolings up to 15–42 °C between 14 and 35 days after the war, with a "baseline" of about 20 °C. Somewhat more sophisticated calculations using 3-D GCMs (Alexandrov and Stenchikov (1983); Covey, Schneider and Thompson (1984); produced similar results: temperature drops of between 20 and 40 °C, though with regional variations.
All calculations show large heating (up to 80 °C) at the top of the smoke layer at about 10 km; this implies a substantial modification of the circulation there and the possibility of advection of the cloud into low latitudes and the southern hemisphere.
The report made no attempt to compare the likely human impacts of the post-war cooling to the direct deaths from explosions.
In 1987 P. M. Kelly of the University of East Anglia Climatic Research Unit stated that "although there are a handful of vociferous critics, the atmospheric community is united in its conclusion that the threat of nuclear winter is genuine".
1990.
In 1990, in a paper entitled "Climate and Smoke: An Appraisal of Nuclear Winter," TTAPS give a more detailed description of the short- and long-term atmospheric effects of a nuclear war using a three-dimensional model:
First 1 to 3 months:
Following 1 to 3 years:
Kuwait wells in the first Gulf War.
Following Iraq's invasion of Kuwait and Iraqi threats of igniting the country's 800 or so oil wells were made, speculation on the cumulative climatic effect of this, presented at the World Climate Conference in Geneva that November in 1990, ranged from a nuclear winter type scenario, to heavy acid rain and even short term immediate global warming. As threatened, the wells were set ablaze by the retreating Iraqis by March 1991 and the 600 or so successfully set Kuwaiti oil wells were not fully extinguished until November 6, 1991, eight months after the end of the war, and they consumed an estimated six million barrels of oil daily at their peak intensity.
In articles printed in the Wilmington morning star and the Baltimore Sun newspapers of January 1991, prominent authors of nuclear winter papers - Richard P. Turco, John W. Birks, Carl Sagan, Alan Robock and Paul Crutzen together collectively stated that they expected catastrophic nuclear winter like effects with continental sized impacts of "sub-freezing" temperatures as a result of if the Iraqis went through with their threats of igniting 300 to 500 pressurized oil wells and they burned for a few months.
Later when Operation Desert Storm had begun in late January 1991, coinciding with the first few oil fires being lit, Dr. S. Fred Singer and Carl Sagan discussed the possible environmental impacts of the Kuwaiti petroleum fires on the ABC News program "Nightline". Sagan again argued that some of the effects of the smoke could be similar to the effects of a nuclear winter, with smoke lofting into the stratosphere, a region of the atmosphere beginning around 48000 ft above sea level at Kuwait, resulting in global effects and that he believed the net effects would be very similar to the explosion of the Indonesian volcano Tambora in 1815, which resulted in the year 1816 being known as the "Year Without a Summer".
He reported on initial modeling estimates that forecast impacts extending to south Asia, and perhaps to the northern hemisphere as well. Sagan stressed this outcome was so likely that, "It should affect the war plans." Singer, on the other hand, said that his calculations showed that the smoke would go to an altitude of about 3000 ft and then be rained out after about three to five days and thus the lifetime of the smoke would be limited. Both height estimates made by Singer and Sagan turned out to be wrong, albeit with Singers narrative being closer to what transpired, with the comparatively minimal atmospheric effects remaining limited to the Persian Gulf region, with smoke plumes, in general, lofting to about 10,000 ft and a few times as high as 20,000 ft.
Sagan later conceded in his book "The Demon-Haunted World" that his predictions obviously did not turn out to be correct: "it "was" pitch black at noon and temperatures dropped 4–6 °C over the Persian Gulf, but not much smoke reached stratospheric altitudes and Asia was spared."
Sagan and his colleagues expected that a "self-lofting" of the sooty smoke would occur when it absorbed the sun's heat radiation, with little to no scavenging occurring, whereby the black particles of soot would be heated by the sun and lifted/lofted higher and higher into the air, thereby injecting the soot into the stratosphere, a position where they argued it would take years for the sun blocking effect of this aerosol of soot to fall out of the air, and with that, catastrophic ground level cooling and agricultural impacts in Asia and possibly the Northern Hemisphere as a whole.
The Atmospheric scientist tasked with studying the atmospheric impact of the Kuwaiti fires by the National Science Foundation, Peter Hobbs, stated that "the fires' modest impact suggested that "some numbers [used to support the Nuclear Winter hypothesis]... were probably a little overblown."
Hobbs found that at the peak of the fires, the smoke absorbed 75 to 80% of the sun’s radiation. The particles rose to a maximum of 20,000 ft, and when combined with scavenging by clouds the smoke had a short residency time of a maximum of a few days in the atmosphere.
Pre-war claims of wide scale, long-lasting, and significant global environmental impacts were thus not borne out, and found to be significantly exaggerated by the media and speculators, with climate models by those not supporting the nuclear winter hypothesis at the time of the fires predicting only more localized effects such as a daytime temperature drop of ~10 °C within ~200 km of the source.
The idea of oil well and oil reserve smoke pluming to the stratosphere serving as a main contributor to the soot of a nuclear winter was a central tenet of the early climatology papers on the hypothesis; they were considered more of a possible contributor than smoke from cities, as the smoke from oil has a higher ratio of black soot, thus absorbing more sunlight. Hobbs compared the papers' assumed "emission factor" or soot generating efficiency from ignited oil pools and found, upon comparing to measured values from oil pools at Kuwait, which were the greatest soot producers, the emissions of soot assumed in the nuclear winter calculations are still "too high". Following the results of the Kuwaiti oil fires being in disagreement with the core nuclear winter promoting scientists, the 1990s nuclear winter papers generally attempted to distance themselves from suggesting oil well and reserve smoke will reach the stratosphere.
In 2007, a nuclear winter study, which will be discussed later, noted that modern computer models have been applied to the Kuwait oil fires, finding that individual smoke plumes are not able to loft smoke into the stratosphere, but that smoke from fires covering a large area like some forest fires can lift smoke into the stratosphere, and this is supported by recent evidence that it occurs far more often than previously thought. The study also suggested that the burning of the comparably smaller cities, which would be expected to follow a nuclear strike, would also loft significant amounts of smoke into the stratosphere:
Stenchikov et al. [2006b] conducted detailed, high-resolution smoke plume simulations with the RAMS regional climate model [e.g., Miguez-Macho et al., 2005] and showed that individual plumes, such as those from the Kuwait oil fires in 1991, would not be expected to loft into the upper atmosphere or stratosphere, because they become diluted. However, much larger plumes, such as would be generated by city fires, produce large, undiluted mass motion that results in smoke lofting. New large eddy simulation model results at much higher resolution also give similar lofting to our results, and no small scale response that would inhibit the lofting [Jensen, 2006].
However the above simulation notably contained the assumption that no dry and wet deposition/rain would occur.
Eruption of Mt. Pinatubo and agriculture.
The eruption of the Philippines volcano Mount Pinatubo in June 1991 ejected roughly 10 km3 of magma and 17000000 t of sulfur dioxide SO2 into the air, introducing ten times as much total SO2 as the Kuwaiti fires, mostly during the explosive Plinian/Ultra-Plinian event of June 15, 1991, creating a global stratospheric SO2 haze layer which persisted for years. This resulted in the global average temperature dropping by about 0.5 C-change. As volcanic ash falls out of the atmosphere rapidly, the negative agricultural effects of the eruption were largely immediate and localized to a relatively small area in close proximity to the eruption, as they were caused by the resulting thick ash cover that resulted. Globally however, despite a several-month 5% drop in overall solar irradiation, and a reduction in direct sunlight by 30%, there was no negative impact to global agriculture. Surprisingly, a 3-4 year increase in global Agricultural productivity and forestry growth was observed, excepting boreal forest regions. The means by which this was discovered, is that initially at the time, a mysterious drop in the rate at which carbon dioxide (CO2) was filling the atmosphere was observed, and numerous scientists assumed this to be due to the lowering of the Earth's temperature, and with that, a slow down in plant and soil respiration, indicating a deleterious impact to global agriculture from the volcanic haze layer. However upon actual investigation, the reduction of the rate at which carbon dioxide filled the atmosphere did not match up with the hypothesis that plant respiration rates had declined. Instead the advantagous anomaly was relatively firmly linked to an unprecedented increase in the growth/net primary production, of global plant life. The mechanism by which the increase in plant growth was possible, was that the 30% reduction of direct sunlight can also be expressed as an increase or "enhancement" in the amount of diffuse sunlight. With, owing to its intrinsic nature, can illuminate under-canopy leaves permitting more efficient total whole-plant photosynthesis than would otherwise be the case. In stark contrast to the effect of totally clear skies and the direct sunlight that results from it, which casts shadows onto understorey leaves, strickly limiting plant photosynthesis to the top canopy layer. This increase in global agriculture from the volcanic haze layer also naturally results as a product of other aerosols that are not emitted by volcanoes, such as man-made "moderately thick smoke loading" pollution, as the same mechanism, the "aerosol direct radiative effect" is behind both.
Recent modeling.
Based on new work published in 2007 and 2008 by some of the authors of the original studies, several new hypotheses have been put forth. However far from being "new", the very same beginning to "significant" nuclear winter effects, was in the mid 1980s models, similarly regarded to have been a threat from a total of 100 or so city firestorms.
A minor nuclear war with each country using 50 Hiroshima-sized atom bombs as airbursts on urban areas could produce climate change unprecedented in recorded human history. A nuclear war between the United States and Russia today could produce nuclear winter, with temperatures plunging below freezing in the summer in major agricultural regions, threatening the food supply for most of the planet. The climatic effects of the smoke from burning cities and industrial areas would last for several years, much longer than previously thought. New climate model simulations, which are said to have the capability of including the entire atmosphere and oceans, show that the smoke would be lofted by solar heating to the upper stratosphere, where it would remain for years.
Compared to climate change for the past millennium, even the smallest exchange modeled would plunge the planet into temperatures colder than the Little Ice Age (the period of history between approximately A.D. 1600 and A.D. 1850). This would take effect instantly, and agriculture would be severely threatened. Larger amounts of smoke would produce larger climate changes, and for the 150 teragrams (Tg) case produce a true nuclear winter (1 Tg is 1012 grams), making agriculture impossible for years. In both cases, new climate model simulations show that the effects would last for more than a decade.
2007 study on global nuclear war.
A study published in the "Journal of Geophysical Research" in July 2007, "Nuclear winter revisited with a modern climate model and current nuclear arsenals: Still catastrophic consequences", used current climate models to look at the consequences of a global nuclear war involving most or all of the world's current nuclear arsenals (which the authors judged to be one the size of the world's arsenals twenty years earlier). The authors used a global circulation model, ModelE from the NASA Goddard Institute for Space Studies, which they noted "has been tested extensively in global warming experiments and to examine the effects of volcanic eruptions on climate." The model was used to investigate the effects of a war involving the entire current global nuclear arsenal, projected to release about 150 Tg of smoke into the atmosphere, as well as a war involving about one third of the current nuclear arsenal, projected to release about 50 Tg of smoke. In the 150 Tg case they found that:
A global average surface cooling of –7 °C to –8 °C persists for years, and after a decade the cooling is still –4 °C (Fig. 2). Considering that the global average cooling at the depth of the last ice age 18,000 yr ago was about –5 °C, this would be a climate change unprecedented in speed and amplitude in the history of the human race. The temperature changes are largest over land ... Cooling of more than –20 °C occurs over large areas of North America and of more than –30 °C over much of Eurasia, including all agricultural regions.
In addition, they found that this cooling caused a weakening of the global hydrological cycle, reducing global precipitation by about 45%. As for the 50 Tg case involving one third of current nuclear arsenals, they said that the simulation "produced climate responses very similar to those for the 150 Tg case, but with about half the amplitude," but that "the time scale of response is about the same." They did not discuss the implications for agriculture in depth, but noted that a 1986 study which assumed no food production for a year projected that "most of the people on the planet would run out of food and starve to death by then" and commented that their own results show that, "This period of no food production needs to be extended by many years, making the impacts of nuclear winter even worse than previously thought."
2014.
In 2014, M. J. Mills (at the US National Center for Atmospheric Research, NCAR), O. B. Toon (of the original TTAPS team), J. Lee-Taylor, and A. Robock published "Multi-decadal global cooling and unprecedented ozone loss following a regional nuclear conflict" in the journal "Earth's Future". The authors used computational models developed by NCAR to simulate the climatic effects of a regional nuclear war in which 100 "small" (15 kt) weapons are detonated over cities. They concluded, in part, that
global ozone losses of 20-50% over populated areas, levels unprecedented in human history, would accompany the coldest average surface temperatures in the last 1000 years. We calculate summer enhancements in UV indices of 30-80% over Mid-Latitudes, suggesting widespread damage to human health, agriculture, and terrestrial and aquatic ecosystems. Killing frosts would reduce growing seasons by 10-40 days per year for 5 years. Surface temperatures would be reduced for more than 25 years, due to thermal inertia and albedo effects in the ocean and expanded sea ice. The combined cooling and enhanced UV would put significant pressures on global food supplies and could trigger a global nuclear famine.
Criticism and debate.
The TTAPS study was widely reported and criticized in the media. Later model runs in some cases predicted less severe effects, but continued to support the overall conclusion of significant global cooling. Recent studies (2006) substantiate that smoke from urban firestorms in a local nuclear war would lead to long lasting global cooling but in a less dramatic manner than a global nuclear war, while a 2007 study of the effects of global nuclear war supported the conclusion that it would lead to full-scale nuclear winter.
The original work by Sagan and others was criticized as a "myth" and "discredited theory" in the 1987 book "Nuclear War Survival Skills", a civil defense manual by Cresson Kearny for the Oak Ridge National Laboratory. Kearny said the amount of cooling would last only a few days. Kearny, who was not a climate scientist himself, based his conclusions almost entirely on the 1986 paper "Nuclear Winter Reappraised" by Starley Thompson and Stephen Schneider. However, a 1988 article by Brian Martin in "Science and Public Policy" states that although their paper concluded the effects would be less severe than originally thought, with the authors describing these effects as a "nuclear autumn", other statements by Thompson and Schneider show that they "resisted the interpretation that this means a rejection of the basic points made about nuclear winter". In addition, the authors of the 2007 study above state that "because of the use of the term 'nuclear autumn' by Thompson and Schneider [1986], even though the authors made clear that the climatic consequences would be large, in policy circles the theory of nuclear winter is considered by some to have been exaggerated and disproved [e.g., Martin, 1988]." And in 2007 Schneider emphasized the danger of serious climate changes from a limited nuclear war of the kind analyzed in the 2006 study above, saying "The sun is much stronger in the tropics than it is in mid-latitudes. Therefore, a much more limited war [there] could have a much larger effect, because you are putting the smoke in the worst possible place."
John Maddox, editor of the journal "Nature", issued a series of skeptical comments about nuclear winter studies during his tenure, being a long-time critic of environmental doomsdayism. Similarly S. Fred Singer was a long term vocal critic of the hypothesis in the journal and in televised debates with Carl Sagan.
Russell Seitz, Associate of the Harvard University Center for International Affairs, argues that the models' assumptions give results which the researchers want to achieve and is a case of "worst-case analysis run amok". Seitz's opposition caused the proponents of nuclear winter to issue responses in the media, and while both sides made important points, they were largely incapable of collaborating as the proponents believed it was simply necessary to show only the possibility of climatic catastrophe, often a worst-case scenario, while opponents insisted that to be taken seriously, nuclear winter should be shown as likely under “reasonable” scenarios. One of these areas of contention, as elucidated by Lynn R. Anspaugh, is upon the question of which season should be used as the backdrop for the models, most models choose the summer in the Northern Hemisphere as the start point to produce the maximum cooling effect, whereas it has been pointed out that if the firestorms occurred in the winter months, when there is much less intense sunlight to loft soot into a stable region of the stratosphere, the magnitude of the cooling effect from the same number of firestorms as ignited in the summer models, would be zero according to a January model run by Covey et al.
Lynn R. Anspaugh also expressed frustration that although a managed forest fire in Canada is said to have been lit by proponents of nuclear winter, with the fire potentially serving as an opportunity to do some basic measurements of the optical properties of the smoke and smoke-to-fuel ratio, which would have helped refine the estimates of these critical model inputs, the proponents did not indicate that any such measurements were made.
In 1986, atmospheric scientist Joyce Penner from the Lawrence Livermore National Laboratory published an article in "Nature" in which she focused on the specific variables of the smoke's optical properties and the quantity of smoke remaining airborne after the city fires and found that the published estimates of these variables varied so widely that depending on which estimates were chosen the climate effect could be negligible, minor or massive.
The assumed optical properties for black carbon in more recent nuclear winter papers(2006) are still "based on those assumed in earlier nuclear winter simulations".
William R. Cotton Professor of Atmospheric Science at Colorado State University, specialist in cloud physics modeling and co-creator of the highly influential, and previously mentioned RAMS atmosphere model, had in the 1980s modeled and supported the predictions made by earlier nuclear winter papers, but has since reversed this position according to a book co-authored by him in 2007, stating that, amongst other systematically examined assumptions; far more rain out/wet deposition of soot will occur than is assumed in modern papers on the subject and that "We must wait for a new generation of GCMs to be implemented to examine potential consequences quantitatively".
The contribution of smoke from the ignition of live non-desert vegetation, living forests and so on near to many missile silos, a source of smoke originally brought up in the initial "Twilight at Noon" paper, was found after examination by Bush and Small in 1987, that the burning of live vegetation would contribute only slightly to the estimated total "nonurban smoke production". With the vegetation likely to only sustain burning if it is within a radius or two from the surface of the nuclear fireball, which is at a distance that would also experience extreme blast winds that would influence any such fires. This conclusion is supported by the 1950-60s in-field examination of tropical forests after Operation Castle, and Operation Redwing.
In a homeland security paper finalized in 2010, fire experts stated that due to the nature of modern city design and construction, with the U.S.  serving as an example, a firestorm is unlikely after a nuclear detonation in a modern city. This is not to say that fires won't occur over a large area after a detonation, but that the fires would not coalesce and form the all important stratosphere punching firestorm plume that the nuclear winter papers require as a prerequisite assumption in their climate computer models. The nuclear bombing of Nagasaki for example, did not produce a firestorm. However, even a conflagration that is not a firestorm can inject smoke into the stratosphere, as happened in a wildfire in Alberta in 2001.
Policy implications.
During the early 1980s, Fidel Castro recommended to the Kremlin a harder line against Washington, even suggesting the possibility of nuclear strikes. The pressure stopped after Soviet officials gave Castro a briefing on the ecological impact on Cuba of nuclear strikes on the United States. In 2010 Alan Robock, a co-author of nuclear winter papers was summoned to Cuba to help Castro promote his new view that nuclear war would bring about Armageddon, Robock's 90 minute lecture was later aired on nationwide television in the country. 
However according to Robock, in so far as getting US government attention and affecting nuclear policy, he has failed. In 2009, together with Owen Toon, he gave a talk to the United States Congress but nothing transpired from it and the then presidential science adviser, John Holdren, did not respond to their requests in 2009 or at the time of writing in 2011.
In an interview in 2000, Mikhail Gorbachev, in response to the comment "In the 1980s, you warned about the unprecedented dangers of nuclear weapons and took very daring steps to reverse the arms race," said "Models made by Russian and American scientists showed that a nuclear war would result in a nuclear winter that would be extremely destructive to all life on Earth; the knowledge of that was a great stimulus to us, to people of honor and morality, to act in that situation."
However a 1984 US "Interagency Intelligence Assessment" expresses a far more skeptical and cautious approach by stating that as the hypothesis is not convincing scientifically, they predicted that Soviet nuclear policy would be to maintain their strategic nuclear posture, such as their fielding of the high throw-weight SS-18 missile and they would merely attempt to exploit the hypothesis for propaganda purposes, such as directing scrutiny on the US portion of the nuclear arms race. Moreover it goes on to express the belief that if Soviet officials did begin to take nuclear winter seriously, it would probably make them demand exceptionally high standards of scientific proof for the hypothesis, as the implications of it would undermine their military doctrine—a level of scientific proof which perhaps could not be met without field experimentation. The un-redacted portion of the document ends with the suggestion that substantial increases in Soviet Civil defense food stockpiles might be an early indicator that Nuclear Winter was beginning to influence Soviet upper echelon thinking.
In 1985 "Time" magazine noted "the suspicions of some Western scientists that the nuclear winter hypothesis was promoted by Moscow to give anti-nuclear groups in the U.S. and Europe some fresh ammunition against America's arms buildup."
In 1986, the Defense Nuclear Agency document "An update of Soviet research on and exploitation of Nuclear winter 1984-1986" charted the minimal research contribution on, and Soviet propaganda usage of, the nuclear winter phenomenon.
Dr. Vitalii Nikolaevich Tsygichko, a Senior Analyst at the Soviet Academy of Sciences, author of the study, "Mathematical Model of Soviet Strategic Operations on the Continental Theater", and a former member of the General Staff, has said that Soviet military analysts discussed the idea of a "nuclear winter" (although they did not use that exact term) years before U.S. scientists wrote about it in the 1980s. Starley L. Thompson, of the National Center for Atmospheric Research, Boulder, Colorado, says that Soviet research into nuclear winter in 1983 used US computer models that had been developed in the early 1970s. Soviet intelligence officer Sergei Tretyakov, who defected in 1990, maintained that "the KGB was responsible for creating the entire nuclear winter story to stop the Pershing missiles".
In 1989 Carl Sagan and colleague Richard Turco wrote a policy implications paper that appeared in Ambio that suggests that as nuclear winter is a "well-established prospect", both superpowers should jointly reduce their nuclear arsenals to "Canonical Deterrent Force" levels of 100-300 individual warheads each, such that in "the event of nuclear war [this] would minimize the likelihood of nuclear winter."
As the implications of nuclear winter began to be taken seriously in the late 1980s, military analysts turned to reinforce "existing trends" in warhead miniaturization, of higher accuracy and lower yield nuclear warheads. This trend, enabled by GPS navigation etc., was motivated by the desire to still destroy the target but while reducing the severity of fallout collateral damage depositing on neighboring, and potentially friendly, countries. As it relates to the likelihood of nuclear winter, the hazard from thermal radiation ignited fires would also be reduced. While the TTAPS paper had described a 3000 Mt counterforce attack on ICBM sites; Michael Altfeld of Michigan State University and political scientist Stephen Cimbala of Pennsylvania State University argued that smaller, more accurate warheads and lower detonation heights could produce the same counterforce strike with only 3 Mt and produce less climatic effects, even if cities were targeted, as lower fuzing heights, such as surface bursts, would limit the range of the burning thermal rays due to terrain masking and shadows cast by buildings, while also temporarily lofting far more radioactive soil into the atmosphere. This logic is similarly reflected in the 1984 "Interagency Intelligence assessment", which suggests that targeting planners would simply have to consider target combustibility along with yield, height of burst, timing and other factors to reduce the amount of smoke to safeguard against the potentiality of a nuclear winter. Therefore as a consequence of attempting to limit the target fire hazard by reducing the range of thermal radiation with fuzing for surface and sub-surface bursts, this will result in a scenario where the far more concentrated, and therefore deadlier, "local" fallout that is generated following a surface burst forms, as opposed to the comparatively dilute "global" fallout created when nuclear weapons are fuzed in air burst mode.
Altfeld and Cimbala also argued that belief in the possibility of nuclear winter would actually make nuclear war more likely, contrary to the views of Sagan and others, because it would inspire the development of more accurate, and lower explosive yield, nuclear weapons. As it suggests that the replacement of the then Cold War viewed strategic nuclear weapons in the multi-megaton yield range, with weapons of explosive yields closer to tactical nuclear weapons, such as the Robust Nuclear Earth Penetrator, would safeguard against the nuclear winter potential. Tactical nuclear weapons, on the low end of the scale have yields that overlap with large conventional weapons, and are therefore often viewed "as blurring the distinction between conventional and nuclear weapons", making the prospect of using them "easier" in a conflict.
Mitigation techniques.
A number of solutions have been proposed to mitigate the potential harm of a nuclear winter if one appears inevitable; with the problem being attacked at both ends, from those focusing on preventing the growth of fires and therefore limiting the amount of smoke that reaches the stratosphere in the first place, to food production under dimmed skies with the assumption that the very worst-case analysis results of the nuclear winter models prove accurate and no other mitigation strategies are fielded.
Fire control.
In a report from 1967, techniques included various methods of applying liquid nitrogen, dry ice, and water to nuclear-caused fires. The report considered attempting to stop the spread of fires by creating firebreaks by blasting combustible material out of an area, possibly even with nuclear weapons, along with the use of preventative Hazard reduction burns. According to the report, one of the most promising techniques investigated was initiation of rain from seeding of mass-fire thunderheads and other clouds passing over the developing, and then steady-state, firestorm.
Producing food without sunlight.
David Denkenberger and Joshua Pearce have proposed in Feeding Everyone No Matter What a variety of alternate foods which convert fossil fuels or biomass into food without sunlight to address nuclear winter. The solution using a fossil fuel energy source is natural-gas-digesting bacteria. One example of a biomass alternate food is that mushrooms can grow directly on wood without sunlight. Another example is that cellulosic biofuel production typically already creates sugar as an intermediate product.
Large-scale food stockpiling.
The minimum annual global wheat storage is approximately 2 months. To feed everyone despite nuclear winter, years of food storage prior to the event has been proposed. While the suggested masses of preserved food would likely never get used as a nuclear winter is comparatively unlikely to occur, the stockpiling of food would have the positive result of ameliorating the impact of the far more frequent distruptions to regional food supplies caused by lower-level conflicts and droughts. There is however the danger that if a sudden rush to food stockpiling occurs without the buffering effect offered by Victory gardens etc., it may exacerbate current food security problems by elevating present food prices.
Climate engineering.
Despite the name "nuclear winter", nuclear events are not necessary to produce the modeled climatic effect. In an effort to find a quick and cheap solution to the global warming projection of at least two degrees of surface warming as a result of doubling of CO2 levels in the atmosphere, through solar radiation management, a form of climate engineering, the underlying nuclear winter effect has been looked at as perhaps holding potential. Besides the more common suggestion to inject sulfur compounds into the stratosphere to approximate the effects of a volcanic winter, the injection of other chemical species such as the release of a particular type of soot particle, to create minor "nuclear winter" conditions, has also been proposed by Paul Crutzen and others. According to the threshold/minor "nuclear winter" computer models, if one to five teragrams of firestorm-generated soot is injected into the low stratosphere, it is modeled, through the anti-greenhouse effect, to heat the stratosphere but cool the lower troposphere and produce 1.25 °C cooling for two to three years; after 10 years, average global temperatures would still be 0.5 °C lower than before the soot injection.
Potential climatic precedence.
Similar climatic effects to "nuclear winter" followed historical supervolcano eruptions, which plumed sulfate aerosols high into the stratosphere, with this being known as a volcanic winter.
Similarly, extinction-level comet and asteroid impacts are also believed to have generated impact winters by the pulverization of massive amounts of fine rock dust. This pulverized rock can also produce "volcanic winter" effects, if sulfate-bearing rock is hit in the impact and lofted high into the air, and "nuclear winter" effects, with the heat of the heavier rock ejecta igniting regional and possibly even global forest firestorms.
This global "impact firestorms" hypothesis, initially supported by Wolbach, Melosh and veteran nuclear winter modeler Owen Toon, suggests that as a result of massive impact events, the small sand-grain-sized ejecta fragments created can meteorically re-enter the atmosphere forming a hot blanket of global debris high in the air, potentially turning the entire sky red-hot for minutes to hours, and with that, burning the complete global inventory of above-ground carbonaceous material, including rain forests. This hypothesis is suggested as a means to explain the severity of the Cretaceous–Paleogene extinction event, as the earth impact of an asteroid about 10 km wide which precipitated the extinction is not regarded as sufficiently energetic to have caused the level of extinction from the initial impact's energy release alone.
The global "impact firestorms"/firestorm winter, however, has been questioned in more recent years (2003-2013) by Claire Belcher, Tamara Goldin and H. Jay Melosh, with this re-evaluation being dubbed the "Cretaceous-Palaeogene firestorm debate" by Belcher. The issues raised by these scientists in the debate are the perceived low quantity of soot in the sediment beside the fine-grained iridium-rich asteroid dust layer, if the quantity of re-entering ejecta was perfectly global in blanketing the atmosphere, and if so, the duration and profile of the re-entry heating, whether it was a high thermal pulse of heat or the more prolonged and therefore more incendiary "oven" heating, and finally, how much the "self-shielding effect" from the first wave of now-cooled meteors in dark flight contributed to diminishing the total heat experienced on the ground from later waves of meteors, in part due to the Cretaceous period being a high-atmospheric-oxygen era, with concentrations above that of the present day. In 2013, Owen Toon et al. were critical of the re-evaluations the hypothesis is undergoing. It will be difficult to successfully tease out the percentage contribution of the soot in this period's geological sediment record from living plants and fossil fuels present at the time, in much the same manner that the fraction of the material ignited by the meteor's heating effects will be difficult to determine, as other ignition sources that were also present at, or soon after, the impact such as mantle lava flows complicate the matter.
References.
</dl>

</doc>
<doc id="22172" url="http://en.wikipedia.org/wiki?curid=22172" title="Ode">
Ode

Ode (from Ancient Greek: ᾠδή "ōidē") is a type of lyrical stanza. A classic ode is structured in three major parts: the "strophe", the "antistrophe", and the "epode". Different forms such as the "homostrophic ode" and the "irregular ode" also exist. It is an elaborately structured poem praising or glorifying an event or individual, describing nature intellectually as well as emotionally.
Greek odes were originally poetic pieces performed with musical accompaniment. As time passed on, they gradually became known as personal lyrical compositions whether sung (with or without musical instruments) or merely recited (always with accompaniment). The primary instruments used were the aulos and the lyre (the latter was the most revered instrument to the Ancient Greeks). 
There are three typical forms of odes: the Pindaric, Horatian, and irregular. Pindaric odes follow the form and style of Pindar. Horatian odes follow conventions of Horace; the odes of Horace deliberately imitated the Greek lyricists such as Alcaeus and Anacreon. Irregular odes use rhyme, but not the three-part form of the Pindaric ode, nor the two- or four-line stanza of the Horatian ode.
English ode.
An English ode is a lyrical stanza in praise of, or dedicated to someone or something that captures the poet's interest or serves as an inspiration for the ode. The lyrics can be on various themes. The earliest odes in the English language, using the word in its strict form, were the "Epithalamium" and "Prothalamium" of Edmund Spenser. 
In the 17th century, the most important original odes in English are by Abraham Cowley. These were iambic, but had irregular line length patterns and rhyme schemes. Cowley based the principle of his Pindariques on an apparent misunderstanding of Pindar's metrical practice but, nonetheless, others widely imitated his style, with notable success by John Dryden.
With Pindar's metre being better understood in the 18th century, the fashion for Pindaric odes faded, though there are notable actual Pindaric odes by Thomas Gray, and .
Around 1800, William Wordsworth revived Cowley's Pindarick for one of his finest poems, the "" ode. Others also wrote odes: Samuel Taylor Coleridge, John Keats, and Percy Bysshe Shelley who wrote odes with regular stanza patterns. Shelley's "Ode to the West Wind", written in fourteen line terza rima stanzas, is a major poem in the form. Perhaps the greatest odes of the 19th century, however, were Keats's "Five Great Odes of 1819", which included "Ode to a Nightingale", "Ode on Melancholy", "Ode on a Grecian Urn", "Ode to Psyche", and "To Autumn". After Keats, there have been comparatively few major odes in English. One major exception is the fourth verse of the poem "For the Fallen" by Laurence Binyon, which is often known as "The Ode to the Fallen", or simply as "The Ode".
W.H. Auden also wrote "Ode", one of the most popular poems from his earlier career when he lived in London, in opposition to people's ignorance over the reality of war. In an interview, Auden once stated that he had intended to title the poem "My Silver Age" in mockery of the supposedly imperial Golden age, however chose "Ode" as it seemed to provide a more sensitive exploration of warfare. 
"Ode on a Grecian Urn", while an ekphrasis, also functions as an ode to the artistic beauty the narrator observes. The English ode's most common rhyme scheme is ABABCDECDE.

</doc>
<doc id="22189" url="http://en.wikipedia.org/wiki?curid=22189" title="Temple of Olympian Zeus, Athens">
Temple of Olympian Zeus, Athens

The Temple of Olympian Zeus (Greek: Ναός του Ολυμπίου Διός, "Naos tou Olympiou Dios"), also known as the Olympieion or Columns of the Olympian Zeus, is a colossal ruined temple in the center of the Greek capital Athens that was dedicated to Zeus, king of the Olympian gods. Construction began in the 6th century BC during the rule of the Athenian tyrants, who envisaged building the greatest temple in the ancient world, but it was not completed until the reign of the Roman Emperor Hadrian in the 2nd century AD some 638 years after the project had begun. During the Roman periods it was renowned as the largest temple in Greece and housed one of the largest cult statues in the ancient world.
The temple's glory was short-lived, as it fell into disuse after being pillaged in a barbarian invasion in the 3rd century AD. It was probably never repaired and was reduced to ruins thereafter. In the centuries after the fall of the Roman Empire, it was extensively quarried for building materials to supply building projects elsewhere in the city. Despite this, a substantial part of the temple remains today, and it continues to be a major tourist attraction.
History.
Classical and Hellenistic periods.
The temple is located 1640 feet south-east of the Acropolis, and about 700 m (2,300 feet) south of the center of Athens, Syntagma Square. Its foundations were laid on the site of an ancient outdoor sanctuary dedicated to Zeus. An earlier temple had stood there, constructed by the tyrant Pisistratus around 550 BC. The building was demolished after the death of Peisistratos and the construction of a colossal new Temple of Olympian Zeus was begun around 520 BC by his sons, Hippias and Hipparchos. 
They sought to surpass two famous contemporary temples, the Heraion of Samos and the Temple of Artemis at Ephesus, which was one of the Seven Wonders of the Ancient World. Designed by the architects Antistates, Callaeschrus, Antimachides and Porinus, the Temple of Olympian Zeus was intended to be built of local limestone in the Corinthian style on a colossal platform measuring 41 m (134.5 feet) by 108 m (353.5 feet). It was to be flanked by a double colonnade of eight columns across the front and back and twenty-one on the flanks, surrounding the cella. 
The work was abandoned when the tyranny was overthrown and Hippias was expelled in 510 BC. Only the platform and some elements of the columns had been completed by this point, and the temple remained in this state for 336 years. The temple was left unfinished during the years of Athenian democracy, apparently because the Greeks thought it is hubristic to build on such a scale. In the treatise "Politics", Aristotle cited the temple as an example of how tyrannies engaged the populace in great works for the state (like a white elephant) and left them no time, energy or means to rebel.
It was not until 174 BC that the Seleucid king Antiochus IV Epiphanes, who presented himself as the earthly embodiment of Zeus, revived the project and placed the Roman architect Decimus Cossutius in charge. The design was changed to have three rows of eight columns across the front and back of the temple and a double row of twenty on the flanks, for a total of 104 columns. The columns would stand 17 m (55.5 feet) high and 2 m (6.5 ft) in diameter. The building material was changed to the expensive but high-quality Pentelic marble and the order was changed from Doric to Corinthian, marking the first time that this order had been used on the exterior of a major temple. However, the project ground to a halt again in 164 BC with the death of Antiochus. The temple was still only half-finished by this stage.
Serious damage was inflicted on the partly built temple by Lucius Cornelius Sulla's sack of Athens in 86 BC. While looting the city, Sulla seized some of the incomplete columns and transported them back to Rome, where they were re-used in the Temple of Jupiter on the Capitoline Hill. A half-hearted attempt was made to complete the temple during Augustus' reign as the first Roman emperor, but it was not until the accession of Hadrian in the 2nd century AD that the project was finally completed around 638 years after it had begun. 
In 124-125 AD, when the strongly Philhellene Hadrian visited Athens, a massive building programme was begun that included the completion of the Temple of Olympian Zeus. A walled marble-paved precinct was constructed around the temple, making it a central focus of the ancient city. Cossutius's design was used with few changes and the temple was formally dedicated by Hadrian in 132, who took the title of "Panhellenios" in commemoration of the occasion. The temple and the surrounding precinct were adorned with numerous statues depicting Hadrian, the gods and personifications of the Roman provinces. A colossal statue of Hadrian was raised behind the building by the people of Athens in honour of the emperor's generosity. An equally colossal chryselephantine statue of Zeus occupied the cella of the temple. The statue's form of construction was unusual, as the use of chryselephantine was by this time regarded as archaic. It has been suggested that Hadrian was deliberately imitating Phidias' famous statue of Athena Parthenos in the Parthenon, seeking to draw attention to the temple and himself by doing so.
The Temple of Olympian Zeus was badly damaged during the Herulian sack of Athens in 267. It is unlikely to have been repaired, given the extent of the damage to the rest of the city. Assuming that it was not abandoned it would certainly have been closed down in 425 by the Christian emperor Theodosius II when he prohibited the worship of the old Roman and Greek gods. Material from the (presumably now ruined) building was incorporated into a basilica constructed nearby during the 5th or 6th century.
Medieval and Modern periods.
Over the following centuries, the temple was systematically quarried to provide building materials and material for the houses and churches of medieval Athens. By the end of the Byzantine period, it had been almost totally destroyed; when Ciriaco de' Pizzicolli (Cyriacus of Ancona) visited Athens in 1436 he found only 21 of the original 104 columns still standing. The fate of one of the columns is recorded by a Greek inscription on one of the surviving columns, which states that "on 27 April 1759 he pulled down the column". This refers to the Turkish governor of Athens, Tzisdarakis, who is recorded by a chronicler as having "destroyed one of Hadrian's columns with gunpowder" in order to re-use the marble to make plaster for the mosque that he was building in the Monastiraki district of the city. During the Ottoman period the temple was known to the Greeks as the Palace of Hadrian, while the Turks called it the Palace of Belkis, from a Turkish legend that the temple had been the residence of Solomon's wife.
Fifteen columns remain standing today and a sixteenth column lies on the ground where it fell during a storm in 1852. Nothing remains of the cella or the great statue that it once housed.
The temple was excavated in 1889-1896 by Francis Penrose of the British School in Athens (who also played a leading role in the restoration of the Parthenon), in 1922 by the German archaeologist Gabriel Welter and in the 1960s by Greek archaeologists led by Ioannes Travlos. The temple, along with the surrounding ruins of other ancient structures, is a historical precinct administered by Ephorate of Antiquites of the Greek Interior Ministry.
On 21 January 2007, a group of Hellenic neopagans held a ceremony honoring Zeus on the grounds of the temple. The event was organized by Ellinais, an organization which won a court battle to obtain recognition for Ancient Greek religious practices in the fall of 2006.

</doc>
<doc id="22190" url="http://en.wikipedia.org/wiki?curid=22190" title="Organic electronics">
Organic electronics

Organic electronics is a field of materials science concerning the design, synthesis, characterization, and application of organic small molecules or polymers that show desirable electronic properties such as conductivity. Unlike conventional inorganic conductors and semiconductors, organic electronic materials are constructed from organic (carbon-based) small molecules or polymers using synthetic strategies developed in the context of organic and polymer chemistry. One of the benefits of organic electronics is their low cost compared to traditional inorganic electronics.
History.
Conductive materials are substances that can transmit electrical charges. Traditionally, most known conductive materials have been inorganic. Metals such as copper and aluminum are the most familiar conductive materials, and have high electrical conductivity due to their abundance of delocalized electrons that move freely throughout the inter-atomic spaces. Some metallic conductors are alloys of two or more metal elements, common examples of such alloys include steel, brass, bronze, and pewter.
In the eighteenth and early nineteenth centuries, people began to study the electrical conduction in metals. In his experiments with lightning, Benjamin Franklin proved that an electrical charge travels along a metallic rod. Later, Georg Simon Ohm discovered that the current passing through a substance is directly proportional to the potential difference, known as Ohm's law. This relationship between potential difference and current became a widely used measure of the ability of various materials to conduct electricity. Since the discovery of conductivity, studies have focused primarily on inorganic conductive materials with only a few exceptions.
Henry Letheby discovered the earliest known organic conductive material in 1862. Using anodic oxidation of aniline in sulfuric acid, he produced a partly conductive material, that was later identified as polyaniline. In the 1950s, the phenomenon that polycyclic aromatic compounds formed semi-conducting charge-transfer complex salts with halogens was discovered, showing that some organic compounds could be conductive as well.
More recent work has expanded the range of known organic conductive materials. A high conductivity of 1 S/cm (S = Siemens) was reported in 1963 for a derivative of tetraiodopyrrole. In 1972, researchers found metallic conductivity(conductivity comparable to a metal) in the charge-transfer complex TTF-TCNQ.
In 1977, it was discovered that polyacetylene can be oxidized with halogens to produce conducting materials from either insulating or semiconducting materials. In recent decades, research on conductive polymers has prospered, and the 2000 Nobel Prize in Chemistry was awarded to Alan J. Heeger, Alan G. MacDiarmid, and Hideki Shirakawa jointly for their work on conductive polymers.
Conductive plastics have recently undergone development for applications in industry. In 1987, the first organic diode device of was produced at Eastman Kodak by Ching W. Tang and Steven Van Slyke. spawning the field of organic light-emitting diodes (OLED) research and device production. For his work, Ching W. Tang is widely considered as the father of organic electronics.
Technology for plastic electronics constructed on thin and flexible plastic substrates was developed in the 1990s. In 2000, the company Plastic Logic was founded as a spin-off of Cavendish Laboratory to develop a broad range of products using the plastic electronics technology.
Conductive organic materials.
Attractive properties of polymer conductors include a wide range of electrical conductivity that, can be tuned by varying the concentrations of chemical dopants, mechanical flexibility, and high thermal stability. organic conductive materials can be grouped into two main classes: conductive polymers and conductive small molecules.
Conductive small molecules are usually used in the construction of organic semiconductors, which exhibit degrees of electrical conductivity between those of insulators and metals. Semiconducting small molecules include polycyclic aromatic compounds such as pentacene, anthracene and rubrene.
Conductive polymers are typically intrinsically conductive. Their conductivity can be comparable to metals or semiconductors. Most conductive polymers are not thermoformable, during production. However they can provide very high electrical conductivity without showing similar mechanical properties to other commercially available polymers. Both organic synthesis and advanced dispersion techniques can be used to tune the electrical properties of conductive polymers, unlike typical inorganic conductors. The most well-studied class of conductive polymers is the so-called linear-backbone “polymer blacks” including polyacetylene, polypyrrole, polyaniline, and their copolymers. Poly(p-phenylene vinylene) and its derivatives are used for electroluminescent semiconducting polymers. Poly(3-alkythiophenes) are also a typical material for use in solar cells and transistors.
Organic light-emitting diode.
An OLED (organic light-emitting diode) consists of a thin film of organic material that emits light under stimulation by an electric current. A typical OLED consists of an anode, a cathode, OLED organic material and a conductive layer. 
Discovery of OLED.
André Bernanose was the first to observe electroluminescence in organic materials, and Ching W. Tang, reported fabrication of an OLED device in 1987. The OLED device incorporated a double-layer structure motif consisting of separate hole transporting and electron-transporting layers, with light emission taking place in between the two layers. Their discovery opened a new era of current OLED research and device design.
Classification and current research.
OLED organic materials can be divided into two major families: small-molecule-based and polymer-based. Small molecule OLEDs (SM-OLEDs) include organometallic chelates(Alq3), fluorescent and phosphorescent dyes, and conjugated dendrimers. Fluorescent dyes can be selected according to the desired range of emission wavelengths; compounds like perylene and rubrene are often used. Very recently, Dr. Kim J. et al. at University of Michigan reported a pure organic light emitting crystal, Br6A, by modifying its halogen bonding, they succeeded in tuning the phosphorescence to different wavelengths including green, blue and red. By modifying the structure of Br6A, scientists are attempting to achieve a next generation organic light emitting diode. Devices based on small molecules are usually fabricated by thermal evaporation under vacuum. While this method enables the formation of well-controlled homogeneous film; is hampered by high cost and limited scalability.
Polymer light-emitting diodes (PLEDs), similar to SM-OLED, emit light under an applied electrical current. Polymer-based OLEDs are generally more efficient than SM-OLEDs requiring a comparatively lower amount of energy to produce the same luminescence. Common polymers used in PLEDs include derivatives of poly(p-phenylene vinylene) and polyfluorene. The emitted color can be tuned by substitution of different side chains onto the polymer backbone or modifying the stability of the polymer. In contrast to SM-OLEDs, polymer-based OLEDs cannot be fabricated through vacuum evaporation, and must instead be processed using solution-based techniques. Compared to thermal evaporation, solution based methods are more suited to creating films with large dimensions. Zhenan Bao. et al. at Stanford University reported a novel way to construct large-area organic semiconductor thin films using aligned single crystalline domains.
Organic field-effect transistor.
An Organic field-effect transistor is a field-effect transistor utilizing organic molecules or polymers as the active semiconducting layer. A field-effect transistor(FET) is any semiconductor material that utilizes electric field to control the shape of a channel of one type of charge carrier, thereby changing its conductivity. Two major classes of FET are n-type and p-type semiconductor, classified according to the charge type carried. In the case of organic FETs (OFETs), p-type OFET compounds are generally more stable than n-type due to the susceptibility of the latter to oxidative damage.
Discovery of the OFET.
J.E. Lilienfeld first proposed the field-effect transistor in 1930, but the first OFET was not reported until 1987, when Koezuka et al. constructed one using Polythiophene which shows extremely high conductivity. Other conductive polymers have been shown to act as semiconductors, and newly synthesized and characterized compounds are reported weekly in prominent research journals. Many review articles exist documenting the development of these materials.
Classification of OFETs and current research.
Like OLEDs, OFETs can be classified into small-molecule and polymer-based system. Charge transport in OFETs can be quantified using a measure called carrier mobility; currently, rubrene-based OFETs show the highest carrier mobility of 20–40 cm2/(V·s). Another popular OFET material is Pentacene. Due to its low solubility in most organic solvents, it's difficult to fabricate thin film transistors (TFTs) from pentacene itself using conventional spin-cast or, dip coating methods, but this obstacle can be overcome by using the derivative TIPS-pentacene. Current research focuses more on thin-film transistor (TFT) model, which eliminates the usage of conductive materials. Very recently, two studies conducted by Dr. Bao Z. et al. and Dr. Kim J. et al. demonstrated control over the formation of designed thin-film transistors. By controlling the formation of crystalline TFT, it is possible to create an aligned (as opposed to randomly ordered) charge transport pathway, resulting in enhanced charge mobility.
Organic electronic devices.
Organic solar cells could cut the cost of solar power by making use of inexpensive organic polymers rather than the expensive crystalline silicon used in most solar cells. What's more, the polymers can be processed using low-cost equipment such as ink-jet printers or coating equipment employed to make photographic film, which reduces both capital and operating costs compared with conventional solar-cell manufacturing.
Silicon thin film solar cells on flexible substrates allow a significant cost reduction of large-area photovoltaics for several reasons:
Inexpensive polymeric substrates like polyethylene terephthalate (PET) or polycarbonate (PC) have the potential for further cost reduction in photovoltaics. Protomorphous solar cells prove to be a promising concept for efficient and low-cost photovoltaics on cheap and flexible substrates for large-area production as well as small and mobile applications.
One advantage of printed electronics is that different electrical and electronic components can be printed on top of each other, saving space and increasing reliability and sometimes they are all transparent. One ink must not damage another, and low temperature annealing is vital if low-cost flexible materials such as paper and plastic film are to be used. There is much sophisticated engineering and chemistry involved here, with iTi, Pixdro, Asahi Kasei, Merck & Co.|Merck, BASF, HC Starck, Hitachi Chemical and Frontier Carbon Corporation among the leaders.
Electronic devices based on organic compounds are now widely used, with many new products under development. Sony reported the first full-color, video-rate, flexible, plastic display made purely of organic materials; television screen based on OLED materials; biodegradable electronics based on organic compound and low-cost organic solar cell are also available.
Fabrication methods.
There are important differences between the processing of small molecule organic semiconductors and semiconducting polymers. Small molecule semiconductors are quite often insoluble and typically require deposition via vacuum sublimation. While usually thin films of soluble conjugated polymers. Devices based on conductive polymers can be prepared by solution processing methods. Both solution processing and vacuum based methods produce amorphous and polycrystalline films with variable degree of disorder. “Wet” coating techniques require polymers to be dissolved in a volatile solvent, filtered and deposited onto a substrate. Common examples of solvent-based coating techniques include drop casting, spin-coating, doctor-blading, inkjet printing and screen printing. Spin-coating is a widely used technique for small area thin film production. It may result in a high degree of material loss. The doctor-blade technique results in a minimal material loss and was primarily developed for large area thin film production. Vacuum based thermal deposition of small molecules requires evaporation of molecules from a hot source. The molecules are then transported through vacuum onto a substrate. The process of condensing these molecules on the substrate surface results in thin film formation. Wet coating techniques can in some cases be applied to small molecules depending on their solubility.
Organic solar cells.
Compared to conventional inorganic solar cell, organic solar cells have the advantage of lower fabrication cost. An organic solar cell is a device that uses organic electronics to convert light into electricity. Organic solar cells utilize organic photovoltaic materials, organic semiconductor diodes that convert light into electricity. Figure to the right shows five commonly used organic photovoltaic materials. Electrons in these organic molecules can be delocalized in a delocalized π orbital with a corresponding π* antibonding orbital. The difference in energy between the π orbital, or highest occupied molecular orbital(HOMO), and π* orbital, or lowest unoccupied molecular orbital(LUMO) is called the band gap of organic photovoltaic materials. Typically, the band gap lies in the range of 1-4eV.
The difference in the band gap of organic photovoltaic materials leads to different chemical structures and forms of organic solar cells. Different forms of solar cells includes single-layer organic photovoltaic cells, bilayer organic photovoltaic cells and heterojunction photovoltaic cells. However, all three of these types of solar cells share the approach of sandwiching the organic electronic layer between two metallic conductors, typically indium tin oxide.
Organic field-effect transistors.
An organic field-effect transistor device consists of three major components: the source, the drain and the gate. Generally, a field-effect transistor has two plates, source in contact with drain and the gate respectively, working as conducting channel. The electrons move from source to the drain, and the gate serves to control the electrons’ movement from source to drain. Different types of FETs are designed based on carrier properties. Thin film transistor (TFT), among them, is an easy fabricating one. In a thin film transistor, the source and drain are made by directly depositing a thin layer of semiconductor followed by a thin film of insulator between semiconductor and the metal gate contact. Such a thin film is made by either thermal evaporation, or simply spins coating. In a TFT device, there is no carrier movement between the source and drain. After applying a positive charge, accumulation of electrons on the interface cause bending of the semiconductor and ultimately lowers the conduction band with regards to the Fermi-level of the semiconductor. Finally, a highly conductive channel is formed at the interface.
Features.
Conductive polymers are lighter, more flexible, and less expensive than inorganic conductors. This makes them a desirable alternative in many applications. It also creates the possibility of new applications that would be impossible using copper or silicon.
Organic electronics not only includes organic semiconductors, but also organic dielectrics, conductors and light emitters.
New applications include smart windows and electronic paper. Conductive polymers are expected to play an important role in the emerging science of molecular computers.
In general organic conductive polymers have a higher resistance and therefore conduct electricity poorly and inefficiently, as compared to inorganic conductors. Researchers currently are exploring ways of "doping" organic semiconductors, like melanin, with relatively small amounts of conductive metals to boost conductivity. Another method is to rely on two layers of gate dielectric, thereby allowing the high turn-on voltage caused by the fluorinated polymer to be mitigated by the low-voltage electrical characteristics of the high-k metal-oxide and the low defect count at the interface of the former to be used instead of the leaky defects of the latter. However, for many applications, inorganic conductors will remain the only viable option.

</doc>
<doc id="22194" url="http://en.wikipedia.org/wiki?curid=22194" title="Operating system">
Operating system

An operating system (OS) is software that manages computer hardware and software resources and provides common services for computer programs. The operating system is an essential component of the system software in a computer system. Application programs usually require an operating system to function.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or be interrupted by it. Operating systems are found on many devices that contain a computer—from cellular phones and video game consoles to web servers and supercomputers.
Examples of popular modern operating systems include Android, BlackBerry 10, BSD, Chrome OS, iOS, Linux, OS X, QNX, Microsoft Windows, Windows Phone, and z/OS. The first eight of these examples share roots in UNIX.
Types of operating systems.
Single- and multi-tasking.
A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency. This is achieved by time-sharing, dividing the available processor time between multiple processes which are each interrupted repeatedly in time-slices by a task scheduling subsystem of the operating system. Multi-tasking may be characterized in pre-emptive and co-operative types. In pre-emptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support pre-emptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking. 32-bit versions of both Windows NT and Win9x, used pre-emptive multi-tasking. Mac OS prior to OS X also used to support cooperative multitasking.
Single- and multi-user.
Single-user operating systems have no facilities to distinguish users, but may allow multiple programs to run at the same time. A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
Distributed.
A distributed operating system manages a group of distinct computers and makes them appear to be a single computer. The development of networked computers that could be linked and communicate with each other gave rise to distributed computing. Distributed computations are carried out on more than one machine. When computers in a group work in cooperation, they form a distributed system.
Templated.
In an OS, distributed and cloud computing context, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines (Gagne, 2012, p. 716). The technique is used both in virtualization and cloud computing management, and is common in large server warehouses. 
Embedded.
Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines like PDAs with less autonomy. They are able to operate with a limited number of resources. They are very compact and extremely efficient by design. Windows CE and Minix 3 are some examples of embedded operating systems.
Real-time.
A real-time operating system is an operating system that guaranties to process events or data within a certain short amount of time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. An event-driven system switches between tasks based on their priorities or external events while time-sharing operating systems switch tasks based on clock interrupts.
History.
Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plug boards. These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards. After programmable general purpose computers were invented, machine languages (consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).
In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period of time and would arrive at a scheduled time with program and data on punched paper cards and/or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the Universal Turing machine.
Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and generating computer code from human-readable symbolic code. This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England the job queue was at one time a washing line from which tapes were hung with different colored clothes-pegs to indicate job-priority.
An improvement was the Atlas Supervisor introduced with the Manchester Atlas commissioned in 1962, ‘considered by many to be the first recognisable modern operating system’. Brinch Hansen described it as "the most significant breakthrough in the history of operating systems."
Mainframes.
Through the 1950s, many major features were pioneered in the field of operating systems, including batch processing, input/output interrupt, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094.
During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and applications written for OS/360 can still be run on modern machines.
OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during update. When the process is terminated for any reason, all of these resources are re-claimed by the operating system.
The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: COS/360 (Compatibility Operating System), DOS/360 (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).
Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.
In 1961, Burroughs Corporation introduced the B5000 with the MCP, (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no machine language or assembler, and indeed the MCP was the first OS to be written exclusively in a high-level language – ESPOL, a dialect of ALGOL. MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. During development of the AS400, IBM made an approach to Burroughs to licence MCP to run on the AS400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. MCP is still in use today in the Unisys ClearPath/MCP line of computers.
UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems. Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
General Electric and MIT developed General Electric Comprehensive Operating Supervisor (GECOS), which introduced the concept of ringed security privilege levels. After acquisition by Honeywell it was renamed General Comprehensive Operating System (GCOS).
Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community.
From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/165 and 360/168) were microprogrammed implementations.
The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
Microcomputers.
The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as "monitors". One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS). In the '80s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative Graphical User Interface (GUI) to the Mac OS.
The introduction of the Intel 80386 CPU chip with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X.
The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
Examples of operating systems.
Unix and Unix-like operating systems.
Unix was originally written in assembly language. Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).
The "Unix-like" family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name "UNIX" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. "UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
Four operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris Operating System can run on multiple types of hardware, including x86 and Sparc servers, and PCs. Apple's OS X, a replacement for Apple's earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD.
Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
BSD and its descendants.
A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NextStep.
BSD has its roots in Unix. In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.
Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. Eventually, after two years of legal disputes, the BSD project came out ahead and spawned a number of free derivatives, such as FreeBSD and NetBSD.
OS X.
OS X (formerly "Mac OS X") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. OS X is the successor to the original Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, OS X is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.
The operating system was first released in 1999 as Mac OS X Server 1.0, with a desktop-oriented version (Mac OS X v10.0 "Cheetah") following in March 2001. Since then, six more distinct "client" and "server" editions of OS X have been released, until the two were merged in OS X 10.7 "Lion". Releases of OS X v10.0 through v10.8 are named after big cats. Starting with v10.9, "Mavericks", OS X versions are named after inspirational places in California. OS X 10.10 "Yosemite", the most recent version, was announced and released on June 2, 2014 at the WWDC 2014.
Prior to its merging with OS X, the server edition – OS X Server – was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. OS X Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as "OS X" (dropping "Mac" from the name). The server tools are now offered as an application.
Linux and GNU.
The GNU project is a collaboration of many programmers who envisioned to create a free and open operating system that was similar to Unix but with new code licensed on the open-source license model. It was started in 1983 by Richard Stallman, and is responsible for many components of most Linux variants. Thousands of pieces of software for virtually every operating system are licensed under the GNU General Public License. Meanwhile, the Linux kernel originated in 1991 as a side project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel. GNU programmers joint the effort and both groups worked to integrate the finished GNU parts with the Linux kernel to create a complete operating system.
Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smart-watches. Although estimates suggest that Linux and GNU software are used on only 1.82% of all personal computers, it has been widely adopted for use in servers and embedded systems such as cell phones. GNU/Linux has superseded Unix on many platforms and is used on the ten most powerful supercomputers in the world. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android.
Google Chromium OS.
Chromium is an operating system based on the Linux kernel and designed by Google. Since Chromium OS targets computer users who spend most of their time on the Internet, it is mainly a web browser with limited ability to run local applications, though it has a built-in file manager and media player. Instead, it relies on Internet applications (or Web apps) used in the web browser to accomplish tasks such as word processing. Chromium OS differs from Chrome OS in that Chromium is open-source and used primarily by developers whereas Chrome OS is the operating system shipped out in Chromebooks.
Microsoft Windows.
Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers. The newest version is Windows 8.1 for workstations and Windows Server 2012 R2 for servers. Windows 7 recently overtook Windows XP as most used OS.
Microsoft Windows originated in 1985 as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS and 16 bits Windows 3.x drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and 32-bit ARM microprocessors. In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures.
Server editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. However, Windows' usage on servers is not as widespread as on personal computers, as Windows competes against Linux and BSD for server market share.The first PC that used windows operating system was the IBM Personal System/2.
Other.
There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; Mac OS, the non-Unix precursor to Apple's Mac OS X; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. OpenVMS formerly from DEC, is still under active development by Hewlett-Packard. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research.
Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9.
Components.
The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
Kernel.
With the aid of the firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.
Program execution.
The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices.
Interrupts.
Interrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment. The alternative — having the operating system "watch" the various sources of input for events (polling) that require action — can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.
When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call. In modern operating systems, interrupts are handled by the operating system's kernel. Interrupts may come from either the computer's hardware or the running program.
When a hardware device triggers an interrupt, the operating system's kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called a device driver, which may be part of the operating system's kernel, part of another program, or both. Device drivers may then relay information to a running program by various means.
A program may also trigger an interrupt to the operating system. If a program wishes to access hardware, for example, it may interrupt the operating system's kernel, which causes control to be passed back to the kernel. The kernel then processes the request. If a program wishes additional resources (or wishes to shed resources) such as memory, it triggers an interrupt to get the kernel's attention.
Modes.
Modern CPUs support multiple modes of operation. CPUs with this capability use at least two modes: protected mode and supervisor mode. The supervisor mode is used by the operating system's kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is written and erased, and communication with devices like graphics cards. Protected mode, in contrast, is used for almost everything else. Applications operate within protected mode, and can only use hardware by communicating with the kernel, which controls everything in supervisor mode. CPUs might have other modes similar to protected mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.
When a computer first starts up, it is automatically running in supervisor mode. The first few programs to run on the computer, being the BIOS or EFI, bootloader, and the operating system have unlimited access to hardware – and this is required because, by definition, initializing a protected environment can only be done outside of one. However, when the operating system passes control to another program, it can place the CPU into protected mode.
In protected mode, programs may have access to a more limited set of the CPU's instructions. A user program may leave protected mode only by triggering an interrupt, causing control to be passed back to the kernel. In this way the operating system can maintain exclusive control over things like access to hardware and memory.
The term "protected mode resource" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting (for example, by killing the program).
Memory management.
Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn't exist in all computers.
In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt which cause the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.
Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.
Virtual memory.
The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.
When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.
In modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
"Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.
Multitasking.
Multitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.
An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.
Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.
The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)
On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The AmigaOS is an exception, having pre-emptive multitasking from its very first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals).
Disk access and file systems.
Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use out of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.
Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.
While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.
A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.
When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.
Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ext3 and ReiserFS in Linux. However, in practice, third party drives are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).
Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.
Device drivers.
A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.
Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.
Networking.
Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's network address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.
Security.
A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.
The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between "privileged" and "non-privileged", systems commonly have a form of requester "identity", such as a user name. To establish identity there may be a process of "authentication". Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is "authorization"; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.
In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, "who has been reading this file?"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.
External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the "Trusted Computer System Evaluation Criteria" (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.
Network services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.
An alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java.
Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing.
User interface.
Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
Graphical user interfaces.
Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of Mac OS, the GUI is integrated into the kernel.
While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and Mac OS X are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.
Many computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma Desktop is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.
Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).
Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.
Real-time operating systems.
A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.
Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase. Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.
Operating system development as a hobby.
Operating system development is one of the most complicated activities in which a computing hobbyist may engage. A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.
In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
Examples of a hobby operating system include ReactOS and Syllable.
Diversity of operating systems and portability.
Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
Unix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11. 
This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms like Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
Further reading.
</dl>

</doc>
<doc id="22196" url="http://en.wikipedia.org/wiki?curid=22196" title="Orson Welles">
Orson Welles

George Orson Welles (; May 6, 1915 – October 10, 1985) was an American actor, director, writer and producer who worked in theater, radio and film. He is best remembered for his innovative work in all three media: in theatre, most notably "Caesar" (1937), a groundbreaking Broadway adaptation of "Julius Caesar"; in radio, the 1938 broadcast "The War of the Worlds", one of the most famous in the history of radio; and in film, "Citizen Kane" (1941), consistently ranked as one of the all-time greatest films.
Welles directed a number of high-profile stage productions for the Federal Theatre Project in his early twenties, including an innovative adaptation of "Macbeth" and "The Cradle Will Rock". In 1937 he and John Houseman founded the Mercury Theatre, an independent repertory theatre company that presented an acclaimed series of productions on Broadway through 1941. Welles found national and international fame as the director and narrator of a 1938 radio adaptation of H. G. Wells' novel "The War of the Worlds" performed for the radio anthology series "The Mercury Theatre on the Air". It reportedly caused widespread panic when listeners thought that an invasion by extraterrestrial beings was occurring. Although some contemporary sources claim these reports of panic were mostly false and overstated, they rocketed Welles to notoriety.
His first film was "Citizen Kane" (1941), which he co-wrote, produced, directed, and starred in as Charles Foster Kane. Welles was an outsider to the studio system and directed only 13 full-length films in his career. Because of this, he struggled for creative control from the major film studios, and his films were either heavily edited or remained unreleased. His distinctive directorial style featured layered and nonlinear narrative forms, innovative uses of lighting such as chiaroscuro, unusual camera angles, sound techniques borrowed from radio, deep focus shots, and long takes. He has been praised as a major creative force and as "the ultimate auteur".:6 Welles followed up "Citizen Kane" with critically acclaimed films including "The Magnificent Ambersons" in 1942 and "Touch of Evil" in 1958. Although the three are generally considered his greatest works, some film critics have also argued other works of his, such as "The Lady from Shanghai" (1947) and "Chimes at Midnight" (1966),
are under-appreciated.
In 2002, Welles was voted the greatest film director of all time in two British Film Institute polls among directors and critics, and a wide survey of critical consensus, best-of lists, and historical retrospectives calls him the most acclaimed director of all time. Well known for his baritone voice, Welles was a well-regarded actor in radio and film, a celebrated Shakespearean stage actor, and an accomplished magician noted for presenting troop variety shows in the war years.
Early life.
George Orson Welles was born May 6, 1915, in Kenosha, Wisconsin, son of Richard Head Welles (b. Richard Hodgdon Wells, November 12, 1872, near St. Joseph, Missouri; d. December 28, 1930, Chicago, Illinois):26 and Beatrice Ives Welles (b. September 1, 1881, Springfield, Illinois; d. May 10, 1924, Chicago). He was named after his paternal great-grandfather, influential Kenosha attorney Orson S. Head, and his brother George Head.:37
Despite his family's affluence, Welles encountered hardship in childhood. His parents separated and moved to Chicago in 1919. His father, who made a fortune as the inventor of a popular bicycle lamp, became an alcoholic and stopped working. Welles's mother, a pianist, played during lectures by Dudley Crafts Watson at the Art Institute of Chicago to support her son and herself; the oldest Welles boy, "Dickie", was institutionalized at an early age because he had learning difficulties. Beatrice died of hepatitis in a Chicago hospital:3–5 May 10, 1924, aged 42, just after Welles's ninth birthday.:326 The Gordon String Quartet, which had made its first appearance at her home in 1921, played at Beatrice's funeral.
After his mother's death Welles ceased pursuing music. It was decided that he would spend the summer with the Watson family at a private art colony in Wyoming, New York, established by Lydia Avery Coonley Ward.:8 There he played and became friends with the children of the Aga Khan, including the 12-year-old Prince Aly Khan. Then, in what Welles later described as "a hectic period" in his life, he lived in a Chicago apartment with both his father and Dr. Maurice Bernstein, a Chicago physician who had been a close friend of both his parents. Welles briefly attended public school:133 before his alcoholic father left business altogether and took him along on his travels to Jamaica and the Far East. When they returned they settled in a hotel in Grand Detour, Illinois, that was owned by his father. When the hotel burned down Welles and his father took to the road again.:9
"During the three years that Orson lived with his father, some observers wondered who took care of whom", wrote biographer Frank Brady.:9
"In some ways, he was never really a young boy, you know," said Roger Hill, who became Welles's teacher and lifelong friend.:24
Welles briefly attended public school in Madison, Wisconsin, enrolled in the fourth grade.:9 On September 15, 1926, he entered the Todd Seminary for Boys,:3 an expensive independent school in Woodstock, Illinois, that his older brother, Richard Ives Welles, had attended ten years before but was expelled for misbehavior.:48 At Todd School Welles came under the influence of Roger Hill, a teacher who was later Todd's headmaster. Hill provided Welles with an "ad hoc" educational environment that proved invaluable to his creative experience, allowing Welles to concentrate on subjects that interested him. Welles performed and staged theatrical experiments and productions there.
"Todd provided Welles with many valuable experiences", wrote critic Richard France. "He was able to explore and experiment in an atmosphere of acceptance and encouragement. In addition to a theater the school's own radio station was at his disposal.":27 Welles's first radio performance was on the Todd station, an adaptation of "Sherlock Holmes" that he also wrote.:7
On December 28, 1930, when Welles was 15, his father died at the age of 58, alone in a hotel in Chicago. His will left it to Orson to name his guardian. When Roger Hill declined, Welles chose Maurice Bernstein.:71–72
Following graduation from Todd in May 1931,:3 Welles was awarded a scholarship to Harvard University, while his mentor Roger Hill advocated he attend Cornell College in Iowa. Rather than enrolling, he chose travel. He studied for a few weeks at the Art Institute of Chicago:117 with Boris Anisfeld, who encouraged him to pursue painting.:18
Welles would occasionally return to Woodstock, the place he eventually named when he was asked in a 1960 interview, "Where is home?" Welles replied, "I suppose it's Woodstock, Illinois, if it's anywhere. I went to school there for four years. If I try to think of a home, it's that."
Early career (1931–1935).
After his father's death, Welles traveled to Europe using a small portion of his inheritance. Welles said that while on a walking and painting trip through Ireland, he strode into the Gate Theatre in Dublin and claimed he was a Broadway star. The manager of Gate, Hilton Edwards, later said he had not believed him but was impressed by his brashness and an impassioned quality in his audition.:134 Welles made his stage debut at the Gate Theatre on October 13, 1931, appearing in Ashley Dukes's adaptation of "Jew Suss" as Duke Karl Alexander of Württemberg. He performed small supporting roles in subsequent Gate productions, and he produced and designed productions of his own in Dublin. In March 1932 Welles performed in W. Somerset Maugham's "The Circle" at Dublin's Abbey Theatre and travelled to London to find additional work in the theatre. Unable to obtain a work permit, he returned to the U.S.:327–330
Welles found his fame ephemeral and turned to a writing project at Todd School that would become the immensely successful, first entitled "Everybody's Shakespeare" and subsequently, "The Mercury Shakespeare". Welles traveled to North Africa while working on thousands of illustrations for the "Everybody's Shakespeare" series of educational books, a series that remained in print for decades.
In 1933, Roger and Hortense Hill invited Welles along to a party in Chicago, where Welles met Thornton Wilder. Wilder arranged for Welles to meet Alexander Woollcott in New York, in order that he be introduced to Katharine Cornell, who was assembling a repertory theatre company. Cornell's husband, director Guthrie McClintic, immediately put Welles under contract and cast him in three plays.:46–49 "Romeo and Juliet", "The Barretts of Wimpole Street" and "Candida" toured in repertory for 36 weeks beginning in November 1933, with the first of more than 200 performances taking place in Buffalo, New York.:330–331
In 1934, Welles got his first job on radio — on "The American School of the Air" — through actor-director Paul Stewart, who introduced him to director Knowles Entrikin.:331 That summer Welles staged a drama festival with the Todd School in Woodstock, Illinois, inviting Micheál Mac Liammóir and Hilton Edwards from Dublin's Gate Theatre to appear along with New York stage luminaries in productions including "Trilby", "Hamlet", "The Drunkard" and "Tsar Paul". At the old firehouse in Woodstock he also shot his first film, an eight-minute short titled "The Hearts of Age".:330–331
A revised production of Katharine Cornell's "Romeo and Juliet" opened December 20, 1934, at the Martin Beck Theatre in New York.:331–332 The Broadway production brought the 19-year-old Welles (now playing Tybalt) to the notice of John Houseman, a theatrical producer who was casting the lead role in the debut production of Archibald MacLeish's verse play, "Panic".:144–158
On November 14, 1934, Welles married Chicago socialite and actress Virginia Nicolson:332 (often misspelled "Nicholson") in a civil ceremony in New York. To appease the Nicolsons, who were furious at the couple's elopement, a formal ceremony took place December 23, 1934, at the New Jersey mansion of the bride's godmother. Welles wore a cutaway borrowed from his friend George Macready.:182
By 1935 Welles was supplementing his earnings in the theater as a radio actor in Manhattan, working with many actors who would later form the core of his Mercury Theatre on programs including "America's Hour", "Cavalcade of America", "Columbia Workshop" and "The March of Time".:331–332 "Within a year of his debut Welles could claim membership in that elite band of radio actors who commanded salaries second only to the highest paid movie stars," wrote critic Richard France.:172
Theatre (1936–1938).
Federal Theatre Project.
Part of the Works Progress Administration, the Federal Theatre Project (1935–39) was a New Deal program to fund theatre and other live artistic performances and entertainment programs in the United States during the Great Depression. It was created as a relief measure to employ artists, writers, directors and theater workers. Under national director Hallie Flanagan it was shaped into a true national theatre that created relevant art, encouraged experimentation and innovation, and made it possible for millions of Americans to see live theatre for the first time. Within a year the Federal Theatre Project employed 15,000 men and women at just over $20 a week. During its nearly four years of existence it played to 30 million people in more than 200 theaters nationwide, as well as portable stages, public parks and schools.:174
Welles joined the Federal Theatre Project and, with John Houseman, he came to run two of its New York City theatres. Far from unemployed — "I was so employed I forgot how to sleep" — Welles put a large share of his $1,500-a-week radio earnings into his stage productions, bypassing administrative red tape and mounting the projects more quickly and professionally. "Roosevelt once said that I was the only operator in history who ever illegally siphoned money "into" a Washington project," Welles said.:11–13
The Federal Theatre Project was the ideal environment in which Welles could develop his art. Its purpose was employment, so he was able to hire any number of artists, craftsmen and technicians, and he filled the stage with performers.:3 The company for the first production, "Macbeth", numbered 150. The second, the farcical "Horse Eats Hat", was selected specifically to employ many dozens of vaudevillians and circus performers who had been rejected by other Federal Theatre projects.:212:250
"Macbeth".
In 1935 Welles was hired by John Houseman and assigned to direct a play for the Federal Theatre Project's Negro Theater Unit. He offered "Macbeth". The production became known as the "Voodoo Macbeth", because Welles set it in the Haitian court of King Henri Christophe, with voodoo witch doctors for the three Weird Sisters. Jack Carter played Macbeth. Canada Lee, who two years before had rescued Welles from a potentially dangerous scrape with an armed theater-goer, played Banquo. The incidental music was composed by Virgil Thomson. The play opened April 14, 1936, at the Lafayette Theatre in Harlem and was received rapturously. At 20, Welles was hailed as a prodigy. The production then made a 4,000-mile national tour:333 that included two weeks at the Texas Centennial Exposition in Dallas.
"Horse Eats Hat".
After the success of "Macbeth", Welles mounted the farce "Horse Eats Hat", an adaptation by Welles and Edwin Denby of Eugène Labiche's play, "Un Chapeau de Paille d'Italie".:114 The play was presented September 26 – December 5, 1936, at Maxine Elliott's Theatre, New York.:334 Joseph Cotten was featured in his first starring role.
"Faustus".
Welles consolidated his "White Hope" reputation with "Dr. Faustus", which used light as a prime unifying scenic element in a nearly black stage. "Faustus" was presented January 8 – May 9, 1937, at Maxine Elliott's Theatre, New York.:335
"The Second Hurricane".
In 1937 American composer Aaron Copland chose Welles to direct "The Second Hurricane", an operetta with a libretto by Edwin Denby, and one of Copland's least known works. Presented at the Henry Street Settlement Music School in New York for the benefit of high school students, the production opened April 21, 1937, and ran its scheduled three performances.:337 Among the few adult performers in the production was actor Joseph Cotten, Welles's longtime friend and collaborator, who was paid $10 for his performance.
"The Cradle Will Rock".
In 1937, Welles rehearsed Marc Blitzstein's political operetta, "The Cradle Will Rock". It was originally scheduled to open June 16, 1937, in its first public preview. Because of severe federal cutbacks in the Works Progress projects, the show's premiere at the Maxine Elliott Theatre was canceled. The theater was locked and guarded to prevent any government-purchased materials from being used for a commercial production of the work. In a last-minute move, Welles announced to waiting ticket-holders that the show was being transferred to the Venice, 20 blocks away. Some cast, and some crew and audience, walked the distance on foot. The union musicians refused to perform in a commercial theater for lower non-union government wages. The actors' union stated that the production belonged to the Federal Theater Project and could not be performed outside that context without permission. Lacking the participation of the union members, "The Cradle Will Rock" began with Blitzstein introducing the show and playing the piano accompaniment on stage with some cast members performing from the audience. This impromptu performance was well received by its audience.
Mercury Theatre.
Breaking with the Federal Theatre Project in 1937, Welles and Houseman founded their own repertory company, which they called the Mercury Theatre. The name was inspired by the title of the iconoclastic magazine, "The American Mercury".:119–120
Welles became executive producer and the repertory company eventually included actors such as Ray Collins, George Coulouris, Joseph Cotten, Dolores del Río, Agnes Moorehead, Erskine Sanford and Everett Sloane, all of whom worked for Welles for years.
The first Mercury Theatre stage production was a melodramatic edited version of William Shakespeare's tragedy "Julius Caesar", set in a contemporary frame of fascist Italy. Cinna, the Poet dies at the hands not of a mob but of a secret police force. According to Norman Lloyd, who played Cinna the Poet, "it stopped the show."
"Caesar" opened November 11, 1937, followed by "The Shoemaker's Holiday" (January 11, 1938), "Heartbreak House" (April 29, 1938) and "Danton's Death" (November 5, 1938).:344
Radio (1936–1940).
Simultaneously with his work in the theatre, Welles worked extensively in radio as an actor, writer, director and producer, often without credit.:77 Between 1935 and 1937 he was earning as much as $2,000 a week, shuttling between radio studios at such a pace that he would arrive barely in time for a quick scan of his lines before he was on the air. While he was directing the "Voodoo Macbeth" Welles was dashing between Harlem and midtown Manhattan three times a day to meet his radio commitments.:172
"What didn't I do on the radio?" Welles reflected in February 1983:
Radio is what I love most of all. The wonderful excitement of what could happen in live radio, when everything that could go wrong did go wrong. I was making a couple of thousand a week, scampering in ambulances from studio to studio, and committing much of what I made to support the Mercury. I wouldn't want to return to those frenetic 20-hour working day years, but I miss them because they are so irredeemably gone.:53
In addition to continuing as a repertory player on "The March of Time", in the fall of 1936 Welles adapted and performed "Hamlet" in an early two-part episode of CBS Radio's "Columbia Workshop". His performance as the announcer in the series' April 1937 presentation of Archibald MacLeish's verse drama "The Fall of the City" was an important development in his radio career:78 and made the 21-year-old Welles an overnight star.:46
In July 1937, the Mutual Network gave Welles a seven-week series to adapt "Les Misérables". It was his first job as a writer-director for radio,:338 the radio debut of the Mercury Theatre, and one of Welles's earliest and finest achievements.:160 He invented the use of narration in radio.:88
"By making himself the center of the storytelling process, Welles fostered the impression of self-adulation that was to haunt his career to his dying day," wrote critic Andrew Sarris. "For the most part, however, Welles was singularly generous to the other members of his cast and inspired loyalty from them above and beyond the call of professionalism.":8
That September, Mutual chose Welles to play Lamont Cranston, also known as "The Shadow". He performed the role anonymously through mid-September 1938.:83
"The Mercury Theatre on the Air".
After the theatrical successes of the Mercury Theatre, CBS Radio invited Orson Welles to create a summer show for 13 weeks. The series began July 11, 1938, initially titled "First Person Singular", with the formula that Welles would play the lead in each show. Some months later the show was called "The Mercury Theatre on the Air".:12 The weekly hour-long show presented radio plays based on classic literary works, with original music composed and conducted by Bernard Herrmann.
"The War of the Worlds".
The Mercury Theatre's radio adaptation of "The War of the Worlds" by H. G. Wells October 30, 1938, brought Welles instant fame. The combination of the news bulletin form of the performance with the between-breaks dial spinning habits of listeners was later reported to have created widespread confusion among listeners who failed to hear the introduction, although the extent of this confusion has come into question. Panic was reportedly spread among listeners who believed the fictional news reports of a Martian invasion. The myth of the result created by the combination was reported as fact around the world and disparagingly mentioned by Adolf Hitler in a public speech some months later.
Welles's growing fame drew Hollywood offers, lures that the independent-minded Welles resisted at first. "The Mercury Theatre on the Air," which had been a sustaining show (without sponsorship) was picked up by Campbell Soup and renamed "The Campbell Playhouse."
"The Campbell Playhouse".
As a direct result of the front-page headlines Orson Welles generated with his 1938 Halloween production "The War of the Worlds", Campbell's Soup signed on as sponsor. "The Mercury Theatre on the Air" made its last broadcast on December 4, 1938, and "The Campbell Playhouse" began five days later.
Welles began commuting from Hollywood to New York for the two Sunday broadcasts of "The Campbell Playhouse" after signing a film contract with RKO Pictures in August 1939. In November 1939, production of the show moved from New York to Los Angeles.:353
After 20 shows, Campbell began to exercise more creative control and had complete control over story selection. As his contract with Campbell came to an end, Welles chose not to sign on for another season. After the broadcast of March 31, 1940, Welles and Campbell parted amicably.:221–226
Hollywood (1939–1948).
RKO Radio Pictures president George Schaefer eventually offered Welles what generally is considered the greatest contract offered to an untried director: complete artistic control.
After signing a summary agreement with RKO on July 22, Welles signed a full-length 63-page contract August 21, 1939.:353
RKO signed Welles in a two-picture deal; including script, cast, crew and most importantly, final cut, although Welles had a budget limit for his projects. With this contract in hand, Welles (and nearly the whole Mercury Theatre troupe) moved to Hollywood.
Welles toyed with various ideas for his first project for RKO Radio Pictures, settling on an adaptation of Joseph Conrad's "Heart of Darkness", which he worked on in detail. He planned to film the action with a subjective camera (a technique later used in the Robert Montgomery film "Lady in the Lake"). When a budget was drawn up, RKO's enthusiasm cooled because it was greater than the agreed limit.
Welles's first experience on a Hollywood film was narrator for RKO's 1940 production of "Swiss Family Robinson".
"Citizen Kane".
Production.
RKO, having rejected Welles's first two movie proposals, agreed on the third offer, "Citizen Kane," which Welles co-wrote, produced and directed, also performing the lead role.
Welles found a suitable film project in an idea he conceived with screenwriter Herman J. Mankiewicz, then writing radio plays for "The Campbell Playhouse".:16 Initially titled "American", it eventually became Welles's first feature film (his most famous and honored role), "Citizen Kane" (1941).
Mankiewicz based the original outline on an "exposé" of the life of William Randolph Hearst, whom he knew socially and came to hate, having once been great friends with Hearst's mistress, Marion Davies.
Supplying Mankiewicz with 300 pages of notes, Welles urged him to write the first draft screenplay under John Houseman, who was posted to ensure Mankiewicz stayed sober. On Welles's instruction, Houseman wrote the opening narration as a pastiche of "The March of Time" newsreels. Orson Welles explained to Peter Bogdanovich about the writers working separately by saying, "I left him on his own finally, because we'd started to waste too much time haggling. So, after mutual agreements on storyline and character, Mank went off with Houseman and did his version, while I stayed in Hollywood and wrote mine.":54 Taking these drafts, Welles drastically condensed and rearranged them, then added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, "At the end, naturally, I was the one making the picture, after all—who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own.":54
Charles Foster Kane is based loosely on areas of Hearst's life. Nonetheless, autobiographical allusions to Welles were worked in, most noticeably in the treatment of Kane's childhood and particularly, regarding his guardianship. Welles added features from other famous American lives to create a general and mysterious personality, rather than the narrow journalistic portrait drawn by Mankiewicz, whose first drafts included scandalous claims about the death of film director Thomas Ince.
Once the script was complete, Welles attracted some of Hollywood's best technicians, including cinematographer Gregg Toland, who walked into Welles's office and announced he wanted to work on the picture. Welles described Toland as "the fastest cameraman who ever lived." For the cast, Welles primarily used actors from his Mercury Theatre. He invited suggestions from everyone but only if they were directed through him. Filming "Citizen Kane" took ten weeks.
Reaction.
Mankiewicz handed a copy of the shooting script to his friend, Charles Lederer, husband of Welles's ex-wife, Virginia Nicolson, and the nephew of Hearst's mistress, Marion Davies. Gossip columnist Hedda Hopper saw a small ad in a newspaper for a preview screening of "Citizen Kane" and went. Hopper realized immediately that the film was based on features of Hearst's life. Thus began a struggle, the attempted suppression of "Citizen Kane".
Hearst's media outlets boycotted the film. They exerted enormous pressure on the Hollywood film community by threatening to expose fifteen years of suppressed scandals and the fact that most studio bosses were Jewish. At one point, heads of the major studios jointly offered RKO the cost of the film in exchange for the negative and existing prints, fully intending to burn them. RKO declined, and the film was given a limited release. Hearst intimidated theater chains by threatening to ban advertising for their other films in his papers if they showed "Citizen Kane".
The film was well-received critically, with Bosley Crowther, film critic for the "New York Times" calling it "close to being the most sensational film ever made in Hollywood". By the time it reached the general public, the publicity had waned. It garnered nine Academy Award nominations (Orson nominated as a producer, director, writer and actor), but won only for Best Original Screenplay, shared by Mankiewicz and Welles. Although it was largely ignored at the Academy Awards, "Citizen Kane" is now hailed as one of the greatest films ever made. Andrew Sarris called it "the work that influenced the cinema more profoundly than any American film since "The Birth of a Nation"."
The delay in its release and uneven distribution contributed to mediocre results at the box office; it earned back its budget and marketing, but RKO lost any chance of a major profit. The fact that "Citizen Kane" ignored many Hollywood conventions meant that the film confused and angered the 1940s cinema public. Exhibitor response was scathing; most theater owners complained bitterly about the adverse audience reaction and the many walkouts. Only a few saw fit to acknowledge Welles's artistic technique. RKO shelved the film and did not re-release it until 1956.
During the 1950s, the film came to be seen by young French film critics such as François Truffaut as exemplifying the "auteur theory", in which the director is the "author" of a film. Truffaut, Godard and others inspired by Welles's example made their own films, giving birth to the Nouvelle Vague. In the 1960s "Citizen Kane" became popular on college campuses as a film-study exercise and as an entertainment subject. Its revivals on television, home video and DVD have enhanced its "classic" status and ultimately recouped costs. The film is considered by most film critics and historians to be one of, if not the, greatest motion pictures in cinema history.
"The Magnificent Ambersons".
Welles's second film for RKO was "The Magnificent Ambersons", adapted from the Pulitzer Prize-winning novel by Booth Tarkington. George Schaefer hoped to make money with this film, since he lost money with "Citizen Kane". "Ambersons" had been adapted for "The Campbell Playhouse" by Welles, for radio, and Welles then wrote the screen adaptation. Toland was not available, so Stanley Cortez was named cinematographer. The meticulous Cortez worked slowly and the film lagged behind schedule and over budget. Prior to production, Welles's contract was renegotiated, revoking his right to control the final cut.
"The Magnificent Ambersons" was in production October 28, 1941 – January 22, 1942. Throughout the shooting of the film Welles was also producing a weekly half-hour radio series, "The Orson Welles Show". Many of the "Ambersons" cast participated in the CBS Radio series, which ran September 15, 1941 – February 2, 1942.:525
"Journey into Fear".
At RKO's request, Welles worked on an adaptation of Eric Ambler's spy thriller, "Journey into Fear", co-written with Joseph Cotten. In addition to acting in the film, Welles was the producer. Direction was credited to Norman Foster. Welles later said that they were in such a rush that the director of each scene was determined by whoever was closest to the camera.
"Journey into Fear" was in production January 6–March 12, 1942.
War work.
Goodwill ambassador.
In late November 1941, Welles was appointed as a goodwill ambassador to Latin America by Nelson Rockefeller, U.S. Coordinator of Inter-American Affairs and a principal stockholder in RKO Radio Pictures.:244 The Office of the Coordinator of Inter-American Affairs was established in August 1940 by order of the U.S. Council of National Defense, and operated with funds from both the government and the private sector.:10–11 By executive order July 30, 1941, President Franklin D. Roosevelt established the OCIAA within the Office for Emergency Management of the Executive Office of the President, "to provide for the development of commercial and cultural relations between the American Republics and thereby increasing the solidarity of this hemisphere and furthering the spirit of cooperation between the Americas in the interest of hemisphere defense."
The mission of the OCIAA was cultural diplomacy, promoting hemispheric solidarity and countering the growing influence of the Axis powers in Latin America. The OCIAA's Motion Picture Division played an important role in documenting history and shaping opinion toward the Allied nations, particularly after the U.S. entered World War II in December 1941. To support the war effort — and for their own audience development throughout Latin America — Hollywood studios partnered with the U.S. government on a nonprofit basis, making films and incorporating Latin American stars and content into their commercial releases.:10–11
The OCIAA's Motion Picture Division was led by John Hay Whitney, who was asked by the Brazilian government to produce a documentary of the annual Rio Carnival celebration taking place in early February 1942.:40–41 In a telegram December 20, 1941, Whitney wrote Welles, "Personally believe you would make great contribution to hemisphere solidarity with this project.":65
Artists working in a variety of disciplines were sent to Latin America as goodwill ambassadors by the OCIAA, most on tours of two to four months. A select listing includes Misha Reznikoff and photojournalist Genevieve Naylor (October 1940–May 1943); Bing Crosby (August–October 1941); Walt Disney (August–October 1941); Aaron Copland (August–December 1941); George Balanchine and the American Ballet (1941); Rita Hayworth (1942); Grace Moore (1943); John Ford (1943) and Gregg Toland (1943). Welles was thoroughly briefed in Washington, D.C., immediately before his departure for Brazil, and film scholar Catherine L. Benamou, a specialist in Latin American affairs, finds it "not unlikely" that he was among the goodwill ambassadors who were asked to gather intelligence for the U.S. government in addition to their cultural duties. She concludes that Welles's acceptance of Whitney's request was "a logical and patently patriotic choice".:245–247
In addition to working on his ill-fated film project, "It's All True", Welles was responsible for radio programs, lectures, interviews and informal talks as part of his OCIAA-sponsored cultural mission, which was a success.:192 He spoke on topics ranging from Shakespeare to visual art to American theatre at gatherings of Brazil's elite, and his two intercontinental radio broadcasts in April 1942 were particularly intended to tell U.S. audiences that President Vargas was a partner with the Allies. Welles's ambassadorial mission would be extended to permit his travel to other nations including Argentina, Bolivia, Chile, Colombia, Ecuador, Guatemala, Mexico, Peru and Uraguay.:247–249, 328
As an emissary of the U.S. government, Welles received no salary.:41, 328
"What's really and ironically true about "It's All True"," wrote associate producer Richard Wilson, "is that Welles was approached to make a non-commercial picture, then was bitterly reproached for making a non-commercial picture. Right here I'd like to make it a matter of record," Wilson continued:
Both RKO and Welles got into the project by trying to do their bit for the war effort. However: RKO, as a company responsible to stockholders, negotiated a private and tough agreement for the U.S. Government to pay it 300,000 dollars to undertake its bit. This speaks eloquently enough for its evaluation of the project as a non-commercial venture. I personally think that Orson's waiving any payment whatever for his work, and his giving up a lucrative weekly radio program, is even more eloquent. For a well-paid creative artist to work for over half a year for no remuneration is a most uncommon occurrence.:189
Welles's own expectations for the film were modest, as he told biographer Barbara Leaming: ""It's All True" was not going to make any cinematic history, nor was it intended to. It was intended to be a perfectly honorable execution of my job as a goodwill ambassador, bringing entertainment to the Northern Hemisphere that showed them something about the Southern one.":253
"It's All True".
In July 1941, Orson Welles conceived "It's All True" as an omnibus film mixing documentary and docufiction.:221:27
"In addition to the tenuous boundary between 'real' and 'staged' events," wrote film scholar Catherine L. Benamou, "there was a thematic emphasis on the achievement of dignity by the working person, along with the celebration of cultural and ethnic diversity of North America.":109
It was to have been his third film for RKO, following "Citizen Kane" (1941) and "The Magnificent Ambersons" (1942).:109 Duke Ellington was put under contract to score a segment with the working title, "The Story of Jazz", drawn from Louis Armstrong's 1936 autobiography, "Swing That Music".:232–233 The episode was to be a brief dramatization of the history of jazz performance, from its roots to its place in American culture in the 1940s. Cast as himself, Louis Armstrong would play the central role.:109 "The Story of Jazz" was to go into production in December 1941.:119–120
Mercury Productions purchased the stories for two other segments — "My Friend Bonito" and "The Captain's Chair" — from documentary filmmaker Robert J. Flaherty.:33, 326 Adapted by Norman Foster and John Fante (author of a fourth proposed segment, "Love Story"), "My Friend Bonito" was the only segment of the original "It's All True" to go into production.:109 Filming took place in Mexico September–December 1941, with Norman Foster directing under Welles's supervision.:311
In December 1941, shortly after Welles's appointment as a goodwill ambassador to Latin America, the Office of the Coordinator of Inter-American Affairs asked Welles to make a film in Brazil that would showcase the Carnaval in Rio de Janeiro.:65 With filming of "My Friend Bonito" about two-thirds complete, Welles decided he could shift the geography of "It's All True" and incorporate Flaherty's story into an omnibus film about Latin America — supporting the Roosevelt administration's Good Neighbor policy, which Welles strongly advocated.:41, 246 In this revised concept, "The Story of Jazz" was replaced by the story of samba, a musical form with a comparable history and one that came to fascinate Welles. He also decided to do a ripped-from-the-headlines episode about the epic voyage of four poor Brazilian fishermen, the jangadeiros, who had become national heroes. Welles later said this was the most valuable story.:158–159:15
Required to film the Carnaval in Rio de Janeiro in early February 1942, Welles rushed to edit "The Magnificent Ambersons" and finish his acting scenes in "Journey into Fear". He ended his CBS radio show February 2, flew to Washington, D.C., for a briefing, and then lashed together a rough cut of "Ambersons" in Miami with editor Robert Wise.:369–370 Welles recorded the film's narration the night before he left for South America: "I went to the projection room at about four in the morning, did the whole thing, and then got on the plane and off to Rio — and the end of civilization as we know it.":115
Welles left for Brazil on February 4 and began filming in Rio February 8.:369–370
"Welles's diplomatic appointment did not appear at first to disrupt the continuity of either "The Magnificent Ambersons" or the "It's All True" projects," wrote Catherine L. Benamou:
Robert Wise planned to fly down to Rio to work with Welles on the final edit of "The Magnificent Ambersons", and although it would necessarily undergo a shift in geocultural emphasis, "It's All True" would retain its basic division into four episodes, along with its narrative foundation in historical experience. … In the very short run, however, the ambassadorial appointment would be the first in a series of turning points leading — in "zigs" and "zags," rather than in a straight line — to Welles's loss of complete directorial control over both "The Magnificent Ambersons" and "It's All True", the cancellation of his contract at RKO Radio Studio, the expulsion of his company Mercury Productions from the RKO lot, and, ultimately, the total suspension of "It's All True".:46
As a result of difficult financial circumstances at RKO in 1940–42, major changes occurred at the studio in 1942. Floyd Odlum and the Atlas Corporation took control of RKO and began changing its direction. Nelson Rockefeller, the most significant backer of the Brazil project, left the RKO board of directors. Around that time, the principal sponsor of Welles at RKO, studio president George Schaefer, resigned. The changes throughout RKO caused reevaluations of projects. RKO took control of "Ambersons" and edited the film into what the studio considered a commercial format. Welles's attempts to protect his version ultimately failed.
In South America, Welles requested resources to finish "It's All True". He was given a limited amount of black-and-white film stock and a silent camera. He finished shooting the episode about the jangadeiros, but RKO refused to support further production on the film.
"So I was fired from RKO," Welles told BBC interviewer Leslie Megahey ("The Orson Welles Story") in 1982:
And they made a great publicity point of the fact that I had gone to South America without a script and thrown all this money away. I never recovered from that attack. … RKO had its stationery that year, its official stationery, RKO Pictures and its slogan for that year. Printed on every piece of paper that went out from RKO was "Showmanship Instead of Genius". In other words, the reason you should buy an RKO picture was that you didn't get Orson Welles.:188
Radio projects 1942–43.
Welles returned to the United States August 22, 1942, after more than six months in South America.:372 A week after his return he produced and emceed the first two hours of a seven-hour coast-to-coast War Bond drive broadcast titled "I Pledge America". Airing August 29, 1942, on the Blue Network, the program was presented in cooperation with the United States Department of the Treasury, Western Union (which wired bond subscriptions free of charge) and the American Women's Voluntary Services. Featuring 21 dance bands and a score of stage and screen and radio stars including Fanny Brice, Bob Burns, Jane Cowl, Nelson Eddy, Duke Ellington and His Orchestra, Jane Froman, Edward G. Robinson, Lanny Ross, Carl Sandburg, Dinah Shore, Red Skelton and Meredith Willson, the broadcast raised more than $10 million — more than $146 million today — for the war effort.
On October 12, 1942, "Cavalcade of America" presented Welles's radio play, "Admiral of the Ocean Sea", an entertaining and factual look at the legend of Christopher Columbus.
"It belongs to a period when hemispheric unity was a crucial matter and many programs were being devoted to the common heritage of the Americas," wrote broadcasting historian Erik Barnouw. "Many such programs were being translated into Spanish and Portuguese and broadcast to Latin America, to counteract many years of successful Axis propaganda to that area. The Axis, trying to stir Latin America against Anglo-America, had constantly emphasized the differences between the two. It became the job of American radio to emphasize their common experience and essential unity.":3
"Admiral of the Ocean Sea", also known as "Columbus Day", begins with the words, "Hello Americans" — the title Welles would choose for his own series five weeks later.:373
"Hello Americans", a CBS Radio series broadcast November 15, 1942 – January 31, 1943, was produced, directed and hosted by Welles under the auspices of the Office of the Coordinator for Inter-American Affairs. The 30-minute weekly program promoted inter-American understanding and friendship, drawing upon the research amassed for the ill-fated film, "It's All True". The series was produced concurrently with Welles's other CBS series, "Ceiling Unlimited" (November 9, 1942 – February 1, 1943), sponsored by the Lockheed-Vega Corporation. The program was conceived to glorify the aviation industry and dramatize its role in World War II.
"Welles wrote, produced, and narrated this show, and his work was considered a prime contribution to the war effort," wrote the Museum of Broadcasting.:64
Throughout the war Welles worked on patriotic radio programs including "Command Performance", "G.I. Journal", "Mail Call", "Nazi Eyes on Canada", "Stage Door Canteen" and "Treasury Star Parade".
"The Mercury Wonder Show".
In early 1943, the two concurrent radio series ("Ceiling Unlimited", "Hello Americans") that Orson Welles created for CBS to support the war effort had ended. Filming also had wrapped on the 1943 film adaptation of "Jane Eyre" and that fee, in addition to the income from his regular guest-star roles in radio, made it possible for Welles to fulfill a lifelong dream. He approached the War Assistance League of Southern California and proposed a show that evolved into a big-top spectacle, part circus and part magic show. He offered his services as magician and director,:40 and invested some $40,000 of his own money in an extravaganza he co-produced with his friend Joseph Cotten: "The Mercury Wonder Show for Service Men". Members of the U.S. armed forces were admitted free of charge, while the general public had to pay.:26 The show entertained more than 1,000 service members each night, and proceeds went to the War Assistance League, a charity for military service personnel.
The development of the show coincided with the resolution of Welles's oft-changing draft status in May 1943, when he was finally declared 4-F — unfit for military service — for a variety of medical reasons. "I felt guilty about the war," Welles told biographer Barbara Leaming. "I was guilt-ridden about my civilian status.":86 He had been publicly hounded about his patriotism since "Citizen Kane", when the Hearst press began persistent inquiries about why Welles had not been drafted.:66–67
"The Mercury Wonder Show" ran August 3–September 9, 1943, in an 80-by-120-foot tent located at 9000 Cahuenga Boulevard, in the heart of Hollywood.:377:26
At intermission September 7, 1943, KMPC radio interviewed audience and cast members of "The Mercury Wonder Show" — including Welles and Rita Hayworth, who were married earlier that day. Welles remarked that "The Mercury Wonder Show" had been performed for approximately 48,000 members of the U.S. armed forces.:378:129
A portion of the stage show — in which "Orson the Magnificent" performs tricks like sawing a woman in half — was filmed and included in the morale-boosting 1944 variety film "Follow the Boys". The sequence was directed by Welles, uncredited, and features Marlene Dietrich.
Radio projects 1944–45.
The idea of doing a radio variety show occurred to Welles after his success as substitute host of four consecutive episodes (March 14–April 4, 1943) of "The Jack Benny Program", radio's most popular show, when Benny contracted pneumonia on a performance tour of military bases.:368 A half-hour variety show broadcast January 26–July 19, 1944, on the Columbia Pacific Network, "The Orson Welles Almanac" presented sketch comedy, magic, mindreading, music and readings from classic works. Many of the shows originated from U.S. military camps, where Welles and his repertory company and guests entertained the troops with a reduced version of "The Mercury Wonder Show".:64 The performances of the all-star jazz group Welles brought together for the show were so popular that the band became a regular feature and was an important force in reviving interest in traditional New Orleans jazz.:85
Welles was placed on the U.S. Treasury payroll May 15, 1944, as an expert consultant for the duration of the war, with a retainer of $1 a year. On the recommendation of President Franklin D. Roosevelt, Secretary of the Treasury Henry Morgenthau asked Welles to lead the Fifth War Loan Drive, which opened June 12 with a one-hour radio show on all four networks, broadcast from Texarkana, Texas. Including a statement by the President, the program defined the causes of the war and encouraged Americans to buy $16 billion in bonds to finance the Normandy landings and the most violent phase of World War II. Welles produced additional war loan drive broadcasts June 14 from the Hollywood Bowl, and June 16 from Soldier Field, Chicago.:371–373 Americans purchased $20.6 billion in War Bonds during the Fifth War Loan Drive, which ended July 8, 1944.
Welles campaigned ardently for Roosevelt in 1944. A longtime supporter and campaign speaker for FDR, he occasionally sent the president ideas and phrases that were sometimes incorporated into what Welles characterized as "less important speeches".:372, 374 One of these ideas was the joke in what came to be called the Fala speech, Roosevelt's nationally broadcast September 23 address to the International Teamsters Union which opened the 1944 presidential campaign.:292–293 Welles campaigned for the Roosevelt–Truman ticket almost full-time in the fall of 1944, traveling to nearly every state:373–374 to the detriment of his own health:293–294 and at his own expense.:219 In addition to his radio addresses he filled in for Roosevelt, opposite Republican presidential nominee Thomas E. Dewey, at "The New York Herald Tribune Forum" broadcast October 18 on the Blue Network.:386:292 Welles accompanied FDR to his last campaign rally, speaking at an event November 4 at Boston's Fenway Park before 40,000 people,:294 and took part in a historic election-eve campaign broadcast November 6 on all four radio networks.:387:166–167
"During a White House dinner," Welles recalled in a 1983 conversation with his friend Roger Hill, "when I was campaigning for Roosevelt, in a toast, with considerable tongue in cheek, he said, 'Orson, you and I are the two greatest actors alive today'. In private that evening, and on several other occasions, he urged me to run for a Senate seat either in California or Wisconsin. He wasn't alone.":115
On November 21, 1944, Welles began his association with "This Is My Best", a CBS radio series he would briefly produce, direct, write and host (March 13–April 24, 1945). He wrote a political column called "Orson Welles' Almanac" (later titled "Orson Welles Today") for "The New York Post" January–November 1945, and advocated the continuation of FDR's New Deal policies and his international vision, particularly the establishment of the United Nations and the cause of world peace.:84
On April 12, 1945, the day Franklin D. Roosevelt died, the Blue-ABC network marshalled its entire executive staff and national leaders to pay homage to the late president. "Among the outstanding programs which attracted wide attention was a special tribute delivered by Orson Welles", reported "Broadcasting" magazine. Welles spoke at 10:10 p.m Eastern War Time, from Hollywood, and stressed the importance of continuing FDR's work:
He has no need for homage and we who loved him have no time for tears … Our fighting sons and brothers cannot pause tonight to mark the death of him whose name will be given to the age we live in … We cannot do him reverence this April twelfth. There will be time for tears only when his work is done.
Welles presented another special broadcast on the death of Roosevelt the following evening: "We must move on beyond mere death to that free world which was the hope and labor of his life.":390:242
He dedicated the April 17 episode of "This Is My Best" to Roosevelt and the future of America on the eve of the United Nations Conference on International Organization.:390 Welles was an advisor and correspondent for the Blue-ABC radio network's coverage of the San Francisco conference that formed the UN, taking place April 24–June 23, 1945. He presented a half-hour dramatic program written by Ben Hecht on the opening day of the conference, and on Sunday afternoons (April 29–June 10) he led a weekly discussion from the San Francisco Civic Auditorium.
Post-war work.
"The Stranger".
In the fall of 1945 Welles began work on "The Stranger" (1946), a film noir drama about a war crimes investigator who tracks a high-ranking Nazi fugitive to an idyllic New England town. Edward G. Robinson, Loretta Young and Welles star.
Producer Sam Spiegel initially planned to hire director John Huston, who had rewritten the screenplay by Anthony Veiller. When Huston entered the military, Welles was given the chance to direct and prove himself able to make a film on schedule and under budget:19 — something he was so eager to do that he accepted a disadvantageous contract. One of its concessions was that he would defer to the studio in any creative dispute.:379:309–310
"The Stranger" was Welles's first job as a film director in four years.:391 He was told that if the film was successful he could sign a four-picture deal with International Pictures, making films of his own choosing.:379
Welles was given some degree of creative control,:19 and he endeavored to personalize the film and develop a nightmarish tone.:2:30 He worked on the general rewrite of the script and wrote scenes at the beginning of the picture that were shot but subsequently cut by the producers.:186 He filmed in long takes that largely thwarted the control given to editor Ernest J. Nims under the terms of the contract.:15:45
"The Stranger" was the first commercial film to use documentary footage from the Nazi concentration camps.:189 Welles had seen the footage in early May 1945:102:03 in San Francisco,:56 as a correspondent and discussion moderator at the UN Conference on International Organization.:304 He wrote of the Holocaust footage in his syndicated "New York Post" column May 7, 1945.:56–57
Completed a day ahead of schedule and under budget,:379–380 "The Stranger" was the only film made by Welles to have been a "bona fide" box office success upon its release. Its cost was $1.034 million; 15 months after its release it had grossed $3.216 million.
Within weeks of the completion of the film, International Pictures backed out of its promised four-picture deal with Welles. No reason was given, but the impression was left that "The Stranger" would not make money.:381
"Around the World".
In the summer of 1946, Welles directed "Around the World", a musical stage adaptation of the Jules Verne novel "Around the World in Eighty Days" with the book by Welles and music by Cole Porter. Producer Mike Todd, who would later produce the successful 1956 film adaptation, pulled out from the lavish and expensive Broadway production, leaving Welles to support the finances. When Welles ran out of money he convinced Columbia Pictures president Harry Cohn to send enough money to continue the show, and in exchange Welles promised to write, produce, direct and star in a film for Cohn for no further fee. The stage show soon failed due to poor box-office, with Welles unable to claim the losses on his taxes.
Radio series.
In 1946, Welles began two new radio series — "The Mercury Summer Theatre on the Air" for CBS, and "Orson Welles Commentaries" for ABC. While "Mercury Summer Theatre" featured half-hour adaptations of some classic Mercury radio shows from the 1930s, the first episode was a condensation of his "Around the World" stage play, and is the only record of Cole Porter's music for the project. Several original Mercury actors returned for the series, as well as Bernard Herrmann. It was only scheduled for the summer months, and Welles invested his earnings into his failing stage play. "Commentaries" was a political vehicle for him, continuing the themes from his "New York Post" column. Again, Welles lacked a clear focus, until the NAACP brought to his attention the case of Isaac Woodard. Welles brought significant attention to Woodard's cause.
"The Lady from Shanghai".
The film that Welles was obliged to make in exchange for Harry Cohn's help in financing the stage production "Around the World" was "The Lady from Shanghai", filmed in 1947 for Columbia Pictures. Intended as a modest thriller, the budget skyrocketed after Cohn suggested that Welles's then-estranged second wife Rita Hayworth co-star.
Cohn disliked Welles's rough-cut, particularly the confusing plot and lack of close-ups, and was not in sympathy with Welles's Brechtian use of irony and black comedy, especially in a farcical courtroom scene. Cohn ordered extensive editing and re-shoots. After heavy editing by the studio, approximately one hour of Welles's first cut was removed, including much of a climactic confrontation scene in an amusement park funhouse. While expressing displeasure at the cuts, Welles was appalled particularly with the musical score. The film was considered a disaster in America at the time of release, though the closing shootout in a hall of mirrors has since become a touchstone of film noir. Not long after release, Welles and Hayworth finalized their divorce.
Although "The Lady From Shanghai" was acclaimed in Europe, it was not embraced in the U.S. until decades later. A similar difference in reception on opposite sides of the Atlantic followed by greater American acceptance befell the Welles-inspired Chaplin film "Monsieur Verdoux", originally to be directed by Welles starring Chaplin, then directed by Chaplin with the idea credited to Welles.
"Macbeth".
Prior to 1948, Welles convinced Republic Pictures to let him direct a low-budget version of "Macbeth", which featured highly stylized sets and costumes, and a cast of actors lip-syncing to a pre-recorded soundtrack, one of many innovative cost-cutting techniques Welles deployed in an attempt to make an epic film from B-movie resources. The script, adapted by Welles, is a violent reworking of Shakespeare's original, freely cutting and pasting lines into new contexts via a collage technique and recasting "Macbeth" as a clash of pagan and proto-Christian ideologies. Some voodoo trappings of the famous Welles/Houseman Negro Theatre stage adaptation are visible, especially in the film's characterization of the Weird Sisters, who create an effigy of Macbeth as a charm to enchant him. Of all Welles's post-"Kane" Hollywood productions, "Macbeth" is stylistically closest to "Citizen Kane" in its long takes and deep focus photography.
Republic initially trumpeted the film as an important work but decided it did not care for the Scottish accents and held up general release for almost a year after early negative press reaction, including "Life"'s comment that Welles's film "doth foully slaughter Shakespeare." Welles left for Europe, while co-producer and lifelong supporter Richard Wilson reworked the soundtrack. Welles returned and cut 20 minutes from the film at Republic's request and recorded narration to cover some gaps. The film was decried as a disaster. "Macbeth" had influential fans in Europe, especially the French poet and filmmaker Jean Cocteau, who hailed the film's "crude, irreverent power" and careful shot design, and described the characters as haunting "the corridors of some dreamlike subway, an abandoned coal mine, and ruined cellars oozing with water."
Europe (1948–1956).
In Italy he starred as Cagliostro in the 1948 film "Black Magic". His co-star, Akim Tamiroff, impressed Welles so much that Tamiroff would appear in four of Welles's productions during the 1950s and 1960s.
The following year, Welles starred as Harry Lime in Carol Reed's "The Third Man", alongside Joseph Cotten, his friend and co-star from "Citizen Kane", with a script by Graham Greene and a memorable score by Anton Karas.
A few years later, British radio producer Harry Alan Towers would resurrect the Lime character in the radio series "The Adventures of Harry Lime".
Welles appeared as Cesare Borgia in the 1949 Italian film "Prince of Foxes", with Tyrone Power and Mercury Theatre alumnus Everett Sloane, and as the Mongol warrior Bayan in the 1950 film version of the novel "The Black Rose" (again with Tyrone Power).
"Othello".
During this time, Welles was channeling his money from acting jobs into a self-financed film version of Shakespeare's play "Othello". From 1949 to 1951, Welles worked on "Othello", filming on location in Europe and Morocco. The film featured Welles's friends, Micheál Mac Liammóir as Iago and Hilton Edwards as Desdemona's father Brabantio. Suzanne Cloutier starred as Desdemona and Campbell Playhouse alumnus Robert Coote appeared as Iago's associate Roderigo.
Filming was suspended several times as Welles ran out of funds and left for acting jobs, accounted in detail in MacLiammóir's published memoir "Put Money in Thy Purse". The American release prints had a technically flawed soundtrack, suffering from a drop-out of sound at every quiet moment. Welles's daughter, Beatrice Welles-Smith, restored "Othello" in 1992 for a wide re-release. The restoration included reconstructing Angelo Francesco Lavagnino's original musical score, which was originally inaudible, and adding ambient stereo sound effects, which were not in the original film. The restoration went on to a successful theatrical run in America.
In 1952, Welles continued finding work in England after the success of the "Harry Lime" radio show. Harry Alan Towers offered Welles another series, "The Black Museum", which ran for 52 weeks with Welles as host and narrator. Director Herbert Wilcox offered Welles the part of the murdered victim in "Trent's Last Case", based on the novel by E. C. Bentley. In 1953, the BBC hired Welles to read an hour of selections from Walt Whitman's epic poem "Song of Myself". Towers hired Welles again, to play Professor Moriarty in the radio series, "The Adventures of Sherlock Holmes", starring John Gielgud and Ralph Richardson.
Welles briefly returned to America to make his first appearance on television, starring in the "Omnibus" presentation of "King Lear", broadcast live on CBS October 18, 1953. Directed by Peter Brook, the production costarred Natasha Parry, Beatrice Straight and Arnold Moss.
In 1954, director George More O'Ferrall offered Welles the title role in the 'Lord Mountdrago' segment of "Three Cases of Murder", co-starring Alan Badel. Herbert Wilcox cast Welles as the antagonist in "Trouble in the Glen" opposite Margaret Lockwood, Forrest Tucker and Victor McLaglen. Old friend John Huston cast him as Father Mapple in his 1956 film adaptation of Herman Melville's "Moby-Dick", starring Gregory Peck.
"Mr. Arkadin".
Welles's next turn as director was the film "Mr. Arkadin" (1955), which was produced by his political mentor from the 1940s, Louis Dolivet. It was filmed in France, Germany, Spain and Italy on a very limited budget. Based loosely on several episodes of the Harry Lime radio show, it stars Welles as a billionaire who hires a man to delve into the secrets of his past. The film stars Robert Arden, who had worked on the Harry Lime series; Welles's third wife, Paola Mori, whose voice was dubbed by actress Billie Whitelaw; and guest stars Akim Tamiroff, Michael Redgrave, Katina Paxinou and Mischa Auer. Frustrated by his slow progress in the editing room, producer Dolivet removed Welles from the project and finished the film without him. Eventually five different versions of the film would be released, two in Spanish and three in English. The version that Dolivet completed was retitled "Confidential Report". In 2005 Stefan Droessler of the Munich Film Museum oversaw a reconstruction of the surviving film elements.
In 1955, Welles also directed two television series for the BBC. The first was "Orson Welles' Sketch Book", a series of six 15-minute shows featuring Welles drawing in a sketchbook to illustrate his reminiscences for the camera (including such topics as the filming of "It's All True" and the Isaac Woodard case), and the second was "Around the World with Orson Welles", a series of six travelogues set in different locations around Europe (such as Venice, the Basque Country between France and Spain, and England). Welles served as host and interviewer, his commentary including documentary facts and his own personal observations (a technique he would continue to explore in later works).
In 1956, Welles completed "Portrait of Gina". The film cans would remain in a lost-and-found locker at the hotel for several decades, where they were discovered after Welles's death.
Return to Hollywood (1956–1959).
In 1956, Welles returned to Hollywood.
He began filming a projected pilot for Desilu, owned by Lucille Ball and her husband Desi Arnaz, who had recently purchased the former RKO studios. The film was "The Fountain of Youth", based on a story by John Collier. Originally deemed not viable as a pilot, the film was not aired until 1958 — and won the Peabody Award for excellence.
Welles guest starred on television shows including "I Love Lucy". On radio, he was narrator of "Tomorrow" (October 17, 1956), a nuclear holocaust drama produced and syndicated by ABC and the Federal Civil Defense Administration.
Welles's next feature film role was in "Man in the Shadow" for Universal Pictures in 1957, starring Jeff Chandler.
"Touch of Evil".
Welles stayed on at Universal to direct (and co-star with) Charlton Heston in the 1958 film "Touch of Evil", based on Whit Masterson's novel "Badge of Evil". Originally only hired as an actor, Welles was promoted to director by Universal Studios at the insistence of Charlton Heston.:154 The film reunited many actors and technicians with whom Welles had worked in Hollywood in the 1940s, including cameraman Russell Metty ("The Stranger"), makeup artist Maurice Seiderman ("Citizen Kane"), and actors Joseph Cotten, Marlene Dietrich and Akim Tamiroff. Filming proceeded smoothly, with Welles finishing on schedule and on budget, and the studio bosses praising the daily rushes. Nevertheless, after the end of production, the studio re-edited the film, re-shot scenes, and shot new exposition scenes to clarify the plot.:175–176 Welles wrote a 58-page memo outlining suggestions and objections, stating that the film was no longer his version—it was the studio's, but as such, he was still prepared to help with it.:175–176
In 1978, a longer preview version of the film was discovered and released.
As Universal reworked "Touch of Evil", Welles began filming his adaptation of Miguel de Cervantes' novel "Don Quixote" in Mexico, starring Mischa Auer as Quixote and Akim Tamiroff as Sancho Panza.
Return to Europe (1959–1970).
He continued shooting "Don Quixote" in Spain and Italy, but replaced Mischa Auer with Francisco Reiguera, and resumed acting jobs.
In Italy in 1959, Welles directed his own scenes as King Saul in Richard Pottier's film "David and Goliath". In Hong Kong he co-starred with Curt Jürgens in Lewis Gilbert's film "Ferry to Hong Kong". In 1960, in Paris he co-starred in Richard Fleischer's film "Crack in the Mirror". In Yugoslavia he starred in Richard Thorpe's film "The Tartars" and Veljko Bulajić's "Battle of Neretva".
Throughout the 1960s, filming continued on "Quixote" on-and-off until the decade, as Welles evolved the concept, tone and ending several times. Although he had a complete version of the film shot and edited at least once, he would continue toying with the editing well into the 1980s, he never completed a version film he was fully satisfied with, and would junk existing footage and shoot new footage. (In one case, he had a complete cut ready in which Quixote and Sancho Panza end up going to the moon, but he felt the ending was rendered obsolete by the 1969 moon landings, and burned 10 reels of this version.) As the process went on, Welles gradually voiced all of the characters himself and provided narration. In 1992, the director Jesús Franco constructed a film out of the portions of "Quixote" left behind by Welles. Some of the film stock had decayed badly. While the Welles footage was greeted with interest, the post-production by Franco was met with harsh criticism.
In 1961, Welles directed "In the Land of Don Quixote", a series of eight half-hour episodes for the Italian television network RAI. Similar to the "Around the World with Orson Welles" series, they presented travelogues of Spain and included Welles's wife, Paola, and their daughter, Beatrice. Though Welles was fluent in Italian, the network was not interested in him providing Italian narration because of his accent, and the series sat unreleased until 1964, by which time the network had added Italian narration of its own. Ultimately, versions of the episodes were released with the original musical score Welles had approved, but without the narration.
"The Trial".
In 1962, Welles directed his adaptation of "The Trial", based on the novel by Franz Kafka and produced by Alexander Salkind and Michael Salkind. The cast included Anthony Perkins as Josef K, Jeanne Moreau, Romy Schneider, Paola Mori and Akim Tamiroff. While filming exteriors in Zagreb, Welles was informed that the Salkinds had run out of money, meaning that there could be no set construction. No stranger to shooting on found locations, Welles soon filmed the interiors in the Gare d'Orsay, at that time an abandoned railway station in Paris. Welles thought the location possessed a "Jules Verne modernism" and a melancholy sense of "waiting", both suitable for Kafka. The film failed at the box-office. Peter Bogdanovich would later observe that Welles found the film riotously funny. During the filming, Welles met Oja Kodar, who would later become his muse, star and mistress for the last twenty years of his life. Welles also stated in an interview with the BBC that it was his best film.
Welles played a film director in "La Ricotta" (1963)—Pier Paolo Pasolini's segment of the "Ro.Go.Pa.G." movie, although his renowned voice was dubbed by Italian writer Giorgio Bassani.:516 He continued taking what work he could find acting, narrating or hosting other people's work, and began filming "Chimes at Midnight", which was completed in 1966. Filmed in Spain, it was a condensation of five Shakespeare plays, telling the story of Falstaff and his relationship with Prince Hal. The cast included Keith Baxter, John Gielgud, Jeanne Moreau, Fernando Rey and Margaret Rutherford, with narration by Ralph Richardson. Music was again by Angelo Francesco Lavagnino. Jess Franco served as second unit director.
"Chimes at Midnight".
"Chimes at Midnight" was based on Welles's play "Five Kings" which condensed five of Shakespeare's plays into one show in order to focus on the story of Falstaff. Welles produced the show in New York in 1939 but the opening night, where part 1 was acted, was a disaster and part 2 was never put on. He revamped the show and revisited it in 1960 at the Gate Theatre in Dublin. But again, it was not successful. However, this later production was used as the base for the movie. The script contained text from five plays: primarily "Henry IV, Part 1" and "Henry IV, Part 2", but also "Richard II", "Henry V" and "The Merry Wives of Windsor". Keith Baxter played Prince Hal, and internationally respected Shakespearean interpreter, John Gielgud, played the King, Henry IV. The film's narration, spoken by Ralph Richardson, is taken from the chronicler Raphael Holinshed. According to Jeanne Moreau, Welles delayed filming for two weeks due to stage fright. Welles held this film in high regard and considered it, along with "The Trial", his best work. As he remarked in 1982, "If I wanted to get into heaven on the basis of one movie, that's the one I'd offer up."
In 1966, Welles directed a film for French television, an adaptation of "The Immortal Story", by Karen Blixen. Released in 1968, it stars Jeanne Moreau, Roger Coggio and Norman Eshley. The film had a successful run in French theaters. At this time Welles met Oja Kodar again, and gave her a letter he had written to her and had been keeping for four years; they would not be parted again. They immediately began a collaboration both personal and professional. The first of these was an adaptation of Blixen's "The Heroine", meant to be a companion piece to "The Immortal Story" and starring Kodar. Unfortunately, funding disappeared after one day's shooting. After completing this film, he appeared in a brief cameo as Cardinal Wolsey in Fred Zinnemann's adaptation of "A Man for All Seasons"—a role for which he won considerable acclaim.
In 1967, Welles began directing "The Deep", based on the novel "Dead Calm" by Charles Williams and filmed off the shore of Yugoslavia. The cast included Jeanne Moreau, Laurence Harvey and Kodar. Personally financed by Welles and Kodar, they could not obtain the funds to complete the project, and it was abandoned a few years later after the death of Harvey. The surviving footage was eventually edited and released by the Filmmuseum München. In 1968 Welles began filming a TV special for CBS under the title "Orson's Bag", combining travelogue, comedy skits and a condensation of Shakespeare's play "The Merchant of Venice" with Welles as Shylock. Funding for the show sent by CBS to Welles in Switzerland was seized by the IRS. Without funding, the show was not completed. The surviving film clips portions were eventually released by the Filmmuseum München.
In 1969, Welles authorized the use of his name for a cinema in Cambridge, Massachusetts. The Orson Welles Cinema remained in operation until 1986, with Welles making a personal appearance there in 1977. Also in 1969 he played a supporting role in John Huston's "The Kremlin Letter". Drawn by the numerous offers he received to work in television and films, and upset by a tabloid scandal reporting his affair with Kodar, Welles abandoned the editing of "Don Quixote" and moved back to America in 1970.
Later career (1970–1985).
Welles returned to Hollywood, where he continued to self-finance his film and television projects. While offers to act, narrate and host continued, Welles also found himself in great demand on television talk shows. He made frequent appearances for Dick Cavett, Johnny Carson, Dean Martin and Merv Griffin.
Welles's primary focus during his final years was "The Other Side of the Wind", an unfinished project that was filmed intermittently between 1970 and 1976. Written by Welles, it is the story of an aging film director (John Huston) looking for funds to complete his final film. The cast includes Peter Bogdanovich, Susan Strasberg, Norman Foster, Edmond O'Brien, Cameron Mitchell and Dennis Hopper. Financed by Iranian backers, ownership of the film fell into a legal quagmire after the Shah of Iran was deposed. While there have been several reports of all the legal disputes concerning ownership of the film being settled, enough disputes still exist to prevent its release.
Welles portrayed Louis XVIII of France in the 1970 film "Waterloo", and narrated the beginning and ending scenes of the historical comedy "Start the Revolution Without Me" (1970).
In 1971, Welles directed a short adaptation of "Moby-Dick", a one-man performance on a bare stage, reminiscent of his 1955 stage production "Moby Dick—Rehearsed". Never completed, it was eventually released by the Filmmuseum München. He also appeared in "Ten Days' Wonder", co-starring with Anthony Perkins and directed by Claude Chabrol, based on a detective novel by Ellery Queen. That same year, the Academy of Motion Picture Arts and Sciences gave him an honorary award "For superlative artistry and versatility in the creation of motion pictures". Welles pretended to be out of town and sent John Huston to claim the award, thanking the Academy on film. Huston criticized the Academy for awarding Welles, even while they refused to give Welles any work.
In 1972, Welles acted as on-screen narrator for the film documentary version of Alvin Toffler's 1970 book "Future Shock". Working again for a British producer, Welles played Long John Silver in director John Hough's "Treasure Island" (1972), an adaptation of the Robert Louis Stevenson novel, which had been the second story broadcast by "The Mercury Theatre on the Air" in 1938. This was the last time he played the lead role in a major film. Welles also contributed to the script, his writing credit was attributed to the pseudonym 'O. W. Jeeves'. Some of Welles' original recorded dialog was redubbed by Robert Rietty.
In 1973, Welles completed "F for Fake", a personal essay film about art forger Elmyr de Hory and the biographer Clifford Irving. Based on an existing documentary by François Reichenbach, it included new material with Oja Kodar, Joseph Cotten, Paul Stewart and William Alland. An excerpt of Welles's 1930s "War of the Worlds" broadcast was recreated for this film; however, none of the dialogue heard in the film actually matches what was originally broadcast. Welles filmed a five-minute trailer, rejected in the U.S., that featured several shots of a topless Kodar.
Welles hosted and narrated a syndicated anthology series, "Orson Welles's Great Mysteries," over the 1973–1974 television season. It did not last beyond that season; however, the program could be perceived as a television revival of the Mercury Theatre whose executive producer Welles had been in the 1930s and 1940s. The year 1974 also saw Welles lending his voice for that year's remake of Agatha Christie's classic thriller "Ten Little Indians" produced by his former associate, Harry Alan Towers and starring an international cast that included Oliver Reed, Elke Sommer and Herbert Lom.
In 1975, Welles narrated the documentary "", focusing on Warner Bros. cartoons from the 1940s. Also in 1975, the American Film Institute presented Welles with its third Lifetime Achievement Award (the first two going to director John Ford and actor James Cagney). At the ceremony, Welles screened two scenes from the nearly finished "The Other Side of the Wind".
In 1976, Paramount Television purchased the rights for the entire set of Rex Stout's Nero Wolfe stories for Orson Welles. Welles had once wanted to make a series of Nero Wolfe movies, but Rex Stout – who was leery of Hollywood adaptations during his lifetime after two disappointing 1930s films – turned him down. Paramount planned to begin with an ABC-TV movie and hoped to persuade Welles to continue the role in a mini-series. Frank D. Gilroy was signed to write the television script and direct the TV movie on the assurance that Welles would star, but by April 1977 Welles had bowed out. In 1980 the Associated Press reported "the distinct possibility" that Welles would star in a Nero Wolfe TV series for NBC television. Again, Welles bowed out of the project due to creative differences and William Conrad was cast in the role.:87–88
In 1979, Welles completed his documentary "Filming Othello", which featured Michael MacLiammoir and Hilton Edwards. Made for West German television, it was also released in theaters. That same year, Welles completed his self-produced pilot for "The Orson Welles Show" television series, featuring interviews with Burt Reynolds, Jim Henson and Frank Oz and guest-starring The Muppets and Angie Dickinson. Unable to find network interest, the pilot was never broadcast. Also in 1979, Welles appeared in the biopic "The Secret of Nikola Tesla", and a cameo in "The Muppet Movie" as Lew Lord.
Beginning in the late 1970s, Welles participated in a series of famous television commercial advertisements. For two years he was on-camera spokesman for the Paul Masson Vineyards, and sales grew by one third during the time Welles intoned what became a popular catchphrase: "We will sell no wine before its time." He was also the voice behind the long-running Carlsberg "Probably the best lager in the world" campaign, promoted Domecq sherry on British television and provided narration on adverts for Findus, though the actual adverts have been overshadowed by a famous blooper reel of voice recordings, known as the Frozen Peas reel. He also did commercials for the Preview Subscription Television Service seen on stations around the country including WCLQ/Cleveland, KNDL/St. Louis and WSMW/Boston.
In 1981, Welles hosted the documentary "The Man Who Saw Tomorrow", about Renaissance-era prophet Nostradamus. In 1982, the BBC broadcast "The Orson Welles Story" in the "Arena" series. Interviewed by Leslie Megahey, Welles examined his past in great detail, and several people from his professional past were interviewed as well. It was reissued in 1990 as "With Orson Welles: Stories of a Life in Film". Welles provided narration for the tracks "Defender" from Manowar's 1987 album "Fighting the World" and "Dark Avenger" on their 1982 album, "Battle Hymns". His name was misspelled on the latter album, as he was credited as "Orson Wells".
During the 1980s, Welles worked on such film projects as "The Dreamers", based on two stories by Isak Dinesen and starring Oja Kodar, and "Orson Welles' Magic Show", which reused material from his failed TV pilot. Another project he worked on was "Filming The Trial", the second in a proposed series of documentaries examining his feature films. While much was shot for these projects, none of them was completed. All of them were eventually released by the Filmmuseum München.
In 1984, Welles narrated the short-lived television series "Scene of the Crime". During the early years of "Magnum, P.I.", Welles was the voice of the unseen character Robin Masters, a famous writer and playboy. Welles's death forced this minor character to largely be written out of the series. In an oblique homage to Welles, the "Magnum, P.I." producers ambiguously concluded that story arc by having one character accuse another of having hired an actor to portray Robin Masters. He also, in this penultimate year released a music single, titled "I Know What It Is To Be Young (But You Don't Know What It Is To Be Old)", which he recorded under Italian label Compagnia Generale del Disco. The song was performed with the Nick Perito Orchestra and the Ray Charles Singers and produced by Jerry Abbott who was father to famed metal guitarist Dimebag Darrell.
The last film roles before Welles's death included voice work in the animated films "The Enchanted Journey" (1984) and "" (1986), in which he played the planet-eating robot Unicron. His last film appearance was in Henry Jaglom's 1987 independent film "Someone to Love", released after his death but produced before his voice-over in "Transformers: The Movie". His last television appearance was on the television show "Moonlighting". He recorded an introduction to an episode entitled "The Dream Sequence Always Rings Twice", which was partially filmed in black and white. The episode aired five days after his death and was dedicated to his memory.
In the mid-1980s, Henry Jaglom taped lunch conversations with Welles at Los Angeles's Ma Maison as well as in New York. Edited transcripts of these sessions appear in Peter Biskind's 2013 book "My Lunches With Orson: Conversations Between Henry Jaglom and Orson Welles".
Personal life.
Relationships and family.
Orson Welles and Chicago-born actress and socialite Virginia Nicolson (1916–1996) were married November 14, 1934.:332 The couple divorced February 1, 1940.
Welles fell in love with Mexican actress Dolores del Río, ten years his senior, with whom he was involved between 1938 and 1942. They acted together in the movie "Journey into Fear" (1943) but the affair ended soon after filming ended. Rebecca Welles, the daughter of Welles and Hayworth, met Del Rio in 1954 and said, "My father considered her the great love of his life … She was a living legend in the history of my family".
Welles married Rita Hayworth in 1943. The couple became estranged by 1946 – Welles blamed Hayworth for making unfounded accusations of infidelity, and after he was turned out of the marital bed he then actually started to have affairs, which in turn prompted Hayworth to have affairs of her own. They briefly reconciled in 1947 during the making of "The Lady from Shanghai", before finally separating. They were divorced November 10, 1947.:142 During his last interview, recorded for "The Merv Griffin Show" the evening before his death, Welles called Hayworth "one of the dearest and sweetest women that ever lived … and we were a long time together — I was lucky enough to have been with her longer than any of the other men in her life."
In 1955, Welles married actress Paola Mori (née Countess Paola di Girifalco), an Italian aristocrat who starred as Raina Arkadin in his 1955 film, "Mr. Arkadin". The couple had embarked on a passionate affair, and after she became pregnant they were married at her parents' insistence.:168 They were wed in London May 8, 1955,:417, 419 and never divorced.
Croatian-born actress Oja Kodar became Welles's longtime companion both personally and professionally from 1966 onwards, and they lived together for some of the last 19 years of his life. They first met in Zagreb in 1962, while Welles was filming "The Trial", and embarked on a passionate, short-lived affair which ended when Paola Mori had a cancer scare and Welles returned to his wife. Kodar assumed Welles had left for good, and Welles hired a private detective to track down Kodar, to no avail. Three years passed, and Kodar was by then living in Paris and in a relationship with a struggling young actor. When they saw a press feature that Welles was in Paris, the young actor persuaded a reluctant Kodar to use her influence with Welles to get him a job. When she telephoned him, Welles immediately rushed to her hotel room, broke down the door, and pulled out a small metal box from his jacket. It contained a love letter to her.
With the passing years, Welles's domestic arrangements became more complicated. From 1966 he always maintained at least two separate homes, one with Kodar, the other with Mori and their daughter Beatrice. In the 1960s and 1970s, he shared houses just outside Paris and Madrid with Kodar. Although British tabloids reported his affair with Kodar as early as 1969 (which was a factor in his moving permanently to the United States in 1970), both Mori and Beatrice remained oblivious as to Kodar's existence until 1984. Welles set up a home with Mori and Beatrice in the United States (first in Sedona, then in Las Vegas), ostensibly because the climate would be good for his asthma. But while they lived in Las Vegas, he spent most of his time in Los Angeles, where he openly shared a house with Kodar.
This situation had serious ramifications for the copyright status of his work after his death. Welles left Kodar his Los Angeles home and the rights to his unfinished films, and turned the rest over to Mori. Mori contended that she should have been left everything, and a year after Welles's death, Mori and Kodar finally agreed on the settlement of his will. On the way to their meeting to sign the papers, however, Mori was killed in a car accident in August 1986. Mori's half of the estate was inherited by Beatrice, who refused to come to an arrangement with Kodar, who she blames for undermining her parents' marriage.
Welles had three daughters from his marriages: Christopher Welles Feder (born March 27, 1938, with Virginia Nicolson); Rebecca Welles Manning (December 17, 1944 – October 17, 2004, with Rita Hayworth); and Beatrice Welles (born November 13, 1955, with Paola Mori). His only known son, British director Michael Lindsay-Hogg (Sir Michael Lindsay-Hogg, 5th baronet, born May 5, 1940), is from Welles's affair with Irish actress Geraldine Fitzgerald, then the wife of Sir Edward Lindsay-Hogg, 4th baronet. Although Hogg knew Welles sporadically and occasionally worked as his assistant, and had long been rumoured to be his son given their strong physical resemblance, he refused to believe such rumours until it was confirmed to him by Gloria Vanderbilt in 2010. In her autobiography, "In My Father's Shadow", Feder wrote about being a childhood friend and neighbor of Lindsay-Hogg's and always suspecting he might be her half-brother.
After the death of Rebecca Welles Manning, a man named Marc McKerrow was revealed to be her biological son, and therefore the direct descendant of Orson Welles and Rita Hayworth. McKerrow's reactions to the revelation and his meeting with Oja Kodar are documented in the 2008 film "Prodigal Sons". McKerrow died June 18, 2010.
Despite an urban legend promoted by Welles himself, he was not related to Abraham Lincoln's wartime Secretary of the Navy, Gideon Welles. The myth dates back to the first newspaper feature ever written about Welles — "Cartoonist, Actor, Poet and only 10" — in the February 19, 1926, issue of "The Capital Times". The article falsely states that he was descended from "Gideon Welles, who was a member of President Lincoln's cabinet".:47–48:311 As presented by Charles Higham in a genealogical chart that introduces his 1985 biography of Welles, Orson Welles's father was Richard Head Welles (born Wells), son of Richard Jones Wells, son of Henry Hill Wells (who had an uncle named Gideon "Wells"), son of William Hill Wells, son of Richard Wells (1734–1801).
Physical characteristics.
In his 1956 biography, Peter Noble describes Welles as "a magnificent figure of a man, over six feet tall, handsome, with flashing eyes and a gloriously resonant speaking-voice".:19 Welles said that a voice specialist once told him he was born to be a heldentenor, a heroic tenor, but that when he was young and working at the Dublin Gate Theatre he forced his voice down into a bass-baritone.:144
"Never robust, even as a baby Welles was given to ill health", wrote biographer Frank Brady, who notes that from infancy Welles suffered from asthma, sinus headaches and back pain, with bouts of diphtheria, measles, whooping cough and malaria. "As he grew older," Brady wrote, "his ill health was exacerbated by the late hours he was allowed to keep [and] an early penchant for alcohol and tobacco".:8
In 1928, at age 13, Welles was already more than six feet tall and weighed over 180 pounds.:50 He reached a height of six feet three and a half inches,:242 but biographer Simon Callow notes a loss of height detailed in a medical examination Welles had April 24, 1941, after the physical trials of making "Citizen Kane". Welles complained of "attacks of knife-like pain behind the sternal notch with sensations of smothering":
The physical examination (which records his height as 72 inches, three and a half inches shorter than his usual reported height, and his weight as 218 lbs: 15.5 stones) further reveals scoliosis of the spine, and spina bifida occulta. 'These congenital anomalies of the spine give rise to backache resulting from trauma.' In addition he has 'a very marked degree of pes planus [flat foot: everted] which accounts for the great amount of foot and ankle trouble … There is nothing very serious with the heart action but you cannot afford to abuse that organ because of a tendency to be susceptible to damage.' It must have been a relief to discover that, despite a vast alcoholic intake, coupled with regular infusions of benzedrine and amphetamines, the sorely abused organ in question was holding up so well.:560
"Crash diets, drugs, and corsets had slimmed him for his early film roles," wrote biographer Barton Whaley. "Then always back to
gargantuan consumption of high-caloric food and booze. By summer 1949, when he was 34, his weight had crept up to a stout 230 pounds. In 1953 he ballooned from 250 to 275 pounds. After 1960 he remained permanently obese.":329
His obesity was severe to the point that it restricted his ability to travel, aggravated other health conditions, including his asthma, and even required him to go on a diet in order to play the famously portly character Sir John Falstaff.
Religious beliefs.
When Peter Bogdanovich once asked him about his religion, Orson Welles gruffly replied that it was none of his business, then misinformed him that he was raised Catholic.:xxx:12
Although the Welles family was no longer devout, it was fourth-generation Protestant Episcopalian and, before that, Quaker and Puritan.:12 Welles's earliest paternal forebear in America, Richard Wells, was a leader of the Quaker community in Pennsylvania. His earliest maternal ancestor in America was John Alden, a crew member on the Pilgrim ship "Mayflower".:5
The funeral of Welles's father Richard H. Welles was Episcopalian.:12
In April 1982, when interviewer Merv Griffin asked him about his religious beliefs, Welles replied, "I try to be a Christian. I don't pray really, because I don't want to bore God.":576 Near the end of his life Welles was dining at Ma Maison, his favorite restaurant in Los Angeles, when proprietor Patrick Terrail conveyed an invitation from the head of the Greek Orthodox Church, who asked Welles to be his guest of honor at divine liturgy at Saint Sophia Cathedral. Welles replied, "Please tell him I really appreciate that offer, but I am an atheist.":104–105
"Orson never joked or teased about the religious beliefs of others," wrote biographer Barton Whaley. "He accepted it as a cultural artifact, suitable for the births, deaths, and marriages of strangers and even some friends — but without emotional or intellectual meaning for himself.":12
Politics.
Welles was politically active from the beginning of his career. He remained aligned with the left throughout his life, and always defined his political orientation as "progressive". He was a strong supporter of Franklin D. Roosevelt and the New Deal, and often spoke out on radio in support of progressive politics. He campaigned heavily for Roosevelt in the 1944 election.
For several years, he wrote a newspaper column on political issues and considered running for the U.S. Senate in 1946, representing his home state of Wisconsin (a seat that was ultimately won by Joseph McCarthy).
In 1970, Welles narrated (but did not write) a satirical political record on the administration of President Richard Nixon titled "The Begatting of the President".
He was also an early and outspoken critic of American racism and the practice of segregation.
Death and tributes.
On the evening of October 9, 1985, Welles recorded his final interview on the syndicated TV program, "The Merv Griffin Show", appearing with biographer Barbara Leaming. "Both Welles and Leaming talked of Welles's life and the segment was a nostalgic interlude," wrote biographer Frank Brady.:590–591 Welles returned to his house in Hollywood and worked into the early hours typing stage directions for the project he and Gary Graver were planning to shoot at UCLA the following day. Welles died sometime on the morning of October 10, following a heart attack.:453 He was found by his chauffeur at around 10 a.m.; the first of Welles's friends to arrive was Paul Stewart.:295–297
Welles was cremated by prior agreement with the executor of his estate, Greg Garrison.:592 A successful television producer, Garrison had encouraged Welles to make guest appearances on TV in the 1970s, which proved so lucrative that Welles was able to pay off a portion of the taxes he owed the IRS.:549–550 Garrison and Welles were first acquainted in 1946, during the Broadway production of "Around the World", for which Garrison was a stagehand.:470–471
A brief private funeral took place at Cunningham and O'Connor in Hollywood, the mortuary where the visitation for John Ford had taken place. The service was attended by Paola Mori and Welles's three daughters — the first time they had ever been together. Only a few close friends were invited: Garrison, Graver, Roger Hill:298 and Prince Alessandro Tasca di Cuto. Chris Welles Feder later described the funeral as an awful experience.:1–9
Within days of Welles's death Richard Wilson and other friends began to organize a public memorial tribute:593 which took place November 2, 1985, at the Directors Guild of America Theater in Los Angeles. Host Peter Bogdanovich introduced speakers including Charles Champlin, Geraldine Fitzgerald, Greg Garrison, Charlton Heston, Roger Hill, Henry Jaglom, Arthur Knight, Oja Kodar, Barbara Leaming, Janet Leigh, Norman Lloyd, Dan O'Herlihy, Patrick Terrail and Robert Wise.:594:299–300
In 1987 the cremated remains of Welles and Mori (killed in a 1986 car crash) were taken to Ronda, Spain, and buried in an old well covered by flowers on the rural estate of a longtime friend, retired bullfighter Antonio Ordóñez.:298–299
Unfinished projects.
Welles's reliance on self-production meant that many of his later projects were filmed piecemeal or were not completed. Welles financed his later projects through his own fundraising activities. He often also took on other work to obtain money to fund his own films.
"Don Quixote".
In the mid-1950s, Welles began work on "Don Quixote", initially a commission from CBS television. Welles expanded the film to feature length, developing the screenplay to take Quixote and Sancho Panza into the modern age. Filming stopped with the death of Francisco Reiguera, the actor playing Quixote, in 1969. Orson Welles continued editing the film into the early 1970s. At the time of his death, the film remained largely a collection of footage in various states of editing. The project and more importantly Welles's conception of the project changed radically over time. A version of the film was created from available fragments in 1992 and released to a very negative reception.
A version Oja Kodar supervised, with help from Jess Franco, assistant director during production, was released in 2008 to mixed reactions.
"The Merchant of Venice".
In 1969, Welles was given another TV commission to film a condensed adaptation of "The Merchant of Venice".:XXXIV Although Welles had actually completed the film by 1970 the finished negative was later mysteriously stolen from his Rome production office.:234
"The Other Side of the Wind".
In 1970, Welles began shooting "The Other Side of the Wind". The film relates the efforts of a film director (played by John Huston) to complete his last Hollywood picture and is largely set at a lavish party. By 1972 the filming was reported by Welles as being "96% complete",:546 though it is likely that Welles had only edited about 40 minutes of the film by 1979.:320 In that year, legal complications over the ownership of the film forced the negative into a Paris vault. In 2004 director Peter Bogdanovich, who acted in the film, announced his intention to complete the production. As of 2009, legal complications over the Welles estate had kept the film from being finished or released.
On October 28, 2014, the Los Angeles-based production company Royal Road Entertainment announced that it had negotiated an agreement, with the assistance of producer Frank Marshall, and would purchase the rights to complete and release "The Other Side of the Wind". Bogdanovich and Marshall will complete Welles's nearly finished film in Los Angeles, aiming to have it ready for screening May 6, 2015 — the 100th anniversary of Welles's birth. Royal Road Entertainment and German producer Jens Koethner Kaul acquired the rights held by Les Films de l'Astrophore and the late Mehdi Boushehri. They reached an agreement with Oja Kodar, who inherited Welles's ownership of the film, and Beatrice Welles, manager of the Welles estate.
Some footage is included in the documentaries "Working with Orson Welles" (1993) and "Orson Welles: One Man Band" (1995).

</doc>
<doc id="22197" url="http://en.wikipedia.org/wiki?curid=22197" title="Open content">
Open content

Open content is a neologism coined by David Wiley in 1998 which describes a creative work that others can copy or modify. The term evokes open source software, which is a related concept in software. 
When the term OpenContent was first used by Wiley, it described works licensed under the Open Content License (a non-free share-alike license, see 'Free content' below) and perhaps other works licensed under similar terms. It has since come to describe a broader class of content without conventional copyright restrictions. The openness of content can be assessed under the '5Rs Framework' based on the extent to which it can be reused, revised, remixed and redistributed by members of the public without violating copyright law. Unlike open source and free content, there is no clear threshold that a work must reach to qualify as 'open content'.
Although open content has been described as a counterbalance to copyright, open content licenses rely on a copyright holder's power to license their work.
Definition.
The OpenContent website once defined OpenContent as 'freely available for modification, use and redistribution under a license similar to those used by the Open Source / Free Software community'. However, such a definition would exclude the Open Content License (OPL) because that license forbade charging 'a fee for the [OpenContent] itself', a right required by free and open source software licenses.
The term since shifted in meaning. OpenContent "is licensed in a manner that provides users with free and perpetual permission to engage in the 5R activities."
The 5Rs are put forward on the OpenContent website as a framework for assessing the extent to which content is open:
This broader definition distinguishes open content from open source software, since the latter must be available for commercial use by the public. However, it is similar to several definitions for open educational resources, which include resources under noncommercial and verbatim licenses.
The Open Definition, which purports to define open content and open knowledge, draws heavily on the Open Source Definition; it preserves the limited sense of open content as libre content.
Open access.
"Open access" refers to toll-free or gratis access to content, consisting mainly of published peer-reviewed scholarly journal articles. Some open access works are also licensed for reuse and redistribution, which would qualify them as open content.
Open content and education.
Over the past decade, open content has been used to develop alternative routes towards higher education. Traditional universities are expensive, and their tuition rates are increasing.
 Open content allows a free way of obtaining higher education that is "focused on collective knowledge and the sharing and reuse of learning and scholarly content."
There are multiple projects and organizations that promote learning through open content, including OpenCourseWare Initiative, The Saylor Foundation and Khan Academy. Some universities, like MIT, Yale, and Tufts are making their courses freely available on the internet.
Textbooks.
The textbook industry is one of the educational industries in which open content can make the biggest impact. Traditional textbooks, aside from being expensive can also be inconvenient and out of date, because of publishers' tendency to constantly print new editions. Open textbooks help to eliminate this problem, because they are online and thus easily updatable. Being openly licensed and online can be helpful to teachers, because it allows the textbook to be modified according to the teacher's unique curriculum. There are multiple organizations promoting the creation of openly licensed textbooks. Some of these organizations and projects include , Connexions, OpenStax College, and Wikibooks
For more information on open content as it relates to education and textbooks see Open Education Resources.
Licenses.
According to the current definition of open content on the OpenContent website, any general, royalty-free copyright license would qualify as an open license because it 'provides users with the right to make more kinds of uses than those normally permitted under the law. These permissions are granted to users free of charge.'
However, the narrower definition used in the Open Definition effectively limits open content to libre content; any free content license would qualify as an open content license. According to this narrower criteria, the following still-maintained licenses qualify:
External links.
 at DMOZ

</doc>
<doc id="22199" url="http://en.wikipedia.org/wiki?curid=22199" title="Ohio">
Ohio

Ohio is a state in the Midwestern United States. Ohio is the 34th largest (by area), the 7th most populous, and the 10th most densely populated of the 50 United States. The state's capital and largest city is Columbus.
The name "Ohio" originated from Iroquois word "ohi-yo’", meaning "great river" or "large creek". The state, originally partitioned from the Northwest Territory, was admitted to the Union as the 17th state (and the first under the Northwest Ordinance) on March 1, 1803. Although there are conflicting narratives regarding the origin of the nickname, Ohio is historically known as the "Buckeye State" (relating to the Ohio buckeye tree) and Ohioans are also known as "Buckeyes".
The government of Ohio is composed of the executive branch, led by the Governor; the legislative branch, which comprises the Ohio General Assembly; and the judicial branch, which is led by the Supreme Court. Currently, Ohio occupies 16 seats in the United States House of Representatives. Ohio is known for its status as both a swing state and a bellwether in national elections. Owing to its strategic electoral importance, six Presidents of the United States have called Ohio their home state.
Geography.
Ohio's geographic location has proven to be an asset for economic growth and expansion. Because Ohio links the Northeast to the Midwest, much cargo and business traffic passes through its borders along its well-developed highways. Ohio has the nation's 10th largest highway network, and is within a one-day drive of 50% of North America's population and 70% of North America's manufacturing capacity. To the north, Lake Erie gives Ohio 312 mi of coastline, which allows for numerous cargo ports. Ohio's southern border is defined by the Ohio River (with the border being at the 1793 low-water mark on the north side of the river), and much of the northern border is defined by Lake Erie. Ohio's neighbors are Pennsylvania to the east, Michigan to the northwest, Ontario Canada, to the north, Indiana to the west, Kentucky on the south, and West Virginia on the southeast. Ohio's borders were defined by metes and bounds in the Enabling Act of 1802 as follows:
Bounded on the east by the Pennsylvania line, on the south by the Ohio River, to the mouth of the Great Miami River, on the west by the line drawn due north from the mouth of the Great Miami aforesaid, and on the north by an east and west line drawn through the southerly extreme of Lake Michigan, running east after intersecting the due north line aforesaid, from the mouth of the Great Miami until it shall intersect Lake Erie or the territorial line, and thence with the same through Lake Erie to the Pennsylvania line aforesaid.
Ohio is bounded by the Ohio River, but nearly all of the river itself belongs to Kentucky and West Virginia. In 1980, the U.S. Supreme Court held that, based on the wording of the cessation of territory by Virginia (which at that time included what is now Kentucky and West Virginia), the boundary between Ohio and Kentucky (and, by implication, West Virginia) is the northern low-water mark of the river as it existed in 1792. Ohio has only that portion of the river between the river's 1792 low-water mark and the present high-water mark.
The border with Michigan has also changed, as a result of the Toledo War, to angle slightly northeast to the north shore of the mouth of the Maumee River.
Much of Ohio features glaciated plains, with an exceptionally flat area in the northwest being known as the Great Black Swamp. This glaciated region in the northwest and central state is bordered to the east and southeast first by a belt known as the glaciated Allegheny Plateau, and then by another belt known as the unglaciated Allegheny Plateau. Most of Ohio is of low relief, but the unglaciated Allegheny Plateau features rugged hills and forests.
The rugged southeastern quadrant of Ohio, stretching in an outward bow-like arc along the Ohio River from the West Virginia Panhandle to the outskirts of Cincinnati, forms a distinct socio-economic unit. Geologically similar to parts of West Virginia and southwestern Pennsylvania, this area's coal mining legacy, dependence on small pockets of old manufacturing establishments, and distinctive regional dialect set this section off from the rest of the state. In 1965 the United States Congress passed the Appalachian Regional Development Act, at attempt to "address the persistent poverty and growing economic despair of the Appalachian Region." This act defines 29 Ohio counties as part of Appalachia. While 1/3 of Ohio's land mass is part of the federally defined Appalachian region, only 12.8% of Ohioans live there (1.476 million people.)
Significant rivers within the state include the Cuyahoga River, Great Miami River, Maumee River, Muskingum River, and Scioto River. The rivers in the northern part of the state drain into the northern Atlantic Ocean via Lake Erie and the St. Lawrence River, and the rivers in the southern part of the state drain into the Gulf of Mexico via the Ohio River and then the Mississippi.
The worst weather disaster in Ohio history occurred along the Great Miami River in 1913. Known as the Great Dayton Flood, the entire Miami River watershed flooded, including the downtown business district of Dayton. As a result, the Miami Conservancy District was created as the first major flood plain engineering project in Ohio and the United States.
Grand Lake St. Marys in the west central part of the state was constructed as a supply of water for canals in the canal-building era of 1820–1850. For many years this body of water, over 20 sqmi, was the largest artificial lake in the world. It should be noted that were not the economic fiasco that similar efforts were in other states. Some cities, such as Dayton, owe their industrial emergence to location on canals, and as late as 1910 interior canals carried much of the bulk freight of the state.
Climate.
The climate of Ohio is a humid continental climate (Köppen climate classification "Dfa") throughout most of the state except in the extreme southern counties of Ohio's Bluegrass region section which are located on the northern periphery of the humid subtropical climate and Upland South region of the United States. Summers are typically hot and humid throughout the state, while winters generally range from cool to cold. Precipitation in Ohio is moderate year-round. Severe weather is not uncommon in the state, although there are typically fewer tornado reports in Ohio than in states located in what is known as the Tornado Alley. Severe lake effect snowstorms are also not uncommon on the southeast shore of Lake Erie, which is located in an area designated as the Snowbelt.
Although predominantly not in a subtropical climate, some warmer-climate flora and fauna does reach well into Ohio. For instance, a number of trees with more southern ranges, such as the blackjack oak, "Quercus marilandica", are found at their northernmost in Ohio just north of the Ohio River. Also evidencing this climatic transition from a subtropical to continental climate, several plants such as the Southern magnolia "(Magnolia grandiflora)", Albizia julibrissin (mimosa), Crape Myrtle, and even the occasional Needle Palm are hardy landscape materials regularly used as street, yard, and garden plantings in the Bluegrass region of Ohio; but these same plants will simply not thrive in much of the rest of the State. This interesting change may be observed while traveling through Ohio on Interstate 75 from Cincinnati to Toledo; the observant traveler of this diverse state may even catch a glimpse of Cincinnati's common wall lizard, one of the few examples of permanent "subtropical" fauna in Ohio.
Records.
The highest recorded temperature was 113 F, near Gallipolis on July 21, 1934.
The lowest recorded temperature was -39 F, at Milligan on February 10, 1899.
Earthquakes.
Although few have registered as noticeable to the average resident, more than 30 earthquakes occurred in Ohio between 2002 and 2007, and more than 200 quakes with a magnitude of 2.0 or higher have occurred since 1776.
The most substantial known earthquake in Ohio history was the Anna (Shelby County) earthquake, which occurred on March 9, 1937. It was centered in western Ohio, and had a magnitude of 5.4, and was of intensity VIII.
Other significant earthquakes in Ohio include: one of magnitude 4.8 near Lima on September 19, 1884; one of magnitude 4.2 near Portsmouth on May 17, 1901; and one of 5.0 in LeRoy Township in Lake County on January 31, 1986, which continued to trigger 13 aftershocks of magnitude 0.5 to 2.4 for two months.
The most recent earthquake in Ohio of any appreciable magnitude occurred on December 31, 2011, at 3:05pm EST. It had a magnitude of 4.0, and its epicenter was located approximately 4 kilometres northwest of Youngstown (), near the Trumbull/Mahoning county border.
The Ohio Seismic Network (OhioSeis), a group of seismograph stations at several colleges, universities, and other institutions, and coordinated by the Division of Geological Survey of the Ohio Department of Natural Resources, maintains an extensive catalog of Ohio earthquakes from 1776 to the present day, as well as earthquakes located in other states whose effects were felt in Ohio.
Major cities.
Columbus (home of The Ohio State University, Franklin University, Capital University, and Ohio Dominican University) is the capital of Ohio, near the geographic center of the state.
Other Ohio cities functioning as centers of United States metropolitan areas include:
Note: The Cincinnati metropolitan area extends into Kentucky and Indiana, the Steubenville metropolitan area extends into West Virginia, and the Youngstown metropolitan area extends into Pennsylvania.
Ohio cities that function as centers of United States micropolitan areas include:
History.
Native Americans.
Archeological evidence suggests that the Ohio Valley was inhabited by nomadic people as early as 13,000 BC. These early nomads disappeared from Ohio by 1,000 BC, "but their material culture provided a base for those who followed them". Between 1,000 and 800 BC, the sedentary Adena culture emerged. As Ohio historian George W. Knepper notes, this sophisticated culture was "so named because evidences of their culture were excavated in 1902 on the grounds of Adena, Thomas Worthington's estate located near Chillicothe". The Adena were able to establish "semi-permanent" villages because they domesticated plants, which included squash, sunflowers, and perhaps corn. Cultivation of these in addition to hunting and gathering supported more settled, complex villages. The most spectacular remnant of the Adena culture is the Great Serpent Mound, located in Adams County, Ohio.
Around 100 BC, the Adena were joined in Ohio Country by the Hopewell people, who were named for the farm owned by Captain M. C. Hopewell, where evidence of their unique culture was discovered. Like the Adena, the Hopewell people participated in a mound-building culture. Their complex, large and technologically sophisticated earthworks can be found in modern-day Marietta, Newark, and Circleville. The Hopewell, however, disappeared from the Ohio Valley in about 600 AD. Little is known about the people who replaced them. Researchers have identified two additional, distinct prehistoric cultures: the Fort Ancient people and the Whittlesey Focus people. Both cultures apparently disappeared in the 17th century, perhaps decimated by infectious diseases spread in epidemics from early European contact. The Native Americans had no immunity to common European diseases. Some scholars believe that the Fort Ancient people "were ancestors of the historic Shawnee people, or that, at the very least, the historic Shawnees absorbed remnants of these older peoples."
American Indians in the Ohio Valley were greatly affected by the aggressive tactics of the Iroquois Confederation, based in central and western New York. After the so-called Beaver Wars in the mid-17th century, the Iroquois claimed much of the Ohio country as hunting and, more importantly, beaver-trapping ground. After the devastation of epidemics and war in the mid-17th century, which largely emptied the Ohio country of indigenous people by the mid-to-late 17th century, the land gradually became repopulated by the mostly Algonquian-speaking descendants of its ancient inhabitants, that is, descendants of the Adena, Hopewell, and Mississippian cultures. Many of these Ohio-country nations were multi-ethnic (sometimes multi-linguistic) societies born out of the earlier devastation brought about by disease, war, and subsequent social instability. They subsisted on agriculture (corn, sunflowers, beans, etc.) supplemented by seasonal hunts. By the 18th century, they were part of a larger global economy brought about by European entry into the fur trade.
The indigenous nations to inhabit Ohio in the historical period included the Miamis (a large confederation); Wyandots (made up of refugees, especially from the fractured Huron confederacy); Delawares (pushed west from their historic homeland in New Jersey); Shawnees (also pushed west, although they may have been descended from the Fort Ancient people of Ohio); Ottawas (more commonly associated with the upper Great Lakes region); Mingos (like the Wyandot, a group recently formed of refugees from Iroquois); and Eries (gradually absorbed into the new, multi-ethnic "republics," namely the Wyandot). Ohio country was also the site of Indian massacres, such as the Yellow Creek Massacre, Gnadenhutten and Pontiac's Rebellion school massacre.
Colonial and Revolutionary eras.
During the 18th century, the French set up a system of trading posts to control the fur trade in the region. In 1754, France and Great Britain fought a war that was known in North America as the French and Indian War and in Europe as the Seven Years' War. As a result of the Treaty of Paris, the French ceded control of Ohio and the remainder of the Old Northwest to Great Britain.
Pontiac's Rebellion in the 1760s, however, posed a challenge to British military control. This came to an end with the colonists' victory in the American Revolution. In the Treaty of Paris in 1783, Britain ceded all claims to Ohio country to the United States.
Northwest Territory: 1787–1803.
The United States created the Northwest Territory under the Northwest Ordinance of 1787. Slavery was not permitted in the new territory. Settlement began with the founding of Marietta by the Ohio Company of Associates, which had been formed by a group of American Revolutionary War veterans. Following the Ohio Company, the Miami Company (also referred to as the "Symmes Purchase") claimed the southwestern section, and the Connecticut Land Company surveyed and settled the Connecticut Western Reserve in present-day Northeast Ohio.
The old Northwest Territory originally included areas previously known as Ohio Country and Illinois Country. As Ohio prepared for statehood, the Indiana Territory was created, reducing the Northwest Territory to approximately the size of present-day Ohio plus the eastern half of the Lower Peninsula of Michigan and the eastern tip of the Upper Peninsula.
Under the Northwest Ordinance, areas of the territory could be defined and admitted as states once their population reached 60,000. Although Ohio's population numbered only 45,000 in December 1801, Congress determined that the population was growing rapidly and Ohio could begin the path to statehood. The assumption was that it would exceed 60,000 residents by the time it was admitted as a state. Furthermore, in regards to the Leni Lenape Native Americans living in the region, Congress decided that 10,000 acres on the Muskingum River in the present state of Ohio would "be set apart and the property thereof be vested in the Moravian Brethren . . . or a society of the said Brethren for civilizing the Indians and promoting Christianity."
Statehood: 1803–present.
On February 19, 1803, US President Thomas Jefferson signed an act of Congress that approved Ohio's boundaries and constitution. However, Congress had never passed a resolution formally admitting Ohio as the 17th state. The current custom of Congress declaring an official date of statehood did not begin until 1812, with Louisiana's admission as the 18th state. Although no formal resolution of admission was required, when the oversight was discovered in 1953, Ohio congressman George H. Bender introduced a bill in Congress to admit Ohio to the Union retroactive to March 1, 1803, the date on which the Ohio General Assembly first convened. At a special session at the old state capital in Chillicothe, the Ohio state legislature approved a new petition for statehood that was delivered to Washington, D.C. on horseback. On August 7, 1953 (the year of Ohio's 150th anniversary), President Eisenhower signed a congressional joint resolution that officially declared March 1, 1803 the date of Ohio's admittance into the Union.
Although many Native Americans had migrated west to evade American encroachment, others remained settled in the state, sometimes assimilating in part. In 1830 under President Andrew Jackson, the US government forced Indian Removal of most tribes to the Indian Territory west of the Mississippi River.
In 1835, Ohio fought with Michigan in the Toledo War, a mostly bloodless boundary war over the Toledo Strip. Congress intervened, making Michigan's admittance as a state conditional on ending the conflict. In exchange for giving up its claim to the Toledo Strip, Michigan was given the western two-thirds of the Upper Peninsula, in addition to the eastern third that was already considered part of the state.
Ohio's central position and its population gave it an important place during the Civil War. The Ohio River was a vital artery for troop and supply movements, as were Ohio's railroads. Ohio contributed more soldiers per-capita than any other state in the Union. In 1862, the state's morale was badly shaken in the aftermath of the battle of Shiloh, a costly victory in which Ohio forces suffered 2,000 casualties. Later that year, when Confederate troops under the leadership of Stonewall Jackson threatened Washington, D.C., Ohio governor David Tod still could recruit 5,000 volunteers to provide three months of service. Almost 35,000 Ohioans died in the conflict, and thirty thousand were physically wounded. By the end of the Civil War, the Union's top three generals–Ulysses S. Grant, William Tecumseh Sherman, and Philip Sheridan–were all from Ohio.
In 1912 a Constitutional Convention was held with Charles B. Galbreath as secretary. The result reflected the concerns of the Progressive Era. It introduced the initiative and the referendum. In addition, it allowed the General Assembly to put questions on the ballot for the people to ratify laws and constitutional amendments originating in the Legislature. Under the Jeffersonian principle that laws should be reviewed once a generation, the constitution provided for a recurring question to appear on Ohio's general election ballots every 20 years. The question asks whether a new convention is required. Although the question has appeared in 1932, 1952, 1972, and 1992, it has never been approved. Instead constitutional amendments have been proposed by petition to the legislature hundreds of times and adopted in a majority of cases.
Eight US Presidents hailed from Ohio at the time of their elections, giving rise to its nickname "Mother of Presidents", a sobriquet it shares with Virginia. It is also termed "Modern Mother of Presidents," in contrast to Virginia's status as the origin of presidents earlier in American history. Seven Presidents were born in Ohio, making it second to Virginia's eight. Virginia-born William Henry Harrison lived most of his life in Ohio and is also buried there. Harrison conducted his political career while living on the family compound, founded by his father-in-law, John Cleves Symmes, in North Bend, Ohio. The seven presidents born in Ohio were Ulysses S. Grant, Rutherford B. Hayes, James A. Garfield, Benjamin Harrison (grandson of William Henry Harrison), William McKinley, William Howard Taft and Warren G. Harding.
Demographics.
Population.
From just over 45,000 residents in 1800, Ohio's population grew at rates of over 10% per decade until the 1970 census, which recorded just over 10.65 million Ohioans. Growth then slowed for the next four decades. The United States Census Bureau estimates that the population of Ohio was 11,594,163 on July 1, 2014, a 0.5% increase since the 2010 United States Census. Ohio's population growth lags that of the entire United States, and Caucasians are found in a greater density than the United States average. s of 2000[ [update]], Ohio's center of population is located in Morrow County, in the county seat of Mount Gilead. This is approximately 6346 ft south and west of Ohio's population center in 1990.
As of 2011, 27.6% of Ohio's children under the age of 1 belonged to minority groups.
6.2% of Ohio's population is under 5 years of age, 23.7 percent under 18 years of age, and 14.1 percent were 65 or older. Females made up approximately 51.2 percent of the population.
Racial and ancestry groups.
According to the 2010 United States census, the racial composition of Ohio was the following:
In 2010, there were 469,700 foreign-born residents in Ohio, corresponding to 4.1% of the total population. Of these, 229,049 (2.0%) were naturalized US citizens and 240,699 (2.1%) were not. The largest groups were: Mexico (54,166), India (50,256), China (34,901), Germany (19,219), Philippines (16,410), United Kingdom (15,917), Canada (14,223), Russia (11,763), South Korea (11,307), and Ukraine (10,681).
The largest ancestry groups (which the Census defines as not including racial terms) in the state are:
Ancestries claimed by less than 1% of the population include Sub-Saharan African, Puerto Rican, Swiss, Swedish, Arab, Greek, Norwegian, Romanian, Austrian, Lithuanian, Finnish, West Indian, and Portuguese.
Languages.
About 6.7% of the population age 5 years and over reported speaking a language other than English, with 2.2% of the population speaking Spanish, 2.6% speaking other Indo-European languages, 1.1% speaking Asian and Austronesian languages, and 0.8% speaking other languages. Numerically: 10,100,586 spoke English, 239,229 Spanish, 55,970 German, 38,990 Chinese, 33,125 Arabic, and 32,019 French. In addition 59,881 spoke a Slavic language and 42,673 spoke another West Germanic language according to the 2010 Census. Ohio also had the nation's largest population of Slovene speakers, second largest of Slovak speakers, second largest of Pennsylvania Dutch (German) speakers, and the third largest of Serbian speakers.
Religion.
According to a Pew Forum poll, as of 2008, 76% of Ohioans identified as Christian. Specifically, 26% of Ohio's population identified as Evangelical Protestant, 22% as Mainline Protestant, and 21% as Roman Catholic. 17% of the population is unaffiliated with any religious body. 1.3% (148,380) were Jewish. There are also small minorities of Jehovah's Witnesses (1%), Muslims (1%), Hindus (<0.5%), Buddhists (<0.5%), Mormons (<0.5%), and other faiths (1-1.5%).
According to the Association of Religion Data Archives (ARDA), in 2010 the largest denominations by adherents were the Roman Catholic Church with 1,992,567; the United Methodist Church with 496,232; the Evangelical Lutheran Church in America with 223,253, the Southern Baptist Convention with 171,000, the Christian Churches and Churches of Christ with 141,311, the United Church of Christ with 118,000, and the Presbyterian Church (USA) with 110,000.
According to the same data, a majority of Ohioans, 55%, feel that religion is "very important," 30% say that it is "somewhat important," and 15% responded that religion is "not too important/not important at all." 36% of Ohioans indicate that they attend religious services at least once weekly, 35% attend occasionally, and 27% seldom or never participate in religious services.
Economy.
In 2010, Ohio was ranked No. 2 in the country for best business climate by Site Selection magazine, based on a business-activity database. The state has also won three consecutive Governor's Cup awards from the magazine, based on business growth and developments. s of 2010[ [update]], Ohio's gross domestic product (GDP) was $478 billion. This ranks Ohio's economy as the seventh-largest of all fifty states and the District of Columbia.
The Small Business & Entrepreneurship Council ranked the state No. 10 for best business-friendly tax systems in their Business Tax Index 2009, including a top corporate tax and capital gains rate that were both ranked No. 6 at 1.9%. Ohio was ranked No. 11 by the council for best friendly-policy states according to their Small Business Survival Index 2009. The Directorship's Boardroom Guide ranked the state No. 13 overall for best business climate, including No. 7 for best litigation climate. Forbes ranked the state No. 8 for best regulatory environment in 2009. Ohio has 5 of the top 115 colleges in the nation, according to U.S. News and World Report's 2010 rankings, and was ranked No. 8 by the same magazine in 2008 for best high schools.
Ohio's unemployment rate stood at 10.7 in May 2010, adding 17,000 new jobs that month. Ohio's per capita income stands at $34,874. Moody's is predicting a 1.3% increase in personal income in 2009 for Ohio, compared to the 2007 rate of 4.7%. s of 2007[ [update]], Ohio's median household income is $46,645, and 13.1% of the population is below the poverty line, slightly above the national rate of 13%. Ohio's employment base is expected to grow 5% from 2006 to 2016, a net gain of 290,700 jobs.
The manufacturing and financial activities sectors each compose 18.3% of Ohio's GDP, making them Ohio's largest industries by percentage of GDP. Ohio has the largest bioscience sector in the Midwest, and is a national leader in the "green" economy. Ohio is the largest producer in the country of plastics, rubber, fabricated metals, electrical equipment, and appliances. 5,212,000 Ohioans are currently employed by wage or salary.
By employment, Ohio's largest sector is trade/transportation/utilities, which employs 1,010,000 Ohioans, or 19.4% of Ohio's workforce, while the health care and education sector employs 825,000 Ohioans (15.8%). Government employs 787,000 Ohioans (15.1%), manufacturing employs 669,000 Ohioans (12.9%), and professional and technical services employs 638,000 Ohioans (12.2%). Ohio's manufacturing sector is the third-largest of all fifty United States states in terms of gross domestic product. Fifty-nine of the United States' top 1,000 publicly traded companies (by revenue in 2008) are headquartered in Ohio, including Procter & Gamble, Goodyear Tire & Rubber, AK Steel, Timken, Abercrombie & Fitch, and Wendy's.
Ohio is also one of 41 states with its own lottery, the Ohio Lottery. The Ohio Lottery has contributed over $15.5 billion to public education in its 34-year history.
Transportation.
Ground travel
Many major east-west transportation corridors go through Ohio. One of those pioneer routes, known in the early 20th century as "Main Market Route 3", was chosen in 1913 to become part of the historic Lincoln Highway which was the first road across America, connecting New York City to San Francisco. In Ohio, the Lincoln Highway linked many towns and cities together, including Canton, Mansfield, Wooster, Lima, and Van Wert. The arrival of the Lincoln Highway to Ohio was a major influence on the development of the state. Upon the advent of the federal numbered highway system in 1926, the Lincoln Highway through Ohio became U.S. Route 30.
Ohio also is home to 228 mi of the Historic National Road, now U.S. Route 40.
Ohio has a highly developed network of roads and interstate highways. Major east-west through routes include the Ohio Turnpike (I-80/I-90) in the north, I-76 through Akron to Pennsylvania, I-70 through Columbus and Dayton, and the Appalachian Highway (State Route 32) running from West Virginia to Cincinnati. Major north-south routes include I-75 in the west through Toledo, Dayton, and Cincinnati, I-71 through the middle of the state from Cleveland through Columbus and Cincinnati into Kentucky, and I-77 in the eastern part of the state from Cleveland through Akron, Canton, New Philadelphia and Marietta down into West Virginia. Interstate 75 between Cincinnati and Dayton is one of the heaviest traveled sections of interstate in Ohio.
Ohio also has a highly developed network of signed state bicycle routes. Many of them follow rail trails, with conversion ongoing. The Ohio to Erie Trail (route 1) connects Cincinnati, Columbus, and Cleveland. U.S. Bicycle Route 50 traverses Ohio from Steubenville to the Indiana state line outside Richmond.
Air travel
Ohio has 5 international airports, 4 commercial and 2 military. The 5 international includes Cleveland Hopkins International Airport, Port Columbus International Airport, and Dayton International Airport, Ohio's third largest airport. Akron Fulton International Airport handles cargo and for private use. Rickenbacker International Airport is one of two military airfields which is also home to the 7th largest FedEx building in America. The other military airfield is Wright Patterson Air Force Base which is one of the largest Air Force bases in the United States. Other major airports are located in Toledo and Akron.
Cincinnati/Northern Kentucky International Airport is in Hebron, Kentucky and therefore is not listed above.
Law and government.
The state government of Ohio consists of the executive, judicial, and legislative branches.
Executive branch.
The executive branch is headed by the Governor of Ohio. The current governor is John Kasich, a Republican elected in 2010. A lieutenant governor succeeds the governor in the event of any removal from office, and performs any duties assigned by the governor. The current lieutenant governor is Mary Taylor. The other elected constitutional offices in the executive branch are the secretary of state (Jon A. Husted), auditor (Dave Yost), treasurer (Josh Mandel), and attorney general (Mike DeWine).
Judicial branch.
There are three levels of the Ohio state judiciary. The lowest level is the court of common pleas: each county maintains its own constitutionally mandated court of common pleas, which maintain jurisdiction over "all justiciable matters." The intermediate-level court system is the district court system. Twelve courts of appeals exist, each retaining jurisdiction over appeals from common pleas, municipal, and county courts in a set geographical area. A case heard in this system is decided by a three-judge panel, and each judge is elected.
The highest-ranking court, the Ohio Supreme Court, is Ohio's "court of last resort." A seven-justice panel composes the court, which, by its own discretion, hears appeals from the courts of appeals, and retains original jurisdiction over limited matters.
Legislative branch.
The Ohio General Assembly is a bicameral legislature consisting of the Senate and House of Representatives. The Senate is composed of 33 districts, each of which is represented by one senator. Each senator represents approximately 330,000 constituents. The House of Representatives is composed of 99 members.
National politics.
Ohio, nicknamed the "Mother of Presidents," has sent seven of its native sons (Ulysses S. Grant, Rutherford B. Hayes, James A. Garfield, Benjamin Harrison, William McKinley, William Howard Taft, and Warren G. Harding) to the White House. All seven were Republicans. Virginia native William Henry Harrison, a Whig, resided in Ohio. Historian R. Douglas Hurt asserts that not since Virginia 'had a state made such a mark on national political affairs.' "The Economist" notes that "This slice of the mid-west contains a bit of everything American — part north-eastern and part southern, part urban and part rural, part hardscrabble poverty and part booming
suburb," Ohio is the only state that has voted for the winning Presidential candidate in each election since 1964, and in 33 of the 37 held since the Civil War. No Republican has ever won the presidency without winning Ohio.
s of 2008[ [update]], Ohio's voter demographic leans towards the Democratic Party. An estimated 2,408,178 Ohioans are registered to vote as Democrats, while 1,471,465 Ohioans are registered to vote as Republicans. These are changes from 2004 of 72% and 32%, respectively, and Democrats have registered over 1,000,000 new Ohioans since 2004. Unaffiliated voters have an attrition of 15% since 2004, losing an estimated 718,000 of their kind. The total now rests at 4,057,518 Ohioans. In total, there are 7,937,161 Ohioans registered to vote. In the United States presidential election of 2008, then-Senator Barack Obama of Illinois won 51.50% of Ohio's popular vote, 4.59 percentage points more than his nearest rival, Senator John McCain of Arizona (with 46.91% of the popular vote). However, Obama won only 22 of Ohio's 88 counties. Since 2010, the Republicans have largely controlled Ohio state politics, including a super-majority in the state's House, a majority in the state Senate, the Governorship, etc. As of 2014, the state Senate is 1 Republican away from a super-majority.
Following the 2000 census, Ohio lost one congressional district in the United States House of Representatives, which leaves Ohio with 18 districts, and consequently, 18 representatives. The state lost two more seats following the 2010 Census, leaving it with 18 votes for the next 3 presidential elections in 2012, 2016 and 2020. The 2008 elections, Democrats gained three seats in Ohio's delegation to the House of Representatives. This leaves eight Republican-controlled seats in the Ohio delegation. Ohio's U.S. Senators in the 112th Congress are Republican Rob Portman and Democrat Sherrod Brown. Marcia Kaptur (D-9) is the dean, or most senior member, of the Ohio delegation to the United States House of Representatives.
Education.
Ohio's system of public education is outlined in Article VI of the state constitution, and in Title XXXIII of the Ohio Revised Code. Ohio University, the first university in the Northwest Territory, was also the first public institution in Ohio. Substantively, Ohio's system is similar to those found in other states. At the State level, the Ohio Department of Education, which is overseen by the Ohio State Board of Education, governs primary and secondary educational institutions. At the municipal level, there are approximately 700 school districts statewide. The Ohio Board of Regents coordinates and assists with Ohio's institutions of higher education which have recently been reorganized into the University System of Ohio under Governor Strickland. The system averages an annual enrollment of over 400,000 students, making it one of the five largest state university systems in the U.S.
Libraries.
Ohio is home to some of the nation's highest-ranking public libraries. The 2008 study by Thomas J. Hennen Jr. ranked Ohio as number one in a state-by-state comparison. For 2008, 31 of Ohio's library systems were all ranked in the top ten for American cities of their population category.
The Ohio Public Library Information Network (OPLIN) is an organization that provides Ohio residents with internet access to their 251 public libraries. OPLIN also provides Ohioans with free home access to high-quality, subscription research databases.
Ohio also offers the OhioLINK program, allowing Ohio's libraries (particularly those from colleges and universities) access to materials in other libraries. The program is largely successful in allowing researchers access to books and other media that might not otherwise be available.
Sports.
Professional sports leagues.
Ohio is home to major professional sports teams in baseball, basketball, football, hockey, lacrosse and soccer. The state's major professional sporting teams include: Cincinnati Reds (Major League Baseball), Ohio Machine (Major League Lacrosse), Cleveland Indians (Major League Baseball), Cincinnati Bengals (National Football League), Cleveland Browns (National Football League), Cleveland Cavaliers (National Basketball Association), Columbus Blue Jackets (National Hockey League), and the Columbus Crew (Major League Soccer).
Ohio played a central role in the development of both Major League Baseball and the National Football League. Baseball's first fully professional team, the Cincinnati Red Stockings of 1869, were organized in Ohio. An informal early 20th century American football association, the Ohio League, was the direct predecessor of the NFL, although neither of Ohio's modern NFL franchises trace their roots to an Ohio League club. The Pro Football Hall of Fame is located in Canton.
On a smaller scale, Ohio hosts minor league baseball, arena football, indoor football, mid-level hockey, and lower division soccer.
Individual sports.
The Mid-Ohio Sports Car Course has hosted several auto racing championships, including CART World Series, IndyCar Series, NASCAR Nationwide Series, Can-Am, Formula 5000, IMSA GT Championship, American Le Mans Series and Rolex Sports Car Series.
The Grand Prix of Cleveland also hosted CART races from 1982 to 2007. The Eldora Speedway is a major dirt oval that hosts NASCAR Camping World Truck Series, World of Outlaws Sprint Cars and USAC Silver Crown Series races.
Ohio hosts two PGA Tour events, the WGC-Bridgestone Invitational and Memorial Tournament.
The Cincinnati Masters is an ATP World Tour Masters 1000 and WTA Premier 5 tennis tournament.
College football (NCAA DI-A).
Ohio has eight NCAA Division I-A college football teams, divided among three different conferences. It has also experienced considerable success in the secondary and tertiary tiers of college football divisions.
In Division I-A, representing the Big Ten, the Ohio State Buckeyes football team ranks 5th among all-time winningest programs, with seven national championships and seven Heisman Trophy winners. Their biggest rivals are the Michigan Wolverines, whom they traditionally play each year as the last game of their regular season schedule.
Ohio has six teams represented in the Mid-American Conference: the University of Akron, Bowling Green, Kent State, Miami University, Ohio University and the University of Toledo. The MAC headquarters are based in Cleveland.
The University of Cincinnati Bearcats represent Ohio in the American Athletic Conference.
State symbols.
Ohio's state symbols:

</doc>
<doc id="22201" url="http://en.wikipedia.org/wiki?curid=22201" title="Orbital">
Orbital

Orbital may refer to:
In chemistry and physics:
In astronomy and space flight:
In entertainment:
In other fields:

</doc>
<doc id="22203" url="http://en.wikipedia.org/wiki?curid=22203" title="Organic compound">
Organic compound

An organic compound is any member of a large class of gaseous, liquid, or solid chemical compounds whose molecules contain carbon. For historical reasons discussed below, a few types of carbon-containing compounds, such as carbides, carbonates, simple oxides of carbon (such as CO and CO2), and cyanides are considered inorganic. The distinction between "organic" and "inorganic" carbon compounds, while "useful in organizing the vast subject of chemistry... is somewhat arbitrary".
Organic chemistry is the science concerned with all aspects of organic compounds. Organic synthesis is the methodology of their preparation.
History.
Vitalism.
The word "organic" is historical, dating to the 1st century. For many centuries, Western alchemists believed in vitalism. This is the theory that certain compounds could be synthesized only from their classical elements—earth, water, air, and fire—by the action of a "life-force" ("vis vitalis") that only organisms possessed. Vitalism taught that these "organic" compounds were fundamentally different from the "inorganic" compounds that could be obtained from the elements by chemical manipulation.
Vitalism survived for a while even after the rise of modern atomic theory and the replacement of the Aristotelian elements by those we know today. It first came under question in 1824, when Friedrich Wöhler synthesized oxalic acid, a compound known to occur only in living organisms, from cyanogen. A more decisive experiment was Wöhler's 1828 synthesis of urea from the inorganic salts potassium cyanate and ammonium sulfate. Urea had long been considered an "organic" compound, as it was known to occur only in the urine of living organisms. Wöhler's experiments were followed by many others, where increasingly complex "organic" substances were produced from "inorganic" ones without the involvement of any living organism.
Modern classification.
Even though vitalism has been discredited, scientific nomenclature retains the distinction between "organic" and "inorganic" compounds. The modern meaning of "organic compound" is any compound that contains a significant amount of carbon—even though many of the organic compounds known today have no connection to any substance found in living organisms.
There is no single "official" definition of an organic compound. Some textbooks define an organic compound as one that contains one or more C-H bonds. Others include C-C bonds in the definition. Others state that if a molecule contains carbon―it is organic.
Even the broader definition of "carbon-containing molecules" requires the exclusion of carbon-containing alloys (including steel), a relatively small number of carbon-containing compounds, such as metal carbonates and carbonyls, simple oxides of carbon and cyanides, as well as the allotropes of carbon and simple carbon halides and sulfides, which are usually considered inorganic.
The "C-H" definition excludes compounds that are historically and practically considered organic. Neither urea nor oxalic acid is organic by this definition, yet they were two key compounds in the vitalism debate. The IUPAC Blue Book on organic nomenclature specifically mentions urea and oxalic acid. Other compounds lacking C-H bonds that are also traditionally considered organic include benzenehexol, mesoxalic acid, and carbon tetrachloride. Mellitic acid, which contains no C-H bonds, is considered a possible organic substance in Martian soil. C-C bonds are found in most organic compounds, except some small molecules like methane and methanol, which have only one carbon atom in their structure.
The "C-H bond-only" rule also leads to somewhat arbitrary divisions in sets of carbon-fluorine compounds, as, for example, Teflon is considered by this rule to be "inorganic", whereas Tefzel is considered to be organic. Likewise, many Halons are considered inorganic, whereas the rest are considered organic. For these and other reasons, most sources believe that C-H compounds are only a subset of "organic" compounds.
In summary, most carbon-containing compounds are organic, and almost all organic compounds contain at least a C-H bond or a C-C bond. A compound does not need to contain C-H bonds to be considered organic (e.g., urea), but many organic compounds do.
Classification.
Organic compounds may be classified in a variety of ways. One major distinction is between natural and synthetic compounds. Organic compounds can also be classified or subdivided by the presence of heteroatoms, e.g., organometallic compounds, which feature bonds between carbon and a metal, and organophosphorus compounds, which feature bonds between carbon and a phosphorus.
Another distinction, based on the size of organic compounds, distinguishes between small molecules and polymers.
Natural compounds.
Natural compounds refer to those that are produced by plants or animals. Many of these are still extracted from natural sources because they would be more expensive to produce artificially. Examples include most sugars, some alkaloids and terpenoids, certain nutrients such as vitamin B12, and, in general, those natural products with large or stereoisometrically complicated molecules present in reasonable concentrations in living organisms.
Further compounds of prime importance in biochemistry are antigens, carbohydrates, enzymes, hormones, lipids and fatty acids, neurotransmitters, nucleic acids, proteins, peptides and amino acids, lectins, vitamins, and fats and oils.
Synthetic compounds.
Compounds that are prepared by reaction of other compounds are known as "synthetic". They may be either compounds that already are found in plants or animals or those that do not occur naturally.
Most polymers (a category that includes all plastics and rubbers), are organic synthetic or semi-synthetic compounds.
Biotechnology.
Several compounds are industrially manufactured utilizing the biochemistry of organisms such as bacteria and yeast. Two examples are ethanol and insulin. Regularly, the DNA of the organism is altered to express desired compounds that are often not ordinarily produced by that organism. Sometimes the biotechnologically engineered compounds were never present in nature in the first place.
Nomenclature.
The IUPAC nomenclature of organic compounds slightly differs from the CAS nomenclature.
Databases.
There is a great number of more specialized databases for diverse branches of organic chemistry.
Structure determination.
Today, the main tools are proton and carbon-13 NMR spectroscopy, IR Spectroscopy, Mass spectrometry, UV/Vis Spectroscopy and X-ray crystallography.

</doc>
<doc id="22204" url="http://en.wikipedia.org/wiki?curid=22204" title="Oligopoly">
Oligopoly

An oligopoly (from " "ὀλίγος" (olígos)", meaning "few", and " "πωλεῖν" (polein)", meaning "to sell") is a market form in which a market or industry is dominated by a small number of sellers (oligopolists). Oligopolies can result from various forms of collusion which reduce competition and lead to higher prices for consumers. Oligopoly has its own market structure.
With few sellers, each oligopolist is likely to be aware of the actions of the others. According to game theory, the decisions of one firm therefore influence and are influenced by the decisions of other firms. Strategic planning by oligopolists needs to take into account the likely responses of the other market participants.
Description.
Oligopoly is a common market form where a number of firms are in competition. As a quantitative description of oligopoly, the four-firm concentration ratio is often utilized. This measure expresses the market share of the four largest firms in an industry as a percentage. For example, as of fourth quarter 2008, Verizon, AT&T, Sprint, and T-Mobile together control 89% of the US cellular phone market.
Oligopolistic competition can give rise to a wide range of different outcomes. In some situations, the firms may employ restrictive trade practices (collusion, market sharing etc.) to raise prices and restrict production in much the same way as a monopoly. Where there is a formal agreement for such collusion, this is known as a cartel. A primary example of such a cartel is OPEC which has a profound influence on the international price of oil.
Firms often collude in an attempt to stabilize unstable markets, so as to reduce the risks inherent in these markets for investment and product development. There are legal restrictions on such collusion in most countries. There does not have to be a formal agreement for collusion to take place (although for the act to be illegal there must be actual communication between companies)–for example, in some industries there may be an acknowledged market leader which informally sets prices to which other producers respond, known as price leadership.
In other situations, competition between sellers in an oligopoly can be fierce, with relatively low prices and high production. This could lead to an efficient outcome approaching perfect competition. The competition in an oligopoly can be greater when there are more firms in an industry than if, for example, the firms were only regionally based and did not compete directly with each other.
Thus the welfare analysis of oligopolies is sensitive to the parameter values used to define the market's structure. In particular, the level of dead weight loss is hard to measure. The study of product differentiation indicates that oligopolies might also create excessive levels of differentiation in order to stifle competition.
Oligopoly theory makes heavy use of game theory to model the behavior of oligopolies:
—==Characteristics==
Modeling.
There is no single model describing the operation of an oligopolistic market. The variety and complexity of the models exist because you can have two to 10 firms competing on the basis of price, quantity, technological innovations, marketing, and reputation. Fortunately, there are a series of simplified models that attempt to describe market behavior by considering certain circumstances. Some of the better-known models are the dominant firm model, the Cournot-Nash model, the Bertrand model and the kinked demand model.
Cournot-Nash model.
The Cournot-Nash model is the simplest oligopoly model. The model assumes that there are two “equally positioned firms”; the firms compete on the basis of quantity rather than price and each firm makes an “output decision assuming that the other firm’s behavior is fixed.” The market demand curve is assumed to be linear and marginal costs are constant. To find the Cournot-Nash equilibrium one determines how each firm reacts to a change in the output of the other firm. The path to equilibrium is a series of actions and reactions. The pattern continues until a point is reached where neither firm desires “to change what it is doing, given how it believes the other firm will react to any change.” The equilibrium is the intersection of the two firm’s reaction functions. The reaction function shows how one firm reacts to the quantity choice of the other firm. For example, assume that the firm 1’s demand function is P = (M - Q2) - Q1 where Q2 is the quantity produced by the other firm and Q1 is the amount produced by firm 1, and M=60 is the market. Assume that marginal cost is CM=12. Firm 1 wants to know its maximizing quantity and price. Firm 1 begins the process by following the profit maximization rule of equating marginal revenue to marginal costs. Firm 1’s total revenue function is RT = Q1 P= Q1(M - Q2 - Q1) = M Q1- Q1 Q2 - Q12. The marginal revenue function is formula_1.
Equation 1.1 is the reaction function for firm 1. Equation 1.2 is the reaction function for firm 2.
To determine the Cournot-Nash equilibrium you can solve the equations simultaneously. The equilibrium quantities can also be determined graphically. The equilibrium solution would be at the intersection of the two reaction functions. Note that if you graph the functions the axes represent quantities. The reaction functions are not necessarily symmetric. The firms may face differing cost functions in which case the reaction functions would not be identical nor would the equilibrium quantities.
Bertrand model.
The Bertrand model is essentially the Cournot-Nash model except the strategic variable is price rather than quantity.
The model assumptions are:
The only Nash equilibrium is PA = PB = MC.
Neither firm has any reason to change strategy. If the firm raises prices it will lose all its customers. If the firm lowers price P < MC then it will be losing money on every unit sold.
The Bertrand equilibrium is the same as the competitive result. Each firm will produce where P = marginal costs and there will be zero profits. A generalization of the Bertrand model is the Bertrand-Edgeworth Model that allows for capacity constraints and more general cost functions.
Kinked demand curve model.
According to this model, each firm faces a demand curve kinked at the existing price. The conjectural assumptions of the model are; if the firm raises its price above the current existing price, competitors will not follow and the acting firm will lose market share and second if a firm lowers prices below the existing price then their competitors will follow to retain their market share and the firm's output will increase only marginally.
If the assumptions hold then: 
The gap in the marginal revenue curve means that marginal costs can fluctuate without changing equilibrium price and quantity. Thus prices tend to be rigid.
Examples.
In industrialized economies, barriers to entry have resulted in oligopolies forming in many sectors, with unprecedented levels of competition fueled by increasing globalization. Market shares in an oligopoly are typically determined by product development and advertising. For example, there are now only a small number of manufacturers of civil passenger aircraft, though Brazil (Embraer) and Canada (Bombardier) have participated in the small passenger aircraft market sector. Oligopolies have also arisen in heavily-regulated markets such as wireless communications: in some areas only two or three providers are licensed to operate.
Demand curve.
In an oligopoly, firms operate under imperfect competition. With the fierce price competitiveness created by this sticky-upward demand curve, firms use non-price competition in order to accrue greater revenue and market share.
"Kinked" demand curves are similar to traditional demand curves, as they are downward-sloping. They are distinguished by a hypothesized convex bend with a discontinuity at the bend–"kink". Thus the first derivative at that point is undefined and leads to a jump discontinuity in the marginal revenue curve.
Classical economic theory assumes that a profit-maximizing producer with some market power (either due to oligopoly or monopolistic competition) will set marginal costs equal to marginal revenue. This idea can be envisioned graphically by the intersection of an upward-sloping marginal cost curve and a downward-sloping marginal revenue curve (because the more one sells, the lower the price must be, so the less a producer earns per unit). In classical theory, any change in the marginal cost structure (how much it costs to make each additional unit) or the marginal revenue structure (how much people will pay for each additional unit) will be immediately reflected in a new price and/or quantity sold of the item. This result does not occur if a "kink" exists. Because of this jump discontinuity in the marginal revenue curve, marginal costs could change without necessarily changing the price or quantity.
The motivation behind this kink is the idea that in an oligopolistic or monopolistically competitive market, firms will not raise their prices because even a small price increase will lose many customers. This is because competitors will generally ignore price increases, with the hope of gaining a larger market share as a result of now having comparatively lower prices. However, even a large price decrease will gain only a few customers because such an action will begin a price war with other firms. The curve is therefore more price-elastic for price increases and less so for price decreases. Theory predicts that firms will enter the industry in the long run.

</doc>
<doc id="22205" url="http://en.wikipedia.org/wiki?curid=22205" title="Oasis">
Oasis

In geography, an oasis (plural: oases) or cienega (Southwestern United States) is an isolated area of vegetation in a desert, typically surrounding a spring or similar water source. Oases also provide habitat for animals and even humans if the area is big enough. The location of oases has been of critical importance for trade and transportation routes in desert areas; caravans must travel via oases so that supplies of water and food can be replenished. Thus, political or military control of an oasis has in many cases meant control of trade on a particular route. For example, the oases of Awjila, Ghadames, and Kufra, situated in modern-day Libya, have at various times been vital to both North-South and East-West trade in the Sahara Desert.
Oases are formed from underground rivers or aquifers such as an artesian aquifer, where water can reach the surface naturally by pressure or by man-made wells. Occasional brief thunderstorms provide subterranean water to sustain natural oases, such as the Tuat. Substrata of impermeable rock and stone can trap water and retain it in pockets, or on long faulting subsurface ridges or volcanic dikes water can collect and percolate to the surface. Any incidence of water is then used by migrating birds, which also pass seeds with their droppings which will grow at the water's edge forming an oasis.
Etymology.
The word "oasis" comes into English via Latin: "oasis" from Ancient Greek: ὄασις "óasis", which in turn is a direct borrowing from Demotic Egyptian. The word for "oasis" in the later attested Coptic language (the descendant of Demotic Egyptian) is "wahe" or "ouahe" which means a "dwelling place".
Growing plants.
People who live in an oasis must manage land and water use carefully; fields must be irrigated to grow plants like apricots, dates, figs, and olives. The most important plant in an oasis is the date palm, which forms the upper layer. These palm trees provide shade for smaller trees like peach trees, which form the middle layer. By growing plants in different layers, the farmers make best use of the soil and water. Many vegetables are also grown and some cereals, such as barley, millet, and wheat, where there is more moisture.

</doc>
<doc id="22206" url="http://en.wikipedia.org/wiki?curid=22206" title="Oboe">
Oboe

The oboe is a family of double reed woodwind musical instruments. The most common oboe plays in the treble or soprano range. Oboes are usually made of wood, but there are also oboes made of synthetic materials. A soprano oboe measures roughly 65 cm (25½ inches) long, with metal keys, a conical bore and a flared bell. Sound is produced by blowing into the reed and vibrating a column of air. The distinctive oboe tone is versatile, and has been described as "bright". When the term "oboe" is used alone, it is generally taken to mean the standard treble instrument rather than other instruments of the family, such as the cor anglais (English horn) or the oboe d'amore.
In English, prior to 1770, the standard instrument was called the hautbois, hoboy, or French hoboy (pronounced /ˈhəʊbɔɪ/, or "HOE-boy", borrowed from the French name, a compound word made of "haut" ["high, loud"] and "bois" ["wood, woodwind"]). The spelling "oboe" was adopted into English c. 1770 from the Italian "oboè", a transliteration in that language's orthography of the 17th-century pronunciation of the French name. A musician who plays the oboe is called an oboist.
Today, the oboe is commonly used in concert bands, orchestras, chamber music, film music, in some genres of folk music, and as a solo instrument, and is occasionally heard in jazz, rock music, pop music, and popular music.
Sound.
In comparison to other modern woodwind instruments, the treble oboe is sometimes referred to as having a clear and penetrating voice. "The Sprightly Companion", an instruction book published by Henry Playford in 1695, describes the oboe as "Majestical and Stately, and not much Inferior to the Trumpet." More humorously, the voice is described in the play "Angels in America" as sounding like "that of a duck if the duck were a songbird". The rich timbre of the oboe is derived from the oboe's conical bore (as opposed to the generally cylindrical bore of flutes and clarinets). As a result, oboes are readily audible over other instruments in large ensembles.
Music for the standard oboe is written in concert pitch (i.e., it is not a transposing instrument), and the instrument has a soprano range, usually from B flat 3 up to G 6. Orchestras normally tune to a concert A played by the oboe. According to the League of American Orchestras, this is done because the pitch of the oboe is secure and its penetrating sound makes it ideal for tuning purposes. The pitch of the regular oboe is affected by the way in which the reed is made. The reed has a significant effect on the sound of the instrument. Variations in cane and other construction materials, the age of the reed, and differences in scrape and length will all affect the pitch of the instrument. German and French reeds, for instance, differ in many ways, causing the sound of the oboe to vary accordingly. Weather conditions such as temperature and humidity will also affect the pitch. Skilled oboists adjust their embouchure to compensate for these factors. Subtle manipulation of embouchure and air pressure allows the player to express timbre and dynamics.
<span id="Hautbois"/><span id="Hautboy"/><span id="Baroque"/>
History.
The regular oboe first appeared in the mid-17th century, when it was called "hautbois". This name was also used for its predecessor, the shawm, from which the basic form of the "hautbois" was derived. Major differences between the two instruments include the division of the "hautbois" into three sections, or joints (which allowed for more precise manufacture), and the elimination of the "pirouette", the wooden ledge below the reed which allowed players to rest their lips.
The exact date and place of origin of the "hautbois" are obscure, as are the individuals who were responsible. Circumstantial evidence, such as the statement by the flautist composer Michel de la Barre in his "Memoire", points to members of the Philidor (Filidor) and Hotteterre families. The instrument may in fact have had multiple inventors. The "hautbois" quickly spread throughout Europe, including Great Britain, where it was called "hautboy", "hoboy", "hautboit", "howboye", and similar variants of the French name. It was the main melody instrument in early military bands, until it was succeeded by the clarinet.
The standard baroque oboe was generally made of boxwood and had three keys: a "great" key and two side keys (The side key was often doubled to facilitate use of either the right or left hand on the bottom holes). In order to produce higher pitches, the player had to "overblow", or increase the air stream to reach the next harmonic. Notable oboe-makers of the period are the Germans Jacob Denner and J.H. Eichentopf, and the English Thomas Stanesby (died 1734) and his son Thomas Jr (died 1754). The range for the baroque oboe comfortably extends from C4 to D6. With the resurgence of interest in early music in the mid 20th century, a few makers began producing copies to specifications taken from surviving historical instruments.
Classical.
The classical period brought a regular oboe whose bore was gradually narrowed, and the instrument became outfitted with several keys, among them were those for the notes D♯, F, and G♯. A key similar to the modern octave key was also added called the "slur key", though it was at first used more like the "flick" keys on the modern German bassoon. Only later did French instrument makers redesign the octave key to be used in the manner of the modern key (i.e. held open for the upper register, closed for the lower). The narrower bore allowed the higher notes to be more easily played, and composers began to more often utilize the oboe's upper register in their works. Because of this, the oboe's tessitura in the Classical era was somewhat broader than that found in baroque works. The range for the Classical oboe extends from C4 to F6 (using the scientific pitch notation system), though some German and Austrian oboes were capable of playing one half-step lower. Classical-era composers who wrote concertos for oboe include Mozart (both the solo concerto in C major K. 314/285d and the lost original of Sinfonia Concertante in E♭ major K. 297b, as well as a fragment of F major concerto K. 417f), Haydn, (both the Sinfonia Concertante in B♭ Hob. I:105 and the spurious concerto in C major Hob. VIIg:C1), Beethoven (the F major concerto, Hess 12, of which only sketches survive, though the second movement was reconstructed in the late twentieth century), and numerous other composers including Johann Christian Bach, Johann Christian Fischer, Jan Antonín Koželuh, and Ludwig August Lebrun. Many solos exist for the regular oboe in chamber, symphonic, and operatic compositions from the Classical era.
Viennese or Wiener oboe.
The Wiener oboe is a type of modern oboe that retains the essential bore and tonal characteristics of the historical oboe. The Akademiemodel Wiener Oboe, first developed in the late 19th century by Josef Hajek from earlier instruments by C. T. Golde of Dresden (1803–73), is now made by several makers such as André Constantinides, Karl Rado, Guntram Wolf, Christian Rauch and Yamaha. It has a wider internal bore, a shorter and broader reed and the fingering-system is very different than the Conservatoire oboe. In "The Oboe", Geoffrey Burgess and Bruce Haynes write "The differences are most clearly marked in the middle register, which is reedier and more pungent, and the upper register, which is richer in harmonics on the Viennese oboe". Guntram Wolf describes them: "From the concept of the bore, the Viennese oboe is the last representative of the historical oboes, adapted for the louder, larger orchestra, and fitted with an extensive mechanism. Its great advantage is the ease of speaking, even in the lowest register. It can be played very expressively and blends well with other instruments." The Viennese oboe is, along with the Vienna horn, perhaps the most distinctive member of the Wiener Philharmoniker instrumentarium.
Conservatoire oboe.
This oboe was developed further in the 19th century by the Triebert family of Paris. Using the Boehm flute as a source of ideas for key work, Guillaume Triebert and his sons, Charles and Frederic, devised a series of increasingly complex yet functional key systems. A variant form using large tone holes, the Boehm system oboe, was never in common use, though it was used in some military bands in Europe into the 20th century. F. Lorée of Paris made further developments to the modern instrument. Minor improvements to the bore and key work have continued through the 20th century, but there has been no fundamental change to the general characteristics of the instrument for several decades.
The modern standard oboe is most commonly made from grenadilla, also known as African Blackwood, though some manufacturers also make oboes out of other members of the genus "Dalbergia", which includes cocobolo, rosewood, and violetwood (also known as kingwood). Ebony (genus Diospyros) has also been used. Student model oboes are often made from plastic resin, to avoid instrument cracking to which wood instruments are prone, but also to make the instrument more economical. The oboe has an extremely narrow conical bore. It is played with a double reed consisting of two thin blades of cane tied together on a small-diameter metal tube (staple) which is inserted into the reed socket at the top of the instrument. The commonly accepted range for the oboe extends from B♭3 to about G6, over two and a half octaves, though its common tessitura lies from C4 to E♭6. Some student oboes only extend down to B3 (the key for B♭ is not present). However this variant is becoming less common.
A modern oboe with the "full conservatoire" ("conservatory" in the USA) or Gillet key system has 45 pieces of keywork, with the possible additions of a third octave key and alternate (left little finger) F- or C-key. The keys are usually made of nickel silver, and are silver- or occasionally gold-plated. Besides the full conservatoire system, oboes are also made using the British thumbplate system. Most have "semi-automatic" octave keys, in which the second octave action closes the first, and some have a fully automatic octave key system, as used on saxophones. Some full conservatory oboes have finger holes covered with rings rather than plates ("open-holed"), and most of the professional models have at least the right hand third key open-holed. Professional oboes used in the UK and Iceland frequently feature conservatoire system combined with a thumb plate. Releasing the thumb plate has the same effect as pressing down the right hand index finger key. This produces alternate options which eliminate the necessity for most of the common cross-intervals (intervals where two or more keys need to be released and pressed down simultaneously), but cross intervals are much more difficult to execute in such a way that the sound remains clear and continuous throughout the frequency change (a quality also called legato and often called-for in the oboe repertoire).
Other members of the oboe family.
The standard oboe has several siblings of various sizes and playing ranges. The most widely known today is the cor anglais, or English horn, the tenor (or alto) member of the family. A transposing instrument; it is pitched in F, a perfect fifth lower than the oboe. The oboe d'amore, the alto (or mezzo-soprano) member of the family, is pitched in A, a minor third lower than the oboe. J.S. Bach made extensive use of both the oboe d'amore as well as the "taille" and "oboe da caccia", Baroque antecedents of the cor anglais. Even less common is the bass oboe (also called baritone oboe), which sounds one octave lower than the oboe. Delius and Holst both scored for the instrument. Similar to the bass oboe is the more powerful heckelphone, which has a wider bore and larger tone than the baritone oboe. Only 165 heckelphones have ever been made. Not surprisingly, competent heckelphone players are difficult to find due to the extreme rarity of this particular instrument. The least common of all are the musette (also called oboe musette or piccolo oboe), the sopranino member of the family (it is usually pitched in E♭ or F above the oboe), and the contrabass oboe (typically pitched in C, two octaves deeper than the standard oboe).
Folk versions of the oboe, sometimes equipped with extensive keywork, are found throughout Europe. These include the musette (France) and the Piston oboe and bombarde (Brittany), the "piffaro" and "ciaramella" (Italy), and the "xirimia" or "chirimia" (Spain). Many of these are played in tandem with local forms of bagpipe, particularly with the Italian zampogna or Breton biniou. Similar oboe-like instruments, most believed to derive from Middle Eastern models, are also found throughout Asia as well as in North Africa.
Reeds.
Most professional oboists make their own reeds since every oboist needs a slightly different reed to suit his or her individual needs. By making their own reeds, oboists can precisely control factors such as tone colour and tuning.
Occasionally, novice oboists may begin with a "Fibrecane" reed, which is made of a synthetic material. Commercially available cane reeds are available in several degrees of hardness; a medium reed is usually used, and most beginners use medium-soft reeds. These reeds, like clarinet, saxophone, and bassoon reeds, are made from "Arundo donax".
As oboists gain more experience, they may start making their own reeds after the model of their teacher, or buying hand-made reeds (usually from a professional oboist) and using special tools including gougers, pre-gougers, guillotines, knives, and other tools to make the reed to their own liking.
 According to the late John Mack, former principal oboist of the Cleveland Orchestra, an oboe student must fill a laundry basket with finished reeds in order to master the art . "Making good reeds requires years of practice, and the amateur is often well advised not to embark on making his own reeds, ... Orchestral musicians sometimes do this [make reeds], and co-principals in particular often earn a bit on the side in this way. ... Many professional musicians import their reed cane ... directly from the growers in southern France and split it vertically into three parts themselves. Oboes require thicknesses of about 10 millimetres, bassoons of 20 to 25 millimetres." This allows each player to adjust the reeds precisely for individual embouchure, oral cavity, oboe angle, and air support. The reed is considered the part of oboe playing that makes it so difficult because slight variations in temperature, altitude, weather, and climate will change a perfectly working reed into an unplayable collection of cane. The reed is in some points of view, the most important part of playing the oboe. The reed almost controls everything that comes out of the oboe, although the user of that oboe also contributes a significant amount of importance to the music too.
Use in non-classical music.
Traditional and folk music.
Although folk oboes are still used in many European folk music traditions, the modern oboe has been little used in folk music. One exception was Derek Bell, harpist for the Irish group The Chieftains, who used the regular instrument in some performances and recordings. The United States contra dance band , based in western Massachusetts, also uses the oboe, played by David Cantieni. The folk musician plays the oboe in several English folk bands including and . Welsh bagpipe player and bagpipe maker Jonathan Shorland plays a 'rustic oboe' similar to the Breton 'piston' with the bands Primeaval and Juice. He formerly played with Fernhill, who play traditional Welsh music. The popular traditional music of Brittany boasts a significant professional class of musicians playing increasingly sophisticated double reed instruments, supported by professional instrument makers, reed manufacturers and the educational system. The Breton 'Piston' Oboe and Bombard have expanded from traditional roles into genres as diverse as jazz, rock, and classical music.
Jazz.
The oboe remains uncommon in jazz music, but there have been notable uses of the instrument. Some early bands in the 1920s and '30s, most notably that of Paul Whiteman, included it for coloristic purposes. The multi-instrumentalist Garvin Bushell (1902–1991) played the oboe in jazz bands as early as 1924 and used the instrument throughout his career, eventually recording with John Coltrane in 1961. Gil Evans featured oboe in sections of his famous "Sketches of Spain" collaboration with trumpeter Miles Davis. Though primarily a tenor saxophone and flute player, Yusef Lateef was among the first (in 1963) to use the oboe as a solo instrument in modern jazz performances and recordings. Composer and double bassist Charles Mingus gave the oboe a brief but prominent role (played by Dick Hafer) in his composition "I.X. Love" on the 1963 album "Mingus Mingus Mingus Mingus Mingus". Marshall Allen occasionally played an oboe with Sun Ra.
With the birth of Jazz fusion in the late 1960s, and its continuous development through the following decade, the oboe became somewhat more prominent, replacing on some occasions the saxophone as the focal point. The oboe was used with great success by the Welsh multi-instrumentalist Karl Jenkins in his work with the groups Nucleus and Soft Machine, and by the American woodwind player Paul McCandless, co-founder of the Paul Winter Consort and later Oregon. Romeo Penque also played the oboe on Roland Kirk's 1975 album "Return of the 5000 Lb. Man", in the song "Theme for the Eulipions."
The 1980s saw an increasing number of oboists try their hand at non-classical work, and many players of note have recorded and performed alternative music on oboe. Some present-day jazz groups influenced by classical music, such as the Maria Schneider Orchestra, feature the oboe.
Rock and pop.
The oboe has been used sporadically in rock/pop recordings (e.g., The Carpenters' "For All We Know," 1970; Donovan's "Jennifer Juniper," 1968), generally by studio musicians on recordings of specific songs.
Peter Gabriel, during his period as lead singer in the progressive rock band Genesis, played oboe on some of the group's studio recordings. Andy Mackay played oboe for Roxy Music both in the studio and on stage.
In the 2000s, Robbie J. de Klerk, the vocalist of the Dutch melodic doom/death metal band Another Messiah also played the oboe in most songs. In America, the band Hoboe defines itself as a rock band showcasing amplified oboe since 2000, fronted by oboist Zen Ben. Indie singer-songwriter and composer Sufjan Stevens, having studied the instrument in school, often includes the instrument in his arrangements and compositions, most frequently in his geographic tone-poems Illinois, Michigan, and his orchestral suite The BQE.
Film music.
The oboe is frequently featured in film music, often to underscore a particularly poignant or sad scene, for example in the motion picture "Born on the Fourth of July", where an oboe delicately takes the theme with a romantic and harmonic touch before the strings hand it over once again to the trumpet. One of the most prominent uses of the oboe in a film score is Ennio Morricone's "Gabriel's Oboe" theme from the 1986 film "The Mission".
It is featured as a solo instrument in the theme "Across the Stars" from the John Williams score to "". The oboe is also used in "The Search" from the Basil Poledouris score to Conan The Barbarian.
Ilaiyaraja, a famous Indian film music composer, has also used the oboe in much of his film music. Examples include "Dalapathi" (1991); the title track of "Aditya 369" (1991); “Pazhassiraja” (2009); and “Nandalaala”(2010). The oboe has also been used by more recent Indian music composers, such as A. R. Rahman, who has used it in the movie "Jodha Akbar" (2008).

</doc>
