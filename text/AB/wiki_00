<doc id="13602" url="http://en.wikipedia.org/wiki?curid=13602" title="Huldrych Zwingli">
Huldrych Zwingli

Huldrych Zwingli or Ultricht Zwingli(1 January 1484 – 11 October 1531) was a leader of the Reformation in Switzerland. Born during a time of emerging Swiss patriotism and increasing criticism of the Swiss mercenary system, he attended the University of Vienna and the University of Basel, a scholarly centre of Renaissance humanism. He continued his studies while he served as a pastor in Glarus and later in Einsiedeln, where he was influenced by the writings of Erasmus.
In 1518, Zwingli became the pastor of the Grossmünster in Zurich where he began to preach ideas on reforming the Catholic Church. In his first public controversy in 1522, he attacked the custom of fasting during Lent. In his publications, he noted corruption in the ecclesiastical hierarchy, promoted clerical marriage, and attacked the use of images in places of worship. In 1525, Zwingli introduced a new communion liturgy to replace the Mass. Zwingli also clashed with the Anabaptists, which resulted in their persecution.
The Reformation spread to other parts of the Swiss Confederation, but several cantons resisted, preferring to remain Catholic. Zwingli formed an alliance of Reformed cantons which divided the Confederation along religious lines. In 1529, a war between the two sides was averted at the last moment. Meanwhile, Zwingli's ideas came to the attention of Martin Luther and other reformers. They met at the Marburg Colloquy and although they agreed on many points of doctrine, they could not reach an accord on the doctrine of the Real Presence of Christ in the Eucharist.
In 1531 Zwingli's alliance applied an unsuccessful food blockade on the Catholic cantons. The cantons responded with an attack at a moment when Zurich was ill prepared. Zwingli was killed in battle at the age of 47. His legacy lives on in the confessions, liturgy, and church orders of the Reformed churches of today.
Historical context.
The Swiss Confederation in Huldrych Zwingli's time consisted of thirteen states (cantons) as well as affiliated states and common lordships. Unlike the current modern state of Switzerland, which operates under a federal government, the thirteen states were nearly independent, conducting their own domestic and foreign affairs. Each state formed its own alliances within and without the Confederation. This relative independence served as the basis for conflict during the time of the Reformation when the various states divided between different confessional camps. Military ambitions were given an additional impetus with the competition to acquire new territory and resources, as seen for example in the Old Zurich War.
The political environment in Europe during the 15th and 16th centuries was also volatile. For centuries the foreign policies of the Confederation were determined by its relationship with its powerful neighbour, France. Nominally, the Confederation was under the control of the Holy Roman Empire. However, through a succession of wars culminating in the Swabian War, the Confederation had become "de facto" independent. As the two continental powers and minor states such as the Duchy of Milan, Duchy of Savoy, and the Papal States competed and fought against each other, there were far-reaching political, economic, and social consequences for the Confederation. It was during this time that the mercenary pension system became a subject of disagreement. The religious factions of Zwingli's time debated vociferously regarding the merits of sending young Swiss men to fight in foreign wars mainly for the enrichment of the cantonal authorities.
These internal and external factors contributed to the rise of a Confederation national consciousness, in which the term "fatherland" ("patria") began to take on meaning beyond an individual canton. At the same time, Renaissance humanism, with its universal values and emphasis on scholarship (as exemplified by Erasmus, the "prince of humanism"), had taken root in the country. It was within this environment, defined by the confluence of Swiss patriotism and humanism, that Zwingli was born.
Life.
Early years (1484–1518).
Huldrych Zwingli was born on 1 January 1484 in Wildhaus, in the Toggenburg valley of Switzerland, to a family of farmers, the third child of nine. His father, Ulrich, played a leading role in the administration of the community ("Amtmann" or chief local magistrate). Zwingli's primary schooling was provided by his uncle, Bartholomew, a cleric in Weesen, where he probably met Katharina von Zimmern. At ten years old, Zwingli was sent to Basel to obtain his secondary education where he learned Latin under Magistrate Gregory Bünzli. After three years in Basel, he stayed a short time in Bern with the humanist, Henry Wölfflin. The Dominicans in Bern tried to persuade Zwingli to join their order and it is possible that he was received as a novice. However, his father and uncle disapproved of such a course and he left Bern without completing his Latin studies. He enrolled in the University of Vienna in the winter semester of 1498 but was expelled, according to the university's records. However, it is not certain that Zwingli was indeed expelled, and he re-enrolled in the summer semester of 1500; his activities in 1499 are unknown. Zwingli continued his studies in Vienna until 1502, after which he transferred to the University of Basel where he received the Master of Arts degree ("Magister") in 1506.
Zwingli was ordained in Constance, the seat of the local diocese, and he celebrated his first Mass in his hometown, Wildhaus, on 29 September 1506. As a young priest he had studied little theology, but this was not considered unusual at the time. His first ecclesiastical post was the pastorate of the town of Glarus, where he stayed for ten years. It was in Glarus, whose soldiers were used as mercenaries in Europe, that Zwingli became involved in politics. The Swiss Confederation was embroiled in various campaigns with its neighbours: the French, the Habsburgs, and the Papal States. Zwingli placed himself solidly on the side of the Roman See. In return, Pope Julius II honoured Zwingli by providing him with an annual pension. He took the role of chaplain in several campaigns in Italy, including the Battle of Novara in 1513. However, the decisive defeat of the Swiss in the Battle of Marignano caused a shift in mood in Glarus in favour of the French rather than the pope. Zwingli, the papal partisan, found himself in a difficult position and he decided to retreat to Einsiedeln in the canton of Schwyz. By this time, he had become convinced that mercenary service was immoral and that Swiss unity was indispensable for any future achievements. Some of his earliest extant writings, such as "The Ox" (1510) and "The Labyrinth" (1516), attacked the mercenary system using allegory and satire. His countrymen were presented as virtuous people within a French, imperial, and papal triangle. Zwingli stayed in Einsiedeln for two years during which he withdrew completely from politics in favour of ecclesiastical activities and personal studies.
Zwingli's time as the pastor of Glarus and Einsiedeln was characterized by inner growth and development. He perfected his Greek and he took up the study of Hebrew. His library contained over three hundred volumes from which he was able to draw upon classical, patristic, and scholastic works. He exchanged scholarly letters with a circle of Swiss humanists and began to study the writings of Erasmus. Zwingli took the opportunity to meet him while Erasmus was in Basel between August 1514 and May 1516. Zwingli's turn to relative pacifism and his focus on preaching can be traced to the influence of Erasmus.
In late 1518, the post of the "Leutpriestertum" (people's priest) of the Grossmünster at Zurich became vacant. The canons of the foundation that administered the Grossmünster recognised Zwingli's reputation as a fine preacher and writer. His connection with humanists was a decisive factor as several canons were sympathetic to Erasmian reform. In addition, his opposition to the French and to mercenary service was welcomed by Zurich politicians. On 11 December 1518, the canons elected Zwingli to become the stipendiary priest and on 27 December he moved permanently to Zurich.
Zurich ministry begins (1519–1521).
On 1 January 1519, Zwingli gave his first sermon in Zurich. Deviating from the prevalent practice of basing a sermon on the Gospel lesson of a particular Sunday, Zwingli, using Erasmus' New Testament as a guide, began to read through the Gospel of Matthew, giving his interpretation during the sermon, known as the method of "lectio continua". He continued to read and interpret the book on subsequent Sundays until he reached the end and then proceeded in the same manner with the Acts of the Apostles, the New Testament epistles, and finally the Old Testament. His motives for doing this are not clear, but in his sermons he used exhortation to achieve moral and ecclesiastical improvement which were goals comparable with Erasmian reform. Sometime after 1520, Zwingli's theological model began to evolve into an idiosyncratic form that was neither Erasmian nor Lutheran. Scholars do not agree on the process of how he developed his own unique model. One view is that Zwingli was trained as an Erasmian humanist and Luther played a decisive role in changing his theology. Another view is that Zwingli did not pay much attention to Luther's theology and in fact he considered it as part of the humanist reform movement. A third view is that Zwingli was not a complete follower of Erasmus, but had diverged from him as early as 1516 and that he independently developed his theology. 
Zwingli's theological stance was gradually revealed through his sermons. He attacked moral corruption and in the process he named individuals who were the targets of his denunciations. Monks were accused of indolence and high living. In 1519, Zwingli specifically rejected the veneration of saints and called for the need to distinguish between their true and fictional accounts. He cast doubts on hellfire, asserted that unbaptised children were not damned, and questioned the power of excommunication. His attack on the claim that tithing was a divine institution, however, had the greatest theological and social impact. This contradicted the immediate economic interests of the foundation. One of the elderly canons who had supported Zwingli's election, Konrad Hofmann, complained about his sermons in a letter. Some canons supported Hofmann, but the opposition never grew very large. Zwingli insisted that he was not an innovator and that the sole basis of his teachings was Scripture.
Within the diocese of Constance, Bernhardin Sanson was offering a special indulgence for contributors to the building of St Peter's in Rome. When Sanson arrived at the gates of Zurich at the end of January 1519, parishioners prompted Zwingli with questions. He responded with displeasure that the people were not being properly informed about the conditions of the indulgence and were being induced to part with their money on false pretences. This was over a year after Martin Luther published his Ninety-five theses (31 October 1517). The council of Zurich refused Sanson entry into the city. As the authorities in Rome were anxious to contain the fire started by Luther, the Bishop of Constance denied any support of Sanson and he was recalled.
In August 1519, Zurich was struck by an outbreak of the plague during which at least one in four persons died. All of those who could afford it left the city, but Zwingli remained and continued his pastoral duties. In September, he caught the disease and nearly died. He described his preparation for death in a poem, Zwingli's "Pestlied", consisting of three parts: the onset of the illness, the closeness to death, and the joy of recovery. The final verses of the first part read:
In the years following his recovery, Zwingli's opponents remained in the minority. When a vacancy occurred among the canons of the Grossmünster, Zwingli was elected to fulfill that vacancy on 29 April 1521. In becoming a canon, he became a full citizen of Zurich. He also retained his post as the people's priest of the Grossmünster.
First rifts (1522–1524).
The first public controversy regarding Zwingli's preaching broke out during the season of Lent in 1522. On the first fasting Sunday, 9 March, Zwingli and about a dozen other participants consciously transgressed the fasting rule by cutting and distributing two smoked sausages (the "Wurstessen" in Christoph Froschauer's workshop). Zwingli defended this act in a sermon which was published on 16 April, under the title "Von Erkiesen und Freiheit der Speisen" (Regarding the Choice and Freedom of Foods). He noted that no general valid rule on food can be derived from the Bible and that to transgress such a rule is not a sin. The event, which came to be referred to as the Affair of the Sausages, is considered to be the start of the Reformation in Switzerland. Even before the publication of this treatise, the diocese of Constance reacted by sending a delegation to Zurich. The city council condemned the fasting violation, but assumed responsibility over ecclesiastical matters and requested the religious authorities clarify the issue. The bishop responded on 24 May by admonishing the Grossmünster and city council and repeating the traditional position.
Following this event, Zwingli and other humanist friends petitioned the bishop on 2 July to abolish the requirement of celibacy on the clergy. Two weeks later the petition was reprinted for the public in German as "Eine freundliche Bitte und Ermahnung an die Eidgenossen" (A Friendly Petition and Admonition to the Confederates). The issue was not just an abstract problem for Zwingli, as he had secretly married a widow, Anna Reinhard, earlier in the year. Their cohabitation was well-known and their public wedding took place on 2 April 1524, three months before the birth of their first child. They would eventually have four children: Regula, William, Huldrych, and Anna. As the petition was addressed to the secular authorities, the bishop responded at the same level by notifying the Zurich government to maintain the ecclesiastical order. Other Swiss clergymen joined in Zwingli's cause which encouraged him to make his first major statement of faith, "Apologeticus Archeteles" (The First and Last Word). He defended himself against charges of inciting unrest and heresy. He denied the ecclesiastical hierarchy any right to judge on matters of church order because of its corrupted state.
Zurich disputations (1523).
The events of 1522 brought no clarification on the issues. Not only did the unrest between Zurich and the bishop continue, tensions were growing among Zurich's Confederation partners in the Swiss Diet. On 22 December, the Diet recommended that its members prohibit the new teachings, a strong indictment directed at Zurich. The city council felt obliged to take the initiative and find its own solution.
First Disputation.
On 3 January 1523, the Zurich city council invited the clergy of the city and outlying region to a meeting to allow the factions to present their opinions. The bishop was invited to attend or to send a representative. The council would render a decision on who would be allowed to continue to proclaim their views. This meeting, the first Zurich disputation, took place on 29 January 1523.
The meeting attracted a large crowd of approximately six hundred participants. The bishop sent a delegation led by his vicar general, Johannes Fabri. Zwingli summarised his position in the "Schlussreden" (Concluding Statements or the Sixty-seven Articles). Fabri, who had not envisaged an academic disputation in the manner Zwingli had prepared for, was forbidden to discuss high theology before laymen, and simply insisted on the necessity of the ecclesiastical authority. The decision of the council was that Zwingli would be allowed to continue his preaching and that all other preachers should teach only in accordance with Scripture.
Second Disputation.
In September 1523, Leo Jud, Zwingli's closest friend and colleague and pastor of St. Peterskirche, publicly called for the removal of statues of saints and other icons. This led to demonstrations and iconoclastic activities. The city council decided to work out the matter of images in a second disputation. The essence of the mass and its sacrificial character was also included as a subject of discussion. Supporters of the mass claimed that the eucharist was a true sacrifice, while Zwingli claimed that it was a commemorative meal. As in the first disputation, an invitation was sent out to the Zurich clergy and the bishop of Constance. This time, however, the lay people of Zurich, the dioceses of Chur and Basel, the University of Basel, and the twelve members of the Confederation were also invited. About nine hundred persons attended this meeting, but neither the bishop nor the Confederation sent representatives. The disputation started on 26 October 1523 and lasted two days.
Zwingli again took the lead in the disputation. His opponent was the aforementioned canon, Konrad Hofmann, who had initially supported Zwingli's election. Also taking part was a group of young men demanding a much faster pace of reformation, who among other things pleaded for replacing infant baptism with adult baptism. This group was led by Conrad Grebel, one of the initiators of the Anabaptist movement. During the first three days of dispute, although the controversy of images and the mass were discussed, the arguments led to the question of whether the city council or the ecclesiastical government had the authority to decide on these issues. At this point, Konrad Schmid, a priest from Aargau and follower of Zwingli, made a pragmatic suggestion. As images were not yet considered to be valueless by everyone, he suggested that pastors preach on this subject under threat of punishment. He believed the opinions of the people would gradually change and the voluntary removal of images would follow. Hence, Schmid rejected the radicals and their iconoclasm, but supported Zwingli's position. In November the council passed ordinances in support of Schmid's motion. Zwingli wrote a booklet on the evangelical duties of a minister, "Kurze, christliche Einleitung" (Short Christian Introduction), and the council sent it out to the clergy and the members of the Confederation.
Reformation progresses in Zurich (1524–1525).
In December 1523, the council set a deadline of Pentecost in 1524 for a solution to the elimination of the mass and images. Zwingli gave a formal opinion in "Vorschlag wegen der Bilder und der Messe" (Proposal Concerning Images and the Mass). He did not urge an immediate, general abolition. The council decided on the orderly removal of images within Zurich, but rural congregations were granted the right to remove them based on majority vote. The decision on the mass was postponed.
Evidence of the effect of the Reformation was seen in early 1524. Candlemas was not celebrated, processions of robed clergy ceased, worshippers did not go with palms or relics on Palm Sunday to the Lindenhof, and triptychs remained covered and closed after Lent. Opposition to the changes came from Konrad Hofmann and his followers, but the council decided in favour of keeping the government mandates. When Hofmann left the city, opposition from pastors hostile to the Reformation broke down. The bishop of Constance tried to intervene in defending the mass and the veneration of images. Zwingli wrote an official response for the council and the result was the severance of all ties between the city and the diocese.
Although the council had hesitated in abolishing the mass, the decrease in the exercise of traditional piety allowed pastors to be unofficially released from the requirement of celebrating mass. As individual pastors altered their practices as each saw fit, Zwingli was prompted to address this disorganised situation by designing a communion liturgy in the German language. This was published in "Aktion oder Brauch des Nachtmahls" (Act or Custom of the Supper). Shortly before Easter, Zwingli and his closest associates requested the council to cancel the mass and to introduce the new public order of worship. On Maundy Thursday, 13 April 1525, Zwingli celebrated communion under his new liturgy. Wooden cups and plates were used to avoid any outward displays of formality. The congregation sat at set tables to emphasise the meal aspect of the sacrament. The sermon was the focal point of the service and there was no organ music or singing. The importance of the sermon in the worship service was underlined by Zwingli's proposal to limit the celebration of communion to four times a year.
For some time Zwingli had accused mendicant orders of hypocrisy and demanded their abolition in order to support the truly poor. He suggested the monasteries be changed into hospitals and welfare institutions and incorporate their wealth into a welfare fund. This was done by reorganising the foundations of the Grossmünster and Fraumünster and pensioning off remaining nuns and monks. The council secularised the church properties and established new welfare programs for the poor. Zwingli requested permission to establish a Latin school, the "Prophezei" (Prophecy) or "Carolinum", at the Grossmünster. The council agreed and it was officially opened on 19 June 1525 with Zwingli and Jud as teachers. It served to retrain and re-educate the clergy. The Zurich Bible translation, traditionally attributed to Zwingli and printed by Christoph Froschauer, bears the mark of teamwork from the Prophecy school. Scholars have not yet attempted to clarify Zwingli's share of the work based on external and stylistic evidence.
Conflict with the Anabaptists (1525–1527).
Shortly after the second Zurich disputation, many in the radical wing of the Reformation became convinced that Zwingli was making too many concessions to the Zurich council. They rejected the role of civil government and demanded the immediate establishment of a congregation of the faithful. Conrad Grebel, the leader of the radicals and the emerging Anabaptist movement, spoke disparagingly of Zwingli in private. On 15 August 1524 the council insisted on the obligation to baptise all newborn infants. Zwingli secretly conferred with Grebel's group and late in 1524, the council called for official discussions. When talks were broken off, Zwingli published "Wer Ursache gebe zu Aufruhr" (Whoever Causes Unrest) clarifying the opposing points-of-view. On 17 January 1525 a public debate was held and the council decided in favour of Zwingli. Anyone refusing to have their children baptised was required to leave Zurich. The radicals ignored these measures and on 21 January, they met at the house of the mother of another radical leader, Felix Manz. Grebel and a third leader, George Blaurock, performed the first recorded Anabaptist adult baptisms.
On February 2, the council repeated the requirement on the baptism of all babies and some who failed to comply were arrested and fined, Manz and Blaurock among them. Zwingli and Jud interviewed them and more debates were held before the Zurich council. Meanwhile, the new teachings continued to spread to other parts of the Confederation as well as a number of Swabian towns. On 6–8 November, the last debate on the subject of baptism took place in the Grossmünster. Grebel, Manz, and Blaurock defended their cause before Zwingli, Jud, and other reformers. There was no serious exchange of views as each side would not move from their positions and the debates degenerated into an uproar, each side shouting abuse at the other.
The Zurich council decided that no compromise was possible. On 7 March 1526 it released the notorious mandate that no one shall rebaptise another under the penalty of death. Although Zwingli, technically, had nothing to do with the mandate, there is no indication that he disapproved. Felix Manz, who had sworn to leave Zurich and not to baptise any more, had deliberately returned and continued the practice. After he was arrested and tried, he was executed on 5 January 1527 by being drowned in the Limmat river. He was the first Anabaptist martyr; three more were to follow, after which all others either fled or were expelled from Zurich.
Reformation in the Confederation (1526–1528).
On 8 April 1524, five cantons, Lucerne, Uri, Schwyz, Unterwalden, and Zug, formed an alliance, "die fünf Orte" (the Five States) to defend themselves from Zwingli's Reformation. They contacted the opponents of Martin Luther including John Eck, who had debated Luther in the Leipzig Disputation of 1519. Eck offered to dispute Zwingli and he accepted. However, they could not agree on the selection of the judging authority, the location of the debate, and the use of the Swiss Diet as a court. Because of the disagreements, Zwingli decided to boycott the disputation. On 19 May 1526, all the cantons sent delegates to Baden. Although Zurich's representatives were present, they did not participate in the sessions. Eck led the Catholic party while the reformers were represented by Johannes Oecolampadius of Basel, a theologian from Württemberg who had carried on an extensive and friendly correspondence with Zwingli. While the debate proceeded, Zwingli was kept informed of the proceedings and printed pamphlets giving his opinions. It was of little use as the Diet decided against Zwingli. He was to be banned and his writings were no longer to be distributed. Of the thirteen Confederation members, Glarus, Solothurn, Fribourg, and Appenzell as well as the Five States voted against Zwingli. Bern, Basel, Schaffhausen, and Zurich supported him.
The Baden disputation exposed a deep rift in the Confederation on matters of religion. The Reformation was now emerging in other states. The city of St Gallen, an affiliated state to the Confederation, was led by a reformed mayor, Joachim Vadian, and the city abolished the mass in 1527, just two years after Zurich. In Basel, although Zwingli had a close relationship with Oecolampadius, the government did not officially sanction any reformatory changes until 1 April 1529 when the mass was prohibited. Schaffhausen, which had closely followed Zurich's example, formally adopted the Reformation in September 1529. In the case of Bern, Berchtold Haller, the priest at St Vincent Münster, and Niklaus Manuel, the poet, painter, and politician, had campaigned for the reformed cause. But it was only after another disputation that Bern counted itself as a canton of the Reformation. Four hundred and fifty persons participated, including pastors from Bern and other cantons as well as theologians from outside the Confederation such as Martin Bucer and Wolfgang Capito from Strasbourg, Ambrosius Blarer from Constance, and Andreas Althamer from Nuremberg. Eck and Fabri refused to attend and the Catholic cantons did not send representatives. The meeting started on 6 January 1528 and lasted nearly three weeks. Zwingli assumed the main burden of defending the Reformation and he preached twice in the Münster. On 7 February 1528 the council decreed that the Reformation be established in Bern.
First Kappel War (1529).
Even before the Bern disputation, Zwingli was canvassing for an alliance of reformed cities. Once Bern officially accepted the Reformation, a new alliance, "das Christliche Burgrecht" (the Christian Civic Union) was created. The first meetings were held in Bern between representatives of Bern, Constance, and Zurich on 5–6 January 1528. Other cities, including Basel, Biel, Mülhausen, Schaffhausen, and St Gallen, eventually joined the alliance. The Five (Catholic) States felt encircled and isolated, so they searched for outside allies. After two months of negotiations, the Five States formed "die Christliche Vereinigung" (the Christian Alliance) with Ferdinand of Austria on 22 April 1529.
Soon after the Austrian treaty was signed, a reformed preacher, Jacob Kaiser, was captured in Uznach and executed in Schwyz. This triggered a strong reaction from Zwingli; he drafted "Ratschlag über den Krieg" (Advice About the War) for the government. He outlined justifications for an attack on the Catholic states and other measures to be taken. Before Zurich could implement his plans, a delegation from Bern that included Niklaus Manuel arrived in Zurich. The delegation called on Zurich to settle the matter peacefully. Manuel added that an attack would expose Bern to further dangers as Catholic Valais and the Duchy of Savoy bordered its southern flank. He then noted, "You cannot really bring faith by means of spears and halberds." Zurich, however, decided that it would act alone, knowing that Bern would be obliged to acquiesce. War was declared on 8 June 1529. Zurich was able to raise an army of 30,000 men. The Five States were abandoned by Austria and could raise only 9,000 men. The two forces met near Kappel, but war was averted due to the intervention of Hans Aebli, a relative of Zwingli, who pleaded for an armistice.
Zwingli was obliged to state the terms of the armistice. He demanded the dissolution of the Christian Alliance; unhindered preaching by reformers in the Catholic states; prohibition of the pension system; payment of war reparations; and compensation to the children of Jacob Kaiser. Manuel was involved in the negotiations. Bern was not prepared to insist on the unhindered preaching or the prohibition of the pension system. Zurich and Bern could not agree and the Five (Catholic) States pledged only to dissolve their alliance with Austria. This was a bitter disappointment for Zwingli and it marked his decline in political influence. The first Land Peace of Kappel, "der erste Landfriede", ended the war on 24 June.
Marburg Colloquy (1529).
While Zwingli carried on the political work of the Swiss Reformation, he developed his theological views with his colleagues. The famous disagreement between Luther and Zwingli on the interpretation of the eucharist originated when Andreas Karlstadt, Luther's former colleague from Wittenberg, published three pamphlets on the Lord's Supper in which Karlstadt rejected the idea of a real presence in the elements. These pamphlets, published in Basel in 1524, received the approval of Oecolampadius and Zwingli. Luther rejected Karlstadt's arguments and considered Zwingli primarily to be a partisan of Karlstadt. Zwingli began to express his thoughts on the eucharist in several publications including "de Eucharistia" (On the Eucharist). He attacked the idea of the real presence and argued that the word "is" in the words of the institution—"This is my body, this is my blood"—means "signifies". Hence, the words are understood as a metaphor and Zwingli claimed that there was no real presence during the eucharist. In effect, the meal was symbolic of the Last Supper.
By spring 1527, Luther reacted strongly to Zwingli's views in the treatise "Dass Diese Worte Christi "Das ist mein Leib etc." noch fest stehen wider die Schwarmgeister" (That These Words of Christ "This is My Body etc." Still Stand Firm Against the Fanatics). The controversy continued until 1528 when efforts to build bridges between the Lutheran and the Zwinglian views began. Martin Bucer tried to mediate while Philip of Hesse, who wanted to form a political coalition of all Protestant forces, invited the two parties to Marburg to discuss their differences. This event became known as the Marburg Colloquy.
Zwingli accepted Philip's invitation fully believing that he would be able to convince Luther. By contrast, Luther did not expect anything to come out of the meeting and had to be urged by Philip to attend. Zwingli, accompanied by Oecolampadius, arrived on 28 September 1529 with Luther and Philipp Melanchthon arriving shortly thereafter. Other theologians also participated including Martin Bucer, Andreas Osiander, Johannes Brenz, and Justus Jonas. The debates were held from 1–3 October and the results were published in the fifteen "Marburg Articles". The participants were able to agree on fourteen of the articles, but the fifteenth article established the differences in their views on the presence of Christ in the eucharist. Afterwards, each side was convinced that they were the victors, but in fact the controversy was not resolved and the final result was the formation of two different Protestant confessions.
Politics, confessions, the Kappel Wars, and death (1529–1531).
With the failure of the Marburg Colloquy and the split of the Confederation, Zwingli set his goal on an alliance with Philip of Hesse. He kept up a lively correspondence with Philip. Bern refused to participate, but after a long process, Zurich, Basel, and Strasbourg signed a mutual defence treaty with Philip in November 1530. Zwingli also personally negotiated with France's diplomatic representative, but the two sides were too far apart. France wanted to maintain good relations with the Five States. Approaches to Venice and Milan also failed.
As Zwingli was working on establishing these political alliances, Charles V, the Holy Roman Emperor, invited Protestants to the Augsburg Diet to present their views so that he could make a verdict on the issue of faith. The Lutherans presented the Augsburg Confession. Under the leadership of Martin Bucer, the cities of Strasbourg, Constance, Memmingen, and Lindau produced the Tetrapolitan Confession. This document attempted to take a middle position between the Lutherans and Zwinglians. It was too late for the "Burgrecht" cities to produce a confession of their own. Zwingli then produced his own private confession, "Fidei ratio" (Account of Faith) in which he explained his faith in twelve articles conforming to the articles of the Apostles' Creed. The tone was strongly anti-Catholic as well as anti-Lutheran. The Lutherans did not react officially, but criticised it privately. Zwingli's and Luther's old opponent, John Eck, counter-attacked with a publication, "Refutation of the Articles Zwingli Submitted to the Emperor".
When Philip of Hesse formed the Schmalkaldic League at the end of 1530, the four cities of the Tetrapolitan Confession joined on the basis of a Lutheran interpretation of that confession. Given the flexibility of the league's entrance requirements, Zurich, Basel, and Bern also considered joining. However, Zwingli could not reconcile the Tetrapolitan Confession with his own beliefs and wrote a harsh refusal to Bucer and Capito. This offended Philip to the point where relations with the League were severed. The "Burgrecht" cities now had no external allies to help deal with internal Confederation religious conflicts. 
The peace treaty of the First Kappel War did not define the right of unhindered preaching in the Catholic states. Zwingli interpreted this to mean that preaching should be permitted, but the Five States suppressed any attempts to reform. The "Burgrecht" cities considered different means of applying pressure to the Five States. Basel and Schaffhausen preferred quiet diplomacy while Zurich wanted armed conflict. Zwingli and Jud unequivocally advocated an attack on the Five States. Bern took a middle position which eventually prevailed. In May 1531, Zurich reluctantly agreed to impose a food blockade. It failed to have any effect and in October, Bern decided to withdraw the blockade. Zurich urged its continuation and the "Burgrecht" cities began to quarrel among themselves.
On 9 October 1531, in a surprise move, the Five States declared war on Zurich. Zurich's mobilisation was slow due to internal squabbling and on 11 October, 3500 poorly deployed men encountered a Five States force nearly double their size near Kappel on 11 October. Many pastors, including Zwingli, were among the soldiers. The battle lasted less than one hour and Zwingli was among the 500 casualties in the Zurich army.
Zwingli had considered himself first and foremost a soldier of Christ; second a defender of his country, the Confederation; and third a leader of his city, Zurich, where he had lived for the previous twelve years. Ironically, he died at the age of 47, not for Christ nor for the Confederation, but for Zurich. 
Luther wrote, "It is well that Zwingli, Carlstadt and Pellican lie dead on the battlefield, for otherwise we could not have kept the Landgrave, Strasbourg and others of our neighbors. Oh, what a triumph this is, that they have perished. How well God knows his business." A false report had added Carlstadt and Pellican to the fatalities. Erasmus wrote, "We are freed from great fear by the death of the two preachers, Zwingli and Oecolampadius, whose fate has wrought an incredible change in the mind of many. This is the wonderful hand of God on high." Oecolampadius had died on 24 November. Erasmus also wrote, "If Bellona had favoured them, it would have been all over with us."
Theology.
According to Zwingli, the cornerstone of theology is the Bible. Zwingli appealed to scripture constantly in his writings. He placed its authority above other sources such as the ecumenical councils or the Church Fathers, although he did not hesitate to use other sources to support his arguments. The principles that guide Zwingli's interpretations are derived from his rationalist humanist education and his Reformed understanding of the Bible. He rejected literalist interpretations of a passage, such as those of the Anabaptists, and used synecdoche and analogies, methods he describes in "A Friendly Exegesis" (1527). Two analogies that he used quite effectively were between baptism and circumcision and between the eucharist and Passover. He also paid attention to the immediate context and attempted to understand the purpose behind it, comparing passages of scripture with each other.
Zwingli rejected the word "sacrament" in the popular usage of his time. For ordinary people, the word meant some kind of holy action of which there is inherent power to free the conscience from sin. For Zwingli, a sacrament was an initiatory ceremony or a pledge, pointing out that the word was derived from "sacramentum" meaning an oath. (However, the word is also translated "mystery.") In his early writings on baptism, he noted that baptism was an example of such a pledge. He challenged Catholics by accusing them of superstition when they ascribed the water of baptism a certain power to wash away sin. Later, in his conflict with the Anabaptists, he defended the practice of infant baptism, noting that there is no law forbidding the practice. He argued that baptism was a sign of a covenant with God, thereby replacing circumcision in the Old Testament.
Zwingli approached the eucharist in a similar manner to baptism. During the first Zurich disputation in 1523, he denied that an actual sacrifice occurred during the mass, arguing that Christ made the sacrifice only once and for all eternity. Hence, the eucharist was "a memorial of the sacrifice". Following this argument, he further developed his view, coming to the conclusion of the "signifies" interpretation for the words of the institution. He used various passages of scripture to argue against transubstantiation as well as Luther's views, the key text being John 6:63, "It is the Spirit who gives life, the flesh is of no avail". Zwingli's rational approach and use of scripture to understand the meaning of the eucharist was one reason he could not reach a consensus with Luther.
The impact of Luther on Zwingli's theological development has long been a source of interest and discussion among Zwinglian scholars. Zwingli himself asserted vigorously his independence of Luther. The most recent studies have lent credibility to this claim, although some scholars still claim his theology was dependent upon Luther's. Zwingli appears to have read Luther's books in search of confirmation from Luther for his own views. Zwingli did, however, admire Luther greatly for the stand he took against the pope. This, more than Luther's theology, was a key influence on Zwingli's convictions as a reformer. What Zwingli considered Luther's courageous stance at the Leipzig Disputation had a decisive impact on Zwingli during his earliest years as a priest, and during this time Zwingli praised and promoted Luther's writings to support his own similar ideas. Like Luther, Zwingli was also a student and admirer of Augustine. His later writings continued to show characteristic differences from Luther such as the inclusion of non-Christians in heaven as described in "An Exposition of the Faith".
Music.
Zwingli enjoyed music and could play several instruments, including the violin, harp, flute, dulcimer and hunting horn. He would sometimes amuse the children of his congregation on his lute and was so well known for his playing that his enemies mocked him as "the evangelical lute-player and fifer". Three of Zwingli's "Lieder" or hymns have been preserved: the "Pestlied" mentioned above, an adaptation of Psalm 65 (c. 1525), and the "Kappeler Lied", which is believed to have been composed during the campaign of the first war of Kappel (1529). These songs were not meant to be sung during worship services and are not identified as hymns of the Reformation, though they were published in some 16th-century hymnals.
Zwingli criticised the practice of priestly chanting and monastic choirs. The criticism dates from 1523 when he attacked certain worship practices. He associated music with images and vestments, all of which he felt diverted people's attention from true spiritual worship. It is not known what he thought of the musical practices in early Lutheran churches. Zwingli, however, eliminated instrumental music from worship in the church, stating that God had not commanded it in worship. The organist of the People's Church in Zurich is recorded as weeping upon seeing the great organ broken up. Although Zwingli did not express an opinion on congregational singing, he made no effort to encourage it. Nevertheless, scholars have found that Zwingli was supportive of a role for music in the church. Gottfried W. Locher writes, "The old assertion 'Zwingli was against church singing' holds good no longer ... Zwingli's polemic is concerned exclusively with the medieval Latin choral and priestly chanting and not with the hymns of evangelical congregations or choirs". Locher goes on to say that "Zwingli freely allowed vernacular psalm or choral singing. In addition, he even seems to have striven for lively, antiphonal, unison recitative". Locher then summarizes his comments on Zwingli's view of church music as follows: "The chief thought in his conception of worship was always 'conscious attendance and understanding'—'devotion', yet with the lively participation of all concerned".
The as of today Musikabteilung (literally: music departement), located in the choir of the "Predigern" church in Zürich was founded in 1971, being a scientific music collection of European importance. It publishes the materials entrusted to it at irregular intervals as CD's, the repertoire ranges of early 16th-century spiritual mucic of Huldrych Zwingli's to the late 20th century, published under the label "Musik aus der Zentralbibliothek Zürich".
Legacy.
Zwingli was a humanist and a scholar with many devoted friends and disciples. He communicated as easily with the ordinary people of his congregation as with rulers such as Philip of Hesse. His reputation as a stern, stolid reformer is counterbalanced by the fact that he had an excellent sense of humour and used satiric fables, spoofing, and puns in his writings. He was more conscious of social obligations than Luther and he genuinely believed that the masses would accept a government guided by God's word. He tirelessly promoted assistance to the poor, whom he believed should be cared for by a truly Christian community.
In December 1531, the Zurich council selected Heinrich Bullinger as his successor. He immediately removed any doubts about Zwingli's orthodoxy and defended him as a prophet and a martyr. During Bullinger's rule, the confessional divisions of the Confederation were stabilised. He rallied the reformed cities and cantons and helped them to recover from the defeat at Kappel. Zwingli had instituted fundamental reforms, while Bullinger consolidated and refined them.
Scholars have found it difficult to assess Zwingli's impact on history, for several reasons. There is no consensus on the definition of "Zwinglianism"; by any definition, Zwinglianism evolved under his successor, Heinrich Bullinger; and research into Zwingli's influence on Bullinger and John Calvin is still rudimentary. Bullinger adopted most of Zwingli's points of doctrine. Like Zwingli, he summarised his theology several times, the best-known being the Second Helvetic Confession of 1566. Meanwhile, Calvin had taken over the Reformation in Geneva. Calvin differed with Zwingli on the eucharist and criticised him for regarding it as simply a metaphorical event. In 1549, however, Bullinger and Calvin succeeded in overcoming the differences in doctrine and produced the "Consensus Tigurinus" (Zurich Consensus). They declared that the eucharist was not just symbolic of the meal, but they also rejected the Lutheran position that the body and blood of Christ is in union with the elements. With this rapprochement, Calvin established his role in the Swiss Reformed Churches and eventually in the wider world.
Outside of Switzerland, no church counts Zwingli as its founder. Scholars speculate as to why Zwinglianism has not diffused more widely, even though Zwingli's theology is considered the first expression of Reformed theology. Although his name is not widely recognised, Zwingli's legacy lives on in the basic confessions of the Reformed churches of today. He is often called, after Martin Luther and John Calvin, the "Third Man of the Reformation".
List of works.
Zwingli's collected works are expected to fill 21 volumes. A collection of selected works was published in 1995 by the "Zwingliverein" in collaboration with the "Theologischer Verlag Zürich" This four-volume collection contains the following works:
The complete 21-volume edition is being undertaken by the "Zwingliverein" in collaboration with the "Institut für schweizerische Reformationsgeschichte", and is projected to be organised as follows:
Vols. XIII and XIV have been published, vols. XV and XVI are under preparation. Vols. XVII to XXI are planned to cover the New Testament.
Older German / Latin editions available online include:
See also the following English translations of selected works by Zwingli:

</doc>
<doc id="13603" url="http://en.wikipedia.org/wiki?curid=13603" title="Homeschooling">
Homeschooling

Homeschooling, also known as home education, is the education of children inside the home, as opposed to in the formal settings of a public or private school. Home education is usually conducted by a parent or tutor. Many families that start out with a formal school structure at home often switch to less formal and, in their view, more effective ways of imparting education outside of school, and many prefer the term "home education" to the more prevalently used "homeschooling". "Homeschooling" is the term commonly used in North America, whereas "home education" is more commonly used in the United Kingdom, elsewhere in Europe, and in many Commonwealth countries.
Prior to the introduction of compulsory school attendance laws, most childhood education was imparted by the family or community. However, in developed countries, homeschooling in the modern sense is an alternative to attending public or private schools, and is a legal option for parents in many countries.
Parents cite two main motivations for homeschooling their children: dissatisfaction with the local schools and the interest in increased involvement with (and greater control over) their children's learning and development. Parents' dissatisfaction with available schools includes concerns about the school environment, the quality of academic instruction, the curriculum, and bullying as well as lack of faith in the school's ability to cater to their child's special needs. Some parents homeschool in order to have greater control over what and how their children are taught, to better cater for children's individual aptitudes and abilities adequately, to provide a specific religious or moral instruction, and to take advantage of the efficiency of one-to-one instruction, which allows the child to spend more time on childhood activities, socializing, and non-academic learning. Many parents are also influenced by alternative educational philosophies espoused by the likes of Susan Sutherland Isaacs, Charlotte Mason, John Holt, and Sir Kenneth Robinson, among others.
Homeschooling may also be a factor in the choice of parenting style. Homeschooling can be an option for families living in isolated rural locations, for those temporarily abroad, and for those who travel frequently. Many young athletes, actors, and musicians are taught at home to better accommodate their training and practice schedules. Homeschooling can be about mentorship and apprenticeship, in which a tutor or teacher is with the child for many years and gets to know the child very well. Recently, homeschooling has increased in popularity in the United States, and the percentage of children ages 5 through 17 who are homeschooled increased from 1.7% in 1999 to 2.9% in 2007.
Homeschooling can be used as a form of supplemental education and as a way of helping children learn under specific circumstances. The term may also refer to instruction in the home under the supervision of correspondence schools or umbrella schools. In some places, an approved curriculum is legally required if children are homeschooled. A curriculum-free philosophy of homeschooling is sometimes called "unschooling", a term coined in 1977 by American educator and author John Holt in his magazine "Growing Without Schooling". In some cases, a liberal arts education is provided using the trivium and quadrivium as the main models.
History.
For much of history and in many cultures, enlisting professional teachers (whether as tutors or in a formal academic setting) was an option available only to the elite social classes. Thus, until relatively recently, the vast majority of people, especially during early childhood, were educated by family members, family friends, or anyone with useful knowledge.
The earliest public schools in modern Western culture were established in the early 16th century in the German states of Gotha and Thuringia. However, even in the 18th century, the majority of people in Europe lacked formal schooling, meaning they were homeschooled, tutored, or received no education at all. Regional differences in schooling existed in colonial America; in the south, farms and plantations were so widely dispersed that community schools such as those in the more compact settlements were impossible. In the middle colonies, the educational situation varied when comparing New York with New England until the 1850s. Formal schooling in a classroom setting has been the most common means of schooling throughout the world, especially in developed countries, since the early- and mid-19th century. Native Americans, who traditionally used homeschooling and apprenticeship, vigorously resisted compulsory education in the United States.
In the 1960s, Rousas John Rushdoony began to advocate homeschooling, which he saw as a way to combat the intentionally secular nature of the public school system in the United States. He vigorously attacked progressive school reformers such as Horace Mann and John Dewey, and argued for the dismantling of the state's influence in education in three works: "Intellectual Schizophrenia", a general and concise study of education, "The Messianic Character of American Education", a history and castigation of public education in the U.S., and "The Philosophy of the Christian Curriculum", a parent-oriented pedagogical statement. Rushdoony was frequently called as an expert witness by the Home School Legal Defense Association (HSLDA) in court cases.
During this time, American educational professionals Raymond and Dorothy Moore began to research the academic validity of the rapidly growing Early Childhood Education movement. This research included independent studies by other researchers and a review of over 8,000 studies bearing on early childhood education and the physical and mental development of children.
They asserted that formal schooling before ages 8–12 not only lacked the anticipated effectiveness, but also harmed children. The Moores published their view that formal schooling was damaging young children academically, socially, mentally, and even physiologically. The Moores presented evidence that childhood problems such as juvenile delinquency, nearsightedness, increased enrollment of students in special education classes and behavioral problems were the result of increasingly earlier enrollment of students. The Moores cited studies demonstrating that orphans who were given surrogate mothers were measurably more intelligent, with superior long-term effects – even though the mothers were "mentally retarded teenagers" – and that illiterate tribal mothers in Africa produced children who were socially and emotionally more advanced than typical western children, "by western standards of measurement".
Their primary assertion was that the bonds and emotional development made at home with parents during these years produced critical long-term results that were cut short by enrollment in schools, and could neither be replaced nor corrected in an institutional setting afterward. Recognizing a necessity for early out-of-home care for some children, particularly special needs and impoverished children and children from exceptionally inferior homes, they maintained that the vast majority of children were far better situated at home, even with mediocre parents, than with the most gifted and motivated teachers in a school setting. They described the difference as follows: "This is like saying, if you can help a child by taking him off the cold street and housing him in a warm tent, then warm tents should be provided for "all" children – when obviously most children already have even more secure housing."
Like Holt, the Moores embraced homeschooling after the publication of their first work, "Better Late Than Early", in 1975, and went on to become important homeschool advocates and consultants with the publication of books such as "Home Grown Kids", 1981, "Homeschool Burnout" and others.
Simultaneously, other authors published books questioning the premises and efficacy of compulsory schooling, including "Deschooling Society" by Ivan Illich, 1970 and "No More Public School" by Harold Bennet, 1972.
In 1976, Holt published "Instead of Education; Ways to Help People Do Things Better". In its conclusion, he called for a "Children's Underground Railroad" to help children escape compulsory schooling. In response, Holt was contacted by families from around the U.S. to tell him that they were educating their children at home. In 1977, after corresponding with a number of these families, Holt began producing "Growing Without Schooling", a newsletter dedicated to home education.
In 1980, Holt said, "I want to make it clear that I don't see homeschooling as some kind of answer to badness of schools. I think that the home is the proper base for the exploration of the world which we call learning or education. Home would be the best base no matter how good the schools were." Holt later wrote a book about homeschooling, "Teach Your Own", in 1981.
One common theme in the homeschool philosophies of both Holt and that of the Moores is that home education should not attempt to bring the school construct into the home, or be seen as a view of education as an academic preliminary to life. They viewed home education as a natural, experiential aspect of life that occurs as the members of the family are involved with one another in daily living.
Methodology.
Homeschools use a wide variety of methods and materials. Families, for a variety of reasons (parent education, finances, educational philosophies, future educational plans, where they live, past educational experiences of the child, child’s interests and temperament) choose different educational methods, which represent a variety of educational philosophies and paradigms.
Some of the methods used include Classical education (including Trivium, Quadrivium), Charlotte Mason education, Montessori method, Theory of multiple intelligences, Unschooling, Radical Unschooling, Waldorf education, School-at-home (curriculum choices from both secular and religious publishers), A Thomas Jefferson Education, unit studies, curriculum made up from private or small publishers, apprenticeship, hands-on-learning, distance learning (both on-line and correspondence), dual enrollment in local schools or colleges, and curriculum provided by local schools and many others. Some of these approaches are used in private and public schools. Educational research and studies support the use of some of these methods. Unschooling, natural learning, Charlotte Mason Education, Montessori, Waldorf, apprenticeship, hands-on-learning, unit studies are supported to varying degrees by research by constructivist learning theories and situated cognitive theories. Elements of these theories may be found in the other methods as well.
A student’s education may be customized to support his or her learning level, style, and interests. It is not uncommon for a student to experience more than one approach as the family discovers what works best as the students grow and their circumstances change. Many families use an eclectic approach, picking and choosing from various suppliers. For sources of curricula and books, "Homeschooling in the United States: 2003" found that 78 percent utilized "a public library"; 77 percent used "a homeschooling catalog, publisher, or individual specialist"; 68 percent used "retail bookstore or other store"; 60 percent used "an education publisher that was not affiliated with homeschooling." "Approximately half" used curriculum or books from "a homeschooling organization", 37 percent from a "church, synagogue or other religious institution" and 23 percent from "their local public school or district." 41 percent in 2003 utilized some sort of distance learning, approximately 20 percent by "television, video or radio"; 19 percent via "Internet, e-mail, or the World Wide Web"; and 15 percent taking a "correspondence course by mail designed specifically for homeschoolers."
Individual governmental units, e. g. states and local districts, vary in official curriculum and attendance requirements.
Unit studies.
In a unit study approach, multiple subjects such as math, science, history, art, and geography, are studied in relation to a single topic such as Native Americans, ancient Rome, or whales. For example, a unit study of Native Americans might combine age-appropriate lessons and projects teaching literature (Native American legends), writing (report on a famous native American), vocabulary and spelling (Native American words that are now part of the English language), art and crafts (pottery, beadwork, sand painting, making moccasins), geography (original locations of tribes in the Americas), social studies (cultures of the different tribes), and science (plants and animals used by Native Americans). Unit studies may be purchased or be parent prepared. Unit studies are useful for teaching multiple grades simultaneously as the difficulty level can be adjusted for each student. An extended form of unit studies, Integrated Thematic Instruction utilizes one central theme integrated throughout the curriculum so that students finish a school year with a deep understanding of a certain broad subject or idea.
All-in-one curricula.
All-in-one homeschooling curricula (variously known as "school-at-home", "The Traditional Approach", "school-in-a-box" or "The Structured Approach"), are instructionist methods of teaching in which the curriculum and homework of the student are similar or identical to those used in a public or private school. Purchased as a grade level package or separately by subject, the package may contain all of the needed books, materials, internet access for remote testing, traditional tests, answer keys, and extensive teacher guides. These materials cover the same subject areas as do public schools, allowing for an easy transition back into the school system. These are among the more expensive options for homeschooling, but they require minimal preparation and are easy to use. Examples of curriculum providers are K12.com, Calvert School, A Beka Book, Bob Jones Press, Alpha Omega Publications, Educator’s Publishing Service, My Father's World, Sonlight (the original literature-based homeschool company), Modern Curriculum Press, University of North Dakota Distance Education, etc. Some localities provide the same materials used at local schools to homeschoolers. The purchase of a complete curriculum and their teaching/grading service from an accredited distance learning curriculum provider may allow students to obtain an accredited high school diploma.
Unschooling and natural learning.
Some people use the terms "unschooling" or "radical unschooling" to describe all methods of education that are not based in a school.
"Natural learning" refers to a type of learning-on-demand where children pursue knowledge based on their interests and parents take an active part in facilitating activities and experiences conducive to learning but do not rely heavily on textbooks or spend much time "teaching", looking instead for "learning moments" throughout their daily activities. Parents see their role as that of affirming through positive feedback and modeling the necessary skills, and the child's role as being responsible for asking and learning.
The term "unschooling" as coined by John Holt describes an approach in which parents do not authoritatively direct the child's education, but interact with the child following the child's own interests, leaving them free to explore and learn as their interests lead. "Unschooling" does not indicate that the child is not being educated, but that the child is not being "schooled", or educated in a rigid school-type manner. Holt asserted that children learn through the experiences of life, and he encouraged parents to live their lives with their child. Also known as interest-led or child-led learning, unschooling attempts to follow opportunities as they arise in real life, through which a child will learn without coercion. An unschooled child may utilize texts or classroom instruction, but these are not considered central to education. Holt asserted that there is no specific body of knowledge that is, or should be, required of a child.
"Unschooling" should not be confused with "deschooling," which may be used to indicate an anti-"institutional school" philosophy, or a period or form of deprogramming for children or parents who have previously been schooled.
Both unschooling and natural learning advocates believe that children learn best by doing; a child may learn reading to further an interest about history or other cultures, or math skills by operating a small business or sharing in family finances. They may learn animal husbandry keeping dairy goats or meat rabbits, botany tending a kitchen garden, chemistry to understand the operation of firearms or the internal combustion engine, or politics and local history by following a zoning or historical-status dispute. While any type of homeschoolers may also use these methods, the unschooled child initiates these learning activities. The natural learner participates with parents and others in learning together.
Another prominent proponent of unschooling is John Taylor Gatto, author of Dumbing Us Down, The Exhausted School, A Different Kind of Teacher, and Weapons of Mass Instruction. Gatto argues that public education is the primary tool of "state controlled consciousness" and serves as a prime illustration of the total institution — a social system which impels obedience to the state and quells free thinking or dissent.
Autonomous learning.
"Autonomous learning" is a school of education which sees learners as individuals who can and should be i.e. be responsible for their own learning climate.
Autonomous education helps students develop their self-consciousness, vision, practicality and freedom of discussion. These attributes serve to aid the student in his/her independent learning.
Autonomous learning is very popular with those who home educate their children. The child usually gets to decide what projects they wish to tackle or what interests to pursue. In home education this can be instead of or in addition to regular subjects like doing math or English.
According to the autonomous education philosophy emerged from the epistemology of Karl Popper in "The Myth of the Framework: In Defence of Science and Rationality", which is developed in the debates, which seek to rebut the neo-Marxist social philosophy of convergence proposed by the Frankfurt School (e.g. Theodor W. Adorno, Jürgen Habermas, Max Horkheimer).
Homeschool cooperatives.
A Homeschool Cooperative is a cooperative of families who homeschool their children. It provides an opportunity for children to learn from other parents who are more specialized in certain areas or subjects. Co-ops also provide social interaction for homeschooled children. They may take lessons together or go on field trips. Some co-ops also offer events such as prom and graduation for homeschoolers.
Homeschoolers are beginning to utilize Web 2.0 as a way to simulate homeschool cooperatives online. With social networks homeschoolers can chat, discuss threads in forums, share information and tips, and even participate in online classes via blackboard systems similar to those used by colleges.
Research.
Supportive.
Test results.
According to the Home School Legal Defense Association (HSLDA) in 2004, "Many studies over the last few years have established the academic excellence of homeschooled children." Homeschooling Achievement—a compilation of studies published by the HSLDA—supported the academic integrity of homeschooling. This booklet summarized a 1997 study by Ray and the 1999 Rudner study. The Rudner study noted two limitations of its own research: it is not necessarily representative of all homeschoolers and it is not a comparison with other schooling methods. Among the homeschooled students who took the tests, the average homeschooled student outperformed his public school peers by 30 to 37 percentile points across all subjects. The study also indicates that public school performance gaps between minorities and genders were virtually non-existent among the homeschooled students who took the tests.
A study conducted in 2008 found that 11,739 homeschooled students, on average, scored 37 percentile points above public school students on standardized achievement tests. This is consistent with the Rudner study (1999). However, Rudner has said that these same students in public school may have scored just as well because of the dedicated parents they had. The Ray study also found that homeschooled students who had a certified teacher as a parent scored one percentile lower than homeschooled students who did not have a certified teacher as a parent.
In 2011, Martin-Chang found that unschooling children ages 5–10 scored significantly below traditionally educated children, while academically oriented homeschooled children scored from one half grade level above to 4.5 grade levels above traditionally schooled children on standardized tests (n=37 home schooled children matched with children from the same socioeconomic and educational background).
In the 1970s, Raymond S. and Dorothy N. Moore conducted four federally funded analyses of more than 8,000 early childhood studies, from which they published their original findings in "Better Late Than Early", 1975. This was followed by "School Can Wait", a repackaging of these same findings designed specifically for educational professionals. They concluded that, "where possible, children should be withheld from formal schooling until at least ages eight to ten." Their reason was that children "are not mature enough for formal school programs until their senses, coordination, neurological development and cognition are ready". They concluded that the outcome of forcing children into formal schooling is a sequence of "1) uncertainty as the child leaves the family nest early for a less secure environment, 2) puzzlement at the new pressures and restrictions of the classroom, 3) frustration because unready learning tools – senses, cognition, brain hemispheres, coordination – cannot handle the regimentation of formal lessons and the pressures they bring, 4) hyperactivity growing out of nerves and jitter, from frustration, 5) failure which quite naturally flows from the four experiences above, and 6) delinquency which is failure's twin and apparently for the same reason." According to the Moores, "early formal schooling is burning out our children. Teachers who attempt to cope with these youngsters also are burning out." Aside from academic performance, they think early formal schooling also destroys "positive sociability", encourages peer dependence, and discourages self-worth, optimism, respect for parents, and trust in peers. They believe this situation is particularly acute for boys because of their delay in maturity.
The Moores cited a Smithsonian Report on the development of genius, indicating a requirement for "1) much time spent with warm, responsive parents and other adults, 2) very little time spent with peers, and 3) a great deal of free exploration under parental guidance." Their analysis suggested that children need "more of home and less of formal school", "more free exploration with... parents, and fewer limits of classroom and books", and "more old fashioned chores – children working with parents – and less attention to rivalry sports and amusements."
Socialization.
Using the Piers-Harris Children's Self-Concept Scale, John Taylor later found that, "while half of the conventionally schooled children scored at or below the 50th percentile (in self-concept), only 10.3% of the home-schooling children did so." He further stated that "the self-concept of home-schooling children is significantly higher statistically than that of children attending conventional school. This has implications in the areas of academic achievement and socialization which have been found to parallel self-concept. Regarding socialization, Taylor's results would mean that very few home-schooling children are socially deprived. He states that critics who speak out against homeschooling on the basis of social deprivation are actually addressing an area which favors homeschoolers.
In 2003, the National Home Education Research Institute conducted a survey of 7,300 U.S. adults who had been homeschooled (5,000 for more than seven years). Their findings included:
Criticism.
People claim the studies that show that homeschooled students do better on standardized tests compare voluntary homeschool testing with mandatory public-school testing.
By contrast, SAT and ACT tests are self-selected by homeschooled and formally schooled students alike. Homeschoolers averaged higher scores on these college entrance tests in South Carolina. Other scores (1999 data) showed mixed results, for example showing higher levels for homeschoolers in English (homeschooled 23.4 vs national average 20.5) and reading (homeschooled 24.4 vs national average 21.4) on the ACT, but mixed scores in math (homeschooled 20.4 vs national average 20.7 on the ACT as opposed homeschooled 535 vs national average 511 on the 1999 SAT math).
Some advocates of homeschooling and educational choice counter with an input-output theory, pointing out that home educators expend only an average of $500–$600 a year on each student, in comparison to $9,000-$10,000 for each public school student in the United States, which suggests home-educated students would be especially dominant on tests if afforded access to an equal commitment of tax-funded educational resources.
Controversy and criticism.
Opposition to homeschooling comes from some organizations of teachers and school districts. The National Education Association, a United States teachers' union and professional association, opposes homeschooling. Criticisms by such opponents include:
Stanford University political scientist Professor Rob Reich (not to be confused with former U.S. Secretary of Labor Robert Reich) wrote in "The Civic Perils of Homeschooling" (2002) that homeschooling can probably result in biased students, as many homeschooling parents view the education of their children as a matter properly under their control and no one else's. He also claims that most parents choose to educate their children at home because they believe that their children's moral and spiritual needs will not be met in campus-based schools.
Gallup polls of American voters have shown a significant change in attitude in the last 20 years, from 73% opposed to home education in 1985 to 54% opposed in 2001.
Many teachers and school districts oppose the idea of homeschooling. However, research has shown that homeschooled children often excel in many areas of academic endeavor. According to a study done on the homeschool movement, homeschoolers often achieve academic success and admission into elite universities. There is also evidence that most are remarkably well socialized. According to the National Home Education Research Institute president, Brian Ray, socialization is not a problem for homeschooling children, many of whom are involved in community sports, volunteer activities, book groups, or homeschool co-ops.
According to an annual Gallup poll, public opinion is mixed. Respondents who regard homeschooling as a "bad thing" dropped from 73 percent in 1985 to 57 percent in 1997. In 1988, when asked whether parents should have a right to choose homeschooling, 53 percent thought that they should, as revealed by another poll.
International status and statistics.
Homeschooling is legal in some countries. Countries with the most prevalent home education movements include Australia, Canada, New Zealand, the United Kingdom, and the United States. Some countries have highly regulated home education programs as an extension of the compulsory school system; others, such as Sweden and Germany, have outlawed it entirely. Brazil has a law project in process. In other countries, while not restricted by law, homeschooling is not socially acceptable or considered desirable and is virtually non-existent.

</doc>
<doc id="13605" url="http://en.wikipedia.org/wiki?curid=13605" title="Heteroatom">
Heteroatom

In organic chemistry, a heteroatom (from Ancient Greek "heteros", different, + "atomos") is any atom that is not carbon or hydrogen. Usually, the term is used to indicate that non-carbon atoms have replaced carbon in the backbone of the molecular structure. Typical heteroatoms are nitrogen, oxygen, sulfur, phosphorus, chlorine, bromine, and iodine.
In the description of protein structure, in particular in the Protein Data Bank file format, a heteroatom record (HETATM) describes an atom as belonging to a small molecule cofactor rather than to be part of a biopolymer chain.

</doc>
<doc id="13606" url="http://en.wikipedia.org/wiki?curid=13606" title="Half-life">
Half-life

Half-life (t1⁄2) is the amount of time required for a quantity to fall to half its value as measured at the beginning of the time period. While the term "half-life" can be used to describe any quantity which follows an exponential decay, it is most often used within the context of nuclear physics and nuclear chemistry—that is, the time required, probabilistically, for half of the unstable, radioactive atoms in a sample to undergo radioactive decay.
The original term, dating to Ernest Rutherford's discovery of the principle in 1907, was "half-life period", which was shortened to "half-life" in the early 1950s. Rutherford applied the principle of a radioactive element's half-life to studies of age determination of rocks by measuring the decay period of radium to lead-206.
Half-life is used to describe a quantity undergoing exponential decay, and is constant over the lifetime of the decaying quantity. It is a characteristic unit for the exponential decay equation. The term "half-life" may generically be used to refer to any period of time in which a quantity falls by half, even if the decay is not exponential. The table on the right shows the reduction of a quantity in the number of half-lives elapsed. For a general introduction and description of exponential decay, see exponential decay. For a general introduction and description of non-exponential decay, see rate law. The converse of half-life is doubling time.
Probabilistic nature of half-life.
A half-life usually describes the decay of discrete entities, such as radioactive atoms. In that case, it does not work to use the definition "half-life is the time required for exactly half of the entities to decay". For example, if there are 3 radioactive atoms with a half-life of one second, there will not be "1.5 atoms" left after one second.
Instead, the half-life is defined in terms of probability: "Half-life is the time required for exactly half of the entities to decay "on average"". In other words, the "probability" of a radioactive atom decaying within its half-life is 50%.
For example, the image on the right is a simulation of many identical atoms undergoing radioactive decay. Note that after one half-life there are not "exactly" one-half of the atoms remaining, only "approximately", because of the random variation in the process. Nevertheless, when there are many identical atoms decaying (right boxes), the law of large numbers suggests that it is a "very good approximation" to say that half of the atoms remain after one half-life.
There are various simple exercises that demonstrate probabilistic decay, for example involving flipping coins or running a statistical computer program.
Formulas for half-life in exponential decay.
An exponential decay process can be described by any of the following three equivalent formulas:
where
The three parameters formula_4, formula_2, and formula_3 are all directly related in the following way:
where ln(2) is the natural logarithm of 2 (approximately 0.693).
By plugging in and manipulating these relationships, we get all of the following equivalent descriptions of exponential decay, in terms of the half-life:
Regardless of how it's written, we can plug into the formula to get
Decay by two or more processes.
Some quantities decay by two exponential-decay processes simultaneously. In this case, the actual half-life T1⁄2 can be related to the half-lives t1 and t2 that the quantity would have if each of the decay processes acted in isolation:
For three or more processes, the analogous formula is:
For a proof of these formulas, see Exponential decay#Decay by two or more processes.
Examples.
There is a half-life describing any exponential-decay process. For example:
The half life of a species is the time it takes for the concentration of the substance to fall to half of its initial value.
Half-life in non-exponential decay.
The decay of many physical quantities is not exponential—for example, the evaporation of water from a puddle, or (often) the chemical reaction of a molecule. In such cases, the half-life is defined the same way as before: as the time elapsed before half of the original quantity has decayed. However, unlike in an exponential decay, the half-life depends on the initial quantity, and the prospective half-life will change over time as the quantity decays.
As an example, the radioactive decay of carbon-14 is exponential with a half-life of 5730 years. A quantity of carbon-14 will decay to half of its original amount (on average) after 5730 years, regardless of how big or small the original quantity was. After another 5730 years, one-quarter of the original will remain. On the other hand, the time it will take a puddle to half-evaporate depends on how deep the puddle is. Perhaps a puddle of a certain size will evaporate down to half its original volume in one day. But on the second day, there is no reason to expect that one-quarter of the puddle will remain; in fact, it will probably be much less than that. This is an example where the half-life reduces as time goes on. (In other non-exponential decays, it can increase instead.)
The decay of a mixture of two or more materials which each decay exponentially, but with different half-lives, is not exponential. Mathematically, the sum of two exponential functions is not a single exponential function. A common example of such a situation is the waste of nuclear power stations, which is a mix of substances with vastly different half-lives. Consider a sample containing a rapidly decaying element A, with a half-life of 1 second, and a slowly decaying element B, with a half-life of one year. After a few seconds, almost all atoms of the element A have decayed after repeated halving of the initial total number of atoms; but very few of the atoms of element B will have decayed yet as only a tiny fraction of a half-life has elapsed. Thus, the mixture taken as a whole does not decay by halves.
Half-life in biology and pharmacology.
A biological half-life or elimination half-life is the time it takes for a substance (drug, radioactive nuclide, or other) to lose one-half of its pharmacologic, physiologic, or radiological activity. In a medical context, the half-life may also describe the time that it takes for the concentration in blood plasma of a substance to reach one-half of its steady-state value (the "plasma half-life").
The relationship between the biological and plasma half-lives of a substance can be complex, due to factors including accumulation in tissues, active metabolites, and receptor interactions.
While a radioactive isotope decays almost perfectly according to so-called "first order kinetics" where the rate constant is a fixed number, the elimination of a substance from a living organism usually follows more complex chemical kinetics.
For example, the biological half-life of water in a human being is about 9 to 10 days, though this can be altered by behavior and various other conditions. The biological half-life of cesium in human beings is between one and four months. This can be shortened by feeding the person prussian blue, which acts as a solid ion exchanger that absorbs the cesium while releasing potassium ions in their place.

</doc>
<doc id="13607" url="http://en.wikipedia.org/wiki?curid=13607" title="Humus">
Humus

In soil science, humus (coined 1790–1800; from the Latin "humus": earth, ground) refers to the fraction of soil organic matter that is amorphous and without the "cellular structure characteristic of plants, micro-organisms or animals." Humus significantly influences the bulk density of soil and contributes to moisture and nutrient retention.
Soil formation begins with the weathering of humus.
In agriculture, humus is sometimes also used to describe mature, or natural compost extracted from a forest or other spontaneous source for use to amend soil. It is also used to describe a topsoil horizon that contains organic matter (humus type, humus form, humus profile).
Humification.
Transformation of organic matter into humus.
The process of "humification" can occur naturally in soil, or in the production of compost. The importance of chemically stable humus is thought by some to be the fertility it provides to soils in both a physical and chemical sense, though some agricultural experts put a greater focus on other features of it, such as its ability to suppress disease. It helps the soil retain moisture by increasing microporosity, and encourages the formation of good soil structure. The incorporation of oxygen into large organic molecular assemblages generates many active, negatively charged sites that bind to positively charged ions (cations) of plant nutrients, making them more available to the plant by way of ion exchange. Humus allows soil organisms to feed and reproduce, and is often described as the "life-force" of the soil.
It is difficult to define humus precisely; it is a highly complex substance, which is still not fully understood. Humus should be differentiated from decomposing organic matter. The latter is rough-looking material and remains of the original plant are still visible. Fully humified organic matter, on the other hand, has a uniform dark, spongy, jelly-like appearance, and is amorphous. It may remain like this for millennia or more. It has no determinate shape, structure or character. However, humified organic matter, when examined under the microscope may reveal tiny plant, animal or microbial remains that have been mechanically, but not chemically, degraded. This suggests a fuzzy boundary between humus and organic matter. In most literature, humus is considered an integral part of soil organic matter.
Plant remains (including those that passed through an animal gut and were excreted as feces) contain organic compounds: sugars, starches, proteins, carbohydrates, lignins, waxes, resins, and organic acids. The process of organic matter decay in the soil begins with the decomposition of sugars and starches from carbohydrates, which break down easily as detritivores initially invade the dead plant organs, while the remaining cellulose and lignin break down more slowly. Simple proteins, organic acids, starches and sugars break down rapidly, while crude proteins, fats, waxes and resins remain relatively unchanged for longer periods of time. Lignin, which is quickly transformed by white-rot fungi, is one of the main precursors of humus, together with by-products of microbial and animal activity. The end-product of this process, the humus, is thus a mixture of compounds and complex life chemicals of plant, animal, or microbial origin that has many functions and benefits in the soil. Earthworm humus (vermicompost) is considered by some to be the best organic manure there is.
Stability.
Much of the humus in most soils has persisted for more than a hundred years (rather than having been decomposed to CO2), and can be regarded as stable; this is organic matter that has been protected from decomposition by microbial or enzyme action because it is hidden (occluded) inside small aggregates of soil particles or tightly attached (sorbed or complexed) to clays. Most humus that is not protected in this way is decomposed within ten years and can be regarded as less stable or more labile. Thus stable humus contributes little to the pool of plant-available nutrients in the soil, but it does play a part in maintaining its physical structure. A very stable form of humus is that formed from the slow oxidation of black carbon, after the incorporation of finely powdered charcoal into the topsoil. This process is thought to have been important in the formation of the fertile Amazonian dark earths or Terra preta do Indio.

</doc>
<doc id="13609" url="http://en.wikipedia.org/wiki?curid=13609" title="Hydrogen bond">
Hydrogen bond

A hydrogen bond is the electrostatic attraction between polar molecules that occurs when a hydrogen (H) atom bound to a highly electronegative atom such as nitrogen (N), oxygen (O) or fluorine (F) experiences attraction to some other nearby highly electronegative atom. The name "hydrogen bond" is something of a misnomer, as it is not a true bond but a particularly strong dipole-dipole attraction, and should not be confused with a covalent bond.
These hydrogen-bond attractions can occur between molecules ("intermolecular") or within different parts of a single molecule ("intramolecular"). The hydrogen bond (5 to 30 kJ/mole) is stronger than a van der Waals interaction, but weaker than covalent or ionic bonds. This type of bond can occur in inorganic molecules such as water and in organic molecules like DNA and proteins.
Intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides that have no hydrogen bonds. Intramolecular hydrogen bonding is partly responsible for the secondary and tertiary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.
In 2011, an IUPAC Task Group recommended a modern evidence-based definition of hydrogen bonding, which was published in the IUPAC journal "Pure and Applied Chemistry". This definition specifies that "The hydrogen bond is an attractive interaction between a hydrogen atom from a molecule or a molecular fragment X–H in which X is more electronegative than H, and an atom or a group of atoms in the same or a different molecule, in which there is evidence of bond formation." An accompanying detailed technical report provides the rationale behind the new definition.
Bonding.
A hydrogen atom attached to a relatively electronegative atom will play the role of the hydrogen bond "donor". This electronegative atom is usually fluorine, oxygen, or nitrogen. A hydrogen attached to carbon can also participate in hydrogen bonding when the carbon atom is bound to electronegative atoms, as is the case in chloroform, CHCl3. An example of a hydrogen bond donor is ethanol, which has a hydrogen bonded to an oxygen. 
An electronegative atom such as fluorine, oxygen, or nitrogen will be the hydrogen bond "acceptor", irrespective of whether it is bonded to a hydrogen atom or not. An example of a hydrogen bond acceptor that does not have a hydrogen atom bonded to it is the oxygen atom in diethyl ether.
In the donor molecule, the electronegative atom attracts the electron cloud from around the hydrogen nucleus of the donor, and, by decentralizing the cloud, leaves the atom with a positive partial charge. Because of the small size of hydrogen relative to other atoms and molecules, the resulting charge, though only partial, represents a large charge density. A hydrogen bond results when this strong positive charge density attracts a lone pair of electrons on another heteroatom, which then becomes the hydrogen-bond acceptor.
The hydrogen bond is often described as an electrostatic dipole-dipole interaction. However, it also has some features of covalent bonding: it is directional and strong, produces interatomic distances shorter than the sum of the van der Waals radii, and usually involves a limited number of interaction partners, which can be interpreted as a type of valence. These covalent features are more substantial when acceptors bind hydrogens from more electronegative donors.
The partially covalent nature of a hydrogen bond raises the following questions: "To which molecule or atom does the hydrogen nucleus belong?" and "Which should be labeled 'donor' and which 'acceptor'?" Usually, this is simple to determine on the basis of interatomic distances in the X−H…Y system, where the dots represent the hydrogen bond: the X−H distance is typically ≈110 pm, whereas the H…Y distance is ≈160 to 200 pm. Liquids that display hydrogen bonding (such as water) are called associated liquids.
Hydrogen bonds can vary in strength from very weak (1–2 kJ mol−1) to extremely strong (161.5 kJ mol−1 in the ion HF2-). Typical enthalpies in vapor include:
Quantum chemical calculations of the relevant interresidue potential constants (compliance constants) revealed large differences between individual H bonds of the same type. For example, the central interresidue N−H···N hydrogen bond between guanine and cytosine is much stronger in comparison to the N−H···N bond between the adenine-thymine pair.
The length of hydrogen bonds depends on bond strength, temperature, and pressure. The bond strength itself is dependent on temperature, pressure, bond angle, and environment (usually characterized by local dielectric constant). The typical length of a hydrogen bond in water is 197 pm. The ideal bond angle depends on the nature of the hydrogen bond donor. The following hydrogen bond angles between a hydrofluoric acid donor and various acceptors have been determined experimentally:
History.
In the book "The Nature of the Chemical Bond", Linus Pauling credits T. S. Moore and T. F. Winmill with the first mention of the hydrogen bond, in 1912. Moore and Winmill used the hydrogen bond to account for the fact that trimethylammonium hydroxide is a weaker base than tetramethylammonium hydroxide. The description of hydrogen bonding in its better-known setting, water, came some years later, in 1920, from Latimer and Rodebush. In that paper, Latimer and Rodebush cite work by a fellow scientist at their laboratory, Maurice Loyal Huggins, saying, "Mr. Huggins of this laboratory in some work as yet unpublished, has used the idea of a hydrogen kernel held between two atoms as a theory in regard to certain organic compounds."
Hydrogen bonds in water.
The most ubiquitous and perhaps simplest example of a hydrogen bond is
found between water molecules. In a discrete water molecule, there are two hydrogen atoms and one oxygen atom. Two molecules of water can form a hydrogen bond between them; the simplest case, when only two molecules are present, is called the water dimer and is often used as a model system. When more molecules are present, as is the case with liquid water, more bonds are possible because the oxygen of one water molecule has two lone pairs of electrons, each of which can form a hydrogen bond with a hydrogen on another water molecule. This can repeat such that every water molecule is H-bonded with up to four other molecules, as shown in the figure (two through its two lone pairs, and two through its two hydrogen atoms). Hydrogen bonding strongly affects the crystal structure of ice, helping to create an open hexagonal lattice. The density of ice is less than the density of water at the same temperature; thus, the solid phase of water floats on the liquid, unlike most other substances.
Liquid water's high boiling point is due to the high number of hydrogen bonds each molecule can form, relative to its low molecular mass. Owing to the difficulty of breaking these bonds, water has a very high boiling point, melting point, and viscosity compared to otherwise similar liquids not conjoined by hydrogen bonds. Water is unique because its oxygen atom has two lone pairs and two hydrogen atoms, meaning that the total number of bonds of a water molecule is up to four. For example, hydrogen fluoride—which has three lone pairs on the F atom but only one H atom—can form only two bonds; (ammonia has the opposite problem: three hydrogen atoms but only one lone pair).
The exact number of hydrogen bonds formed by a molecule of liquid water fluctuates with time and depends on the temperature. From TIP4P liquid water simulations at 25 °C, it was estimated that each water molecule participates in an average of 3.59 hydrogen bonds. At 100 °C, this number decreases to 3.24 due to the increased molecular motion and decreased density, while at 0 °C, the average number of hydrogen bonds increases to 3.69. A more recent study found a much smaller number of hydrogen bonds: 2.357 at 25 °C. The differences may be due to the use of a different method for defining and counting the hydrogen bonds.
Where the bond strengths are more equivalent, one might instead find the atoms of two interacting water molecules partitioned into two polyatomic ions of opposite charge, specifically hydroxide (OH−) and hydronium (H3O+). (Hydronium ions are also known as "hydroxonium" ions.)
Indeed, in pure water under conditions of standard temperature and pressure, this latter formulation is applicable only rarely; on average about one in every 5.5 × 108 molecules gives up a proton to another water molecule, in accordance with the value of the dissociation constant for water under such conditions. It is a crucial part of the uniqueness of water.
Because water forms hydrogen bonds with the donors and acceptors on solutes dissolved within it, it inhibits the formation of a hydrogen bond between two molecules of those solutes or the formation of intramolecular hydrogen bonds within those solutes through competition for their donors and acceptors. Consequently, hydrogen bonds between or within solute molecules dissolved in water are almost always unfavorable relative to hydrogen bonds between water and the donors and acceptors for hydrogen bonds on those solutes. Hydrogen bonds between water molecules have a duration of about 10−10 seconds.
Bifurcated and over-coordinated hydrogen bonds in water.
A single hydrogen atom can participate in two hydrogen bonds, rather than one. This type of bonding is called "bifurcated" (split in two or "two-forked"). It can exist for instance in complex natural or synthetic organic molecules. It has been suggested that a bifurcated hydrogen atom is an essential step in water reorientation.
Acceptor-type hydrogen bonds (terminating on an oxygen's lone pairs) are more likely to form bifurcation (it is called overcoordinated oxygen, OCO) than are donor-type hydrogen bonds, beginning on the same oxygen's hydrogens.
Hydrogen bonds in DNA and proteins.
Hydrogen bonding also plays an important role in determining the three-dimensional structures adopted by proteins and nucleic bases. In these macromolecules, bonding between parts of the same macromolecule cause it to fold into a specific shape, which helps determine the molecule's physiological or biochemical role. For example, the double helical structure of DNA is due largely to hydrogen bonding between its base pairs (as well as pi stacking interactions), which link one complementary strand to the other and enable replication.
In the secondary structure of proteins, hydrogen bonds form between the backbone oxygens and amide hydrogens. When the spacing of the amino acid residues participating in a hydrogen bond occurs regularly between positions "i" and "i" + 4, an alpha helix is formed. When the spacing is less, between positions "i" and "i" + 3, then a 310 helix is formed. When two strands are joined by hydrogen bonds involving alternating residues on each participating strand, a beta sheet is formed. Hydrogen bonds also play a part in forming the tertiary structure of protein through interaction of R-groups. (See also protein folding).
The role of hydrogen bonds in protein folding has also been linked to osmolyte-induced protein stabilization. Protective osmolytes, such as trehalose and sorbitol, shift the protein folding equilibrium toward the folded state, in a concentration dependent manner. While the prevalent explanation for osmolyte action relies on excluded volume effects, that are entropic in nature, recent Circular dichroism (CD) experiments have shown osmolyte to act through an enthalpic effect. The molecular mechanism for their role in protein stabilization is still not well established, though several mechanism have been proposed. Recently, computer molecular dynamics simulations suggested that osmolytes stabilize proteins by modifying the hydrogen bonds in the protein hydration layer.
Several studies have shown that hydrogen bonds play an important role for the stability between subunits in multimeric proteins. For example, a study of sorbitol dehydrogenase displayed an important hydrogen bonding network which stabilizes the tetrameric quaternary structure within the mammalian sorbitol dehydrogenase protein family.
A protein backbone hydrogen bond incompletely shielded from water attack is a dehydron. Dehydrons promote the removal of water through proteins or ligand binding. The exogenous dehydration enhances the electrostatic interaction between the amide and carbonyl groups by de-shielding their partial charges. Furthermore, the dehydration stabilizes the hydrogen bond by destabilizing the nonbonded state consisting of dehydrated isolated charges.
Hydrogen bonds in polymers.
Many polymers are strengthened by hydrogen bonds in their main chains. Among the synthetic polymers, the best known example is nylon, where hydrogen bonds occur in the repeat unit and play a major role in crystallization of the material. The bonds occur between carbonyl and amine groups in the amide repeat unit. They effectively link adjacent chains to create crystals, which help reinforce the material. The effect is greatest in aramid fibre, where hydrogen bonds stabilize the linear chains laterally. The chain axes are aligned along the fibre axis, making the fibres extremely stiff and strong. Hydrogen bonds are also important in the structure of cellulose and derived polymers in its many different forms in nature, such as wood and natural fibres such as cotton and flax.
The hydrogen bond networks make both natural and synthetic polymers sensitive to humidity levels in the atmosphere because water molecules can diffuse into the surface and disrupt the network. Some polymers are more sensitive than others. Thus nylons are more sensitive than aramids, and nylon 6 more sensitive than nylon-11.
Symmetric hydrogen bond.
A symmetric hydrogen bond is a special type of hydrogen bond in which the proton is spaced exactly halfway between two identical atoms. The strength of the bond to each of those atoms is equal. It is an example of a three-center four-electron bond. This type of bond is much stronger than a "normal" hydrogen bond. The effective bond order is 0.5, so its strength is comparable to a covalent bond. It is seen in ice at high pressure, and also in the solid phase of many anhydrous acids such as hydrofluoric acid and formic acid at high pressure. It is also seen in the bifluoride ion [F−H−F]−.
Symmetric hydrogen bonds have been observed recently spectroscopically in formic acid at high pressure (>GPa). Each hydrogen atom forms a partial covalent bond with two atoms rather than one. Symmetric hydrogen bonds have been postulated in ice at high pressure (Ice X). Low-barrier hydrogen bonds form when the distance between two heteroatoms is very small.
Dihydrogen bond.
The hydrogen bond can be compared with the closely related dihydrogen bond, which is also an intermolecular bonding interaction involving hydrogen atoms. These structures have been known for some time, and well characterized by crystallography; however, an understanding of their relationship to the conventional hydrogen bond, ionic bond, and covalent bond remains unclear. Generally, the hydrogen bond is characterized by a proton acceptor that is a lone pair of electrons in nonmetallic atoms (most notably in the nitrogen, and chalcogen groups). In some cases, these proton acceptors may be pi-bonds or metal complexes. In the dihydrogen bond, however, a metal hydride serves as a proton acceptor, thus forming a hydrogen-hydrogen interaction. Neutron diffraction has shown that the molecular geometry of these complexes is similar to hydrogen bonds, in that the bond length is very adaptable to the metal complex/hydrogen donor system.
Advanced theory of the hydrogen bond.
In 1999, Isaacs "et al." showed from interpretations of the anisotropies in the Compton profile of ordinary ice that the hydrogen bond is partly covalent. Some NMR data on hydrogen bonds in proteins also indicate covalent bonding.
Most generally, the hydrogen bond can be viewed as a metric-dependent electrostatic scalar field between two or more intermolecular bonds. This is slightly different from the intramolecular bound states of, for example, covalent or ionic bonds; however, hydrogen bonding is generally still a bound state phenomenon, since the interaction energy has a net negative sum. The initial theory of hydrogen bonding proposed by Linus Pauling suggested that the hydrogen bonds had a partial covalent nature. This remained a controversial conclusion until the late 1990s when NMR techniques were employed by F. Cordier "et al." to transfer information between hydrogen-bonded nuclei, a feat that would only be possible if the hydrogen bond contained some covalent character. While much experimental data has been recovered for hydrogen bonds in water, for example, that provide good resolution on the scale of intermolecular distances and molecular thermodynamics, the kinetic and dynamical properties of the hydrogen bond in dynamic systems remain unchanged.
Dynamics probed by spectroscopic means.
The dynamics of hydrogen bond structures in water can be probed by the IR spectrum of OH stretching vibration. In terms of hydrogen bonding network in protic organic ionic plastic crystals (POIPCs), which are a type of phase change materials exhibiting solid-solid phase transitions prior to melting, variable-temperature infrared spectroscopy can reveal the temperature dependence of hydrogen bonds and the dynamics of both the anions and the cations. The sudden weakening of hydrogen bonds during the solid-solid phase transition seems to be coupled with the onset of orientational or rotational disorder of the ions.

</doc>
<doc id="13610" url="http://en.wikipedia.org/wiki?curid=13610" title="Heraldry">
Heraldry

Heraldry () is the profession, study, or art of creating, granting, and blazoning arms and ruling on questions of rank or protocol, as exercised by an officer of arms. "Heraldry" comes from Anglo-Norman "herald", from the Germanic compound "harja-waldaz", "army commander". The word, in its most general sense, encompasses all matters relating to the duties and responsibilities of officers of arms. To most, though, heraldry is the practice of designing, displaying, describing, and recording coats of arms and heraldic badges.
Historically, it has been variously described as "the shorthand of history" and "the floral border in the garden of history". The origins of heraldry lie in the need to distinguish participants in combat when their faces were hidden by iron and steel helmets. Eventually a formal system of rules developed into ever more complex forms of heraldry.
Though the practice of heraldry is nearly 900 years old, it is still very much in use. Many cities and towns in Europe and around the world still make use of arms. Personal heraldry, both legally protected and lawfully assumed, has continued to be used around the world. Heraldic societies exist to promote education and understanding about the subject.
Blazon.
To "blazon" arms means to describe them using the formal language of heraldry. This language has its own vocabulary and syntax, or rules governing word order, which becomes essential for comprehension when blazoning a complex coat of arms. The verb comes from the Middle English "blasoun", itself a derivative of the French "blason" meaning "shield". The system of blazoning arms used in English-speaking countries today was developed by heraldic officers in the Middle Ages. The blazon includes a description of the arms contained within the escutcheon or shield, the crest, supporters where present, motto and other insignia. Complex rules, such as the rule of tincture, apply to the physical and artistic form of newly created arms, and a thorough understanding of these rules is essential to the art of heraldry. Though heraldic forms initially were broadly similar across Europe, several national styles had developed by the end of the Middle Ages, and artistic and blazoning styles today range from the very simple to extraordinarily complex.
History.
Predecessors.
There are various conjectures as to the origins of heraldic arms. As early as predynastic Egypt c. 3100 BC, an emblem known as a "serekh" was used to indicate the extent of influence of a particular regime, sometimes carved on ivory labels attached to trade goods, but also used to identify military allegiances and in a variety of other ways. It led to the development of the earliest hieroglyphs. This practice seems to have grown out of the use of animal mascots, whose pelts or bodies were affixed to staves or standards, as depicted on the earliest cosmetic palettes of the period. Some of the oldest "serekhs" consist of a striped or cross-hatched box, representing a palace or city, with a crane, scorpion, or other animal drawn standing on top. Before long, a falcon representing Horus became the norm as the animal on top, with the individual Pharaoh's symbol usually appearing in the box beneath the falcon, and above the stripes representing the palace.
The antiquity of standards and symbols may be illustrated by the Book of Numbers:
And the children of Israel shall pitch their tents, every man by his own camp, and every man by his own standard, throughout their hosts. . Every man of the children of Israel shall pitch by his own standard, with the ensign of their fathers house . And the children of Israel did according to all that the Lord commanded to Moses: so they pitched by their standards, and so they set forward, every one after their families, according to the house of their fathers. 
Army units of the Roman Empire were identified by the distinctive markings on their shields. These were not heraldic in the medieval and modern sense, as they were associated with units, not individuals or families.
Origins in the High Middle Ages.
At the time of the Norman conquest of England, heraldry in its essential sense of an "inheritable" emblem had not yet been developed. The knights in the Bayeux Tapestry carry shields, but there appears to have been no system of hereditary coats of arms. The seeds of heraldic structure in personal identification can be detected in the account in a contemporary chronicle of Henry I of England, on the occasion of his knighting his son-in-law Geoffrey V, Count of Anjou, in 1127. He placed to hang around his neck a shield painted with golden lions. The funerary enamel of Geoffrey (died 1151), dressed in blue and gold and bearing his blue shield emblazoned with gold lions, is the first recorded depiction of a coat of arms.
By the middle of the 12th century, coats of arms were being inherited by the children of armigers (persons entitled to use a coat of arms) across Europe. Between 1135 and 1155, seals representing the generalized figure of the owner attest to the general adoption of heraldic devices in England, France, Germany, Spain, and Italy. By the end of the century, heraldry appears as the sole device on seals. In England, the practice of using marks of cadency arose to distinguish one son from another: the conventions became standardized in about 1500, and are traditionally supposed to have been devised by John Writhe.
Development of classical heraldry.
In the late Middle Ages and the Renaissance, heraldry became a highly developed discipline, regulated by professional officers of arms. As its use in jousting became obsolete, coats of arms remained popular for visually identifying a person in other ways – impressed in sealing wax on documents, carved on family tombs, and flown as a banner on country homes. The first work of heraldic jurisprudence, "De Insigniis et Armis", was written in the 1350s by Bartolus de Saxoferrato, a professor of law at the University of Padua.
From the beginning of heraldry, coats of arms have been executed in a wide variety of media, including on paper, painted wood, embroidery, enamel, stonework and stained glass. For the purpose of quick identification in all of these, heraldry distinguishes only seven basic colours and makes no fine distinctions in the precise size or placement of charges on the field. Coats of arms and their accessories are described in a concise jargon called "blazon". This technical description of a coat of arms is the standard that is adhered to no matter what artistic interpretations may be made in a particular depiction of the arms.
The specific meaning of each element of a coat of arms is subjective. Though the original armiger may have placed particular meaning on a charge, these meanings are not necessarily retained from generation to generation. Unless canting arms incorporate an obvious pun on the bearer's name, it may be difficult to find meaning in them. As changes in military technology and tactics made plate armour obsolete, heraldry became detached from its original function. This brought about the development of "paper heraldry" under the Tudors. Designs and shields became more elaborate at the expense of clarity.
Early modern and modern history.
In Scottish heraldry, the Lord Lyon King of Arms in the Act of 1672 is empowered to grant arms to "vertuous [virtuous] and well deserving persons."
During the 19th century, especially in Germany, many coats of arms were designed to depict a natural landscape, including several charges tinctured "proper" ("i.e." the way they appear in nature). This form has been termed "Landscape heraldry". 
The 20th century's taste for stark iconic emblems made the simple styles of early heraldry fashionable again.
Components and rules.
 Escutcheon 
 Field 
 Supporter 
 Supporter 
 Motto (alternative) 
 Crest 
 Torse 
 Mantling 
 Helm 
 Coronet 
 Compartment 
 Order 
 Ordinaries 
 Charges 
 Motto 
 Dexter 
 Sinister 
Conventional elements of an achievement
Shield and lozenge.
The focus of modern heraldry is the armorial achievement, or the coat of arms, the central element of which is the escutcheon or shield. In general, the shape of the shield employed in a coat of arms is irrelevant, because the fashion for the shield-shapes employed in heraldic art has changed through the centuries. Sometimes a blazon specifies a particular shape of shield. These specifications mostly occur in non-European contexts – such as the coat of arms of Nunavut and the former Republic of Bophuthatswana, with the arms of North Dakota (as distinguished from its seal) providing an even more unusual example, while the State of Connecticut specifies a "rococo" shield – but not completely, as the Scottish Public Register records an escutcheon of oval form for the Lanarkshire Master Plumbers' and Domestic Engineers' (Employers') Association, and a shield of square form for the Anglo Leasing organisation.
Women and coats of arms.
Traditionally, as women did not go to war, they did not bear a shield. Instead, women's coats of arms were shown on a lozenge – a rhombus standing on one of its acute corners, an oval or a cartouche. This remains true in much of the world, though some heraldic authorities, such as Scotland's, with its ovals for women's arms, make exceptions. In Canada the restriction against women's bearing arms on a shield was eliminated. In Scotland and Ireland, women may, under certain circumstances, be permitted to display their arms on a shield. Non-combatant clergy also have used the lozenge and the cartouche – an oval – for their display.
Tinctures.
Tinctures are the colours, metals, and furs used in heraldry, though the depiction of charges in their natural colours or "proper" are also regarded as tinctures, the latter distinct from any colour that such a depiction might approximate. Heraldry is essentially a system of identification, so the most important convention of heraldry is the rule of tincture. To provide for contrast and visibility, metals (generally lighter tinctures) must never be placed on metals, and colours (generally darker tinctures) must never be placed on colours. Where a charge overlies a partition of the field, the rule does not apply. There are other exceptions – the most famous being the arms of the kingdom of Jerusalem, consisting of gold crosses on a silver field.
The names used in English blazon for the colours and metals come mainly from French and include "Or" (gold), "argent" (silver), "azure" (blue), "gules" (red), "sable" (black), "vert" (green), and "purpure" (purple). A number of other colours (such as "bleu-celeste" and the stains "sanguine", "tenné" and "murrey") are occasionally found, typically for special purposes.
Certain patterns called "furs" can appear in a coat of arms, though they are (rather arbitrarily) defined as tinctures, not patterns. The two common furs are "ermine" and "vair". Ermine represents the winter coat of the stoat, which is white with a black tail. Vair represents a kind of squirrel with a blue-gray back and white belly. Sewn together, it forms a pattern of alternating blue and white shapes.
Heraldic charges can be displayed in their natural colours. Many natural items such as plants and animals are described as "proper" in this case. Proper charges are very frequent as crests and supporters. Overuse of the tincture "proper" is viewed as decadent or bad practice.
Variations of the field.
The field of a shield, or less often a charge or crest, is sometimes made up of a pattern of colours, or "variation". A pattern of horizontal (barwise) stripes, for example, is called "barry", while a pattern of vertical (palewise) stripes is called "paly". A pattern of diagonal stripes may be called "bendy" or "bendy sinister", depending on the direction of the stripes. Other variations include "chevrony", "gyronny" and "chequy". Wave shaped stripes are termed "undy". For further variations, these are sometimes combined to produce patterns of "barry-bendy", "paly-bendy", "lozengy" and "fusilly". Semés, or patterns of repeated charges, are also considered variations of the field. The Rule of tincture applies to all semés and variations of the field.
Divisions of the field.
The field of a shield in heraldry can be divided into more than one tincture, as can the various heraldic charges. Many coats of arms consist simply of a division of the field into two contrasting tinctures. These are considered divisions of a shield, so the rule of tincture can be ignored. For example, a shield divided azure and gules would be perfectly acceptable. A line of partition may be straight or it may be varied. The variations of partition lines can be wavy, indented, embattled, engrailed, nebuly, or made into myriad other forms; see Line (heraldry).
Ordinaries.
In the early days of heraldry, very simple bold rectilinear shapes were painted on shields. These could be easily recognized at a long distance and could be easily remembered. They therefore served the main purpose of heraldry: identification. As more complicated shields came into use, these bold shapes were set apart in a separate class as the "honorable ordinaries". They act as charges and are always written first in blazon. Unless otherwise specified they extend to the edges of the field. Though ordinaries are not easily defined, they are generally described as including the cross, the fess, the pale, the bend, the chevron, the saltire, and the pall.
There is a separate class of charges called sub-ordinaries which are of a geometrical shape subordinate to the ordinary. According to Friar, they are distinguished by their order in blazon. The sub-ordinaries include the inescutcheon, the orle, the tressure, the double tressure, the bordure, the chief, the canton, the label, and flaunches.
Ordinaries may appear in parallel series, in which case blazons in English give them different names such as pallets, bars, bendlets, and chevronels. French blazon makes no such distinction between these diminutives and the ordinaries when borne singly. Unless otherwise specified an ordinary is drawn with straight lines, but each may be indented, embattled, wavy, engrailed, or otherwise have their lines varied.
Charges.
A charge is any object or figure placed on a heraldic shield or on any other object of an armorial composition. Any object found in nature or technology may appear as a heraldic charge in armory. Charges can be animals, objects, or geometric shapes. Apart from the ordinaries, the most frequent charges are the cross – with its hundreds of variations – and the lion and eagle. Other common animals are stags, Wild Boars, martlets, and fish. Dragons, bats, unicorns, griffins, and more exotic monsters appear as charges and as supporters.
Animals are found in various stereotyped positions or "attitudes". Quadrupeds can often be found rampant (standing on the left hind foot). Another frequent position is passant, or walking, like the lions of the coat of arms of England. Eagles are almost always shown with their wings spread, or displayed. A pair of wings conjoined is called a vol.
In English heraldry the crescent, mullet, martlet, annulet, fleur-de-lis, and rose may be added to a shield to distinguish cadet branches of a family from the senior line. These cadency marks are usually shown smaller than normal charges, but it still does not follow that a shield containing such a charge belongs to a cadet branch. All of these charges occur frequently in basic undifferenced coats of arms.
Marshalling.
To "marshal" two or more coats of arms is to combine them in one shield, to express inheritance, claims to property, or the occupation of an office. This can be done in a number of ways, of which the simplest is impalement: dividing the field "per pale" and putting one whole coat in each half. Impalement replaced the earlier dimidiation – combining the dexter half of one coat with the sinister half of another – because dimidiation can create ambiguity between, for example, a bend and a chevron. "Dexter" (from Latin "dextra", right) means to the right from the viewpoint of the bearer of the arms and "sinister" (from Latin "sinistra", left) means to the left. The dexter side is considered the side of greatest honour (see also Dexter and sinister).
A more versatile method is quartering, division of the field by both vertical and horizontal lines. This practice originated in Spain after the 13th century. As the name implies, the usual number of divisions is four, but the principle has been extended to very large numbers of "quarters".
Quarters are numbered from the dexter chief (the corner nearest to the right shoulder of a man standing behind the shield), proceeding across the top row, and then across the next row and so on. When three coats are quartered, the first is repeated as the fourth; when only two coats are quartered, the second is also repeated as the third. The quarters of a personal coat of arms correspond to the ancestors from whom the bearer has inherited arms, normally in the same sequence as if the pedigree were laid out with the father's father's ... father (to as many generations as necessary) on the extreme left and the mother's mother's...mother on the extreme right. A few lineages have accumulated hundreds of quarters, though such a number is usually displayed only in documentary contexts. The Scottish and Spanish traditions resist allowing more than four quarters, preferring to subdivide one or more "grand quarters" into sub-quarters as needed.
The third common mode of marshalling is with an inescutcheon, a small shield placed in front of the main shield. In Britain this is most often an "escutcheon of pretence" indicating, in the arms of a married couple, that the wife is an heraldic heiress (i.e., she inherits a coat of arms because she has no brothers). In continental Europe an inescutcheon (sometimes called a "heart shield") usually carries the ancestral arms of a monarch or noble whose domains are represented by the quarters of the main shield.
In German heraldry, animate charges in combined coats usually turn to face the centre of the composition.
Helm and crest.
In English the word "crest" is commonly (but erroneously) used to refer to an entire heraldic achievement of armorial bearings. The technical use of the heraldic term crest refers to just one component of a complete achievement. The crest rests on top of a helmet which itself rests on the most important part of the achievement: the shield.
The modern crest has grown out of the three-dimensional figure placed on the top of the mounted knights' helms as a further means of identification. In most heraldic traditions, a woman does not display a crest, though this tradition is being relaxed in some heraldic jurisdictions, and the stall plate of Lady Marion Fraser in the Thistle Chapel in St Giles, Edinburgh, shows her coat on a lozenge but with helmet, crest, and motto.
The crest is usually found on a wreath of twisted cloth and sometimes within a coronet. Crest-coronets are generally simpler than coronets of rank, but several specialized forms exist; for example, in Canada, descendants of the United Empire Loyalists are entitled to use a Loyalist military coronet (for descendants of members of Loyalist regiments) or Loyalist civil coronet (for others).
When the helm and crest are shown, they are usually accompanied by a mantling. This was originally a cloth worn over the back of the helmet as partial protection against heating by sunlight. Today it takes the form of a stylized cloak hanging from the helmet. Typically in British heraldry, the outer surface of the mantling is of the principal colour in the shield and the inner surface is of the principal metal, though peers in the United Kingdom use standard colourings (Gules doubled Argent - Red/White) regardless of rank or the colourings of their arms. The mantling is sometimes conventionally depicted with a ragged edge, as if damaged in combat, though the edges of most are simply decorated at the emblazoner's discretion.
Clergy often refrain from displaying a helm or crest in their heraldic achievements. Members of the clergy may display appropriate headwear. This often takes the form of a small crowned, wide brimmed hat called a galero with the colours and tassels denoting rank; or, in the case of Papal coats of arms until the inauguration of Pope Benedict XVI in 2005, an elaborate triple crown known as a tiara. Benedict broke with tradition to substitute a mitre in his arms. Orthodox and Presbyterian clergy do sometimes adopt other forms of head gear to ensign their shields. In the Anglican tradition, clergy members may pass crests on to their offspring, but rarely display them on their own shields.
Mottoes.
An armorial motto is a phrase or collection of words intended to describe the motivation or intention of the armigerous person or corporation. This can form a pun on the family name as in Thomas Nevile's motto "Ne vile velis". Mottoes are generally changed at will and do not make up an integral part of the armorial achievement. Mottoes can typically be found on a scroll under the shield. In Scottish heraldry where the motto is granted as part of the blazon, it is usually shown on a scroll above the crest, and may not be changed at will. A motto may be in any language.
Supporters and other insignia.
Supporters are human or animal figures or, very rarely, inanimate objects, usually placed on either side of a coat of arms as though supporting it. In many traditions, these have acquired strict guidelines for use by certain social classes. On the European continent, there are often fewer restrictions on the use of supporters. In the United Kingdom, only peers of the realm, a few baronets, senior members of orders of knighthood, and some corporate bodies are granted supporters. Often, these can have local significance or a historical link to the armiger.
If the armiger has the title of baron, hereditary knight, or higher, he may display a coronet of rank above the shield. In the United Kingdom, this is shown between the shield and helmet, though it is often above the crest in Continental heraldry.
Another addition that can be made to a coat of arms is the insignia of a baronet or of an order of knighthood. This is usually represented by a collar or similar band surrounding the shield. When the arms of a knight and his wife are shown in one achievement, the insignia of knighthood surround the husband's arms only, and the wife's arms are customarily surrounded by a meaningless ornamental garland of leaves for visual balance.
Differencing and cadency.
Since arms pass from parents to offspring, and there is frequently more than one child per couple, it is necessary to distinguish the arms of siblings and extended family members from the original arms as passed on from eldest son to eldest son. Over time several schemes have been used.
National styles.
The emergence of heraldry occurred across western Europe almost simultaneously in the various countries. Originally, heraldic style was very similar from country to country. Over time, heraldic tradition diverged into four broad styles: German-Nordic, Gallo-British, Latin, and Eastern. In addition it can be argued that newer national heraldic traditions, such as South African and Canadian, have emerged in the 20th century.
German-Nordic heraldry.
Coats of arms in Germany, the Scandinavian countries, Estonia, Latvia, Czech lands and northern Switzerland generally change very little over time. Marks of difference are very rare in this tradition as are heraldic furs. One of the most striking characteristics of German-Nordic heraldry is the treatment of the crest. Often, the same design is repeated in the shield and the crest. The use of multiple crests is also common. The crest is rarely used separately as in British heraldry, but can sometimes serve as a mark of difference between different branches of a family. Torse is optional. Heraldic courtoisie is observed: that is, charges in a composite shield (or two shields displayed together) usually turn to face the centre.
Coats consisting only of a divided field are somewhat more frequent in Germany than elsewhere.
Greek heraldry.
Ancient Greeks were among the first civilizations to use symbols consistently in order to identify a warrior, clan or a state. The first record of a shield blazon is illustrated in Aeschylus' tragedy "Seven Against Thebes". The Greek Heraldry Society is a useful source of information on Hellenic Heraldry and Byzantine etiquette.
Dutch heraldry.
The Low Countries were great centres of heraldry in medieval times. One of the famous armorials is the Gelre Armorial or "Wapenboek", written between 1370 and 1414.
Coats of arms in the Netherlands were not controlled by an official heraldic system like the two in the United Kingdom, nor were they used solely by noble families. Any person could develop and use a coat of arms if they wished to do so, provided they did not usurp someone else's arms, and historically, this right was enshrined in Roman Dutch law. As a result, many merchant families had coats of arms even though they were not members of the nobility. These are sometimes referred to as "burgher arms," and it is thought that most arms of this type were adopted while the Netherlands was a republic (1581–1806). This heraldic tradition was also exported to the erstwhile Dutch colonies.
Dutch heraldry is characterised by its simple and rather sober style, and in this sense, is closer to its medieval origins than the elaborate styles which developed in other heraldic traditions.
Turkish heraldry.
Every sultan of the Ottoman Empire had his own monogram, called the tughra, which served as a royal symbol. A coat of arms in the European heraldic sense was created in the late 19th century. Hampton Court requested from Ottoman Empire the coat of arms to be included in their collection. As the coat of arms had not been previously used in Ottoman Empire, it was designed after this request and the final design was adopted by Sultan Abdul Hamid II on April 17, 1882. It included two flags: the flag of the Ottoman Dynasty, which had a crescent and a star on red base, and the flag of the Islamic Caliph, which had three crescents on a green base.
Gallo-British heraldry.
The use of cadency marks to difference arms within the same family and the use of semy fields are distinctive features of Gallo-British heraldry (in Scotland the most significant mark of cadency being the bordure, the small brisures playing a very minor role). It is common to see heraldic furs used. In the United Kingdom, the style is notably still controlled by royal officers of arms. French heraldry experienced a period of strict rules of construction under the Emperor Napoleon. English and Scots heraldries make greater use of supporters than other European countries.
Furs, chevrons and five-pointed stars are more frequent in France and Britain than elsewhere.
Latin heraldry.
The heraldry of southern France, Andorra, Spain, and Italy is characterized by a lack of crests, and uniquely shaped shields. Portuguese heraldry, however, does use them. Portuguese and Spanish heraldry occasionally introduce words to the shield of arms, a practice disallowed in British heraldry. Latin heraldry is known for extensive use of quartering, because of armorial inheritance via the male and the female lines. Moreover, Italian heraldry is dominated by the Roman Catholic Church, featuring many shields and achievements, most bearing some reference to the Church.
Trees are frequent charges in Latin arms. Charged bordures, including bordures inscribed with words, are seen often in Spain.
Central and Eastern European heraldry.
Eastern European heraldry is in the traditions developed in Belarus, Bulgaria, Serbia, Croatia, Hungary, Romania, Lithuania, Poland, Ukraine, and Russia. Eastern coats of arms are characterized by a pronounced, territorial, clan system – often, entire villages or military groups were granted the same coat of arms irrespective of family relationships. In Poland, nearly six hundred unrelated families are known to bear the same Jastrzębiec coat of arms. Marks of cadency are almost unknown, and shields are generally very simple, with only one charge. Many heraldic shields derive from ancient house marks. At the least, fifteen per cent of all Hungarian personal arms bear a severed Turk's head, referring to their wars against the Ottoman Empire.
Modern heraldry.
Heraldry flourishes in the modern world; institutions, companies, and private persons continue using coats of arms as their pictorial identification. In the United Kingdom and Ireland, the English Kings of Arms, Scotland's Lord Lyon King of Arms, and the Chief Herald of Ireland continue making grants of arms. There are heraldic authorities in Canada, South Africa, Spain, and Sweden that grant or register coats of arms. In South Africa, the right to armorial bearings is also determined by Roman Dutch law, due to its origins as a 17th-century colony of the Netherlands.
Heraldic societies abound in Africa, Asia, Australasia, the Americas and Europe. Heraldry aficionados participate in the Society for Creative Anachronism, medieval revivals, micronationalism, et cetera. People see heraldry as a part of their national and personal heritages, and as a manifestation of civic and national pride. Today, heraldry is not a worldly expression of aristocracy, merely a form of identification.
Military heraldry continues developing, incorporating blazons unknown in the medieval world. Nations and their subdivisions – provinces, states, counties, cities, etc. – continue to build on the traditions of civic heraldry. The Roman Catholic Church, the Church of England, and other Churches maintain the tradition of ecclesiastical heraldry for their high-rank prelates, religious orders, universities, and schools.
Heraldry in many countries with heraldic authorities are governed by certain laws, granting rights and possession to bear arms as well as protection against misuse by others. Other countries without heraldic authorities to grant arms usually treat coat of arms in much the same way as logos and such could be protected under copyright laws if registered appropriately.
Further reading.
</dl>

</doc>
<doc id="13611" url="http://en.wikipedia.org/wiki?curid=13611" title="Heretic (video game)">
Heretic (video game)

Heretic is a dark fantasy first-person shooter video game created by Raven Software, published by id Software, and distributed by GT Interactive in 1994. It was made available on Steam on August 3, 2007.
Using a modified "Doom" engine, "Heretic" was one of the first first-person games to feature inventory manipulation and the ability to look up and down. It also introduced multiple gib objects spawned when a character suffered a death by extreme force or heat. Previously, the character would simply crumple into a heap. The game used randomized ambient sounds and noises, such as evil laughter, chains rattling, distantly ringing bells and water dripping in addition to the background music to further enhance the atmosphere. All music in the game was composed by Kevin Schilder. An indirect sequel, "", was released a year later. "Heretic II" was released in 1998, which served as a direct sequel continuing the story.
Plot.
Three brothers, known as the Serpent Riders, have used their powerful magic to possess seven kings of Parthoris into mindless puppets and corrupt their armies. The Sidhe elves resist the Serpent Riders' magic. The Serpent Riders thus declared the Sidhe as heretics and waged war against them. The Sidhe are forced to take a drastic measure to sever the natural power of the kings destroying them and their armies, but at the cost of weakening the elves' power, giving the Serpent riders an advantage to slay the elders. While the Sidhe retreat, one elf (revealed in the sequel to be named Corvus), sets off on a quest of vengeance against the weakest of the three Serpent Riders, D'Sparil. He travels through the "City of the Damned", the ruined capital of the Sidhe (its real name is revealed to be Silverspring in "Heretic II"), then past Hell's Maw and finally the Dome of D'Sparil.
The player must first fight through the undead hordes infesting the site where the elders performed their ritual. At its end is the gateway to Hell's Maw, guarded by the Iron Liches. After defeating them, the player must seal the portal and so prevent further infestation, but after he enters the portal guarded by the Maulotaurs, he finds himself inside D'Sparil's dome. After the slaughter of D'sparil, Corvus ends up on a perilous journey with little hope to return home.
Gameplay.
The gameplay of Heretic is heavily derived from "Doom", with a level-based structure and an emphasis on finding the proper keys to progress. Many weapons are similar to those from Doom; the early weapons in particular are near-exact copies in functionality to those seen in Doom. Raven did however add a number of touches to "Heretic" that differentiated it from "Doom", notably interactive environments, such as rushing water that pushes the player along, and items. In "Heretic", the player can pick up many different items to use at their discretion. These items range from health potions to the "morph ovum", which transforms enemies into chickens. One of the most notable pickups that can be found is the "Tome of Power" which creates a much more powerful projectile from each weapon, some of which change the look of the projectile entirely. Heretic also features an improved version of the "Doom" engine, sporting the ability to look up and down within constraints, as well as fly. However, the rendering method for looking up and down merely uses a proportional pixel-shearing effect rather than any new rendering algorithm, which distorts the view considerably when looking at high-elevation angles.
As with "Doom", "Heretic" contains different cheat codes that variously give the player invulnerability, all weapons, kill everything on a particular level. However, if the player uses the "all weapons" cheat code from "Doom", a message appears warning the player against cheating and takes away all of his weapons, leaving him with only a quarterstaff. If the player uses the "god mode" cheat from Doom, the game will display a message saying "Trying to cheat, eh? Now you die!" and kills the player.
The original shareware release was the first game to come bundled with support for online multiplayer through the nascent DWANGO service.
"Shadow of the Serpent Riders".
The original edition of "Heretic" was only available through shareware registration (i.e. mail order) and contains three episodes. A retail edition, distributed by GT Interactive, was titled "Heretic: Shadow of the Serpent Riders" and features two additional episodes: "The Ossuary", which takes the player to the shattered remains of a world conquered by the Serpent Riders many centuries ago, and "The Stagnant Demesne", where the player enters D'Sparil's birthplace. A free content patch was downloadable from Raven's website to update the original "Heretic" to match "Shadow of the Serpent Riders".
Source release.
In early 1999, the source code of the "Heretic" engine was published by Raven Software under a license that granted rights to non-commercial use, and was re-released under the GNU General Public License on September 4, 2008. This resulted in ports to Linux, Amiga and other operating systems, and updates to the "Heretic" engine to utilize 3D acceleration. The shareware version of a console port for the Dreamcast was also released.
The "Blasphemer" project aims to create a free content package for the "Heretic" engine, with a theme of metal-inspired dark fantasy - in a similar way as "Freedoom" does with the "Doom" engine.
Reception.
Heretic received mixed reviews, garnering an aggregated score of 62.00% on GameRankings.

</doc>
<doc id="13613" url="http://en.wikipedia.org/wiki?curid=13613" title="Hexen II">
Hexen II

Hexen II is a dark fantasy first-person shooter developed by Raven Software from 1996 to 1997, published by id Software and distributed by Activision. It was the third game in the "Hexen"/"Heretic" series, and the last in the Serpent Riders trilogy. It was made available on Steam on August 3, 2007. Using a modified "Quake" engine, it featured single-player and multi-player game modes, as well as four character classes to choose from, each with different abilities. These included the offensive Paladin, the defensive Crusader, the spell-casting Necromancer, and the stealthy Assassin.
Improvements from "Hexen" and "Quake" included destructible environments, mounted weapons, and unique level up abilities. Like its predecessor, "Hexen II" also used a hub system. These hubs were a number of interconnected levels; changes made in one level had effects in another. The Tome of Power artifact made a return from "Heretic".
Gameplay.
The gameplay of "Hexen II" is very similar to that of the original "Hexen". Instead of three classes, "Hexen II" features four: Paladin, Crusader, Assassin, and Necromancer, each with their own unique weapons and play style.
"Hexen II" also adds certain role-playing video game elements to the mix. Each character has a series of statistics which increase as they gain experience. This then causes the player character to grow in power as his or her HP and Mana increases.
Plot.
Thyrion is a world that was enslaved by the Serpent Riders. The two previous games in the series documented the liberation of two other worlds, along with the death of their Serpent Rider overlords. Now, the oldest and most powerful of the three Serpent Rider brothers, Eidolon, must be defeated to free Thyrion. Eidolon is supported by his four generals, themselves a reference to the Four Horsemen of the Apocalypse. To confront each general, the player has to travel to four different continents, each possessing a distinct theme (Medieval European for Blackmarsh, Mesoamerican for Mazaera, Ancient Egyptian for Thysis, and Greco-Roman for Septimus). Then, finally, the player returns to Blackmarsh in order to confront Eidolon himself inside of his own dominion Cathedral.
Development.
"Hexen II", by way of the "Quake" engine, uses OpenGL for 3D acceleration. However, due to the prevalence of 3dfx hardware at the time of release, the Windows version of the game installs an OpenGL ICD (opengl32.dll) designed specifically for 3dfx's hardware. This driver acts as a wrapper for the proprietary Glide API, and thus is only compatible with 3dfx hardware. Custom OpenGL drivers were also released by PowerVR and Rendition for running "Hexen II" with their respective (and also now defunct) products. Removal of the ICD allows the game to use the default OpenGL system library.
"Siege".
A modification titled "Siege" was created and released by Raven Software in 1998 using updated QuakeWorld architecture, aptly dubbed "HexenWorld". The production concept was to eliminate a normal deathmatch environment in favor of a teamplay castle siege. The basic premise was to divide the players into two teams—attackers and defenders—with each side either assaulting or protecting the castle respectively. At the end of the time limit, whichever team controlled the crown was declared victorious. The mod featured appropriate objects used in the single-player portion of the game, namely catapults and ballistae. The classes, however, were drastically altered with new weapons and abilities, reflecting the departure from the normal deathmatch experience presented in "HexenWorld".
Source release.
Following the tradition from "Hexen" and "Heretic", Raven released the source code of the "Hexen II" engine in 2000. This time the source was released under the GNU General Public License, allowing source ports to be made to different platforms like Linux and the Dreamcast.
"Portal of Praevus".
An expansion pack called "Hexen II Mission Pack: Portal of Praevus" was released in 1998 featuring new levels, new enemies and a new playable character class, The Demoness. It focuses on the attempted resurrection of the three Serpent Riders by the evil wizard Praevus, and takes place in a fifth continent, Tulku, featuring a Sino-Tibetan setting. Unlike the original game, the expansion was not published by id Software, and as such is not currently available via digital re-releases.
The expansion features new quest items, new enemies, and new weapons for the Demoness. She is the only player class to have a ranged starting weapon (like the Mage class in the original Hexen), whereas all other characters start with melee weapons. It also introduced minor enhancements to the game engine, mostly related to user interface, level scripts, particle effects (rain or snow), and 3D objects. "Portal of Praevus" also features a secret (easter egg) skill level, with respawning monsters. The only released patch for the expansion added respawning of certain items (such as health and ammo) in Nightmare mode, so that it would be slightly easier for playing.
Reception.
Because of the popularity of the original Hexen, the game was heavily anticipated. Upon its release, "Hexen II" received mixed to positive reviews. IGN gave the game a score of 7.8 out of 10; however, GameSpot rated the game (7.3 out of 10) much lower than it rated the expansion (8.6 out of 10).

</doc>
<doc id="13614" url="http://en.wikipedia.org/wiki?curid=13614" title="Heretic II">
Heretic II

Heretic II is a dark fantasy action-adventure game developed by Raven Software and published by Activision in 1998 continuing the story of Corvus, the main character from its predecessor, "Heretic".
Using a modified Quake II engine, the game features a mix of a third-person camera with a first-person shooter's action, making for a new gaming experience at the time. While progressive, this was a controversial design decision among fans of the original title, a well-known first-person shooter built on the Doom engine. The music was composed by Kevin Schilder. Gerald Brom contributed conceptual work to characters and creatures for the game. This is the only Heretic/Hexen video game that is unrelated to id Software, apart from its role as engine licenser.
"Heretic II" was later ported to Linux by Loki Software and to the Amiga by Hyperion Entertainment and Macintosh by MacPlay.
Gameplay.
Players control Corvus from a camera fixed behind the player in 3rd-person perspective. Players are able to use a combination of both melee and ranged attacks, similar to its predecessor. Defensive spells are also available, and they draw from a separate ammunition pool. The game consists of a wide variety of high fantasy medieval backdrops to Corvus's adventure. The third-person perspective and three-dimensional game environment allowed developers to introduce a wide variety of gymnastic moves, like pole vaulting, in a much more dynamic environment than the original game's engine could produce. Both games invite comparison with their respective game-engine namesake: the original Heretic was built on the "Doom" engine, and Heretic II was built using the Quake II engine, later known as id Tech 2. Heretic II was favorably received at release because it took a different approach to its design.
Plot.
After Corvus returns from his banishment, he finds that a mysterious plague has swept the land of Parthoris, taking the sanity of those it does not kill. Corvus, the protagonist of the first game, is forced to flee his hometown of Silverspring after the infected attack him, but not before he is infected himself. The effects of the disease are held at bay in Corvus’ case because he holds one of the Tomes of Power, but he still must find a cure before he succumbs.
His quest leads him through the city and swamps to a jungle palace, then through a desert canyon and insect hive, followed by a dark network of mines and finally a to a castle on a high mountain where he finds an ancient Seraph named Morcalavin. Morcalavin is trying to reach immortality using the seven Tomes of Power, but he uses a false tome, as Corvus has one of them. This has caused Morcalavin to go insane and create the plague. During a battle between Corvus and Morcalavin, Corvus switches the false tome for his real one, curing Morcalavin’s insanity and ending the plague.

</doc>
<doc id="13615" url="http://en.wikipedia.org/wiki?curid=13615" title="Household hardware">
Household hardware

Household hardware, or simply, hardware, is a general term for equipment that can be touched/held by hand such as screws, nuts, washers, keys, locks, hinges, latches, handles, wire, chains, belts, plumbing supplies, electrical supplies, tools, utensils, cutlery and machine parts. Household hardware is typically sold in hardware stores.

</doc>
<doc id="13616" url="http://en.wikipedia.org/wiki?curid=13616" title="Howard Carter">
Howard Carter

Howard Carter (9 May 1874 – 2 March 1939) was an English archaeologist and Egyptologist who became world famous after discovering the intact tomb of 14th century BC pharaoh Tutankhamun (colloquially known as "King Tut" and "the boy king") in November 1922.
Early life.
Howard Carter was born in Kensington, London, the son of Samuel Carter, an artist, and Martha Joyce Carter. His father trained and developed Howard's artistic talents.
Howard Carter spent much of his childhood with relatives in the Norfolk market town of Swaffham, the birthplace of both his parents. In 1891 the Egypt Exploration Fund (EEF) sent Carter to assist Percy Newberry in the excavation and recording of Middle Kingdom tombs at Beni Hasan. 
Although only 17, Carter was innovative in improving the methods of copying tomb decoration. In 1892 he worked under the tutelage of Flinders Petrie for one season at Amarna, the capital founded by the pharaoh Akhenaten. From 1894 to 1899 he worked with Édouard Naville at Deir el-Bahari, where he recorded the wall reliefs in the temple of Hatshepsut.
In 1899, Carter was appointed as the first chief inspector of the Egyptian Antiquities Service (EAS). He supervised a number of excavations at Thebes (now known as Luxor). In 1904 he was transferred to the Inspectorate of Lower Egypt. Carter was praised for his improvements in the protection of, and accessibility to, existing excavation sites, and his development of a grid-block system for searching for tombs. The Antiquities Service also provided funding for Carter to head his own excavation projects and during this period Carter discovered the Tombs of Thutmose I and Thutmose III, although both tombs had been robbed of treasures long before.
Carter resigned from the Antiquities Service in 1905 after formal inquiry into what became known as the Saqqara Affair, a noisy confrontation between Egyptian site guards and a group of French tourists. Carter sided with the Egyptian personnel.
Tutankhamun's tomb.
In 1907, after three hard years for Carter, Lord Carnarvon employed him to supervise Carnarvon's Egyptian excavations in the Valley of the Kings. The intention of Gaston Maspero, who introduced the two, was to ensure that Howard Carter imposed modern archaeological methods and systems of recording.
Carnarvon financed Carter's work in the Valley of the Kings to 1914, but until 1917 excavations and study were interrupted by the First World War. Following the end of the First World War, Carter aggressively resumed his work.
After several years of finding little, Lord Carnarvon became dissatisfied with the lack of results, and in 1922 informed Carter that he had one more season of funding to search the Valley of the Kings and find the tomb.
On 4 November 1922, Howard Carter's excavation group found steps which Carter hoped led to Tutankhamun's tomb (subsequently designated KV62) (the tomb that would be considered the best preserved and most intact pharaonic tomb ever found in the Valley of the Kings).
He wired Lord Carnarvon to come, and on 26 November 1922, with Carnarvon, Carnarvon's daughter and others in attendance, Carter made the "tiny breach in the top left hand corner" of the doorway (with a chisel his grandmother had given him for his 17th birthday.) He was able to peer in by the light of a candle and see that many of the gold and ebony treasures were still in place. He did not yet know whether it was "a tomb or merely a cache", but he did see a promising sealed doorway between two sentinel statues. When Carnarvon asked "Can you see anything?", Carter replied with the famous words: "Yes, wonderful things!"
The next several months were spent cataloging the contents of the antechamber under the "often stressful" supervision of Pierre Lacau, director general of the Department of Antiquities of Egypt. On 16 February 1923, Carter opened the sealed doorway, and found that it did indeed lead to a burial chamber, and he got his first glimpse of the sarcophagus of Tutankhamun. All of these discoveries were eagerly covered by the world's press, but most of their representatives were kept in their hotels; only H. V. Morton was allowed on the scene, and his vivid descriptions helped to cement Carter's reputation with the British public.
Carter's own notes and photographic evidence indicate that he, Lord Carnarvon and Lady Evelyn Herbert entered the burial chamber shortly after the tomb's discovery and before the official opening.
Later work and death.
The clearance of the tomb with its thousands of objects continued until 1932. Following his sensational discovery, Howard Carter retired from archaeology and became a part-time agent for collectors and museums, including the Cleveland Museum of Art and the Detroit Institute of Arts. He visited the United States in 1924, and gave a series of illustrated lectures in New York and other cities in the United States that were attended by very large and enthusiastic audiences, sparking Egyptomania in America.
He died of lymphoma in Kensington, London, on 2 March 1939 at the age of 64. The archaeologist's natural death so long after the opening of the tomb, despite being the leader of the expedition, is the piece of evidence most commonly put forward by sceptics to refute the idea of a "curse of the pharaohs" plaguing the party that might have "violated" Tutankhamun's tomb.
Carter is now buried in Putney Vale Cemetery in London. His epitaph reads ""May your spirit live, may you spend millions of years, you who love Thebes, sitting with your face to the north wind, your eyes beholding happiness", a quotation taken from the Wishing Cup of Tutankhamun, and "O night, spread thy wings over me as the imperishable stars"".
In popular culture.
Film and television.
Carter has been portrayed by the following actors:
Literature.
He appears as a character throughout most of the "Amelia Peabody" series of books by Elizabeth Peters (a pseudonym of Egyptologist Dr Barbara Mertz); and in much of Arthur Phillips's "The Egyptologist".
In the book "The Tutankhamun Affair" by Christian Jacq he is a key character.
He appears as a main character in "A Cloudy Day on the West Side", a novel by Egyptian writer Muhammad Al-Mansi Qindeel.
James Patterson and Martin Dugard's book "The Murder of King Tut" focuses on Carter's search for King Tut's tomb.
He is referenced in Hergé's "The Adventures of Tintin, The Seven Crystal Balls" published in 1944 by Le Soir. ISBN 2-203-00112-7
He is referenced in "Wedding of the Season" by Laura Lee Guhrke. In this historical romance novel, Carter's telegram to the fictional British Egyptologist the Duke of Sunderland reports discovering "steps to a new tomb" and creates a climactic conflict. Published 2011 by Avon Books. ISBN 978-0-06-196315-5
He is referenced in Rick Riordan's "The Kane Chronicles", The Red Pyramid. In this novel, Carter Kane said that his father, Julius Kane, had named him after Howard Carter.
He is referenced in Sally Beauman's "The Visitors", a novel re-creation of the hunt for Tutankhamun’s tomb in Egypt’s Valley of the Kings.
Music.
"In Search of the Pharaohs" is a 30-minute cantata for narrator, junior choir and piano by composer Robert Steadman and commissioned by the City of London Freemen's School, which uses extracts from Carter's diaries as its text.
The Finnish metal band Nightwish mentions Carter in the song "Tutankhamun" on its début album "Angels Fall First": "For Carter has come / To free my beloved".
Art.
A paraphrased extract from Howard Carter's diary of 26 November 1922 is used as the plaintext for Part 3 of the encrypted Kryptos sculpture at the CIA Headquarters in Langley, Virginia.
On 9 May 2012 Google commemorated his 138th birthday with a Google doodle.
Further reading.
</dl>

</doc>
<doc id="13617" url="http://en.wikipedia.org/wiki?curid=13617" title="History of Scotland">
History of Scotland

The history of Scotland is known to have begun by the end of the last glacial period (in the paleolithic), roughly 10,000 years ago. Prehistoric Scotland entered the Neolithic Era about 4000 BC, the Bronze Age about 2000 BC, and the Iron Age around 700 BC. Scotland's recorded history began with the arrival of the Roman Empire in the 1st century, when the province of Britannia reached as far north as the line between the firths of Clyde to the Forth. North of this was Caledonia, whose people were known in Latin as "Picti", "the painted ones". Constant risings forced Rome's legions back: Hadrian's Wall attempted to seal off the Roman south and the Antonine Wall attempted to move the Roman border north. The latter was swiftly abandoned and the former overrun, most spectacularly during the Great Conspiracy of the 360s. As Rome finally withdrew from Britain, Gaelic raiders called the "Scoti" began colonizing Western Scotland and Wales.
According to 9th- and 10th-century sources, the Gaelic kingdom of Dál Riata was founded on the west coast of Scotland in the 6th century. In the following century, the Irish missionary Columba founded a monastery on Iona and introduced the previously pagan Scoti and pagan Picts to Celtic Christianity. Following England's Gregorian mission, the Pictish king Nechtan chose to abolish most Celtic practices in favor of the Roman rite, restricting Gaelic influence on his kingdom and avoiding war with Saxon Northumbria. Towards the end of the 8th century, the Viking invasions began. Successive defeats by the Norse forced the Picts and Gaels to cease their historic hostility to each other and to unite in the 9th century, forming the Kingdom of Scotland.
The Kingdom of Scotland was united under the descendants of Kenneth MacAlpin, first king of a united Scotland. His descendants, known to modern historians as the House of Alpin, fought among each other during frequent disputed successions. The last Alpin king, Malcolm II, died without issue in the early 11th century and the kingdom passed through his daughter's son, Duncan I, who started a new line of kings known to modern historians as the House of Dunkeld or Canmore. The last Dunkeld king, Alexander III, died in 1286 leaving only a single infant granddaughter as heir; four years later, Margaret, Maid of Norway herself died in a tragic shipwreck "en route" to Scotland. England, under Edward I, would take advantage of the questioned succession in Scotland to launch a series of conquests into Scotland. The resulting Wars of Scottish Independence were fought in the late 13th and early 14th centuries as Scotland passed back and forth between the House of Balliol and the House of Bruce. Scotland's ultimate victory in the Wars of Independence under David II confirmed Scotland as a fully independent and sovereign kingdom. When David II died without issue, his nephew Robert II established the House of Stewart (the spelling would be changed to Stuart in the 16th century), which would rule Scotland uncontested for the next three centuries. James VI, Stuart king of Scotland, also inherited the throne of England in 1603, and the Stuart kings and queens ruled both independent kingdoms until the Act of Union in 1707 merged the two kingdoms into a new state, the Kingdom of Great Britain. Queen Anne was the last Stuart monarch, ruling until 1714. Since 1714, the succession of the British monarchs of the houses of Hanover and Saxe-Coburg and Gotha (Windsor) has been due to their descent from James VI and I of the House of Stuart.
During the Scottish Enlightenment and Industrial Revolution, Scotland became one of the commercial, intellectual and industrial powerhouses of Europe. Its industrial decline following the Second World War was particularly acute, but in recent decades the country has enjoyed something of a cultural and economic renaissance, fuelled in part by a resurgent financial services sector, the proceeds of North Sea oil and gas.
Prehistory.
People lived in Scotland for at least 8,500 years before Britain's recorded history. At times during the last interglacial period (130,000–70,000 BC) Europe had a climate warmer than today's, and early humans may have made their way to Scotland, with the discovery of ten pre-ice age axes on Orkney and mainland Scotland. Glaciers then scoured their way across most of Britain, and only after the ice retreated did Scotland again become habitable, around 9600 BC. Mesolithic hunter-gatherer encampments formed the first known settlements, and archaeologists have dated an encampment near Biggar to around 8500 BC. Numerous other sites found around Scotland build up a picture of highly mobile boat-using people making tools from bone, stone and antlers. The oldest house for which there is evidence in Britain is the oval structure of wooden posts found at South Queensferry near the Firth of Forth, dating from the Mesolithic period, about 8240 BC. The earliest stone structures are probably the three hearths found at Jura, dated to about 6000 BC.
Neolithic farming brought permanent settlements. Evidence of these includes the well-preserved stone house at Knap of Howar on Papa Westray, dating from around 3500 BC and the village of similar houses at Skara Brae on West Mainland, Orkney from about 500 years later. The settlers introduced chambered cairn tombs from around 3500 BC, as at Maeshowe, and from about 3000 BC the many standing stones and circles such as those at Stenness on the mainland of Orkney, which date from about 3100 BC, of four stones, the tallest of which is 16 ft in height. These were part of a pattern that developed in many regions across Europe at about the same time.
The creation of cairns and Megalithic monuments continued into the Bronze Age, which began in Scotland about 2000 BC. As elsewhere in Europe, hill forts were first introduced in this period, including the occupation of Eildon Hill near Melrose in the Scottish Borders, from around 1000 BC, which accommodated several hundred houses on a fortified hilltop. From the Early and Middle Bronze Age there is evidence of cellular round houses of stone, as at Jarlshof and Sumburgh on Shetland. There is also evidence of the occupation of crannogs, roundhouses partially or entirely built on artificial islands, usually in lakes, rivers and estuarine waters.
In the early Iron Age, from the seventh century BC, cellular houses began to be replaced on the northern isles by simple Atlantic roundhouses, substantial circular buildings with a dry stone construction. From about 400 BC, more complex Atlantic roundhouses began to be built, as at Howe, Orkney and Crosskirk, Caithness. The most massive constructions that date from this era are the circular broch towers, probably dating from about 200 BC. This period also saw the first wheelhouses, a roundhouse with a characteristic outer wall, within which was a circle of stone piers (bearing a resemblance to the spokes of a wheel), but these would flourish most in the era of Roman occupation. There is evidence for about 1,000 Iron Age hill forts in Scotland, most located below the Clyde-Forth line, which have suggested to some archaeologists the emergence of a society of petty rulers and warrior elites recognisable from Roman accounts.
Roman invasion.
The surviving pre-Roman accounts of Scotland originated with the Greek Pytheas of Massalia, who may have circumnavigated the British Isles of Albion (Britain) and Ierne (Ireland) sometime around 325 BC. The most northerly point of the island of Great Britain was called "Orcas" (Orkney). By the time of Pliny the Elder, who died in AD 79, Roman knowledge of the geography of Scotland had extended to the "Hebudes" (The Hebrides), "Dumna" (probably the Outer Hebrides), the Caledonian Forest and the people of the Caledonii, from whom the Romans named the region north of their control Caledonia. Ptolemy, possibly drawing on earlier sources of information as well as more contemporary accounts from the Agricolan invasion, identified 18 tribes in Scotland in his "Geography", but many of the names are obscure and the geography becomes less reliable in the north and west, suggesting early Roman knowledge of these area was confined to observations from the sea.
The Roman invasion of Britain began in earnest in AD 43, leading to the establishment of the Roman province of Britannia in the south. By the year 71, the Roman governor Quintus Petillius Cerialis had launched an invasion of what is now Scotland. In the year 78, Gnaeus Julius Agricola arrived in Britain to take up his appointment as the new governor and began a series of major incursions. He is said to have pushed his armies to the estuary of the "River Taus" (usually assumed to be the River Tay) and established forts there, including a legionary fortress at Inchtuthil. After his victory over the northern tribes at Mons Graupius in 84, a series of forts and towers were established along the Gask Ridge, which marked the boundary between the Lowland and Highland zones, probably forming the first Roman "limes" or frontier in Scotland. Agricola's successors were unable or unwilling to further subdue the far north. By the year 87, the occupation was limited to the Southern Uplands and by the end of the first century the northern limit of Roman expansion was a line drawn between the Tyne and Solway Firth. The Romans eventually withdrew to a line in what is now northern England, building the fortification known as Hadrian's Wall from coast to coast.
Around 141, the Romans undertook a reoccupation of southern Scotland, moving up to construct a new "limes" between the Firth of Forth and the Firth of Clyde, which became the Antonine Wall. The largest Roman construction inside Scotland, it is a sward-covered wall made of turf around 20 ft high, with nineteen forts. It extended for 37 miles. Having taken twelve years to build, the wall was overrun and abandoned soon after 160. The Romans retreated to the line of Hadrian's Wall. Roman troops penetrated far into the north of modern Scotland several more times, with at least four major campaigns. The most notable invasion was in 209 when the emperor Septimius Severus led a major force north. After the death of Severus in 210 they withdrew south to Hadrian's Wall, which would be Roman frontier until it collapsed in the 5th century. By the close of the Roman occupation of southern and central Britain in the 5th century, the Picts had emerged as the dominant force in northern Scotland, with the various Brythonic tribes the Romans had first encountered there occupying the southern half of the country. Roman influence on Scottish culture and history was not enduring.
Post-Roman Scotland.
In the centuries after the departure of the Romans from Britain, there were four groups within the borders of what is now Scotland. In the east were the Picts, with kingdoms between the river Forth and Shetland. In the late 6th century the dominant force was the Kingdom of Fortriu, whose lands were centred on Strathearn and Menteith and who raided along the eastern coast into modern England. In the west were the Gaelic (Goidelic)-speaking people of Dál Riata with their royal fortress at Dunadd in Argyll, with close links with the island of Ireland, from whom comes the name Scots. In the south was the British (Brythonic) Kingdom of Strathclyde, descendants of the peoples of the Roman influenced kingdoms of "The Old North", often named Alt Clut, the Brythonic name for their capital at Dumbarton Rock. Finally, there were the English or "Angles", Germanic invaders who had overrun much of southern Britain and held the Kingdom of Bernicia, in the south-east. The first English king in the historical record is Ida, who is said to have obtained the throne and the kingdom about 547. Ida’s grandson, Æthelfrith, united his kingdom with Deira to the south to form Northumbria around the year 604. There were changes of dynasty, and the kingdom was divided, but it was re-united under Æthelfrith's son Oswald (r. 634-42).
Scotland was largely converted to Christianity by Irish-Scots missions associated with figures such as St Columba, from the fifth to the seventh centuries. These missions tended to found monastic institutions and collegiate churches that served large areas. Partly as a result of these factors, some scholars have identified a distinctive form of Celtic Christianity, in which abbots were more significant than bishops, attitudes to clerical celibacy were more relaxed and there was some significant differences in practice with Roman Christianity, particularly the form of tonsure and the method of calculating Easter, although most of these issues had been resolved by the mid-7th century.
Rise of the Kingdom of Alba.
Conversion to Christianity may have speeded a long term process of gaelicisation of the Pictish kingdoms, which adopted Gaelic language and customs. There was also a merger of the Gaelic and Pictish crowns, although historians debate whether it was a Pictish takeover of Dál Riata, or the other way around. This culminated in the rise of Cínaed mac Ailpín (Kenneth MacAlpin) in the 840s, which brought to power the House of Alpin. In 867 AD the Vikings seized the southern half of Northumbria, forming the Kingdom of York; three years later they stormed the Britons’ fortress of Dumbarton and subsequently conquered much of England except for a reduced Kingdom of Wessex, leaving the new combined Pictish and Gaelic kingdom almost encircled. When he died as king of the combined kingdom in 900, Domnall II (Donald II) was the first man to be called "rí Alban" (i.e. "King of Alba"). The term Scotia was increasingly used to describe the kingdom between North of the Forth and Clyde and eventually the entire area controlled by its kings was referred to as Scotland.
The long reign (900–942/3) of Causantín (Constantine II) is often regarded as the key to formation of the Kingdom of Alba. He was later credited with bringing Scottish Christianity into conformity with the Catholic Church. After fighting many battles, his defeat at Brunanburh was followed by his retirement as a Culdee monk at St. Andrews. The period between the accession of his successor Máel Coluim I (Malcolm I) and Máel Coluim mac Cináeda (Malcolm II) was marked by good relations with the Wessex rulers of England, intense internal dynastic disunity and relatively successful expansionary policies. In 945, Máel Coluim I annexed Strathclyde as part of a deal with King Edmund of England, where the kings of Alba had probably exercised some authority since the later 9th century, an event offset somewhat by loss of control in Moray. The reign of King Donnchad I (Duncan I) from 1034 was marred by failed military adventures, and he was defeated and killed by MacBeth, the Mormaer of Moray, who became king in 1040. MacBeth ruled for seventeen years before he was overthrown by Máel Coluim, the son of Donnchad, who some months later defeated MacBeth's step-son and successor Lulach to become king Máel Coluim III (Malcolm III).
It was Máel Coluim III, who acquired the nickname "Canmore" ("Cenn Mór", "Great Chief"), which he passed to his successors and who did most to create the Dunkeld dynasty that ruled Scotland for the following two centuries. Particularly important was his second marriage to the Anglo-Hungarian princess Margaret. This marriage, and raids on northern England, prompted William the Conqueror to invade and Máel Coluim submitted to his authority, opening up Scotland to later claims of sovereignty by English kings. When Malcolm died in 1093, his brother Domnall III (Donald III) succeeded him. However, William II of England backed Máel Coluim's son by his first marriage, Donnchad, as a pretender to the throne and he seized power. His murder within a few months saw Domnall restored with one of Máel Coluim sons by his second marriage, Edmund, as his heir. The two ruled Scotland until two of Edmund's younger brothers returned from exile in England, again with English military backing. Victorious, Edgar, the oldest of the three, became king in 1097. Shortly afterwards Edgar and the King of Norway, Magnus Bare Legs concluded a treaty recognizing Norwegian authority over the Western Isles. In practice Norse control of the Isles was loose, with local chiefs enjoying a high degree of independence. He was succeeded by his brother Alexander, who reigned 1107–24.
When Alexander died in 1124, the crown passed to Margaret's fourth son David I, who had spent most of his life as an English baron. His reign saw what has been characterised as a "Davidian Revolution", by which native institutions and personnel were replaced by English and French ones, underpinning the development of later Medieval Scotland. Members of the Anglo-Norman nobility took up places in the Scottish aristocracy and he introduced a system of feudal land tenure, which produced knight service, castles and an available body of heavily armed cavalry. He created an Anglo-Norman style of court, introduced the office of justicar to oversee justice, and local offices of sheriffs to administer localities. He established the first royal burghs in Scotland, granting rights to particular settlements, which led to the development of the first true Scottish towns and helped facilitate economic development as did the introduction of the first recorded Scottish coinage. He was continued a process begun by his mother and brothers helping to establish foundations who brought reform to Scottish monasticism based on those at Cluny and he played a part in the organisation of diocese on lines closer to those in the rest of Western Europe.
These reforms were pursued under his successors and grandchildren Malcolm IV of Scotland and William I, with the crown now passing down the main line of descent through primogeniture, leading to the first of a series of minorities. The benefits of greater authority were reaped by William's son Alexander II and his son Alexander III, who pursued a policy of peace with England to expand their authority in the Highlands and Islands. By the reign of Alexander III, the Scots were in a position to annexe the remainder of the western seaboard, which they did following Haakon Haakonarson's ill-fated invasion and the stalemate of the Battle of Largs with the Treaty of Perth in 1266.
The Wars of Independence.
The death of king Alexander III in 1286, and the death of his granddaughter and heir Margaret, Maid of Norway in 1290, left 14 rivals for succession. To prevent civil war the Scottish magnates asked Edward I of England to arbitrate, for which he extracted legal recognition that the realm of Scotland was held as a feudal dependency to the throne of England before choosing John Balliol, the man with the strongest claim, who became king in 1292. Robert Bruce, 5th Lord of Annandale, the next strongest claimant, accepted this outcome with reluctance. Over the next few years Edward I used the concessions he had gained to systematically undermine both the authority of King John and the independence of Scotland. In 1295 John, on the urgings of his chief councillors, entered into an alliance with France, known as the Auld Alliance.
In 1296 Edward invaded Scotland, deposing King John. The following year William Wallace and Andrew de Moray raised forces to resist the occupation and under their joint leadership an English army was defeated at the Battle of Stirling Bridge. For a short time Wallace ruled Scotland in the name of John Balliol as Guardian of the realm. Edward came north in person and defeated Wallace at the Battle of Falkirk (1298). Wallace escaped but probably resigned as Guardian of Scotland. In 1305 he fell into the hands of the English, who executed him for treason despite the fact that he owed no allegiance to England.
Rivals John Comyn and Robert the Bruce, grandson of the claimant, were appointed as joint guardians in his place. On 10 February 1306, Bruce participated in the murder of Comyn, at Greyfriars Kirk in Dumfries. Less than seven weeks later, on 25 March, Bruce was crowned as King. However, Edward's forces overran the country after defeating Bruce's small army at the Battle of Methven. Despite the excommunication of Bruce and his followers by Pope Clement V, his support slowly strengthened; and by 1314 with the help of leading nobles such as Sir James Douglas and Thomas Randolph only the castles at Bothwell and Stirling remained under English control. Edward I had died in 1307. His heir Edward II moved an army north to break the siege of Stirling Castle and reassert control. Robert defeated that army at the Battle of Bannockburn in 1314, securing "de facto" independence. In 1320 the Declaration of Arbroath, a remonstrance to the Pope from the nobles of Scotland, helped convince Pope John XXII to overturn the earlier excommunication and nullify the various acts of submission by Scottish kings to English ones so that Scotland's sovereignty could be recognised by the major European dynasties. The Declaration has also been seen as one of the most important documents in the development of a Scottish national identity.
In 1326, what may have been the first full Parliament of Scotland met. The parliament had evolved from an earlier council of nobility and clergy, the "colloquium", constituted around 1235, but perhaps in 1326 representatives of the burghs — the burgh commissioners — joined them to form the Three Estates. In 1328, Edward III signed the Treaty of Northampton acknowledging Scottish independence under the rule of Robert the Bruce. However, four years after Robert's death in 1329, England once more invaded on the pretext of restoring Edward Balliol, son of John Balliol, to the Scottish throne, thus starting the Second War of Independence. Despite victories at Dupplin Moor and Halidon Hill, in the face of tough Scottish resistance led by Sir Andrew Murray, the son of Wallace's comrade in arms, successive attempts to secure Balliol on the throne failed. Edward III lost interest in the fate of his protégé after the outbreak of the Hundred Years' War with France. In 1341 David II, King Robert's son and heir, was able to return from temporary exile in France. Balliol finally resigned his claim to the throne to Edward in 1356, before retiring to Yorkshire, where he died in 1364.
The Stewarts.
After David II's death, Robert II, the first of the Stewart kings, came to the throne in 1371. He was followed in 1390 by his ailing son John, who took the regnal name Robert III. During Robert III's reign (1390–1406), actual power rested largely in the hands of his brother, Robert Stewart, Duke of Albany. After the suspicious death (possibly on the orders of the Duke of Albany) of his elder son, David, Duke of Rothesay in 1402, Robert, fearful for the safety of his younger son, the future James I, sent him to France in 1406. However, the English captured him en route and he spent the next 18 years as a prisoner held for ransom. As a result, after the death of Robert III, regents ruled Scotland: first, the Duke of Albany; and later his son Murdoch. When Scotland finally paid the ransom in 1424, James, aged 32, returned with his English bride determined to assert this authority. Several of the Albany family were executed; but he succeeded in centralising control in the hands of the crown, at the cost of increasing unpopularity, and was assassinated in 1437. His son James II (reigned 1437–1460), when he came of age in 1449, continued his father's policy of weakening the great noble families, most notably taking on the powerful Black Douglas family that had come to prominence at the time of the Bruce.
In 1468 the last significant acquisition of Scottish territory occurred when James III was engaged to Margaret of Denmark, receiving the Orkney Islands and the Shetland Islands in payment of her dowry. Berwick upon Tweed was captured by England in 1482. With the death of James III in 1488 at the Battle of Sauchieburn, his successor James IV successfully ended the quasi-independent rule of the Lord of the Isles, bringing the Western Isles under effective Royal control for the first time. In 1503, he married Margaret Tudor, daughter of Henry VII of England, thus laying the foundation for the 17th century Union of the Crowns.
Scotland advanced markedly in educational terms during the 15th century with the founding of the University of St Andrews in 1413, the University of Glasgow in 1450 and the University of Aberdeen in 1495, and with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools. James IV's reign is often considered to have seen a flowering of Scottish culture under the influence of the European Renaissance.
In 1512 the Auld Alliance was renewed and under its terms, when the French were attacked by the English under Henry VIII, James IV invaded England in support. The invasion was stopped decisively at the Battle of Flodden Field during which the King, many of his nobles, and a large number of ordinary troops were killed, commemorated by the song "Flowers of the Forest". Once again Scotland's government lay in the hands of regents in the name of the infant James V.
James V finally managed to escape from the custody of the regents in 1528. He continued his father's policy of subduing the rebellious Highlands, Western and Northern isles and the troublesome borders. He also continued the French alliance, marrying first the French noblewoman Madeleine of Valois and then after her death Marie of Guise. James V's domestic and foreign policy successes were overshadowed by another disastrous campaign against England that led to defeat at the Battle of Solway Moss (1542). James died a short time later, a demise blamed by contemporaries on "a broken heart". The day before his death, he was brought news of the birth of an heir: a daughter, who would become Mary, Queen of Scots.
Once again, Scotland was in the hands of a regent. Within two years, the Rough Wooing began, Henry VIII's military attempt to force a marriage between Mary and his son, Edward. This took the form of border skirmishing and several English campaigns into Scotland. In 1547, after the death of Henry VIII, forces under the English regent Edward Seymour, 1st Duke of Somerset were victorious at the Battle of Pinkie Cleugh, the climax of the Rough Wooing, and followed up by the occupation of Haddington. Mary was then sent to France at the age of five, as the intended bride of the heir to the French throne. Her mother, Marie de Guise, stayed in Scotland to look after the interests of Mary — and of France — although the Earl of Arran acted officially as regent. Guise responded by calling on French troops, who helped stiffen resistance to the English occupation. By 1550, after a change of regent in England, the English withdrew from Scotland completely.
From 1554, Marie de Guise, took over the regency, and continued to advance French interests in Scotland. French cultural influence resulted in a large influx of French vocabulary into Scots. But anti-French sentiment also grew, particularly among Protestants, who saw the English as their natural allies. In 1560 Marie de Guise died, and soon after the Auld Alliance also ended, with the signing of the Treaty of Edinburgh, which provided for the removal of French and English troops from Scotland. The Scottish Reformation took place only days later when the Scottish Parliament abolished the Roman Catholic religion and outlawed the Mass.
Meanwhile Queen Mary had been raised as a Catholic in France, and married to the Dauphin, who became king as Francis II in 1559, making her queen consort of France. When Francis died in 1560, Mary, now 19, returned to Scotland to take up the government. Despite her private religion, she did not attempt to re-impose Catholicism on her largely Protestant subjects, thus angering the chief Catholic nobles. Her six-year personal reign was marred by a series of crises, largely caused by the intrigues and rivalries of the leading nobles. The murder of her secretary, David Riccio, was followed by that of her unpopular second husband Lord Darnley, and her abduction by and marriage to the Earl of Bothwell, who was implicated in Darnley's murder. Mary and Bothwell confronted the lords at Carberry Hill and after their forces melted away, he fled and she was captured by Bothwell's rivals. Mary was imprisoned in Loch Leven Castle, and in July 1567, was forced to abdicate in favour of her infant son James VI. Mary eventually escaped and attempted to regain the throne by force. After her defeat at the Battle of Langside in 1568, she took refuge in England, leaving her young son in the hands of regents. In Scotland the regents fought a civil war on behalf of James VI against his mother's supporters. In England, Mary became a focal point for Catholic conspirators and was eventually tried for treason and executed on the orders of her kinswoman Elizabeth I.
Protestant Reformation.
During the 16th century, Scotland underwent a Protestant Reformation that created a predominately Calvinist national Kirk, which became Presbyterian in outlook and severely reduced the powers of bishops. In the earlier part of the century, the teachings of first Martin Luther and then John Calvin began to influence Scotland, particularly through Scottish scholars, often training for the priesthood, who had visited Continental universities. The Lutheran preacher Patrick Hamilton was executed for heresy in St. Andrews in 1528. The execution of others, especially the Zwingli-influenced George Wishart, who was burnt at the stake on the orders of Cardinal Beaton in 1546, angered Protestants. Wishart's supporters assassinated Beaton soon after and seized St. Andrews Castle, which they held for a year before they were defeated with the help of French forces. The survivors, including chaplain John Knox, were condemned to be galley slaves in France, stoking resentment of the French and creating martyrs for the Protestant cause.
Limited toleration and the influence of exiled Scots and Protestants in other countries, led to the expansion of Protestantism, with a group of lairds declaring themselves Lords of the Congregation in 1557 and representing their interests politically. The collapse of the French alliance and English intervention in 1560 meant that a relatively small, but highly influential, group of Protestants were in a position to impose reform on the Scottish church. A confession of faith, rejecting papal jurisdiction and the mass, was adopted by Parliament in 1560, while the young Mary, Queen of Scots, was still in France.
Knox, having escaped the galleys and spent time in Geneva as a follower of Calvin, emerged as the most significant figure of the period. The Calvinism of the reformers led by Knox resulted in a settlement that adopted a Presbyterian system and rejected most of the elaborate trappings of the medieval church. The reformed Kirk gave considerable power to local lairds, who often had control over the appointment of the clergy. There were widespread, but generally orderly outbreaks of iconoclasm. At this point the majority of the population was probably still Catholic in persuasion and the Kirk found it difficult to penetrate the Highlands and Islands, but began a gradual process of conversion and consolidation that, compared with reformations elsewhere, was conducted with relatively little persecution.
17th century.
In 1603, James VI King of Scots inherited the throne of the Kingdom of England, and became King James I of England, leaving Edinburgh for London, uniting England and Scotland under one monarch. The Union was a personal or dynastic union, with the Crowns remaining both distinct and separate—despite James's best efforts to create a new "imperial" throne of "Great Britain". The acquisition of the Irish crown along with the English, facilitated a process of settlement by Scots in what was historically the most troublesome area of the kingdom in Ulster, with perhaps 50,000 Scots settling in the province by the mid-17th century. Attempts to found a Scottish colony in North America in Nova Scotia were largely unsuccessful, with insufficient funds and willing colonists.
Wars of the Three Kingdoms and the Puritan Commonwealth.
Bishops' Wars.
Although James had tried to get the Scottish Church to accept some of the High Church Anglicanism of his southern kingdom, he met with limited success. His son and successor, Charles I, took matters further, introducing an English-style Prayer Book into the Scottish church in 1637. This resulted in anger and widespread rioting. (The story goes that it was initiated by a certain Jenny Geddes who threw a stool in St Giles Cathedral.) Representatives of various sections of Scottish society drew up the National Covenant in 1638, objecting to the King's liturgical innovations. In November of the same year matters were taken even further, when at a meeting of the General Assembly in Glasgow the Scottish bishops were formally expelled from the Church, which was then established on a full Presbyterian basis. Charles gathered a military force; but as neither side wished to push the matter to a full military conflict, a temporary settlement was concluded at Pacification of Berwick. Matters remained unresolved until 1640 when, in a renewal of hostilities, Charles's northern forces were defeated by the Scots at the Battle of Newburn to the west of Newcastle. During the course of these Bishops' Wars Charles tried to raise an army of Irish Catholics, but was forced to back down after a storm of protest in Scotland and England. The backlash from this venture provoked a rebellion in Ireland and Charles was forced to appeal to the English Parliament for funds. Parliament's demands for reform in England eventually resulted in the English Civil War. This series of civil wars that engulfed England, Ireland and Scotland in the 1640s and 1650s is known to modern historians as the Wars of the Three Kingdoms. The Covenanters meanwhile, were left governing Scotland, where they raised a large army of their own and tried to impose their religious settlement on Episcopalians and Roman Catholics in the north of the country. In England his religious policies caused similar resentment and he ruled without recourse to parliament from 1629.
Civil war.
As the civil wars developed, the English Parliamentarians appealed to the Scots Covenanters for military aid against the King. A Solemn League and Covenant was entered into, guaranteeing the Scottish Church settlement and promising further reform in England. Scottish troops played a major part in the defeat of Charles I, notably at the battle of Marston Moor. An army under the Earl of Leven occupied the North of England for some time.
However, not all Scots supported the Covenanter's taking arms against their King. In 1644, James Graham, 1st Marquess of Montrose attempted to raise the Highlands for the King. Few Scots would follow him, but, aided by 1,000 Irish, Highland and Islesmen troops sent by the Irish Confederates under Alasdair MacDonald (MacColla), and an instinctive genius for mobile warfare, he was stunningly successful. A Scottish Civil War began in September 1644 with his victory at battle of Tippermuir. After a series of victories over poorly trained Covenanter militias, the lowlands were at his mercy. However, at this high point, his army was reduced in size, as MacColla and the Highlanders preferred to continue the war in the north against the Campbells. Shortly after, what was left of his force was defeated at the Battle of Philiphaugh. Escaping to the north, Montrose attempted to continue the struggle with fresh troops; but in July 1646 his army was disbanded after the King surrendered to the Scots army at Newark, and the civil war came to an end.
The following year Charles, while he was being held captive in Carisbrooke Castle, entered into an agreement with moderate Scots Presbyterians. In this secret 'Engagement', the Scots promised military aid in return for the King's agreement to implement Presbyterianism in England on a three-year trial basis. The Duke of Hamilton led an invasion of England to free the King, but he was defeated by Oliver Cromwell in August 1648 at the Battle of Preston.
Cromwellian occupation and Restoration.
The execution of Charles I in 1649 was carried out in the face of objections by the Covenanter government and his son was immediately proclaimed as King Charles II in Edinburgh. Oliver Cromwell led an invasion of Scotland in 1650, and defeated the Scottish army at Dunbar and then defeated a Scottish invasion of England at Worcester on 3 September 1651 (the anniversary of his victory at Dunbar). Cromwell emerged as the leading figure in the English government and Scotland was occupied by an English force under George Monck. The country was incorporated into the Puritan-governed Commonwealth and lost its independent church government, parliament and legal system, but gained access to English markets. Various attempts were made to legitimise the union, calling representatives from the Scottish burghs and shires to negotiations and to various English parliaments, where they were always under-represented and had little opportunity for dissent. However, final ratification was delayed by Cromwell's problems with his various parliaments and the union did not become the subject of an act until 1657 (see Tender of Union).
After the death of Cromwell and the regime's collapse, Charles II was restored in 1660 and Scotland again became an independent kingdom. Scotland regained its system of law, parliament and kirk, but also the Lords of the Articles (by which the crown managed parliament), bishops and a king who did not visit the country. He ruled largely without reference to Parliament, through a series of commissioners. These began with John, Earl of Middleton and ended with the king's brother and heir, James, Duke of York (known in Scotland as the Duke of Albany). The English Navigation Acts prevented the Scots engaging in what would have been lucrative trading with England's colonies. The restoration of episcopacy was a source of trouble, particularly in the south-west of the country, an area with strong Presbyterian sympathies. Abandoning the official church, many of the inhabitants began to attend illegal field assemblies, known as conventicles. Official attempts to suppress these led to a rising in 1679, defeated by James, Duke of Monmouth, the King's illegitimate son, at the Battle of Bothwell Bridge. In the early 1680s a more intense phase of persecution began, later to be called "the Killing Time". When Charles died in 1685 and his brother, a Roman Catholic, succeeded him as James VII of Scotland (and II of England), matters came to a head.
The deposition of James VII.
James put Catholics in key positions in the government and attendance at conventicles was made punishable by death. He disregarded parliament, purged the Council and forced through religious toleration to Roman Catholics, alienating his Protestant subjects. It was believed that the king would be succeeded by his daughter Mary, a Protestant and the wife of William of Orange, Stadtholder of the Netherlands, but when in 1688, James produced a male heir, James Francis Edward Stuart, it was clear that his policies would outlive him. An invitation by seven leading Englishmen led William to land in England with 40,000 men, and James fled, leading to the almost bloodless "Glorious Revolution". The Estates issued a "Claim of Right" that suggested that James had forfeited the crown by his actions (in contrast to England, which relied on the legal fiction of an abdication) and offered it to William and Mary, which William accepted, along with limitations on royal power. The final settlement restored Presbyterianism and abolished the bishops who had generally supported James. However, William, who was more tolerant than the Kirk tended to be, passed acts restoring the Episcopalian clergy excluded after the Revolution.
Although William's supporters dominated the government, there remained a significant following for James, particularly in the Highlands. His cause, which became known as Jacobitism, from the Latin "(Jacobus)" for James, led to a series of risings. An initial Jacobite military attempt was led by John Graham, Viscount Dundee. His forces, almost all Highlanders, defeated William's forces at the Battle of Killiecrankie in 1689, but they took heavy losses and Dundee was slain in the fighting. Without his leadership the Jacobite army was soon defeated at the Battle of Dunkeld. In the aftermath of the Jacobite defeat on 13 February 1692, in an incident since known as the Massacre of Glencoe, 38 members of the Clan MacDonald of Glencoe were killed by members of the Earl of Argyll's Regiment of Foot, on the grounds that they had not been prompt in pledging allegiance to the new monarchs.
Economic crisis of the 1690s.
The closing decade of the 17th century saw the generally favourable economic conditions that had dominated since the Restoration come to an end. There was a slump in trade with the Baltic and France from 1689 to 1691, caused by French protectionism and changes in the Scottish cattle trade, followed by four years of failed harvests (1695, 1696 and 1698-9), an era known as the "seven ill years". The result was severe famine and depopulation, particularly in the north. The Parliament of Scotland of 1695 enacted proposals to help the desperate economic situation, including setting up the Bank of Scotland. The "Company of Scotland Trading to Africa and the Indies" received a charter to raise capital through public subscription.
Failure of Darien scheme.
With the dream of building a lucrative overseas colony for Scotland, the Company of Scotland invested in the Darien scheme, an ambitious plan devised by William Paterson to establish a colony on the Isthmus of Panama in the hope of establishing trade with the Far East. The Darién scheme won widespread support in Scotland as the landed gentry and the merchant class were in agreement in seeing overseas trade and colonialism as routes to upgrade Scotland's economy. Since the capital resources of the Edinburgh merchants and landholder elite were insufficient, the company appealed to middling social ranks, who responded with patriotic fervour to the call for money; the lower classes volunteered as colonists. But the English government opposed the idea: involved in the War of the Grand Alliance from 1689 to 1697 against France, it did not want to offend Spain, which claimed the territory as part of New Granada. The English investors withdrew. Returning to Edinburgh, the Company raised 400,000 pounds in a few weeks. Three small fleets with a total of 3,000 men eventually set out for Panama in 1698. The exercise proved a disaster. Poorly equipped; beset by incessant rain; under attack by the Spanish from nearby Cartagena; and refused aid by the English in the West Indies, the colonists abandoned their project in 1700. Only 1,000 survived and only one ship managed to return to Scotland.
18th century.
Scotland was a poor rural, agricultural society with a population of 1.3 million in 1755. Although Scotland lost home rule, the Union allowed it to break free of a stultifying system and opened the way for the Scottish enlightenment as well as a great expansion of trade and increase in opportunity and wealth. Edinburgh economist Adam Smith concluded in 1776 that "By the union with England, the middling and inferior ranks of people in Scotland gained a complete deliverance from the power of an aristocracy which had always before oppressed them." Historian Jonathan Israel holds that the Union "proved a decisive catalyst politically and economically," by allowing ambitious Scots entry on an equal basis to a rich expanding empire and its increasing trade.
Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly in the next 150 years, following its union with England in 1707 and its integration with the advanced English and imperial economies. The transformation was led by two cities that grew rapidly after 1770. Glasgow, on the river Clyde, was the base for the tobacco and sugar trade with an emerging textile industry. Edinburgh was the administrative and intellectual centre where the Scottish Enlightenment was chiefly based.
Union with England.
By the start of the 18th century, a political union between Scotland and England became politically and economically attractive, promising to open up the much larger markets of England, as well as those of the growing British Empire. With economic stagnation since the late 17th century, which was particularly acute in 1704; the country depended more and more heavily on sales of cattle and linen to England, who used this to create pressure for a union. The Scottish parliament voted on 6 January 1707, by 110 to 69, to adopt the Treaty of Union. It was also a full economic union; indeed, most of its 25 articles dealt with economic arrangements for the new state known as "Great Britain". It added 45 Scots to the 513 members of the House of Commons and 16 Scots to the 190 members of the House of Lords, and ended the Scottish parliament. It also replaced the Scottish systems of currency, taxation and laws regulating trade with laws made in London. Scottish law remained separate from English law, and the religious system was not changed. England had about five times the population of Scotland at the time, and about 36 times as much wealth.
Jacobitism.
Jacobitism was revived by the unpopularity of the union. In 1708 James Francis Edward Stuart, the son of James VII, who became known as "The Old Pretender", attempted an invasion with a French fleet carrying 6,000 men, but the Royal Navy prevented it from landing troops. A more serious attempt occurred in 1715, soon after the death of Anne and the accession of the first Hanoverian king, the eldest son of Sophie, as George I of Great Britain. This rising (known as "The 'Fifteen") envisaged simultaneous uprisings in Wales, Devon, and Scotland. However, government arrests forestalled the southern ventures. In Scotland, John Erskine, Earl of Mar, nicknamed "Bobbin' John", raised the Jacobite clans but proved to be an indecisive leader and an incompetent soldier. Mar captured Perth, but let a smaller government force under the Duke of Argyll hold the Stirling plain. Part of Mar's army joined up with risings in northern England and southern Scotland, and the Jacobites fought their way into England before being defeated at the Battle of Preston, surrendering on 14 November 1715. The day before, Mar had failed to defeat Argyll at the Battle of Sheriffmuir. At this point, James belatedly landed in Scotland, but was advised that the cause was hopeless. He fled back to France. An attempted Jacobite invasion with Spanish assistance in 1719 met with little support from the clans and ended in defeat at the Battle of Glen Shiel.
In 1745 the Jacobite rising known as "The 'Forty-Five" began. Charles Edward Stuart, son of the "Old Pretender", often referred to as "Bonnie Prince Charlie" or the "Young Pretender", landed on the island of Eriskay in the Outer Hebrides. Several clans unenthusiastically joined him. At the outset he was successful, taking Edinburgh and then defeating the only government army in Scotland at the Battle of Prestonpans. The Jacobite army marched into England, took Carlisle and advanced as far as south as Derby. However, it became increasingly evident that England would not support a Roman Catholic Stuart monarch. The Jacobite leadership had a crisis of confidence and they retreated to Scotland as two English armies closed in and Hanoverian troops began to return from the continent. Charles' position in Scotland began to deteriorate as the Whig supporters rallied and regained control of Edinburgh. After an unsuccessful attempt on Stirling, he retreated north towards Inverness. He was pursued by the Duke of Cumberland and gave battle with an exhausted army at Culloden on 16 April 1746, where the Jacobite cause was crushed. Charles hid in Scotland with the aid of Highlanders until September 1746, when he escaped back to France. There were bloody reprisals against his supporters and foreign powers abandoned the Jacobite cause, with the court in exile forced to leave France. The Old Pretender died in 1760 and the Young Pretender, without legitimate issue, in 1788. When his brother, Henry, Cardinal of York, died in 1807, the Jacobite cause was at an end.
Post-Jacobite politics.
With the advent of the Union and the demise of Jacobitism, access to London and the Empire opened up very attractive career opportunities for ambitious middle-class and upper-class Scots, who seized the chance to become entrepreneurs, intellectuals, and soldiers. Thousands of Scots, mainly Lowlanders, took up positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes that “after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland”. Davidson also states that “far from being ‘peripheral’ to the British economy, Scotland – or more precisely, the Lowlands – lay at its core”. British officials especially appreciated Scottish soldiers. As the Secretary of War told Parliament in 1751, "I am for having always in our army as many Scottish soldiers as possible...because they are generally more hardy and less mutinous". The national policy of aggressively recruiting Scots for senior civilian positions stirred up resentment among Englishmen, ranging from violent diatribes by John Wilkes, to vulgar jokes and obscene cartoons in the popular press, and the haughty ridicule by intellectuals such as Samuel Johnson that was much resented by Scots. In his great "Dictionary" Johnson defined oats as, "a grain, which in England is generally given to horses, but in Scotland supports the people." To which Lord Elibank retorted, "Very true, and where will you find such men and such horses?"
Scottish politics in the late 18th century was dominated by the Whigs, with the benign management of Archibald Campbell, 3rd Duke of Argyll (1682–1761), who was in effect the "viceroy of Scotland" from the 1720s until his death in 1761. Scotland generally supported the king with enthusiasm during the American Revolution. Henry Dundas (1742–1811) dominated political affairs in the latter part of the century. Dundas put a brake on intellectual and social change through his ruthless manipulation of patronage in alliance with Prime Minister William Pitt the Younger, until he lost power in 1806.
The main unit of local government was the parish, and since it was also part of the church, the elders imposed public humiliation for what the locals considered immoral behaviour, including fornication, drunkenness, wife beating, cursing and Sabbath breaking. The main focus was on the poor and the landlords ("lairds") and gentry, and their servants, were not subject to the parish's control. The policing system weakened after 1800 and disappeared in most places by the 1850s.
Collapse of the clan system.
After the battle of Culloden the leaders were declared to be traitors, with Jacobite officers executed and many of the rebel soldiers shipped to the colonies as indentured servants. Key laws included the Dress Act 1746, the Act of Proscription 1746, and especially the Heritable Jurisdictions Act of 1746. All aspects of Highland culture, especially the Scottish Gaelic language were forbidden. Parliament also banned the bearing of arms and the wearing of tartans, and limited the activities of the Episcopalian Church. After a generation the Highlands had been transformed and the laws were no longer needed; they were mostly repealed.
Historians debate whether the dramatic changes merely reflect long-term trends that were more-or-less inevitable, or whether government intervention played the decisive role in changing the goals and roles of the chiefs. As Conway (2006) concludes, the new policies "went far beyond earlier efforts to promote economic development in the Highlands and ... represented the first real endeavour to transform the region's social system ... the post-rebellion legislation certainly seems to have accelerated the change." However Devine (1999) and Ray (2001) argue that long-term economic and social changes were already undermining the clan system.
The major result of these changes were the Highland Clearances, by which much of the population of the Highlands suffered forced displacement as lands were enclosed, principally so that they could be used for sheep farming. The clearances followed patterns of agricultural change throughout Britain, but were particularly notorious as a result of the late timing, the lack of legal protection for year-by-year tenants under Scots law, the abruptness of the change from the traditional clan system, and the brutality of many evictions. One result was a continuous exodus from the land—to the cities, or further afield to England, Canada, America or Australia. Many of those who remained were now crofters: poor families living on "crofts"—very small rented farms with indefinite tenure used to raise various crops and animals, with kelping industry (where men burned kelp for the ashes), fishing, spinning of linen and military service as important sources of revenue.
The era of the Napoleonic wars, 1790–1815, brought prosperity, optimism, and economic growth to the Highlands. The economy grew thanks to higher wages, as well as large-scale infrastructure spending such as the Caledonian Canal project. On the East Coast, farmlands were improved, and high prices for cattle brought money to the community. Service in the Army was also attractive to young men from Highlands, who sent pay home and retired there with their army pensions, but the prosperity ended after 1815, and long-run negative factors began to undermine the economic position of the crofters.
Enlightenment.
Historian Jonathan Israel argues that by 1750 Scotland's major cities had created an intellectual infrastructure of mutually supporting institutions, such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was "predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment ." In France Voltaire said "we look to Scotland for all our ideas of civilization," and the Scots in turn paid close attention to French ideas. Historian Bruce Lenman says their "central achievement was a new capacity to recognize and interpret social patterns." The first major philosopher of the Scottish Enlightenment was Francis Hutcheson, who held the Chair of Philosophy at the University of Glasgow from 1729 to 1746. A moral philosopher who produced alternatives to the ideas of Thomas Hobbes, one of his major contributions to world thought was the utilitarian and consequentialist principle that virtue is that which provides, in his words, "the greatest happiness for the greatest numbers". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. He and other Scottish Enlightenment thinkers developed what he called a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behave in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution) and when popularised by Dugald Stewart, would be the basis of classical liberalism. Adam Smith published "The Wealth of Nations", often considered the first work on modern economics. It had an immediate impact on British economic policy and in the 21st century still framed discussions on globalisation and tariffs. The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of the physician and chemist William Cullen, the agriculturalist and economist James Anderson, chemist and physician Joseph Black, natural historian John Walker and James Hutton, the first modern geologist.
Beginnings of industrialisation.
With tariffs with England now abolished, the potential for trade for Scottish merchants was considerable. However, Scotland in 1750 was still a poor rural, agricultural society with a population of 1.3 million. Some progress was visible: agriculture in the Lowlands was steadily upgraded after 1700 and standards remained high. there were the sales of linen and cattle to England, the cash flows from military service, and the tobacco trade that was dominated by Glasgow Tobacco Lords after 1740. Merchants who profited from the American trade began investing in leather, textiles, iron, coal, sugar, rope, sailcloth, glassworks, breweries, and soapworks, setting the foundations for the city's emergence as a leading industrial centre after 1815. The tobacco trade collapsed during the American Revolution (1776–83), when its sources were cut off by the British blockade of American ports. However, trade with the West Indies began to made up for the loss of the tobacco business, reflecting the British demand for sugar and the demand in the West Indies for herring and linen goods.
Linen was Scotland's premier industry in the 18th century and formed the basis for the later cotton, jute, and woollen industries. Scottish industrial policy was made by the Board of Trustees for Fisheries and Manufactures in Scotland, which sought to build an economy complementary, not competitive, with England. Since England had woollens, this meant linen. Encouraged and subsidised by the Board of Trustees so it could compete with German products, merchant entrepreneurs became dominant in all stages of linen manufacturing and built up the market share of Scottish linens, especially in the American colonial market. The British Linen Company, established in 1746, was the largest firm in the Scottish linen industry in the 18th century, exporting linen to England and America. As a joint-stock company, it had the right to raise funds through the issue of promissory notes or bonds. With its bonds functioning as bank notes, the company gradually moved into the business of lending and discounting to other linen manufacturers, and in the early 1770s banking became its main activity. It joined the established Scottish banks such as the Bank of Scotland (Edinburgh, 1695) and the Royal Bank of Scotland (Edinburgh, 1727). Glasgow would soon follow and Scotland had a flourishing financial system by the end of the century. There were over 400 branches, amounting to one office per 7,000 people, double the level in England, where banks were also more heavily regulated. Historians have emphasised that the flexibility and dynamism of the Scottish banking system contributed significantly to the rapid development of the economy in the 19th century.
German sociologist Max Weber mentioned Scottish Presbyterianism in The Protestant Ethic and the Spirit of Capitalism (1905), and many scholars in recent decades argued that "this worldly asceticism" of Calvinism was integral to Scotland's rapid economic modernization.
Religious fragmentation.
In the 1690s the Presbyterian establishment purged the land of Episcopalians and heretics, and made blasphemy a capital crime. Thomas Aitkenhead, the son of an Edinburgh surgeon, aged 18, was indicted for blasphemy by order of the Privy Council for calling the New Testament "The History of the Imposter Christ"; he was hanged in 1696. Their extremism led to a reaction known as the "Moderate" cause that ultimately prevailed and opened the way for liberal thinking in the cities.
The early 18th century saw the beginnings of a fragmentation of the Church of Scotland. These fractures were prompted by issues of government and patronage, but reflected a wider division between the hard-line Evangelicals and the theologically more tolerant Moderate Party. The battle was over fears of fanaticism by the former and the promotion of Enlightenment ideas by the latter. The Patronage Act of 1712 was a major blow to the evangelicals, for it meant that local landlords could choose the minister, not the members of the congregation. Schisms erupted as the evangelicals left the main body, starting in 1733 with the First Secession headed by figures including Ebenezer Erskine. The second schism in 1761 lead to the foundation of the independent Relief Church. These churches gained strength in the Evangelical Revival of the later 18th century. a key result was the main Presbyterian church was in the hands of the Moderate faction, which provided critical support for the Enlightenment in the cities.
Long after the triumph of the Church of Scotland in the Lowlands, Highlanders and Islanders clung to an old-fashioned Christianity infused with animistic folk beliefs and practices. The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw some success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society. Catholicism had been reduced to the fringes of the country, particularly the Gaelic-speaking areas of the Highlands and Islands. Conditions also grew worse for Catholics after the Jacobite rebellions and Catholicism was reduced to little more than a poorly run mission. Also important was Episcopalianism, which had retained supporters through the civil wars and changes of regime in the 17th century. Since most Episcopalians had given their support to the Jacobite rebellions in the early 18th century, they also suffered a decline in fortunes.
Literature.
Although Scotland increasingly adopted the English language and wider cultural norms, its literature developed a distinct national identity and began to enjoy an international reputation. Allan Ramsay (1686–1758) laid the foundations of a reawakening of interest in older Scottish literature, as well as leading the trend for pastoral poetry, helping to develop the Habbie stanza as a poetic form. James Macpherson was the first Scottish poet to gain an international reputation, claiming to have found poetry written by Ossian, he published translations that acquired international popularity, being proclaimed as a Celtic equivalent of the Classical epics. "Fingal" written in 1762 was speedily translated into many European languages, and its deep appreciation of natural beauty and the melancholy tenderness of its treatment of the ancient legend did more than any single work to bring about the Romantic movement in European, and especially in German, literature, influencing Herder and Goethe. Eventually it became clear that the poems were not direct translations from the Gaelic, but flowery adaptations made to suit the aesthetic expectations of his audience. Both the major literary figures of the following century, Robert Burns and Walter Scott, would be highly influenced by the Ossian cycle. Burns, an Ayrshire poet and lyricist, is widely regarded as the national poet of Scotland and a major figure in the Romantic movement. As well as making original compositions, Burns also collected folk songs from across Scotland, often revising or adapting them. His poem (and song) "Auld Lang Syne" is often sung at Hogmanay (the last day of the year), and "Scots Wha Hae" served for a long time as an unofficial national anthem of the country.
Education.
A legacy of the Reformation in Scotland was the aim of having a school in every parish, which was underlined by an act of the Scottish parliament in 1696 (reinforced in 1801). In rural communities this obliged local landowners (heritors) to provide a schoolhouse and pay a schoolmaster, while ministers and local presbyteries oversaw the quality of the education. The headmaster or "dominie" was often university educated and enjoyed high local prestige. The kirk schools were active in the rural lowlands but played a minor role in the Highlands, the islands, and in the fast-growing industrial towns and cities. The schools taught in English, not in Gaelic, because that language was seen as a leftover of Catholicism and was not an expression of Scottish nationalism. In cities such as Glasgow the Catholics operated their own schools, which directed their youth into clerical and middle class occupations, as well as religious vocations.
A "democratic myth" emerged in the 19th century to the effect that many a "lad of pairts" had been able to rise up through the system to take high office and that literacy was much more widespread in Scotland than in neighbouring states, particularly England. Historical research has largely undermined the myth. Kirk schools were not free, attendance was not compulsory and they generally imparted only basic literacy such as the ability to read the Bible. Poor children, starting at age 7, were done by age 8 or 9; the majority were finished by age 11 or 12. The result was widespread basic reading ability; since there was an extra fee for writing, half the people never learned to write. Scots were not significantly better educated than the English and other contemporary nations. A few talented poor boys did go to university, but usually they were helped by aristocratic or gentry sponsors. Most of them became poorly paid teachers or ministers, and none became important figures in the Scottish Enlightenment or the Industrial Revolution.
By the 18th century there were five universities in Scotland, at Edinburgh, Glasgow, St. Andrews and King's and Marischial Colleges in Aberdeen, compared with only two in England. Originally oriented to clerical and legal training, after the religious and political upheavals of the 17th century they recovered with a lecture-based curriculum that was able to embrace economics and science, offering a high quality liberal education to the sons of the nobility and gentry. It helped the universities to become major centres of medical education and to put Scotland at the forefront of Enlightenment thinking.
19th century.
Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly. The population grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. The economy, long based on agriculture, began to industrialize after 1790. At first the leading industry, based in the west, was the spinning and weaving of cotton. In 1861 the American Civil War suddenly cut off the supplies of raw cotton and the industry never recovered. Thanks to its many entrepreneurs and engineers, and its large stock of easily mined coal, Scotland became a world centre for engineering, shipbuilding, and locomotive construction, with steel replacing iron after 1870.
Party politics.
The Scottish Reform Act 1832 increased the number of Scottish MPs and significantly widened the franchise to include more of the middle classes. From this point until the end of the century, the Whigs and (after 1859) their successors the Liberal Party, managed to gain a majority of the Westminster Parliamentary seats for Scotland, although these were often outnumbered by the much larger number of English and Welsh Conservatives. The English-educated Scottish peer Lord Aberdeen (1784–1860) led a coalition government from 1852-5, but in general very few Scots held office in the government. From the mid-century there were increasing calls for Home Rule for Scotland and when the Conservative Lord Salisbury became prime minister in 1885 he responded to pressure by reviving the post of Secretary of State for Scotland, which had been in abeyance since 1746. He appointed the Duke of Richmond, a wealthy landowner who was both Chancellor of Aberdeen University and Lord Lieutenant of Banff. Towards the end of the century Prime Ministers of Scottish descent included the Tory, Peelite and Liberal William E. Gladstone, who held the office four times between 1868 and 1894. The first Scottish Liberal to become prime minister was the Earl of Rosebery, from 1894 to 1895, like Aberdeen before him a product of the English education system. In the later 19th century the issue of Irish Home Rule led to a split among the Liberals, with a minority breaking away to form the Liberal Unionists in 1886. The growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.
Industrial expansion.
From about 1790 textiles became the most important industry in the west of Scotland, especially the spinning and weaving of cotton, which flourished until in 1861 the American Civil War cut off the supplies of raw cotton. The industry never recovered, but by that time Scotland had developed heavy industries based on its coal and iron resources. The invention of the hot blast for smelting iron (1828) revolutionised the Scottish iron industry. As a result Scotland became a centre for engineering, shipbuilding and the production of locomotives. Toward the end of the 19th century, steel production largely replaced iron production. Coal mining continued to grow into the 20th century, producing the fuel to heat homes, factories and drive steam engines locomotives and steamships. By 1914 there were 1,000,000 coal miners in Scotland. The stereotype emerged early on of Scottish colliers as brutish, non-religious and socially isolated serfs; that was an exaggeration, for their life style resembled the miners everywhere, with a strong emphasis on masculinity, equalitarianism, group solidarity, and support for radical labour movements.
Britain was the world leader in the construction of railways, and their use to expand trade and coal supplies. The first successful locomotive-powered line in Scotland, between Monkland and Kirkintilloch, opened in 1831. Not only was good passenger service established by the late 1840s, but an excellent network of freight lines reduce the cost of shipping coal, and made products manufactured in Scotland competitive throughout Britain. For example, railways opened the London market to Scottish beef and milk. They enabled the Aberdeen Angus to become a cattle breed of worldwide reputation. By 1900 Scotland had 3500 miles of railway; their main economic contribution was moving supplies in and product out for heavy industry, especially coal-mining.
Scotland was already one of the most urbanised societies in Europe by 1800. The industrial belt ran across the country from southwest to northeast; by 1900 the four industrialised counties of Lanarkshire, Renfrewshire, Dunbartonshire, and Ayrshire contained 44 per cent of the population. Glasgow became one of the largest cities in the world, and known as "the Second City of the Empire" after London. Shipbuilding on Clydeside (the river Clyde through Glasgow and other points) began when the first small yards were opened in 1712 at the Scott family's shipyard at Greenock. After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. "Clydebuilt" became an industry benchmark of quality, and the river's shipyards were given contracts for warships.
Public health and welfare.
The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis. The companies attracted rural workers, as well as immigrants from Catholic Ireland, by inexpensive company housing that was a dramatic move upward from the inner-city slums. This paternalistic policy led many owners to endorse government sponsored housing programs as well as self-help projects among the respectable working class.
Intellectual life.
While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the mathematicians and physicists James Clerk Maxwell, Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain.
In literature the most successful figure of the mid-nineteenth century was Walter Scott, who began as a poet and also collected and published Scottish ballads. His first prose work, Waverley in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations. Robert Louis Stevenson's work included the urban Gothic novella "Strange Case of Dr Jekyll and Mr Hyde" (1886), and played a major part in developing the historical adventure in books like "Kidnapped" and "Treasure Island". Arthur Conan Doyle's "Sherlock Holmes" stories helped found the tradition of detective fiction. The "kailyard tradition" at the end of the century, brought elements of fantasy and folklore back into fashion as can be seen in the work of figures like J. M. Barrie, most famous for his creation of Peter Pan, and George MacDonald, whose works, including "Phantasies", played a major part in the creation of the fantasy genre.
Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts Movement, and Japonisme, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Among the most prominent members were the loose collective of The Four: acclaimed architect Charles Rennie Mackintosh, his wife the painter and glass artist Margaret MacDonald, her sister the artist Frances, and her husband, the artist and teacher Herbert MacNair.
Decline and romanticism of the Highlands.
This period saw a process of rehabilitation for highland culture. Tartan had already been adopted for highland regiments in the British army, which poor highlanders joined in large numbers until the end of the Napoleonic Wars in 1815, but by the 19th century it had largely been abandoned by the ordinary people. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. The world paid attention to their literary redefinition of Scottishness, as they forged an image largely based on characteristics in polar opposition to those associated with England and modernity. This new identity made it possible for Scottish culture to become integrated into a wider European and North American context, not to mention tourist sites, but it also locked in a sense of "otherness" which Scotland began to shed only in the late 20th century. Scott's "staging" of the royal Visit of King George IV to Scotland in 1822 and the king's wearing of tartan, resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish linen industry. The designation of individual clan tartans was largely defined in this period and became a major symbol of Scottish identity. The fashion for all things Scottish was maintained by Queen Victoria, who helped secure the identity of Scotland as a tourist resort, with Balmoral Castle in Aberdeenshire becoming a major royal residence from 1852.
Despite these changes the highlands remained very poor and traditional, with few connections to the uplift of the Scottish Enlightenment and little role in the Industrial Revolution. A handful of powerful families, typified by the dukes of Argyll, Atholl, Buccleuch, and Sutherland, owned the best lands and controlled local political, legal and economic affairs. Particularly after the end of the boom created by the Revolutionary and Napoleonic Wars (1790–1815), these landlords needed cash to maintain their position in London society, and had less need of soldiers. They turned to money rents, displaced farmers to raise sheep, and downplayed the traditional patriarchal relationship that had historically sustained the clans. This was exacerbated after the repeal of the Corn Laws in mid-century, when Britain adopted a free trade policy, and grain imports from America undermined the profitability of crop production. The Irish potato famine of the 1840s was caused by a plant disease that reached the Highlands in 1846, where 150,000 people faced disaster because their food supply was largely potatoes (with a little herring, oatmeal and milk). They were rescued by an effective emergency relief system that stands in dramatic contrast to the failures of relief in Ireland.
Rural life.
The unequal concentration of land ownership remained an emotional subject and eventually became a cornerstone of liberal radicalism. The politically powerless poor crofters embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800, and the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. This energised the crofters and separated them from the landlords, preparing them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League. Violence began on the Isle of Skye when Highland landlords cleared their lands for sheep and deer parks. It was quieted when the government stepped in passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. In 1885 three Independent Crofter candidates were elected to Parliament, leading to explicit security for the Scottish smallholders; the legal right to bequeath tenancies to descendants; and creating a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained most of their votes.
Migration.
The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry there were insufficient good jobs, as a result, during the period 1841-1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England. Scotland lost a much higher proportion of its population than England and Wales, reaching perhaps as much as 30.2 per cent of its natural increase from the 1850s onwards. This not only limited Scotland's population increase, but meant that almost every family lost members due to emigration and, because more of the migrants were young males, it skewed the sex and age ratios of the country.
Scots-born migrants that played a leading role in the foundation and development of the United States included cleric and revolutionary John Witherspoon, sailor John Paul Jones, industrialist and philanthropist Andrew Carnegie and scientist and inventor Alexander Graham Bell. In Canada they included soldier and governor of Quebec James Murray, Prime Minister John A. Macdonald and politician and social reformer Tommy Douglas. For Australia they included soldier and governor Lachlan Macquarie, governor and scientist Thomas Brisbane and Prime Minister Andrew Fisher. For New Zealand they included politician Peter Fraser and outlaw James Mckenzie. By the 21st century, there would be about as many people who were Scottish Canadians and Scottish Americans as the 5 million remaining in Scotland.
Religious schism and revival.
After prolonged years of struggle, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. The evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly in the Highlands and Islands, appealing much more strongly than did the established church. Chalmers's ideas shaped the breakaway group. He stressed a social vision that revived and preserved Scotland's communal traditions at a time of strain on the social fabric of the country. Chalmers's idealized small equalitarian, kirk-based, self-contained communities that recognized the individuality of their members and the need for cooperation. That vision also affected the mainstream Presbyterian churches, and by the 1870s it had been assimilated by the established Church of Scotland. Chalmers's ideals demonstrated that the church was concerned with the problems of urban society, and they represented a real attempt to overcome the social fragmentation that took place in industrial towns and cities.
In the late 19th century the major debates were between fundamentalist Calvinists and theological liberals, who rejected a literal interpretation of the Bible. This resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. There were, however, also moves towards reunion, beginning with the unification of some secessionist churches into the United Secession Church in 1820, which united with the Relief Church in 1847 to form the United Presbyterian Church, which in turn joined with the Free Church in 1900 to form the United Free Church of Scotland. The removal of legislation on lay patronage would allow the majority of the Free Church to rejoin Church of Scotland in 1929. The schisms left small denominations including the Free Presbyterians and a remnant that had not merged in 1900 as the Free Church.
Catholic Emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, principally to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland. Episcopalianism also revived in the 19th century as the issue of succession receded, becoming established as the Episcopal Church in Scotland in 1804, as an autonomous organisation in communion with the Church of England. Baptist, Congregationalist and Methodist churches had appeared in Scotland in the 18th century, but did not begin significant growth until the 19th century, partly because more radical and evangelical traditions already existed within the Church of Scotland and the free churches. From 1879 they were joined by the evangelical revivalism of the Salvation Army, which attempted to make major inroads in the growing urban centres.
Development of state education.
Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants, then from 1846 it was funding schools by direct sponsorship, and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. Overall administration was in the hands of the Scotch (later Scottish) Education Department in London. Education was now compulsory from five to thirteen and many new board schools were built. Larger urban school boards established "higher grade" (secondary) schools as a cheaper alternative to the burgh schools. The Scottish Education Department introduced a Leaving Certificate Examination in 1888 to set national standards for secondary education and in 1890 school fees were abolished, creating a state-funded national system of free basic education and common examinations.
At the beginning of the 19th century, Scottish universities had no entrance exam, students typically entered at ages of 15 or 16, attended for as little as two years, chose which lectures to attend and could leave without qualifications. After two commissions of enquiry in 1826 and 1876 and reforming acts of parliament in 1858 and 1889, the curriculum and system of graduation were reformed to meet the needs of the emerging middle classes and the professions. Entrance examinations equivalent to the School Leaving Certificate were introduced and average ages of entry rose to 17 or 18. Standard patterns of graduation in the arts curriculum offered 3-year ordinary and 4-year honours degrees and separate science faculties were able to move away from the compulsory Latin, Greek and philosophy of the old MA curriculum. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as well as the upper class. It prepared students for non-commercial careers in government, the law, medicine, education, and the ministry and a smaller group for careers in science and engineering. St Andrews pioneered the admission of women to Scottish universities, creating the Lady Licentiate in Arts (LLA), which proved highly popular. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.
Early 20th century.
Fishing.
The years before the First World War were the golden age of the inshore fisheries. Landings reached new heights, and Scottish catches dominated Europe's herring trade, accounting for a third of the British catch. High productivity came about thanks to the transition to more productive steam-powered boats, while the rest of Europe's fishing fleets were slower because they were still powered by sails.
Political realignment.
In the Khaki Election of 1900, nationalist concern with the Boer War meant that the Conservatives and their Liberal Unionist allies gained a majority of Scottish seats for the first time, although the Liberals regained their ascendancy in the next election. The Unionists and Conservatives merged in 1912, usually known as the Conservatives in England and Wales, they adopted the name Unionist Party in Scotland. Scots played a major part in the leadership of UK political parties producing a Conservative Prime Minister in Arthur Balfour (1902–05) and a Liberal one in Henry Campbell-Bannerman (1905–08). Various organisations, including the Independent Labour Party, joined to make the British Labour Party in 1906, with Keir Hardie as its first chairman.
First World War 1914-18.
Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, food (particularly fish) and money, engaging with the conflict with some enthusiasm. With a population of 4.8 million in 1911, Scotland sent 690,000 men to the war, of whom 74,000 died in combat or from disease, and 150,000 were seriously wounded. Scottish urban centres, with their poverty and unemployment were favourite recruiting grounds of the regular British army, and Dundee, where the female dominated jute industry limited male employment had one of the highest proportion of reservists and serving soldiers than almost any other British city. Concern for their families' standard of living made men hesitate to enlist; voluntary enlistment rates went up after the government guaranteed a weekly stipend for life to the survivors of men who were killed or disabled. After the introduction of conscription from January 1916 every part of the country was affected. Occasionally Scottish troops made up large proportions of the active combatants, and suffered corresponding loses, as at the Battle of Loos, where there were three full Scots divisions and other Scottish units. Thus, although Scots were only 10 per cent of the British population, they made up 15 per cent of the national armed forces and eventually accounted for 20 per cent of the dead. Some areas, like the thinly populated Island of Lewis and Harris suffered some of the highest proportional losses of any part of Britain. Clydeside shipyards and the nearby engineering shops were the major centres of war industry in Scotland. In Glasgow, radical agitation led to industrial and political unrest that continued after the war ended. After the end of the war in June 1919 the German fleet interned in Scapa Flow was scuttled by its German crews, to avoid its ships being taken over by the victorious allies.
Economic boom and stagnation.
A boom was created by the First World War, with the shipbuilding industry expanding by a third, but a serious depression hit the economy by 1922. The most skilled craftsmen were especially hard hit, because there were few alternative uses for their specialised skills. The main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.
A few industries did grow, such as chemicals and whisky, which developed a global market for premium "Scotch". However, in general the Scottish economy stagnated leading to growing unemployment and political agitation among industrial workers.
Interwar politics.
After World War I the Liberal Party began to disintegrate and Labour emerged as the party of progressive politics in Scotland, gaining a solid following among working classes of the urban lowlands. As a result the Unionists were able to gain most of the votes of the middle classes, who now feared Bolshevik revolution, setting the social and geographical electoral pattern in Scotland that would last until the late 20th century. The fear of the left had been fuelled by the emergence of a radical movement led by militant trades unionists. John MacLean emerged as a key political figure in what became known as Red Clydeside, and in January 1919, the British Government, fearful of a revolutionary uprising, deployed tanks and soldiers in central Glasgow. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base in the Irish Catholic working class districts. Women were especially active in building neighbourhood solidarity on housing and rent issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament; in the face of heavy unemployment the workers' mood changed to passive despair by the late 1920s. Scottish educated Andrew Bonar Law led a Conservative government from 1922 to 1923 and another Scot, Ramsay MacDonald, would be the Labour Party's first Prime Minister in 1924 and again from 1929 to 1935.
With all the main parties committed to the Union, new nationalist and independent political groupings began to emerge, including the National Party of Scotland in 1928 and Scottish Party in 1930. They joined to form the Scottish National Party (SNP) in 1934, with the goal of creating an independent Scotland, but it enjoyed little electoral success in the Westminster system.
Second World War 1939-45.
As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian. The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life. As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic. Shetland's relative proximity to occupied Norway resulted in the Shetland Bus by which fishing boats helped Norwegians flee the Nazis, and expeditions across the North Sea to assist resistance. Significant individual contributions to the war effort by Scots included the invention of radar by Robert Watson-Watt, which was invaluable in the Battle of Britain, as was the leadership at RAF Fighter Command of Air Chief Marshal Hugh Dowding.
In World War II, Prime Minister Winston Churchill appointed Labour politician Tom Johnston as Secretary of State for Scotland in February 1941; he controlled Scottish affairs until the war ended. He launched numerous initiatives to promote Scotland, attracting businesses and new jobs through his new Scottish Council of Industry. He set up 32 committees to deal with social and economic problems, ranging from juvenile delinquency to sheep farming. He regulated rents, and set up a prototype national health service, using new hospitals set up in the expectation of large numbers of casualties from German bombing. His most successful venture was setting up a system of hydro electricity using water power in the Highlands. A long-standing supporter of the Home Rule movement, Johnston persuaded Churchill of the need to counter the nationalist threat north of the border and created a Scottish Council of State and a Council of Industry as institutions to devolve some power away from Whitehall.
In World War II, despite extensive bombing by the Luftwaffe, Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships. Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25 per cent, and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition; the average height of 13-year-olds in Glasgow increased by 2 in.
End of mass migration.
While emigration began to tail off in England and Wales after the First World War, it continued apace in Scotland, with 400,000 Scots, ten per cent of the population, estimated to have left the country between 1921 and 1931. The economic stagnation was only one factor; other push factors included a zest for travel and adventure, and the pull factors of better job opportunities abroad, personal networks to link into, and the basic cultural similarity of the United States, Canada, and Australia. Government subsidies for travel and relocation facilitated the decision to emigrate. Personal networks of family and friends who had gone ahead and wrote back, or sent money, prompted emigrants to retrace their paths. When the Great Depression hit in the 1930s there were no easily available jobs in the US and Canada and emigration fell to less than 50,000 a year, bringing to an end the period of mass migrations that had opened in the mid-18th century.
Literary renaissance.
In the early 20th century there was a new surge of activity in Scottish literature, influenced by modernism and resurgent nationalism, known as the Scottish Renaissance. The leading figure in the movement was Hugh MacDiarmid (the pseudonym of Christopher Murray Grieve). MacDiarmid attempted to revive the Scots language as a medium for serious literature in poetic works including "A Drunk Man Looks at the Thistle" (1936), developing a form of Synthetic Scots that combined different regional dialects and archaic terms. Other writers that emerged in this period, and are often treated as part of the movement, include the poets Edwin Muir and William Soutar, the novelists Neil Gunn, George Blake, Nan Shepherd, A. J. Cronin, Naomi Mitchison, Eric Linklater and Lewis Grassic Gibbon, and the playwright James Bridie. All were born within a fifteen-year period (1887 and 1901) and, although they cannot be described as members of a single school, they all pursued an exploration of identity, rejecting nostalgia and parochialism and engaging with social and political issues.
Educational reorganisation and retrenchment.
In the 20th century the centre of the education system became more focused on Scotland, with the ministry of education partly moving north in 1918 and then finally having its headquarters relocated to Edinburgh in 1939. The school leaving age was raised to 14 in 1901, but despite attempts to raise it to 15 this was only made law in 1939 and then postponed because of the outbreak of war. In 1918 Roman Catholic schools were brought into the state system, but retained their distinct religious character, access to schools by priests and the requirement that school staff be acceptable to the Church.
The first half of the 20th century saw Scottish universities fall behind those in England and Europe in terms of participation and investment. The decline of traditional industries between the wars undermined recruitment. English universities increased the numbers of students registered between 1924 and 1927 by 19 per cent, but in Scotland the numbers fell, particularly among women. In the same period, while expenditure in English universities rose by 90 per cent, in Scotland the increase was less than a third of that figure.
Naval role.
Scotland's Scapa Flow was the main base for the Royal Navy in the 20th century. As the Cold War intensified in 1961, the United States deployed Polaris ballistic missiles, and submarines, in the Firth of Clyde's Holy Loch. Public protests from CND campaigners proved futile. The Royal Navy successfully convinced the government to allow the base because it wanted its own "Polaris"-class submarines, and it obtained them in 1963. The RN's nuclear submarine base opened four "Resolution" class Polaris submarines at the expanded Faslane Naval Base on the Gare Loch. The first patrol of a Trident-armed submarine occurred in 1994, although the US base was closed at the end of the Cold War.
Postwar.
After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. This period saw the emergence of the Scottish National Party and movements for both Scottish independence and more popularly devolution. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal.)
Politics and devolution.
In the second half of the 20th century the Labour Party usually won most Scottish seats in the Westminster parliament, losing this dominance briefly to the Unionists in the 1950s. Support in Scotland was critical to Labour's overall electoral fortunes as without Scottish MPs it would have gained only two UK electoral victories in the 20th century (1945 and 1966). The number of Scottish seats represented by Unionists (known as Conservatives from 1965 onwards) went into steady decline from 1959 onwards, until it fell to zero in 1997. Politicians with Scottish connections continued to play a prominent part in UK political life, with Prime Ministers including the Conservatives Harold Macmillan (whose father was Scottish) from 1955 to 1957 and Alec Douglas-Home from 1963 to 1964.
The Scottish National Party gained its first seat at Westminster in 1945 and became a party of national prominence during the 1970s, achieving 11 MPs in 1974. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the necessary support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal) and the SNP went into electoral decline during the 1980s. The introduction in 1989 by the Thatcher-led Conservative government of the Community Charge (widely known as the Poll Tax), one year before the rest of the United Kingdom, contributed to a growing movement for a return to direct Scottish control over domestic affairs. The electoral success of New Labour in 1997 was led by two Prime Ministers with Scottish connections: Tony Blair (who was brought up in Scotland) from 1997 to 2007 and Gordon Brown from 2007 to 2010, opened the way for constitutional change. On 11 September 1997, the 700th anniversary of Battle of Stirling Bridge, the Blair led Labour government again held a referendum on the issue of devolution. A positive outcome led to the establishment of a devolved Scottish Parliament in 1999. A coalition government, which would last until 2007, was formed between Labour and the Liberal Democrats, with Donald Dewar as First Minister. The new Scottish Parliament Building, adjacent to Holyrood House in Edinburgh, opened in 2004. Although not reaching its 1970s peak in Westminster elections, the SNP had more success in the Scottish Parliamentary elections with their system of mixed member proportional representation. It became the official opposition in 1999, a minority government in 2007 and a majority government from 2011. In 2014 the independence referendum saw voters reject independence, choosing instead to remain in the United Kingdom.
Economic reorientation.
After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. The discovery of the giant Forties oilfield in October 1970 signalled that Scotland was about to become a major oil producing nation, a view confirmed when Shell Expro discovered the giant Brent oilfield in the northern North Sea east of Shetland in 1971. Oil production started from the Argyll field (now Ardmore) in June 1975, followed by Forties in November of that year. Deindustrialisation took place rapidly in the 1970s and 1980s, as most of the traditional industries drastically shrank or were completely closed down. A new service-oriented economy emerged to replace traditional heavy industries. This included a resurgent financial services industry and the electronics manufacturing of Silicon Glen.
Religious diversity and decline.
In the 20th century existing Christian denominations were joined by other organisations, including the Brethren and Pentecostal churches. Although some denominations thrived, after World War II there was a steady overall decline in church attendance and resulting church closures for most denominations. Talks began in the 1950s aiming at a grand merger of the main Presbyterian, Episcopal and Methodist bodies in Scotland. The talks were ended in 2003, when the General Assembly of the Church of Scotland rejected the proposals. In the 2011 census, 53.8% of the Scottish population identified as Christian (declining from 65.1% in 2001). The Church of Scotland is the largest religious grouping in Scotland, with 32.4% of the population. The Roman Catholic Church accounted for 15.9% of the population and is especially important in West Central Scotland and the Highlands. In recent years other religions have established a presence in Scotland, mainly through immigration and higher birth rates among ethnic minorities, with a small number of converts. Those with the most adherents in the 2011 census are Islam (1.4%, mainly among immigrants from South Asia), Hinduism (0.3%), Buddhism (0.2%) and Sikhism (0.2%). Other minority faiths include the Bahá'í Faith and small Neopagan groups. There are also various organisations which actively promote humanism and secularism, included within the 43.6% who either indicated no religion or did not state a religion in the 2011 census.
Educational reforms.
Although plans to raise the school leaving age to 15 in the 1940s were never ratified, increasing numbers stayed on beyond elementary education and it was eventually raised to 16 in 1973. As a result secondary education was the major area of growth in the second half of the 20th century. New qualifications were developed to cope with changing aspirations and economics, with the Leaving Certificate being replaced by the Scottish Certificate of Education Ordinary Grade ('O-Grade') and Higher Grade ('Higher') qualifications in 1962, which became the basic entry qualification for university study. The higher education sector expanded in the second half of the 20th century, with four institutions being given university status in the 1960s (Dundee, Heriot-Watt, Stirling and Strathclyde) and five in the 1990s (Abertay, Glasgow Caledonian, Napier, Paisley and Robert Gordon). After devolution, in 1999 the new Scottish Executive set up an Education Department and an Enterprise, Transport and Lifelong Learning Department. One of the major diversions from practice in England, possible because of devolution, was the abolition of student tuition fees in 1999, instead retaining a system of means-tested student grants.
New literature.
Some writers that emerged after the Second World War followed Hugh MacDiarmid by writing in Scots, including Robert Garioch and Sydney Goodsir Smith. Others demonstrated a greater interest in English language poetry, among them Norman MacCaig, George Bruce and Maurice Lindsay. George Mackay Brown from Orkney, and Iain Crichton Smith from Lewis, wrote both poetry and prose fiction shaped by their distinctive island backgrounds. The Glaswegian poet Edwin Morgan became known for translations of works from a wide range of European languages. He was also the first Scots Makar (the official national poet), appointed by the inaugural Scottish government in 2004. Many major Scottish post-war novelists, such as Muriel Spark, with "The Prime of Miss Jean Brodie" (1961) spent much or most of their lives outside Scotland, but often dealt with Scottish themes. Successful mass-market works included the action novels of Alistair MacLean, and the historical fiction of Dorothy Dunnett. A younger generation of novelists that emerged in the 1960s and 1970s included Shena Mackay, Alan Spence, Allan Massie and the work of William McIlvanney. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of Glasgow writers focused around critic, poet and teacher Philip Hobsbaum and editor Peter Kravitz. In the 1990s major, prize winning, Scottish novels, often overtly political, that emerged from this movement included Irvine Welsh's "Trainspotting" (1993), Warner's "Morvern Callar" (1995), Gray’s "Poor Things" (1992) and Kelman’s "How Late It Was, How Late" (1994). Scottish crime fiction has been a major area of growth, particularly the success of Edinburgh's Ian Rankin and his Inspector Rebus novels. This period also saw the emergence of a new generation of Scottish poets that became leading figures on the UK stage, including Carol Ann Duffy, who was named as Poet Laureate in May 2009, the first woman, the first Scot and the first openly gay poet to take the post.

</doc>
<doc id="13621" url="http://en.wikipedia.org/wiki?curid=13621" title="Hadrian">
Hadrian

Hadrian (; Latin: "Publius Aelius Hadrianus Augustus"; 24 January, 76 AD – 10 July, 138 AD) was Roman emperor from 117 to 138. He rebuilt the Pantheon and constructed the Temple of Venus and Roma. He is also known for building Hadrian's Wall, which marked the northern limit of Britannia. Hadrian was regarded by some as a humanist and was philhellene in most of his tastes. He is regarded as one of the Five Good Emperors.
Hadrian was born Publius Aelius Hadrianus into a Romanized family from Hispania of partial known Italian ancestry. Although Italica near Santiponce (in modern-day Spain) is often considered his birthplace, his place of birth remains uncertain, though it is known that his family has centuries-old roots in Hispania. His predecessor Trajan was a maternal cousin of Hadrian's father. Trajan never officially designated an heir, but according to his wife Pompeia Plotina, Trajan named Hadrian emperor immediately before his death. Trajan's wife and his friend Licinius Sura were well-disposed towards Hadrian, and he may well have owed his succession to them.
During his reign, Hadrian traveled to nearly every province of the Empire. An ardent admirer of Greece, he sought to make Athens the cultural capital of the Empire and ordered the construction of many opulent temples in the city. He used his relationship with his Greek lover Antinous to underline his philhellenism and led to the creation of one of the most popular cults of ancient times. He spent extensive amounts of his time with the military; he usually wore military attire and even dined and slept amongst the soldiers. He ordered military training and drilling to be more rigorous and even made use of false reports of attack to keep the army alert.
Upon his accession to the throne, Hadrian withdrew from Trajan's conquests in Mesopotamia and Armenia, and even considered abandoning Dacia. Late in his reign he suppressed the Bar Kokhba revolt in Judaea, renaming the province Syria Palaestina. In 136 an ailing Hadrian adopted Lucius Aelius as his heir, but the latter died suddenly two years later. In 138, Hadrian resolved to adopt Antoninus Pius if he would in turn adopt Marcus Aurelius and Aelius' son Lucius Verus as his own eventual successors. Antoninus agreed, and soon afterward Hadrian died at Baiae.
Sources.
As is the case with his predecessor Trajan, we lack a continuous account of the political history of Hadrian's reign: what we have in the way of such an account is, as on Trajan's reign, Book 69 of the "Roman History" by Cassius Dio, in the form of a much later, Byzantine era abridgment by the Eleventh Century monk Xiphilinius, who made a selection on Dio's account of Hadrian's reign based on his mostly religious interests, covering the Bar Kokhba war relatively fully, to the exclusion of much else. Hadrian is the first imperial biography of the Historia Augusta, but the notorious unreliability of that work ("a mish mash of actual fact, Cloak and dagger, Sword and Sandal, with a sprinkling of Ubu Roi") does not allow for using it as a source without care.Hadrian's biography is considered, however, as one of the most factual of the collection, as all of the earlier lives of the emperors up to Caracalla. Contemporary Greek authors such as Philostratus and Pausanias, who wrote shortly after Hadrian's reign, offer information on Hadrian's relations with the provincial Greek world, and Fronto, in his Latin correspondence, sheds some light on the general character of the reign's internal policies. As in the case of Trajan, using epigraphical, numismatic, archaeological and other non-literary sources is absolutely necessary in tracing a detailed historical account.
Early life.
Hadrian was born Publius Aelius Hadrianus in Italica, or in Rome, from a well-established, ethnically Hispanic family with partial, distant links to another family from Picenum in Italy that had subsequently settled in Italica, Hispania Baetica (the republican Hispania Ulterior), near the present-day location of Seville, Spain.
Although it was an accepted part of Hadrian's personal history that he was born in Spain, his biography in Augustan History states that he was born in Rome on 24 January 76, of an ethnically Hispanic family with partial Italian origins. However, this may be a ruse to make Hadrian look like a person from Rome instead of a person hailing from the provinces, since his parents, grandparents, and great-grandparents were all born and raised in Hispania. His father was Publius Aelius Hadrianus Afer, who as a senator of praetorian rank would spend much of his time in Rome, away from his homeland of Hispania.
A small part of Hadrian's known paternal ancestry can be linked to a family from Hadria, modern Atri, an ancient town of Picenum in Italy, this family had settled in Italica in Hispania Baetica soon after its founding by Scipio Africanus. Afer, also born and raised in Hispania was a paternal cousin of the future Emperor Trajan. His mother was Domitia Paulina who came from Gades (Cádiz). Paulina was a daughter of a distinguished Spanish-Roman Senatorial family.
Hadrian’s elder sister and only sibling was Aelia Domitia Paulina, married with the triple consul Lucius Julius Ursus Servianus, his niece was Julia Serviana Paulina and his great-nephew was Gnaeus Pedanius Fuscus Salinator, from Barcino (Barcelona). His parents died in 86 when Hadrian was ten, and the boy then became a ward of both Trajan and Publius Acilius Attianus (who was later Trajan’s Praetorian Prefect). Hadrian was schooled in various subjects particular to young aristocrats of the day, and was so fond of learning Greek literature that he was nicknamed "Graeculus" ("Greekling").
Hadrian visited Italica when (or never left it until) he was 14 years old, when he was recalled by Trajan, who thereafter looked after his development. He never returned to Italica although it was later made a colonia in his honour.
Public service.
His first military service was as a tribune of the Legio II "Adiutrix". Later, he was to be transferred to the Legio I "Minervia" in Germany. When Nerva died in 98, Hadrian rushed to inform Trajan personally. He later became legate of a legion in Upper Pannonia and eventually governor of said province. The fact that he had three spells of military service - instead of just one or two, as was customary to the regular senator - points to a through military career. At the same time, he failed to achieve the honor of a regular consulate before his reign, being only suffect consul for 108. He was also archon in Athens for a brief time, and was elected an Athenian citizen.
His career before becoming emperor follows:
Hadrian was involved in the wars against the Dacians (as legate of the V "Macedonica") and reputedly won awards from Trajan for his successes. Hadrian's military skill is not well-attested due to a lack of military action during his reign; however, his keen interest in and knowledge of the army and his demonstrated skill of leadership show possible strategic talent.
Hadrian joined Trajan's expedition against Parthia as a legate on Trajan’s staff. Neither during the first victorious phase, nor during the second phase of the war when rebellion swept Mesopotamia did Hadrian do anything of note. However, when the governor of Syria had to be sent to sort out renewed troubles in Dacia, Hadrian was appointed as a replacement, giving him an independent command.
Trajan, seriously ill by that time, decided to return to Rome while Hadrian remained in Syria to guard the Roman rear. Trajan only got as far as Selinus before he became too ill to go further. While Hadrian may have been the obvious choice as successor, he had never been adopted as Trajan's heir. As Trajan lay dying, nursed by his wife, Plotina (a supporter of Hadrian), he at last adopted Hadrian as heir. Since the document was signed by Plotina, it has been suggested that Trajan may have already been dead.
Emperor (117).
Securing power.
Hadrian quickly secured the support of the legions — one potential opponent, Lusius Quietus, was promptly dismissed. The Senate's endorsement followed when possibly falsified papers of adoption from Trajan were presented (although he had been the ward of Trajan). The rumour of a falsified document of adoption carried little weight — Hadrian's legitimacy arose from the endorsement of the Senate and the Syrian armies.
Hadrian did not at first go to Rome — he was busy sorting out the East and suppressing the Jewish revolt that had broken out under Trajan, then moving on to sort out the Danube frontier. Instead, Attianus, Hadrian's former guardian, was put in charge in Rome. There he "discovered" a conspiracy involving four leading Senators including Lusius Quietus and demanded of the Senate their deaths.
There was no question of a public trial—they were hunted down and killed out of hand. Because Hadrian was not in Rome at the time, he was able to claim that Attianus had acted on his own initiative. According to Elizabeth Speller, the real reason for their deaths was that they were Trajan's men. Or better, the reason for their elimination is simply that all four were prominent senators of consular rank and, as such, prospective candidates for the imperial office ("capaces imperii"). Also, the four consulars were the chiefs of war hawk group of senators that was committed to Trajan's expansionist policies, which Hadrian intended to change.
Hadrian's instrument for getting rid of the four consulars, the Praetorian Prefect Attianus, was made a senator and promoted to consular rank, being afterwards discarded by Hadrian, who suspected his personal ambition. It's probable that Attianus was executed (or was already dead ) by the end of Hadrian's reign The four consulars episode, however, was to strain Hadrian's relations with the Senate for his entire reign. This tense relationship - and Hadrian's authoritarian stance towards the Senate - was acknowledged one generation later by Fronto, himself a senator, who wrote in one of his letters to Marcus Aurelius that "“I praised the deified Hadrian, your grandfather, in the senate on a number of occasions with great enthusiasm, and I did this willingly, too [...] But, if it can be said – respectfully acknowledging your devotion towards your grandfather – I wanted to appease and assuage Hadrian as I would Mars Gradivus or Dis Pater, rather than to love him". The strain in the relationship between Hadrian and the Senate, however, never took the form of an overt confrontation, as had happened during the reigns of other previous "bad" emperors: Hadrian knew how to remain aloof in order to avoid an open clash. The Senate's political role, however, was effaced behind Hadrian's personal rule (in Ronald Syme's view, Hadrian "was a Führer, a Duce, a Caudillo"). The fact that Hadrian was to spend half of his reign away from Rome in constant travel undoubtedly helped the management of this strained relationship. Hadrian, however, underscored the autocratic character of his reign by counting the day of his acclamation by the armies as his "dies imperiii" and by enforcing new laws through imperial decree (as constitution)instead of having them approved by the Senate as a formality.
Hadrian and the military.
Despite his own great reputation as a military administrator, Hadrian's reign was marked by a general lack of documented major military conflicts, apart from the Second Roman-Jewish War. However, disturbances on the Danubian frontier early in the reign led to the killing of the governor of Dacia, Caius Julius Quadratus Bassus, to which Hadrian responded by placing the then equestrian governor of Mauretania Caesariensis Q. Marcius Turbo, who had a long record of distinguished military service, as joint governor of Dacia and Pannonia Inferior with the powers of a Prefect. Shortly after, it was decided by Hadrian that all the part of Dacia that had been added to the province of Moesia Inferior - that is, present-day Southern Moldavia and the Wallachian Plain - was to be surrendered to the Roxolani Sarmatians, whose king Rasparaganus received Roman citizenship, client king status - and possibly an increased subsidy.The Roman partial withdrawal was probably supervised by the governor of Moesia Quintus Pompeius Falco Presence of Hadrian on the Dacian front at this juncture, implied by the unreliable Historia Augusta, is merely conjectural and speculative. Hadrian did not visit Dacia in the course of his subsequent travels, but nevertheless included it into his subsequent monetary series of coins with allegories of the provinces. The notion that he contemplated the idea of withdrawing from Dacia altogether, as stated by Eutropius, appears, therefore, as unfounded.
Hadrian had already surrendered Trajan's conquests in Mesopotamia, considering them to be indefensible. In the East, Hadrian contented himself with retaining suzerainty over Osroene, which was ruled by the client king Parthamaspates, once client king of Parthia under Trajan. There was almost a new war with Parthia around 121, but the threat was averted when Hadrian succeeded in negotiating a peace.Late in the reign (135), an invasion of the Alani in Capadocia, covertly supported by the king of Caucasian Iberia Pharasmanes, was successful repulsed by Hadrian's governor, the Greek historian Arrian, who subsequently installed a Roman "adviser" in Iberia.
This abandonment of an aggressive policy was something for which the Senate and its historians never forgave Hadrian: the Fourth Century historian Aurelius Victor charged him with being jealous of Trajan's exploits and deliberately trying to downplay their worthiness: "Traiani gloriae invidens". It's more probable that Hadrian simply considered that the financial strain to be incurred in keeping on a policy of conquests was something the Roman Empire could not afford: proof to it is the disappearance during his reigns of two entire legions: Legio XXII Deiotariana and the famous "lost legion" IX Hispania, possibly destroyed during a late Trajanic uprising by the Brigantes in Britain. Also, the acknowledgement of the indefensible character of the Mesopotamian conquests had perhaps already been made by Trajan himself, who had disengaged from them at the time of his death.
The peace policy was strengthened by the erection of permanent fortifications along the empire's borders ("limites", sl. "limes"). The most famous of these is the massive Hadrian's Wall in Great Britain, built on stone and doubled on its rear by a ditch ("Vallum Hadriani") which marked the boundary between a strictly military zone and the province. On the Danube and Rhine borders were strengthened with a series of mostly wooden fortifications, forts, outposts and watchtowers, the latter specifically improving communications and local area security. These defensive activities, however, generated very few "literary" records: the information that it was Hadrian who built the Wall in Britain can only be found, in the entire corpus of Ancient authors, in his "Historia Augusta" biography. But then, Hadrian's military activities were, in a certain measure, ideological, in that they emphasized a community of interests between all peoples living within the Roman Empire, instead an hegemony of conquest centered on the city of Rome and its Senate.
To maintain morale and prevent the troops from becoming restive, Hadrian established intensive drill routines, and personally inspected the armies. Although his coins showed military images almost as often as peaceful ones, Hadrian's policy was peace through strength, even threat, with an emphasis on "disciplina" (discipline) which was the subject of two monetary series. This emphasis on spit and polish was heartily praised by Cassius Dio, who saw it as a useful deterrent and therefore the cause of the general peaceful character of Hadrian's reign. Fronto, however, expressed other views on the subject: Hadrian, to him, liked to play war games and "giving eloquent speeches to the armies" - like the series of addresses, inscribed in a column, that he made while on an inspecting tour during 128 at the new headquarters of Legio III Augusta in Lambaesis - rather than actual warfare. In general, Fronto was very critical of Hadrian's pacifist policy, charging it with the decline in military standards of the Roman army of his own time. It was, however, Hadrian who at least systematized the employment of the "numeri" - ethnic non-citizen troops with special weapons, such as Eastern mounted archers - in low-intensity defensive tasks such as dealing with infiltrators and skirmishers.
Antinous.
Hadrian had a close relationship with a Bithynian Greek youth, Antinous, which was most likely sexual.
During a journey on the Nile he lost Antinous, his favourite, and for this youth he wept like a woman. Concerning this incident there are varying rumours; for some claim that he had devoted himself to death for Hadrian, and others — what both his beauty and Hadrian's sensuality suggest. But however this may be, the Greeks deified him at Hadrian's request, and declared that oracles were given through his agency, but these, it is commonly asserted, were composed by Hadrian himself.
Antinous drowned in 130. Deeply saddened, Hadrian founded the Egyptian city of Antinopolis in his memory, and had Antinous deified – an unprecedented honour for one not of the ruling family. The cult of Antinous became very popular in the Greek-speaking world. It has been suggested that Hadrian created the cult as a political move to reconcile the Greek-speaking East to Roman rule.
Cultural pursuits and patronage.
Hadrian has been described, firstly in an Ancient anonymous source later echoed by Ronald Syme, among others, as the most versatile of all the Roman Emperors ("varius multiplex multiformis"). He also liked to demonstrate knowledge of all intellectual and artistic fields. Above all, Hadrian patronized the arts: Hadrian's Villa at Tibur (Tivoli) was the greatest Roman example of an Alexandrian garden, recreating a sacred landscape, albeit lost in large part to the despoliation of the ruins by the Cardinal d'Este who had much of the marble removed to build Villa d'Este. In Rome, the Pantheon, originally built by Agrippa but destroyed by fire in 80, was rebuilt under Hadrian (working on a blueprint left by Trajan: see below) in the domed form it retains to this day. It is among the best preserved of Rome's ancient buildings and was highly influential to many of the great architects of the Italian Renaissance and Baroque periods.
From well before his reign, Hadrian displayed a keen interest in architecture, but it seems that his eagerness was not always well received. For example, Apollodorus of Damascus, famed architect of the Forum of Trajan, dismissed his designs. When Trajan, predecessor to Hadrian, consulted Apollodorus about an architectural problem, Hadrian interrupted to give advice, to which Apollodorus replied, "Go away and draw your pumpkins. You know nothing about these problems." "Pumpkins" refers to Hadrian's drawings of domes like the Serapeum in his villa. The historian Cassius Dio wrote that, once Hadrian succeeded Trajan to become emperor, he had Apollodorus exiled and later put to death.The story, however, is problematical - archaeological evidence (brickstamps with consular dates) has demonstrated, e.g., that the Pantheon's dome was already under construction late in Trajan's reign (115) and probably under Appollodorus's sponsorship.
Hadrian wrote poetry in both Latin and Greek; one of the few surviving examples is a Latin poem he reportedly composed on his deathbed (see below). He also wrote an autobiography – not, apparently, a work of great length or revelation, but designed to scotch various rumours or explain his various actions. Hadrian was a passionate hunter from the time of his youth according to one source. In northwest Asia, he founded and dedicated a city to commemorate a she-bear he killed. It is documented that in Egypt he and his beloved Antinous killed a lion. In Rome, eight reliefs featuring Hadrian in different stages of hunting decorate a building that began as a monument celebrating a kill.
Another of Hadrian's contributions to "popular" culture was the beard, which symbolised his philhellenism. Since the time of Scipio Africanus it had been fashionable among the Romans to be clean-shaven. Also all Roman Emperors before Hadrian, except for Nero (also a great admirer of Greek culture), were clean shaven. Most of the emperors after Hadrian would be portrayed with beards. Their beards, however, were not worn out of an appreciation for Greek culture but because the beard had, thanks to Hadrian, become fashionable. This new fashion lasted until the reign of Constantine the Great and was revived again by
Phocas at the start of the 7th century.
As a cultural Hellenophile Hadrian was familiar with the work of the philosophers Epictetus, Heliodorus and Favorinus. At home he attended to social needs. Hadrian mitigated but did not abolish slavery, had the legal code humanized and forbade torture. He built libraries, aqueducts, baths and theaters. Hadrian is considered by many historians to have been wise and just: Schiller called him "the Empire's first servant", and British historian Edward Gibbon admired his "vast and active genius", as well as his "equity and moderation". In 1776, he stated that Hadrian's era was part of the "happiest era of human history".
While visiting Greece in 131–132, Hadrian attempted to create a kind of provincial parliament to bind all the semi-autonomous former city states across all Greece and Ionia (in Asia Minor). This parliament, known as the Panhellenion, failed despite spirited efforts to foster cooperation among the Hellenes. Hadrian died at his villa in Baiae. He was buried in a mausoleum on the western bank of the Tiber, in Rome, a building later transformed into a papal fortress, Castel Sant'Angelo. The dimensions of his mausoleum, in its original form, were deliberately designed to be slightly larger than the earlier Mausoleum of Augustus.
According to Cassius Dio, a gigantic equestrian statue was erected to Hadrian after his death. "It was so large that the bulkiest man could walk through the eye of each horse, yet because of the extreme height of the foundation persons passing along on the ground below believe that the horses themselves as well as Hadrian are very very small." This may refer to the huge statuary group placed atop the mausoleum, which disappeared at some later time, depicting Hadrian driving a four-horse quadriga chariot.
Hadrian's travels.
Purpose.
The Emperor travelled broadly, inspecting and correcting the legions in the field. Even prior to becoming emperor, he had travelled abroad with the Roman military, giving him much experience in the matter. More than half his reign was spent outside of Italy. Other emperors often left Rome simply to go to war, returning soon after conflicts concluded. A previous emperor, Nero, once travelled through Greece and was condemned for his self-indulgence. According to modern historians as Paul Veyne, what Hadrian 
intended by his incessant travelling was what Nero had failed to achieve: to break with the sedentary ("casanière") tradition of previous emperors, who saw the Empire as a purely Roman hegemony; instead, Hadrian sought to make his subjects to feel part of a commonwealth of civilized peoples, sharing a common Hellenic culture.
Hadrian, traveled as a fundamental part of his governing, and made this clear to the Roman Senate and the people. In order to check the Roman populace, he had resource to his chief equestrian adviser, Marcius Turbo, who was made Pretorian Prefect in 121- while he was still joint-governor of Dacia and Pannonia Inferior - and who had as his task to adjudicate non-senators. Busy as he was, however, Turbo was not allowed to keep check on the Senate, as Hadrian forbade equestrians to try case against senators. The Senate had ultimate legal authority over its members, as it remained formally as the highest court of appeal, from which appealing to the Emperor was forbidden. There are hints within certain sources that Hadrian also employed a secret police force, the frumentarii, to snoop primarily on people of high social standing, such as his close friends.
His visits were marked by handouts which often contained instructions for the construction of new public buildings. His intention was to strengthen the Empire from within through improved infrastructure, as opposed to conquering or annexing perceived enemies. This was often the purpose of his journeys; commissioning new structures, projects and settlements. His almost evangelical belief in Greek culture strengthened his views: like many emperors before him, Hadrian's will was almost always obeyed. Later the Greek rhetorician Aelius Aristides was to extol his activities by writing that he "extended over his subjects a protecting hand, raising them as one helps fallen men on their feet".
His travelling court was large, including administrators and likely architects and builders. The burden on the areas he passed through were sometimes great. While his arrival usually brought some benefits it is possible that those who had to bear the burden were of different class to those who reaped the benefits. For example, huge amounts of provisions were requisitioned during his visit to Egypt, this suggests that the burden on the mainly subsistence farmers must have been intolerable, causing some measure of starvation and hardship.
At the same time, as in later times all the way through the European Renaissance, kings were welcomed into their cities or lands, and the financial burden was completely on them, and only indirectly on the poorer class. Hadrian's first tour came just four years after assuming the office of Caesar, when he sought a cure for a skin disease thought to be leprosy and travelled to Judea Roman province while en route to Egypt. This time also allowed himself the freedom to concern himself with his general cultural aims. At some point, he travelled north, towards Germania and inspected the Rhine-Danube frontier, allocating funds to improve the defences. However, it was a voyage to the Empire's very frontiers that represented his perhaps most significant visit; upon hearing of a recent revolt, he journeyed to Britannia.
Britannia (122).
Prior to Hadrian's arrival in Britain, there had been a major rebellion in Britannia from 119 to 121. Although operations in Britannia at the time got no mention worthy of note in the literary sources, inscriptions tell of an "expeditio Britannica" involving major troop movements, including sending a vexillatio (i.e., a detachment) of some 3,000 men taken from legions stationed on the Rhine and in Spain; Fronto writes about military losses in Britannia at the time. The "Historia Augusta" notes that the Britons could not be kept under Roman control; Pompeius Falco was sent to Britain to restore order and coins of 119–120 refer to this. In 122 Hadrian initiated the construction of Hadrian's Wall. The wall was built, "to separate Romans from barbarians," according to the "Historia Augusta" (Augustan Histories). It deterred attacks on Roman territory and controlled cross-border trade and immigration.
Unlike the Germanic limes, built of wood palisades, the lack of suitable wood in the area required a stone construction. The western third of the wall, from modern-day Carlisle to the River Irthing, was originally built of turf because of the lack of suitable building stone. This problem also led to the narrowing of the width of the wall, from the original 12 feet to 7. The turf wall was however later rebuilt in stone, and a large ditch with adjoining mounds, known today as the Vallum, was dug to the south of the wall.
Under Hadrian, a shrine was erected in York to Britain as a Goddess, and coins that introduced a female figure as the personification of Britain, labeled , were struck. By the end of 122, he had concluded his visit to Britannia, and from there headed south by sea to Mauretania, never to return. Thus he never saw the finished wall that bears his name.
Africa, Parthia and Anatolia (123-124).
In 123, he arrived in Mauretania where he personally led a campaign against local rebels. However, this visit was to be short, as reports came through that the Eastern nation of Parthia was again preparing for war; as a result, Hadrian quickly headed eastwards. On his journey east it is known that at some point he visited Cyrene during which he personally made available funds for the training of the young men of well-bred families for the Roman military. This might well have been a stop off during his journey East. Cyrene had already benefited from his generosity when he in 119 had provided funds for the rebuilding of public buildings destroyed in the recent Jewish revolt.
When Hadrian arrived on the Euphrates, he characteristically solved the problem through a negotiated settlement with the Parthian king Osroes I. He then proceeded to check the Roman defences before setting off West along the coast of the Black Sea. He probably spent the winter in Nicomedia, the main city of Bithynia. As Nicomedia had been hit by an earthquake only shortly prior to his stay, Hadrian was generous in providing funds for rebuilding. Thanks to his generosity he was acclaimed as the chief restorer of the province as a whole.
It is more than possible that Hadrian visited Claudiopolis and there espied the beautiful Antinous, a young boy who was destined to become the emperor's beloved. Sources say nothing about when Hadrian met Antinous; however, there are depictions of Antinous that shows him as a young man of 20 or so. As this was shortly before Antinous's drowning in 130, Antinous would most likely have been a youth of 13 or 14. It is possible that Antinous may have been sent to Rome to be trained as a page to serve the emperor, and only gradually did he rise to the status of imperial favourite.
After meeting Antinous, Hadrian travelled through Anatolia. The route he took is uncertain. Various incidents are described, such as his founding of a city within Mysia, Hadrianutherae, after a successful boar hunt. (The building of the city was probably more than a mere whim — low-populated wooded areas such as the location of the new city were already ripe for development). Some historians dispute whether Hadrian did in fact commission the city's construction at all. At about this time, plans to complete the Temple of Zeus in Cyzicus, begun by the kings of Pergamon, were put into practice. The temple, whose completion had been contemplated by Trajan, received a colossal statue of Hadrian, and was built with dazzling white marble with gold tread. Cyzicus received the additional honor of being declared a regional center for the Imperial cult ("neocoros"), sharing it with Pergamon, Smyrna, Ephesus and Sardes - something that offered the benefits of Imperial sponsorship of sacred games - attracting tourism and simulating private expenditure - as well as channeling intercity rivalry into common acceptance of Roman rule.
Greece (124-125).
The climax of this tour was the destination that the hellenophile Hadrian must all along have had in mind, Greece. He arrived in the autumn of 124 in time to participate in the Eleusinian Mysteries. By tradition, at one stage in the ceremony the initiates were supposed to carry arms; but, this was waived to avoid any risk to the emperor. At the Athenians' request, he conducted a revision of their constitution — among other things, a new phyle (tribe) was added bearing his name. Also, a system of coercive purchases of oil was imposed on Athenian producers in order to grant an adequate supply of the commodity; management of the system was left in the hands of the local Assembly and Council, appeals to the Emperor notwithstanding. It was possibly at this time that Hadrian received, according to Eusebius, an apology (i.e., a defense) of the Christian faith made by two Christians, Quadratus and Aristides. Apparently Hadrian simply kept to Trajan's policy of passive tolerance, by which Christians should not be sought after but sentenced only after due trial.
During the winter he toured the Peloponnese. His exact route is uncertain; however, Pausanias reports of tell-tale signs, such as temples built by Hadrian and the statue of the emperor built by the grateful citizens of Epidaurus in thanks to their "restorer". He was especially generous to Mantinea, where he restored the Temple of Poseidon Hippios ; this supports the theory that Antinous was in fact already Hadrian's lover because of the strong link between Mantinea and Antinous's home in Bithynia. However, as this kinship between Mantinea and Bythinia was itself a mythological fiction of the kind used at the time for favoring political alliances between polities, a more serious reason might exist for Hadrian's particular generosity. Hadrian's buildings in Greece were no mere whims, as they followed a pattern of favoring old religious centers: besides the temple at Mantinea, Hadrian restored other ancient shrines in Abae, Argos - where he restored the Heraion - and Megara. This was a way of gathering legitimacy to Roman imperial rule by associating it to the glories of classical Greece - something well in line with contemporary antiquarian taste in cultural matters.
By March 125, Hadrian had reached Athens, presiding over the festival of Dionysia. The building program that Hadrian initiated was substantial. Various rulers had done work on building the Temple of Olympian Zeus over a timespan of more than five centuries — it was Hadrian and the vast resources he could command that ensured that the job would be finished. He also initiated the construction of several public buildings on his own whim and even organized the building of an aqueduct.
Return to Italy and trip to Africa (126-128).
On his return to Italy, Hadrian made a detour to Sicily. Coins celebrate him as the restorer of the island, though there is no record of what he did to earn this accolade.
Back in Rome, he was able to see for himself the completed work of rebuilding the Pantheon. Also completed by then was Hadrian's villa nearby at Tibur, a pleasant retreat by the Sabine Hills for whenever Rome became too much for him. At the beginning of March 127 Hadrian set off for a tour of Italy. Once again, historians are able to reconstruct his route by evidence of his hand-outs rather than the historical records.
For instance, in that year he restored the Picentine earth goddess Cupra in the town of Cupra Maritima. At some unspecified time he improved the drainage of the Fucine lake. Less welcome than such largesse was his decision in 127 to divide Italy into 4 regions under imperial legates with consular rank, who had jurisdiction over all of Italy excluding Rome itself, therefore shifting cases from the courts of Rome. Actually, the four consulars acted as governors of the regions assigned to them. Having Italy effectively reduced to the status of a group of mere provinces did not go down well with Italian hegemonic feelings (specially with the Roman Senate) and this innovation did not long outlive Hadrian.
Hadrian fell ill around this time, though the nature of his sickness is not known. Whatever the illness was, it did not stop him from setting off in the spring of 128 to visit Africa. His arrival began with the good omen of rain ending a drought. Along with his usual role as benefactor and restorer, he found time to inspect the troops; his speech to the troops survives to this day. Hadrian returned to Italy in the summer of 128 but his stay was brief, as he set off on another tour that would last three years.
Greece, Asia, and Egypt (128–130).
In September 128, Hadrian again attended the Eleusinian mysteries. This time his visit to Greece seems to have concentrated on Athens and Sparta — the two ancient rivals for dominance of Greece. Hadrian had played with the idea of focusing his Greek revival round Amphictyonic League based in Delphi, but he by now had decided on something far grander. His new Panhellenion was going to be a council that would bring Greek cities together wherever they might be found. The meeting place was to be the new temple to Zeus in Athens. Having set in motion the preparations — deciding whose claim to be a Greek city was genuine would in itself take time — Hadrian set off for Ephesus. The notion of "Greek city", however, was mostly political and mythological, in preference to historical: it involved fabricated claims to Greek origins and imperial favour. Most important, it linked appreciation of an idealized cultural Hellenism with loyalty to Rome and her Emperor.The Panhellenion was devised with a view of associating the Roman Emperor with protection of Greek culture and of the "liberties" of Greece - in the case , urban self-government. It allowed Hadrian to appear as the fictive heir to Pericles, who supposedly had convened a previous Panhellenic Congress - which is mentioned, by the way, only in his biography by Plutarch, whose sympathies to the Imperial order are well-known. 
In October 130, while Hadrian and his entourage were sailing on the Nile, Antinous drowned for unknown reasons; accident, suicide, murder or religious sacrifice have all been postulated. The emperor was grief-stricken. He ordered Antinous deified, and cities were named after the boy, medals struck with his effigy, and statues erected to him in all parts of the empire. Temples were built for his worship in Bithynia, Mantineia in Arcadia. In Athens, festivals were celebrated in his honour and oracles delivered in his name. The city of Antinopolis or Antinoe was founded on the ruins of Besa, where he died.
Greece and the East; return to Rome (130-133).
Hadrian’s movements subsequent to the founding of Antinopolis on 30 October 130 are obscure. Whether or not he returned to Rome, he traveled in the East during 130/131 (see below) and spent the winter of 131–32 in Athens, where he dedicated the Olympeion, and probably remained in Greece or went East because of the Jewish rebellion which broke out in Judaea in 132 (see below). Inscriptions make it clear that he took the field in person against the rebels with his army in 133; he then returned to Rome, probably in that year and almost certainly (judging again from inscriptions) via Illyricum. 
Legal reforms and State apparatus.
It was around that time that Hadrian enacted, through the jurist Salvius Julianus, what was to become the first attempt to codify Roman Law: the Perpetual Edict, according to which the various forms of legal action introduced yearly by pretors were to remain fixed. The practical meaning of this measure was that the law could no more be changed by a magistrate's personal interpretation of it; it had become a fixed statute, which only the Emperor could alter. At the same time, following a procedure initiated by Domitian, Hadrian professionalized the Emperor's advisory board on legal matters, the Prince's Counsel or "consilia principis", which became a permanent body staffed by salaried legal aids. By so doing, Hadrian developed a professional bureaucracy, consisting mainly of equestrians, in place of the earlier freedmen of the Imperial household, that was to control the political field instead of the Senate's individual members.Something that was a sure marker of the superseding of surviving Republican institutions by an openly autocratic political system.Hadrian's bureaucracy was supposed to fill the permanent administrative functions never exercised before by the old magistrates, and therefore it didn't objectively detract from the Senate's position: the new civil servants were free men and as such supposed to act in behalf of the interests of the "Crown", not of the Emperor as an individual. However, the Senate never accepted the loss on its prestige caused by the emergence of a new aristocracy alongside it.Therefore the increased strain of the relations between the Senate and the Emperor that was to be a hallmark of the end of Hadrian's reign.
Hadrian and Judea; Second Roman-Jewish War and Jewish persecution (132–136).
In 130/131, Hadrian toured the East, bestowing honorific titles on many regional centers. Palmyra received a state visit and the civic name Hadriana Palmyra. It was then that Hadrian visited the ruins of Jerusalem, in Roman Judaea, left after the First Roman-Jewish War of 66–73. According to a midrashic tradition, he firstly showed himself sympathetic to the Jews, allegedly planning to have the city rebuilt and allowing the rebuilding of the Temple, but when told by Samaritans that it would be the cause for much sedition, he then changed his mind. The reliability of this tradition is, however, doubtful, and ascribing such a motivation to Hadrian is something that seems to have been undone or contradicted by his -allegedly -subsequent decision to build a temple to the Roman god Jupiter on the ruins of the Temple Mount instead and other temples to various Roman gods throughout Jerusalem, including a large temple to the goddess Venus.
According to a modern scholar, Hadrian's original intention might have been to rebuild Jerusalem as a Roman colony -such as Vespasian had done earlier to Caesarea Maritima - with various honorific and fiscal privileges, as well as a pagan population, nonetheless paying attention to the city's special religious status to the Jews: witness the fact that the actual postwar existence of the Temple of Jupiter on the precise site of Herod's temple is doubtful; what is attested is the existence of an honorary statue of Hadrian at the place - the Temple of Jupiter was possibly located elsewhere. It is accepted that the usual Roman policy in other colonies involved exempting the Jewish population from participating in pagan rituals. What was demanded from Jewish communities was political support to the Roman imperial order, as attested in Caesarea, where epigraphy attests that some of its Jewish citizens served in the Roman army during both the 66 and 132 rebellions. It has been speculated, therefore, that Hadrian intended somehow to assimilate the Jewish Temple into the civic-religious basis of support to his reign, as he had just done with Greek and other traditional places of worship. But then, after the war, Hadrian even renamed Jerusalem itself, as Aelia Capitolina after himself and Jupiter Capitolinus, the chief Roman deity. According to Epiphanius, Hadrian appointed Aquila from Sinope in Pontus as "overseer of the work of building the city," seeing that Aquila was related to him by marriage. Hadrian is said to have placed the city's main Forum at the junction of the main Cardo and Decumanus Maximus, now the location for the (smaller) Muristan.
A tradition based on the Historia Augusta suggests that tensions grew higher when Hadrian abolished circumcision ("brit milah"), which he, a Hellenist, viewed as mutilation. However one scholar, Peter Schäfer, maintains that there is no evidence for this claim, given the notoriously problematical nature of the "Historia Augusta" as a source, the "tomfoolery" shown by the writer in this particular relevant passage, and the fact that contemporary Roman legislation on "genital mutilation" seems to address the general issue of castration of slaves by their masters The notion of an "antisemitic" legislation by Hadrian is, therefore, possibly an anachronistic ("midrashic", in the words of a modern scholar) reading of Ancient sources.
It's possible that other issues intervened between Hadrian's intention to rebuild Jerusalem and the outbreak of the war: the tension between incoming Roman colonists and supporters who had appropriated land confiscated after the First Jewish War and the landless poor, as well as the existence of messianic groups triggered by an interpretation of Jeremiah's prophecy promising that the Temple would be rebuilt seventy years after its destruction, repeating the timing of the restoration of the First Temple after the Babylonian exile- something that would put the restoration of the Second Temple to around 140 
Hadrian's anti-Jewish policies (or, alternatively, assimilation policies by means of cultural and political hellenization) triggered in Judaea a massive anti-Hellenistic and anti-Roman Jewish uprising, led by Simon bar Kokhba. Based on the delineation of years in Eusebius' "Chronicon" (now Chronicle of Jerome), it was only in the 16th year of Hadrian's reign, or what was equivalent to the 4th year of the 227th Olympiad, that the Jewish revolt began, under the Roman governor Tineius (Tynius) Rufus who had asked for an army to crush the resistance. Bar Kokhba, the leader of the resistance, punished any Jew who refused to join his ranks. It was then that Hadrian called his general Sextus Julius Severus from Britain, and troops were brought from as far as the Danube. The Fifth Macedonian Legion and the Eleventh Claudian Legion had also taken part in war operations in Judea at the time. Roman losses were very heavy- as they were compared by Fronto to the casualties of the earlier British uprising - and it is believed that an entire legion, the XXII Deiotariana, which according to epigraphy didn't outlast Hadrian's reign, was destroyed in the rebellion. Indeed, Roman losses were so heavy that Hadrian's report to the Roman Senate omitted the customary salutation, "If you and your children are in health, it is well; I and the legions are in health."
Hadrian's army eventually put down the rebellion in 135. According to Cassius Dio, overall war operations in the land of Judea left some 580,000 Jews killed, and 50 fortified towns and 985 villages razed to the ground. The most famous battle took place in Beitar, a fortified city 10 km. southwest of Jerusalem. The city only fell after a lengthy siege of three and a half years, at which time Hadrian prohibited the Jews from burying their dead. They were eventually afforded burial when Antoninus (Pius) succeeded Hadrian as Roman Emperor. According to the Babylonian Talmud, after the war Hadrian continued the persecution of Jews.
The rabbinical sources, however, seem more concerned with morals and religion than with history, therefore offering a legendary account of the war and its aftermath, according to whom Hadrian attempted to root out Judaism, which he saw as the cause of continuous rebellions, prohibited the Torah law, the Hebrew calendar and executed Judaic scholars (see Ten Martyrs). The sacred scroll was ceremonially burned on the Temple Mount. In an attempt to erase the memory of Judaea, he renamed the province Syria Palaestina (after the Philistines), and Jews were barred from entering its rededicated capital. When Jewish sources mention Hadrian it is always with the epitaph "may his bones be crushed" (שחיק עצמות or שחיק טמיא, the Aramaic equivalent), an expression never used even with respect to Vespasian or Titus who destroyed the Second Temple.
Opposedly, scholars of a more revisionist tinge remark that Hadrian's strictures on circumcision and no-entry policy were poorly enforced, falling into abeyance with his death, and that, enslaving of war prisoners, war casualties and wantom destruction notwithstanding, Palestine remained predominantly Jewish in population, as well as its culture and religious life, a fact reflected by the completion of the Mishnah in the early Third Century, and that the "exile" of the Jewish people as a whole was originally a Christian legend coined by Justin Martyr, who saw it as divine punishment for Jesus' Crucifixion. The doubtful notion of the continued exclusion of the Jews from Jerusalem, as a measure that was strictly enforced long after Hadrian's reign, comes also from a Christian source, namely Eusebius' ecclesiastical histories, and intends to extol Jerusalem's alternative identity as a purely Christian center of worship. Eventually, in the view of these historians, this legendary corpus would become secularized so as to support a legitimization of the Zionist political project. Modern controversies aside, what Hadrian's bloody repression of the revolt undoubtedly achieved was to put an end to any pretension of Jewish political independence alongside the Roman Imperial order.
In Rabbinic literature.
Rabbinic literature is critical of Hadrian's policy, particularly that of religious intolerance concerning the Jews. Indeed, his policies were viewed as an attack on the religious freedom of Torah law practice. Most of the stories related by the Sages of Israel reflect a two-faced approach to tolerance of the Jewish people. In one story he punishes a Jew who failed to greet him, and then punishes another Jew who wished him well. When asked what the logic was for his punishing both men, he replied: "You wish to give me advice on how to kill my enemies?"
In another story, Hadrian got down from his chariot and bowed to a Jewish girl afflicted with leprosy. When queried by his soldiers as to why he did this, Hadrian responded with a dual verse from the book of Isaiah in praise of the nation of Israel "So says God the redeemer of Israel to the downtrodden soul to the (made) repulsive nation, kings will view and stand."
The Malbim commentary to the book of Daniel comments how Hadrian erected a statue of himself at the site of the Bet HaMikdash on a day marking the anniversary of the Temple's destruction by Titus.
According to Jewish historical records of that time, the famous rabbi and scholar and a contemporary of Hadrian, Rabbi Yehoshua, the son of Hananiah, opposed any Jewish military intervention against the occupying Roman army, in spite of Rome's harsh decrees against the Jewish people. Rabbi Yehoshua, the son of Hananiah, is reported as saying: "A lion once pounced upon its prey and got a bone stuck in his throat. He then said, 'Whosoever comes and takes it out, I will give to him a reward.' An Egyptian heron came along whose bill is long, and reaching down into the lion's throat, extracted the bone. The bird then said to the lion, 'Give to me my reward.' The lion replied, 'Just be happy that you can say, I went down into the lion's mouth and I came out alive and well.' It is the same with us. It is enough that we have gone into this nation and came out with our lives."
Final years.
Succession.
Hadrian spent the final years of his life at Rome. In 134, he took an Imperial salutation for the end of the Second Jewish War (which was not actually concluded until the following year). In 136, he dedicated a new Temple of Venus and Roma on the former site of Nero's Golden House.
About this time, suffering from poor health, he turned to the problem of the succession. In 136 he adopted one of the ordinary consuls of that year, Lucius Ceionius Commodus, who took the name Lucius Aelius Caesar. He was the son-in-law of Gaius Avidius Nigrinus, one of the "four consulars" executed in 118, but was himself in delicate health. It has been speculated that Hadrian was fully aware that Aelius would never outlive him, and that the adoption of an aristocrat scion with no blood ties to the Emperor was a belatedly attempt to make amends for the episode of the four consulars, therefore aiming at a reconciliation with the powerful clan of Italian families in the Senate. Granted tribunician power and the governorship of Pannonia, Aelius Caesar held a further consulship in 137, but died on 1 January 138.
Following the death of Aelius Caesar, Hadrian next adopted Titus Aurelius Fulvus Boionius Arrius Antoninus (the future emperor Antoninus Pius), who had served as one of the five imperial legates of Italy (a post created by Hadrian) and as proconsul of Asia. On 25 February 138 Antoninus received tribunician power and imperium. Moreover, to ensure the future of the dynasty, Hadrian required Antoninus to adopt both Lucius Ceionius Commodus (son of the deceased Aelius Caesar) and Marcus Annius Verus (who was the grandson of an influential senator of the same name who had been Hadrian’s close friend; Annius was already betrothed to Aelius Caesar’s daughter Ceionia Fabia). Hadrian’s precise intentions in this arrangement are debatable.
Though the consensus is that he wanted Annius Verus (who would later become the Emperor Marcus Aurelius) to succeed Antoninus, it has also been argued that he actually intended Ceionius Commodus, the son of his own adopted son, to succeed, but was constrained to show favour simultaneously to Annius Verus because of his strong connections to the Hispano-Narbonensian nexus of senatorial families of which Hadrian himself was a part. As Annius Verus was the step grandson of the then Prefect of Rome Lucius Catilius Severus, one of the remnants of the all-powerful group of Spanish senators from Trajan's reign, it was unavoidable that Hadrian should show some favor to the grandson in order to count with the grandfather's support. It is possible, according to one prosopography, that Catilius Severus was the third and last husband of Hadrian's mother, Domitia Lucilla "Major". As Lucilla Major's second husband, Publius Calvisius Ruso, was the father of Domitia Lucilla "Minor", Annius Verus' mother, Lucilla Minor would actually be Hadrian's half-sister, and Annius Verus, therefore his (half)nephew. In this case, in advancing Annius Verus, Hadrian would promote his own bloodline's fortunes. Note, however, that this prosopography is not universally accepted by other scholars, who argue that Hadrian's mother was known, according to "Historia Augusta", as Domitia "Paulina".
Alternatively, it may well not have been Hadrian, but rather Antoninus Pius — who was Annius Verus’s uncle – who advanced the latter to the principal position. The fact that Annius would divorce Ceionia Fabia and remarry to Antoninus' daughter Annia Faustina points in the same direction. When he eventually became Emperor, Marcus Aurelius would co-opt Ceionius Commodus as his co-Emperor (under the name of Lucius Verus) on his own initiative.
The ancient sources present Hadrian's last few years as marked by conflict and unhappiness. The adoption of Aelius Caesar proved unpopular, not least with Hadrian's brother-in-law Lucius Julius Ursus Servianus and Servianus' grandson Gnaeus Pedanius Fuscus Salinator. Servianus, though now far too old, had stood in line of succession at the beginning of the reign; Fuscus is said to have had designs on the imperial power for himself, and in 137 he may have attempted a coup in which his grandfather was implicated. Whatever the truth, Hadrian ordered that both be put to death. Servianus is reported to have prayed before his execution that Hadrian would "long for death but be unable to die". The prayer was fulfilled; as Hadrian suffered from his final, protracted illness, he had to be prevented from suicide on several occasions.
Death.
Hadrian died in the year 138 on the 10th of July, in his villa at Baiae at the age of 62. The cause of death is believed to have been heart failure. Dio Cassius and the Historia Augusta record details of his failing health.
He was buried first at Puteoli, near Baiae, on an estate which had once belonged to Cicero. Soon after, his remains were transferred to Rome and buried in the Gardens of Domitia, close by the almost-complete mausoleum. Upon completion of the Tomb of Hadrian in Rome in 139 by his successor Antoninus Pius, his body was cremated, and his ashes were placed there together with those of his wife Vibia Sabina and his first adopted son, Lucius Aelius, who also died in 138. After threatening the Senate - which toyed with refusing Hadrian's divine honors - by refusing to assume power himself, Antoninus eventually succeed in having his predecessor deified in 139 and given a temple on the Campus Martius. The Senate in consequence agreed to grant Antoninus the title Pius for his filial piety in granting his adoptive father honors.
Poem by Hadrian.
According to the "Historia Augusta", Hadrian composed shortly before his death the following poem:
References.
Primary sources.
Inscriptions:

</doc>
<doc id="13623" url="http://en.wikipedia.org/wiki?curid=13623" title="Herman Melville">
Herman Melville

Herman Melville (August 1, 1819 – September 28, 1891) was an American novelist, writer of short stories, and poet from the American Renaissance period. Most of his writings were published between 1846 and 1857. Best known for his sea adventure "Typee" (1846) and his whaling novel "Moby-Dick" (1851), he was almost forgotten during the last thirty years of his life. Melville's writing draws on his experience at sea as a common sailor, exploration of literature and philosophy, and engagement in the contradictions of American society in a period of rapid change. The main characteristic of his style is probably its heavy allusiveness, a reflection of his use of written sources. Melville's way of adapting what he read for his own new purposes, scholar Stanley T. Williams wrote, "was a transforming power comparable to Shakespeare's".
Born in New York City as the third child of a merchant in French dry-goods who went bankrupt, his formal education stopped abruptly after the death of his father in 1832, shortly after bankruptcy. Melville briefly became a schoolteacher before he first took to sea in 1839, as a common sailor on a merchant voyage to Liverpool, the basis for his fourth book, "Redburn" (1849). In late December 1840 he signed up for his first whaling voyage aboard the whaler "Acushnet", but jumped ship eighteen months later in the Marquesas Islands. He lived among the natives for up to a month, of which his first book, "Typee" (1846), is a fictionalized account that became such a success that he worked up a sequel, "Omoo" (1847). The same year Melville married Elizabeth Knapp Shaw; their four children were born between 1849 and 1855.
In August 1850, Melville moved to a farm near Pittsfield, Massachusetts, where he established a profound but short-lived friendship with Nathaniel Hawthorne and wrote "Moby-Dick", which, published in 1851 to mixed reviews, became a commercial failure. Less than a year later, Melville's career as a popular author was definitely over with the cool reception of '. The next years he turned to writing short fiction for magazines, of which "Bartleby, the Scrivener" was the first. After the serialized novel "Israel Potter" was published as a book in 1855, the short stories were collected in 1856 as "The Piazza Tales". In 1857, Melville voyaged to England and the Near East: twenty years later, he worked his experience in Egypt and Palestine into an epic poem, ' (1876). In 1857 "The Confidence-Man" appeared, the last prose work published during his lifetime. Having secured a position of Customs Inspector in New York, he now turned to poetry, the first example of which is his poetic reflection on the Civil War, "Battle-Pieces and Aspects of the War"(1866).
In 1867 his oldest child Malcolm died at home from a self-inflicted gunshot. In 1886 he retired as Customs Inspector and privately published two volumes of poetry. During the last years of his life, he returned to prose once again and worked on "Billy Budd, Sailor", left unfinished at his death, and eventually published in 1924. His death in 1891 from cardiovascular disease subdued the reviving interest in him, but the centennial of his birth marked the starting point of the "Melville Revival", and Melville's rise through the canon to the eventual appreciation of his writings as world classics.
Biography.
Early life.
Born Herman Melvill in New York City on August 1, 1819, to Allan Melvill (1782–1832) and Maria Gansevoort Melvill (1791–1872), Herman was the third of eight children born between 1815 and 1830. Part of a well-established and colorful Boston family, Melville's father, Allan, spent a good deal of time abroad as a commission merchant and an importer of French dry goods.
Both Melville's grandfathers were heroes of the Revolutionary War. Major Thomas Melvill (1751–1832) had taken part in the Boston Tea Party, and his maternal grandfather, General Peter Gansevoort (1749–1812), was famous for having commanded the defense of Fort Stanwix in 1777. Melville found satisfaction in his "double revolutionary descent". Major Melvill sent Allan not to college but to France at the turn of the century, where he spent two years in Paris and learned to speak and write French fluently. He subscribed to his own father's Unitarianism. The wife he married in 1814, Maria Gansevoort Melvill, was committed to the Dutch Reformed version of the Calvinist creed that had ruled in her family. The severe Protestantism of the Gansevoort's tradition ensured that she knew her Bible well, in English as well as Dutch, the language she had grown up speaking with her parents.
Almost three weeks after his birth, on August 19, Melville was baptized at home by a Reverend of the South Reformed Dutch Church. During the 1820s Melville lived a privileged, opulent life, in a household with three or more servants at a time. Once in every four years the family moved to more spacious and prestigious quarters, all the way to Broadway in 1828. Allan Melvill lived beyond his means and on large sums he borrowed from both his father and his wife's widowed mother. His wife's opinion of his financial conduct is unknown. Biographer Hershel Parker suggests Maria "thought her mother's money was infinite and that she was entitled to much of her portion now, while she had small children." How well, biographer Delbanco adds, the parents managed to hide the truth from their children is "impossible to know." Things came to a halt in 1830 when Maria's family finally had enough, at which point Allan's total debt to both families exceeded $20,000,-.
Melville's education began when he was five years old, around the time the Melvills moved to a newly built house at 33 Bleecker Street. In 1826, the same year that Melville contracted scarlet fever, Allan Melvill, who sent both Gansevoort and Herman to the New York Male High School, described Melville in a letter to Peter Gansevoort Jr. as "very backwards in speech & somewhat slow in comprehension". His older brother Gansevoort appeared to be the brightest of the children, but soon Melville's development increased its pace. "You will be as much surprised as myself to know," Allan wrote Peter Gansevoort Jr., "that Herman proved the best Speaker in the introductory Department, at the examination of the High School, he has made rapid progress during the 2 last quarters." In 1829 both Gansevoort and Herman were transferred to Columbia Grammar & Preparatory School, with Herman enrolling in the English Department on 28 September. "Herman I think is making more progress than formerly," Allan wrote in May 1830 to Major Melvill, "& without being a bright Scholar, he maintains a respectable standing, & would proceed further, if he could only be induced to study more – being a most amiable & innocent child, I cannot find it in my heart to coerce him."
Emotionally unstable and behind with paying the rent for the house on Broadway, Allan tried to recover from his setbacks by moving his family to Albany in 1830 and going into the fur business. In Albany, Melville attended the Albany Academy from October 1830 to October 1831, where he took the standard preparatory course, studying reading and spelling; penmanship; arithmetic; English grammar; geography; natural history; universal, Greek, Roman and English history; classical biography; and Jewish antiquities. It is unknown why he left the Academy in October 1831'Parker suggests that by then "even the tiny tuition fee seemed too much to pay." His brothers Gansevoort and Allan continued their attendance a few months longer, Gansevoort until March the next year. "The ubiquitous classical references in Melville's published writings", as Melville scholar Merton Sealts observed, "suggest that his study of ancient history, biography, and literature during his school days left a lasting impression on both his thought and his art, as did his almost encyclopedic knowledge of both the Old and the New Testaments".
In December Melville's father Allan returned from New York City by steamboat, but ice forced him to travel the last seventy miles for two days and two nights in an open horse carriage at two degrees below zero, with the result that he developed a cold. In early January he began to show "signs of delirium" and his situation grew worse until he – in the words of his wife – "by reason of severe suffering was deprive'd of his Intellect." Two months before reaching fifty, Allan Melvill died on 28 January 1832. Since Melville was no longer attending school, he must have witnessed these scenes: twenty years later he described such a death of Pierre's father in "Pierre" (bk. 4 ch. 2).
1832–1838: After father's death.
Two months after his father's death, Gansevoort entered the cap and fur business and Maria sought consolation in her faith, and in April she was admitted as a member of the First Reformed Dutch Church. Uncle Peter Gansevoort, who was one of the directors of the New York State Bank, got Herman a job as clerk for $150 a year. The issue of his emotional response to all the dramas in his young life, is a question biographers answer by citing from "Redburn": "I had learned to think much and bitterly before my time," the narrator remarks, "I must not think of those delightful days, before my father became a bankrupt...and we removed from the city; for when I think of those days, something rises up in my throat and almost strangles me."
When Melville's grandfather Melvill died at on 16 September 1832, it turned out that Allan had borrowed more than his share of the inheritance. He left Maria Melvill only $20. The widowed grandmother died on 12 April 1833. Melville did his job well at the bank; though he was only fourteen in 1834, the bank considered him competent enough to be sent over to Schenectady on an errand. Not much else is known from this period, except that he was very fond of drawing. The visual arts became a lifelong interest.
Around May 1834 the Melvilles moved to another house in Albany, a three-story brick house. According to biographer Hershel Parker, that same month a fire destroyed Gansevoort's skin-preparing factory, which left him with personnel he could neither use nor afford anymore. Instead he pulled Melville out of the bank to man the cap and fur store. Presenting a different sequence of events, biographer Andrew Delbanco says that Gansevoort was doing so well he could hire his younger brother until a fire broke out, this time in 1835 and destroying both factory and the store. In any case, his older brother Gansevoort served as a role model for Melville in various ways. In early 1834 Gansevoort had become a member of the Albany's Young Men's Association for Mutual Improvement, and in January 1835 Melville himself became a member as well.
In 1835, while still working in the store, Melville enrolled in Albany Classical School. Biographer Parker suggests that perhaps this could be afforded with Maria's part of the proceeds from the sale of the estate of his maternal grandmother in March 1835. In September of the following year he was back in Albany Academy, in the Latin course. He also joined debating societies, in an apparent effort to make up as much as he could for his missed years of schooling. In this period he also became acquainted with Shakespeare's "Macbeth" at least, and teased his sisters with a passage from the witch scenes.
In March 1837 he was again withdrawn from Albany Academy. Gansevoort's copies of John Todd's "Index Rerum", a blank book, more of a register, in which one could index remarkable passages from books read, for easy retrieval. Printed was a sample entry: "Pequot, beautiful description of the war with" with a short title reference to where in Benjamin Trumbull's "A Complete History of Connecticut" (1797 or 1818) the description could be found. The two surviving volumes are the best evidence for Melville's reading in this period, because there is little doubt that Gansevoort's reading served him as a guide. The entries include books that Melville later used for "Moby-Dick" and "Clarel": "Parsees—of India—an excellent description of their character, & religion & an account of their descent—East India Sketch Book p. 21." Other entries are on Panther, the pirate's cabin and storm at sea from James Fenimore Cooper's "Red Rover", Saint-Saba.
That April an economic crisis forced Gansevoort to file for bankruptcy and Uncle Thomas Jr. secretly planned to leave Pittsfield, where he did not pay taxes on the farm. On June 5 Maria informed the younger children that they had to move to some village where the rent was cheaper than in Albany. Gansevoort became a Law student in New York City and Melville took care of the farm while his Uncle was in Galena. That summer he decided to become a schoolteacher. He got a position at Sikes District School near Lenox, where he taught some thirty students of various ages, including his own.
His term over he returned to his mother in 1838. In February he was elected president of the Philo Logos Society, which Peter Gansevoort invited to move into Stanwix Hall for no rent. Many chambers were vacant as a result of the economic crisis. In March 1838 Melville published in the Albany Microscope two polemical letters about issues in the debating societies he was engaged in, but it is not entirely clear what the polemic was about. Biographers Leon Howard and Hershel Parker suggest that the real issue was the youthful desire to exercise one's rhetoric skills in public, and the first appearance in print would have been an exciting experience for all young men involved.
In May the Melvilles moved to Lansingburgh, almost a dozen miles north of Albany, into a rented house on the river in what is now Troy, at River Street and 114th. The family's retreat, in biographer Delbanco's words, was now complete: from the metropolis to a provincial city to a village. What Melville was doing after his term at Sikes ended until November, or if he even had a job after that, remains a mystery. Apparently he courted a local Lansingburgh girl sometime during the summer, but nothing else is known.
On 7 November Melville arrived in Lansingburgh. Where he had come from is unknown. Five days later he paid for a term at Lansingburgh Academy where he took a course in surveying and engineering. In April 1839 Peter Gansevoort failed to get him a job at the Canal.
First and attributed writings.
Only weeks after he failed to find a job as an engineer, Melville—using the unresolved initials L.A.V.--contributed "Fragments from a Writing Desk" to the "Democratic Press and Lansingburgh Advertiser", a weekly newspaper, which printed the piece in two installments, the first on 4 May. According to scholar Sealts, the heavy-handed allusions reveal his early familiarity with the writings of William Shakespeare, John Milton, Walter Scott, Richard Brinsley Sheridan, Edmund Burke, Samuel Taylor Coleridge, Lord Byron, and Thomas Moore. Biographer Parker calls the piece "characteristic Melvillean mood-stuff," and considers its prose style "excessive enough to allow him to indulge his extravagances and just enough overdone to allow him to deny that he was taking his style seriously." Biographer Delbanco finds the prose "overheated in the manner of Poe, with sexually charged echoes of Byron and "The Arabian Nights"."
1839–1844: Years at sea.
On 31 May 1839 Gansevoort, then living in New York City, wrote that he was sure Melville could get a job on a whaler or merchant vessel if he would come to Manhattan. On June 2 Melville arrived from Albany by boat. He signed aboard the merchant ship "St. Lawrence" as a "boy" (a green hand) for a cruise from New York to Liverpool.
He returned on the same ship on the first of October, after five weeks in England. "Redburn: His First Voyage" (1849) is partly based on his experiences of this journey. At least two of the nine guide-books listed in chapter 30 had been part of Allan Melvill's library. Melville resumed teaching, now at Greenbush, New York, but left after one term because he was not paid. In the summer of 1840 he and a friend went to Galena, Illinois, presumably to find out if a relative could help them find work. Unsuccessful, they returned home in autumn.
Probably inspired by his reading of Richard Henry Dana, Jr.'s new book Two Years Before the Mast, and by Jeremiah N. Reynolds's account in the May 1839 issue of "The Knickerbocker" magazine of the hunt for a great white sperm whale named Mocha Dick, Melville and Gansevoort traveled to New Bedford, where Melville signed up for a whaling voyage aboard a new ship, the "Acushnet". At less than 360 tons, this ship with two decks and three masts was smaller than the average whaler. The contract he signed on Christmas Day with the ship's agent shows that he signed up as a "green hand," for the 1/175th of whatever profits the voyage would yield. On Sunday the 27th the brothers heard the Reverend Enoch Mudge during a church service at the Seamen's Bethel on Johnny-Cake Hill, renowned for, in biographer Parker's description, "white marble cenotaphs on the walls eloquently memorializing local men who had died at sea, often in battle with whales."
On January 3, 1841, he sailed from Fairhaven, Massachusetts, on the whaler "Acushnet", which was bound for the Pacific Ocean. He was later to comment that his life began that day. The vessel sailed around Cape Horn and traveled to the South Pacific. Melville left few direct accounts of the events of this 18-month voyage, although his whaling romance, "Moby-Dick; or, The Whale," probably describes many aspects of life on board the "Acushnet". Melville deserted the "Acushnet" in the Marquesas Islands in July 1842.
For three weeks he lived among the Typee natives, who were called cannibals by the two other tribal groups on the island—though they treated Melville very well. "Typee", Melville's first book, describes a brief love affair with a beautiful native girl, Fayaway, who generally "wore the garb of Eden" and came to epitomize the guileless noble savage in the popular imagination.
Melville did not seem to be concerned about the consequences of leaving the "Acushnet". He boarded an Australian whale ship, the "Lucy Ann", bound for Tahiti; took part in a mutiny and was briefly jailed in the native "Calabooza Beretanee". After release, he spent several months as beachcomber and island rover ('omoo' in Tahitian), eventually crossing over to Moorea. He signed articles on yet another whaler for a six-month cruise (November 1842 − April 1843), which terminated in Honolulu. After working as a clerk for four months, he joined the crew of the frigate USS "United States", which reached Boston in October 1844. He drew from these experiences in his books "Typee", "Omoo", and "White-Jacket."
The encounter with the wide ocean, seemingly abandoned by God, led Melville to experience a "metaphysical estrangement," Milder believes, and his social thought was influenced in two ways by his specific adventures in the Pacific. First, by birth and breeding Melville belonged to the genteel classes, but found himself not only placed among but also sympathizing with the "disinherited commons." Second, his acquaintance with the cultures of Polynesia enabled him to view the West from an outsider's perspective.
1845–1850: Successful writer.
Melville completed "Typee" in the summer of 1845, while living in Troy, New York. Though based upon his own adventures, the book is not a strict autobiography, if only because Melville's later experiences in Tahiti and the Sandwich Islands are worked into the narrative as well. Neither is it a fictional romance. Instead, scholar Robert Milder calls "Typee" "an appealing mixture of adventure, anecdote, ethnography, and social criticism presented with a genial latitudinarianism that gave novelty to a South Sea idyll at once erotically suggestive and romantically chaste." After some difficulty in arranging publication, he saw it first published in 1846 in London, where it became an overnight bestseller. The Boston publisher subsequently accepted "Omoo" sight unseen. "Omoo" is "a slighter but more professional book." "Typee" and "Omoo" gave Melville overnight renown as a writer and adventurer, and he often entertained by telling stories to his admirers. As the writer and editor Nathaniel Parker Willis wrote, "With his cigar and his Spanish eyes, he "talks" Typee and Omoo, just as you find the flow of his delightful mind on paper". These did not generate enough royalties to support him financially, however.
On August 4, 1847, Melville married Elizabeth Shaw, daughter of Lemuel Shaw, the Chief Justice of the Massachusetts Supreme Judicial Court. The couple honeymooned in Canada and then moved into a house on Fourth avenue in New York City. Melville wrote a long, philosophical work "Mardi", an allegorical narrative that proved a disappointment for readers who wanted another rollicking and exotic sea yarn. Actually the book began as another South Sea story, but as he wrote Melville left that genre behind, first in favor of "a romance of the narrator Taji and the lost maiden Yillah" and then "to an allegorical voyage of the philosopher Babbalanja and his companions through the imaginary archipelago of Mardi."
On 16 February the Melvilles' first child, Malcolm, was born, which may have stirred memories of his own father. The bankruptcy and death of Allan Melville, and Melville's own youthful humiliations surface in "Redburn" (1849), "a story of outward adaptation and inner impairment." Melville based the book on his first sea voyage, of 1839 to Liverpool, just as he drawn on his experiences of 1844 aboard the warship "United States" for "White-Jacket" (1850).
In 1850, the Melvilles moved to Massachusetts. They had four children: two sons and two daughters.
1850–1851: Hawthorne and "Moby-Dick".
At first "Moby-Dick" moved swiftly. In early May 1850 he wrote to Richard Henry Dana, also a sea author, saying he was already "half way" done. In June he described the book to his English publisher as "a romance of adventure, founded upon certain wild legends in the Southern Sperm Whale Fisheries", and promised it would be done by the fall. Since the manuscript for the book has not survived, it is impossible to know for sure its state at this critical juncture. Over the next several months, Melville's plan for the book underwent a radical transformation into what has been described as "the most ambitious book ever conceived by an American writer".
In September 1850 the Melvilles purchased Arrowhead, a farm house in Pittsfield, Massachusetts. (It is now preserved as a house museum and has been designated a National Historic Landmark). Here Melville and Elizabeth lived for 13 years. While living at Arrowhead, Melville befriended the author Nathaniel Hawthorne, who lived in nearby Lenox. Melville wrote ten letters to Hawthorne, "all of them effusive, profound, deeply affectionate". Melville was inspired and encouraged by his new relationship with Hawthorne during the period that he was writing "Moby-Dick." He dedicated this new novel to Hawthorne, though their friendship was to wane only a short time later.
On 18 October "The Whale" was published in Britain in three volumes, and on 14 November "Moby-Dick" appeared in the United States as a single volume. In between these dates, on 22 October, the Melvilles' second child, Stanwix, was born.
1852–1857: Unsuccessful writer.
"", a novel partly autobiographical and difficult in style, was not well received. The New York "Day Book" on September 8, 1852, published a venomous attack headlined "HERMAN MELVILLE CRAZY". The item, offered as a news story, reported, 
A critical friend, who read Melville's last book, "Ambiguities," between two steamboat accidents, told us that it appeared to be composed of the ravings and reveries of a madman. We were somewhat startled at the remark, but still more at learning, a few days after, that Melville was really supposed to be deranged, and that his friends were taking measures to place him under treatment. We hope one of the earliest precautions will be to keep him stringently secluded from pen and ink.
On 22 May 1853 Elizabeth (Bessie) was born, the Melvilles' third child and first daughter, and on or about that day Herman finished work on "Isle of the Cross"—one relative wrote that 'The Isle of the Cross' is almost a twin sister of the little one..." Herman traveled to New York, but later wrote that publisher, Harper & Brothers, was "prevented" from publishing his manuscript, presumed to be "Isle of the Cross", which has been lost.
Between 1853 and 1856, Melville published fourteen tales and sketches in "Putnam"s and "Harper"s magazines. In 1856 a selection of the short fiction, including "Bartleby, the Scrivener" and "Benito Cereno", was published as "The Piazza Tales". The title story was especially written for the collection as an introductory story. One of the magazine pieces was a serialized book, "Israel Potter", the narrative of a Revolutionary War veteran.
On 2 March 1855 Frances (Fanny) was born, the Melvilles' fourth child. In this period "Israel Potter" appeared as a book.
In late 1856 he made a six-month Grand Tour of the British Isles and the Mediterranean. While in England, he spent three days with Hawthorne, who had taken an embassy position there. At the seaside village of Southport, amid the sand dunes where they had stopped to smoke cigars, they had a conversation which Hawthorne later described in his journal:
Melville, as he always does, began to reason of Providence and futurity, and of everything that lies beyond human ken, and informed me that he 'pretty much made up his mind to be annihilated'; but still he does not seem to rest in that anticipation; and, I think, will never rest until he gets hold of a definite belief. It is strange how he persists—and has persisted ever since I knew him, and probably long before—in wandering to-and-fro over these deserts, as dismal and monotonous as the sand hills amid which we were sitting. He can neither believe, nor be comfortable in his unbelief; and he is too honest and courageous not to try to do one or the other. If he were a religious man, he would be one of the most truly religious and reverential; he has a very high and noble nature, and better worth immortality than most of us.
Melville's subsequent visit to the Holy Land inspired his epic poem "Clarel."
On April 1, 1857, Melville published his last full-length novel, "The Confidence-Man". This novel, subtitled "His Masquerade", has won general acclaim in modern times as a complex and mysterious exploration of issues of fraud and honesty, identity and masquerade. But, when it was published, it received reviews ranging from the bewildered to the denunciatory.
1857–1876: Poet.
To repair his faltering finances, Melville was advised by friends to enter what was, for others, the lucrative field of lecturing. From 1857 to 1860, he spoke at lyceums, chiefly on Roman statuary and sightseeing in Rome. Melville's lectures, which mocked the pseudo-intellectualism of lyceum culture, were panned by contemporary audiences.
Turning to poetry, he submitted a collection of verse to a publisher in 1860, but it was not accepted. In 1863 he and his wife resettled in New York City with their four children. As his professional fortunes waned, Melville had difficulties at home. Elizabeth's relatives repeatedly urged her to leave him under the belief that he may have been insane, but she refused.
After the end of the American Civil War, he published "Battle Pieces and Aspects of the War" (1866), a collection of over 70 poems that has been described as "a polyphonic verse journal of the conflict." It was generally ignored by reviewers, who gave him at best patronizingly favorable reviews. The volume did not sell well; of the Harper & Bros. printing of 1200 copies, only 525 had been sold ten years later. Uneven as a collection of individual poems, "its achievement lies in the interplay of voices and moods throughout which Melville patterns a shared historical experience into formative myth."
In 1866 Melville's wife and her relatives used their influence to obtain a position for him as customs inspector for the City of New York (a humble but adequately paying appointment). He held the post for 19 years and won the reputation of being the only honest employee in a notoriously corrupt institution. In 1867 his oldest son Malcolm shot himself, perhaps accidentally, and died at home.
But from 1866, his professional writing career can be said to have come to an end yet he remained dedicated to his writing. Melville devoted years to "his autumnal masterpiece," an 18,000-line epic poem entitled "Clarel: A Poem and a Pilgrimage", inspired by his 1856 trip to the Holy Land. His uncle, Peter Gansevoort, by a bequest, paid for the publication of the massive epic in 1876. The epic-length verse-narrative about a student's spiritual pilgrimage to the Holy Land, was considered quite obscure, even in his own time. Among the longest single poems in American literature, the book had an initial printing of 350 copies, but sales failed miserably, and the unsold copies were burned when Melville was unable to afford to buy them at cost. The critic Lewis Mumford found a copy of the poem in the New York Public Library in 1925 "with its pages uncut"—in other words, it had sat there unread for 50 years.
"Clarel" is a narrative in 18,000 verse lines, featuring a young American student of divinity as the title character. He travels to Jerusalem to renew his faith. One of the central characters, Rolfe, is similar to Melville in his younger days, a seeker and adventurer. Scholars also agree that the reclusive Vine is based on Hawthorne, who had died twelve years before.
1877–1891: Final years.
While Melville had his steady customs job, he no longer showed signs of depression, which recurred after the death of his second son. On 23 February 1886, Stanwix Melville died in San Francisco. Melville retired in 1886, after several of his wife's relatives died and left the couple legacies which Mrs. Melville administered with skill and good fortune.
As English readers, pursuing the vogue for sea stories represented by such writers as G. A. Henty, rediscovered Melville's novels in the late nineteenth century, the author had a modest revival of popularity in England, though not in the United States. He wrote a series of poems, with prose head notes, inspired by his early experiences at sea. He published them in two collections, each issued in a tiny edition of 25 copies for his relatives and friends. Of these, scholar Robert Milder calls "John Marr and Other Poems" (1888), "the finest of his late verse collections," the other privately printed volume is "Timoleon" (1891).
Intrigued by one of these poems, he began to rework the headnote, expanding it first as a short story and eventually as a novella. He worked on it on and off for several years, but when he died in September 1891, the piece was unfinished. His widow Elizabeth added notes and edited it, but the manuscript was not discovered until 1919, by Raymond Weaver, his first biographer. He worked at transcribing and editing a full text, which he published in 1924 as "Billy Budd, Sailor." It was an immediate critical success in England and soon one in the United States. The authoritative version was published in 1962, after two scholars studied the papers for several years.
Death.
Melville died at his home in New York City early on the morning of September 28, 1891, at age 72. The doctor listed "cardiac dilation" on the death certificate. He was interred in the Woodlawn Cemetery in The Bronx, New York. A common story recounts that his "New York Times" obituary called him ""Henry" Melville", implying that he was unknown and unappreciated at his time of death, but the story is not true. A later article was published on October 6 in the same paper, referring to him as "the late Hiram Melville", but this appears to have been a typesetting error.
Writing style.
Melville's writing style shows enormous changes throughout the years as well as consistencies. As early a juvenile piece as "Fragments from a Writing Desk" from 1839, scholar Sealts points out, already shows "a number of elements that anticipate Melville's later writing, especially his characteristic habit of abundant literary allusion." "Typee" and "Omoo" were documentary adventures that called for a division of the narrative in short chapters. Such compact organization bears the risk of fragmentation when applied to a lengthy work such as "Mardi", but with "Redburn" and "White Jacket" Melville had turned the short chapter into an instrument of form and concentration. A number of chapters of "Moby-Dick" are no longer than two pages in standard editions, and an extreme example is Chapter 122, consisting of a single paragraph of 36 words (including the thrice-repeated "Um, um, um.") The skillful handling of chapters in "Moby-Dick" is one of the fullest developed Melvillean signatures, and is a measure of "his manner of mastery as a writer," Individual chapters have become "a touchstone for appreciation of Melville's art and for explanation" of his themes. In contrast, the chapters in "Pierre", called Books, are divided into short numbered sections, seemingly an "odd formal compromise" between Melville's natural length and his purpose to write a regular romance that called for longer chapters. As satirical elements were introduced, the chapter arrangement restores "some degree of organization and pace from the chaos." The usual chapter unit then reappears for "Israel Potter", "The Confidence-Man" and even "Clarel", but only becomes "a vital part in the whole creative achievement" again in the juxtaposition of accents and of topics in "Billy Budd".
Melville's early works were "increasingly baroque" in style, and with "Moby-Dick" Melville's vocabulary had grown superabundant. Bezanson calls it an "immensely varied style." Three characteristic uses of language can be recognized. First, the exaggerated repetition of words, as in the series "pitiable," "pity," "pitied," and "piteous" (Ch. 81, "The Pequod Meets the Virgin"). A second typical device is the use of unusual adjective-noun combinations, as in "concentrating brow" and "immaculate manliness" (Ch. 26, "Knights and Squires"). A third characteristic is the presence of a participial modifier to emphasize and to reinforce the already established expectations of the reader, as the words "preluding" and "foreshadowing" ("so still and subdued and yet somehow preluding was all the scene...", "In this foreshadowing interval...").
After the hyphenated compounds of "Pierre", words and phrases became less exploratory and less provocative. Instead of providing a lead "into possible meanings and openings-out of the material at hand," the style now served "to crystallize governing impressions." The diction no longer attracted attention to itself, except as an effort at exact definition. The language reflects a "controlling intelligence, of right judgment and completed understanding." The sense of free inquiry and exploration which infused his earlier writing and accounted for its "rare force and expansiveness," tended to give way to "static enumeration." Added "seriousness of consideration" came at the cost of losing "pace and momentum." The verbal music and kinetic energy of "Moby-Dick" seem "relatively muted, even withheld" in the later works.
Melville's paragraphing, in his best work, is the virtuous result of "compactness of form and free assembling of unanticipated further data," such as when the mysterious sperm whale is compared with Exodus's invisibility of God's face in the final paragraph of Chapter 86 ("The Tail"). Over time Melville's paragraphs became shorter as his sentences grew longer, until he arrived at the "one-sentence paragraphing characteristic of his later prose." The opening chapter of "The Confidence-Man" counts fifteen paragraphs, seven of which consist of only one, elaborate, sentence, and four that have only two sentences. This contributes in large part, Berthoff says, to the "remarkable narrative economy" of "Billy Budd".
In Nathalia Wright's view, Melville's sentences generally have a looseness of structure, easy to use for devices as catalogue and allusion, parallel and refrain, proverb and allegory. The length of his clauses may vary greatly, but the "torterous" writing in "Pierre" and "The Confidence-Man" is there to convey feeling, not thought. Unlike Henry James, who was an innovator of sentence ordering to render the subtlest nuances in thought, Melville made few such innovations. His domain is the mainstream of English prose, with its rhythm and simplicity influenced by the King James Bible.
Another important characteristic of Melville's writing style is in its echoes and overtones. Melville's imitation of certain distinct styles is responsible for this. His three most important sources, in order, are the Bible, Shakespeare, and Milton. Scholar Nathalia Wright has identified three stylistic categories of Biblical influence. Actual quotation from any of the sources is slight; only one sixths of his Biblical allusions can be qualified as such.
First, far more unmarked than acknowledged quotations occur, some favorites even numerous times throughout his whole body of work, taking on the nature of refrains. Examples of this idiom are the injunctions to be as wise as serpents and as harmless as doves, death on a pale horse, the man of sorrows, the many mansions of heaven; proverbs as the hairs on our heads are numbered, pride goes before a fall, the wages of sin is death; adverbs and pronouns as verily, whoso, forasmuch as; phrases as come to pass, children's children, the fat of the land, vanity of vanities, outer darkness, the apple of his eye, Ancient of Days, the rose of Sharon.
Second, there are paraphrases of individual and combined verses. Redburn's "Thou shalt not lay stripes upon these Roman citizens" makes use of language of the Ten Commandments in Ex.20, and Pierre's inquiry of Lucy: "Loveth she me with the love past all understanding?" combines John 21:15–17 and Philippians 4:7
Third, certain Hebraisms are used, such as a succession of genitives ("all the waves of the billows of the seas of the boisterous mob"), the cognate accusative ("I dreamed a dream," "Liverpool was created with the Creation"), and the parallel ("Closer home does it go than a rammer; and fighting with steel is a play without ever an interlude"). 
Melville's style seamlessly flows over into theme, because all these borrowings have an artistic purpose, which is to suggest an appearance "larger and more significant than life" for characters and themes that are "essentially simple and mundane." The allusions suggest that beyond the world of appearances another world exists, one that "exerts influence upon this world, and in which ultimate truth resides." Moreover, the ancient background thus suggested for Melville's narratives – ancient allusions being next in number to the Biblical ones – invests them "with a certain timeless quality."
A passage from "Redburn" (see quotebox) shows how all these different ways of alluding interlock and result in a fabric texture of Biblical language, though there is very little direct quotation.
The other world beyond this, which was longed for by the devout before Columbus' time, was found in the New; and the deep-sea land, that first struck these soundings, brought up the soil of Earth's Paradise. Not a Paradise then, or now; but to be made so at God's good pleasure, and in the fulness and mellowness of time. The seed is sown, and the harvest must come; and our children's children, on the world's jubilee morning, shall all go with their sickles to the reaping. Then shall the curse of Babel be revoked, a new Pentecost come, and the language they shall speak shall be the language of Britain. Frenchmen, and Danes, and Scots; and the dwellers on the shores of the Mediterranean, and in the regions round about; Italians, and Indians, and Moors; there shall appear unto them cloven tongues as of fire.
— The American melting pot described in Redburn's Biblical language, with Nathalia Wright's glosses.
In addition to this, Melville successfully imitates three Biblical strains: he sustains the apocalyptic for a whole chapter of Mardi; the prophetic strain is a presence in "Moby-Dick", most notably in Father Mapple's sermon; and the tradition of the Psalms is imitated at length in "The Confidence-Man".
An edition of Shakespeare's works came in Melville's possession in 1849, and his reading of it greatly influenced the style of his next book, "Moby-Dick" (1851). The language of "Shakespeare went far beyond all other influences" upon the book, in that it made Melville discover his own full strength. On almost every page debts to Shakespeare can be discovered. The "mere sounds, full of Leviathanism, but signifying nothing" at the end of "Cetology" (Ch.32) echoe the famous phrase in Macbeth: "Told by an idiot, full of sound and fury, Signifying nothing." Ahab's first extended speech to the crew, in the "Quarter-Deck" (Ch.36), is "virtually blank verse" and so is Ahab's sololoquy at the beginning of "Sunset" (Ch.37):'I leave a white and turbid wake;/ Pale waters, paler cheeks, where'er I sail./ The envious billows sidelong swell to whelm/ My track; let them; but first I pass. ' Through Shakespeare, Melville infused "Moby-Dick" with a power of expression he had not previously possessed. Reading Shakespeare had been "a catalytic agent" for Melville, one that transformed his writing "from limited reporting to the expression of profound natural forces." The extent to which Melville assimilated Shakespeare is evident in the description of Ahab, which ends in language "that suggests Shakespeare's but is not an imitation of it: 'Oh, Ahab! what shall be grand in thee, it must needs be plucked from the skies and dived for in the deep, and featured in the unbodied air!' The imaginative richness of the final phrase seems particularly Shakespearean, "but its two key words appear only once each in the plays...and to neither of these usages is Melville indebted for his fresh combination." Melville's diction depended upon no source, and his prose is not based on anybody else's verse but on "a sense of speech rhythm." Melville's mastering of Shakespeare supplied him with verbal resources that enabled him "to make language itself dramatic" through three essential techniques:
Melville's style seamlessly flows over into theme, because all these borrowings have an artistic purpose, which is to suggest an appearance "larger and more significant than life" for characters and themes that are "essentially simple and mundane." The allusions suggest that beyond the world of appearances another world exists, one that "exerts influence upon this world, and in which ultimate truth resides." Moreover, the ancient background thus suggested for Melville's narratives – ancient allusions being next in number to the Biblical ones – invests them "with a certain timeless quality."
Critical response.
Contemporary criticism.
Melville was not financially successful as a writer, having earned just over $10,000 for his writing during his lifetime. After his success with travelogues based on voyages to the South Seas and stories based on misadventures in the merchant marine and navy, Melville's popularity declined dramatically. By 1876, all of his books were out of print. In the later years of his life and during the years after his death, he was recognized, if at all, as a minor figure in American literature.
Melville revival and Melville studies.
The "Melville Revival" of the late 1910s and 1920s brought about a reassessment of his work. The starting point was Raymond Weaver's 1921 biography "Herman Melville: Mariner and Mystic" and his 1924 publication of Melville's unfinished manuscript, "Billy Budd", which he discovered among papers shown to him by Melville's granddaughter. Other works that helped fan the flames were Carl Van Doren's "The American Novel" (1921), D. H. Lawrence's "Studies in Classic American Literature" (1923), Carl Van Vechten's essay in "The Double Dealer" (1922), and Lewis Mumford's biography, "Herman Melville: A Study of His Life and Vision" (1929).
Starting in the mid-1930s, the Yale University scholar Stanley Williams supervised more than a dozen dissertations on Melville which were eventually published as books. Where the first wave of Melville scholars focused on psychology, his students were prominent in establishing Melville Studies as an academic field concerned with texts and manuscripts, tracing Melville's influences and borrowings (even plagiarism), and exploring archives and local publications. Jay Leyda, known for his work in film, spent more than a decade gathering documents and records for the day by day "Melville Log" (1951). Led by Leyda, the second phase of the Melville Revival emphasized research and tended to feel that Weaver, Murray, and Mumford favored Freudian interpretations which read Melville's fiction too literally as autobiography, exaggerated Melville’s suffering in the family, mistakenly inferred a homosexual attachment to Hawthorne, and saw a tragic withdrawal after the cold critical reception for his last prose works rather than a turn to new forms. Other post-war studies, however, continued the broad imaginative and interpretive style. Charles Olson's "Call Me Ishmael" (1947) presented Ahab as a Shakespearean character, and Newton Arvin's critical biography, "Herman Melville" won the nonfiction National Book Award.
In 1951 "Billy Budd" was adapted as an award-winning play on Broadway, and premiered as an opera by Benjamin Britten, with a libretto on which the author E. M. Forster collaborated. In 1962 Peter Ustinov wrote, directed and produced a film based on the stage version, starring the young Terence Stamp and for which he took the role of Captain Vere. All these works brought more attention to Melville.
In the 1960s, Northwestern University Press, in alliance with the Newberry Library and the Modern Language Association, launched a project to edit and published reliable critical texts of Melville's complete works, including unpublished poems, journals, and correspondence. The aim of the editors was to present a text "as close as possible to the author's intention as surviving evidence permits". The volumes have extensive appendices, including textual variants from each of the editions published in Melville's lifetime, an historical note on the publishing history and critical reception, and related documents. In many cases, it was not possible to establish a "definitive text", but the edition supplies all evidence available at the time. Since the texts were prepared with financial support from the United States Department of Education, no royalties are charged, and they have been widely reprinted.
The Melville Society.
In 1945, The Melville Society was founded, a non-profit organisation dedicated to the study of Melville's life and works. Between 1969 and 2003 it published 125 issues of Melville Society Extracts, which are now freely available on the society's website. Since 1999 it publishes "Leviathan: A Journal of Melville Studies", currently three issues a year, published by Johns Hopkins university Press.
Melville's poetry.
Melville did not publish poetry until late in life and his reputation as a poet was not high until late in the 20th century.
Melville, says recent literary critic Lawrence Buell, “is justly said to be nineteenth-century America’s leading poet after Whitman and Dickinson, yet his poetry remains largely unread even by many Melvillians.” True, Buell concedes, even more than most Victorian poets, Melville turned to poetry as an “instrument of meditation rather than for the sake of melody or linguistic play.” It is also true that he turned from fiction to poetry late in life. Yet he wrote twice as much poetry as Dickinson and probably as many lines as Whitman, and he wrote distinguished poetry for a quarter of a century, twice as long as his career publishing prose narratives. The three novels of the 1850s which Melville worked on most seriously to present his philosophical explorations, "Moby-Dick", "Pierre", and "The Confidence Man", seem to make the step to philosophical poetry a natural one rather than simply a consequence of commercial failure.
In 2000 the Melville scholar Elizabeth Renker wrote "a sea change in the reception of the poems is incipient". Some critics now place him as the first modernist poet in the United States; others assert that his work more strongly suggests what today would be a postmodern view. Henry Chapin wrote in an introduction to "John Marr and Other Sailors (1888)", a collection of Melville's late poetry, "Melville's loveable freshness of personality is everywhere in evidence, in the voice of a true poet". The poet and novelist Robert Penn Warren was a leading champion of Melville as a great American poet. Warren issued a selection of Melville's poetry prefaced by an admiring critical essay. The poetry critic Helen Vendler remarked of "Clarel" : "What it cost Melville to write this poem makes us pause, reading it. Alone, it is enough to win him, as a poet, what he called 'the belated funeral flower of fame'".
Gender studies revisionism.
Although not the primary focus of Melville scholarship, there has been an emerging interest in the role of gender and sexuality in some of his writings. Some critics, particularly those interested in gender studies, have explored the male-dominant social structures in Melville's fiction. For example, Alvin Sandberg claimed that the short story "The Paradise of Bachelors and the Tartarus of Maids" offers "an exploration of impotency, a portrayal of a man retreating to an all-male childhood to avoid confrontation with sexual manhood", from which the narrator engages in "congenial" digressions in heterogeneity. In line with this view, Warren Rosenberg argues the homosocial "Paradise of Bachelors" is shown to be "superficial and sterile".
David Harley Serlin observes in the second half of Melville's diptych, "The Tartarus of Maids", the narrator gives voice to the oppressed women he observes: 
As other scholars have noted, the "slave" image here has two clear connotations. One describes the exploitation of the women's physical labor, and the other describes the exploitation of the women's reproductive organs. Of course, as models of women's oppression, the two are clearly intertwined.
In the end he says that the narrator is never fully able to come to terms with the contrasting masculine and feminine modalities.
Issues of sexuality have been observed in other works as well. Rosenberg notes Taji, in "Mardi", and the protagonist in "Pierre" "think they are saving young 'maidens in distress' (Yillah and Isabel) out of the purest of reasons but both are also conscious of a lurking sexual motive". When Taji kills the old priest holding Yillah captive, he says, 
[R]emorse smote me hard; and like lightning I asked myself whether the death deed I had done was sprung of virtuous motive, the rescuing of a captive from thrall, or whether beneath the pretense I had engaged in this fatal affray for some other selfish purpose, the companionship of a beautiful maid.
In "Pierre," the motive of the protagonist's sacrifice for Isabel is admitted: "womanly beauty and not womanly ugliness invited him to champion the right". Rosenberg argues, 
This awareness of a double motive haunts both books and ultimately destroys their protagonists who would not fully acknowledge the dark underside of their idealism. The epistemological quest and the transcendental quest for love and belief are consequently sullied by the erotic.
Rosenberg says that Melville fully explores the theme of sexuality in his major epic poem, "Clarel." When the narrator is separated from Ruth, with whom he has fallen in love, he is free to explore other sexual (and religious) possibilities before deciding at the end of the poem to participate in the ritualistic order marriage represents. In the course of the poem, "he considers every form of sexual orientation – celibacy, homosexuality, hedonism, and heterosexuality – raising the same kinds of questions as when he considers Islam or Democracy".
Some passages and sections of Melville's works demonstrate his willingness to address all forms of sexuality, including the homoerotic, in his works. Commonly noted examples from "Moby-Dick" are the "marriage bed" episode involving Ishmael and Queequeg, which is interpreted as male bonding; and the "Squeeze of the Hand" chapter, describing the camaraderie of sailors' extracting spermaceti from a dead whale. Rosenberg notes that critics say that "Ahab's pursuit of the whale, which they suggest can be associated with the feminine in its shape, mystery, and in its naturalness, represents the ultimate fusion of the epistemological and sexual quest." In addition, he notes that Billy Budd's physical attractiveness is described in quasi-feminine terms: "As the Handsome Sailor, Billy Budd's position aboard the seventy-four was something analogous to that of a rustic beauty transplanted from the provinces and brought into competition with the highborn dames of the court."
Law and literature.
In recent years, "Billy Budd" has become a central text in the field of legal scholarship known as law and literature. In the novel, Billy, a handsome and popular young sailor is impressed from the merchant vessel "Rights of Man" to serve aboard H.M.S. "Bellipotent" in the late 1790s, during the war between Revolutionary France and Great Britain. He excites the enmity and hatred of the ship's master-at-arms, John Claggart. Claggart accuses Billy of phony charges of mutiny and other crimes, and the Captain, the Honorable Edward Fairfax Vere, brings them together for an informal inquiry. At this encounter, Billy strikes Claggart in frustration, as his stammer prevents him from speaking. The blow catches Claggart squarely on the forehead and, after a gasp or two, the master-at-arms dies.
Vere immediately convenes a court-martial, at which, after serving as sole witness and as Billy's "de facto" counsel, Vere urges the court to convict and sentence Billy to death. The trial is recounted in chapter 21, the longest chapter in the book. It has become the focus of scholarly controversy; was Captain Vere a good man trapped by bad law, or did he deliberately distort and misrepresent the applicable law to condemn Billy to death?
Themes.
As early as 1839, in the juvenile sketch "Fragments from a Writing Desk" a problem occurs which would reappear in the short stories "Bartleby" (1853) and "Benito Cereno" (1855): the impossibility to find common ground for mutual communication. The sketch centers on the protagonist and a mute lady, leading scholar Sealts to observe: "Melville's deep concern with expression and communication evidently began early in his career." According to scholar Nathalia Wright, Melville's characters are all preoccupied by the same intense, superhuman and eternal quest for "the absolute amidst its relative manifestations." There can be no doubt that this is the essence of every segment of Melville's whole body of work: "All Melville's plots describe this pursuit, and all his themes represent the delicate and shifting relationship between its truth and its illusion." It is not clear, however, what the moral and metaphysical implications of this quest are, because Melville did not distinguish between these two aspects. Throughout his life Melville struggled with and gave shape to the same set of epistemological doubts and the metaphysical issues these doubts engendered. An obsession for the limits of knowledge led to the question of God's existence and nature, the indifference of the universe and the problem of evil.

</doc>
<doc id="13624" url="http://en.wikipedia.org/wiki?curid=13624" title="High fidelity">
High fidelity

High fidelity—or hi-fi or hifi—reproduction is a term used by home stereo listeners and home audio enthusiasts (audiophiles) to refer to high-quality reproduction of sound to distinguish it from the poorer quality sound produced by inexpensive audio equipment, or the inferior quality of sound reproduction characteristic of recordings made until the late 1940s. Ideally, high-fidelity equipment has minimal amounts of noise and distortion and an accurate frequency response.
History.
Bell Laboratories began experimenting with wider-range recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the labs in New Jersey. Some multi-track recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and Twentieth Century Fox (as early as 1941). RCA Victor began recording performances by several orchestras on optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs.
Also during the 1930s Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestra—that would achieve high fidelity to the original sound.
Beginning in 1948, several innovations created the conditions for a major improvement of home-audio quality:
In the 1950s, audio manufacturers employed the phrase "high fidelity" as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted "high fidelity" as fancy and expensive equipment, many found the difference in quality between "hi-fi" and the then standard AM radios and 78 rpm records readily apparent and bought 33⅓ LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system); and high-fidelity phonographs. Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts assembled their own loudspeaker systems. In the 1950s, "hi-fi" became a generic term, to some extent displacing "phonograph" and "record player".
In the late 1950s and early 1960s, the development of the Westrex single-groove stereophonic record cutterhead led to the next wave of home-audio improvement, and in common parlance, "stereo" displaced "hi-fi". Records were now played on "a stereo". In the world of the audiophile, however, "high fidelity" continued and continues to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is most widely regarded as "The Golden Age of Hi-Fi", when tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state equipment was introduced to the market, subsequently replacing tube equipment as mainstream.
A popular type of system for reproducing music beginning in the 1970s was the integrated music centre—which combined phonograph, radio tuner, tape player, preamp, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction.
Listening tests.
Blind tests refer to experiments where researchers can see the components under test, but not individuals undergoing the experiments. In a double-blind experiment, neither the individuals nor the researchers know who belongs to the control group and the experimental group. Only after all the data has been recorded (and in some cases, analyzed) do the researchers learn which individuals are which. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample "A", the reference, and sample "B", an alternative), and one unknown sample "X," for three samples total. "X" is randomly selected from "A" and "B", and the subject identifies "X" as being either "A" or "B". Although there is no way to prove that a certain lossy methodology is transparent, a properly conducted double-blind test can prove that a lossy method is "not" transparent.
Scientific double-blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these double-blind tests is not accepted by some "audiophile" magazines such as "Stereophile" and "The Absolute Sound" in their evaluations of audio equipment. John Atkinson, current editor of "Stereophile", stated (in a 2005 July editorial named "Blind Tests & Bus Stops") that he once purchased a solid-state amplifier, the Quad 405, in 1978 after blind tests, but came to realize months later that "the magic was gone" until he replaced it with a tube amp. Robert Harley of "The Absolute Sound" wrote, in a 2008 editorial (on Issue 183), that: "...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon."
Doug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009. He stated: "Blind tests are at the core of the decades’ worth of research into loudspeaker design done at Canada’s National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so." Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Many audio professionals like Sean Olive of Harman International share this view.
Semblance of realism.
Stereophonic sound provided a partial solution to the problem of creating some semblance of the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves slightly to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound but, again, the technology at that time was insufficient for the task. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became affordable, and many consumers were willing to tolerate the six to eight channels required in a home theater. The advances made in signal processors to synthesize an approximation of a good concert hall can now provide a somewhat more realistic illusion of listening in a concert hall.
In addition to spatial realism, the playback of music must be subjectively free from noise to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range, which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall.
Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz.
 Most adults can't hear higher than 15 kHz. CDs are capable of reproducing frequencies as low as 10 Hz and as high as 22.05 kHz, making them adequate for reproducing the frequency range that most humans can hear.
The equipment must also provide no noticeable distortion of the signal or emphasis or de-emphasis of any frequency in this frequency range.
Modularity.
"Integrated", "mini", or "lifestyle" systems, also known as music centres or minisystems, contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from "separates" (or "components"), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.
For slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an "integrated amplifier"; with a tuner, it is a "receiver". A monophonic power amplifier, which is called a "monoblock", is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.
This modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system.
Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Some of the disadvantages of this approach are increased cost, complexity, and space required for the components.
Modern equipment.
Modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support.
Another modern component is the "music server" consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system.

</doc>
<doc id="13625" url="http://en.wikipedia.org/wiki?curid=13625" title="Holden">
Holden

GM Holden Ltd, commonly designated Holden, is an Australian automaker that operates in Australasia and is headquartered in Port Melbourne, Victoria. The company was founded in 1856 as a saddlery manufacturer in South Australia. In 1908 it moved into the automotive field, before becoming a subsidiary of the United States-based General Motors (GM) in 1931. After becoming a subsidiary of GM, the company was named General Motors-Holden's Ltd, becoming Holden Ltd in 1998—the current name was adopted in 2005.
Holden is responsible for GM's vehicle operations in Australasia, and on their behalf, held partial ownership of GM Daewoo in South Korea between 2002 and 2009. Holden has offered a broad range of locally produced vehicles, supplemented by imported GM models. Holden has offered the following badge engineered models in sharing arrangements: Chevrolet, Isuzu, Nissan, Opel, Suzuki, Toyota and Vauxhall Motors. As of 2013, the vehicle lineup consists of models from GM Korea, GM Thailand, GM in the US, and self-developed Commodore, Caprice, and Ute. Holden also distributed the European Opel brand in Australia in 2012 until the brand's Australian demise in mid-2013.
All Australian-built Holden vehicles are manufactured at Elizabeth, South Australia, and engines are produced at the Fishermans Bend plant in Port Melbourne, Victoria. Historically, production or assembly plants were operated in all mainland states of Australia, with GM's New Zealand subsidiary Holden New Zealand operating a plant until 1990. The consolidation of car production at Elizabeth was completed in 1988, but some assembly operations continued at Dandenong until 1994. 
Although Holden's involvement in exports has fluctuated since the 1950s, the declining sales of large cars in Australia has led the company to look to international markets to increase profitability. Holden announced on 11 December 2013 that local manufacturing would cease by the end of 2017 and that the company would continue to have a large and ongoing presence in Australia importing and selling cars as national sales company. Holden will retain their design center, but with reduced staffing. In the last few years, Holden incurred losses due to the strong Australian dollar, with government grants being reduced in future. In May 2014 GM reversed their decision to abandon the Lang Lang Proving Ground and have decided to keep it as part of their engineering capability in Australia.
History of the company.
Early history.
In 1852, James Alexander Holden emigrated to South Australia from Walsall, England and in 1856 established "J.A. Holden & Co", a saddlery business in Adelaide. In 1885 German-born Henry Frederick Frost joined the business as a junior partner and J.A. Holden & Co became "Holden & Frost Ltd". Edward Holden, James' grandson, joined the firm in 1905 with an interest in automobiles. From there, the firm evolved through various partnerships and, in 1908, Holden & Frost moved into the business of minor repairs to car upholstery. The company began to produce complete motorcycle sidecar bodies in 1913, and Edward experimented with fitting bodies to different types of carriages. After 1917, wartime trade restrictions led the company to start full-scale production of vehicle body shells. J.A. Holden founded a new company in 1919, "Holden's Motor Body Builders Ltd" (HMBB) specialising in car bodies and utilising a facility on King William Street in Adelaide. 
By 1923, HMBB were producing 12,000 units per year. During this time, HMBB was the first company to assemble bodies for Ford Australia until their Geelong, plant was completed. From 1924, HMBB became the exclusive supplier of car bodies for GM in Australia, with manufacturing taking place at the new Woodville, South Australia plant. These bodies were made to suit a number of chassis imported from manufacturers such as Chevrolet and Dodge. In 1926 General Motors (Australia) was established with assembly plants at Newstead, Queensland; Marrickville, New South Wales; City Road, Melbourne, Victoria; Birkenhead, South Australia; and Cottesloe, Western Australia utilizing bodies produced by Holden Motor Body Builders and imported complete knock down (CKD) chassis. The Great Depression led to a substantial downturn in production by Holden, from 34,000 units annually in 1930 to just 1,651 units one year later. In 1931 General Motors purchased Holden Motor Body Builders and merged it with General Motors (Australia) Pty Ltd to form General Motors-Holden's Ltd (GM-H). 
Throughout the 1920s Holden also supplied tramcars to Melbourne and Metropolitan Tramways Board. Several have been preserved in both Australia and New Zealand.
1940s.
Holden's second full-scale car factory, located in Fishermans Bend (Port Melbourne), was completed in 1936, with construction beginning in 1939 on a new plant in Pagewood, New South Wales. However, World War II delayed car production with efforts shifted to the construction of vehicle bodies, field guns, aircraft and engines. Before the war ended, the Australian Government took steps to encourage an Australian automotive industry. Both GM and Ford provided studies to the Australian Government outlining the production of the first Australian-designed car. Ford's proposal was the government's first choice, but required substantial financial assistance. GM's study was ultimately chosen because of its low level of government intervention. After the war, Holden returned to producing vehicle bodies, this time for Buick, Chevrolet, Pontiac and Vauxhall. The Oldsmobile Ace was also produced from 1946 to 1948.
From here, Holden continued to pursue the goal of producing an Australian car. This involved compromise with GM, as Holden's managing director, Laurence Hartnett, favoured development of a local design, while GM preferred to see an American design as the basis for "Australia's Own Car". In the end, the design was based on a previously rejected post-war Chevrolet proposal. The Holden was launched in 1948, creating long waiting lists extending through 1949 and beyond. The name "Holden" was chosen in honour of Sir Edward Holden, the company's first chairman and grandson of J.A. Holden. Other names considered were "GeM", "Austral", "Melba", "Woomerah", "Boomerang", "Emu" and "Canbra", a phonetic spelling of Canberra. Although officially designated "48-215", the car was marketed simply as the "Holden". The unofficial usage of the name "FX" originated within Holden, referring to the updated suspension on the 48-215 of 1953.
1950s.
During the 1950s, Holden dominated the Australian car market. GM invested heavily in production capacity, which allowed the company to meet increased post-war demand for motor cars. Less expensive four-cylinder cars did not offer Holden's ability to deal with rugged rural areas. 48-215 sedans were produced in parallel with the 50-2106 coupé utility from 1951; the latter was known colloquially as the "ute" and became ubiquitous in Australian rural areas as the workhorse of choice. Production of both the utility and sedan continued with minor changes until 1953, when they were replaced by the facelifted FJ model, introducing a third panel van body style. The FJ was the first major change to the Holden since its 1948 introduction. Over time it gained iconic status and remains one of Australia's most recognisable automotive symbols. A new horizontally slatted grille dominated the front-end of the FJ, which received various other trim and minor mechanical revisions. In 1954 Holden began exporting the FJ to New Zealand. Although little changed from the 48-215, marketing campaigns and price cuts kept FJ sales steady until a completely redesigned model was launched. At the 2005 Australian International Motor Show in Sydney, Holden paid homage to the FJ with the Efijy concept car.
Holden's next model, the FE, launched in 1956; offered in a new station wagon body style dubbed "Station Sedan" in the company's sales literature. In the same year Holden commenced exports to Malaya, Thailand and North Borneo. Strong sales continued in Australia, and Holden achieved a market share of more than 50 percent in 1958 with the revised FC model. This was the first Holden to be tested on the new "Holden Proving Ground" based in Lang Lang, Victoria. 1957 saw Holden's export markets grow to 17 countries, with new additions including Indonesia, Hong Kong, Singapore, Fiji, Sudan, the East Africa region and South Africa. The opening of the Dandenong, Victoria, production facility in 1956 brought further jobs; by 1959 Holden employed 19,000 workers country-wide. In 1959 complete knock down assembly began in South Africa and Indonesia.
1960s.
In 1960, Holden introduced its third major new model, the FB. The car's style was inspired by 1950s Chevrolets, with tailfins and a wrap-around windshield with "dog leg" A-pillars. By the time it was introduced, many considered the appearance dated. Much of the motoring industry at the time noted that the adopted style did not translate well to the more compact Holden. The FB became the first Holden that was adapted for left-hand-drive markets, enhancing its export potential, and as such was exported to New Caledonia, New Hebrides, the Philippines, and Hawaii.
In 1960, Ford unveiled the new Falcon in Australia, only months after its introduction in the United States. To Holden's advantage, the Falcon was not durable, particularly in the front suspension, making it ill-suited for Australian conditions. In response to the Falcon, Holden introduced the facelifted EK series in 1961; the new model featured two-tone paintwork and optional "Hydramatic" automatic transmission. A restyled EJ series came in 1962, debuting the new luxury oriented Premier model. The EH update came a year later bringing the new "Red" motor, providing better performance than the previous "Grey" motor. The HD series of 1965 saw the introduction of the "Powerglide" automatic transmission. At the same time, an "X2" performance option with a more powerful version of the 179 cuin six-cylinder engine was made available. In 1966, the HR was introduced, including changes in the form of new front and rear styling and higher-capacity engines. More significantly, the HR fitted standard front seat belts; Holden thus became the first Australian automaker to provide the safety device as standard equipment across all models. This coincided with the completion of the production plant in Acacia Ridge, Queensland. By 1963, Holden was exporting cars to Africa, the Middle East, South-East Asia, the Pacific Islands, and the Caribbean.
Holden began assembling the compact HA series Vauxhall Viva in 1964. This was superseded by the Holden Torana in 1967, a development of the Viva ending Vauxhall production in Australia. Holden offered the LC, a Torana with new styling, in 1969 with the availability of Holden's six-cylinder engine. In the development days, the six-cylinder Torana was reserved for motor racing, but research had shown that there was a business case for such a model. The LC Torana was the first application of Holden's new three-speed "Tri-Matic" automatic transmission. This was the result of Holden's A$16.5 million transformation of the Woodville, South Australia factory for its production.
Holden's association with the manufacture of Chevrolets and Pontiacs ended in 1968, coinciding with the year of Holden's next major new model, the HK . This included Holden's first V8 engine, a Chevrolet engine imported from Canada. Models based on the HK series included an extended-length prestige model, the Brougham, and a two-door coupé, the Monaro. The mainstream Holden Special was rebranded the Kingswood, and the basic fleet model, the Standard, became the Belmont. On 3 March 1969 Alexander Rhea, managing director of General Motors-Holden's at the time, was joined by press photographers and the Federal Minister of Shipping and Transport, Ian Sinclair as the two men drove the two millionth Holden, an HK Brougham off the production line. This came just over half a decade since the one millionth car, an EJ Premier sedan rolled off the Dandenong line on 25 October 1962. Following the Chevrolet V8 fitted to the HK, the first Australian-designed and mass-produced V8, the Holden V8 engine debuted in the Hurricane concept of 1969 before fitment to facelifted HT model. This was available in two capacities: 253 cuin and 308 cuin. Late in HT production, use of the new "Tri-Matic" automatic transmission, first seen in the LC Torana was phased in as "Powerglide" stock was exhausted, but Holden's official line was that the HG of 1971 was the first full-size Holden to receive it.
Despite the arrival of serious competitors—namely, the Ford Falcon, Chrysler Valiant, and Japanese cars—in the 1960s, Holden's locally produced large six- and eight-cylinder cars remained Australia's top-selling vehicles. Sales were boosted by exporting the Kingswood sedan, station wagon, and utility body styles to Indonesia, Trinidad and Tobago, Pakistan, the Philippines, and South Africa in complete knock down form.
1970s.
Holden launched the new HQ series in 1971. At this time, the company was producing all of its passenger cars in Australia, and every model was of Australian design; however, by the end of the decade, Holden was producing cars based on overseas designs. The HQ was thoroughly re-engineered, featuring a perimeter frame and semi-monocoque (unibody) construction. Other firsts included an all-coil suspension and an extended wheelbase for station wagons, while the utilities and panel vans retained the traditional coil/leaf suspension configuration. The series included the new prestige Statesman brand, which also had a longer wheelbase, replacing the Brougham. The Statesman remains noteworthy because it was not marketed as a "Holden", but rather a "Statesman".
The HQ framework led to a new generation of two-door Monaros, and, despite the introduction of the similar sized competitors, the HQ range became the top-selling Holden of all time, with 485,650 units sold in three years. 14,558 units were exported and 72,290 CKD kits were constructed. The HQ series was facelifted in 1974 with the introduction of the HJ, heralding new front panel styling and a revised rear fascia. This new bodywork was to remain, albeit with minor upgrades through the HX and HZ series. Detuned engines adhering to government emission standards were brought in with the HX series, whilst the HZ brought considerably improved road handling and comfort with the introduction of "Radial Tuned Suspension" (RTS). As a result of GM's toying with the Wankel rotary engine, as used by Mazda of Japan, an export agreement was initiated in 1975. This involved Holden exporting with powertrains, HJ, and later, HX series Premiers as the Mazda Roadpacer AP. Mazda then fitted these cars with the "13B" rotary engine and three-speed automatic transmission. Production ended in 1977, after just 840 units sold.
During the 1970s, Holden ran an advertising jingle "", based on the "Baseball, Hot Dogs, Apple Pies and Chevrolet" jingle used by Chevrolet in the United States. Also, development of the Torana continued in with the larger mid-sized LH series released in 1974, offered only as a four-door sedan. The LH Torana was one of the few cars worldwide engineered to occupy four-, six-and eight-cylinder engines. This trend continued until Holden introduced the Sunbird in 1976; essentially the four-cylinder Torana with a new name. Designated LX, both the Sunbird and Torana introduced a three-door hatchback variant. A final UC update appeared in 1978. During its production run, the Torana achieved legendary racing success in Australia, achieving victories at the Mount Panorama Circuit in Bathurst, New South Wales.
In 1975, Holden introduced the compact Gemini, the Australian version of the "T-car", based on the Opel Kadett C. The Gemini was an overseas design developed jointly with Isuzu, GM's Japanese affiliate; and was powered by a 1.6-litre four-cylinder engine. Fast becoming a popular car, the Gemini rapidly attained sales leadership in its class, and the nameplate lived on until 1987.
Holden's most popular car to date, the Commodore, was introduced in 1978 as the VB. The new family car was loosely based on the Opel Rekord E body shell, but with the front from the Opel Senator grafted to accommodate the larger Holden six-cylinder and V8 engines. Initially, the Commodore maintained Holden's sales leadership in Australia. However, some of the compromises resulting from the adoption of a design intended for another market hampered the car's acceptance. In particular, it was narrower than its predecessor and its Falcon rival, making it less comfortable for three rear-seat passengers. With the abandonment of left-hand drive markets, Holden exported almost 100,000 Commodores to markets such as New Zealand, Thailand, Hong Kong, Malaysia, Indonesia, Malta and Singapore.
Holden discontinued the Torana in 1979 and the Sunbird in 1980. After the 1978 introduction of the Commodore, the Torana became the "in-between" car, surrounded by the smaller and more economical Gemini and the larger, more sophisticated Commodore. The closest successor to the Torana was the Camira, released in 1982 as Australia's version of GM's medium-sized "J-car".
1980s.
The 1980s were challenging for Holden and the Australian automotive industry. The Australian Government tried to revive the industry with the Button car plan, which encouraged car makers to focus on producing fewer models at higher, more economical volumes, and to export cars. The decade opened with the shut-down of the Pagewood, New South Wales production plant and introduction of the light commercial Rodeo, sourced from Isuzu in Japan. The Rodeo was available in both two- and four-wheel drive chassis cab models with a choice of petrol and diesel powerplants. The range was updated in 1988 with the TF series, based on the Isuzu TF. Other cars sourced from Isuzu during the 1980s were the four-wheel drive Jackaroo (1981), the Shuttle (1982) van and the Piazza (1986) three-door sports hatchback. The second generation Holden Gemini from 1985 was also based on an Isuzu design, although, its manufacture was undertaken in Australia.
While GM Australia's commercial vehicle range had originally been mostly based on Bedford products, these had gradually been replaced by Isuzu products. This process began in the 1970s and by 1982 Holden's commercial vehicle arm no longer offered any Bedford products.
The new Holden WB commercial vehicles and the Statesman WB limousines were introduced in 1980. However, the designs, based on the HQ and updated HJ, HX and HZ models from the 1970s were less competitive than similar models in Ford's lineup. Thus, Holden abandoned those vehicle classes altogether in 1984. Sales of the Commodore also fell, with the effects of the 1979 energy crisis lessening, and for the first time the Commodore lost ground to the Ford Falcon. Sales in other segments also suffered when competition from Ford intensified, and other Australian manufacturers: Mitsubishi, Nissan and Toyota gained market share. When released in 1982, the Camira initially generated good sales, which later declined because buyers considered the 1.6-litre engine underpowered, and the car's build and ride quality below-average. The Camira lasted just seven years, and contributed to Holden's accumulated losses of over A$500 million by the mid-1980s.
In 1984 Holden introduced the VK Commodore, with significant styling changes from the previous VH. The Commodore was next updated in 1986 as the VL, which had new front and rear styling. Controversially, the VL was powered by the 3.0-litre Nissan "RB30" six-cylinder engine and had a Nissan-built, electronically controlled four-speed automatic transmission. Holden even went to court in 1984 to stop local motoring magazine Wheels from reporting on the matter. The engine change was necessitated by the legal requirement that all new cars sold in Australia after 1986 had to consume unleaded petrol. Because it was unfeasible to convert the existing six-cylinder engine to run on unleaded fuel, the Nissan engine was chosen as the best engine available. However, changing exchange rates doubled the cost of the engine and transmission over the life of the VL. The decision to opt for a Japanese-made transmission led to the closure of the Woodville, South Australia assembly plant. Confident by the apparent sign of turnaround, GM paid off Holden's mounted losses of A$780 million on 19 December 1986. At GM headquarters’ request, Holden was then reorganised and recapitalised, separating the engine and car manufacturing divisions in the process. This involved the splitting of Holden into "Holden's Motor Company" (HMC) and "Holden's Engine Company" (HEC). For the most part, car bodies were now manufactured at Elizabeth, South Australia, with engines as before, confined to the Fishermans Bend plant in Port Melbourne, Victoria. The engine manufacturing business was successful, building four-cylinder "Family II" engines for use in cars built overseas. The final phase of the Commodore's recovery strategy involved the 1988 VN, a significantly wider model powered by the American-designed, Australian-assembled 3.8-litre Buick V6 engine.
Holden began to sell the subcompact Suzuki Swift-based Barina in 1985. The Barina was launched concurrently with the Suzuki-sourced Holden Drover, followed by the Scurry later on in 1985. In the previous year, Nissan Pulsar hatchbacks were rebadged as the Holden Astra, as a result of a deal with Nissan. This arrangement ceased in 1989 when Holden entered a new alliance with Toyota, forming a new company: United Australian Automobile Industries (UAAI). UAAI resulted in Holden selling rebadged versions of Toyota's Corolla and Camry, as the Holden Nova and Apollo respectively, with Toyota re-branding the Commodore as the Lexcen.
1990s.
The company changed throughout the 1990s, increasing its Australian market share from 21 percent in 1991 to 28.2 percent in 1999. Besides manufacturing Australia's best selling car, which was exported in significant numbers, Holden continued to export many locally produced engines to power cars made elsewhere. In this decade, Holden adopted a strategy of importing cars it needed to offer a full range of competitive vehicles. During 1998, General Motors-Holden's Ltd name was shortened to "Holden Ltd".
On 26 April 1990, GM's New Zealand subsidiary Holden New Zealand announced that production at the assembly plant based in Trentham would be phased out and vehicles would be imported duty-free—this came after the 1984 closure of the Petone assembly line due to low output volumes. During the 1990s, Holden, other Australian automakers and trade unions pressured the Australian Government to halt the lowering of car import tariffs. By 1997, the federal government had already cut tariffs to 22.5 percent, from 57.5 percent ten years earlier; by 2000, a plan was formulated to reduce the tariffs to 15 percent. Holden was critical, saying that Australia's population was not large enough, and that the changes could tarnish the local industry.
Holden re-introduced its defunct Statesman title in 1990—this time under the Holden marque, as the Statesman and Caprice. For 1991, Holden updated the Statesman and Caprice with a range of improvements, including the introduction of four-wheel anti-lock brakes (ABS); although, a rear-wheel system had been standard on the Statesman Caprice from March 1976. ABS was added to the short-wheelbase Commodore range in 1992. Another returning variant was the full-size utility, and on this occasion it was based on the Commodore. The VN Commodore received a major facelift in 1993 with the VR—compared to the VN, approximately 80 percent of the car model was new. Exterior changes resulted in a smoother overall body and a "twin-kidney" grille—a Commodore styling trait that remained until the 2002 VY model and, as of 2013, remains a permanent staple on HSV variants.
Holden introduced the all-new VT Commodore in 1997, the outcome of a A$600 million development programme that spanned more than five years. The new model featured a rounded exterior body shell, improved dynamics and many firsts for an Australian-built car. Also, a stronger body structure increased crash safety. The locally produced Buick-sourced V6 engine powered the Commodore range, as did the 5.0-litre Holden V8 engine, and was replaced in 1999 by the 5.7-litre "LS" unit.
The UAAI badge-engineered cars first introduced in 1989 sold in far fewer numbers than anticipated, but the Holden Commodore, Toyota Camry, and Corolla were all successful when sold under their original nameplates. The first generation Nova and the donor Corolla were produced at Holden's Dandenong, Victoria facility until 1994. UAAI was dissolved in 1996, and Holden returned to selling only GM products. The Holden Astra and Vectra, both designed by Opel in Germany, replaced the Toyota-sourced Holden Nova and Apollo. This came after the 1994 introduction of the Opel Corsa replacing the already available Suzuki Swift as the source for the Holden Barina. Sales of the full-size Holden Suburban SUV sourced from Chevrolet commenced in 1998—lasting until 2001. Also in 1998, local assembly of the Vectra began at Elizabeth, South Australia. These cars were exported to Japan and Southeast Asia with Opel badges. However, the Vectra did not achieve sufficient sales in Australia to justify local assembly, and reverted to being fully imported in 2000.
2000s.
Holden's market surge from the 1990s reversed in the 2000s decade. In Australia, Holden's market share dropped from 27.5 percent in 2000 to 15.2 percent in 2006. From March 2003, Holden no longer held the number one sales position in Australia, losing ground to Toyota.
This overall downturn affected Holden's profits; the company recorded a combined gain of A$842.9 million between 2002 and 2004, and a combined loss of A$290 million between 2005 and 2006. Factors contributing to the loss included the development of an all-new model, the strong Australian dollar and the cost of reducing the workforce at the Elizabeth plant, including the loss of 1,400 jobs after the closure of the third-shift assembly line in 2005, after two years in operation. Holden fared better in 2007, posting an A$6 million loss. This was followed by an A$70.2 million loss in the 2008, an A$210.6 million loss in 2009, and a profit of A$112 million in 2010. On 18 May 2005, "Holden Ltd" became "GM Holden Ltd", coinciding with the resettling to the new Holden headquarters on 191 Salmon Street, Port Melbourne, Victoria.
Holden caused controversy in 2005 with their television advertisement, which ran between October and December 2005. The campaign publicised, "for the first time ever, all Australians can enjoy the financial benefit of Holden Employee Pricing". However, this did not include a discounted dealer delivery fee and savings on factory fitted options and accessories that employees received. At the same time, employees were given a further discount between 25 and 29 percent on selected models.
Holden revived the Monaro coupe in 2001. Based on the VT Commodore architecture, the coupe attracted worldwide attention after being shown as a concept car at Australian auto shows. The VT Commodore received its first major update in 2002 with the VY series. A mildly facelifted VZ model launched in 2004, introducing the "High Feature" engine. This was built at the Fishermans Bend facility completed in 2003, with a maximum output of 900 engines per day. This has reportedly added A$5.2 billion to the Australian economy; exports account for about A$450 million alone. After the VZ, the "High Feature" engine powered the all-new VE Commodore. In contrast to previous models, the VE no longer utilises an Opel-sourced platform adapted both mechanically and in size.
Throughout the 1990s, Opel had also been the source of many Holden models. To increase profitability, Holden looked to the South Korean Daewoo brand for replacements after acquiring a 44.6 percent stake—worth US$251 million—in the company in 2002 as a representative of GM. This was increased to 50.9 percent in 2005, but when GM further increased its stake to 70.1 percent around the time of its 2009 Chapter 11 reorganisation, Holden's interest was relinquished and transferred to another (undisclosed) part of GM.
The commencement of the Holden-branded Daewoo models began with the 2005 Holden Barina, which based on the Daewoo Kalos, replaced the Opel Corsa as the source of the Barina. In the same year, the Viva, based on the Daewoo Lacetti, replaced the entry-level Holden Astra Classic, although the new-generation Astra introduced in 2004 continued on. The Captiva crossover SUV came next in 2006. After discontinuing the Frontera and Jackaroo models in 2003, Holden was only left with one all-wheel drive model: the Adventra, a Commodore-based station wagon. The fourth model to be replaced with a South Korean alternative was the Vectra by the mid-size Epica in 2007. As a result of the split between GM and Isuzu, Holden lost the rights to use the "Rodeo" nameplate. Consequently, the Holden Rodeo was facelifted and relaunched as the Colorado in 2008. Following Holden's successful application for a A$149 million government grant to build a localised version of the Chevrolet Cruze in Australia from 2011, Holden in 2009 announced that it would initially import the small car unchanged from South Korea as the Holden Cruze.
Following the government grant announcement, Kevin Rudd, Australia's Prime Minister at the time, stated that production would support 600 new jobs at the Elizabeth facility; however, this failed to take into account Holden's previous announcement, whereby 600 jobs would be shed when production of the "Family II" engine ceased in late 2009. In mid-2013, Holden sought a further A$265 million, in addition to the A$275 million that was already committed by the governments of Canberra, South Australia and Victoria, to remain viable as a car manufacturer in Australia. A source close to Holden informed the "Australian" news publication that the car company is losing money on every vehicle that it produces and consequently initiated negotiations to reduce employee wages by up to A$200 per week to cut costs, following the announcement of 400 job cuts and an assembly line reduction of 65 (400 to 335) cars per day.
2010s.
In March 2012, Holden was given a $270 million lifeline by the Gillard, Weatherill and Baillieu Ministrys. In return, Holden planned to inject over $1 billion into car manufacturing in Australia. They estimated the new investment package would return around $4 billion to the Australian economy and see GM Holden continue making cars in Australia until at least 2022.
Industry Minister Kim Carr confirmed on 10 July 2013 that talks had been scheduled between the Australian government and Holden. On 13 August 2013, 1,700 employees at the Elizabeth plant in northern Adelaide voted to accept a three-year wage freeze in order to decrease the chances of the production line's closure in 2016. Holden's ultimate survival, though, depended on continued negotiations with the Federal Government—to secure funding for the period from 2016 to 2022—and the final decision of the global headquarters in Detroit, US.
On 10 December 2013, General Motors announced that Holden will cease engine and vehicle manufacturing operations in Australia by the end of 2017. As a result, 2,900 jobs would be lost over four years. Beyond 2017 Holden's Australian presence will consist of: a national sales company, a parts distribution centre and a global design studio.
Corporate affairs and identity.
As of 08 May 2015, Jeff Rolfs, Holden's CFO, is interim chairman and managing director. Holden announced on 6 February 2015 that Mark Bernhard would return to Holden as Chairman and Managing Director, the first Australian to hold the post in 25 years. Vehicles are sold countrywide through the Holden Dealer Network (310 authorised stores and 12 service centres), which employs more than 13,500 people.
In 1987, Holden Special Vehicles (HSV) was formed in partnership with Tom Walkinshaw, who primarily manufactures modified, high-performance Commodore variants. To further reinforce the brand, HSV introduced the HSV Dealer Team into the V8 Supercar fold in 2005 under the naming rights of Toll HSV Dealer Team.
The logo, or "Holden lion and stone" as it is known, has played a vital role in establishing Holden's identity. In 1928, Holden's Motor Body Builders appointed Rayner Hoff to design the emblem. The logo refers to a prehistoric fable, in which observations of lions rolling stones led to the invention of the wheel. With the 1948 launch of the 48-215, Holden revised its logo and commissioned another redesign in 1972 to better represent the company. The emblem was reworked once more in 1994.
Exports.
Holden began to export vehicles in 1954, sending the FJ to New Zealand. Exports to New Zealand have continued ever since, but to broaden their export potential, Holden began to cater their Commodore, Monaro and Statesman/Caprice models for both right- and left-hand drive markets. The Middle East is now Holden's largest export market, with the Commodore sold as the Chevrolet Lumina since 1998, and the Statesman since 1999 as the Chevrolet Caprice. Commodores are also sold as the Chevrolet Lumina in Brunei, Fiji and South Africa, and as the Chevrolet Omega in Brazil. Pontiac in North America also imported Commodore sedans from 2008 through to 2009 as the G8. However, Pontiac went bankrupt in late 2009 and GM had to shut down Pontiac in 2010. The G8's cessation was a consequence of GM's Chapter 11 bankruptcy resulting in the demise of the Pontiac brand.
Sales of the Monaro began in 2003 to the Middle East as the Chevrolet Lumina Coupe. Later on that year, a modified version of the Monaro began selling in North America as the Pontiac GTO, and under the Monaro name through Vauxhall dealerships in the United Kingdom. This arrangement continued through to 2005 when the car was discontinued. The long-wheelbase Statesman sales in the Chinese market as the Buick Royaum began in 2005, before being replaced in 2007 by the Statesman-based Buick Park Avenue. Statesman/Caprice exports to South Korea also began in 2005. These Korean models were sold as the Daewoo Statesman, and later as the Daewoo Veritas from 2008. Holden's move into international markets has been profitable; export revenue increased from A$973 million in 1999 to just under $1.3 billion in 2006.
Since 2011 the WM Caprice has been exported to North America as the Chevrolet Caprice PPV, a version of the Caprice built exclusively for law enforcement in North America sold only to police. Since 2007, the HSV-based Commodore has been exported to the United Kingdom as the Vauxhall VXR8.
In 2013, it was announced that exports of the Commodore would resume to North America in the form of the VF Commodore as the Chevrolet SS sedan for the 2014 model year.
Motorsport.
Holden has been involved with factory backed teams in Australian touring car racing since 1968, and has won the Bathurst 1000 29 times: more than any other factory. The main factory backed teams have been the Holden Dealer Team (1969–1987) and the Holden Racing Team (1990–present). Holden also currently competes in V8 Supercars regularly with other winning teams Red Bull Racing Australia, Brad Jones Racing and Tekno Autosports making it to have the most cars of a brand racing in the championship.
Holden operate a driver training facility at Norwell on the northern Gold Coast, Queensland. In addition to driver safety programs at a number of levels of experience, the centre also offers 4WD training and performance driving courses and "hot laps" in a high-performance car.
References.
</dl>

</doc>
<doc id="13627" url="http://en.wikipedia.org/wiki?curid=13627" title="Hank Greenberg">
Hank Greenberg

Henry Benjamin "Hank" Greenberg (January 1, 1911 – September 4, 1986), nicknamed "Hammerin' Hank," "Hankus Pankus" or "The Hebrew Hammer," was an American Major League Baseball (MLB) first baseman in the 1930s and 1940s. A member of the Baseball Hall of Fame, he was one of the premier power hitters of his generation and is widely considered as one of the greatest sluggers in baseball history.He served over four years in the United States Army and in World War II which took place during his major league career.
Greenberg played primarily for the Detroit Tigers. He was an All-Star for four seasons and an American League (AL) Most Valuable Player two seasons. His 58 home runs for the Tigers in 1938 equaled Jimmie Foxx's 1932 mark for the most in one season by anyone but Babe Ruth, and tied Foxx for the most home runs between Ruth's record 60 in 1927 and Roger Maris' record 61 in 1961. Greenberg was the first major league player to hit 25 or more home runs in a season in each league, and remains the AL record-holder for most RBIs in a single season by a right-handed batter (183 in 1937, a 154-game schedule). 
Greenberg was the first Jewish superstar in American team sports. He attracted national attention in 1934 when he refused to play on Yom Kippur, the holiest holiday in Judaism, even though he was not particularly observant religiously and the Tigers were in the middle of a pennant race. In 1947, Greenberg signed a contract with a $30,000 raise to a record $85,000 before being sold to the Pittsburgh Pirates. He was one of the few opposing players to publicly welcome Jackie Robinson that year to the major leagues.
Early life.
Hank Greenberg was born Hyman Greenberg on January 1, 1911, in Greenwich Village, New York City to Romanian-born Jewish immigrant parents David and Sarah Greenberg, who owned a successful cloth-shrinking plant in New York. He had two brothers, Ben, four years older, and Joe, five years younger, who also played baseball, and a sister, Lillian, two years older. His family moved to the Bronx when he was about seven. Greenberg lacked coordination as a youngster and flat feet prevented him from running fast. But he worked diligently to overcome his inadequacies, which also included acne and a stutter. He attended James Monroe High School in the Bronx, where he was an outstanding all-around athlete and was bestowed with the long-standing nickname of "Bruggy" by his basketball coach. His preferred sport was baseball, and his preferred position was first base. However, Greenberg became a basketball standout in high school, helping Monroe win the city championship.
In 1929, the 18-year-old 193-cm (6-foot-4-inch) Greenberg was recruited by the New York Yankees, who already had Lou Gehrig at first base. Greenberg turned them down and instead attended New York University for a year, where he was a member of Sigma Alpha Mu, after which he signed with the Detroit Tigers for $9,000 ($ today).
Professional baseball.
Minor Leagues.
Greenberg played minor league baseball for three years.
Greenberg played 17 games in 1930 for Hartford, then played at Raleigh, North Carolina, where he hit .314 with 19 home runs.
In 1931, he played at Evansville in the Illinois-Indiana-Iowa League (.318, 15 homers, 85 RBIs).
In 1932, at Beaumont in the Texas League, he hit 39 homers with 131 RBIs, won the MVP award, and led Beaumont to the Texas League title.
Major leagues.
Early years.
In 1930, Greenberg was the youngest player in the majors when he first broke in, at 19.
In 1933, he rejoined the Tigers and hit .301 while driving in 87 runs. At the same time, he was third in the league in strikeouts (78).
In 1934, his second major-league season, he hit .339 and helped the Tigers reach their first World Series in 25 years. He led the league in doubles, with 63 (the 4th-highest all-time in a single season), and extra base hits (96). He was 3rd in the AL in slugging percentage (.600) – behind Jimmie Foxx and Lou Gehrig, but ahead of Babe Ruth, and in RBIs (139), 6th in batting average (.339), 7th in home runs (26), and 9th in on-base percentage (.404).
Late in the 1934 season, he announced that he would not play on September 10, which was Rosh Hashanah, the Jewish New Year, or on September 19, the Day of Atonement, Yom Kippur. Fans grumbled, "Rosh Hashanah comes every year but the Tigers haven't won the pennant since 1909." Greenberg did considerable soul-searching, and discussed the matter with his rabbi; finally he relented and agreed to play on Rosh Hashanah, but stuck with his decision not to play on Yom Kippur. Dramatically, Greenberg hit two home runs in a 2–1 Tigers victory over Boston on Rosh Hashanah. The next day's Detroit Free Press ran the Hebrew lettering for "Happy New Year" across its front page. Columnist and poet Edgar A. Guest expressed the general opinion in a poem titled "Speaking of Greenberg," in which he used the Irish (and thus Catholic) names Murphy and Mulroney. The poem ends with the lines "We shall miss him on the infield and shall miss him at the bat / But he's true to his religion—and I honor him for that." The complete text of the poem is at the end of Greenberg's biography page at the website of the International Jewish Sports Hall of Fame. The Detroit press was not so kind regarding the Yom Kippur decision, nor were many fans, but Greenberg in his autobiography recalled that he received a standing ovation from congregants at the Shaarey Zedek synagogue when he arrived. Absent Greenberg, the Tigers lost to the New York Yankees, 5–2. The Tigers went on to face the St. Louis Cardinals in the 1934 World Series.
In 1935 Greenberg led the league in RBIs (170), total bases (389), and extra base hits (98), tied Foxx for the AL title in home runs (36), was 2nd in the league in doubles (46), slugging percentage (.628), was 3rd in the league in triples (16), and in runs scored (121), 6th in on-base percentage (.411) and walks (87), and was 7th in batting average (.328). He also led the Tigers to their first World Series title. (However, he broke his wrist in the second game.) He was unanimously voted the American League's Most Valuable Player. He set a record (still standing) of 103 RBIs at the All-Star break – but was not selected to the AL All-Star Game roster.
In 1936, Greenberg re-broke his wrist in a collision with Jake Powell of the Washington Senators in April of that year. He had accumulated 16 RBIs in 12 games before his injury.
In 1937, Greenberg was voted to the All-Star Team. On September 19, 1937, he hit the first-ever homer into the center field bleachers at Yankee Stadium. He led the AL by driving in 183 runs (3rd all-time, behind Hack Wilson in 1930 and Lou Gehrig in 1931), and in extra base hits (103), while batting .337 with 200 hits. He was 2nd in the league in home runs (40), doubles (49), total bases (397), slugging percentage (.668), and walks (102), 3rd in on-base percentage (.436), and 7th in batting average (.337). Still, Greenberg came in only 3rd in the vote for MVP.
A prodigious home run hitter, Greenberg narrowly missed breaking Babe Ruth's single-season home run record in 1938, when he was again voted to the All-Star Team and hit 58 home runs, leading the league for the second time. That year, he set the major league record with 11 multi-homer games. Sammy Sosa tied Greenberg's mark in 1998. After having been passed over for the All-Star team in 1935 and being left on the bench for the 1937 game, Greenberg refused to participate in the 1938 contest. In 1938 he homered in four consecutive at-bats over two games. He matched what was then the single-season home run record by a right-handed batter, (Jimmie Foxx, 1932); the mark would stand for 66 years until it was broken by Sammy Sosa and Mark McGwire. Greenberg also had a 59th home run washed away in a rainout. It has been long speculated that Greenberg was intentionally walked late in the season to prevent him from breaking Ruth's record, but Greenberg dismissed this speculation, calling it "crazy stories." Nonetheless, Howard Megdal has calculated that in September 1938, Greenberg was walked in over 20% of his plate appearances, the highest percentage in his career by far. Megdal's article cited this walk percentage statistic as evidence of American League teams not wanting Greenberg to break Babe Ruth's record due to anti-Semitism. However, an examination of the box scores indicate this spike in walks was due to a few games against St. Louis Browns' pitchers with horrific control, not a general league tendency.
In 1938, Greenberg led the league in runs scored (144) and at-bats per home run (9.6), tied for the AL lead in walks (119), was second in RBIs (146), slugging percentage (.683), and total bases (380), and third in OBP (.438) and set a still-standing major league record of 39 homers in his home park, the newly reconfigured Briggs Stadium. He also set a major-league record with 11 multiple-home run games. However, he came in third in the vote for MVP.
In 1939 Greenberg was voted to the All-Star Team for the third year in a row. He was second in the American League in home runs (33) and strikeouts (95), third in doubles (42) and slugging percentage (.622), fourth in RBIs (112), sixth in walks (91), and ninth in on-base percentage (.420).
After the 1939 season ended, Greenberg was asked by general manager Jack Zeller to take a salary cut of $5,000 ($ today) as a result of his off year in power and run production. To top it off, he was asked to move to the outfield to accommodate Rudy York, who was one of the best young hitters of his generation, but was tried at catcher, third base and the outfield and proved to be a defensive liability at each position. Greenberg in turn demanded a $10,000 dollar bonus if he mastered the outfield, stating "he" was the one taking the risk in learning a new position. Greenberg received his bonus at the end of spring training.
In 1940, Greenberg switched from playing the first baseman position to the left field position. He was voted to the All-Star team for the fourth consecutive year. He led the AL in home runs (for the third time in 6 years) with 41; in RBIs (150), doubles (50), total bases (384), extra base hits (99), at-bats per home run (14.0), and slugging percentage (.670; 44 points ahead of Joe DiMaggio). He was second in the league behind Ted Williams in runs scored (129) and OBP (.433), all while batting .340 (fifth best in the AL). He also led the Tigers to a pennant, and won his second American League MVP award, becoming the first player in major-league history to win an MVP award at two different positions.
World War II service.
On October 16, 1940, Greenberg became the first American League player to register for the nation's first peacetime draft.In the spring of 1941, the Detroit draft board initially classified Greenberg as 4F for "flat feet" after his first physical for military service and was recommended for light duty. The rumors that he had bribed the board, and concern that he would be likened to Jack Dempsey who had received negative publicity for failure to serve in World War I, led Greenberg to request to be reexamined. On April 18, he was found fit for regular military service and was reclassified.
On May 7, 1941, he was inducted into the U.S. Army after playing in 19 games and reported to Fort Custer at Battle Creek, Michigan. His salary was cut from $55,000 ($ today) a year to $21 ($ today) a month. He was not bitter and stated, "I made up my mind to go when I was called. My country comes first." In November, while serving as an anti-tank gunner, he was promoted to sergeant, but was honorably discharged on December 5 (the United States Congress released men aged 28 years and older from service), two days before Japan bombed Pearl Harbor. 
Greenberg re-enlisted as a sergeant on February 1, 1942, and volunteered for service in the United States Army Air Corps, becoming the first major league player to do so. He graduated from Officer Candidate School and was commissioned as a first lieutenant in the United States Army Air Forces and was assigned to the Physical Education Program. In February 1944, he was sent to the U.S. Army Special Services school. Promoted to captain, he requested overseas duty later that year and served in the China-Burma-India Theater for over six months, scouting locations for B-29 bomber bases and was a physical training officer with the 58th Bomber Wing. He was a Special Services officer of the 20th Bomber Command, 20th Air Force in China when it began bombing Japan on June 15. He was ordered to New York, and in late 1944, Richmond, Virginia. Greenberg served 47 months, the longest of any major league player.
Return to baseball.
Greenberg remained in military uniform until he was placed on the military inactive list and discharged on June 14, 1945. He was the first major league player to return to MLB after the war. He returned to the Tigers team, and in his first game back on July 1, he homered. That season, there was no official MLB All-Star Game (or MLB All-Star roster) in July which was cancelled in February due to travel restrictions during the last days of the war with Germany and Japan and the ending of World War II. In place of the All-Star Game, 8 exhibition games were scheduled (7 played) and an Associated Press All-Star team was selected which included Greenberg during the All-Star break. 
Greenberg helped lead the Tigers to a come-from-behind American League pennant, clinching it with a grand slam home run in the dark—no lights in Sportsman's Park in St. Louis—ninth inning of the final game of the season. It came after the umpire allegedly told Hank that he was ready to call the game due to darkness, because the ump—former Yankee pitching star of the 1920s Murderers Row team, George Pipgras, supposedly said "Sorry Hank, but I'm gonna have to call the game. I can't see the ball." Greenberg replied, "Don't worry, George, I can see it just fine," so the game continued. It ended with Greenberg's grand slam on the next pitch, clinching Hal Newhouser's 25th victory of the season. The slam allowed the Tigers to clinch the pennant and avoid a one-game playoff (that would have been necessary without the win) against the now-second-place Washington Senators. The Tigers went on to beat the Cubs in the World Series in seven games. Only three home runs were hit in that World Series. Phil Cavarretta hit one for the Cubs in Game One. Greenberg hit the only two homers by the Tigers—one in Game Two, where he batted in three runs in a 4–1 win; the other—a two-run job—tied the game in the eighth inning of Game Six, making the score 8–8, but the Cubs won that game with a run in the bottom of the 12th.
In 1946, he returned to peak form and playing at first base. He led the AL in home runs (44) and RBIs (127), both for the fourth time. He was second in slugging percentage (.604) and total bases (316) behind Ted Williams.
In 1947, Greenberg and the Tigers had a lengthy salary dispute. When Greenberg decided to retire rather than play for less, Detroit sold his contract to the Pittsburgh Pirates. To persuade him not to retire, Pittsburgh made Greenberg the first baseball player to earn over $80,000 ($ today) in a season as pure salary (though the exact amount is a matter of some dispute). Team co-owner Bing Crosby recorded a song, "Goodbye, Mr. Ball, Goodbye" with Groucho Marx and Greenberg to celebrate Greenberg's arrival. The Pirates also reduced the size of Forbes Field's cavernous left field, renaming the section "Greenberg Gardens" to accommodate Greenberg's pull-hitting style. Greenberg played first base for the Pirates in 1947 and was one of the few opposing players to publicly welcome Jackie Robinson to the majors.
That year he also had a chance to mentor a young future Hall-of-Famer, the 24-year-old Ralph Kiner. Said Greenberg, "Ralph had a natural home run swing. All he needed was somebody to teach him the value of hard work and self-discipline. Early in the morning on off-days, every chance we got, we worked on hitting." Kiner would go on to hit 51 home runs that year to lead the National League.
In his final season of 1947, Greenberg tied for the league lead in walks with 104, with a .408 on-base percentage and finished eighth in the league in home runs and tenth in slugging percentage. Greenberg became the first major league player to hit 25 or more home runs in a season in each league. Johnny Mize became the second in 1950. Nevertheless, Greenberg retired as a player to take a front-office post with the Cleveland Indians. No player had ever retired after a final season in which they hit so many home runs. Since then, only Ted Williams (1960, 29), Dave Kingman (1986; 35), Mark McGwire (2001; 29), and Barry Bonds (2007; 28) have hit as many or more homers in their final season.
Through 2010, he was first in career home runs and RBIs (ahead of Shawn Green) and batting average (ahead of Ryan Braun), and fourth in hits (behind Lou Boudreau), among all-time Jewish major league baseball players.
As a fielder, the 193-cm (6-foot-4-inch) Greenberg was awkward and unsure of himself early in his career, but mastered first-base through countless hours of practice. Over the course of his career he demonstrated a higher-than-average fielding percentage and range at first base. When asked to move to left field in 1940 to make room for Rudy York, he worked tirelessly to conquer that position as well, reducing his errors in the outfield from 15 in 1940 to 0 in 1945.
Greenberg felt that runs batted in were more important than home runs. He would tell his teammates, "just get on base," or "just get the runner to third," and he would do the rest.
Final seasons.
Greenberg would likely have approached 500 home runs and 1,800 RBIs had he not served in the military. As it was, he compiled 331 home runs and 1,276 RBI in a 1,394-game career. Greenberg also hit for average, earning a lifetime batting average of .313. Starring as a first baseman and outfielder with the Tigers (1930, 1933–46) and doing duty only briefly with the Pirates (1947), Greenberg played only nine full seasons. He missed all but 19 games of the 1941 season, the three full seasons that followed, and most of 1945 to World War II military service and missed most of another season with a broken wrist.
Management and ownership.
After the 1947 season, Greenberg retired from the field to become the Cleveland Indians' farm system director and two years later, their General Manager and part-owner along with Bill Veeck. During his tenure, he sponsored more African American players than any other major league executive. Greenberg's contributions to the Cleveland farm system led to the team's successes throughout the 1950s, although Bill James once wrote that the Indians' late 1950s collapse should also be attributed to him. In 1949, Larry Doby also recommended Greenberg scout three players Doby used to play with in the Negro leagues: Hank Aaron, Ernie Banks, and Willie Mays. The next offseason Doby asked what Indians' scouts said about his recommendations. Said Greenberg, "Our guys checked 'em out and their reports were not good. They said that Aaron has a hitch in his swing and will never hit good pitching. Banks is too slow and didn't have enough range [at shortstop], and Mays can't hit a curveball." When Veeck sold his interest, Greenberg remained as general manager and part-owner until 1957. He was the mastermind behind a potential move of the club to Minneapolis that was vetoed by the rest of ownership at the last minute.
Greenberg was furious and sold his share soon afterwards. In 1959, Greenberg and Veeck teamed up for a second time when their syndicate purchased the Chicago White Sox; Veeck served as team president with Greenberg as vice president and general manager. During Veeck and Greenberg's first season, the White Sox won their first AL pennant since 1919. Veeck would sell his shares in the White Sox in 1961, and Greenberg stepped down as general manager on August 26 of that season.
After the 1960 season, the American League announced plans to put a team in Los Angeles. Greenberg immediately became the favorite to become the new team's first owner and persuaded Veeck to join him as his partner. However, when Dodgers owner Walter O'Malley got wind of these developments, he threatened to scuttle the whole deal by invoking his exclusive rights to operate a major league team in southern California. In truth, O'Malley wanted no part of competing against an expansion team owned by a master promoter such as Veeck, even if he was only a minority partner. Greenberg wouldn't budge and pulled out of the running for what became the Los Angeles Angels (now the Los Angeles Angels of Anaheim). Greenberg later became a successful investment banker, briefly returning to baseball as a minority partner with Veeck when the latter repurchased the White Sox in 1975.
Personal life.
Greenberg married Caral Gimbel (of the New York department store family) on February 18, 1946, three days after signing a $60,000 ($ today) contract with the Tigers. The couple had three children—sons Glenn and Stephen and a daughter, Alva—before divorcing in 1958. Their son, Stephen, played five years in the Washington Senators/Texas Rangers organization. In 1995, Stephen Greenberg co-founded Classic Sports Network with Brian Bedol, which was purchased by ESPN and became ESPN Classic. He also was the chairman of CSTV, the first cable network devoted exclusively to college sports.
Hank's grandson Spencer Greenberg is a machine learning scientist and Wall Street entrepreneur. In 1966, Greenberg married Mary Jo Tarola, a minor actress who appeared on-screen as Linda Douglas, and remained with her until his death. They had no children.
Miscellaneous.
Incidents of anti-Semitism Greenberg faced included having players stare at him and having coarse racial epithets thrown at him by spectators and sometimes opposing players. Examples of these imprecations were: "Hey Mo!" (referring to the Jewish prophet Moses) and "Throw a pork chop—he can't hit that!" (a reference to Judaic kosher laws). Particularly abusive were the St. Louis Cardinals during the 1934 World Series. In the 1935 World Series umpire George Moriarty warned some Chicago Cubs players to stop yelling anti-Semitic slurs at Greenberg and eventually cleared the players from the Cubs bench. Moriarty was disciplined for this action by then-commissioner Kenesaw Mountain Landis.
"When I was playing, I used to resent being singled out as a Jewish ballplayer. I wanted to be known as a great ballplayer, period. I'm not sure why or when I changed, because I'm still not a particularly religious person. Lately, though, I find myself wanting to be remembered not only as a great ballplayer, but even more as a great Jewish ballplayer."
— Hank Greenberg, after his career
Greenberg sometimes retaliated against the ethnic attacks, once going into the Chicago White Sox clubhouse to challenge manager Jimmy Dykes and at another time calling out the entire Yankee team.
Greenberg befriended Jackie Robinson after he signed with the Dodgers in 1947, and encouraged him; Robinson credited Greenberg with helping him through the difficulties of his rookie year.
Jewish fans in Detroit—-and around the American League for that matter—took to Greenberg almost at once, offering him everything from free meals to free cars, all of which he refused.
"Class tells. It sticks out all over Mr. Greenberg."
 — Jackie Robinson
In 23 World Series games, he hit .318, with five homers and 22 RBI.
Greenberg was one of the few baseball people to testify on behalf of Curt Flood in 1970 when the outfielder challenged the reserve clause.
Greenberg died of metastatic kidney cancer in Beverly Hills, California, in 1986, and his remains were entombed at Hillside Memorial Park Cemetery, in Culver City, California.
In an article in 1976 in "Esquire" magazine, sportswriter Harry Stein published an "All Time All-Star Argument Starter," consisting of five ethnic baseball teams. Greenberg was the first baseman on Stein's Jewish team.
In 2006, Greenberg was featured on a United States postage stamp. The stamp is one of a block of four honoring "baseball sluggers", the others being Mickey Mantle, Mel Ott, and Roy Campanella.

</doc>
<doc id="13628" url="http://en.wikipedia.org/wiki?curid=13628" title="Heinrich Schliemann">
Heinrich Schliemann

Heinrich Schliemann (]; 6 January 1822 – 26 December 1890) was a German businessman and a pioneer of field archaeology. He was an advocate of the historical reality of places mentioned in the works of Homer. Schliemann was an archaeological excavator of Hissarlik, now presumed to be the site of Troy, along with the Mycenaean sites Mycenae and Tiryns. His work lent weight to the idea that Homer's "Iliad" and Virgil's "Aeneid" reflect actual historical events. Schliemann's excavation of nine levels of archaeological remains with dynamite has been criticized as destructive of significant historical artifacts, including the level that is believed to be the historical Troy.
Along with Arthur Evans, Schliemann was a pioneer in the study of Aegean civilization in the Bronze Age. The two men knew of each other, Evans having visited Schliemann's sites. Schliemann had planned to excavate at Knossos, but died before fulfilling that dream. Evans bought the site and stepped in to take charge of the project, which was then still in its infancy.
Childhood and youth.
Schliemann was born in Neubukow, Mecklenburg-Schwerin in 1822. His father, Ernst Schliemann, was a Protestant minister. The family moved to Ankershagen in 1823 (today in their house is the "museum of Heinrich Schliemann"). Heinrich's mother, Luise Therese Sophie, died in 1831, when Heinrich was nine years old. After his mother's death, his father sent Heinrich to live with his uncle. When he was eleven years old, his father paid for him to enroll in the Gymnasium (grammar school) at Neustrelitz. Heinrich's later interest in history was initially encouraged by his father, who had schooled him in the tales of the Iliad and the Odyssey and had given him a copy of Ludwig Jerrer's "Illustrated History of the World" for Christmas in 1829. Schliemann later claimed that at the age of 8, he had declared he would one day excavate the city of Troy.
However, Heinrich had to transfer to the Realschule (vocational school) after his father was accused of embezzling church funds and had to leave that institution in 1836 when his father was no longer able to pay for it. His family's poverty made a university education impossible, so it was Schliemann's early academic experiences that influenced the course of his education as an adult. In his archaeological career, however, there was often a division between Schliemann and the educated professionals.
At age 14, after leaving Realschule, Heinrich became an apprentice at Herr Holtz's grocery in Fürstenberg. He later told that his passion for Homer was born when he heard a drunkard reciting it at the grocer's. He laboured for five years, until he was forced to leave because he burst a blood vessel lifting a heavy barrel. In 1841, Schliemann moved to Hamburg and became a cabin boy on the "Dorothea," a steamer bound for Venezuela. After twelve days at sea, the ship foundered in a gale. The survivors washed up on the shores of the Netherlands. Schliemann became a messenger, office attendant, and later, a bookkeeper in Amsterdam.
Career and family.
On March 1, 1844, 22-year old Schliemann took a position with B. H. Schröder & Co., an import/export firm. In 1846 the firm sent him as a General Agent to St. Petersburg. In time, Schliemann represented a number of companies. He learned Russian and Greek, employing a system that he used his entire life to learn languages—Schliemann claimed that it took him six weeks to learn a language and wrote his diary in the language of whatever country he happened to be in. By the end of his life, he could converse in English, French, Dutch, Spanish, Portuguese, Swedish, Polish, Italian, Greek, Latin, Russian, Arabic, and Turkish as well as German.
Schliemann's ability with languages was an important part of his career as a businessman in the importing trade. In 1850, Heinrich learned of the death of his brother, Ludwig, who had become wealthy as a speculator in the California gold fields. Schliemann went to California in early 1851 and started a bank in Sacramento buying and reselling over a million dollars' worth of gold dust in just six months. When the local Rothschild agent complained about short-weight consignments he left California, pretending it was because of illness. While he was there, California became the 31st state in September 1850 and Schliemann acquired United States citizenship.
According to his memoirs, before arriving in California he dined in Washington with President Millard Fillmore and his family, but Eric Cline says that he didn't attend but simply read about it in the papers. He also published what he said was an eyewitness account of the San Francisco fire of 1851 which he said was in June although it took place in May. At the time he was actually in Sacramento and used the report of the fire in the Sacramento Daily Journal to write his report.
On April 7, 1852, he sold his business and returned to Russia. There he attempted to live the life of a gentleman, which brought him into contact with Ekaterina Lyschin, the niece of one of his wealthy friends. Schliemann had previously learned that his childhood sweetheart, Minna, had married.
Heinrich and Ekaterina married on October 12, 1852. The marriage was troubled from the start. Schliemann next cornered the market in indigo dye and then went into the indigo business itself, turning a good profit. Ekaterina and Heinrich had a son, Sergey, and two daughters, Natalya and Nadezhda, born in 1855, 1858 and 1861 respectively.
Schliemann made yet another quick fortune as a military contractor in the Crimean War, 1854-1856. He cornered the market in saltpeter, sulfur, and lead, constituents of ammunition, which he resold to the Russian government.
By 1858, Schliemann was wealthy enough to retire. In his memoirs, he claimed that he wished to dedicate himself to the pursuit of Troy.
As a consequence of his many travels, Schliemann was often separated from his wife and small children. He spent a month studying at the Sorbonne in 1866, while moving his assets from St. Petersburg to Paris to invest in real estate. He asked his wife to join him, but she refused. Schliemann threatened to divorce Ekaterina twice before actually doing so. In 1869, he bought property and settled in Indianapolis for about three months to take advantage of the liberal divorce laws of Indiana, although he obtained the divorce by lying about his residency in the U.S. and his intention to remain in the state. He moved to Athens as soon as an Indiana court granted him the divorce and married again three months later.
Life as an archaeologist.
Schliemann's first interest of a classical nature seems to have been the location of Troy.
At the time Schliemann began excavating in Turkey, the site commonly believed to be Troy was at Pınarbaşı, a hilltop at the south end of the Trojan Plain. The site had been previously excavated by archaeologist and local expert, Frank Calvert. Schliemann performed soundings at Pınarbaşı, but was disappointed by his findings. It was Calvert who identified Hissarlik as Troy and suggested Schliemann dig there on land owned by Calvert's family. In 1868, Schliemann visited sites in the Greek world, published "Ithaka, der Peloponnesus und Troja" in which he asserted that Hissarlik was the site of Troy, and submitted a dissertation in Ancient Greek proposing the same thesis to the University of Rostock. In 1869, he was awarded a PhD "in absentia" from the university of Rostock for that submission. David Traill wrote that the examiners gave him his PhD on the basis of his topographical analysis of Ithaca, which were in part simply translations of another author's work or drawn from poetic descriptions by the same author.
Schliemann was at first skeptical about the identification of Hissarlik with Troy but was persuaded by Calvert and took over Calvert's excavations on the eastern half of the Hissarlik site. The Turkish government owned the western half. Calvert became Schliemann's collaborator and partner.
Schliemann needed an assistant who was knowledgeable in matters pertaining to Greek culture. As he had divorced Ekaterina in 1869, he advertised for a wife in a newspaper in Athens. A friend, the Archbishop of Athens, suggested a relative of his, seventeen-year-old Sophia Engastromenos (1852–1932). Schliemann, age 47, married her in October 1869, despite the 30 year difference in age. They later had two children, Andromache and Agamemnon Schliemann; he reluctantly allowed them to be baptized, but solemnized the ceremony in his own way by placing a copy of the "Iliad" on the children's heads and reciting one hundred hexameters.
Schliemann began work on Troy in 1871. His excavations began before archaeology had developed as a professional field. Thinking that Homeric Troy must be in the lowest level, Schliemann and his workers dug hastily through the upper levels, reaching fortifications that he took to be his target. In 1872, he and Calvert fell out over this method. Schliemann was angry when Calvert published an article stating that the Trojan War period was missing from the site's archaeological record.
Priam's Treasure.
A cache of gold and other objects appeared on or around May 27, 1873; Schliemann named it "Priam's Treasure". He later wrote that he had seen the gold glinting in the dirt and dismissed the workmen so that he and Sophia could excavate it themselves, removing it in her shawl. However, Schliemann's oft-repeated story of the treasure being carried by Sophia in her shawl was untrue. Schliemann later admitted fabricating it; at the time of the discovery Sophia was in fact with her family in Athens, following the death of her father. Sophia later wore "the Jewels of Helen" for the public. Those jewels, taken from the Pergamon Museum in Berlin by the Soviet Army (Red Army) in 1945, are now in the Pushkin Museum in Moscow.
Schliemann published his findings in 1874, in "Trojanische Altertümer" ("Trojan Antiquities").
This publicity backfired when the Turkish government revoked Schliemann's permission to dig and sued him for a share of the gold. Collaborating with Calvert, Schliemann smuggled the treasure out of Turkey. He defended his "smuggling" in Turkey as an attempt to protect the items from corrupt local officials. Priam's Treasure today remains a subject of international dispute.
Schliemann published "Troja und seine Ruinen" ("Troy and Its Ruins") in 1875 and excavated the Treasury of Minyas at Orchomenus. In 1876, he began digging at Mycenae. Upon discovering the Shaft Graves, with their skeletons and more regal gold (including the so-called Mask of Agamemnon), Schliemann cabled the king of Greece. The results were published in "Mykenai" in 1878.
Although he had received permission in 1876 to continue excavation, Schliemann did not reopen the dig site at Troy until 1878–1879, after another excavation in Ithaca designed to locate an actual site mentioned in the "Odyssey". This was his second excavation at Troy. Emile Burnouf and Rudolf Virchow joined him there in 1879. Schliemann made a third excavation at Troy in 1882–1883, an excavation of Tiryns with Wilhelm Dörpfeld in 1884, a fourth excavation at Troy, also with Dörpfeld (who emphasized the importance of strata), in 1888–1890.
Death.
On August 1, 1890, Schliemann returned reluctantly to Athens, and in November travelled to Halle, where his chronic ear infection was operated upon, on November 13. The doctors deemed the operation a success, but his inner ear became painfully inflamed. Ignoring his doctors' advice, he left the hospital and travelled to Leipzig, Berlin, and Paris. From the latter, he planned to return to Athens in time for Christmas, but his ear condition became even worse. Too sick to make the boat ride from Naples to Greece, Schliemann remained in Naples, but managed to make a journey to the ruins of Pompeii. On Christmas Day he collapsed into a coma and died in a Naples hotel room on December 26, 1890. The cause of death was cholesteatoma. His corpse was then transported by friends to the First Cemetery in Athens. It was interred in a mausoleum shaped like a temple erected in ancient Greek style designed by Ernst Ziller in the form of a pedimental sculpture. The frieze circling the outside of the mausoleum shows Schliemann conducting the excavations at Mycenae and other sites. His magnificent residence in the city centre of Athens, the "Iliou Melathron" (Ιλίου Μέλαθρον, "Palace of Ilium") houses today the Numismatic Museum of Athens.
Criticisms.
Further excavation of the Troy site by others indicated that the level he named the Troy of the "Iliad" was inaccurate, although they retain the names given by Schliemann. In an article for The Classical World, D. F. Easton writes that Schliemann "was not very good at separating fact from interpretation." He goes on to claim that "Even in 1872 Frank Calvert could see from the pottery that Troy II had to be hundreds of years too early to be the Troy of the Trojan War, a point finally proved by the discovery of Mycenaean pottery in Troy VI in 1890." 
"King Priam's Treasure" was found in the Troy II level, that of the Early Bronze Age, long before Priam's city of Troy VI or Troy VIIa in the prosperous and elaborate Mycenaean Age. Moreover, the finds were unique. The elaborate gold artifacts do not appear to belong to the Early Bronze Age.
His excavations were condemned by later archaeologists as having destroyed the main layers of the real Troy. Kenneth W. Harl in the Teaching Company's "Great Ancient Civilizations of Asia Minor" lecture series sarcastically claims that Schliemann's excavations were carried out with such rough methods that he did to Troy what the Greeks couldn't do in their times, destroying and levelling down the entire city walls to the ground.
In 1972, Professor William Calder of the University of Colorado, speaking at a commemoration of Schliemann's birthday, claimed that he had uncovered several possible problems in Schliemann's work. Other investigators followed, such as Professor David Traill of the University of California.
An article published by the National Geographic Society called into question Schliemann's qualifications, his motives, and his methods:
Another article presented similar criticisms when reporting on a speech by University of Pennsylvania scholar C. Brian Rose:
Schliemann's methods have been described as "savage and brutal. He plowed through layers of soil and everything in them without proper record keeping—no mapping of finds, few descriptions of discoveries." Carl Blegen forgave his recklessness, saying "Although there were some regrettable blunders, those criticisms are largely colored by a comparison with modern techniques of digging; but it is only fair to remember that before 1876 very few persons, if anyone, yet really knew how excavations should properly be conducted. There was no science of archaeological investigation, and there was probably no other digger who was better than Schliemann in actual field work."
In popular culture.
Schliemann is the subject of Irving Stone's novel "The Greek Treasure" (1975). Stone's book is the basis for the German television production "" ("Hunt for Troy") from 2007.
He is also the subject of the novel The Lost Throne by American author Chris Kuzneski.
The novel "The Fall of Troy" by Peter Ackroyd (2006) is based on Schliemann's excavation of Troy. Schliemann is portrayed as "Heinrich Obermann".
References.
Notes
Further reading

</doc>
<doc id="13629" url="http://en.wikipedia.org/wiki?curid=13629" title="Hypnos">
Hypnos

In Greek mythology, Hypnos (; Greek: Ὕπνος, "sleep") was the personification of sleep; the Roman equivalent was known as Somnus.
Dwelling place.
According to Greek mythology Hypnos lived in a cave, whose mansion does not see the rising, nor the setting sun, nor does it see the "lightsome noon." At the entrance were a number of poppies and other hypnotic plants. His dwelling had no door or gate so that he might not be awakened by the creaking of hinges. The river Lethe, in the underworld, flowed through his cave. This river is known as the river of forgetfulness.
Family.
Hypnos lived next to his twin brother, Thanatos (Θάνατος, "death personified") in the underworld. The underworld is translated into English as Hell in the Septuagint Bible.
Hypnos' mother was Nyx (Νύξ, "Night"), the deity of Night, and his father was Erebus, the deity of Darkness. Nyx was a dreadful and powerful goddess, and even Zeus feared entering her realm.
His wife, Pasithea, was one of the youngest of the Graces and was promised to him by Hera, who is the goddess of marriage and birth. Pasithea is the deity of hallucination or relaxation.
Hypnos' three sons were known as the Oneiroi, which is Greek for "dreams." Morpheus is the Winged God of Dreams and can take human form in dreams. Phobetor is the personification of nightmares and created frightening dreams, he could take the shape of any animal including bears and tigers. Phantasos was known for creating fake dreams full of illusions. Morpheus, Phobetor, and Phantasos appeared in the dreams of kings. The Oneiroi lived in a cave at the shores of the Ocean in the West. The cave had two gates with which to send people dreams; one made from ivory and the other from buckhorn. However, before they could do their work and send out the dreams, first Hypnos had to put the recipient to sleep.
Hypnos in the Iliad.
Hypnos used his powers to trick Zeus. Hypnos was able to trick him and help the Danaans win the Trojan war. During the war, Hera loathed her brother and husband, Zeus, so she devised a plot to trick him. She decided that in order to trick him she needed to make him so enamoured with her that he would fall for the trick. So she went and washed herself with ambrosia and anointed herself with oil, made especially for her to make herself impossible to resist for Zeus. She wove flowers through her hair, put on three brilliant pendants for earrings, and donned a wondrous robe. She then called for Aphrodite, the goddess of love, and asked her for a charm that would ensure that her trick would not fail. In order to procure the charm, however, she lied to Aphrodite because they sided on opposites sides of the war. She told Aphrodite that she wanted the charm to help her parents stop fighting. Aphrodite willingly agreed. Hera was almost ready to trick Zeus, but she needed the help of Hypnos, who had tricked Zeus once before.
Hera called on Hypnos and asked him to help her by putting Zeus to sleep. Hypnos was reluctant because the last time he had put the god to sleep, he was furious when he awoke. It was Hera who had asked him to trick Zeus the first time as well. She was furious that Hercules, Zeus' son, sacked the city of the Trojans. So she had Hypnos put Zeus to sleep, and set blasts of angry winds upon the sea while Heracles was still sailing home. When Zeus awoke he was furious and went on a rampage looking for Hypnos. Hypnos managed to avoid Zeus by hiding with his mother, Nyx. This made Hypnos reluctant to accept Hera's proposal and help her trick Zeus again. Hera first offered him a beautiful golden seat that can never fall apart and a footstool to go with it. He refused this first offer, remembering the last time he tricked Zeus. Hera finally got him to agree by promising that he would be married to Pasithea, one of the youngest Graces, whom he had always wanted to marry. Hypnos made her swear by the river Styx and call on gods of the underworld to be witnesses so that he would be ensured that he would marry Pasithea.
Now, with Hypnos' help, Hera went to see Zeus on Gargarus, the topmost peak of Mount Ida. Zeus was extremely taken by her and suspected nothing as Hypnos was shrouded in a thick mist and hidden upon a pine tree that was close to where Hera and Zeus were talking. Zeus asked Hera what she was doing there and why she had come there from Olympus and she told him the same lie she told her daughter Aphrodite. She told him that she wanted to go help her parents stop quarreling and she stopped there to consult him because she didn't want to go without his knowledge and have him be angry with her when he found out. Zeus said that she could go any time, and that she should postpone her visit and stay there with him so they could enjoy each other's company. He told her that he was never in love with anyone as much as he loved her at that moment. He took her in his embrace and Hypnos went to work putting him to sleep, with Hera in his arms. While this went on, Hypnos traveled to the ships of the Achaeans to tell Poseidon, God of the Sea, that he could now help the Danaans and give them a victory while Zeus was sleeping. This is where Hypnos leaves the story, leaving Poseidon eager to help the Dananns. Thanks to Hypnos helping to trick Zeus, the war changed its course to Hera's favor, and Zeus never found out that Hypnos had tricked him one more time.
Hypnos in art.
Hypnos appears in numerous works of art, most of which are vases. An example of one vase that Hypnos is featured on is called “Ariadne Abandoned by Theseus,” which is part of the Museum of Fine Arts in Boston’s collection. In this vase, Hypnos is shown as a winged god dripping Lethean water upon the head of Ariadne as she sleeps. One of the most famous works of art featuring Hypnos is a bronze head of Hypnos himself, now kept in the British Museum in London. This bronze head has wings sprouting from his temples and the hair is elaborately arranged, some tying in knots and some hanging freely from his head.
Words derived from Hypnos.
The English word "hypnosis" is derived from his name, referring to the fact that when hypnotized, a person is put into a sleep-like state (hypnos "sleep" + -osis "condition").
Additionally, the English word "insomnia" comes from the name of his Latin counterpart, Somnus. (in- "not" + somnus "sleep"), as well as a few less-common words such as "somnolent", meaning sleepy or tending to cause sleep.
External links.
 

</doc>
<doc id="13631" url="http://en.wikipedia.org/wiki?curid=13631" title="Holy orders">
Holy orders

In Christian Churches, Holy Orders are special roles within the church such as bishop, priest or deacon. In the Roman Catholic (Latin: "sacri ordines"), Eastern Catholic, Eastern Orthodox (ιερωσύνη [hierōsynē], ιεράτευμα [hierateuma], Священство [Svyashchenstvo]), Oriental Orthodox, Anglican, Assyrian, Old Catholic, Independent Catholic churches and some Lutheran churches holy orders comprise the three orders of bishop, priest and deacon, or the sacrament or rite by which candidates are ordained to those orders. Except for Lutherans and some Anglicans, these churches regard ordination as a sacrament (the "sacramentum ordinis"). The Anglo-Catholic party within Anglicanism tends to identify with the Roman Catholic position with regard to the sacramental nature of ordination.
Denominations have varied conceptions of holy orders. In the Anglican churches and some Lutheran churches the traditional orders of bishop, priest and deacon are bestowed using ordination rites. The extent to which ordination is considered sacramental in these traditions has, however, been a matter of some internal dispute. Many other denominations do not consider the role of ministry as being sacramental in nature and would not think of it in terms of "holy orders" as such. Historically, the word "order" (Latin "ordo") designated an established civil body or corporation with a hierarchy, and "ordinatio" meant legal incorporation into an "ordo". The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Other positions such as pope, patriarch, cardinal, monsignor, archbishop, archimandrite, archpriest, protopresbyter, hieromonk, protodeacon, archdeacon, etc., are not sacramental orders. These are simply offices or titles.
Eastern Christianity.
The Eastern Orthodox Church considers ordination (known as "Cheirotonia", "laying on of hands") to be a Sacred Mystery (what in the West is called a sacrament). Although all other mysteries may be performed by a presbyter, ordination may only be conferred by a bishop, and ordination of a bishop may only be performed by several bishops together. Cheirotonia always takes place during the Divine Liturgy.
It was the mission of the Apostles to go forth into all the world and preach the Gospel, baptizing those who believed in the name of the Holy Trinity (). In the Early Church those who presided over congregations were referred to variously as "episcopos" (bishop) or "presbyteros" (priest). These successors of the Apostles were ordained to their office by the laying on of hands, and according to Orthodox theology formed a living, organic link with the Apostles, and through them with Jesus Christ himself. This link is believed to continue in unbroken succession to this day. Over time, the ministry of bishops (who hold the fullness of the priesthood) and presbyters or priests (who hold a portion of the priesthood as bestowed by their bishop) came to be distinguished. In Orthodox terminology, "priesthood" or "sacerdotal" refers to the ministry of bishops and priests.
The Eastern Orthodox Church also has ordination to minor orders (known as "cheirothesia", "imposition of hands") which is performed outside of the Divine Liturgy, typically by a bishop, although certain archimandrites of stavropegial monasteries may bestow cheirothesia on members of their communities.
A bishop is the Teacher of the Faith, the carrier of Sacred Tradition, and the living Vessel of Grace through whom the "energeia" (divine grace) of the Holy Spirit flows into the rest of the church. A bishop is consecrated through the laying on of hands by several bishops. (With the consent of several other bishops, a single bishop has performed the ordination of another bishop, in emergency situations, such as times of persecution), The consecration of a bishop takes place near the beginning of the Liturgy, since a bishop can, in addition to performing the Mystery of the Eucharist, also ordain priests and deacons. Before the commencement of the Holy Liturgy, the bishop-elect professes, in the middle of the church before the seated bishops who will consecrate him, in detail the doctrines of the Orthodox Christian Faith and pledges to observe the canons of the Apostles and Councils, the Typikon and customs of the Orthodox Church and to obey ecclesiastical authority. After the Little Entrance, the arch-priest and arch-deacon conduct the bishop-elect before the Royal Gates where he is met by the bishops and kneels before the altar on both knees and the Gospel Book is laid over his head and the consecrating bishops lay their hands upon the Gospel Book, while the prayers of ordination are read by the eldest bishop; after this, the newly consecrated bishop ascends the "synthranon" (bishop's throne in the sanctuary) for the first time.
 Customarily, the newly consecrated bishop ordains a priest and a deacon at the Liturgy during which he is consecrated.
A priest may serve only at the pleasure of his bishop. A bishop bestows faculties (permission to minister within his diocese) giving a priest chrism and an antimins; he may withdraw faculties and demand the return of these items. The ordination of a priest occurs before the Anaphora (Eucharistic Prayer) in order that he may on the same day take part in the celebration of the Eucharist: During the Great Entrance, the candidate for ordination carries the Aër (chalice veil) over his head (rather than on his shoulder, as a deacon otherwise carries it then) as a symbol of giving up his diaconate, and comes last in the procession and stands at the end of the pair of lines of the priests. After the Aër is taken from the candidate to cover the chalice and diskos, a chair is brought for the bishop to sit on by the northeast corner of the Holy Table (altar). Two deacons go to priest-elect who, at that point, had been standing alone in the middle of the church, and bow him down to the west (to the people) and to the east (to the clergy), asking their consent by saying “Command ye!” and then lead him through the holy doors of the altar where the archdeacon asks the bishop’s consent, saying, “Command, most sacred master!” after which a priest escorts the candidate three times around the Holy Table, during which he kisses each corner of the Holy Table as well as the bishop's epigonation and right hand and prostrates himself before the holy table at each circuit. The candidate is then taken to the southeast corner of the Holy Table and kneels on both knees, resting his forehead on the edge of the Holy Table. The ordaining bishop then places his omophor and right hand over the ordinand's head and recites aloud the first "Prayer of Cheirotonia" and then prays silently the other two prayers of cheirotonia while a deacon quietly recites a litany and the clergy, then the congregation, chant “Lord, have mercy”. Afterwards, the bishop brings the newly ordained priest to stand in the Holy Doors and presents him to the faithful. He then clothes the priest in each of his sacerdotal vestments, at each of which the people sing, "Worthy!". Later, after the Epiklesis of the Liturgy, the bishop hands him a portion of the Lamb (Host) saying:
Receive thou this pledge, and preserve it whole and unharmed until thy last breath, because thou shalt be held to an accounting therefore in the second and terrible Coming of our great Lord, God, and Saviour, Jesus Christ.
A deacon may not perform any Sacrament and, indeed, performs no liturgical services on his own but serves only as an assistant to a priest and may not even vest without the blessing of a priest. The ordination of a deacon occurs after the Anaphora (Eucharistic Prayer) since his role is not in performing the Holy Mystery but consists only in serving; the ceremony is much the same as at the ordination of a priest, but the deacon-elect is presented to the people and escorted to the holy doors by two sub-deacons (his peers, analogous to the two deacons who so present a priest-elect) is escorted three times around the Holy Table by a deacon, and he kneels on only one knee during the "Prayer of Cheirotonia". After being vested as a deacon and given a "liturgical fan (ripidion or hexapterygion)", he is led to the side of the Holy Table where he uses the ripidion to gently fan the Holy Gifts (consecrated Body and Blood of Christ).
Anglicanism.
The Anglican churches hold their bishops to be in apostolic succession, although there is some difference of opinion with regard to whether ordination is to be regarded as a sacrament. The Anglican Articles of Religion hold that only Baptism and the Lord's Supper are to be counted as sacraments of the gospel, and assert that other rites considered to be sacraments by such as the Roman Catholic and Eastern churches were not ordained by Christ and do not have the nature of a sacrament in the absence of any physical matter such as the water in Baptism and the bread and wine in the Eucharist. The Book of Common Prayer provides rites for ordination of bishops, priests and deacons. Only bishops may ordain. Within Anglicanism, three bishops are normally required for ordination to the episcopate, while one bishop is sufficient for performing ordinations to the priesthood and diaconate.
Lutheranism.
Lutherans reject the Roman Catholic understanding of holy orders because they do not think sacerdotalism is supported by the Bible. Martin Luther taught that each individual was expected to fulfill his God-appointed task in everyday life. The modern usage of the term vocation as a life-task was first employed by Martin Luther. In Luther's Small Catechism, the holy orders include, but are not limited to the following: bishops, pastors, preachers, governmental offices, citizens, husbands, wives, children, employees, employers, young people, and widows.
Roman Catholicism.
The ministerial orders of the Catholic Church include the orders of bishops, deacons and presbyters, which in Latin is "sacerdos". The ordained priesthood and common priesthood (or priesthood of the all the baptized) are different in function and essence.
A distinction is to be made between "priest" and "presbyter". In the 1983 Code of Canon Law, "The Latin words "sacerdos" and "sacerdotium" are used to refer in general to the ministerial priesthood shared by bishops and presbyters. The words "presbyter, presbyterium and presbyteratus" refer to priests [in the English use of the word] and presbyters".
The priesthood in the Catholic Church includes the priests of both the Latin Rite and the Eastern Rites. As of May 2007, the Vatican website stated that there were some 406,411 priests serving the church worldwide.
While the consecrated life is neither clerical nor lay by definition, clerics can be members of institutes of consecrated or secular (diocesan) life.
Process and sequence.
The sequence in which holy orders are received are: minor orders, deacon, priest, bishop.
For Catholics, it is typical in the year of seminary training that a man will be ordained to the diaconate, called by Catholics in recent times the "transitional diaconate". This is to distinguish men bound for priesthood from permanent deacons. They are licensed to preach sermons (under certain circumstances a permanent deacon may not receive faculties to preach), to perform baptisms, and to witness Catholic marriages, but to perform no other sacraments. They assist at the Eucharist or the Mass, but are not able to consecrate the bread and wine. Normally, after six months or more as a transitional deacon, a man will be ordained to the priesthood. Priests are able to preach, perform baptisms, confirm (with special dispensation from their ordinary), witness marriages, hear confessions and give absolutions, anoint the sick, and celebrate the Eucharist or the Mass.
Orthodox seminarians are typically tonsured as readers before entering seminary, and may later be made subdeacons or deacons; customs vary between seminaries and between Orthodox jurisdictions. Some deacons remain permanently in the diaconate while most subsequently are ordained as priests. Orthodox clergy are typically either married or monastic. Monastic deacons are called hierodeacons, monastic priests are called hieromonks. Orthodox clergy who marry must do so prior to ordination to the subdiaconate (or diaconate, according to local custom) and typically one is either tonsured a monk or married before ordination. A deacon or priest may not marry, or remarry if widowed, without abandoning his clerical office. Often, widowed priests take monastic vows. Orthodox bishops are always monks; a single or widowed man may be elected a bishop but he must be tonsured a monk before consecration as a bishop.
For Anglicans, a person is usually ordained a deacon once he (or she) has completed training at a theological college. The historic practice of a bishop tutoring a candidate himself ("reading for orders") is still to be found.
The candidate then typically serves as an assistant curate and may later be ordained as a priest at the discretion of the bishop. Other deacons may choose to remain in this order. Anglican deacons can preach sermons, perform baptisms and conduct funerals, but, unlike priests, cannot celebrate the Eucharist. In most branches of the Anglican church, women can be ordained as priests, and in some of them, can also be ordained bishops.
Bishops are chosen from among priests in churches that adhere to Catholic usage.
In the Roman Catholic Church, bishops, like priests, are celibate and thus unmarried; further, a bishop is said to possess the fullness of the sacrament of holy orders, empowering him to ordain deacons, priests, and – with papal consent – other bishops. If a bishop, especially one acting as an ordinary – a head of a diocese or archdiocese – is to be ordained, three bishops must usually co-consecrate him with one bishop, usually an archbishop or the bishop of the place, being the chief consecrating prelate.
Among Eastern Rite Catholic and Eastern Orthodox churches, which permit married priests, bishops must either be unmarried or agree to abstain from contact with their wives. It is a common misconception that all such bishops come from religious orders; while this is generally true, it is not an absolute rule. In the case of both Catholics – (Western and) Eastern Catholic, Oriental Orthodox and Eastern Orthodox, they are usually leaders of territorial units called dioceses (or its equivalent in the east, an eparchy). Only bishops can validly administer the sacrament of holy orders.
Recognition of other churches' orders.
The Roman Catholic Church unconditionally recognizes the validity of ordinations in the Eastern churches. Some Eastern Orthodox Churches re-ordain Catholic priests who convert while others accept the Roman Catholic ordination ceremonies using the concept of economia (church economy).
Anglican churches, unlike most other Protestant churches, claim to have maintained apostolic succession. The succession of Anglican bishops is not universally recognized, however. The Roman Catholic Church judged Anglican orders invalid when Pope Leo XIII in 1896 wrote in "Apostolicae curae" that Anglican orders lack validity because the rite by which priests were ordained was not correctly worded from 1547 to 1553 and from 1558 to the time of Archbishop William Laud, (Archbishop of Canterbury 1633–1645). This caused a break of continuity in apostolic succession, making all further ordinations null and void.
Eastern Orthodox bishops have, on occasion, granted "economy" when Anglican priests convert to Orthodoxy. Various Orthodox churches have also declared Anglican orders valid subject to a finding that the bishops in question did indeed maintain the true faith, the Orthodox concept of Apostolic Succession being one in which the faith must be properly adhered to and transmitted, not simply that the ceremony by which a man is made a bishop is conducted correctly.
Changes in the Anglican Ordinal since King Edward VI, and a fuller appreciation of the pre-Reformation ordinals, suggest that the correctness of the enduring dismissal of Anglican orders is questionable. To reduce doubt concerning Anglican apostolic succession, especially since the 1930 Bonn agreement between the Anglican and Old Catholic churches, some Anglican bishops have included among their consecrators bishops of the Old Catholic Church, whose holy orders are recognised as valid and regular by the Roman Catholic Church.
Neither Roman Catholics nor Anglicans recognize the validity of ordinations of ministers in Protestant churches that do not maintain Apostolic Succession; but Anglicans, especially Low Church or Evangelical Anglicans, commonly treat Protestant ministers and their sacraments as valid. Rome also does not recognize the apostolic succession of those Lutheran bodies which retained Apostolic Succession.
Officially, the Anglican Communion accepts the ordinations of those denominations which are in full communion with their own churches, such as the Lutheran state churches of Scandinavia. Those clergy may preside at services requiring a priest if one is not otherwise available.
Marriage and holy orders.
The rules discussed in this section are not considered to be among the infallible dogmas of the Catholic Church, but are mutable rules of discipline. See clerical celibacy for a more detailed discussion.
Married men may be ordained to the diaconate as Permanent Deacons, but in the Latin Rite of the Roman Catholic Church generally may not be ordained to the priesthood. In the Eastern Catholic Churches and in the Eastern Orthodox Church, married deacons may be ordained priests but may not become bishops. Bishops in the Eastern Rites and the Eastern Orthodox churches are almost always drawn from among monks, who have taken a vow of celibacy. They may be widowers, though; it is not required of them never to have been married.
In some cases, widowed permanent deacons have been ordained to the priesthood. There have been some situations in which men previously married and ordained to the priesthood in an Anglican church or in a Lutheran church have been ordained to the Catholic priesthood and allowed to function much as an Eastern Rite priest but in a Latin Rite setting. This is never "sub conditione" (conditionally), as there is in Catholic canon law no true priesthood in Protestant denominations. Such ordination may only happen with the approval of the priest's Bishop and a special permission by the Pope.
Anglican clergy may be married and/or may marry after ordination. In the Old Catholic Church and the Independent Catholic Churches there are no ordination restrictions related to marriage.
Other concepts of ordination.
Ordination ritual and procedures vary by denomination. Different churches and denominations specify more or less rigorous requirements for entering into office, and the process of ordination is likewise given more or less ceremonial pomp depending on the group. Many Protestants still communicate authority and ordain to office by having the existing overseers physically lay hands on the candidates for office.
Methodist churches.
The American Methodist model is an episcopal system loosely based on the Anglican model, as the Methodist Church arose from the Anglican Church. It was first devised under the leadership of Bishops Thomas Coke and Francis Asbury of the Methodist Episcopal Church in the late 18th century. In this approach, an elder (or "presbyter") is ordained to word (preaching and teaching), sacrament (administering Baptism and the Lord's Supper), order (administering the life of the church and, in the case of bishops, ordaining others for mission and ministry), and service. A deacon is a person ordained only to word and service.
In the United Methodist Church, for instance, seminary graduates are examined and approved by the Conference Board of Ordained Ministry and then the Clergy Session. They are accepted as "probationary (provisional) members of the conference." The resident bishop may commission them to full-time ministry as "provisional" ministers. (Before 1996, the graduate was ordained as a transitional deacon at this point, a provisional role since eliminated. The order of deacon is now a separate and distinct clergy order in the United Methodist Church.) After serving the probationary period, of a minimum of two years, the probationer is then examined again and either continued on probation, discontinued altogether, or approved for ordination. Upon final approval by the Clergy Session of the Conference, the probationer becomes a full member of the Conference and is then ordained as an elder or deacon by the resident Bishop. Those ordained as elders are members of the Order of Elders, and those ordained deacons are members of the Order of Deacons.
The British Methodist Conference has two distinct orders of presbyter and deacon. It does not have bishops as a separate order of ministry.
John Wesley appointed Thomas Coke (above mentioned as bishop) as 'Superintendent', his translation of the Greek 'episcopos' - which is normally translated 'bishop' in English. The British Methodist Church has more than 500 Superintendents who are not a separate order of ministry but a role within the order of Presbyters.
In British Methodism the roles normally undertaken by bishops are expressed in ordaining presbyters and deacons by the annual Conference through its President (or a past president); in confirmation by all presbyters; in local oversight by Superintendents and in regional oversight by Chairs of District.
Presbyterian churches.
Presbyterian churches, following their Scottish forebears, reject the traditions surrounding overseers and instead identify the offices of bishop ("episkopos" in Greek) and elder ("presbuteros" in Greek, from which the term "presbyterian" comes). The two terms seem to be used interchangeably in the Bible (compare and ). Their form of church governance is known as presbyterian polity. While there is increasing authority with each level of gathering of elders ('Session' over a congregation or parish, then presbytery, then possibly a synod, then the General Assembly), there is no hierarchy of elders. Each elder has an equal vote at the court on which they stand.
Elders are usually chosen at their local level, either elected by the congregation and approved by the Session, or appointed directly by the Session. Some churches place limits on the term that the elders serve, while others ordain elders for life.
Presbyterians also ordain (by laying on of hands) ministers of Word and Sacrament (sometimes known as 'teaching elders'). These ministers are regarded simply as Presbyters ordained to a different function, but in practice they provide the leadership for local Session.
Some Presbyterians identify those appointed (by the laying on of hands) to serve in practical ways () as deacons ("diakonos" in Greek, meaning "servant"). In many congregations, a group of men or women is thus set aside to deal with matters such as congregational fabric and finance, releasing elders for more 'spiritual' work. These persons may be known as 'deacons', 'board members' or 'managers', depending on the local tradition. Unlike elders and minister, they are not usually 'ordained', and are often elected by the congregation for a set period of time.
Other Presbyterians have used an 'order of deacons' as full-time servants of the wider Church. Unlike ministers, they do not administer sacraments or routinely preach. The Church of Scotland has recently begun ordaining deacons to this role.
Unlike the Episcopalian system, but similar to the United Methodist system described above, the two Presbyterian offices are different in "kind" rather than in "degree", since one need not be a deacon before becoming an elder. Since there is no hierarchy, the two offices do not make up an "order" in the technical sense, but the terminology of holy orders is sometimes still developed.
Congregationalist churches.
Congregationalist churches implement different schemes, but the officers usually have less authority than in the presbyterian or episcopalian forms. Some ordain only ministers and rotate members on an advisory board (sometimes called a board of elders or a board of deacons). Because the positions are by comparison less powerful, there is usually less rigor or fanfare in how officers are ordained.
Latter Day Saint Movement.
The Church of Jesus Christ of Latter-day Saints.
The Church of Jesus Christ of Latter-day Saints (LDS Church) accepts the legal authority of clergy to perform marriages but does not recognize any other sacraments performed by ministers not ordained to the Latter-day Saint priesthood. Although the Latter-day Saints do claim a doctrine of a certain spiritual "apostolic succession," it is significantly different from that claimed by Catholics and Protestants since there is no succession or continuity between the first century and the lifetime of Joseph Smith, the founder of the LDS church. Mormons teach that the priesthood was lost in ancient times not to be restored by Christ until the nineteenth century when it was given to Joseph Smith directly. 
The Church of Jesus Christ of Latter-day Saints has a relatively open priesthood, ordaining nearly all worthy adult males and boys of the age of twelve and older. Latter-day Saint priesthood consists of two divisions: the Melchizedek Priesthood and Aaronic Priesthood. The Melchizedek Priesthood because Melchizedek was such a great high priest. Before his day it was called the Holy Priesthood, after the Order of the Son of God. But out of respect or reverence to the name of the Supreme Being, to avoid the too frequent repetition of his name, the church, in ancient days, called that priesthood after Melchizedek. The lesser priesthood is an appendage to the Melchizedek Priesthood. It is called the Aaronic Priesthood because it was conferred on Aaron and his sons throughout all their generations.
The offices, or ranks, of the Melchizedek order (in roughly descending order) include apostle, seventy, patriarch, high priest, and elder. The offices of the Aaronic order are bishop, priest, teacher, and deacon. The manner of ordination consists of the laying on of hands by two or more men holding at least the office being conferred while one acts as voice in conferring the priesthood and/or office and usually pronounces a blessing upon the recipient. Teachers and deacons do not have the authority to ordain others to the priesthood. All church members are authorized to teach and preach regardless of priesthood ordination so long as they maintain good standing within the church. The church does not use the term "holy orders."
Community of Christ.
Community of Christ has a largely volunteer priesthood, and all members of the priesthood are free to marry (as traditionally defined by the Christian community). The priesthood is divided into two orders, the Aaronic priesthood and the Melchisedec priesthood. The Aaronic order consists of the offices of deacon, teacher and priest. The Melchisedec Order consists of the offices of elder (including the specialized office of seventy) and high priest (including the specialized offices of evangelist, bishop, apostle, & prophet). Paid ministers include “appointees” and the general officers of the church, which include some specialized priesthood offices (such as the office of president, reserved for the three top members of the church leadership team). As of 1984, women have been eligible for priesthood, which is conferred through the sacrament of ordination by the laying-on-of-hands. While there is technically no age requirement for any office of priesthood, there is no automatic ordination or progression as in the LDS Church. Young people are occasionally ordained as deacon, and sometimes teacher or priest, but generally most priesthood members are called following completion of post secondary school education. In March 2007 a woman was ordained for the first time to the office of president.
Ordination of women.
The Roman Catholic Church does not ordain women to any of the orders and has officially declared that it does not have authority to ordain women as priests or bishops. "Ordaining" women as deaconesses is not a possibility in any sacramental sense of the diaconate, for a deaconess is not a female deacon but instead holds a position of lay service. As such, she does not receive the sacrament of holy orders. Many Anglican and Protestant churches ordain women, but in many cases, only to the office of deacon.
Various branches of the Orthodox churches, including the Greek Orthodox, currently set aside women as deaconesses. Some churches are internally divided on whether the Scriptures permit the ordination of women. When one considers the relative size of the churches (1.1 billion Roman Catholics, 300 million Orthodox, 590 million Anglicans and Protestants), it is a minority of Christian churches that ordain women. Protestants constitute about 27 percent of Christians worldwide, and most of their churches that do ordain women have only done so within the past century.
In some traditions women may be ordained to the same orders as men. In others women are restricted from certain offices. Women may be ordained bishop in the Old Catholic churches and in the Anglican/Episcopal churches in Scotland, Ireland, Wales, Cuba, Brazil, South Africa, Canada, US, Australia, Aotearoa New Zealand and Polynesia. The Church of England is still arguing about it. Continuing Anglican churches of the world do not permit women to be ordained. In some Protestant denominations, women may serve as assistant pastors but not as pastors in charge of congregations. In some denominations, women can be ordained to be an elder or deacon. Some denominations allow for the ordination of women for certain religious orders. Within certain traditions, such as the Anglican and Lutheran, there is a diversity of theology and practice regarding ordination of women.
The Roman Catholic Church, in accordance with its understanding of the theological tradition on the issue, and the definitive clarification found in the encyclical letter "Ordinatio Sacerdotalis" (1994) written by Pope John Paul II, officially teaches that it has no authority to ordain women as priests and thus there is no possibility of female priests at any time in the future.
Ordination of homosexual clergy.
The ordination of lesbian, gay, bisexual or transgender clergy who are sexually active, and open about it, represents a fiercely contested subject within many mainline Protestant communities. The majority of churches are opposed to such ordinations because they view homosexuality as incompatible with Biblical teaching and traditional Christian practice. Yet there are an increasing number of Christian congregations and communities that are open to ordaining people who are gay or lesbian. These are liberal Protestant denominations, such as the Episcopal Church the United Church of Christ, and the Evangelical Lutheran Church in America, plus the small Metropolitan Community Church, founded as a gay church, and the Church of Sweden where such clergy may serve in senior clerical positions.
The issue of ordination has caused particular controversy in the worldwide Anglican Communion, following the approval of Gene Robinson to be the Bishop of New Hampshire in the US Episcopal Church.

</doc>
<doc id="13633" url="http://en.wikipedia.org/wiki?curid=13633" title="Homer">
Homer

Homer (Ancient Greek: Ὅμηρος ], "Hómēros") is best known as the author of the "Iliad" and the "Odyssey". He was believed by the ancient Greeks to have been the first and greatest of the epic poets. Author of the first known literature of Europe, he had a lasting effect on the Western canon.
Whether and when he lived is unknown. Herodotus estimates that Homer lived 400 years before his own time, which would place him at around 850 BCE. Pseudo-Herodotus estimates that he was born 622 years before Xerxes I placed a pontoon bridge over the Hellespont in 480 BCE, which would place him at 1102 BCE, 168 years after the fall of Troy in 1270 BCE. These two end points are 252 years apart, representative of the differences in dates given by the other sources.
The importance of Homer to the ancient Greeks is described in Plato's "Republic", which portrays him as the protos didaskalos, "first teacher", of the tragedians, the hegemon paideias, "leader of Greek culture", and the ten Hellada pepaideukon, "teacher of [all] Greece". Homer's works, which are about fifty percent speeches, provided models in persuasive speaking and writing that were emulated throughout the ancient and medieval Greek worlds.
Fragments of Homer account for nearly half of all identifiable Greek literary papyrus finds in Egypt.
Period.
The chronological period of Homer depends on the meaning to be assigned to the word “Homer.” If the works attributed either wholly or partially to a blind poet named Homer, were really authored by such a person, then he must have had biographical dates, or a century or other historical period, which can be described as the life and times of Homer. If on the other hand Homer is to be considered a mythical character, the legendary founder of a guild of rhapsodes called the Homeridae, then “Homer” means the works attributed to the rhapsodes of the guild, who might have composed primarily in a single century or over a period of centuries. And finally, much of the geographic and material content of the Iliad and Odyssey appear to be consistent with the Aegean Late Bronze Age, the time of the floruit of Troy, but not the time of the Greek alphabet. The term “Homer” can be used to mean traditional elements of verse known to the rhapsodes from which they composed oral poetry, which transmitted information concerning the culture of Mycenaean Greece. This information is often called “the world of Homer” (or of Odysseus, or the Iliad). The Homeric period would in that case cover a number of historical periods, especially the Mycenaean Age, prior to the first delivery of a work called the Iliad.
Concurrent with the questions of whether there was a biographical person named Homer, and what role he may have played in the development of the currently known texts, is the question of whether there ever was a uniform text of the Iliad or Odyssey. Considered word-for-word, the printed texts as we know them are the product of the scholars of the last three centuries. Each edition of the Iliad or Odyssey is a little different, as the editors rely on different manuscripts and fragments, and make different choices as to the most accurate text to use. The term “accuracy” reveals a fundamental belief in an original uniform text. The manuscripts of the whole work currently available date to no earlier than the 10th century. These are at the end of a missing thousand-year chain of copies made as each generation of manuscripts disintegrated or were lost or destroyed. These numerous manuscripts are so similar that a single original can be postulated.
The time gap in the chain is bridged by the scholia, or notes, on the existing manuscripts, which indicate that the original had been published by Aristarchus of Samothrace in the 2nd century BCE. Librarian of the Library of Alexandria, he had noticed a wide divergence in the works attributed to Homer, and was trying to restore a more authentic copy. He had collected several manuscripts, which he named: the Sinopic, the Massiliotic, etc. The one he selected for correction was the koine, which Murray translates as “the Vulgate”. Aristarchus was known for his conservative selection of material. He marked lines that he thought were spurious, not of Homer. The entire last book of the Odyssey was marked.
The koine in turn had come from the first librarian at Alexandria, Zenodotus, who flourished at the beginning of the 3rd century BCE. He also was attempting to restore authenticity to manuscripts he found in a state of chaos. He set the precedent by marking passages he considered spurious, and by filling in material that seemed to be missing himself. Neither Zenodotus nor Aristarchus mentioned any authentic master copy from which to make corrections. Their method was intuitive. The current division into 24 books each for the Iliad and Odyssey came from Zenodotus.
Murray rejects the concept that an authoritative text for the Vulgate existed at the time of Zenodotus. He resorts to the fragments, the quotations of Homer in other works. About 200 existed at the time Murray wrote. Some of these match the current texts, some seem to paraphrase them, and some are not represented at all. Murray cites the Shield of Achilles, which also appears as the Shield of Heracles in Hesiod. Murray concludes that the epic poems were still in "a fluid state". He presents 150 BCE as the date after which the text solidifies around the Vulgate. Of the 5th century BCE, Murray said "'Homer' meant to them … 'the author of the Iliad and the Odyssey', but we cannot be sure that either … was exactly what we mean by those words."
The earliest mention of a work of Homer was by Callinus, a poet who flourished about 750 BCE. He attributed the "Thebais", an epic about the attack on Boeotian Thebes by the epigonoi, to Homer. The Thebais was written about the time of the appearance of the Greek alphabet, but it could have been originally oral. The Iliad is mentioned by name in Herodotus with regard to the early 6th century, but there is no telling what Iliad that is. Almost all the ancient sources from the very earliest appear determined that a Homer, author of the Iliad and Odyssey, existed. The author of the Hymn to Apollo identifies himself in the last verse of the poem as a blind man from Chios.
Nevertheless it is possible to make a case that Homer was only a mythological character, the supposed founder of the Homeridae. Martin West has asserted that "Homer" is "not the name of a historical poet, but a fictitious or constructed name." Oliver Taplin, in the Oxford History of the Classical World’s article on Homer, announces that the elements of his life “are largely … demonstrable fictions.” Another attack on the biographical details comes from G.S. Kirk, who said: "Antiquity knew nothing definite about the life and personality of Homer." Taplin prefers instead to speak of Homer as “a historical context for the poems.” His dates for this context are 750-650 BCE, without considering Murray’s “fluid state.”
With or without Homer, according to Murray, there is little likelihood that the Iliad and Odyssey of the early sources are the ones we know. Based on the fact that the Iliad was recited at the Panathenaic Games, which started in 566 BCE, Gregory Nagy selects a date of the 6th century for the fixation of the epics, as opposed to Murray’s 150 BCE. All of these views are only philologic. Regardless of whether there was or was not a Homer, or whether the texts of the Homerica were or were not close to the ones that exist today, philology alone does not shed any light on the similarities between Mycenaean culture and the geographical and material props of the world of Homer.
Archaeology, however, continues to support the theory that much detailed information survived in the form of formulae and stock pieces to be combined creatively by the rhapsodes of later centuries. A number of combined archaeological and philological works have been written on the topic, such as Denys Page’s “History and the Homeric Iliad” and Martin P. Nilsson’s “The Mycenaean Origin of Greek Mythology.” The linguist, Calvert Watkins, went so far as to seek an inherited Proto-Indo-European language origin for some epithets and the epic verse form. If he is correct, the stock themes and verses of rhapsodes may be far older than the Trojan War, which would, in that case, be only the latest opportunity for an epic.
Homer cannot be presented as a single author of a set of works as they are today describing events of history that are more or less real, apart from the obvious mythology. Homeric studies are like the proverbial apple of philosophy. There is no beginning and no end. No matter what starting problem is selected, it leads immediately to another. The total sum of all the problems is known as the Homeric question, which is, of course, generic and not singular.
Life and legends.
"Lives of Homer".
Various traditions have survived purporting to give details of Homer's birthplace and background. The satirist Lucian, in his "True History", describes him as a Babylonian called Tigranes, who assumed the name Homer when taken "hostage" ("homeros") by the Greeks. When the Emperor Hadrian asked the Oracle at Delphi about Homer, the Pythia proclaimed that he was Ithacan, the son of Epikaste and Telemachus, from the "Odyssey". These stories were incorporated into the various "lives of Homer", "compiled from the Alexandrian period onwards".
The "lives of Homer" refer to a set of longer fragments on the topic of the life and works of Homer written by authors who for the most part remain anonymous. Some were attributed to more famous authors. In the 20th century CE, all the vitae were gathered into a standard reference work by Thomas W. Allen and made a part of Homeri Opera, "the Works of Homer", first published in 1912 by Oxford University Press. This edition has been informally known as "the Oxford Homer" and the Vitae Homeri section as "the lives of Homer" or just "the lives". The relevant part of Volume V in scholarship on the vitae is often called just "Allen" with page numbers denoting the vita.
Allen records some several vitae collected from various sources: the Vita Herodotea, pp 192–218, now known as Pseudo-Herodotus, because probably not of Herodotus; the Certamen Homeri et Hesiodi, pp 225–238, with fragments on 218-221; the two Plutarchi vitae (now Pseudo-Plutarch), pp 238–244 and pp 244–245 respectively; some vitae identified as IV (elsewhere known as Vita Scorialenses I), pp 245–246, V (Vita Scorialensis II), pp 247–250, VI (Vita Romana), pp 250–253, and finally VII, which is really three, giving extracts from Eustathius, pp 253–254 and 255, John Tzetzes, pp 254–255, and Suidas, pp 256–268, now identified as Hesychius Milesius. Nagy reorganizes the list into eleven, Vita 1 through Vita 10, with Plutarch being divided into 3a and 3b. In addition he adds Vita 11 from the Chrestomathia of Proclus, pp 99–102. The varying and contradictory biographical information in these sources is termed humorously by Nagy "Variations on a Theme of Homer" after the model of the names of certain musical compositions.
Etymological theories.
“Homer” is a name of unknown origin, ostensibly Greek. However, many Greek words, and especially names in the east, where the Greeks were in contact with eastern language speakers, were loans, approximations, or paraphrases of foreign words. For example, Darius to the Greeks was Dārayava(h)uš, "holding firm the good", to himself and the other Old Persian speakers. Cadmus, overthrown king of Thebes, reported to have been Phoenician, was probably seen as an “easterner,” from Hebrew/Phoenician qdm, "the east". Priam was perhaps from Luwian Priya-muwa-, which means "exceptionally courageous.” Many names have a derivation from a foreign language but also fit or partially fit derivations from Proto-Indo-European through Greek. There are but few rules to assist the linguist in identifying which is the most likely.
Etymologies for the name Homeros reach beyond the Greek. On the one hand, he may have a Hellenized Phoenician name. West conjectures a Phoenician prototype for Homer's name as a patronymic, Homeridae (male progeny from the line of Homer), "*benê ômerîm" ("sons of speakers"); "id est" professional tale-tellers. Here the patronymic would designate the guild. In Greek, the Homer in Homeridae would have to be in the singular, the implied single ancestor of a clan practicing a hereditary trade. The hypothetical semitic ancestors are in the plural; where "ben" can be used for one "father", the id- construction can never designate a plural father.
On the other hand, Proto-Indo-European etymologies are also available. The poet's name is homophonous with Greek ὅμηρος ("hómēros"), "hostage" (or "surety"). This word is in the Attic dialect, and was a word in general use. In the vitae of Pseudo-Herodotus and Plutarch, it had a relatively obscure meaning: "blind", which is interpreted as meaning "he who accompanies; he who is forced to follow" a guide. The geographic specificity of the word typically is explained by a presumption that it was known mainly in Aeolis on the coast of Asia Minor, the locale where Homer performed, and therefore is a word of the Aeolic dialect. There is no linguistic reason other than usage for thinking so. The letter eta brands the word as being East Greek, as opposed to the West Greek Cretan form, which has an alpha instead. Ionic and Attic also were East Greek. Proclus' Chrestomathia, however, explicitly says, "the tuphloi were called homeroi by the Aeolians" Throughout Pseudo-Herodotus, ὅμηρος ("hómēros") is synonymous with the standard Greek τυφλός ("tuphlós"), meaning 'blind'.
The characterization of Homer as a blind bard begins in extant literature with the last verse in the Delian "Hymn to Apollo", the third of the "Homeric Hymns", later cited to support this notion by Thucydides. The author of the hymn claims to be a blind bard from Chios. This claim is quite different from the mere attribution of the hymn to Homer by a third party from a different time. The claim cannot be false without the supposition of a deliberate fraud, rather than a mere mistake. Also, critics have long taken as self-referential a passage in the "Odyssey" describing a blind bard, Demodocus, in the court of the Phaeacian king, who recounts stories of Troy to the shipwrecked Odysseus.
Despite the insistence of the surviving sources that Homer was blind, there are many serious objections to the "blind" theory. A few of the vitae imply that he was not blind. If he could not write, then he was illiterate and incapable of composition. A large poem would have been beyond the capacity of human memory without the assistance of written cues. Moreover, the images in the poem are very graphic, but a blind man would never have experienced the scenes of the images. Answers exist to all the objections. The example of John Milton, who composed and dictated "Paradise Lost" while totally blind, demonstrates that a blind man can compose an epic. Albert B. Lord's "The Singer of Tales", on the topic of epics sung by modern rhapsodes, shows that epics of that size have been in fact being composed spontaneously from memorized elements in modern times. The problem of visual cues can be solved if Homer can be presumed not to have been blind from birth, but to have become blind, which is the point of view of Pseudo-Herodotus.
In the latter source, Homer, after losing his sight to disease, embarks on a career as a wandering rhapsode, or impromptu composer of poems at public gatherings. Either at the beginning of his career or early in it, he assumes a stage name, reputedly "the blind man", which declares himself to be in the category of blind prophets, who see with inspired inner vision, but not with outer, bringing a sort of divine glamor to the performance. Not all the vitae agree on the meaning of the name. There is nothing biological about the Greek roots. The word is segmented Hom-eros, where Hom is from Greek homou, "together", and the second -ar- in arariskein, "join together", the eta in -eros being East Greek. The "blind" meaning joins together the blind man and his guide. Other unions are certainly possible, provided they are attested. Gregory Nagy uses a phrase, phone homereusai, "fitting [the song] together with the voice" found in Hesiod, a contemporary of Homer, to interpret Homeros as "he who fits (the Song) together".
Consideration of the name as a type leaves open the possibility that any rhapsode could conform to it; that is, there was no biographic original named Homer. West says "The probability is that 'Homer' was not the name of a historical Greek poet but is the imaginary ancestor of the Homeridai; such guild-names in -"idai" and -"adai" are not normally based on the name of an historical person". They were upholding their function as rhapsodes or "lay-stitchers" specialising in the recitation of Homeric poetry.
Cultural background.
William Ihne examining the sources counted as many 19 locations in classical times that claimed Homer as a citizen, including Athens, which accepted Smyrna as Homer’s native city, but insisted the city was its colony. The cause of these multiple claims was civic competition for the honor. Ihne chose Smyrna because some of the Vitae identify the word Homer as Aeolic, and Smyrna had an Aeolic background. These circumstances give precedence to the longest, most detailed vita, that of Pseudo-Herodotus, which is one of the sources that identify Smyrna as originally Aeolian.
The Aeolians were one of the three major ethnic groups of ancient Greece, the other two being Ionians and Dorians. Aeolians came mainly from Thessaly, occupying also Boeotia at an early date, after the Trojan War, in parallel to the occupation of Peloponnesus by the Dorians. They had their own dialect of East Greek. Hesiod as a Boeotian was a member of the group, which is substantiated by the Aeolic phrases related to the name of Homer found in his works. The Aeolians colonized the northwest coast of Asia Minor, calling their region Aeolis, and Lesbos. The villages to which they immigrated were already populated by the descendants of the Trojan War population. They were keeping the lore alive, according to Pseudo-Herodotus. Aeolis extended from the coast opposite Lesbos to Smyrna on the edge of Ionia. The Aeolian League contained 12 cities, including Smyrna. To the south were the 12 cities, or dodacapolis, of the Ionian League. At about 688 BCE Smyrna was taken by Colophonians who had ostensibly come to a festival there and passed into Ionian hands.
The political relevance of the two leagues came to a practical end in the latter half of the 5th century BCE when most of the cities around the Aegean joined, or were forced to join, the Delian League, a koine implementing the hegemony of Athens. Each city must contribute men and ships or money to a common defense force. The treasury was kept at Athens. The details and conjoined events are the topic of Thucydides’ History of the Peloponnesian War. Inscriptions from those times offer a basis for the study of Aeolic. Buck distinguished three dialects, Thessalian, Boeotian, and Lesbian.
The Ionian cities in Asia Minor spoke a dialect of Ionic. In the border region between Ionia and Aeolis it was modified to include features taken from Aeolic, creating an Ionic-Aeolic mixture similar to that of the Homeric poems. For example, Chios had always been a member of the Ionian League, and yet Chian “contains a few special characteristics, which are of Aeolic origin.” The same sort of admixture did not occur at the Ionic-Dorian border in southwestern Anatolia.
From the fact that Lesbian acquired more Ionic features in poetry over the course of time Janko argues for “a northward expansion of Ionian population and speech at the expense of the Aeolians.” Aeolic was gradually assimilating to Ionic, but after the 5th century BCE both began to assimilate to the now widespread sister dialect of Ionic, Attic, and the koine that developed from it in the Hellenistic period. Attic began to appear in the inscriptions of Ionia in the 4th century BCE and had displaced Ionian by about 100 BCE. In 281 BC the new kingdom of Pergamon acquired the Aeolic coast of Anatolia, separating Lesbian, which was gone from the kingdom by the 3rd century BCE. Lesbian went on until the 1st century CE and was the last Aeolic dialect to disappear.
G.S. Kirk, who tends to be somewhat skeptical concerning the biographic details given in the vitae, at least extends a limited credibility to some basic circumstances as “at all plausible.” Homer is most frequently said to have been born in the Ionian region of Asia Minor, at Smyrna, or on the island of Chios, dying on the Cycladic island of Ios. These areas were either Aeolian or partially so. Smyrna had not yet been taken by the Ionians. Chios had been settled by pre-Hellenic tribesmen from Thessaly, but the language remains unknown. They may have been Aeolic-speaking. The association with Chios dates back to at least Semonides of Amorgos, who cited "Iliad" 6.146 as by "the man of Chios". An eponymous bardic guild, known as the Homeridae (sons of Homer), or "Homeristae" ('Homerizers') existed there, tracing descent from an ancestor of that name. On Ios were used some words known to be Aeolic; for example, Homêreôn was one of the names for a month in the calendar of Ios. The Smyrna connection is alluded to in the original name posited for him by several vitae: Melesigenes, “born of Meles", a river which flowed by that city.
The poems give evidence of familiarity with the natural details and place-names of this area of Asia Minor; for example, Homer refers to meadow birds at the mouth of the Caystros, a storm in the Icarian sea, and mentions that women in Maeonia and Caria stain ivory with scarlet. However, Homer also had a geographical knowledge of all Mycenaean Greece that has been verified by discovery of most of the sites. Wilhelm Dörpfeld, the classical archaeologist, suggests that Homer had visited many of the places and regions which he describes in his epics, such as Mycenae, Troy and more. According to Diodorus Siculus, Homer had even visited Egypt.
Biographical assertions.
Some vitae depict Homer as a wandering minstrel, like Thamyris or Hesiod, who walked as far as Chalkis to sing at the funeral games of Amphidamas. We are given the image of a "blind, begging singer who hangs around with little people: shoemakers, fisherman, potters, sailors, elderly men in the gathering places of harbour towns". The poems, on the other hand, give us evidence of singers at the courts of the nobility. There is a strong aristocratic bias in the poems demonstrated by the lack of any major protagonists of non-aristocratic stock, and by episodes such as the beating down of the commoner Thersites by the king Odysseus for daring to criticize his superiors. Scholars are divided as to which category, if any, the court singer or the wandering minstrel, the historic "Homer" belonged.
Most of the 12 vitae have little concern for historicty. Scorialenses I says “we only hear the report, and do not know anything.” Most therefore report several origin stories. They are typically at least in part mythical. Whether the latter are given unfeigned credibility is not clear. For instance, Homer was the son of the river Meles and a nymph. Pseudo-Plutarch I, relying less on mythology, presents an alternative genealogy that makes Homer and Hesiod cousins. The only account that presumes a historical character and a real-life setting without resorting to mythology is the more lengthy Pseudo-Herodotus.
In the vita, a colonist of Cyme, Cleanax of Argos, was given custody of the orphaned Chretheis, daughter of deceased friends and fellow colonists, by her parents before their deaths. When she became pregnant without a husband he sent her in disgrace to the new colony of Smyrna in the custody of a protector, a friend from Boeotia, Ismenias. Attending a festival on the banks of the River Meles she gave birth unexpectedly to a son, whom she called Melesi-genes, “river-born.” A single mother, she left the protection of Ismenias, becoming an itinerant laborer. She found work with a schoolmaster, Phemius, processing wool he had been paid by the students. A relationship having developed, he convinced her to live with him (syn-oikein), promising to make the boy his own son, support and educate him.
A prodigy, the young Melisigenes was successful in school. On the deaths of Phemius and his mother years later he inherited the school. He also opened his home hospitably to merchants passing through. A merchant, Mentes, convinced him to leave the school and sign on as a seaman in his ship. He is said to have made the most of his ports of call by researching each one and taking written notes. Having contracted an eye disease he was put ashore for treatment and recovery with a friend of the captain in Ithaca. He used the time to research the story of Odysseus. Having recovered on that occasion, he later suffered a relapse in Colophon, losing his vision altogether.
Retiring to Smyrna he decided to pursue the recitation of poetry. When his resources were exhausted, he went on the road looking for opportunities. In Neonteichus, a colony of Cyme, he stopped by chance before the shop of a shoemaker, Tychius, and began to beg in dactylic hexameter, stringing formulae together. Thus began a habit that he kept for the rest of his life, of communicating in verse about ordinary matters to advertise his skills. On this occasion he was successful. The shoemaker opened his home and allowed him to recite in the shop. He became for a time a fixture in Neontychus, but unable to prosper there, he returned to Cyme. In Larissa en route he was hired to write an epitaph for the tomb of Midas, deceased king of Phrygia.
In Cyme he recited in the salons. He was so successful that he asked the city council (boule) in session for support at public expense, the quid pro quo being that he would make the city famous. One of the councilmen argued that if they were going to support homeroi, or “blind men,” they would soon have a useless crowd of them in Cyme. The measure was defeated. He subsequently departed for Phocaea, an Ionian city. He rhymed, “I will endure the fate that the god gave me when I was born, bearing defeat with a patient heart, but no longer do my limbs wish to remain in the sacred streets of Cyme.” Then he cursed the city, that no poet should be born there to make them famous. Meanwhile, hearing of the incident, the people began to call him Homeros, “the blind man.”
After frequenting the salons of Phocaea without much success, he entered into an agreement with one Thestorides, who would support him in exchange for the title to the authorship of his work. Thestorides wrote down the current works as they were orally composed. After a time he abandoned Phocaea, breaking the support agreement, and went clandestinely to Chios to found a school there, reciting Homer’s verses as his own. Some merchants informed Homer that his verses were being recited on Chios under another name. Attempting to find passage to Chios Homer was turned down by some fishermen but was taken by some woodcutters to the beach at Erythraea opposite. From there he found passage with other fishermen, who landed him at an unnkown beach.
The location was the Troad, near Mount Ida. Homer, following the sound of goats, was beset by the herd dogs, and rescued by the herder, Glaucus. After a night of regaling Glaucus with verses by the campfire, Homer was introduced to his master the next day, who hired him as a tutor for his children. He became successful for the first time, composing many of the poems. Hearing of his fame, Thestorides abandoned the school at Chios. Crossing to the island, Homer founded another, prospered, married, had two daughters, and wrote the Iliad and Odyssey. Going on tour to mainland Greece he stopped at Samos for the festivals there. Heading for Athens in the spring his ship was blown to Ios. While waiting for favorable winds he grew ill and died. The author then goes on to make a case that Homer was Aeolian, not Ionian. He gives the date of his birth as 622 years before Xerxes, which if true would make his mention of writing anachronist if the writing was in the Greek alphabet.
Works attributed to Homer.
The attribution of a work is not the same meaning as a known authorship, the difference being an element of doubt. The Greeks of the sixth and early fifth centuries BCE understood by the works of "Homer", generally, "the whole body of heroic tradition as embodied in hexameter verse". The entire Epic Cycle was included. The genre included further poems on the Trojan War, such as the "Little Iliad", the "Nostoi", the "Cypria", and the "Epigoni", as well as the Theban poems about Oedipus and his sons. Other works, such as the corpus of "Homeric Hymns", the comic mini-epic "Batrachomyomachia" ("The Frog-Mouse War"), and the "Margites", were also attributed to him. Two other poems, the "Capture of Oechalia" and the "Phocais" were also assigned Homeric authorship.
Epics.
Herodotus mentions both the "Iliad" and the "Odyssey" as works of Homer. He quotes a few lines from them both, which are the same in today’s editions. The passage quoted from the Iliad mentions that Paris stopped at Sidon before bringing Helen to Troy. From the fact that the Cypria has Paris going directly to Troy from Sparta, Herodotus concludes that it was not written by Homer. The doubting process had begun.
In "Works and Days", Hesiod says that he crossed to Euboea to contend in the games held by the sons of Amphidamas at Chalcis. There he won with a hymnos and took away the prize of a tripod, which he dedicated to the Muses of Mount Helicon, where he first began with "aoide", “song.” One of the vitae, the “Certamen”, picks up this theme. Homer and Hesiod were contemporaries, it says. They both attended the funeral games of Amphidamas, conducted by his son, Ganyctor, and both contended in the contest of sophia, “wit.” In it, one was required to ask a question of the other, who must reply in verse.
Unable to decide, the judge had them each recite from their poems. Hesiod quoted Works and Days; Homer, ‘Iliad’, both as they are now, but neither poem can have been the modern. Hesiod cannot have described beforehand the very event in which he was participating. The Iliad is supposed to have been written already. It is not called that, however. The victory was given to Hesiod because his poem was about peace, but Homer’s, about war.
After the contest, Homer continued his wandering, composing and reciting epic poetry. The “Certamen” mentions the Thebais, quoting the first line, which differs but little from the first line of the Iliad as it is now. It had 7000 lines, as did the subsequent Epigoni, with a similar first line. The “Certamen” qualifies the attribution to Homer with “some say ….” Subsequently he wrote the epitaph for Midas’ tomb, for which he got a silver bowl, and then the Odyssey in 12,000 lines (today’s is 12110). He had already written the Iliad in 15,500 lines (today’s is 15693). Just these three epics alone are 34,500 lines, word-for-word, we are asked to believe, without reference to the rest of the prodigious Epic Cycle. Then he went to Athens, and to Argos, where he delivered lines 559-568 of Book 2 of the Iliad with the addition of two more not in the current version. Subsequently he went to Delos, where he delivered the Hymn to Apollo, and was made a citizen of all the Ionian states. Going finally to Ios he slipped on some clay and suffered a fatal fall.
The term “Epic Cycle” ("Epikos Kuklos") refers to a series of ten epic poems written by different authors purporting to tell an interconnected sequence of stories covering all Greek mythology. Themes were selected from them for Greek drama as well. The name appears in the Chrestomathia of Eutychius Proclus, a synopsis of Greek literature, known only through further abridged fragments written by Photios I of Constantinople. No etymology was given. Evelyn-White hypothesizes that they were “written round” the Iliad and Odyssey and had a “clearly imitative” structure. In this view Homer need have written no more than the Iliad, or the Iliad and Odyssey, with the Homeridae responsible for all the rest. The unity of theme and structure came from the close association of the authors in the guild or school.
Proclus does not subscribe to the authorships of the “Certamen”. He provides the names of other authors where they were available in his sources. These 10 epics, of which only Photius’ abridgements of Proclus’ synopses survive, and scattered fragments of other authors in other times, are as follows. First and oldest, the “War of the Titans” ("Titanomachia"), eight fragments, is said to have been written by either Eumelus of Corinth, floruit 760-740 BCE, or Arctinus of Miletus, floruit in the First Olympiad, starting 776 BCE.
The Theban Cycle consists of three epics: “Story of Oedipus” (Oidipodeia), 6600 lines by Cinaethon of Sparta, floruit 764 BCE; “Thebaid” (Thebais), attributed to Homer; and “Epigoni (Epigonoi), attributed to Homer. The Trojan Cycle consists of six epics and the Iliad and Odyssey, eight in all: “Cyprian Lays” (kupria) in 11 books, attributed to either Homer, Stasinus, a younger contemporary of Homer, or one Hegesias; “Aethiopis” (Aithiopis) in five books, sequent of the Iliad, which is a sequent of Cypria, by Arctinus; “Little Iliad” (Ilias Mikra) in four books by Lesches of Mitylene, floruit 660 BCE; “Sack of Ilium” (Iliou Persis) by Arctinus; “Returns” (Nostoi) by Agias of Troezen, floruit 740 BCE; and “Telegony” (Telegonia), by Eugammon of Cyrene, floruit 567 BCE.
Identity and authorship.
The idea that Homer was responsible for just the two outstanding epics, the "Iliad" and the "Odyssey", did not win consensus until 350 BCE. While many, such as Gregory Nagy, find it unlikely that both epics were composed by the same person, others, such as W. B. Stanford, argue that the stylistic similarities are too consistent to support the theory of multiple authorship. One view which attempts to bridge the differences holds that the "Iliad" was composed by "Homer" in his maturity, while the "Odyssey" was a work of his old age. The "Batrachomyomachia", "Homeric Hymns" and cyclic epics are generally agreed to be later than the "Iliad" and the "Odyssey".
Most scholars agree that the "Iliad" and "Odyssey" underwent a process of standardisation and refinement out of older material beginning in the 8th century BCE. An important role in this standardisation appears to have been played by the Athenian tyrant Hipparchus, who reformed the recitation of Homeric poetry at the Panathenaic festival. Many classicists hold that this reform must have involved the production of a canonical written text.
Other scholars still support the idea that Homer was a real person. Since nothing is known about the life of this Homer, the common joke—also recycled with regard to Shakespeare—has it that the poems "were not written by Homer, but by another man of the same name." Samuel Butler argues, based on literary observations, that a young Sicilian woman wrote the "Odyssey" (but not the "Iliad"), an idea further pursued by Robert Graves in his novel "Homer's Daughter" and Andrew Dalby in "Rediscovering Homer".
Independent of the question of single authorship is the near-universal agreement, after the work of Milman Parry, that the Homeric poems are dependent on an oral tradition, a generations-old technique that was the collective inheritance of many singer-poets ("aoidoi"). An analysis of the structure and vocabulary of the "Iliad" and "Odyssey" shows that the poems contain many formulaic phrases typical of extempore epic traditions; even entire verses are at times repeated. Parry and his student Albert Lord pointed out that such elaborate oral tradition, foreign to today's literate cultures, is typical of epic poetry in a predominantly oral cultural milieu, the key words being "oral" and "traditional". Parry started with "traditional": the repetitive chunks of language, he said, were inherited by the singer-poet from his predecessors, and were useful to him in composition. Parry called these repetitive chunks "formulas".
Exactly when these poems would have taken on a fixed written form is subject to debate. The traditional solution is the "transcription hypothesis", wherein a non-literate "Homer" dictates his poem to a literate scribe between the 8th and 6th centuries BCE. The Greek alphabet was introduced in the early 8th century BCE, so it is possible that Homer himself was of the first generation of authors who were also literate. The classicist Barry B. Powell suggests that the Greek alphabet was invented c. 800 BCE by one man, whom he calls the "adapter," in order to write down oral epic poetry. More radical Homerists like Gregory Nagy contend that a canonical text of the Homeric poems as "scripture" did not exist until the Hellenistic period (3rd to 1st century BCE).
New methods also try to elucidate the question. Combining information technologies and statistics stylometry analyzes various linguistic units: words, parts of speech, and sounds. Based on the frequencies of Greek letters, a first study of Dietmar Najock particularly shows the internal cohesion of the "Iliad" and the "Odyssey". Taking into account the repartition of the letters, a recent study of Stephan Vonfelt highlights the unity of the works of Homer compared to Hesiod. The thesis of modern analysts being questioned, the debate remains open.
Homeric studies.
The study of Homer is one of the oldest topics in scholarship, dating back to antiquity. The aims and achievements of Homeric studies have changed over the course of the millennia. In the last few centuries, they have revolved around the process by which the Homeric poems came into existence and were transmitted over time to us, first orally and later in writing.
Some of the main trends in modern Homeric scholarship have been, in the 19th and early 20th centuries, "Analysis" and "Unitarianism" (see Homeric Question), schools of thought which emphasized on the one hand the inconsistencies in, and on the other the artistic unity of, Homer; and in the 20th century and later "Oral Theory", the study of the mechanisms and effects of oral transmission, and "Neoanalysis", the study of the relationship between Homer and other early epic material.
Homeric dialect.
The language used by Homer is an archaic version of Ionic Greek, with admixtures from certain other dialects, such as Aeolic Greek. It later served as the basis of Epic Greek, the language of epic poetry, typically in dactylic hexameter.
Homeric style.
Aristotle remarks in his "Poetics" that Homer was unique among the poets of his time, focusing on a single unified theme or action in the epic cycle.
The cardinal qualities of the style of Homer are well articulated by Matthew Arnold:[T]he translator of Homer should above all be penetrated by a sense of four qualities of his author:—that he is eminently rapid; that he is eminently plain and direct, both in the evolution of his thought and in the expression of it, that is, both in his syntax and in his words; that he is eminently plain and direct in the substance of his thought, that is, in his matter and ideas; and finally, that he is eminently noble.
The peculiar rapidity of Homer is due in great measure to his use of hexameter verse. It is characteristic of early literature that the evolution of the thought, or the grammatical form of the sentence, is guided by the structure of the verse; and the correspondence which consequently obtains between the rhythm and the syntax—the thought being given out in lengths, as it were, and these again divided by tolerably uniform pauses—produces a swift flowing movement such as is rarely found when periods are constructed without direct reference to the metre. That Homer possesses this rapidity without falling into the corresponding faults, that is, without becoming either fluctuant or monotonous, is perhaps the best proof of his unequalled poetic skill. The plainness and directness of both thought and expression which characterise him were doubtless qualities of his age, but the author of the "Iliad" (similar to Voltaire, to whom Arnold happily compares him) must have possessed this gift in a surpassing degree. The "Odyssey" is in this respect perceptibly below the level of the "Iliad".
Rapidity or ease of movement, plainness of expression, and plainness of thought are not distinguishing qualities of the great epic poets Virgil, Dante, and Milton. On the contrary, they belong rather to the humbler epico-lyrical school for which Homer has been so often claimed. The proof that Homer does not belong to that school—and that his poetry is not in any true sense ballad poetry—is furnished by the higher artistic structure of his poems and, as regards style, by the fourth of the qualities distinguished by Arnold: the quality of nobleness. It is his noble and powerful style, sustained through every change of idea and subject, that finally separates Homer from all forms of ballad poetry and popular epic.
Like the French epics, such as the "Chanson de Roland", Homeric poetry is indigenous and, by the ease of movement and its resultant simplicity, distinguishable from the works of Dante, Milton and Virgil. It is also distinguished from the works of these artists by the comparative absence of underlying motives or sentiment. In Virgil's poetry, a sense of the greatness of Rome and Italy is the leading motive of a passionate rhetoric, partly veiled by the considered delicacy of his language. Dante and Milton are still more faithful exponents of the religion and politics of their time. Even the French epics display sentiments of fear and hatred of the Saracens; but, in Homer's works, the interest is purely dramatic. There is no strong antipathy of race or religion; the war turns on no political events; the capture of Troy lies outside the range of the "Iliad"; and even the protagonists are not comparable to the chief national heroes of Greece. So far as can be seen, the chief interest in Homer's works is that of human feeling and emotion, and of drama; indeed, his works are often referred to as "dramas".
History and the "Iliad".
The excavations of Heinrich Schliemann at Hisarlik in the late 19th century provided initial evidence to scholars that there was an historical basis for the Trojan War. Research into oral epics in Serbo-Croatian and Turkic languages, pioneered by the aforementioned Parry and Lord, began convincing scholars that long poems could be preserved with consistency by oral cultures until they are written down. The decipherment of Linear B in the 1950s by Michael Ventris (and others) convinced many of a linguistic continuity between 13th century BCE Mycenaean writings and the poems attributed to Homer.
It is probable, therefore, that the story of the Trojan War as reflected in the Homeric poems derives from a tradition of epic poetry founded on a war which actually took place. It is crucial, however, not to underestimate the creative and transforming power of subsequent tradition: for instance, Achilles, the most important character of the "Iliad", is strongly associated with southern Thessaly, but his legendary figure is interwoven into a tale of war whose kings were from the Peloponnese. Tribal wanderings were frequent, and far-flung, ranging over much of Greece and the Eastern Mediterranean. The epic weaves brilliantly the "disiecta membra" (scattered remains) of these distinct tribal narratives, exchanged among clan bards, into a monumental tale in which Greeks join collectively to do battle on the distant plains of Troy.
Hero cult.
In the Hellenistic period, Homer was the subject of a hero cult in several cities. A shrine, the "Homereion", was devoted to him in Alexandria by Ptolemy IV Philopator in the late 3rd century BCE. This shrine is described in Aelian's 3rd century CE work "Varia Historia". He tells how Ptolemy "placed in a circle around the statue [of Homer] all the cities who laid claim to Homer" and mentions a painting of the poet by the artist Galaton, which apparently depicted Homer in the aspect of Oceanus as the source of all poetry.
A marble relief, found in Italy but thought to have been sculpted in Egypt, depicts the apotheosis of Homer. It shows Ptolemy and his wife or sister Arsinoe III standing beside a seated poet, flanked by figures from the "Odyssey" and "Iliad", with the nine Muses standing above them and a procession of worshippers approaching an altar, believed to represent the Alexandrine Homereion. Apollo, the god of music and poetry, also appears, along with a female figure tentatively identified as Mnemosyne, the mother of the Muses. Zeus, the king of the gods, presides over the proceedings. The relief demonstrates vividly that the Greeks considered Homer not merely a great poet but the divinely inspired reservoir of all literature.
Homereia also stood at Chios, Ephesus, and Smyrna, which were among the city-states that claimed to be his birthplace. Strabo (14.1.37) records an Homeric temple in Smyrna with an ancient "xoanon" or cult statue of the poet. He also mentions sacrifices carried out to Homer by the inhabitants of Argos, presumably at another Homereion.
Transmission and publication.
Though evincing many features characteristic of oral poetry, the "Iliad" and "Odyssey" were at some point committed to writing. The Greek script, adapted from a Phoenician syllabary around 800 BCE, made possible the notation of the complex rhythms and vowel clusters that make up hexameter verse. Homer's poems appear to have been recorded shortly after the alphabet's invention: an inscription from Ischia in the Bay of Naples, c. 740 BCE, appears to refer to a text of the "Iliad"; likewise, illustrations seemingly inspired by the Polyphemus episode in the "Odyssey" are found on Samos, Mykonos and in Italy, dating from the first quarter of the seventh century BCE. We have little information about the early condition of the Homeric poems, but in the second century BCE, Alexandrian editors stabilized this text from which all modern texts descend.
In late antiquity, knowledge of Greek declined in Latin-speaking western Europe and, along with it, knowledge of Homer's poems. It was not until the fifteenth century CE that Homer's work began to be read once more in Italy. By contrast it was continually read and taught in the Greek-speaking Eastern Roman Empire where the majority of the classics also survived. The first printed edition appeared in 1488 (edited by Demetrios Chalkokondyles and published by Bernardus Nerlius, Nerius Nerlius, and Demetrius Damilas in Florence, Italy).
One often finds books of the "Iliad" and "Odyssey" cited by the corresponding letter of the Greek alphabet, with upper-case letters referring to a book number of the "Iliad" and lower-case letters referring to the "Odyssey". Thus Ξ 200 would be shorthand for "Iliad" book 14, line 200, while ξ 200 would be "Odyssey" 14.200. The following table presents this system of numeration:
Selected bibliography.
English translations.
This is a partial list of translations into English of Homer's "Iliad" and "Odyssey".

</doc>
<doc id="13635" url="http://en.wikipedia.org/wiki?curid=13635" title="Hugo Gernsback">
Hugo Gernsback

Hugo Gernsback (August 16, 1884 – August 19, 1967), born Hugo Gernsbacher, was a Luxembourgian American inventor, writer, editor, and magazine publisher, best known for publications including the first science fiction magazine. His contributions to the genre as publisher were so significant that, along with the novelists H. G. Wells and Jules Verne, he is one person sometimes called "The Father of Science Fiction". In his honor, annual awards presented at the World Science Fiction Convention are named the "Hugos".
Biography.
Gernsback was born in the Bonnevoie neighborhood of Luxembourg City, to Berta (Dürlacher), a housewife, and Moritz Gernsbacher, a winemaker. His family was Jewish. Gernsback emigrated to the United States in 1904 and later became a naturalized citizen. He married three times: to Rose Harvey in 1906, Dorothy Kantrowitz in 1921, and Mary Hancher in 1951. In 1925, Hugo founded radio station WRNY which broadcast from the 18th floor of The Roosevelt Hotel in New York City and was involved in the first television broadcasts. He is also considered a pioneer in amateur radio.
Before helping to create science fiction, Gernsback was an entrepreneur in the electronics industry, importing radio parts from Europe to the United States and helping to popularize amateur "wireless." In April 1908 he founded "Modern Electrics", the world's first magazine about both electronics and radio, called "wireless" at the time. While the cover of the magazine itself contends it was a catalog, most historians note that it contained articles, features, and plotlines, qualifying it as a magazine. Under its auspices, in January 1909, he founded the Wireless Association of America, which had 10,000 members within a year. In 1912, Gernsback said that he estimated 400,000 people in the U.S. were involved in amateur radio. In 1913, he founded a similar magazine, "The Electrical Experimenter", which became "Science and Invention" in 1920. It was in these magazines that he began including scientific fiction stories alongside science journalism—including his own novel "Ralph 124C 41+" which he ran for 12 months from April 1911 in "Modern Electrics".
He died at Roosevelt Hospital in New York City on August 19, 1967.
Science fiction.
Gernsback started the modern genre of science fiction in 1926 by founding the first magazine dedicated to it, "Amazing Stories". The inaugural April issue comprised a one-page editorial and reissues of six stories, three less than ten years old and three by Poe, Verne, and Wells. He said he became interested in the concept after reading a translation of the work of Percival Lowell as a child. His idea of a perfect science fiction story was "75 percent literature interwoven with 25 percent science.". He also played a key role in starting science fiction fandom, by publishing the addresses of people who wrote letters to his magazines. So, the science fiction fans began to organize, and became aware of themselves as a movement, a social force; this was probably decisive for the subsequent history of the genre. He also created the term “science fiction”, though he preferred the term "scientifiction".
In 1929, he lost ownership of his first magazines after a bankruptcy lawsuit. There is some debate about whether this process was genuine, manipulated by publisher Bernarr Macfadden, or was a Gernsback scheme to begin another company. After losing control of "Amazing Stories", Gernsback founded two new science fiction magazines, "Science Wonder Stories" and "Air Wonder Stories". A year later, due to Depression-era financial troubles, the two were merged into "Wonder Stories", which Gernsback continued to publish until 1936, when it was sold to Thrilling Publications and renamed "Thrilling Wonder Stories". Gernsback returned in 1952–53 with "Science-Fiction Plus".
Gernsback was noted for sharp (and sometimes shady) business practices, and for paying his writers extremely low fees or not paying them at all. H. P. Lovecraft and Clark Ashton Smith referred to him as "Hugo the Rat."
As Barry Malzberg has said:
Gernsback's venality and corruption, his sleaziness and his utter disregard for the financial rights of authors, have been so well documented and discussed in critical and fan literature. That the founder of genre science fiction who gave his name to the field's most prestigious award and who was the Guest of Honor at the 1952 Worldcon was pretty much a crook (and a contemptuous crook who stiffed his writers but paid himself $100K a year as President of Gernsback Publications) has been clearly established. 
Jack Williamson, who had to hire an attorney associated with the American Fiction Guild to force Gernsback to pay him, summed up his importance for the genre:
At any rate, his main influence in the field was simply to start Amazing and Wonder Stories and get SF out to the public newsstands—and to name the genre he had earlier called "scientifiction."
Fiction.
Gernsback wrote fiction, including the novel "Ralph 124C 41+" in 1911; the title is a pun on the phrase "one to foresee for many"("one plus"). Even though "Ralph 124C 41+" is one of the most influential science fiction stories of all time, and filled with numerous science fiction ideas, few people still read the story. Author Brian Aldiss has called the story a "tawdry illiterate tale" and a "sorry concoction" while author and editor Lester del Rey called it "simply dreadful." While most other modern critics have little positive to say about the story's writing, "Ralph 124C 41+" is still considered an "essential text for all studies of science fiction."
Gernsback's second (and final) novel, "Ultimate World", written c.1958, was not published until 1971. Lester del Rey described it simply as "a bad book," marked more by routine social commentary than by scientific insight or extrapolation. James Blish, in a caustic review, described the novel as "incompetent, pedantic, graceless, incredible, unpopulated and boring" and concluded that its publication "accomplishes nothing but the placing of a blot on the memory of a justly honored man."
Gernsback combined his fiction and science into "Everyday Science and Mechanics" magazine, serving as the editor in the 1930s.
Legacy.
The Hugo Awards or "Hugos" are the annual achievement awards presented at the World Science Fiction Convention, selected in a process that ends with vote by current Convention members. They originated and acquired the "Hugo" nickname during the 1950s and were formally defined as a convention responsibility under the name "Science Fiction Achievement Awards" early in the 1960s. The nickname soon became almost universal and its use legally protected; "Hugo Award(s)" replaced the longer name in all official uses after the 1991 cycle.
In 1960 Gernsback received a special Hugo Award as "The Father of Magazine Science Fiction."
The Science Fiction and Fantasy Hall of Fame inducted him in 1996, its inaugural class of two deceased and two living persons.
Gernsback's importance to science fiction was acknowledged by video game developer BioWare, who named a vessel after him in the game "Mass Effect 2".
Influence in radio electronics and broadcasting.
Gernsback made significant contributions to the growth of early broadcasting, mostly through his efforts as a publisher. He originated the industry of specialized publications for radio with "Modern Electrics" and "Electrical Experimenter". Later on, and more influentially, he published "Radio News", which would have the largest readership among radio magazines in radio broadcasting’s formative years. He edited "Radio News" until 1929. For a short time he hired John F. Rider to be editor. Rider was a former engineer working with the US Army Signal Corps and a radio engineer for A. H. Grebe, a radio manufacturer. However Rider would soon leave Gernsback and form his own publishing company, John F. Rider Publisher, New York around 1931.
Gernsback made use of the magazine to promote his own interests, including having his radio station’s call letters on the cover starting in 1925. WRNY and "Radio News" were used to cross-promote each other, with programs on his station often used to discuss articles he had published, and articles in the magazine often covering program activities at WRNY. He also advocated for future directions in innovation and regulation of radio. The magazine contained many drawings and diagrams, encouraging radio listeners of the 1920s to experiment themselves to improve the technology. WRNY was often used as a laboratory to see if various radio inventions were worthwhile.
Articles that were published about television were also tested in this manner when the radio station was used to send pictures to experimental television receivers in August 1928. The technology, however, was primitive and required sending the sight and sound one after the other rather than sending both at the same time. Such experiments were expensive, eventually contributing to Gernsback’s Experimenter Publishing Company going into bankruptcy in 1929.
Patents.
Gernsback held 80 patents by the time of his death in New York City on August 19, 1967.
Further reading.
</dl>

</doc>
<doc id="13636" url="http://en.wikipedia.org/wiki?curid=13636" title="History of computing hardware">
History of computing hardware

The history of computing hardware covers the developments from early simple devices to aid calculation to modern day computers.
Before the 20th century, most calculations were done by humans. Early mechanical tools to help humans with digital calculations were called "calculating machines", by proprietary names, or even as they are now, calculators. The machine operator was called the computer.
The first aids to computation were purely mechanical devices which required the operator to set up the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form, for instance distance along a scale, rotation of a shaft, or a voltage. Numbers could also be represented in the form of digits, automatically manipulated by a mechanical mechanism. Although this approach generally required more complex mechanisms, it greatly increased the precision of results. The invention of transistor and then integrated circuits made a breakthrough in computers. As a result digital computers largely replaced analog computers. The price of computers gradually became so low that first the personal computers and later mobile computers (smartphones and tablets) became ubiquitous.
Early devices.
Ancient era.
Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.
The abacus was early used for arithmetic tasks. What we now call the Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.
Several analog computers were constructed in ancient and medieval times to perform astronomical calculations. These include the Antikythera mechanism and the astrolabe from ancient Greece (c. 150–100 BC), which are generally regarded as the earliest known mechanical analog computers. Hero of Alexandria (c. 10–70 AD) made many complex mechanical devices including automata and a programmable cart. Other early versions of mechanical devices used to perform one or another type of calculations include the planisphere and other mechanical computing devices invented by Abu Rayhan al-Biruni (c. AD 1000); the equatorium and universal latitude-independent astrolabe by Abu Ishaq Ibrahim al-Zarqali (c. AD 1015); the astronomical analog computers of other medieval Muslim astronomers and engineers; and the astronomical clock tower of Su Song (c. AD 1090) during the Song Dynasty.
Renaissance calculating tools.
Scottish mathematician and physicist John Napier discovered that the multiplication and division of numbers could be performed by the addition and subtraction, respectively, of the logarithms of those numbers. While producing the first logarithmic tables, Napier needed to perform many tedious multiplications. It was at this point that he designed his 'Napier's bones', an abacus-like device that greatly simplified calculations that involved multiplication and division.
Since real numbers can be represented as distances or intervals on a line, the slide rule was invented in the 1620s, shortly after Napier's work, to allow multiplication and division operations to be carried out significantly faster than was previously possible. Edmund Gunter built a calculating device with a single logarithmic scale at the University of Oxford. His device greatly simplified arithmetic calculations, including multiplication and division. William Oughtred greatly improved this in 1630 with his circular slide rule. He followed this up with the modern slide rule in 1632, essentially a combination of two Gunter rules, held together with the hands. Slide rules were used by generations of engineers and other mathematically involved professional workers, until the invention of the pocket calculator.
Mechanical calculators.
Wilhelm Schickard, a German polymath, designed a calculating machine in 1623 which combined a mechanised form of Napier's rods with the world's first mechanical adding machine built into the base. Because it made use of a single-tooth gear there were circumstances in which its carry mechanism would jam. A fire destroyed at least one of the machines in 1624 and it is believed Schickard was too disheartened to build another.
In 1642, while still a teenager, Blaise Pascal started some pioneering work on calculating machines and after three years of effort and 50 prototypes he invented a mechanical calculator. He built twenty of these machines (called Pascal's Calculator or Pascaline) in the following ten years. Nine Pascalines have survived, most of which are on display in European museums. A continuing debate exists over whether Schickard or Pascal should be regarded as the "inventor of the mechanical calculator" and the range of issues to be considered is discussed elsewhere.
Gottfried Wilhelm von Leibniz invented the Stepped Reckoner and his famous stepped drum mechanism around 1672. He attempted to create a machine that could be used not only for addition and subtraction but would utilise a moveable carriage to enable long multiplication and division. Leibniz once said "It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used." However, Leibniz did not incorporate a fully successful carry mechanism. Leibniz also described the binary numeral system, a central ingredient of all modern computers. However, up to the 1940s, many subsequent designs (including Charles Babbage's machines of the 1822 and even ENIAC of 1945) were based on the decimal system.
Around 1820, Charles Xavier Thomas de Colmar created what would over the rest of the century become the first successful, mass-produced mechanical calculator, the Thomas Arithmometer. It could be used to add and subtract, and with a moveable carriage the operator could also multiply, and divide by a process of long multiplication and long division. It utilised a stepped drum similar in conception to that invented by Leibniz. Mechanical calculators remained in use until the 1970s.
Punched card data processing.
In 1801, Joseph-Marie Jacquard developed a loom in which the pattern being woven was controlled by punched cards. The series of cards could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punch cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for automatic pianos and more recently numerical control machine tools.
In the late 1880s, the American Herman Hollerith invented data storage on punched cards that could then be read by a machine. To process these punched cards he invented the tabulator, and the key punch machine. His machines used mechanical relays (and solenoids) to increment mechanical counters. Hollerith's method was used in the 1890 United States Census and the completed results were "... finished months ahead of schedule and far under budget". Indeed, the census was processed years faster than the prior census had been. Hollerith's company eventually became the core of IBM.
By 1920, electro-mechanical tabulating machines could add, subtract and print accumulated totals. Machines were programmed by inserting dozens of wire jumpers into removable control panels. When the United States instituted Social Security in 1935, IBM punched card systems were used to process records of 26 million workers. Punch cards became ubiquitous in industry and government for accounting and administration.
Leslie Comrie's articles on punched card methods and W.J. Eckert's publication of "Punched Card Methods in Scientific Computation" in 1940, described punch card techniques sufficiently advanced to solve some differential equations or perform multiplication and division using floating point representations, all on punched cards and unit record machines. Such machines were used during World War II for cryptographic statistical processing, as well as a vast number of administrative uses. The Astronomical Computing Bureau, Columbia University performed astronomical calculations representing the state of the art in computing.
Calculators.
By the 20th century, earlier mechanical calculators, cash registers, accounting machines, and so on were redesigned to use electric motors, with gear position as the representation for the state of a variable. The word "computer" was a job title assigned to people who used these calculators to perform mathematical calculations. By the 1920s, British scientist Lewis Fry Richardson's interest in weather prediction led him to propose human computers and numerical analysis to model the weather; to this day, the most powerful computers on Earth are needed to adequately model its weather using the Navier–Stokes equations.
Companies like Friden, Marchant Calculator and Monroe made desktop mechanical calculators from the 1930s that could add, subtract, multiply and divide. In 1948, the Curta was introduced by Austrian inventor, Curt Herzstark. It was a small, hand-cranked mechanical calculator and as such, a descendant of Gottfried Leibniz's Stepped Reckoner and Thomas's Arithmometer.
The world's first "all-electronic desktop" calculator was the British Bell Punch ANITA, released in 1961. It used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick. The tube technology was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a 5 in CRT, and introduced reverse Polish notation (RPN).
First general-purpose computing device.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.
The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.
There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. (Later drawings depict a regularized grid layout.) Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards.
The machine was about a century ahead of its time. However, the project was slowed by various problems including disputes with the chief machinist building parts for it. All the parts for his machine had to be made by hand - this was a major problem for a machine with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Ada Lovelace, Lord Byron's daughter, translated and added notes to the "Sketch of the Analytical Engine" by Federico Luigi, Conte Menabrea. This appears to be the first published description of programming.
Following Babbage, although unaware of his earlier work, was Percy Ludgate, an accountant from Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.
Analog computers.
In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.
The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.
The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock). It was used by the Imperial Russian Navy in World War I. The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.
Mechanical devices were also used to aid the accuracy of aerial bombing. Drift Sight was the first such aid, developed by Harry Wimperis in 1916 for the Royal Naval Air Service; it measured the wind speed from the air, and used that measurement to calculate the wind's effects on the trajectory of the bombs. The system was later improved with the Course Setting Bomb Sight, and reached a climax with World War II bomb sights, Mark XIV bomb sight (RAF Bomber Command) and the Norden (United States Army Air Forces).
The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927, which built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious; the most powerful was constructed at the University of Pennsylvania's Moore School of Electrical Engineering, where the ENIAC was built.
By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but hybrid analog computers, controlled by digital electronics, remained in substantial use into the 1950s and 1960s, and later in some specialized applications.
Advent of the digital computer.
The principle of the modern computer was first described by computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, "On Computable Numbers". Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the "Entscheidungsproblem" by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.
He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.
Electromechanical computers.
The era of modern computing began with a flurry of development before and during World War II. Most digital computers built in this period were electromechanical - electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes.
The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1939. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.
In the same year, the electro-mechanical devices called bombes were built by British cryptologists to help decipher German Enigma-machine-encrypted secret messages during World War II. The initial design of the bombe was produced in 1939 at the UK Government Code and Cypher School (GC&CS) at Bletchley Park by Alan Turing, with an important refinement devised in 1940 by Gordon Welchman. The engineering design and construction was the work of Harold Keen of the British Tabulating Machine Company. It was a substantial development from a device that had been designed in 1938 by Polish Cipher Bureau cryptologist Marian Rejewski, and known as the "cryptologic bomb" (Polish: "bomba kryptologiczna").
In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code and data were stored on punched film. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was probably a complete Turing machine. In two 1936 patent applications, Zuse also anticipated that machine instructions could be stored in the same storage used for data—the key insight of what became known as the von Neumann architecture, first implemented in the British SSEM of 1948.
Zuse suffered setbacks during World War II when some of his machines were destroyed in the course of Allied bombing campaigns. Apparently his work remained largely unknown to engineers in the UK and US until much later, although at least IBM was aware of it as it financed his post-war startup company in 1946 in return for an option on Zuse's patents.
In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories; it was a similar general purpose electro-mechanical computer to the Z3 and was not quite Turing-complete.
Digital computation.
A mathematical basis of digital computing is Boolean algebra, developed by the British mathematician George Boole in his work "The Laws of Thought", published in 1854. His Boolean algebra was further refined in the 1860s by William Jevons and Charles Sanders Peirce, and was first presented systematically by Ernst Schröder and A. N. Whitehead.
In the 1930s and working independently, American electronic engineer Claude Shannon and Soviet logician Victor Shestakov both showed a one-to-one correspondence between the concepts of Boolean logic and certain electrical circuits, now called logic gates, which are now ubiquitous in digital computers. They showed that electronic relays and switches can realize the expressions of Boolean algebra. This thesis essentially founded practical digital circuit design.
Electronic data processing.
Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. Machines such as the Z3, the Atanasoff–Berry Computer, the Colossus computers, and the ENIAC were built by hand, using circuits containing relays or valves (vacuum tubes), and often used punched cards or punched paper tape for input and as the main (non-volatile) storage medium.
The engineer Tommy Flowers joined the telecommunications branch of the General Post Office in 1926. While working at the research station in Dollis Hill in the 1930s, he began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.
In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first electronic digital calculating device. This design was also all-electronic, and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory. However, its paper card writer/reader was unreliable, and work on the machine was discontinued. The machine's special-purpose nature and lack of a changeable, stored program distinguish it from modern computers.
The electronic programmable computer.
During World War II, the British at Bletchley Park (40 miles north of London) achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. They ruled out possible Enigma settings by performing chains of logical deductions implemented electrically. Most possibilities led to a contradiction, and the few remaining could be tested by hand.
The Germans also developed a series of teleprinter encryption systems, quite different from Enigma. The Lorenz SZ 40/42 machine was used for high-level Army communications, termed "Tunny" by the British. The first intercepts of Lorenz messages began in 1941. As part of an attack on Tunny, Max Newman and his colleagues helped specify the Colossus.
Tommy Flowers, still a senior engineer at the Post Office Research Station was recommended to Max Newman by Alan Turing and spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.
Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process. Mark 2 was designed while Mark 1 was being constructed. Allen Coombs took over leadership of the Colossus Mark 2 project when Tommy Flowers moved on to other projects.
Colossus was able to process 5,000 characters per second with the paper tape moving at 40 ft/s. Sometimes, two or more Colossus computers tried different possibilities simultaneously in what now is called parallel computing, speeding the decoding process by perhaps as much as double the rate of comparison.
Colossus included the first ever use of shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean calculations, on each of the five channels on the punched tape (although in normal operation only one or two channels were examined in any run). Initially Colossus was only used to determine the initial wheel positions used for a particular message (termed wheel setting). The Mark 2 included mechanisms intended to help determine pin patterns (wheel breaking). Both models were programmable using switches and plug panels in a way the Robinsons had not been.
Without the use of these machines, the Allies would have been deprived of the very valuable intelligence that was obtained from reading the vast quantity of encrypted high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Details of their existence, design, and use were kept secret well into the 1970s. Winston Churchill personally issued an order for their destruction into pieces no larger than a man's hand, to keep secret that the British were capable of cracking Lorenz SZ cyphers (from German rotor stream cipher machines) during the oncoming cold war. Two of the machines were transferred to the newly formed GCHQ and the others were destroyed. As a result the machines were not included in many histories of computing. A reconstructed working copy of one of the Colossus machines is now on display at Bletchley Park.
The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.
It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors. One of its major engineering feats was to minimize the effects of tube burnout, which was a common problem in machine reliability at that time. The machine was in almost constant use for the next ten years.
The stored-program computer.
Early computing machines had fixed programs. For example, a desk calculator is a fixed program computer. It can do basic mathematics, but it cannot be used as a word processor or a gaming console. Changing the program of a fixed-program machine requires re-wiring, re-structuring, or re-designing the machine. The earliest computers were not so much "programmed" as they were "designed". "Reprogramming", when it was possible at all, was a laborious process, starting with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically re-wiring and re-building the machine.
With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation.
Theory.
The theoretical basis for the stored-program computer had been laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device.
Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his "First Draft of a Report on the EDVAC" in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.
Turing felt that speed and size of memory were crucial and he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used "Abbreviated Computer Instructions," an early form of programming language.
Manchester "baby".
The Manchester Small-Scale Experimental Machine, nicknamed "Baby", was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.
The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device. Invented by Freddie Williams and Tom Kilburn at the University of Manchester in 1946 and 1947, it was a cathode ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.
Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.
The SSEM had a 32-bit word length and a memory of 32 words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 218 (262,144), a calculation that was known would take a long time to run—and so prove the computer's reliability—by testing every integer from 218 - 1 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17 instructions and ran for 52 minutes before reaching the correct answer of 131,072, after the SSEM had performed 3.5 million operations (for an effective CPU speed of 1.1 kIPS).
Manchester Mark 1.
The Experimental machine led on to the development of the Manchester Mark 1 at the University of Manchester. Work began in August 1948, and the first version was operational by April 1949; a program written to search for Mersenne primes ran error-free for nine hours on the night of 16/17 June 1949.
The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers.
The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the IBM 701 and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark 1 that computers would be used more in scientific roles than in pure mathematics. In 1951 they started development work on Meg, the Mark 1's successor, which would include a floating point unit.
EDSAC.
The other contender for being the first recognizably modern digital stored-program computer was the EDSAC, designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal "First Draft of a Report on the EDVAC" and was one of the first usefully operational electronic digital stored-program computer.
EDSAC ran its first programs on 6 May 1949, when it calculated a table of squares and a list of prime numbers.The EDSAC also served as the basis for the first commercially applied computer, the LEO I, used by food manufacturing company J. Lyons & Co. Ltd. EDSAC 1 and was finally shut down on 11 July 1958, having been superseded by EDSAC 2 which stayed in use until 1965.
EDVAC.
ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944, and design work for the EDVAC commenced at the University of Pennsylvania's Moore School of Electrical Engineering, before the ENIAC was fully operational. The design would implement a number of important architectural and logical improvements conceived during the ENIAC's construction and would incorporate a high speed serial access memory. However, Eckert and Mauchly left the project and its construction floundered.
It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.
Commercial computers.
The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves). A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.
In October 1947, the directors of J. Lyons & Company, a British catering company famous for its teashops but with strong interests in new office management techniques, decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job. On 17 November 1951, the J. Lyons company began weekly operation of a bakery valuations job on the LEO (Lyons Electronic Office). This was the first business to go live on a stored program computer.
In June 1951, the UNIVAC I (Universal Automatic Computer) was delivered to the U.S. Census Bureau. Remington Rand eventually sold 46 machines at more than US$1 million each ($ as of 2015). UNIVAC was the first "mass produced" computer. It used 5,200 vacuum tubes and consumed 125 kW of power. Its primary storage was serial-access mercury delay lines capable of storing 1,000 words of 11decimal digits plus sign (72-bit words).
IBM introduced a smaller, more affordable computer in 1954 that proved very popular. The IBM 650 weighed over 900 kg, the attached power supply weighed around 1350 kg and both were held in separate cabinets of roughly 1.5 meters by 0.9 meters by 1.8 meters. It cost US$500,000 ($ as of 2015) or could be leased for US$3,500 a month ($ as of 2015). Its drum memory was originally 2,000 ten-digit words, later expanded to 4,000 words. Memory limitations such as this were to dominate programming for decades afterward. The program instructions were fetched from the spinning drum as the code ran. Efficient execution using drum memory was provided by a combination of hardware architecture: the instruction format included the address of the next instruction; and software: the Symbolic Optimal Assembly Program, SOAP, assigned instructions to the optimal addresses (to the extent possible by static analysis of the source program). Thus many instructions were, when needed, located in the next row of the drum to be read and additional wait time for drum rotation was not required.
Microprogramming.
In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the Central Processing Unit of a computer could be controlled by a miniature, highly specialised computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode). This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in IEEE Spectrum in 1955.
It was widely used in the CPUs and floating-point units of mainframe and other computers; it was implemented for the first time in EDSAC 2, which also used multiple identical "bit slices" to simplify design. Interchangeable, replaceable tube assemblies were used for each bit of the processor.
Magnetic storage.
By 1954, magnetic core memory was rapidly displacing most other forms of temporary storage, including the Williams tube. It went on to dominate the field through the mid-1970s.
A key feature of the American UNIVAC I system of 1951 was the implementation of a newly invented type of metal magnetic tape, and a high-speed tape unit, for non-volatile storage. Magnetic tape is still used in many computers.
In 1952, IBM publicly announced the IBM 701 Electronic Data Processing Machine, the first in its successful 700/7000 series and its first IBM mainframe computer. The IBM 704, introduced in 1954, used magnetic core memory, which became the standard for large machines.
IBM introduced the first disk storage unit, the IBM 350 RAMAC (Random Access Method of Accounting and Control) in 1956. Using fifty 24 in metal disks, with 100 tracks per side, it was able to store 5 megabytes of data at a cost of US$10,000 per megabyte ($ as of 2015).
Transistor computers.
The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Initially the only devices available were germanium point-contact transistors.
Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. Transistors greatly reduced computers' size, initial cost, and operating cost.
Typically, second-generation computers were composed of large numbers of printed circuit boards such as the IBM Standard Modular System
each carrying one to four logic gates or flip-flops.
At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Initially the only devices available were germanium point-contact transistors, less reliable than the valves they replaced but which consumed far less power. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. The 1955 version used 200 transistors, 1,300 solid-state diodes, and had a power consumption of 150 watts. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer.
That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell. The design featured a 64-kilobyte magnetic drum memory store with multiple moving heads that had been designed at the National Physical Laboratory, UK. By 1953 his team had transistor circuits operating to read and write on a smaller magnetic drum from the Royal Radar Establishment. The machine used a low clock speed of only 58 kHz to avoid having to use any valves to generate the clock waveforms.
CADET used 324 point-contact transistors provided by the UK company Standard Telephones and Cables; 76 junction transistors were used for the first stage amplifiers for data read from the drum, since point-contact transistors were too noisy. From August 1956 CADET was offering a regular computing service, during which it often executed continuous computing runs of 80 hours or more. Problems with the reliability of early batches of point contact and alloyed junction transistors meant that the machine's mean time between failures was about 90 minutes, but this improved once the more reliable bipolar junction transistors became available.
The Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere. Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years.
A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.
Transistorized peripherals.
Transistorized electronics improved not only the CPU (Central Processing Unit), but also the peripheral devices. The second generation disk data storage units were able to store tens of millions of letters and digits. Next to the fixed disk storage units, connected to the CPU via high-speed data transmission, were removable disk data storage units. A removable disk pack can be easily exchanged with another pack in a few seconds. Even if the removable disks' capacity is smaller than fixed disks, their interchangeability guarantees a nearly unlimited quantity of data close at hand. Magnetic tape provided archival capability for this data, at a lower cost than disk.
Many second-generation CPUs delegated peripheral device communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took at least two memory cycles; one for the instruction, one for the operand data fetch.
During the second generation remote terminal units (often in the form of Teleprinters like a Friden Flexowriter) saw greatly increased use. Telephone connections provided sufficient speed for early remote terminals and allowed hundreds of kilometers separation between remote-terminals and the computing center. Eventually these stand-alone computer networks would be generalized into an interconnected "network of networks"—the Internet.
Supercomputers.
The early 1960s saw the advent of supercomputing. The Atlas Computer was a joint development between the University of Manchester, Ferranti, and Plessey, and was first installed at Manchester University and officially commissioned in 1962 as one of the world's first supercomputers - considered to be the most powerful computer in the world at that time. It was said that whenever Atlas went offline half of the United Kingdom's computer capacity was lost. It was a second-generation machine, using discrete germanium transistors. Atlas also pioneered the Atlas Supervisor, "considered by many to be the first recognisable modern operating system".
In the US, a series of computers at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance. The CDC 6600, released in 1964, is generally considered the first supercomputer. The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of three. With performance of about 1 megaFLOPS, the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.
The integrated circuit.
The next great advance in computing power came with the advent of the integrated circuit.
The idea of the integrated circuit was conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952:
The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as “a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated.” The first customer for the invention was the US Air Force.
Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.
Post-1960 (integrated circuit based).
The explosion in the use of computers began with "third-generation" computers, making use of Jack St. Clair Kilby's and Robert Noyce's independent invention of the integrated circuit (or microchip). This led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.
While the earliest microprocessor ICs literally contained only the processor, i.e. the central processing unit, of a computer, their progressive development naturally led to chips containing most or all of the internal electronic parts of a computer. The integrated circuit in the image on the right, for example, an Intel 8742, is an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip.
During the 1960s there was considerable overlap between second and third generation technologies. IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities. It became possible to simulate analog circuits with the "simulation program with integrated circuit emphasis", or SPICE (1971) on minicomputers, one of the programs for electronic design automation ().
The microprocessor led to the development of the microcomputer, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.
In April 1975 at the Hannover Fair, Olivetti presented the P6060, the world's first personal computer with built-in floppy disk: a central processing unit on two cards, code named PUCE1 and PUCE2, with TTL components. It had one or two 8" floppy disk drives, a 32-character plasma display, 80-column graphical thermal printer, 48 Kbytes of RAM, and BASIC language. It weighed 40 kg. It was in competition with a similar product by IBM that had an external floppy disk drive.
MOS Technology KIM-1 and Altair 8800, were sold as kits for do-it-yourselfers, as was the Apple I, soon afterward. The first Apple computer with graphic and sound capabilities came out well after the Commodore PET. Computing has evolved with microcomputer architectures, with features added from their larger brethren, now dominant in most market segments.
A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN HTTPd, and also used to write the first web browser, WorldWideWeb. These facts, along with the close association with Steve Jobs, secure the 68030 NeXT a place in history as one of the most significant computers of all time.
Systems as complicated as computers require very high reliability. ENIAC remained on, in continuous operation from 1947 to 1955, for eight years before being shut down. Although a vacuum tube might fail, it would be replaced without bringing down the system. By the simple strategy of never shutting down ENIAC, the failures were dramatically reduced. The vacuum-tube SAGE air-defense computers became remarkably reliable – installed in pairs, one off-line, tubes likely to fail did so when the computer was intentionally run at reduced power to find them. Hot-pluggable hard disks, like the hot-pluggable vacuum tubes of yesteryear, continue the tradition of repair during continuous operation. Semiconductor memories routinely have no errors when they operate, although operating systems like Unix have employed memory tests on start-up to detect failing hardware. Today, the requirement of reliable performance is made even more stringent when server farms are the delivery platform. Google has managed this by using fault-tolerant software to recover from hardware failures, and is even working on the concept of replacing entire server farms on-the-fly, during a service event.
In the 21st century, multi-core CPUs became commercially available. Content-addressable memory (CAM) has become inexpensive enough to be used in networking, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.
This has allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S. The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices. Computing hardware and its software have even become a metaphor for the operation of the universe.
Future.
Although DNA-based computing and quantum computing are years or decades in the future, the infrastructure is being laid today, for example, with DNA origami on photolithography and with quantum antennae for transferring information between ion traps. By 2011, researchers had entangled qubits. Fast digital circuits (including those based on Josephson junctions and rapid single flux quantum technology) are becoming more nearly realizable with the discovery of nanoscale superconductors.
Fiber-optic and photonic devices, which already have been used to transport data over long distances, are now entering the data center, side by side with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical (this is called "photonic") information processing in one chip. This is denoted "CMOS-integrated nanophotonics" or (CINP). One benefit of optical interconnects is that motherboards which formerly required a certain kind of system on a chip (SoC) can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.
An indication of the rapidity of development of this field can be inferred by the history of the seminal article. By the time that anyone had time to write anything down, it was obsolete. After 1945, others read John von Neumann's "First Draft of a Report on the EDVAC", and immediately started implementing their own systems. To this day, the pace of development has continued, worldwide.
References.
 , "Semiconductor device-and-lead structure", issued 1961-04-25, assigned to Fairchild Semiconductor Corporation<span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=2981877&rft.cc=US&rft.title=Semiconductor%20device-and-lead%20structure&rft.inventor=%5B%5BRobert%20Noyce%5D%5D&rft.assignee=%5B%5BFairchild%20Semiconductor%20Corporation%5D%5D&rft.date=1961-04-25"> .
 , "Complex Computer", issued 1954-02-09, assigned to American Telephone & Telegraph Company<span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=2668661&rft.cc=US&rft.title=Complex%20Computer&rft.inventor=%5B%5BGeorge%20Stibitz%5D%5D&rft.assignee=%5B%5BAmerican%20Telephone%20%26amp%3B%20Telegraph%20Company%5D%5D&rft.date=1954-02-09"> .
 , "Pulse transfer controlling devices", issued 1955-05-17<span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=2708722&rft.cc=US&rft.title=Pulse%20transfer%20controlling%20devices&rft.inventor=%5B%5BAn%20Wang%5D%5D&rft.date=1955-05-17"> .
</dl>

</doc>
<doc id="13637" url="http://en.wikipedia.org/wiki?curid=13637" title="Hausdorff space">
Hausdorff space

In topology and related branches of mathematics, a Hausdorff space, separated space or T2 space is a topological space in which distinct points have disjoint neighbourhoods. Of the many separation axioms that can be imposed on a topological space, the "Hausdorff condition" (T2) is the most frequently used and discussed. It implies the uniqueness of limits of sequences, nets, and filters.
Hausdorff spaces are named after Felix Hausdorff, one of the founders of topology. Hausdorff's original definition of a topological space (in 1914) included the Hausdorff condition as an axiom.
Definitions.
Points "x" and "y" in a topological space "X" can be "separated by neighbourhoods" if there exists a neighbourhood "U" of "x" and a neighbourhood "V" of "y" such that "U" and "V" are disjoint ("U" ∩ "V" = ∅).
"X" is a Hausdorff space if any two distinct points of "X" can be separated by neighborhoods. This condition is the third separation axiom (after T0 and T1), which is why Hausdorff spaces are also called "T2 spaces". The name "separated space" is also used.
A related, but weaker, notion is that of a preregular space. "X" is a preregular space if any two topologically distinguishable points can be separated by neighbourhoods. Preregular spaces are also called "R1 spaces".
The relationship between these two conditions is as follows. A topological space is Hausdorff if and only if it is both preregular (i.e. topologically distinguishable points are separated by neighbourhoods) and Kolmogorov (i.e. distinct points are topologically distinguishable). A topological space is preregular if and only if its Kolmogorov quotient is Hausdorff.
Equivalences.
For a topological space "X", the following are equivalent:
Examples and counterexamples.
Almost all spaces encountered in analysis are Hausdorff; most importantly, the real numbers (under the standard metric topology on real numbers) are a Hausdorff space. More generally, all metric spaces are Hausdorff. In fact, many spaces of use in analysis, such as topological groups and topological manifolds, have the Hausdorff condition explicitly stated in their definitions.
A simple example of a topology that is T1 but is not Hausdorff is the cofinite topology defined on an infinite set.
Pseudometric spaces typically are not Hausdorff, but they are preregular, and their use in analysis is usually only in the construction of Hausdorff gauge spaces. Indeed, when analysts run across a non-Hausdorff space, it is still probably at least preregular, and then they simply replace it with its Kolmogorov quotient, which is Hausdorff.
In contrast, non-preregular spaces are encountered much more frequently in abstract algebra and algebraic geometry, in particular as the Zariski topology on an algebraic variety or the spectrum of a ring. They also arise in the model theory of intuitionistic logic: every complete Heyting algebra is the algebra of open sets of some topological space, but this space need not be preregular, much less Hausdorff.
While the existence of unique limits for convergent nets and filters implies that a space is Hausdorff, there are non-Hausdorff T1 spaces in which every convergent sequence has a unique limit.
Properties.
Subspaces and products of Hausdorff spaces are Hausdorff, but quotient spaces of Hausdorff spaces need not be Hausdorff. In fact, "every" topological space can be realized as the quotient of some Hausdorff space.
Hausdorff spaces are T1, meaning that all singletons are closed. Similarly, preregular spaces are R0.
Another nice property of Hausdorff spaces is that compact sets are always closed. This may fail in non-Hausdorff spaces such as Sierpiński space. 
The definition of a Hausdorff space says that points can be separated by neighborhoods. It turns out that this implies something which is seemingly stronger: in a Hausdorff space every pair of disjoint compact sets can also be separated by neighborhoods, in other words there is a neighborhood of one set and a neighborhood of the other, such that the two neighborhoods are disjoint. This is an example of the general rule that compact sets often behave like points.
Compactness conditions together with preregularity often imply stronger separation axioms. For example, any locally compact preregular space is completely regular. Compact preregular spaces are normal, meaning that they satisfy Urysohn's lemma and the Tietze extension theorem and have partitions of unity subordinate to locally finite open covers. The Hausdorff versions of these statements are: every locally compact Hausdorff space is Tychonoff, and every compact Hausdorff space is normal Hausdorff.
The following results are some technical properties regarding maps (continuous and otherwise) to and from Hausdorff spaces.
Let "f" : "X" → "Y" be a continuous function and suppose "Y" is Hausdorff. Then the graph of "f", formula_1, is a closed subset of "X" × "Y".
Let "f" : "X" → "Y" be a function and let formula_2 be its kernel regarded as a subspace of "X" × "X".
If "f,g" : "X" → "Y" are continuous maps and "Y" is Hausdorff then the equalizer formula_3 is closed in "X". It follows that if "Y" is Hausdorff and "f" and "g" agree on a dense subset of "X" then "f" = "g". In other words, continuous functions into Hausdorff spaces are determined by their values on dense subsets.
Let "f" : "X" → "Y" be a closed surjection such that "f"−1("y") is compact for all "y" ∈ "Y". Then if "X" is Hausdorff so is "Y".
Let "f" : "X" → "Y" be a quotient map with "X" a compact Hausdorff space. Then the following are equivalent
Preregularity versus regularity.
All regular spaces are preregular, as are all Hausdorff spaces. There are many results for topological spaces that hold for both regular and Hausdorff spaces.
Most of the time, these results hold for all preregular spaces; they were listed for regular and Hausdorff spaces separately because the idea of preregular spaces came later.
On the other hand, those results that are truly about regularity generally don't also apply to nonregular Hausdorff spaces.
There are many situations where another condition of topological spaces (such as paracompactness or local compactness) will imply regularity if preregularity is satisfied.
Such conditions often come in two versions: a regular version and a Hausdorff version.
Although Hausdorff spaces aren't generally regular, a Hausdorff space that is also (say) locally compact will be regular, because any Hausdorff space is preregular.
Thus from a certain point of view, it is really preregularity, rather than regularity, that matters in these situations.
However, definitions are usually still phrased in terms of regularity, since this condition is better known than preregularity.
See History of the separation axioms for more on this issue.
Variants.
The terms "Hausdorff", "separated", and "preregular" can also be applied to such variants on topological spaces as uniform spaces, Cauchy spaces, and convergence spaces.
The characteristic that unites the concept in all of these examples is that limits of nets and filters (when they exist) are unique (for separated spaces) or unique up to topological indistinguishability (for preregular spaces).
As it turns out, uniform spaces, and more generally Cauchy spaces, are always preregular, so the Hausdorff condition in these cases reduces to the T0 condition.
These are also the spaces in which completeness makes sense, and Hausdorffness is a natural companion to completeness in these cases.
Specifically, a space is complete if and only if every Cauchy net has at "least" one limit, while a space is Hausdorff if and only if every Cauchy net has at "most" one limit (since only Cauchy nets can have limits in the first place).
Algebra of functions.
The algebra of continuous (real or complex) functions on a compact Hausdorff space is a commutative C*-algebra, and conversely by the Banach–Stone theorem one can recover the topology of the space from the algebraic properties of its algebra of continuous functions. This leads to noncommutative geometry, where one considers noncommutative C*-algebras as representing algebras of functions on a noncommutative space.

</doc>
<doc id="13644" url="http://en.wikipedia.org/wiki?curid=13644" title="Hawkwind">
Hawkwind

Hawkwind are an English rock band, one of the earliest space rock groups. Their lyrics favour urban and science fiction themes. Formed in November 1969, Hawkwind have gone through many incarnations and styles of music. Dozens of musicians, dancers and writers have worked with the group since their inception. 
History.
1969: Formation.
Dave Brock and Mick Slattery had been in the London-based psychedelic band Famous Cure, and a meeting with bassist John Harrison revealed a mutual interest in electronic music which led the trio to embark upon a new musical venture together. Seventeen-year-old drummer Terry Ollis replied to an advert in a music weekly, while Nik Turner and Michael 'Dik Mik' Davies, old acquaintances of Brock, offered help with transport and gear, but were soon pulled into the band.
Gatecrashing a local talent night at the All Saints Hall, Notting Hill, they were so untogether as to not even have a name, plumping for "Group X" at the last minute, nor any songs, choosing to play an extended 20-minute jam on The Byrds "Eight Miles High". BBC Radio 1 DJ John Peel was in the audience and was impressed enough to tell event organiser, Douglas Smith, to keep an eye on them. Smith signed them up and got them a deal with Liberty Records on the back of a deal he was setting up for Cochise.
The band settled on the name Hawkwind after briefly being billed as Hawkwind Zoo, Hawkwind being the nickname of Turner derived from his unappealing habit of clearing his throat (hawking) and excessive flatulence (wind). Another version of the origin of their name says they took it from one of Michael Moorcock's stories. Moorcock himself denies this story, however, and points out that there is no story of that name. An Abbey Road session took place recording demos of "Hurry On Sundown" and others (included on the remasters version of "Hawkwind"), after which Slattery left to be replaced by Huw Lloyd-Langton, who had known Brock from his days working in a music shop selling guitar strings to Brock, then a busker.
1970–1975: United Artists Era.
Pretty Things guitarist Dick Taylor was brought in to produce the 1970 debut album "Hawkwind". Although it was not a commercial success, it did bring them to the attention of the UK underground scene finding them playing free concerts, benefit gigs, and festivals. Playing free outside the Bath Festival, they encountered another Ladbroke Grove based band, the Pink Fairies, who shared similar interests in music and recreational activities; a friendship developed which led to the two bands becoming running partners and performing as "Pinkwind". Their use of drugs, however, led to the departure of Harrison, who did not imbibe, to be replaced briefly by Thomas Crimble (about July '70 - March '71). Crimble played on a few BBC sessions before leaving to help organise the Glastonbury Free Festival 1971; he sat in during the band's performance there. Lloyd-Langton also quit, after a bad LSD trip at the Isle of Wight Festival led to a nervous breakdown.
Their follow up album, 1971's "In Search of Space", brought greater commercial success, reaching number 18 on the UK album charts, and also saw the band's image and philosophy take shape, courtesy of graphic artist Barney Bubbles and underground press writer Robert Calvert, as depicted in the accompanying "Hawklog" booklet which would further be developed into the "Space Ritual" stage show. Science fiction author Michael Moorcock and dancer Stacia also started contributing to the band. Dik Mik had left the band, replaced by sound engineer Del Dettmar, but chose to return for this album giving the band two electronics players. Bass player Dave Anderson, who had been in the German band Amon Düül II, had also joined and played on the album but departed before its release because of personal tensions with some other members of the band. Anderson and Lloyd-Langton then formed the short-lived band Amon Din. Meanwhile, Ollis quit, unhappy with the commercial direction the band were heading in.
The addition of bassist Ian "Lemmy" Kilmister and drummer Simon King propelled the band to greater heights. One of the early gigs this band played was a benefit for the Greasy Truckers at The Roundhouse on 13 February 1972. A live album of the concert "Greasy Truckers Party" was released, and after re-recording the vocal, a single "Silver Machine" was also released, reaching number 3 in the UK charts. This generated sufficient funds for the subsequent album "Doremi Fasol Latido" Space Ritual tour. The show featured dancers Stacia and Miss Renee, mime artist Tony Carrera and a light show by Liquid Len and was recorded on the elaborate package "Space Ritual". At the height of their success in 1973, the band released the single "Urban Guerrilla" which coincided with an IRA bombing campaign in London, so the BBC refused to play it and the band's management reluctantly decided to withdraw it fearing accusations of opportunism, despite the disc having already climbed to number 39 in the UK chart.
Dik Mik departed during 1973 and Calvert ended his association with the band to concentrate on solo projects. Dettmar also indicated that he was to leave the band, so Simon House was recruited as keyboardist and violinist playing live shows, a North America tour and recording the 1974 album "Hall of the Mountain Grill". Dettmar left after a European tour and emigrated to Canada, whilst Alan Powell deputised for an incapacitated King on that European tour, but remained giving the band two drummers.
At the beginning of 1975, the band recorded the album "Warrior on the Edge of Time" in collaboration with Michael Moorcock, loosely based on his Eternal Champion figure. However, during a North America tour in May, Lemmy was caught in possession of amphetamine crossing the border from the USA into Canada. The border police mistook the powder for cocaine and he was jailed, forcing the band to cancel some shows. Fed up with his erratic behaviour, the band fired the bass player replacing him with their long-standing friend and former Pink Fairies guitarist Paul Rudolph. Lemmy then teamed up with another Pink Fairies guitarist, Larry Wallis, to form Motörhead, named after the last song he had written for Hawkwind.
1976–1978: Charisma Era.
Robert Calvert made a guest appearance with band for their headline set at the Reading Festival in August 1975, after which he chose to rejoin the band as a full-time vocalist and front man. Stacia, on the other hand, chose to relinquish her dancing duties and settle down to family life. The band changed record company to Tony Stratton-Smith's Charisma Records and, on Stratton-Smith's suggestion, band management from Douglas Smith to Tony Howard.
1976's "Astounding Sounds, Amazing Music" is the first album of this era and highlights both Calvert's well-crafted lyrics written with stage performance in mind and a greater proficiency and scope in the music. But on the eve of recording the follow-up "Back on the Streets" single, Turner was sacked for his erratic live playing and Powell was deemed surplus to requirements. After a tour to promote the single and during rehearsals for the next album, Rudolph was also sacked for allegedly trying to steer the band into a musical direction at odds with Calvert and Brock's vision.
Adrian "Ade" Shaw, who as bass player for Magic Muscle had supported Hawkwind on the "Space Ritual" tour, came in for the 1977 album "Quark, Strangeness and Charm". The band continued to enjoy moderate commercial success, but Calvert's mental illness often caused problems. A manic phase saw the band abandon a European tour in France, while a depression phase during a 1978 North American tour convinced Brock to disband the group. In between these two tours, the band had recorded the album "PXR5" in January 1978, but its release was delayed until 1979.
1978-1979: Sonic Assassins and Hawklords.
On 23 December 1977 in Barnstaple, Brock and Calvert had performed a one-off gig with Devon band Ark as the Sonic Assassins, and looking for a new project in 1978, bassist Harvey Bainbridge and drummer Martin Griffin were recruited from this event. Steve Swindells was recruited as keyboard player. The band was named Hawklords, (probably for legal reasons, the band having recently split from their management), and recording took place on a farm in Devon using a mobile studio, resulting in the album "25 Years On". King had originally been the drummer for the project but quit during recording sessions to return to London, while House, who had temporarily left the band to join a David Bowie tour, elected to remain with Bowie full-time, but nevertheless contributed violin to these sessions. At the end of the band's UK tour, Calvert, wanting King back in the band, fired Griffin, then promptly resigned himself, choosing to pursue a career in literature. Swindells left to record a solo album after an offer had been made to him by the record company ATCO.
1980s: Bronze, RCA and Flicknife Eras.
In late 1979, Hawkwind reformed with Brock, Bainbridge and King being joined by Huw Lloyd-Langton (who had played on the debut album) and Tim Blake (formerly of Gong), embarking upon a UK tour despite not having a record deal or any product to promote. Some shows were recorded and a deal was made with Bronze Records, resulting in the "Live Seventy Nine" album, quickly followed by the studio album "Levitation". However, during the recording of "Levitation" King quit and Ginger Baker was drafted in for the sessions, but he chose to stay with the band for the tour, during which Tim Blake left to be replaced by Keith Hale.
In 1981 Baker and Hale left after their insistence that Bainbridge should be sacked was ignored, and Brock and Bainbridge elected to handle synthesizers and sequencers themselves, with drummer Griffin from the Hawklords rejoining. Three albums, which again saw Michael Moorcock contributing lyrics and vocals, were recorded for RCA/Active: "Sonic Attack", the electronic "Church of Hawkwind" and "Choose Your Masques". This band headlined the 1981 Glastonbury Festival and made an appearance at the 1982 Donington Monsters of Rock Festival, as well as continuing to play the summer solstice at Stonehenge Free Festival.
In the early 1980s, Brock had started using drum machines for his home demos and became increasingly frustrated at the inability of drummers to keep perfect time, leading to a succession of drummers coming and going. First, Griffin was ousted and the band tried Simon King again, but unhappy with his playing at that time, he was rejected. Andy Anderson briefly joined while he was also playing for The Cure, and Robert Heaton also briefly filled the spot prior to the rise of New Model Army. Lloyd Langton Group drummer John Clark did some recording sessions, and Rik Martinez joined the band for the start of the "Earth Ritual" tour but failed to end it, being replaced by Clive Deamer.
Nik Turner had returned as a guest for the 1982 "Choose Your Masques" tour and was invited back permanently. Further tours ensued with Phil "Dead Fred" Reeves augmenting the line-up on keyboards and violin, but neither Turner nor Reeves would appear on the only recording of 1983/84, "The Earth Ritual Preview", but there was a guest spot for Lemmy. The "Earth Ritual" tour was filmed for Hawkwind's first ever video release, "Night of the Hawk".
Alan Davey was a young fan of the band who had sent a tape of his playing to Brock, and Brock chose to oust Reeves moving Bainbridge from bass to keyboards in order to accommodate Davey. This experimental line-up played at the Stonehenge Free Festival in 1984, which was filmed and release as "Stonehenge 84". Subsequent personal and professional tensions between Brock and Turner led to the latter's expulsion at the beginning of 1985. Clive Deamer, who was deemed "too professional" for the band, was eventually replaced in 1985 by Danny Thompson Jr, a friend of bassist Alan Davey, and remained almost to the end of the decade.
Hawkwind's association with Moorcock climaxed in their most ambitious project, "The Chronicle of the Black Sword", based loosely around the Elric series of books and theatrically staged with Tony Crerar as the central character. Moorcock contributed lyrics, but only performed some spoken pieces on some live dates. The tour was recorded and issued as an album "Live Chronicles" and video "The Chronicle of the Black Sword". A headline appearance at the 1986 Reading Festival was followed by a UK tour to promote the "Live Chronicles" album which was filmed and released as "Chaos". In 1988 the band recorded the album "The Xenon Codex" with Guy Bidmead, but all was not well in the band and soon after, both Lloyd-Langton and Thompson departed.
Drummer Richard Chadwick, who joined in the summer of '88, had been playing in small alternative free festival bands, most notably Bath's Smart Pils, for a decade and had frequently crossed paths with Hawkwind and Brock. He was initially invited simply to play with the band, but eventually replaced stand in drummer Mick Kirton to become the band's drummer to the present day.
To fill in the gap of lead sound, lost when Lloyd-Langton left, violinist Simon House was re-instated into the lineup in 1989 (having previously been a member from 1974 until 1978), and, notably, Hawkwind embarked on their first US visit in 11 years (since the somewhat disastrous 1978 tour), in which House did not partake. The successfully received full American tour was the first of several over the coming years, in an effort by the band to reintroduce themselves to the American market.
1990s: GWR, Essential and Emergency Broadcast System.
Bridget Wishart, an associate of Chadwick's from the festival circuit, also joined to become the band's one and only frontwoman. This band produced two albums, 1990s "Space Bandits" and 1991's "Palace Springs" and also filmed a 1-hour appearance for the "Bedrock TV" series.
1990 saw Hawkwind tour the USA again, the second installment in a series of American visits made at around this time in an effort to re-establish the Hawkwind brand in America. The original business plan was to hold three consecutive US tours, annually, from 1989-1991, with the first losing money, the second breaking even, and the third turning a profit, ultimately bringing Hawkwind back into recognition across the Atlantic. Progress, however, was somewhat stunted, due to ex-member Nik Turner touring the United States with his own bands at the time, in which the shows were often marketed as Hawkwind.
In 1991 Bainbridge, House and Wishart departed and the band continued as a three piece relying heavily on synthesizers and sequencers to create a wall-of-sound. The 1992 album "Electric Tepee" combined hard rock and light ambient pieces, while "It is the Business of the Future to be Dangerous" is almost devoid of the rock leanings. "The Business Trip" is a record of the previous album's tour, but rockier as would be expected from a live outing. The "White Zone" album was released under the alias Psychedelic Warriors to distance itself entirely from the rock expectancy of Hawkwind.
A general criticism of techno music at that time was its facelessness and lack of personality, which the band were coming to feel also plagued them. Ron Tree had known the band on the festival circuit and offered his services as a frontman, and the band duly employed him for the album "Alien 4" and its accompanying tour which resulted in the album "Love in Space" and video "Love in Space".
In 1996, unhappy with the musical direction of the band, bassist Davey left, forming his own Middle-Eastern flavoured hard-rock group Bedouin and a Motörhead tribute act named Ace of Spades. His bass playing role was reluctantly picked up by singer Tree and the band were joined full-time by lead guitarist Jerry Richards (another stalwart of the festival scene, playing for Tubilah Dog who had merged with Brock's Agents of Chaos during 1988) for the albums "Distant Horizons" and "In Your Area". Rasta chanter Captain Rizz also joined the band for guest spots during live shows.
2000s: Hawkestra, Turner-Brock Disputes, Voiceprint and Emergence of Stable Modern Lineup.
"Hawkestra"—a reunion event featuring appearances from past and present members—had originally been intended to coincide with the band's 30th anniversary and the release of the career spanning "Epocheclipse – 30 Year Anthology" set, but logistical problems delayed it until 21 October 2000. It took place at the Brixton Academy with about 20 members taking part in a 3+ hour set which was filmed and recorded. Guests included Samantha Fox who sang "Master of the Universe." However, arguments and disputes over financial recompense and musical input resulted in the prospect of the event being re-staged unlikely, and any album or DVD release being indefinitely shelved.
The Hawkestra had set a template for Brock to assemble a core band of Tree, Brock, Richards, Davey, Chadwick and for the use of former members as guests on live shows and studio recordings. The 2000 Christmas Astoria show was recorded with contributions from House, Blake, Rizz, Moorcock, Jez Huggett and Keith Kniveton and released as "Yule Ritual" the following year. In 2001, Davey agreed to rejoin the band permanently, but only after the departure of Tree and Richards.
Meanwhile, having rekindled relationships with old friends at the Hawkestra, Turner organised further Hawkestra gigs resulting in the formation of xhawkwind.com, a band consisting mainly of ex-Hawkwind members and playing old Hawkwind songs. An appearance at Guilfest in 2002 led to confusion as to whether this actually was Hawkwind, sufficiently irking Brock into taking legal action to prohibit Turner from trading under the name Hawkwind. Turner lost the case and the band now performs as Space Ritual.
An appearance at the Canterbury Sound Festival in August 2001, resulting in another live album "Canterbury Fayre 2001", saw guest appearances from Lloyd-Langton, House, Kniveton with Arthur Brown on "Silver Machine". The band organised the first of their own weekend festivals, named Hawkfest, in Devon in the summer of 2002. Brown joined the band in 2002 for a Winter tour which featured some Kingdom Come songs and saw appearances from Blake and Lloyd-Langton, the Newcastle show being released on DVD as "Out of the Shadows" and the London show on CD as "Spaced Out in London".
In 2005 the long anticipated new album "Take Me to Your Leader" was released. Recorded by the core band of Brock/Davey/Chadwick, contributors included new keyboardist Jason Stuart, Arthur Brown, tabloid writer and TV personality Matthew Wright, 1970s New Wave singer Lene Lovich, Simon House and Jez Huggett. This was followed in 2006 by the CD/DVD "Take Me to Your Future".
The band were the subject of an hour-long television documentary entitled "Hawkwind: Do Not Panic" that aired on BBC Four as part of the "Originals" series. It was broadcast on 30 March 2007 and repeated on 10 August 2007. Although Brock participated in its making he did not appear in the programme, it is alleged that he requested all footage of himself be removed after he was denied any artistic control over the documentary. In one of the documentary's opening narratives regarding Brock, it is stated that he declined to be interviewed for the programme because of Nik Turner's involvement, indicating that the two men have still not been reconciled over the xhawkwind.com incident.
December 2006 saw the official departure of Alan Davey, who left to perform and record with two new bands: Gunslinger and Thunor. He was replaced by Mr Dibs, a long-standing member of the road crew. The band performed at their annual Hawkfest festival and headlined the US festival Nearfest and played gigs in PA and NY. At the end of 2007, Tim Blake once again joined the band filling the lead role playing keyboards and theremin. The band played 5 Christmas dates, the London show being released as an audio CD and video DVD under the title "Knights of Space".
In January 2008 the band reversed its anti-taping policy, long a sore-point with many fans, announcing that it would allow audio recording and non-commercial distribution of such recordings, provided there was no competing official release. At the end of 2008, Atomhenge Records (a subsidiary of Cherry Red Records) commenced the re-issuing of Hawkwind's back catalogue from the years 1976 through to 1997 with the release of two triple CD anthologies "Spirit of the Age (anthology 1976-84)" and "The Dream Goes On (anthology 1985-97)".
On 8 September 2008 keyboard player Jason Stuart died due to a brain haemorrhage. In October 2008, Niall Hone (former Tribe of Cro) joined Hawkwind for their Winter 2008 tour playing guitar, along with returning synth/theremin player Tim Blake. In this period, Hone also occasionally played bass guitar alongside Mr Dibs and used laptops for live electronic improvisation.
In 2009, the band began occasionally featuring Jon Sevink, from The Levellers as guest violinist at some shows. Later that year, Hawkwind embarked on a winter tour to celebrate the band's 40th anniversary, including two gigs on 28 and 29 August marking the anniversary of their first live performances. In 2010, Hawkwind held their annual Hawkfest at the site of the original Isle Of Wight Festival, marking the 40th anniversary of their appearance there.
2010s: Eastworld Era and Beyond.
On 21 June 2010, Hawkwind released a studio album entitled "Blood of the Earth" on Eastworld Records. During and since the "Blood of the Earth" support tours, Hone's primary on-stage responsibility shifted to bass, while Mr. Dibs moved to a more traditional lead singer/front man role.
In 2011, Hawkwind toured Australia for the second time.
April 2012 saw the release of a new album, "Onward", again on Eastworld. Keyboardist Dead Fred rejoined Hawkwind for the 2012 tour in support of "Onward" and has since remained with the band. In November 2012, Brock, Chadwick and Hone—credited as "Hawkwind Light Orchestra"—released "Stellar Variations" on Esoteric Recordings.
2013 marked the first Hawkeaster, a two-day festival held in Seaton, Devon during the Easter weekend. A US tour was booked for October 2013, but due to health issues, was postponed and later cancelled.
In February 2014, as part of a one-off Space Ritual performance, Hawkwind performed at the O2 Shepherd's Bush Empire featuring an appearance by Brian Blessed for the spoken word element of Sonic Attack; a studio recording of this performance was released as a single in September 2014. Later in the year, former Soft Machine guitarist John Etheridge joined the live line-up of the band, though he had departed again prior to early 2015 dates. 
Influence and legacy.
Hawkwind have been cited as an influence by artists such as Al Jourgensen of Ministry, Monster Magnet, the Sex Pistols (who covered "Silver Machine"), Henry Rollins of Black Flag, Ty Segall, and Ozric Tentacles.
Hard rock musician Lemmy of the band Motörhead gained a lot from his tenure in Hawkwind. He has remarked, "I really found myself as an instrumentalist in Hawkwind. Before that I was just a guilt player who was pretending to be good, when actually I was no good at all. In Hawkwind I became a good bass player. It was where I learned I was good at something."
Further reading.
There are three biographies of Hawkwind.

</doc>
<doc id="13645" url="http://en.wikipedia.org/wiki?curid=13645" title="Horse">
Horse

The horse ("Equus ferus caballus") is one of two extant subspecies of "Equus ferus". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature into the large, single-toed animal of today. Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies "caballus" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.
Horses' anatomy enables them to make use of speed to escape predators and they have a well-developed sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under saddle or in harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.
Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods", such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.
Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water and shelter, as well as attention from specialists such as veterinarians and farriers.
Biology.
Specific terms and specialized language are used to describe equine anatomy, different life stages, colors and breeds.
Lifespan and life stages.
Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was "Old Billy", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in "Guinness World Records" as the world's oldest living pony, died in 2007 at age 56.
Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.
The following terminology is used to describe horses of various ages:
In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.
Size and measurement.
The height of horses is usually measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.
In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to 101.6 mm. The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation "h" or "hh" (for "hands high"). Thus, a horse described as "15.2 h" is 15 hands plus 2 inches, for a total of 62 in in height.
The size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from 14<b/> to <b/>16 hands (<b/> to <b/> inches, <b/> to <b/> cm) and can weigh from 380 to. Larger riding horses usually start at about  hands and often are as tall as  hands , weighing from 500 to. Heavy or draft horses are usually at least  hands high and can be as tall as  hands high. They can weigh from about 700 to.
The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood  hands high and his peak weight was estimated at 1500 kg. The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is 17 in tall and weighs 57 lb.
Ponies.
Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.
The traditional standard for height of a horse or a pony at maturity is  hands . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under  hands , For competition in the Western division of the United States Equestrian Federation, the cutoff is  hands The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than 148 cm at the withers without shoes, which is just over 14.2 h, and 149 cm, or just over 14.2½ h, with shoes.
Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.
Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of equine intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages  hands , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than 30 in, are classified by their registries as very small horses, not ponies.
Genetics.
Horses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.
Colors and markings.
Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.
Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the "extension gene" or "red factor," as its recessive form is "red" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.
Horses which have a white coat color are often mislabeled; a horse that looks "white" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no "albino" horses, defined as having both pink skin and red eyes.
Reproduction and development.
Gestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an "anestrus" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.
Horses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.
Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.
Anatomy.
Skeletal system.
The horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's "knee" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the "ankle") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the "knuckles" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.
Hooves.
The critical importance of the feet and legs is summed up by the traditional adage, "no foot, no horse". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average 500 kg, travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.
Teeth.
Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called "tushes". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as "wolf" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or "bars" of the horse's mouth when the horse is bridled.
An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.
Digestion.
Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A 450 kg horse will eat 7 to of food per day and, under normal use, drink 38 to of water. Horses are not ruminants, so they have only one stomach, like humans, but unlike humans, they can digest cellulose, a major component of grass. Cellulose digestion occurs in the cecum, or "water gut", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.
Senses.
The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.
Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.
A horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.
Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.
Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.
Movement.
All horses move naturally with four basic gaits: the four-beat walk, which averages 6.4 km/h; the two-beat trot or jog at 13 to (faster for harness racing horses); the canter or lope, a three-beat gait that is 19 to; and the gallop. The gallop averages 40 to, but the world record for a horse galloping over a short, sprint distance is 88 km/h. Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat "ambling" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.
Behavior.
Horses are prey animals with a strong fight-or-flight response. Their first reaction to threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.
Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, "weaving" (rocking back and forth), and other problems.
Intelligence and learning.
Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between "more or less" if the quantity involved is less than four.
Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that "intelligent" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.
Temperament.
Horses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the "hot-bloods", such as many race horses, exhibit more sensitivity and energy, while the "cold-bloods", such as most draft breeds, are quieter and calmer. Sometimes "hot-bloods" are classified as "light horses" or "riding horses", with the "cold-bloods" classified as "draft horses" or "work horses".
"Hot blooded" breeds include "oriental horses" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.
Muscular, heavy draft horses are known as "cold bloods", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed "gentle giants". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.
"Warmblood" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a "light horse" or "riding horse".
Today, the term "Warmblood" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term "warm blood" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.
Sleep patterns.
Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a "stay apparatus" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.
Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.
Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.
Taxonomy and evolution.
The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), the tapir, and the rhinoceros—have survived to the present day.
The earliest known member of the Equidae family was the "Hyracotherium", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the "Mesohippus", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern "Equus" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.
By about 15,000 years ago, "Equus ferus" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.
Wild species surviving into modern times.
A truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most "wild" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the Tarpan and the Przewalski's Horse, survived into recorded history and only the latter survives today.
The Przewalski's Horse ("Equus ferus przewalskii"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian Wild Horse; Mongolian people know it as the "taki", and the Kyrgyz people call it a "kirtag". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.
The Tarpan or European Wild Horse ("Equus ferus ferus") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the Tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.
Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.
Other modern equids.
Besides the horse, there are seven other species of genus "Equus" in the Equidae family. These are the ass or donkey, "Equus asinus"; the mountain zebra, "Equus zebra"; plains zebra, "Equus quagga"; Grévy's zebra, "Equus grevyi"; the kiang, "Equus kiang"; and the onager, "Equus hemionus".
Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.
Domestication.
Domestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.
The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.
Domestication is also studied by using the genetic material of present day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.
Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.
Feral populations.
Feral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.
There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in "wild" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.
Breeds.
The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called "thoroughbreds". Thoroughbred is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.
Breeds developed due to a need for "form to function", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.
Interaction with humans.
Worldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 "poll" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.
Communication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.
Sport.
Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.
Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as "In-hand" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.
Sports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.
Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: "flat" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.
Work.
There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.
Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.
Entertainment and culture.
Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.
Horses are frequently seen in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.
Therapeutic use.
People of all ages with physical and mental disabilities obtain beneficial results from association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.
Horses also provide psychological benefits to people whether they actually ride or not. "Equine-assisted" or "equine-facilitated" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.
Warfare.
Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 to 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.
Products.
Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.
Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.
Horse meat has been used as food for humans and carnivorous animals throughout the ages. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a "spinto", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.
Care.
Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a 450 kg adult horse could eat up to 11 kg of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.
Horses require a plentiful supply of clean water, a minimum of 10 USgal to 12 USgal per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.
Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.
Sources.
</dl>
Further reading.
</dl>

</doc>
<doc id="13647" url="http://en.wikipedia.org/wiki?curid=13647" title="Hermann Ebbinghaus">
Hermann Ebbinghaus

Hermann Ebbinghaus (January 24, 1850 – February 26, 1909) was a German psychologist who pioneered the experimental study of memory, and is known for his discovery of the forgetting curve and the spacing effect. He was also the first person to describe the learning curve. He was the father of the eminent neo-Kantian philosopher Julius Ebbinghaus.
Early life.
Ebbinghaus was born in Barmen, in the Rhine Province of the Kingdom of Prussia, as the son of a wealthy Lutheran merchant, Carl Ebbinghaus. Little is known about his infancy except that he was brought up in the Lutheran faith and was a pupil at the town Gymnasium. At the age of 17 (1867), he began attending the University of Bonn, where he had planned to study history and philology. However, during his time there he developed an interest in philosophy. In 1870, his studies were interrupted when he served with the Prussian Army in the Franco-Prussian War. Following this short stint in the military, Ebbinghaus finished his dissertation on Eduard von Hartmann's "Philosophie des Unbewussten (Philosophy of the Unconscious)", and received his doctorate on August 16, 1873, when he was 23 years old. During the next three years, he moved around, spending time at Halle and Berlin.
Professional career.
After acquiring his PhD, Ebbinghaus moved around England and France, tutoring students to support himself. In England, he may have taught in two small schools in the South of the country (Gorfein, 1885). In London, in a used bookstore, he came across Gustav Fechner's book "Elemente der Psychophysik" ("Elements of Psychophysics"), which spurred him to conduct his famous memory experiments. After beginning his studies at the University of Berlin, he founded the third psychological testing lab in Germany (third to Wilhelm Wundt and G.E. Muller). He began his memory studies here in 1879. In 1885, the same year that he published his monumental work, "Memory: A Contribution to Experimental Psychology", he was made a professor at the University of Berlin, most likely in recognition of this publication. In 1890, along with Arthur König, he founded the Psychological journal "Zeitschrift für Physiologie und Psychologie der Sinnesorgane" ("The Psychology and Physiology of the Sense Organs").
In 1894, he was passed over for promotion to head of the philosophy department at Berlin, most likely due to his lack of publications. Instead, Carl Stumpf received the promotion. As a result of this, Ebbinghaus left to join the University of Breslau (now Wrocław, Poland), in a chair left open by Theodor Lipps (who took over Stumpf's position when he moved to Berlin). While in Breslau, he worked on a commission that studied how children's mental ability declined during the school day. While the specifics on how these mental abilities were measured have been lost, the successes achieved by the commission laid the groundwork for future intelligence testing.:207 At Breslau, he again founded a psychological testing laboratory.
In 1902, Ebbinghaus published his next piece of writing entitled "Die Grundzüge der Psychologie" ("Fundamentals of Psychology"). It was an instant success and continued to be long after his death. In 1904, he moved to the Halle where he spent the last few years of his life. His last published work, "Abriss der Psychologie" ("Outline of Psychology") was published six years later, in 1908. This, too, continued to be a success, being re-released in eight different editions.:208 Shortly after this publication, on February 26, 1909, Ebbinghaus died from pneumonia at the age of 59.
Research on memory.
Ebbinghaus was determined to show that higher mental processes could actually be studied using experimentation, which was in opposition in the popular held thought of the time. To control for most potentially confounding variables, Ebbinghaus wanted to use simple acoustic encoding and maintenance rehearsal for which a list of words could have been used. As learning would be affected by prior knowledge and understanding, he needed something that could be easily memorized but which had no prior cognitive associations. Easily formable associations with regular words would interfere with his results, so he used items that would later be called "nonsense syllables" (also known as the CVC trigram). A nonsense syllable is a consonant-vowel-consonant combination, where the consonant does not repeat and the syllable does not have prior meaning. BOL (sounds like "Ball") and DOT (already a word) would then not be allowed. However, syllables such as DAX, BOK, and YAT would all be acceptable (though Ebbinghaus left no examples). After eliminating the meaning-laden syllables, Ebbinghaus ended up with 2,300 resultant syllables. Once he had created his collection of syllables, he would pull out a number of random syllables from a box and then write them down in a notebook. Then, to the regular sound of a metronome, and with the same voice inflection, he would read out the syllables, and attempt to recall them at the end of the procedure. One investigation alone required 15,000 recitations.
It was later determined that humans impose meaning even on nonsense syllables to make them more meaningful. The nonsense syllable PED (which is the first three letters of the word "pedal") turns out to be less nonsensical than a syllable such as KOJ; the syllables are said to differ in association value. It appears that Ebbinghaus recognized this, and only referred to the strings of syllables as "nonsense" in that the syllables might be less likely to have a specific meaning and he should make no attempt to make associations with them for easier retrieval.
Limitations to memory research.
There are several limitations to his work on memory. The most important one was that Ebbinghaus was the only subject in his study. This limited the study's generalizability to the population. Although he attempted to regulate his daily routine to maintain more control over his results, his decision to avoid the use of participants sacrificed the external validity of the study despite sound internal validity. In addition, although he tried to account for his personal influences, there is an inherent bias when someone serves as researcher as well as participant. Also, Ebbinghaus's memory research halted research in other, more complex matters of memory such as semantic and procedural memory and mnemonics.
Contributions to memory.
In 1885, he published his groundbreaking "Über das Gedächtnis" ("On Memory", later translated to English as "Memory. A Contribution to Experimental Psychology") in which he described experiments he conducted on himself to describe the processes of learning and forgetting.
Ebbinghaus made several findings that are still relevant and supported to this day. First, arguably his most famous finding, the forgetting curve. The forgetting curve describes the exponential loss of information that one has learned. The sharpest decline occurs in the first twenty minutes and the decay is significant through the first hour. The curve levels off after about one day.
The learning curve described by Ebbinghaus refers to how fast one learns information. The sharpest increase occurs after the first try and then gradually evens out, meaning that less and less new information is retained after each repetition. Like the forgetting curve, the learning curve is exponential. Ebbinghaus had also documented the serial position effect, which describes how the position of an item affects recall. The two main concepts in the serial position effect are recency and primacy. The recency effect describes the increased recall of the most recent information because it is still in the short-term memory. The primacy effect causes better memory of the first items in a list due to increased rehearsal and commitment to long-term memory.
Another important discovery is that of savings. This refers to the amount of information retained in the subconscious even after this information cannot be consciously accessed. Ebbinghaus would memorize a list of items until perfect recall and then would not access the list until he could no longer recall any of its items. He then would relearn the list, and compare the new learning curve to the learning curve of his previous memorization of the list. The second list was generally memorized faster, and this difference between the two learning curves is what Ebbinghaus called "savings". Ebbinghaus also described the difference between involuntary and voluntary memory, the former occurring "with apparent spontaneity and without any act of the will" and the latter being brought "into consciousness by an exertion of the will".
Prior to Ebbinghaus, most contributions to the study of memory were undertaken by philosophers and centered on observational description and speculation. For example, Immanuel Kant used pure description to discuss recognition and its components and Sir Francis Bacon claimed that the simple observation of the rote recollection of a previously learned list was "no use to the art" of memory. This dichotomy between descriptive and experimental study of memory would resonate later in Ebbinghaus's life, particularly in his public argument with former colleague Wilhelm Dilthey. However, more than a century before Ebbinghaus, Johann Andreas Segner invented the "Segner-wheel" to see the length of after-images by seeing how fast a wheel with a hot coal attached had to move for the red ember circle from the coal to appear complete. (see iconic memory)
Ebbinghaus's effect on memory research was almost immediate. With very few works published on memory in the previous two millennia, Ebbinghaus's works spurred memory research in the United States in the 1890s, with 32 papers published in 1894 alone. This research was coupled with the growing development of mechanized mnemometers, or devices that aided in the recording and study of memory.
The reaction to his work in his day was mostly positive. Noted psychologist William James called the studies "heroic" and said that they were "the single most brilliant investigation in the history of psychology". Edward B. Titchener also mentioned that the studies were the greatest undertaking in the topic of memory since Aristotle.
Other contributions.
Ebbinghaus can also be credited with pioneering sentence completion exercises, which he developed in studying the abilities of schoolchildren. It was these same exercises that Alfred Binet had borrowed and incorporated into the Binet-Simon intelligence scale. Sentence completion had since then also been used extensively in memory research, especially in tapping into measures of implicit memory, and also has been used in psychotherapy as a tool to help tap into the motivations and drives of the patient. He had also influenced Charlotte Bühler, who along with Lev Vygotsky and others went on to study language meaning and society.
 Ebbinghaus is also credited with discovering an optical illusion now known after its discoverer—the Ebbinghaus illusion, which is an illusion of relative size perception. In the best-known version of this illusion, two circles of identical size are placed near to each other and one is surrounded by large circles while the other is surrounded by small circles; the first central circle then appears smaller than the second central circle. This illusion is now used extensively in research in cognitive psychology, to find out more about the various perception pathways in our brain.
Ebbinghaus is also largely credited with drafting the first standard research report. In his paper on memory, Ebbinghaus arranged his research into four sections: the introduction, the methods, the results, and a discussion section. The clarity and organization of this format was so impressive to contemporaries that it has now become standard in the discipline, and all research reports follow the same standards laid out by Ebbinghaus.
Unlike notable contemporaries like Titchener and James, Ebbinghaus did not promote any specific school of psychology nor was he known for extensive lifetime research, having done only three works. He never attempted to bestow upon himself the title of the pioneer of experimental psychology, did not seek to have any "disciples", and left the exploitation of the new field to others.
Discourse on the nature of psychology.
In addition to pioneering experimental psychology, Ebbinghaus was also a strong defender of this direction of the new science, as is illustrated by his public dispute with University of Berlin colleague, Wilhelm Dilthey. Shortly after Ebbinghaus left Berlin in 1893, Dilthey published a paper extolling the virtues of descriptive psychology, and condemning experimental psychology as boring, claiming that the mind was too complex, and that introspection was the desired method of studying the mind. The debate at the time had been primarily whether psychology should aim to explain or understand the mind and whether it belonged to the natural or human sciences. Many had seen Dilthey's work as an outright attack on experimental psychology, Ebbinghaus included, and he responded to Dilthey with a personal letter and also a long scathing public article. Amongst his counterarguments against Dilthey he mentioned that it is inevitable for psychology to do hypothetical work and that the kind of psychology that Dilthey was attacking was the one that existed before Ebbinghaus's "experimental revolution". Charlotte Bühler echoed his words some forty years later, stating that people like Ebbinghaus "buried the old psychology in the 1890s". Ebbinghaus explained his scathing review by saying that he could not believe that Dilthey was advocating the status quo of structuralists like Wilhelm Wundt and Titchener and attempting to stifle psychology's progress.
Some contemporary texts still describe Ebbinghaus as a philosopher rather than a psychologist and he had also spent his life as a professor of philosophy. However, Ebbinghaus himself would probably describe himself as a psychologist considering that he fought to have psychology viewed as a separate discipline from philosophy.
Influences.
There has been some speculation as to what influenced Ebbinghaus in his undertakings. None of his professors seem to have influenced him, nor are there suggestions that his colleagues affected him. Von Hartmann's work, on which Ebbinghaus based his doctorate, did suggest that higher mental processes were hidden from view, which may have spurred Ebbinghaus to attempt to prove otherwise. The one influence that has always been cited as having inspired Ebbinghaus was Gustav Fechner's "Elements of Psychophysics", a book which he purchased second-hand in England. It is said that the meticulous mathematical procedures impressed Ebbinghaus so much that he wanted to do for psychology what Fechner had done for psychophysics. This inspiration is also evident in that Ebbinghaus dedicated his second work "Principles of Psychology" to Fechner, signing it "I owe everything to you.":206

</doc>
<doc id="13648" url="http://en.wikipedia.org/wiki?curid=13648" title="Hilbert (disambiguation)">
Hilbert (disambiguation)

Hilbert may refer to:
People:
Places:
Other uses:

</doc>
<doc id="13652" url="http://en.wikipedia.org/wiki?curid=13652" title="Hindi">
Hindi

Hindi (हिन्दी), or more precisely Modern Standard Hindi (मानक हिन्दी), is a standardised and Sanskritised register of the Hindustani language. Hindustani is the native language of most people living in Delhi, Uttar Pradesh, Uttarakhand, Chhattisgarh, Himachal Pradesh, Chandigarh, Bihar, Jharkhand, Madhya Pradesh, Haryana, and Rajasthan. Modern Standard Hindi is one of the official languages of India.
As of 2009, the best figure "Ethnologue" could find for speakers of actual Hindustani Hindi was 180 million in 1991. In the 2001 Indian census, 258 million (258,000,000) people in India reported Hindi to be their native language. However, this number included millions of people who were native speakers of related languages but who thought of their speech as a dialect of Hindi.
Official status.
Article 343 (1) of the Indian constitution states "The official language of the Union shall be Hindi in Devanagari script. The form of numerals to be used for the official purposes of the Union shall be the international form of Indian numerals."
Article 351 of the Indian constitution states "It shall be the duty of the Union to promote the spread of the Hindi language, to develop it so that it may serve as a medium of expression for all the elements of the composite culture of India and to secure its enrichment by assimilating without interfering with its genius, the forms, style and expressions used in Hindustani and in the other languages of India specified in the Eighth Schedule, and by drawing, wherever necessary or desirable, for its vocabulary, primarily on Sanskrit and secondarily on other languages." 
It was envisioned that Hindi would become the sole working language of the Union Government by 1965 (per directives in Article 344 (2) and Article 351), with state governments being free to function in the language of their own choice. However, widespread resistance to the imposition of Hindi on non-native speakers, especially in South India (such as the those in Tamil Nadu), Maharashtra ,Andhra, and West Bengal, led to the passage of the Official Languages Act of 1963, which provided for the continued use of English indefinitely for all official purposes, although the constitutional directive for the Union Government to encourage the spread of Hindi was retained and has strongly influenced its policies.
At the state level, Hindi is the official language of the following Indian states: Bihar, Chhattisgarh, Haryana, Himachal Pradesh, Jharkhand, Madhya Pradesh, Rajasthan, Uttar Pradesh, and Uttarakhand. Each may also designate a "co-official language"; in Uttar Pradesh, for instance, depending on the political formation in power, this language is generally Urdu. Similarly, Hindi is accorded the status of official language in the following Union Territories: Andaman & Nicobar Islands, Chandigarh, Dadra & Nagar Haveli, Daman & Diu, National Capital Territory.
National-language status for Hindi is a long-debated theme. An Indian court clarified that Hindi is not the national language of India because the constitution does not mention it as such.
Outside of Asia, Hindi is also an official language in Fiji. The Constitution of Fiji declares three official languages: English, Fijian, and Hindi. The Hindi spoken there is Fiji Hindi, a form of Awadhi, not Modern Standard Hindi's Hindustani.
History.
The dialect of Hindustani on which Standard Hindi is based is "Khariboli", the vernacular of Delhi and the surrounding western Uttar Pradesh and southern Uttarakhand. This dialect acquired linguistic prestige in the Mughal Empire (1600s) and became known as "Urdu", "the language of the court". In the late 19th century, the movement standardising a written language from Khariboli, for the Indian masses in North India, started to standardise Hindi as a separate language from Urdu, which was learnt by the elite. In 1881 Bihar accepted Hindi as its sole official language, replacing Urdu, and thus became the first state of India to adopt Hindi.
After independence, the government of India instituted the following conventions:
The Constituent Assembly adopted Hindi as the Official Language of the Union on 14 September 1949. Hence, it is celebrated as Hindi Day.
Comparison with Modern Standard Urdu.
Linguistically, Hindi and Urdu are the same language. Hindi is written in the Devanagari script and uses more Sanskrit words, whereas Urdu is written in the Perso-Arabic script and uses more Arabic and Persian words.
Script.
Hindi is written in Devanagari script ("देवनागरी लिपि" devanāgarī lipi) also called Nagari. Devanagari consists of 11 vowels and 33 consonants and is written from left to right.
Sanskrit vocabulary.
Formal Standard Hindi draws much of its academic vocabulary from Sanskrit. Standard Hindi loans words are divided into five principal categories:
The Hindi standard, from which much of the Persian, Arabic and English vocabulary has been purged and replaced by neologisms compounding "tatsam" words, is called "Shuddha Hindi" (pure Hindi), and is viewed as a more prestigious dialect over other more colloquial forms of Hindi.
Excessive use of "tatsam" words creates problems for native speakers. They may have Sanskrit consonant clusters which do not exist in native Hindi. The educated middle class of India may be able to pronounce such words, but others have difficulty. Persian and Arabic vocabulary given 'authentic' pronunciations cause similar difficulty.
Literature.
Hindi literature is broadly divided into four prominent forms or styles, being "Bhakti" (devotional – Kabir, Raskhan); "Shringar" (beauty – Keshav, Bihari); "Virgatha" (extolling brave warriors); and "Adhunik" (modern).
Medieval Hindi literature is marked by the influence of Bhakti movement and the composition of long, epic poems. It was primarily written in other varieties of Hindi, particularly Avadhi and Braj Bhasha, but also in Khariboli. During the British Raj, Hindustani became the prestige dialect. Hindustani with heavily Sanskritised vocabulary or "Sahityik" Hindi (Literary Hindi) was popularised by the writings of Swami Dayananda Saraswati, Bhartendu Harishchandra and others. The rising numbers of newspapers and magazines made Hindustani popular with the educated people. Chandrakanta, written by Devaki Nandan Khatri, is considered the first authentic work of prose in modern Hindi. The person who brought realism in the Hindi prose literature was Munshi Premchand, who is considered as the most revered figure in the world of Hindi fiction and progressive movement.
The "Dwivedi Yug" ("Age of Dwivedi") in Hindi literature lasted from 1900 to 1918. It is named after Mahavir Prasad Dwivedi, who played a major role in establishing the Modern Hindi language in poetry and broadening the acceptable subjects of Hindi poetry from the traditional ones of religion and romantic love.
In the 20th century, Hindi literature saw a romantic upsurge. This is known as "Chhayavaad" ("shadowism") and the literary figures belonging to this school are known as "Chhayavaadi". Jaishankar Prasad, Suryakant Tripathi 'Nirala', Mahadevi Varma and Sumitranandan Pant, are the four major "Chhayavaadi" poets.
"Uttar Adhunik" is the post-modernist period of Hindi literature, marked by a questioning of early trends that copied the West as well as the excessive ornamentation of the Chhayavaadi movement, and by a return to simple language and natural themes.
Internet.
Hindi has a presence on the internet, but due to lack of standard encoding, search engines cannot be used to locate text. Hindi is one of the seven languages of India that can be used to make web addresses.(URLs).
Hindi has also impacted the language of technology, with words such as 'avatar' (meaning a spirit taking a new form) used in computer sciences, artificial intelligence and even robotics.
Sample text.
The following is a sample text in High Hindi, of the Article 1 of the Universal Declaration of Human Rights (by the United Nations):

</doc>
<doc id="13653" url="http://en.wikipedia.org/wiki?curid=13653" title="Huginn and Muninn">
Huginn and Muninn

In Norse mythology, Huginn (from Old Norse "thought") and Muninn (Old Norse "memory" or "mind") are a pair of ravens that fly all over the world, Midgard, and bring information to the god Odin. Huginn and Muninn are attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources: the "Prose Edda" and "Heimskringla", written in the 13th century by Snorri Sturluson; in the "Third Grammatical Treatise", compiled in the 13th century by Óláfr Þórðarson; and in the poetry of skalds. The names of the ravens are sometimes modernly anglicized as Hugin and Munin.
In the "Poetic Edda", a disguised Odin expresses that he fears that they may not return from their daily flights. The "Prose Edda" explains that Odin is referred to as "raven-god" due to his association with Huginn and Muninn. In the "Prose Edda" and the "Third Grammatical Treatise", the two ravens are described as perching on Odin's shoulders. "Heimskringla" details that Odin gave Huginn and Muninn the ability to speak.
Migration Period golden bracteates, Vendel era helmet plates, a pair of identical Germanic Iron Age bird-shaped brooches, Viking Age objects depicting a moustached man wearing a helmet, and a portion of the 10th or 11th century may depict Odin with one of the ravens. Huginn and Muninn's role as Odin's messengers has been linked to shamanic practices, the Norse raven banner, general raven symbolism among the Germanic peoples, and the Norse concepts of the fylgja and the hamingja.
Attestations.
In the "Poetic Edda" poem "Grímnismál", the god Odin (disguised as "Grímnir") provides the young Agnarr with information about Odin's companions. He tells the prince about Odin's wolves Geri and Freki, and, in the next stanza of the poem, states that Huginn and Muninn fly daily across the entire world, Midgard. Grímnir says that he worries Huginn may not come back, yet more does he fear for Muninn:
In the "Prose Edda" book "Gylfaginning" (chapter 38), the enthroned figure of High tells Gangleri (king Gylfi in disguise) that two ravens named Huginn and Muninn sit on Odin's shoulders. The ravens tell Odin everything they see and hear. Odin sends Huginn and Muninn out at dawn, and the birds fly all over the world before returning at dinner-time. As a result, Odin is kept informed of many events. High adds that it is from this association that Odin is referred to as "raven-god". The above-mentioned stanza from "Grímnismál" is then quoted.
In the "Prose Edda" book "Skáldskaparmál" (chapter 60), Huginn and Muninn appear in a list of poetic names for ravens. In the same chapter, excerpts from a work by the skald Einarr Skúlason are provided. In these excerpts Muninn is referenced in a common noun for 'raven' and Huginn is referenced in a kenning for 'carrion'.
In the "Heimskringla" book "Ynglinga saga", an euhemerized account of the life of Odin is provided. Chapter 7 describes that Odin had two ravens, and upon these ravens he bestowed the gift of speech. These ravens flew all over the land and brought him information, causing Odin to become "very wise in his lore."
In the "Third Grammatical Treatise" an anonymous verse is recorded that mentions the ravens flying from Odin's shoulders; Huginn seeking hanged men, and Muninn slain bodies. The verse reads:
Archaeological record.
Migration Period (5th and 6th centuries AD) gold bracteates (types A, B, and C) feature a depiction of a human figure above a horse, holding a spear and flanked by one or more often two birds. The presence of the birds has led to the iconographic identification of the human figure as the god Odin, flanked by Huginn and Muninn. Like Snorri's "Prose Edda" description of the ravens, a bird is sometimes depicted at the ear of the human, or at the ear of the horse. Bracteates have been found in Denmark, Sweden, Norway and, in smaller numbers, England and areas south of Denmark. Austrian Germanist Rudolf Simek states that these bracteates may depict Odin and his ravens healing a horse and may indicate that the birds were originally not simply his battlefield companions but also "Odin's helpers in his veterinary function."
Vendel era helmet plates (from the 6th or 7th century) found in grave in Sweden depict a helmeted figure holding a spear and a shield while riding a horse, flanked by two birds. The plate has been interpreted as Odin accompanied by two birds: his ravens.
A pair of identical Germanic Iron Age bird-shaped brooches from Bejsebakke in northern Denmark may be depictions of Huginn and Muninn. The back of each bird features a mask motif, and the feet of the birds are shaped like the heads of animals. The feathers of the birds are also composed of animal heads. Together, the animal heads on the feathers form a mask on the back of the bird. The birds have powerful beaks and fan-shaped tails, indicating that they are ravens. The brooches were intended to be worn on each shoulder, after Germanic Iron Age fashion. Archaeologist Peter Vang Petersen comments that while the symbolism of the brooches is open to debate, the shape of the beaks and tail feathers confirm that the brooch depictions are ravens. Petersen notes that "raven-shaped ornaments worn as a pair, after the fashion of the day, one on each shoulder, makes one's thoughts turn towards Odin's ravens and the cult of Odin in the Germanic Iron Age." Petersen says that Odin is associated with disguise and that the masks on the ravens may be portraits of Odin.
The Oseberg tapestry fragments, discovered within the Viking Age Oseberg ship burial in Norway, feature a scene containing two black birds hovering over a horse, possibly originally leading a wagon (as a part of a procession of horse-led wagons on the tapestry). In her examination of the tapestry, scholar Anne Stine Ingstad interprets these birds as Huginn and Muninn flying over a covered cart containing an image of Odin, drawing comparison with the images of Nerthus attested by Tacitus in 1 CE.
Excavations in Ribe in Denmark have recovered a Viking Age lead metal-caster's mould and 11 identical casting-moulds. These objects depict a moustached man wearing a helmet that features two head-ornaments. Archaeologist Stig Jensen proposes that these ornaments should be interpreted as Huginn and Muninn, and the wearer as Odin. He notes that "similar depictions occur everywhere the Vikings went—from eastern England to Russia and naturally also in the rest of Scandinavia."
A portion of (a partly surviving runestone erected at Kirk Andreas on the Isle of Man) depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, and a large bird on his shoulder. Andy Orchard comments that this bird may be either Huginn or Muninn. Rundata dates the cross to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by the monstrous wolf Fenrir during the events of Ragnarök.
In November 2009, the Roskilde Museum announced the discovery and subsequent display of a niello-inlaid silver figurine found in Lejre, Denmark, which they dubbed "Odin from Lejre". The silver object depicts a person sitting on a throne. The throne features the heads of animals and is flanked by two birds. The Roskilde Museum identifies the figure as Odin sitting on his throne Hliðskjálf, flanked by the ravens Huginn and Muninn.
Theories.
The "Heliand", an Old Saxon adaptation of the New Testament from the 9th century, differs from the New Testament in that an explicit reference is made to a dove sitting on the shoulder of Christ. Regarding this, G. Ronald Murphy says "In placing the powerful white dove not just above Christ, but right on his shoulder, the "Heliand" author has portrayed Christ, not only as the Son of the All-Ruler, but also as a new Woden. This deliberate image of Christ triumphantly astride the land with the magnificent bird on his shoulders (the author is perhaps a bit embarrassed that the bird is an unwarlike dove!) is an image intended to calm the fears and longings of those who mourn the loss of Woden and who want to return to the old religion's symbols and ways. With this image, Christ becomes a Germanic god, one into whose ears the Spirit of the Almighty whispers".
Scholars have linked Odin's relation to Huginn and Muninn to shamanic practice. John Lindow relates Odin's ability to send his "thought" (Huginn) and "mind" (Muninn) to the trance-state journey of shamans. Lindow says the "Grímnismál" stanza where Odin worries about the return of Huginn and Muninn "would be consistent with the danger that the shaman faces on the trance-state journey."
Rudolf Simek is critical of the approach, stating that "attempts have been made to interpret Odin's ravens as a personification of the god's intellectual powers, but this can only be assumed from the names Huginn and Muninn themselves which were unlikely to have been invented much before the 9th or 10th centuries" yet that the two ravens, as Odin's companions, appear to derive from much earlier times. Instead, Simek connects Huginn and Muninn with wider raven symbolism in the Germanic world, including the Raven Banner (described in English chronicles and Scandinavian sagas), a banner which was woven in a method that allowed it, when fluttering in the wind, to appear as if the raven depicted upon it was beating its wings.
Anthony Winterbourne connects Huginn and Muninn to the Norse concepts of the fylgja—a concept with three characteristics; shape-shifting abilities, good fortune, and the guardian spirit—and the hamingja—the ghostly double of a person that may appear in the form of an animal. Winterbourne states that "The shaman's journey through the different parts of the cosmos is symbolized by the "hamingja" concept of the shape-shifting soul, and gains another symbolic dimension for the Norse soul in the account of Oðin's ravens, Huginn and Muninn." In response to Simek's criticism of attempts to interpret the ravens "philosophically", Winterbourne says that "such speculations [...] simply strengthen the conceptual significance made plausible by other features of the mythology" and that the names "Huginn" and "Muninn" "demand more explanation than is usually provided."
Bernd Heinrich theorizes that Huginn and Muninn, along with Odin and his wolves Geri and Freki, reflect a symbiosis observed in the natural world among ravens, wolves, and humans on the hunt:
References.
</dl>

</doc>
<doc id="13654" url="http://en.wikipedia.org/wiki?curid=13654" title="Heat engine">
Heat engine

In thermodynamics, a heat engine is a system that converts heat or thermal energy to mechanical energy, which can then be used to do mechanical work. It does this by bringing a working substance from a higher state temperature to a lower state temperature. A heat "source" generates thermal energy that brings the working substance to the high temperature state. The working substance generates work in the "working body" of the engine while transferring heat to the colder "sink" until it reaches a low temperature state. During this process some of the thermal energy is converted into work by exploiting the properties of the working substance. The working substance can be any system with a non-zero heat capacity, but it usually is a gas or liquid.
In general an engine converts energy to mechanical work. Heat engines distinguish themselves from other types of engines by the fact that their efficiency is fundamentally limited by Carnot's theorem. Although this efficiency limitation can be a drawback, an advantage of heat engines is that most forms of energy can be easily converted to heat by processes like exothermic reactions (such as combustion), absorption of light or energetic particles, friction, dissipation and resistance. Since the heat source that supplies thermal energy to the engine can thus be powered by virtually any kind of energy, heat engines are very versatile and have a wide range of applicability.
Heat engines are often confused with the cycles they attempt to mimic. Typically when describing the physical device the term 'engine' is used. When describing the model the term 'cycle' is used.
Overview.
In thermodynamics, heat engines are often modeled using a standard engineering model such as the Otto cycle. The theoretical model can be refined and augmented with actual data from an operating engine, using tools such as an indicator diagram. Since very few actual implementations of heat engines exactly match their underlying thermodynamic cycles, one could say that a thermodynamic cycle is an ideal case of a mechanical engine. In any case, fully understanding an engine and its efficiency requires gaining a good understanding of the (possibly simplified or idealized) theoretical model, the practical nuances of an actual mechanical engine, and the discrepancies between the two.
In general terms, the larger the difference in temperature between the hot source and the cold sink, the larger is the potential thermal efficiency of the cycle. On Earth, the cold side of any heat engine is limited to being close to the ambient temperature of the environment, or not much lower than 300 Kelvin, so most efforts to improve the thermodynamic efficiencies of various heat engines focus on increasing the temperature of the source, within material limits. The maximum theoretical efficiency of a heat engine (which no engine ever attains) is equal to the temperature difference between the hot and cold ends divided by the temperature at the hot end, all expressed in absolute temperature or kelvins.
The efficiency of various heat engines proposed or used today has a large range: 
All these processes gain their efficiency (or lack thereof) from the temperature drop across them. Significant energy may be used for auxiliary equipment, such as pumps, which effectively reduces efficiency.
Power.
Heat engines can be characterized by their specific power, which is typically given in kilowatts per litre of engine displacement (in the U.S. also horsepower per cubic inch). The result offers an approximation of the peak power output of an engine. This is not to be confused with fuel efficiency, since high efficiency often requires a lean fuel-air ratio, and thus lower power density. A modern high-performance car engine makes in excess of 75 kW/l (1.65 hp/in3).
Everyday examples.
Examples of everyday heat engines include the steam engine (for example in trains), the diesel engine, and the gasoline (petrol) engine in an automobile. A common toy that is also a heat engine is a drinking bird. Also the stirling engine is a heat engine. All of these familiar heat engines are powered by the expansion of heated gases. The general surroundings are the heat sink, which provides relatively cool gases that, when heated, expand rapidly to drive the mechanical motion of the engine.
Examples of heat engines.
It is important to note that although some cycles have a typical combustion location (internal or external), they often can be implemented with the other. For example, John Ericsson developed an external heated engine running on a cycle very much like the earlier Diesel cycle. In addition, externally heated engines can often be implemented in open or closed cycles.
Earth's heat engine.
Earth's atmosphere and hydrosphere—Earth’s heat engine—are coupled processes that constantly even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation, when distributing heat around the globe.
The Hadley system provides an example of a heat engine. The Hadley circulation is identified with rising of warm and moist air in the equatorial region with descent of colder air in the subtropics corresponding to a thermally driven direct circulation, with consequent net production of kinetic energy.
Phase-change cycles.
In these cycles and engines, the working fluids are gases and liquids. The engine converts the working fluid from a gas to a liquid, from liquid to gas, or both, generating work from the fluid expansion or compression.
Gas-only cycles.
In these cycles and engines the working fluid is always a gas (i.e., there is no phase change):
Liquid only cycle.
In these cycles and engines the working fluid are always like liquid:
Cycles used for refrigeration.
A domestic refrigerator is an example of a heat pump: a heat engine in reverse. Work is used to create a heat differential. Many cycles can run in reverse to move heat from the cold side to the hot side, making the cold side cooler and the hot side hotter. Internal combustion engine versions of these cycles are, by their nature, not reversible.
Refrigeration cycles include:
Evaporative heat engines.
The Barton evaporation engine is a heat engine based on a cycle producing power and cooled moist air from the evaporation of water into hot dry air.
Mesoscopic heat engines.
Mesoscopic heat engines are nanoscale devices that may serve the goal of processing heat fluxes and perform useful work at small scales. Potential applications include e.g. electric cooling devices.
In such mesoscopic heat engines, work per cycle of operation fluctuates due to thermal noise.
There is exact equality that relates average of exponents of work performed by any heat engine and the heat transfer from the hotter heat bath. This relation transforms the Carnot's inequality into exact equality.
Efficiency.
The efficiency of a heat engine relates how much useful work is output for a given amount of heat energy input.
From the laws of thermodynamics:
In other words, a heat engine absorbs heat energy from the high temperature heat source, converting part of it to useful work and delivering the rest to the cold temperature heat sink.
In general, the efficiency of a given heat transfer process (whether it be a refrigerator, a heat pump or an engine) is defined informally by the ratio of "what you get out" to "what you put in".
In the case of an engine, one desires to extract work and puts in a heat transfer.
The "theoretical" maximum efficiency of any heat engine depends only on the temperatures it operates between. This efficiency is usually derived using an ideal imaginary heat engine such as the Carnot heat engine, although other engines using different cycles can also attain maximum efficiency. Mathematically, this is because in reversible processes, the change in entropy of the cold reservoir is the negative of that of the hot reservoir (i.e., formula_7), keeping the overall change of entropy zero. Thus:
where formula_9 is the absolute temperature of the hot source and formula_10 that of the cold sink, usually measured in kelvin. Note that formula_11 is positive while formula_12 is negative; in any reversible work-extracting process, entropy is overall not increased, but rather is moved from a hot (high-entropy) system to a cold (low-entropy one), decreasing the entropy of the heat source and increasing that of the heat sink.
The reasoning behind this being the maximal efficiency goes as follows. It is first assumed that if a more efficient heat engine than a Carnot engine is possible, then it could be driven in reverse as a heat pump. Mathematical analysis can be used to show that this assumed combination would result in a net decrease in entropy. Since, by the second law of thermodynamics, this is statistically improbable to the point of exclusion, the Carnot efficiency is a theoretical upper bound on the reliable efficiency of "any" process.
Empirically, no heat engine has ever been shown to run at a greater efficiency than a Carnot cycle heat engine.
Figure 2 and Figure 3 show variations on Carnot cycle efficiency. Figure 2 indicates how efficiency changes with an increase in the heat addition temperature for a constant compressor inlet temperature. Figure 3 indicates how the efficiency changes with an increase in the heat rejection temperature for a constant turbine inlet temperature.
Endoreversible heat engines.
The most Carnot efficiency as a criterion of heat engine performance is the fact that by its nature, any maximally efficient Carnot cycle must operate at an infinitesimal temperature gradient. This is because "any" transfer of heat between two bodies at differing temperatures is irreversible, and therefore the Carnot efficiency expression only applies in the infinitesimal limit. The major problem with that is that the object of most heat engines is to output some sort of power, and infinitesimal power is usually not what is being sought.
A different measure of ideal heat engine efficiency is given by considerations of endoreversible thermodynamics, where the cycle is identical to the Carnot cycle except in that the two processes of heat transfer are "not" reversible (Callen 1985):
This model does a better job of predicting how well real-world heat engines can do (Callen 1985, see also endoreversible thermodynamics):
As shown, the endoreversible efficiency much more closely models the observed data.
History.
Heat engines have been known since antiquity but were only made into useful devices at the time of the industrial revolution in the 18th century. They continue to be developed today.
Heat engine enhancements.
Engineers have studied the various heat engine cycles extensively in effort to improve the amount of usable work they could extract from a given power source. The Carnot cycle limit cannot be reached with any gas-based cycle, but engineers have worked out at least two ways to possibly go around that limit, and one way to get better efficiency without bending any rules.
Heat engine processes.
Each process is one of the following:

</doc>
<doc id="13655" url="http://en.wikipedia.org/wiki?curid=13655" title="Heimdallr">
Heimdallr

In Norse mythology, Heimdallr is a god who possesses the resounding horn Gjallarhorn, owns the golden-maned horse Gulltoppr, has gold teeth, and is the son of Nine Mothers. Heimdallr is attested as possessing foreknowledge, keen eyesight and hearing, is described as "the whitest of the gods", and keeps watch for the onset of Ragnarök while drinking fine mead in his dwelling Himinbjörg, located where the burning rainbow bridge Bifröst meets heaven. Heimdallr is said to be the originator of social classes among humanity and once regained Freyja's treasured possession Brísingamen while doing battle in the shape of a seal with Loki. Heimdallr and Loki are foretold to kill one another during the events of Ragnarök. Heimdallr is additionally referred to as Rig, Hallinskiði, Gullintanni, and Vindlér or Vindhlér.
Heimdallr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional material; in the "Prose Edda" and "Heimskringla", both written in the 13th century by Snorri Sturluson; in the poetry of skalds; and on an Old Norse runic inscription found in England. Two lines of an otherwise lost poem about the god, "Heimdalargaldr", survive. Due to the problematic and enigmatic nature of these attestations, scholars have produced various theories about the nature of the god, including his apparent relation to rams, that he may be a personification of or connected to the world tree Yggdrasil, and potential Indo-European cognates.
Names and etymology.
"Heimdallr" also appears as Heimdalr and Heimdali. The etymology of the name is obscure, but 'the one who illuminates the world' has been proposed. "Heimdallr" may be connected to "Mardöll", one of Freyja's names. "Heimdallr" and its variants are sometimes modernly anglicized as Heimdall (with the nominative "-r" dropped) or Heimdal.
Heimdallr is attested as having three other names; "Hallinskiði", "Gullintanni", and "Vindlér" or "Vindhlér". The name "Hallinskiði" is obscure, but has resulted in a series of attempts at deciphering it. "Gullintanni" literally means 'the one with the golden teeth'. "Vindlér" (or "Vindhlér") translates as either 'the one protecting against the wind' or 'wind-sea'. All three have resulted in numerous theories about the god.
Attestations.
Saltfleetby spindle whorl inscription.
A lead spindle whorl bearing an Old Norse Younger Futhark inscription that mentions Heimdallr was discovered in Saltfleetby, England on September 1, 2010. The spindle whorl itself is dated from the year 1000 to 1100 AD. On the inscription, the god Heimdallr is mentioned alongside the god Odin and Þjálfi, a name of one of the god Thor's servants. Regarding the inscription reading, John Hines of Cardiff University comments that there is "quite an essay to be written over the uncertainties of translation and identification here; what are clear, and very important, are the names of two of the Norse gods on the side, Odin and Heimdallr, while Þjalfi (masculine, not the feminine in -a) is the recorded name of a servant of the god Thor."
"Poetic Edda".
In the "Poetic Edda", Heimdallr is attested in six poems; "Völuspá", "Grímnismál", "Lokasenna", "Þrymskviða", "Rígsþula", and "Hrafnagaldr Óðins".
Heimdallr is mentioned thrice in "Völuspá". In the first stanza of the poem, the undead völva reciting the poem calls out for listeners to be silent and refers to Heimdallr:
This stanza has led to various scholarly interpretations. The "holy races" have been considered variously as either humanity or the gods. The notion of humanity as "Heimdallr's sons" is otherwise unattested and has also resulted in various interpretations. Some scholars have pointed to the prose introduction to the poem "Rígsþula", where Heimdallr is said to have once gone about mankind, slept between couples, and so doled out classes among them (see "Rígsthula" section below).
Later in "Völuspá", the völva foresees the events of Ragnarök and the role in which Heimdallr and Gjallarhorn will play at its onset; Heimdallr will raise his horn and blow loudly. Due to manuscript differences, translations of the stanza vary:
Regarding this stanza, scholar Andy Orchard comments that the name "Gjallarhorn" may here mean "horn of the river Gjöll" as "Gjöll is the name of one of the rivers of the Underworld, whence much wisdom is held to derive", but notes that in the poem "Grímnismál" Heimdallr is said to drink fine mead in his heavenly home Himinbjörg.
Earlier in the same poem, the völva mentions a scenario involving the hearing or horn (depending on translation of the Old Norse noun "hljóð"—translations bolded below for the purpose of illustration) of the god Heimdallr:
Scholar Paul Schach comments that the stanzas in this section of " Völuspá" are "all very mysterious and obscure, as it was perhaps meant to be". Schach details that ""Heimdallar hljóð" has aroused much speculation. Snorri [in the "Poetic Edda"] seems to have confused this word with "gjallarhorn", but there is otherwise no attestation of the use of "hljóð" in the sense of 'horn' in Icelandic. Various scholars have read this as "hearing" rather than "horn".
Scholar Carolyne Larrington comments that if "hearing" rather than "horn" is understood to appear in this stanza, the stanza indicates that Heimdallr, like Odin, has left a body part in the well; his ear. Larrington says that "Odin exchanged one of his eyes for wisdom from Mimir, guardian of the well, while Heimdall seems to have forfeited his ear."
In the poem "Grímnismál", Odin (disguised as "Grímnir"), tortured, starved and thirsty, tells the young Agnar of a number of mythological locations. The eighth location he mentions is Himinbjörg, where he says that Heimdallr drinks fine mead:
Regarding the above stanza, Henry Adams Bellows comments that "in this stanza the two functions of Heimdall—as father of mankind [ . . . ] and as warder of the gods—seem both to be mentioned, but the second line in the manuscripts is apparently in bad shape, and in the editions it is more or less conjecture".
In the poem "Lokasenna", Loki flyts with various gods who have met together to feast. At one point during the exchanges, the god Heimdallr says that Loki is drunk and witless, and asks Loki why he won't stop speaking. Loki tells Heimdallr to be silent, that he was fated a "hateful life", that Heimdallr must always have a muddy back, and that he must serve as watchman of the gods. The goddess Skaði interjects and the flyting continues in turn.
The poem "Þrymskviða" tells of Thor's loss of his hammer, Mjöllnir, to the jötnar and quest to get it back. At one point in the tale, the gods gather at the thing and debate how to get Thor's hammer back from the jötnar, who demand the beautiful goddess Freyja in return for it. Heimdallr advises that they simply dress Thor up as Freyja, during which he is described as "hvítastr ása"—literally "whitest of the gods" (although Thorpe's translation below renders "hvítastr" as "brightest")—and is said to have foresight like the Vanir, a group of gods:
Regarding Heimdallr's whiteness and the comparison to the Vanir, scholar John Lindow comments that there are no other indications of Heimdallr being considered among the Vanir, and that Heimdallr's status as "whitest of the gods" has not been explained.
The introductory prose to the poem "Rígsþula" says that "people say in the old stories" that Heimdallr, described as a god among the Æsir, once fared on a journey. Heimdallr wandered along a seashore, and referred to himself as "Rígr". In the poem, Rígr, who is described as a wise and powerful god, walks in the middle of roads on his way to steads, where he meets a variety of couples and dines with them, giving them advice and spending three nights at a time between them in their bed. The wives of the couples become pregnant, and from them come the various classes of humanity. Eventually a warrior home produces a promising boy, and as the boy grows older, Rígr comes out of a thicket, teaches the boy runes, gives him a name, and proclaims him to be his son. Rígr tells him to strike out and get land for himself. The boy does so, and so becomes a great war leader with many estates. He marries a beautiful woman and the two have many children and are happy. One of the children eventually becomes so skilled that he is able to share in runic knowledge with Heimdallr, and so earns the title of "Rígr" himself. The poem continues without further mention of the god.
"Prose Edda".
In the "Prose Edda", Heimdallr is mentioned in the books "Gylfaginning", "Skáldskaparmál", and "Háttatal". In "Gylfaginning", the enthroned figure of High tells the disguised mythical king Gangleri of various gods, and, in chapter 25, mentions Heimdallr. High says that Heimdallr is known "the white As", is "great and holy", and that nine maidens, all sisters, gave birth to him. Heimdallr is called "Hallinskiði" and "Gullintanni", and he has gold teeth. High continues that Heimdallr lives in "a place" called Himinbjörg and that it is near Bifröst. Heimdallr is the watchman of the gods, and he sits on the edge of heaven to guard the Bifröst bridge from the berg jötnar. Heimdallr requires less sleep than a bird, can see at night just as well as if it were day, and for over a hundred leagues. Heimdallr's hearing is also quite keen; he can hear grass as it grows on the earth, wool as it grows on sheep, and anything louder. Heimdallr possesses a trumpet, Gjallarhorn, that, when blown, can be heard in all worlds, and "the head is referred to as Heimdall's sword". High then quotes the above-mentioned "Grímnismál" stanza about Himinbjörg and provides two lines from the otherwise lost poem about Heimdallr, "Heimdalargaldr", in which Heimdallr proclaims himself to be the son of Nine Mothers.
In chapter 49, High tells of the god Baldr's funeral procession. Various deities are mentioned as having attended, including Heimdallr, who there rode his horse Gulltopr.
In chapter 51, High foretells the events of Ragnarök. After the enemies of the gods will gather at the plain Vígríðr, Heimdallr will stand and mightily blow into Gjallarhorn. The gods will awake and assemble together at the thing. At the end of the battle between various gods and their enemies, Heimdallr will face Loki and they will kill one another. After, the world will be engulfed in flames. High then quotes the above-mentioned stanza regarding Heimdallr raising his horn in "Völuspá".
At the beginning of "Skáldskaparmál", Heimdallr is mentioned as having attended a banquet in Asgard with various other deities. Later in the book, "Húsdrápa", a poem by 10th century skald Úlfr Uggason, is cited, during which Heimdallr is described as having ridden to Baldr's funeral pyre.
In chapter 8, means of referring to Heimdallr are provided; "son of nine mothers", "guardian of the gods", "the white As" (see "Poetic Edda" discussion regarding "hvítastr ása" above), "Loki's enemy", and "recoverer of Freyja's necklace". The section adds that the poem "Heimdalargaldr" is about him, and that, since the poem, "the head has been called Heimdall's doom: man's doom is an expression for sword". Hiemdallr is the owner of Gulltoppr, is also known as Vindhlér, and is a son of Odin. Heimdallr visits Vágasker and Singasteinn and there vied with Loki for Brísingamen. According to the chapter, the skald Úlfr Uggason composed a large section of his "Húsdrápa" about these events and that "Húsdrápa" says that the two were in the shape of seals. A few chapters later, ways of referring to Loki are provided, including "wrangler with Heimdall and Skadi", and section of Úlfr Uggason's "Húsdrápa" is then provided in reference:
The chapter points out that in the above "Húsdrápa" section Heimdallr is said to be the son of nine mothers.
Heimdallr is mentioned once in "Háttatal". There, in a composition by Snorri Sturluson, a sword is referred to as "Vindhlér's helmet-filler", meaning "Heimdallr's head".
"Heimskringla".
In "Ynglinga saga" compiled in "Heimskringla", Snorri presents a euhemerized origin of the Norse gods and rulers descending from them. In chapter 5, Snorri asserts that the Æsir settled in what is now Sweden and built various temples. Snorri writes that Odin settled in Lake Logrin "at a place which formerly was called Sigtúnir. There he erected a large temple and made sacrifices according to the custom of the Æsir. He took possession of the land as far as he had called it Sigtúnir. He gave dwelling places to the temple priests." Snorri adds that, after this, Njörðr dwelt in Nóatún, Freyr dwelt in Uppsala, Heimdall at Himinbjörg, Thor at Þrúðvangr, Baldr at Breiðablik and that to everyone Odin gave fine estates.
Archaeological record.
A figure holding a large horn to his lips and clasping a sword on his hip appears on a stone cross from the Isle of Man. Some scholars have theorized that this figure is a depiction of Heimdallr with Gjallarhorn.
A 9th or 10th century Gosforth Cross in Cumbria, England depicts a figure holding a horn and a sword standing defiantly before two open-mouthed beasts. This figure has been often theorized as depicting Heimdallr with Gjallarhorn.
Theories and interpretations.
Heimdallr's attestations have proven troublesome and enigmatic to interpret for scholars. Scholar Georges Dumézil summarizes the difficulties as follows:
References.
</dl>

</doc>
<doc id="13658" url="http://en.wikipedia.org/wiki?curid=13658" title="House of Lords">
House of Lords

The House of Lords is the upper house of the Parliament of the United Kingdom. Like the House of Commons, it meets in the Palace of Westminster.
Unlike the elected House of Commons, most members of the House of Lords are appointed. The membership of the House of Lords is made up of Lords Spiritual and Lords Temporal. The Lords Spiritual are 26 bishops in the established Church of England. Of the Lords Temporal, the majority are life peers who are appointed by the monarch on the advice of the Prime Minister, or on the advice of the House of Lords Appointments Commission. However, they also include some hereditary peers. Membership was once an entitlement of all hereditary peers, other than those in the peerage of Ireland, but under the House of Lords Act 1999, the right to membership was restricted to 92 hereditary peers. Since the vast majority of hereditary peerages can only be inherited by men, very few of these are female.
The number of members is not fixed; as of 8 2015[ [update]] the House of Lords has 779 members (not including 49 who are on leave of absence or who are otherwise disqualified from sitting), unlike the House of Commons, which has a 650-seat fixed membership.
The House of Lords scrutinises bills that have been approved by the House of Commons. It regularly reviews and amends Bills from the Commons. While it is unable to prevent Bills passing into law, except in certain limited circumstances, it can delay Bills and force the Commons to reconsider their decisions. In this capacity, the Lords acts as a check on the House of Commons that is independent from the electoral process. Bills can be introduced into either the House of Lords or the House of Commons. Members of the Lords may also take on roles as government ministers. The House of Lords has its own support services, separate from the Commons, including the House of Lords Library.
The Queen's Speech is delivered in the House of Lords during the State Opening of Parliament. In addition to its role as the upper house, until the establishment of the Supreme Court in 2009, the House of Lords, through the Law Lords, acted as the final court of appeal in the British judicial system. The House also has a Church of England role, in that Church Measures must be tabled within the House by the Lords Spiritual.
History.
Today's Parliament of the United Kingdom largely descends, in practice, from the Parliament of England, though the Treaty of Union of 1706 and the Acts of Union that ratified the Treaty in 1707 created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland. This new parliament was, in effect, the continuation of the Parliament of England with the addition of 45 MPs and 16 Peers to represent Scotland.
The Parliament of England developed from the "Magnum Concilium", the "Great Council" that advised the King during medieval times. This royal council came to be composed of ecclesiastics, noblemen, and representatives of the counties of England (afterwards, representatives of the boroughs as well). The first English Parliament is often considered to be the "Model Parliament" (held in 1295), which included archbishops, bishops, abbots, earls, barons, and representatives of the shires and boroughs of it.
The power of Parliament grew slowly, fluctuating as the strength of the monarchy grew or declined. For example, during much of the reign of Edward II (1307–1327), the nobility was supreme, the Crown weak, and the shire and borough representatives entirely powerless. In 1569, the authority of Parliament was for the first time recognised not simply by custom or royal charter, but by an authoritative statute, passed by Parliament itself. 
Further developments occurred during the reign of Edward II's successor, Edward III. It was during this King's reign that Parliament clearly separated into two distinct chambers: the House of Commons (consisting of the shire and borough representatives) and the House of Lords (consisting of the bishops and abbots and the peers). The authority of Parliament continued to grow, and, during the early fifteenth century, both Houses exercised powers to an extent not seen before. The Lords were far more powerful than the Commons because of the great influence of the great landowners and the prelates of the realm.
The power of the nobility suffered a decline during the civil wars of the late fifteenth century, known as the Wars of the Roses. Much of the nobility was killed on the battlefield or executed for participation in the war, and many aristocratic estates were lost to the Crown. Moreover, feudalism was dying, and the feudal armies controlled by the barons became obsolete. Henry VII (1485–1509) clearly established the supremacy of the monarch, symbolised by the "Crown Imperial". The domination of the Sovereign continued to grow during the reigns of the Tudor monarchs in the 16th century. The Crown was at the height of its power during the reign of Henry VIII (1509–1547).
The House of Lords remained more powerful than the House of Commons, but the Lower House continued to grow in influence, reaching a zenith in relation to the House of Lords during the middle 17th century. Conflicts between the King and the Parliament (for the most part, the House of Commons) ultimately led to the English Civil War during the 1640s. In 1649, after the defeat and execution of King Charles I, the Commonwealth of England was declared, but the nation was effectively under the overall control of Oliver Cromwell, Lord Protector of England.
The House of Lords was reduced to a largely powerless body, with Cromwell and his supporters in the Commons dominating the Government. On 19 March 1649, the House of Lords was abolished by an Act of Parliament, which declared that "The Commons of England [find] by too long experience that the House of Lords is useless and dangerous to the people of England." The House of Lords did not assemble again until the Convention Parliament met in 1660 and the monarchy was restored. It returned to its former position as the more powerful chamber of Parliament—a position it would occupy until the 19th century.
19th century.
The 19th century was marked by several changes to the House of Lords. The House, once a body of only about 50 members, had been greatly enlarged by the liberality of George III and his successors in creating peerages. The individual influence of a Lord of Parliament was thus diminished.
Moreover, the power of the House as a whole experienced a decrease, whilst that of the House of Commons grew. Particularly notable in the development of the Lower House's superiority was the Reform Bill of 1832. The electoral system of the House of Commons was not, at the time, democratic: property qualifications greatly restricted the size of the electorate, and the boundaries of many constituencies had not been changed for centuries.
Entire cities such as Manchester were not represented by a single individual in the House of Commons, but the 11 voters of Old Sarum retained their ancient right to elect two members of parliament. A small borough was susceptible to bribery, and was often under the control of a patron, whose nominee was guaranteed to win an election. Some aristocrats were patrons of numerous "pocket boroughs", and therefore controlled a considerable part of the membership of the House of Commons.
When the House of Commons passed a Reform Bill to correct some of these anomalies in 1831, the House of Lords rejected the proposal. The popular cause of reform, however, was not abandoned by the ministry, despite a second rejection of the bill in 1832. Prime Minister Earl Grey advised the King to overwhelm opposition to the bill in the House of Lords by creating about 80 new pro-Reform peers. William IV originally balked at the proposal, which effectively threatened the opposition of the House of Lords, but at length relented.
Before the new peers were created, however, the Lords who opposed the bill admitted defeat, and abstained from the vote, allowing the passage of the bill. The crisis damaged the political influence of the House of Lords, but did not altogether end it. Over the course of the century, however, the power of the Upper House experienced further erosion, and the Commons gradually became the stronger House of Parliament.
20th century.
The status of the House of Lords returned to the forefront of debate after the election of a Liberal Government in 1906. In 1909, the Chancellor of the Exchequer, David Lloyd George, introduced into the House of Commons the "People's Budget", which proposed a land tax targeting wealthy landowners. The popular measure, however, was defeated in the heavily Conservative House of Lords.
Having made the powers of the House of Lords a primary campaign issue, the Liberals were narrowly re-elected in January 1910. Prime Minister H. H. Asquith then proposed that the powers of the House of Lords be severely curtailed. After a further general election in December 1910, and with an undertaking by King George V to create sufficient new Liberal peers to overcome Lords' opposition to the measure if necessary, the Asquith Government secured the passage of a bill to curtail the powers of the House of Lords.
The Parliament Act 1911 effectively abolished the power of the House of Lords to reject legislation, or to amend in a way unacceptable to the House of Commons: most bills could be delayed for no more than three parliamentary sessions or two calendar years. It was not meant to be a permanent solution; more comprehensive reforms were planned. Neither party, however, pursued the matter with much enthusiasm, and the House of Lords remained primarily hereditary. In 1949, the Parliament Act reduced the delaying power of the House of Lords further to two sessions or one year.
In 1958, the predominantly hereditary nature of the House of Lords was changed by the Life Peerages Act 1958, which authorised the creation of life baronies, with no numerical limits. The number of Life Peers then gradually increased, though not at a constant rate.
The Labour Party had for most of the twentieth century a commitment, based on the party's historic opposition to class privilege, to abolish the House of Lords, or at least expel the hereditary element. In 1968, the Labour Government of Harold Wilson attempted to reform the House of Lords by introducing a system under which hereditary peers would be allowed to remain in the House and take part in debate, but would be unable to vote. This plan, however, was defeated in the House of Commons by a coalition of traditionalist Conservatives (such as Enoch Powell), and Labour members who continued to advocate the outright abolition of the Upper House (such as Michael Foot).
When Michael Foot attained the leadership of the Labour Party in 1980, abolition of the House of Lords became a part of the party's agenda; under his successor, Neil Kinnock, however, a reformed Upper House was proposed instead. In the meantime, the creation of hereditary peerages (except for members of the Royal Family) has been arrested, with the exception of three creations during the administration of the Conservative Margaret Thatcher in the 1980s.
Whilst some hereditary peers were at best apathetic the Labour Party's clear commitments were not lost on Baron Sudeley, who for decades was considered an expert on the House of Lords. In December 1979 the Conservative Monday Club published his extensive paper entitled "Lords Reform – Why tamper with the House of Lords?" and in July 1980 "The Monarchist" carried another article by Lord Sudeley entitled "Why Reform or Abolish the House of Lords?". In 1990 he authored a further booklet for the Monday Club entitled "The Preservation of the House of Lords".
Lords Reform.
1997–2010.
The Labour Party included in its 1997 general election Manifesto a commitment to remove the hereditary peerage from the House of Lords. Their subsequent election victory in 1997 under Tony Blair finally heralded the demise of the traditional House of Lords. The Labour Government introduced legislation to expel all hereditary peers from the Upper House as a first step in Lords reform. As a part of a compromise, however, it agreed to permit 92 hereditary peers to remain until the reforms were complete. Thus all but 92 hereditary peers were expelled under the House of Lords Act 1999 (see below for its provisions), making the House of Lords predominantly an appointed house.
Since 1999 however, no further reform has taken place. The Wakeham Commission proposed introducing a 20% elected element to the Lords, but this plan was widely criticised. A Joint Committee was established in 2001 to resolve the issue, but it reached no conclusion and instead gave Parliament seven options to choose from (fully appointed, 20% elected, 40% elected, 50% elected, 60% elected, 80%, and fully elected). In a confusing series of votes in February 2003, all of these options were defeated although the 80% elected option fell by just three votes in the Commons. Socialist MPs favouring outright abolition voted against all the options.
In 2005 a cross-party group of senior MPs (Kenneth Clarke, Paul Tyler, Tony Wright, Sir George Young and Robin Cook) published a report proposing that 70% of members of the House of Lords should be elected – each member for a single long term – by the single transferable vote system. Most of the remainder were to be appointed by a Commission to ensure a mix of "skills, knowledge and experience". This proposal was also not implemented. A cross-party campaign initiative called "Elect the Lords" was set up to make the case for a predominantly elected Second Chamber in the run up to the 2005 general election.
At the 2005 election, the Labour Party proposed further reform of the Lords, but without specific details. The Conservative Party, which had, prior to 1997, opposed any tampering with the House of Lords, favoured an 80% elected Second Chamber, while the Liberal Democrats called for a fully elected Senate. During 2006, a cross-party committee discussed Lords reform, with the aim of reaching a consensus: its findings were published in early 2007.
On 7 March 2007, members of the House of Commons voted ten times on a variety of alternative compositions for the upper chamber. Outright abolition, a wholly appointed house, a 20% elected house, a 40% elected house, a 50% elected house and a 60% elected house were all defeated in turn. Finally the vote for an 80% elected chamber was won by 305 votes to 267, and the vote for a wholly elected chamber was won by an even greater margin: 337 to 224. Significantly this last vote represented an overall majority of MPs.
Furthermore, examination of the names of MPs voting at each division shows that, of the 305 who voted for the 80% elected option, 211 went on to vote for the 100% elected option. Given that this vote took place after the vote on 80% – whose result was already known when the vote on 100% took place – this showed a clear preference for a fully elected upper house among those who voted for the only other option that passed. But this was nevertheless only an indicative vote and many political and legislative hurdles remained to be overcome for supporters of an elected second chamber. The House of Lords, soon after, rejected this proposal and voted for an entirely appointed House of Lords.
In July 2008 Jack Straw, the Secretary of State for Justice and Lord Chancellor, introduced a white paper to the House of Commons proposing to replace the House of Lords with an 80–100% elected chamber, with one third being elected at each general election, for a term of approximately 12–15 years. The white paper states that as the peerage would be totally separated from membership of the upper house, the name "House of Lords" would no longer be appropriate: It goes on to explain that there is cross-party consensus for the new chamber to be titled the "Senate of the United Kingdom", however to ensure the debate remains on the role of the upper house rather than its title, the white paper is neutral on the title of the new house.
In Meg Russell’s article; "Is the House of Lords already reformed?" she states three essential features of a legitimate House of Lords. The first is that it must have adequate powers over legislation to make the government think twice before making a decision. The House of Lords, she argues, currently has enough power to make it relevant. During Tony Blair’s first year he was defeated thirty-eight times in the Lords. Secondly, as to the composition of the Lords, Meg Russell suggests that the composition must be distinct from the Commons, otherwise it would render the Lords useless. The third feature is the perceived legitimacy of the Lords. She writes; "In general legitimacy comes with election."
What will concern ministers in the coalition government is how these features are interlinked. If the Lords have a distinct and elected composition, this would probably come about through fixed term proportional representation. If this happens then the perceived legitimacy of the Lords could arguably outweigh the legitimacy of the Commons. This would especially be the case if the House of Lords had been elected more recently than the House of Commons as it could be said to reflect the will of the people better than the Commons.
In this scenario there may well come a time when the Lords twice reject a Bill from the Commons and it is forced through. This would in turn trigger questions about the amount of power the Lords should have and there would be pressure for it to increase. This hypothetical process is known as the "circumnavigation of power theory". It implies that it would never be in any government's interest to legitimise the Lords as they would be forfeiting their own power.
2010–present.
The Conservative–Liberal Democrat coalition agreed, following the 2010 general election, to clearly outline a provision for a wholly or mainly elected second chamber, elected by a proportional representation system. These proposals sparked a debate on 29 June 2010. As an interim measure, appointment of new peers will reflect shares of the vote secured by the political parties in the last general election.
Detailed proposals for Lords reform including a draft House of Lords Reform Bill were published on 17 May 2011. These include a 300-member hybrid house, of which 80% are elected. A further 20% would be appointed, and reserve space would be included for some Church of England bishops. Under the proposals, members would also serve single non-renewable terms of 15 years. Former MPs would be allowed to stand for election to the Upper House, but members of the Upper House would not be immediately allowed to become MPs.
The details of the proposal were:
The proposals were considered by a Joint Committee on House of Lords Reform made up of both MPs and Peers, which issued its final report on 23 April 2012, making the following suggestions:
House of Lords Reform Act 2014.
A private members bill to introduce some reforms was introduced by Dan Byles in 2013. The House of Lords Reform Act 2014 received the Royal Assent in 2014. Under the new law:
Overcrowding.
The size of the House of Lords has varied greatly throughout its history. From about 50 members in the early 1700s, it increased to a record size of 1,330 in October 1999, before Lords reform reduced it to 669 by March 2000.
In April 2011, a cross-party group of former leading politicians, including many senior members of the House of Lords, called on the Prime Minister David Cameron to stop creating new peers. He had created 117 new peers since becoming prime minister in May 2010, a faster rate of elevation than any PM in British history. The expansion occurred while his government had tried (in vain) to reduce the size of the House of Commons by 50 members, from 650 to 600.
In August 2014, despite there being a seating capacity of only around 230 or 400 seats in the Lords chamber, the House had 774 active members (plus 54 who were not entitled to attend or vote, having been suspended or granted leave of absence). This made the House of Lords the largest parliamentary chamber in any democracy, surpassed in size only by China’s National People’s Congress (which had 2,987 members in 2013). In August 2014, former Speaker of the House of Commons Baroness Boothroyd requested that “older peers should retire gracefully” to ease the overcrowding in the House of Lords. She also criticised successive prime ministers for filling the second chamber with “lobby fodder” in an attempt to help their policies become law. She made her remarks days before a new batch of peers were due to be appointed.
Relationship with the Government.
The House of Lords does not control the term of the Prime Minister or of the Government. Only the Lower House may force the Prime Minister to resign or call elections by passing a motion of no-confidence or by withdrawing supply. Thus, the House of Lords' oversight of the government is limited.
Most Cabinet ministers are from the House of Commons rather than the House of Lords. In particular, all Prime Ministers since 1902 have been members of the Lower House. (Alec Douglas-Home, who became Prime Minister in 1963 whilst still an Earl, disclaimed his peerage and was elected to the Commons soon after his term began.) In recent history, it has been very rare for major cabinet positions (except Lord Chancellor and Leader of the House of Lords) to have been filled by peers.
Exceptions include Lord Carrington, who was the Foreign Secretary between 1979 and 1982, Lord Young of Graffham (Minister without Portfolio, then Secretary of State for Employment and then Secretary of State for Trade and Industry from 1984 to 1989), and Lord Mandelson, who served as First Secretary of State, Secretary of State for Business, Innovation and Skills and President of the Board of Trade. George Robertson was briefly a peer whilst serving as Secretary of State for Defence before resigning to take up the post of Secretary General of NATO. From 1999 to 2010 the Attorney General for England and Wales was a Member of the House of Lords; the most recent was Baroness Scotland of Asthal.
The House of Lords remains a source for junior ministers and members of government. Like the House of Commons, the Lords also has a Government Chief Whip as well as several Junior Whips. Where a government department is not represented by a minister in the Lords or one is not available, government whips will act as spokesmen for them.
Legislative functions.
Legislation, with the exception of money bills, may be introduced in either House.
The House of Lords debates legislation, and has power to amend or reject bills. However, the power of the Lords to reject a bill passed by the House of Commons is severely restricted by the Parliament Acts. Under those Acts, certain types of bills may be presented for the Royal Assent without the consent of the House of Lords (i.e. the Commons can override the Lords' veto). The House of Lords cannot delay a money bill (a bill that, in the view of the Speaker of the House of Commons, solely concerns national taxation or public funds) for more than one month.
Other public bills cannot be delayed by the House of Lords for more than two parliamentary sessions, or one calendar year. These provisions, however, only apply to public bills that originate in the House of Commons, and cannot have the effect of extending a parliamentary term beyond five years. A further restriction is a constitutional convention known as the Salisbury Convention, which means that the House of Lords does not oppose legislation promised in the Government's election manifesto.
By a custom that prevailed even before the Parliament Acts, the House of Lords is further restrained insofar as financial bills are concerned. The House of Lords may neither originate a bill concerning taxation or Supply (supply of treasury or exchequer funds), nor amend a bill so as to insert a taxation or Supply-related provision. (The House of Commons, however, often waives its privileges and allows the Upper House to make amendments with financial implications.) Moreover, the Upper House may not amend any Supply Bill. The House of Lords formerly maintained the absolute power to reject a bill relating to revenue or Supply, but this power was curtailed by the Parliament Acts, as aforementioned.
Former judicial role.
Historically, the House of Lords held several judicial functions. Most notably, until 2009 the House of Lords served as the court of last resort for most instances of UK law. Since 1 October 2009 this role is now held by the Supreme Court of the United Kingdom.
The Lords' judicial functions originated from the ancient role of the Curia Regis as a body that addressed the petitions of the King's subjects. The functions were exercised not by the whole House, but by a committee of "Law Lords". The bulk of the House's judicial business was conducted by the twelve Lords of Appeal in Ordinary, who were specifically appointed for this purpose under the Appellate Jurisdiction Act 1876.
The judicial functions could also be exercised by Lords of Appeal (other members of the House who happened to have held high judicial office). No Lord of Appeal in Ordinary or Lord of Appeal could sit judicially beyond the age of seventy-five. The judicial business of the Lords was supervised by the Senior Lord of Appeal in Ordinary and his or her deputy, the Second Senior Lord of Appeal in Ordinary.
The jurisdiction of the House of Lords extended, in civil and in criminal cases, to appeals from the courts of England and Wales, and of Northern Ireland. From Scotland, appeals were possible only in civil cases; Scotland's High Court of Justiciary is the highest court in criminal matters. The House of Lords was not the United Kingdom's only court of last resort; in some cases, the Judicial Committee of the Privy Council performs such a function. The jurisdiction of the Privy Council in the United Kingdom, however, is relatively restricted; it encompasses appeals from ecclesiastical courts, disputes under the House of Commons Disqualification Act 1975, and a few other minor matters. Issues related to devolution were transferred from the Privy Council to the Supreme Court in 2009.
The twelve Law Lords did not all hear every case; rather, after World War II cases were heard by panels known as Appellate Committees, each of which normally consisted of five members (selected by the Senior Lord). An Appellate Committee hearing an important case could consist of more than five members. Though Appellate Committees met in separate committee rooms, judgement was given in the Lords Chamber itself. No further appeal lay from the House of Lords, although the House of Lords could refer a "preliminary question" to the European Court of Justice in cases involving an element of European Union law, and a case could be brought at the European Court of Human Rights if the House of Lords did not provide a satisfactory remedy in cases where the European Convention on Human Rights was relevant.
A distinct judicial function—one in which the whole House used to participate—is that of trying impeachments. Impeachments were brought by the House of Commons, and tried in the House of Lords; a conviction required only a majority of the Lords voting. Impeachments, however, are to all intents and purposes obsolete; the last impeachment was that of Henry Dundas, 1st Viscount Melville in 1806.
Similarly, the House of Lords was once the court that tried peers charged with high treason or felony. The House would be presided over not by the Lord Chancellor, but by the Lord High Steward, an official especially appointed for the occasion of the trial. If Parliament was not in session, then peers could be tried in a separate court, known as the Lord High Steward's Court. Only peers, their wives, and their widows (unless remarried) were entitled to trials in the House of Lords or the Lord High Steward's Court; the Lords Spiritual were tried in Ecclesiastical Courts. In 1948, the right of peers to be tried in such special courts was abolished; now, they are tried in the regular courts. The last such trial in the House was of Edward Southwell Russell, 26th Baron de Clifford in 1935. An illustrative dramatisation circa 1928 of a trial of a peer (the fictional Duke of Denver) on a charge of murder (a felony) is portrayed in the 1972 BBC Television adaption of Dorothy L. Sayers' Lord Peter Wimsey mystery "Clouds of Witness".
The Constitutional Reform Act 2005 resulted in the creation of a separate Supreme Court of the United Kingdom, to which the judicial function of the House of Lords, and some of the judicial functions of the Judicial Committee of the Privy Council, were transferred. In addition, the office of Lord Chancellor was reformed by the act, removing his ability to act as both a government minister and a judge. This was motivated in part by concerns about the historical admixture of legislative, judicial, and executive power. The new Supreme Court is located at Middlesex Guildhall.
Membership.
Lords Spiritual.
Members of the House of Lords who sit by virtue of their ecclesiastical offices are known as Lords Spiritual. Formerly, the Lords Spiritual were the majority in the English House of Lords, comprising the church's archbishops, (diocesan) bishops, abbots, and those priors who were entitled to wear a mitre. After the English Reformation's highpoint in 1539, only the archbishops and bishops continued to attend, as the Dissolution of the Monasteries had just disproved of and suppressed the positions of abbot and prior. In 1642 during the few Lords' gatherings convened during English Interregnum which saw periodic war, the Lords Spiritual were excluded altogether, but they returned under the Clergy Act 1661.
The number of Lords Spiritual was further restricted by the Bishopric of Manchester Act 1847, and by later acts. The Lords Spiritual can now number no more than 26; these are the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham, the Bishop of Winchester (who sit by right regardless of seniority) and the 21 longest-serving bishops from other dioceses in the Church of England (excluding the dioceses of Sodor and Man and Gibraltar in Europe, as these lie entirely outside the United Kingdom). Following a change to the law in 2014 to allow women to become bishops, the Lords Spiritual (Women) Act 2015 was passed, which provides that whenever a vacancy arises among the Lords Spiritual during the ten years following the Act coming into force, the vacancy has to be filled by a woman bishop, if one is eligible. This does not apply to the five bishops who sit by right.
The current Lords Spiritual represent only the Church of England. Bishops of the Church of Scotland traditionally sat in the Parliament of Scotland but were finally excluded in 1689 (after a number of previous exclusions) when the Church of Scotland became permanently presbyterian. There are no longer bishops in the Church of Scotland in the traditional sense of the word, and that Church has never sent members to sit in the Westminster House of Lords. The Church of Ireland did obtain representation in the House of Lords after the union of Ireland and Great Britain in 1801.
Of the Church of Ireland's ecclesiastics, four (one archbishop and three bishops) were to sit at any one time, with the members rotating at the end of every parliamentary session (which normally lasted approximately one year). The Church of Ireland, however, was disestablished in 1871, and thereafter ceased to be represented by Lords Spiritual. Bishops of Welsh sees in the Church of England originally sat in the House of Lords (after 1847, only if their seniority within the Church entitled them to), but the Church in Wales ceased to be a part of the Church of England in 1920 and was simultaneously disestablished in Wales. Accordingly, bishops of the Church in Wales were no longer eligible to be appointed to the House as bishops of the Church of England.
Other ecclesiastics have sat in the House of Lords as Lords Temporal in recent times: Chief Rabbi Immanuel Jakobovits was appointed to the House of Lords (with the consent of the Queen, who acted on the advice of Prime Minister Margaret Thatcher), as was his successor Chief Rabbi Jonathan Sacks. In recognition of his work at reconciliation and in the peace process in Northern Ireland, the Archbishop of Armagh (the senior Anglican bishop in Northern Ireland), Lord Eames was appointed to the Lords by John Major. Other clergymen appointed include the Reverend Donald Soper, the Reverend Timothy Beaumont, and some Scottish clerics.
By custom at least one of the Bishops reads prayers in each legislative day (a role taken by the chaplain in the Commons). They often speak in debates; in 2004 Rowan Williams, the Archbishop of Canterbury, opened a debate into sentencing legislation. Measures (proposed laws of the Church of England) must be put before the Lords, and the Lords Spiritual have a role in ensuring that this takes place.
Lords Temporal.
Since the Dissolution of the Monasteries, the Lords Temporal have been the most numerous group in the House of Lords. Unlike the Lords Spiritual, they may be publicly partisan, aligning themselves with one or another of the political parties that dominate the House of Commons. Publicly non-partisan Lords are called crossbenchers. Originally, the Lords Temporal included several hundred hereditary peers (that is, those whose peerages may be inherited), who ranked variously as dukes, marquesses, earls, viscounts, and barons (as well as Scottish Lords of Parliament). Such hereditary dignities can be created by the Crown; in modern times this is done on the advice of the Prime Minister of the day (except in the case of members of the Royal Family).
In 1999, the Labour government brought forward the House of Lords Act removing the right of several hundred hereditary peers to sit in the House. The Act provided a temporary measure that only 92 individuals may continue to sit in the Upper House by virtue of hereditary peerages.
Of these, two remain in the House of Lords because they hold royal offices connected with Parliament: the Earl Marshal and the Lord Great Chamberlain. Of the remaining 90 members of the House of Lords sitting by virtue of a hereditary peerage in the House of Lords, 14 are elected by the whole House and 74 are chosen by fellow hereditary peers in the House of Lords, grouped by party. This Act, included the Principality of Wales and the Earldom of Chester and removed all Royal Peers including the Duke of Edinburgh, Duke of York, Earl of Wessex, Duke of Gloucester and the Duke of Kent.
The number of peers to be chosen by a party reflects the proportion of hereditary peers that belonged to that party (see current composition below) in 1999. When an elected hereditary peer dies, a by-election is held, with a variant of the Alternative Vote system being used. If the recently deceased hereditary peer was elected by the whole House, then so is his or her replacement; a hereditary peer elected by a specific party is replaced by a vote of elected hereditary peers belonging to that party (whether elected as part of that party group or by the whole house).
The Lords Temporal also included the Lords of Appeal in Ordinary, a group of individuals appointed to the House of Lords so that they could exercise its judicial functions. Lords of Appeal in Ordinary, more commonly known as Law Lords, were first appointed under the Appellate Jurisdiction Act 1876. They were selected by the Prime Minister, but were formally appointed by the Sovereign. A Lord of Appeal in Ordinary had to retire at the age of 70, or, if his or her term was extended by the government, at the age of 75; after reaching such an age, the Law Lord could not hear any further legal cases.
The number of Lords of Appeal in Ordinary (excluding those who were no longer able to hear cases because of age restrictions) was limited to twelve, but could be changed by statutory instrument. Lords of Appeal in Ordinary traditionally did not participate in political debates, so as to maintain judicial independence. Lords of Appeal in Ordinary held seats in the House of Lords for life, remaining members even after reaching the judicial retirement age of 70 or 75. Former Lord Chancellors and holders of other high judicial office could also sit as Law Lords under the Appellate Jurisdiction Act, although in practice this right was infrequently exercised.
Under the Constitutional Reform Act 2005, the existing Lords of Appeal in Ordinary became judges of the new Supreme Court of the United Kingdom in 2009 and are barred from sitting or voting in the House of Lords until they retire as judges. One of the main justifications for the new Supreme Court was to establish a separation of powers between the judiciary and the legislature. It is therefore unlikely that future appointees to the Supreme Court of the United Kingdom will be made Lords of Appeal in Ordinary.
The largest group of Lords Temporal, and indeed of the whole House, are life peers. Life peerages rank only as barons or baronesses, and are created under the Life Peerages Act 1958. Like all other peers, life peers are created by the Sovereign, who acts on the advice of the Prime Minister or the House of Lords Appointments Commission. By convention, however, the Prime Minister allows leaders of other parties to select some life peers so as to maintain a political balance in the House of Lords. Moreover, some non-party life peers (the number being determined by the Prime Minister) are nominated by an independent House of Lords Appointments Commission.
If a hereditary peerage holder is given a life peerage, he or she becomes a member of the House of Lords without a need for a by-election. In 2000, the government announced it would set up an Independent Appointments Commission, under Lord Stevenson of Coddenham, to select fifteen so-called "People's Peers" for life peerages. However, when the choices were announced in April 2001, from a list of 3,000 applicants, the choices were treated with criticism in the media , as all were distinguished in their field, and none were "ordinary people" as some had originally hoped.
In many historical instances, some peers were not permitted to sit in the Upper House. When Scotland united with England to form Great Britain in 1707, it was provided that the Scottish hereditary peers would only be able to elect 16 representative peers to sit in the House of Lords; the term of a representative was to extend until the next general election. A similar provision was enacted in respect of Ireland when that kingdom merged with Great Britain in 1801; the Irish peers were allowed to elect 28 representatives, who were to retain office for life. Elections for Irish representatives ended in 1922, when most of Ireland became an independent state; elections for Scottish representatives ended with the passage of the Peerage Act 1963, under which all Scottish peers obtained seats in the Upper House.
Cash for Peerages.
The Honours (Prevention of Abuses) Act 1925 made it illegal for a peerage, or other honour, to be bought or sold. Nonetheless, there have been repeated allegations that life peerages (and thus membership of the House of Lords) have been made available to major political donors in exchange for donations. The most prominent case, the 2006 Cash for Honours scandal, saw a police investigation, with no charges being brought. A 2015 study found that of 303 people nominated for peerages in the period 2005-14, a total of 211 were former senior figures within politics (including former MPs), or were non-political appointments. Of the remaining 92 political appointments from outside public life, 27 had made significant donations to political parties. The authors concluded firstly that nominees from outside public life were much more likely to have made large gifts than peers nominated after prior political or public service. They also found that significant donors to parties were far more likely to be nominated for peerages than other party members.
Qualifications.
Several different qualifications apply for membership of the House of Lords. No person may sit in the House of Lords if under the age of 21. Furthermore, only citizens of the United Kingdom, Commonwealth citizens, and citizens of Ireland may sit in the House of Lords. The nationality restrictions were previously more stringent: under the Act of Settlement 1701, and prior to the British Nationality Act 1948, only natural-born subjects were qualified.
Additionally, some bankruptcy-related restrictions apply to members of the Upper House. A person may not sit in the House of Lords if he or she is the subject of a Bankruptcy Restrictions Order (applicable in England and Wales only), or if he or she is adjudged bankrupt (in Northern Ireland), or if his or her estate is sequestered (in Scotland). A final restriction bars an individual convicted of high treason from sitting in the House of Lords until completing his or her full term of imprisonment. An exception applies, however, if the individual convicted of high treason receives a full pardon. Note that an individual serving a prison sentence for an offence other than high treason is "not" automatically disqualified.
Women were excluded from the House of Lords until the Life Peerages Act 1958, passed to address the declining number of active members, made possible the creation of peerages for life. Women were immediately eligible and four were among the first life peers appointed. However, hereditary peeresses continued to be excluded until the passage of the Peerage Act 1963. Since the passage of the House of Lords Act 1999, hereditary peeresses remain eligible for election to the Upper House; there are two among the 90 hereditary peers who continue to sit.
Removal from House membership.
In 2014, the House of Lords Reform Act 2014 made provision for members' resignation from the House, removal for non-attendance, and automatic expulsion upon conviction for a serious criminal offence. In June 2015, under the House of Lords (Expulsion and Suspension) Act 2015, the House's Standing Orders may provide for the expulsion or suspension of a member upon a resolution of the House.
Officers.
Traditionally the House of Lords did not elect its own speaker, unlike the House of Commons; rather, the "ex officio" presiding officer was the Lord Chancellor. With the passage of the Constitutional Reform Act 2005, the post of Lord Speaker was created, a position to which a peer is elected by the House and subsequently appointed by the Crown. The first Lord Speaker, elected on 4 May 2006, was Baroness Hayman, a former Labour peer. As the Speaker is expected to be an impartial presiding officer, Baroness Hayman resigned from the Labour Party. In 2011, Baroness D'Souza was elected as the second Lord Speaker, replacing Baroness Hayman in September 2011.
This reform of the post of Lord Chancellor was made due to the perceived constitutional anomalies inherent in the role. The Lord Chancellor was not only the Speaker of the House of Lords, but also a member of the Cabinet; his or her department, formerly the Lord Chancellor's Department, is now called the Ministry of Justice. The Lord Chancellor is no longer the head of the judiciary of England and Wales. Hitherto, the Lord Chancellor was part of all three branches of government: the legislative, the executive, and the judicial.
The overlap of the legislative and executive roles is a characteristic of the Westminster system, as the entire cabinet consists of members of the House of Commons or the House of Lords; however, in June 2003, the Blair Government announced its intention to abolish the post of Lord Chancellor because of the office's mixed executive and judicial responsibilities. The abolition of the office was rejected by the House of Lords, and the Constitutional Reform Act 2005 was thus amended to preserve the office of Lord Chancellor. The Act no longer guarantees that the office holder of Lord Chancellor is the presiding officer of the House of Lords, and therefore allows the House of Lords to elect a speaker of their own.
The Lord Speaker may be replaced as presiding officer by one of his or her deputies. The Chairman of Committees, the Principal Deputy Chairman of Committees, and several Chairmen are all deputies to the Lord Speaker, and are all appointed by the House of Lords itself at the beginning of each session. By custom, the Crown appoints each Chairman, Principal Deputy Chairman and Deputy Chairman to the additional office of Deputy Speaker of the House of Lords. There was previously no legal requirement that the Lord Chancellor or a Deputy Speaker be a member of the House of Lords (though the same has long been customary).
Whilst presiding over the House of Lords, the Lord Chancellor traditionally wore ceremonial black and gold robes. Robes of black and gold are now worn by the Lord Chancellor and Secretary of State for Justice in the House of Commons, on ceremonial occasions. This is no longer a requirement for the Lord Speaker except for State occasions outside of the chamber. The Speaker or Deputy Speaker sits on the Woolsack, a large red seat stuffed with wool, at the front of the Lords Chamber.
When the House of Lords resolves itself into committee (see below), the Chairman of Committees or a Deputy Chairman of Committees presides, not from the Woolsack, but from a chair at the Table of the House. The presiding officer has little power compared to the Speaker of the House of Commons. He or she only acts as the mouthpiece of the House, performing duties such as announcing the results of votes. This is because, unlike in the House of Commons where all statements are directed to "Mr/Madam Speaker", in the House of Lords they are directed to "My Lords", i.e. the entire body of the House.
The Lord Speaker or Deputy Speaker cannot determine which members may speak, or discipline members for violating the rules of the House; these measures may be taken only by the House itself. Unlike the politically neutral Speaker of the House of Commons, the Lord Chancellor and Deputy Speakers originally remained members of their respective parties, and were permitted to participate in debate; however, this is no longer true of the new role of Lord Speaker.
Another officer of the body is the Leader of the House of Lords, a peer selected by the Prime Minister. The Leader of the House is responsible for steering Government bills through the House of Lords, and is a member of the Cabinet. The Leader also advises the House on proper procedure when necessary, but such advice is merely informal, rather than official and binding. A Deputy Leader is also appointed by the Prime Minister, and takes the place of an absent or unavailable Leader.
The Clerk of the Parliaments is the chief clerk and officer of the House of Lords (but is not a member of the House itself). The Clerk, who is appointed by the Crown, advises the presiding officer on the rules of the House, signs orders and official communications, endorses bills, and is the keeper of the official records of both Houses of Parliament. Moreover, the Clerk of the Parliaments is responsible for arranging by-elections of hereditary peers when necessary. The deputies of the Clerk of the Parliaments (the Clerk Assistant and the Reading Clerk) are appointed by the Lord Speaker, subject to the House's approval.
The Gentleman Usher of the Black Rod is also an officer of the House; he takes his title from the symbol of his office, a black rod. Black Rod (as the Gentleman Usher is normally known) is responsible for ceremonial arrangements, is in charge of the House's doorkeepers, and may (upon the order of the House) take action to end disorder or disturbance in the Chamber. Black Rod also holds the office of Serjeant-at-Arms of the House of Lords, and in this capacity attends upon the Lord Speaker. The Gentleman Usher of the Black Rod's duties may be delegated to the Yeoman Usher of the Black Rod or to the Assistant Serjeant-at-Arms.
Procedure.
The House of Lords and the House of Commons assemble in the Palace of Westminster. The Lords Chamber is lavishly decorated, in contrast with the more modestly furnished Commons Chamber. Benches in the Lords Chamber are coloured red. The Woolsack is at the front of the Chamber; the Government sit on benches on the right of the Woolsack, while members of the Opposition sit on the left. Crossbenchers, sit on the benches immediately opposite the Woolsack.
The Lords Chamber is the site of many formal ceremonies, the most famous of which is the State Opening of Parliament, held at the beginning of each new parliamentary session. During the State Opening, the Sovereign, seated on the Throne in the Lords Chamber and in the presence of both Houses of Parliament, delivers a speech outlining the Government's agenda for the upcoming parliamentary session.
In the House of Lords, members need not seek the recognition of the presiding officer before speaking, as is done in the House of Commons. If two or more Lords simultaneously rise to speak, the House decides which one is to be heard by acclamation, or, if necessary, by voting on a motion. Often, however, the Leader of the House will suggest an order, which is thereafter generally followed. Speeches in the House of Lords are addressed to the House as a whole ("My Lords") rather than to the presiding officer alone (as is the custom in the Lower House). Members may not refer to each other in the second person (as "you"), but rather use third person forms such as "the noble Duke", "the noble Earl", "the noble Lord", "my noble friend", "The most Reverend Primate" etc.
Each member may make no more than one speech on a motion, except that the mover of the motion may make one speech at the beginning of the debate and another at the end. Speeches are not subject to any time limits in the House; however, the House may put an end to a speech by approving a motion "that the noble Lord be no longer heard". It is also possible for the House to end the debate entirely, by approving a motion "that the Question be now put". This procedure is known as Closure, and is extremely rare.
Once all speeches on a motion have concluded, or Closure invoked, the motion may be put to a vote. The House first votes by voice vote; the Lord Speaker or Deputy Speaker puts the question, and the Lords respond either "Content" (in favour of the motion) or "Not Content" (against the motion). The presiding officer then announces the result of the voice vote, but if his assessment is challenged by any Lord, a recorded vote known as a division follows.
Members of the House enter one of two lobbies (the "Content" lobby or the "Not-Content" lobby) on either side of the Chamber, where their names are recorded by clerks. At each lobby are two Tellers (themselves members of the House) who count the votes of the Lords. The Lord Speaker may not take part in the vote. Once the division concludes, the Tellers provide the results thereof to the presiding officer, who then announces them to the House.
If there is an equality of votes, the motion is decided according to the following principles: legislation may proceed in its present form, unless there is a majority in favour of amending or rejecting it; any other motions are rejected, unless there is a majority in favour of approving it. The quorum of the House of Lords is just three members for a general or procedural vote, and 30 members for a vote on legislation. If fewer than three or 30 members (as appropriate) are present, the division is invalid.
Disciplinary powers.
By contrast with the House of Commons, the House of Lords has not had an established procedure for putting sanctions on its members. When a cash for influence scandal was referred to the Committee of Privileges in January 2009, the Leader of the House of Lords also asked the Privileges Committee to report on what sanctions the House had against its members. After seeking advice from the Attorney General for England and Wales and the former Lord Chancellor Lord Mackay of Clashfern, the committee decided that the House "possessed an inherent power" to suspend errant members, although not to withhold a Writ of summons nor to expel a member permanently. When the House subsequently suspended Lord Truscott and Lord Taylor of Blackburn for their role in the scandal, they were the first to meet this fate since 1642.
There are two other motions which have grown up through custom and practice and which govern questionable conduct within the House. They are brought into play by a member standing up, possibly intervening on another member, and moving the motion without notice. When the debate is getting excessively heated, it is open to a member to move "that the Standing Order on Asperity of Speech be read by the Clerk". The motion can be debated, but if agreed by the House, the Clerk of the Parliaments will read out Standing Order 33 which provides "That all personal, sharp, or taxing speeches be forborn". The Journals of the House of Lords record only four instances on which the House has ordered the Standing Order to be read since the procedure was invented in 1871.
For more serious problems with an individual Lord, the option is available to move "That the noble Lord be no longer heard". This motion also is debatable, and the debate which ensues has sometimes offered a chance for the member whose conduct has brought it about to come to order so that the motion can be withdrawn. If the motion is passed, its effect is to prevent the member from continuing their speech on the motion then under debate. The Journals identify eleven occasions on which this motion has been moved since 1884; four were eventually withdrawn, one was voted down, and six were passed.
Leave of absence.
In 1958, to counter criticism that some peers only appeared at major decisions in the House and thereby particular votes were swayed, the Standing Orders of the House of Lords were enhanced. Peers who did not wish to attend meetings regularly or were prevented by ill health, age or further reasons, were now able to request Leave of Absence. During the granted time a peer is expected not to visit the House's meetings until either its expiration or termination, announced at least a month prior to their return.
Committees.
Unlike in the House of Commons, when the term committee is used to describe a stage of a bill, this committee does not take the form of a public bill committee, but what is described as Committee of the Whole House. It is made up of all Members of the House of Lords allowing any Member to contribute to debates if he or she chooses to do so and allows for more flexible rules of procedure. It is presided over by the Chairman of Committees.
The term committee is also used to describe Grand Committee, where the same rules of procedure apply as in the main chamber, except that no divisions may take place. For this reason, business that is discussed in Grand Committee is usually uncontroversial and likely to be agreed unanimously.
Public bills may also be committed to pre-legislative committees. A pre-legislative Committee is specifically constituted for a particular bill. These committees are established in advance of the bill be laid before either the House of Lords or the House of Commons and can take evidence from the public. Such committees are rare and do not replace any of the usual stages of a bill, including committee stage.
The House of Lords also has 15 Select Committees. Typically, these are 'sessional committees', meaning that their members are appointed by the House at the beginning of each session, and continue to serve until the next parliamentary session begins. In practice, these are often permanent committees, which are re-established during every session. These committees are typically empowered to make reports to the House 'from time to time', that is whenever they wish. Other committees are 'ad-hoc' committees, which are set up to investigate a specific issue. When they are set up by a motion in the House, the motion will set a deadline by which the Committee must report. After this date, the Committee will cease to exist unless it is granted an extension. An example of this in the current parliamentary session is the Committee on Public Service and Demographic Change. The House of Lords may appoint a chairman for a committee; if it does not do so, the Chairman of Committees or a Deputy Chairman of Committees may preside instead. Most of the Select Committees are also granted the power to co-opt members, such as the European Union Committee. The primary function of Select Committees is to scrutinise and investigate Government activities; to fulfil these aims, they are permitted to hold hearings and collect evidence. Bills may be referred to Select Committees, but are more often sent to the Committee of the Whole House and Grand Committees.
The committee system of the House of Lords also includes several Domestic Committees, which supervise or consider the House's procedures and administration. One of the Domestic Committees is the Committee of Selection, which is responsible for assigning members to many of the House's other committees.
Current composition.
s of 8 2015[ [update]], the composition of the House of Lords is:
Opposition
px; width:px;height:20px; background-color: #DC241f">
Other
px; width:px;height:20px; background-color: #999">
px; width:px;height:20px; background-color: #FDBB30">
px; width:px;height:20px; background-color: ">
px; width:px;height:20px; background-color: #000">
px; width:px;height:20px; background-color: grey">
"Note: These figures exclude Members who are on leave of absence, disqualified as senior members of the judiciary or disqualified as MEPs."
The House of Lords Act 1999 allocated 75 of the 92 hereditary peers to the parties based on the proportion of hereditary peers that belonged to that party in 1999:
Of the initial 42 hereditary peers elected as Conservatives, one (Lord Willoughby de Broke) now sits as UKIP.
15 hereditary peers are elected by the whole House, and the remaining hereditary peers are the two royal office-holders, the Earl Marshal and the Lord Great Chamberlain, both being currently on leave of absence.
A report in 2007 stated that many members of the Lords (particularly the life peers) do not attend regularly; the average daily attendance was around 408.
While the number of hereditary peers is limited to 92, and that of Lords spiritual to 26, there is no maximum limit to the number of life peers who may be members of the House of Lords at any time.

</doc>
<doc id="13660" url="http://en.wikipedia.org/wiki?curid=13660" title="Homeomorphism">
Homeomorphism

In the mathematical field of topology, a homeomorphism or topological isomorphism or bi continuous function is a continuous function between topological spaces that has a continuous inverse function. Homeomorphisms are the isomorphisms in the category of topological spaces—that is, they are the mappings that preserve all the topological properties of a given space. Two spaces with a homeomorphism between them are called homeomorphic, and from a topological viewpoint they are the same. The word "homeomorphism" comes from the Greek words "ὅμοιος" ("homoios") = similar and "μορφή" ("morphē") = shape, form.
Roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. An often-repeated mathematical joke is that topologists can't tell their coffee cup from their donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.
Topology is the study of those properties of objects that do not change when homeomorphisms are applied.
Definition.
A function "f": "X" → "Y" between two topological spaces ("X", "TX") and ("Y", "TY") is called a homeomorphism if it has the following properties:
A function with these three properties is sometimes called bicontinuous. If such a function exists, we say "X" and "Y" are homeomorphic. A self-homeomorphism is a homeomorphism of a topological space and itself. The homeomorphisms form an equivalence relation on the class of all topological spaces. The resulting equivalence classes are called homeomorphism classes.
Notes.
The third requirement, that "f" −1 be continuous, is essential. Consider for instance the function "f": [0, 2π) → S1 (the unit circle in formula_6) defined by "f"(φ) = (cos(φ), sin(φ)). This function is bijective and continuous, but not a homeomorphism (S1 is compact but [0, 2π) is not). The function "f" −1 is not continuous at the point (1, 0), because although "f" −1 maps (1, 0) to 0, any neighbourhood of this point also includes points that the function maps close to 2π, but the points it maps to numbers in between lie outside the neighbourhood.
Homeomorphisms are the isomorphisms in the category of topological spaces. As such, the composition of two homeomorphisms is again a homeomorphism, and the set of all self-homeomorphisms "X" → "X" forms a group, called the homeomorphism group of "X", often denoted Homeo("X"); this group can be given a topology, such as the compact-open topology, making it a topological group.
For some purposes, the homeomorphism group happens to be too big, but by means of the isotopy relation, one can reduce this group to the mapping class group.
Similarly, as usual in category theory, given two spaces that are homeomorphic, the space of homeomorphisms between them, Homeo("X," "Y"), is a torsor for the homeomorphism groups Homeo("X") and Homeo("Y"), and given a specific homeomorphism between "X" and "Y", all three sets are identified.
Informal discussion.
The intuitive criterion of stretching, bending, cutting and gluing back together takes a certain amount of practice to apply correctly—it may not be obvious from the description above that deforming a line segment to a point is impermissible, for instance. It is thus important to realize that it is the formal definition given above that counts.
This characterization of a homeomorphism often leads to confusion with the concept of homotopy, which is actually "defined" as a continuous deformation, but from one "function" to another, rather than one space to another. In the case of a homeomorphism, envisioning a continuous deformation is a mental tool for keeping track of which points on space "X" correspond to which points on "Y"—one just follows them as "X" deforms. In the case of homotopy, the continuous deformation from one map to the other is of the essence, and it is also less restrictive, since none of the maps involved need to be one-to-one or onto. Homotopy does lead to a relation on spaces: homotopy equivalence.
There is a name for the kind of deformation involved in visualizing a homeomorphism. It is (except when cutting and regluing are required) an isotopy between the identity map on "X" and the homeomorphism from "X" to "Y".

</doc>
<doc id="13661" url="http://en.wikipedia.org/wiki?curid=13661" title="Hvergelmir">
Hvergelmir

In Norse mythology, Hvergelmir (Old Norse "bubbling boiling spring") is a major spring. Hvergelmir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", Hvergelmir is mentioned in a single stanza, which details that it is the location where liquid from the antlers of the stag Eikþyrnir flow, and that the spring, "whence all waters rise", is the source of numerous rivers. The "Prose Edda" repeats this information and adds that the spring is located in Niflheim, that it is one of the three major springs at the primary roots of the cosmic tree Yggdrasil (the other two are Urðarbrunnr and Mímisbrunnr), and that within the spring are a vast amount of snakes and the dragon Níðhöggr.
Attestations.
Hvergelmir is attested in the following works:
"Poetic Edda".
Hvergelmir receives a single mention in the "Poetic Edda", found in the poem "Grímnismál":
This stanza is followed three stanzas consisting mainly of the names of 42 rivers. Some of these rivers lead to the dwelling of the gods (such as Gömul and Geirvimul), while at least two (Gjöll and Leipt), reach to Hel.
"Prose Edda".
Hvergelmir is mentioned several times in the "Prose Edda". In "Gylfaginning", Just-as-High explains that the spring Hvergelmir is located in the foggy realm of Niflheim: "It was many ages before the earth was created that Niflheim was made, and in its midst lies a spring called Hvergelmir, and from it flows the rivers called Svol, Gunnthra, Fiorm, Fimbulthul, Slidr and Hrid, Sylg and Ylg, Vid, Leiptr; Gioll is next to Hell-gates."
Later in "Gylfaginning", Just-as-High describes the central tree Yggdrasil. Just-as-High says that three roots of the tree support it and "extend very, very far" and that the third of these three roots extends over Niflheim. Beneath this root, says Just-as-High, is the spring Hvergelmir, and that the base of the root is gnawed on by the dragon Níðhöggr. Additionally, High says that Hvergelmir contains not only Níðhöggr but also so many snakes that "no tongue can enumerate them".
The spring is mentioned a third time in "Gylfaginning" where High recounts its source: the stag Eikþyrnir stands on top of the afterlife hall Valhalla feeding branches of Yggdrasil, and from the stag's antlers drips great amounts of liquid down into Hvergelmir. High tallies 26 rivers here.
Hvergelmir is mentioned a final time in the "Prose Edda" where Third discusses the unpleasantries of Náströnd. Third notes that Hvergelmir yet worse than the venom-filled Náströnd because—by way of quoting a portion of a stanza from the "Poetic Edda" poem "Völuspá"—"There Nidhogg torments the bodies of the dead".
References.
</dl>

</doc>
<doc id="13665" url="http://en.wikipedia.org/wiki?curid=13665" title="Hausdorff maximal principle">
Hausdorff maximal principle

In mathematics, the Hausdorff maximal principle is an alternate and earlier formulation of Zorn's lemma proved by Felix Hausdorff in 1914 (Moore 1982:168). It states that in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset.
The Hausdorff maximal principle is one of many statements equivalent to the axiom of choice over Zermelo–Fraenkel set theory. The principle is also called the Hausdorff maximality theorem or the Kuratowski lemma (Kelley 1955:33).
Statement.
The Hausdorff maximal principle states that, in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset. Here a maximal totally ordered subset is one that, if enlarged in any way, does not remain totally ordered. The maximal set produced by the principle is not unique, in general; there may be many maximal totally ordered subsets containing a given totally ordered subset.
An equivalent form of the principle is that in every partially ordered set there exists a maximal totally ordered subset.
To prove that it follows from the original form, let "A" be a poset. Then formula_1 is a totally ordered subset of "A", hence there exists a maximal totally ordered subset containing formula_1, in particular "A" contains a maximal totally ordered subset.
For the converse direction, let "A" be a partially ordered set and "T" a totally ordered subset of "A". Then
is partially ordered by set inclusion formula_4, therefore it contains a maximal totally ordered subset "P". Then the set formula_5 satisfies the desired properties.
The proof that the Hausdorff maximal principle is equivalent to Zorn's lemma is very similar to this proof.

</doc>
<doc id="13666" url="http://en.wikipedia.org/wiki?curid=13666" title="Hel (being)">
Hel (being)

In Norse mythology, Hel is a being who presides over a realm of the same name, where she receives a portion of the dead. Hel is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In addition, she is mentioned in poems recorded in "Heimskringla" and "Egils saga" that date from the 9th and 10th centuries, respectively. An episode in the Latin work "Gesta Danorum", written in the 12th century by Saxo Grammaticus, is generally considered to refer to Hel, and Hel may appear on various Migration Period bracteates.
In the "Poetic Edda", "Prose Edda", and "Heimskringla", Hel is referred to as a daughter of Loki, and to "go to Hel" is to die. In the "Prose Edda" book "Gylfaginning", Hel is described as having been appointed by the god Odin as ruler of a realm of the same name, located in Niflheim. In the same source, her appearance is described as half blue and half flesh-coloured and further as having a gloomy, downcast appearance. The "Prose Edda" details that Hel rules over vast mansions with many servants in her underworld realm and plays a key role in the attempted resurrection of the god Baldr.
Scholarly theories have been proposed about Hel's potential connections to figures appearing in the 11th century "Old English Gospel of Nicodemus" and Old Norse "Bartholomeus saga postola", that she may have been considered a goddess with potential Indo-European parallels in Bhavani, Kali, and Mahakali or that Hel may have become a being only as a late personification of the location of the same name.
Attestations.
"Poetic Edda".
The "Poetic Edda", compiled in the 13th century from earlier traditional sources, features various poems that mention Hel. In the "Poetic Edda" poem "Völuspá", Hel's realm is referred to as the "Halls of Hel." In stanza 31 of "Grímnismál", Hel is listed as living beneath one of three roots growing from the world tree Yggdrasil. In "Fáfnismál", the hero Sigurd stands before the mortally wounded body of the dragon Fáfnir, and states that Fáfnir lies in pieces, where "Hel can take" him. In "Atlamál", the phrases "Hel has half of us" and "sent off to Hel" are used in reference to death, though it could be a reference to the location and not the being, if not both. In stanza 4 of "Baldrs draumar", Odin rides towards the "high hall of Hel."
Hel may also be alluded to in "Hamðismál". Death is periphrased as "joy of the troll-woman" (or "ogress") and ostensibly it is Hel being referred to as the troll-woman or the ogre ("flagð"), although it may otherwise be some unspecified "dís".
"Prose Edda".
Hel is referenced in the "Prose Edda", written in the 13th century by Snorri Sturluson. In chapter 34 of the book "Gylfaginning", Hel is listed by High as one of the three children of Loki and Angrboða; the wolf Fenrir, the serpent Jörmungandr, and Hel. High continues that, once the gods found that these three children are being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" then the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.
High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands," Odin threw Hel into Niflheim, and bestowed upon her authority over nine worlds, in that she must "administer board and lodging to those sent to her, and that is those who die of sickness or old age." High details that in this realm Hel has "great Mansions" with extremely high walls and immense gates, a hall called Éljúðnir, a dish called "Hunger," a knife called "Famine," the servant Ganglati (Old Norse "lazy walker"), the serving-maid Ganglöt (also "lazy walker"), the entrance threshold "Stumbling-block," the bed "Sick-bed," and the curtains "Gleaming-bale." High describes Hel as "half black and half flesh-coloured," adding that this makes her easily recognizable, and furthermore that Hel is "rather downcast and fierce-looking."
In chapter 49, High describes the events surrounding the death of the god Baldr. The goddess Frigg asks who among the Æsir will earn "all her love and favour" by riding to Hel, the location, to try to find Baldr, and offer Hel herself a ransom. The god Hermóðr volunteers and sets off upon the eight-legged horse Sleipnir to Hel. Hermóðr arrives in Hel's hall, finds his brother Baldr there, and stays the night. The next morning, Hermóðr begs Hel to allow Baldr to ride home with him, and tells her about the great weeping the Æsir have done upon Baldr's death. Hel says the love people have for Baldr that Hermóðr has claimed must be tested, stating:
Later in the chapter, after the female jötunn Þökk refuses to weep for the dead Baldr, she responds in verse, ending with "let Hel hold what she has." In chapter 51, High describes the events of Ragnarök, and details that when Loki arrives at the field Vígríðr "all of Hel's people" will arrive with him.
In chapter 5 of the "Prose Edda" book "Skáldskaparmál", Hel is mentioned in a kenning for Baldr ("Hel's companion"). In chapter 16, "Hel's [...] relative or father" is given as a kenning for Loki. In chapter 50, Hel is referenced ("to join the company of the quite monstrous wolf's sister") in the skaldic poem "Ragnarsdrápa".
"Heimskringla".
In the "Heimskringla" book "Ynglinga saga", written in the 13th century by Snorri Sturluson, Hel is referred to, though never by name. In chapter 17, the king Dyggvi dies of sickness. A poem from the 9th century "Ynglingatal" that forms the basis of "Ynglinga saga" is then quoted that describes Hel's taking of Dyggvi:
In chapter 45, a section from "Ynglingatal" is given which refers to Hel as "howes'-warder" (meaning "guardian of the graves") and as taking King Halfdan Hvitbeinn from life. In chapter 46, King Eystein Halfdansson dies by being knocked overboard by a sail yard. A section from "Ynglingatal" follows, describing that Eystein "fared to" Hel (referred to as "Býleistr's-brother's-daughter"). In chapter 47, the deceased Eystein's son King Halfdan dies of an illness, and the excerpt provided in the chapter describes his fate thereafter, a portion of which references Hel:
In a stanza from "Ynglingatal" recorded in chapter 72 of the "Heimskringla" book "Saga of Harald Sigurdsson", "given to Hel" is again used as a phrase to referring to death.
"Egils saga".
The Icelanders' saga "Egils saga" contains the poem "Sonatorrek". The saga attributes the poem to 10th century skald Egill Skallagrímsson, and writes that it was composed by Egill after the death of his son Gunnar. The final stanza of the poem contains a mention of Hel, though not by name:
"Gesta Danorum".
In the account of Baldr's death in Saxo Grammaticus' early 13th century work "Gesta Danorum", the dying Baldr has a dream visitation from Proserpina (here translated as "the goddess of death"):
The following night the goddess of death appeared to him in a dream standing at his side, and declared that in three days time she would clasp him in her arms. It was no idle vision, for after three days the acute pain of his injury brought his end.
Scholars have assumed that Saxo used Proserpina as a goddess equivalent to the Norse Hel.
Archaeological record.
It has been suggested that several Migration Period imitation medallions and bracteates feature depictions of Hel. In particular the bracteates IK 14 and IK 124 depict a rider traveling down a slope and coming upon a female being holding a scepter or a staff. The downward slope may indicate that the rider is traveling towards the realm of the dead and the woman with the scepter may be a female ruler of that realm, corresponding to Hel.
Some B-class bracteates showing three godly figures have been interpreted as depicting Baldr's death, the best known of these is the Fakse bracteate. Two of the figures are understood to be Baldr and Odin while both Loki and Hel have been proposed as candidates for the third figure. If it is Hel she is presumably greeting the dying Baldr as he comes to her realm.
Theories.
Seo Hell.
The "Old English Gospel of Nicodemus", preserved in two manuscripts from the 11th century, contains a female figure referred to as "Seo hell" who engages in flyting with Satan and tells him to leave her dwelling (Old English "ut of mynre onwununge"). Regarding Seo Hell in the "Old English Gospel of Nicodemus", Michael Bell states that "her vivid personification in a dramatically excellent scene suggests that her gender is more than grammatical, and invites comparison with the Old Norse underworld goddess Hel and the Frau Holle of German folklore, to say nothing of underworld goddesses in other cultures" yet adds that "the possibility that these genders "are" merely grammatical is strengthened by the fact that an Old Norse version of Nicodemus, possibly translated under English influence, personifies Hell in the neuter (Old Norse "þat helviti")."
"Bartholomeus saga postola".
The Old Norse "Bartholomeus saga postola", an account of the life of Saint Bartholomew dating from the 13th century, mentions a "Queen Hel." In the story, a devil is hiding within a pagan idol, and bound by Bartholomew's spiritual powers to acknowledge himself and confess, the devil refers to Jesus as the one which "made war on Hel our queen" (Old Norse "heriaði a Hel drottning vara"). "Queen Hel" is not mentioned elsewhere in the saga.
Michael Bell says that while Hel "might at first appear to be identical with the well-known pagan goddess of the Norse underworld" as described in chapter 34 of "Gylfaginning", "in the combined light of the Old English and Old Norse versions of "Nicodemus" she casts quite a different a shadow," and that in "Bartholomeus saga postola" "she is clearly the queen of the Christian, not pagan, underworld."
Origins and development.
Jacob Grimm theorized that Hel (whom he refers to here as "Halja", the theorized Proto-Germanic form of the term) is essentially an "image of a greedy, unrestoring, female deity" and that "the higher we are allowed to penetrate into our antiquities, the less hellish and more godlike may "Halja" appear. Of this we have a particularly strong guarantee in her affinity to the Indian Bhavani, who travels about and bathes like Nerthus and Holda, but is likewise called "Kali" or "Mahakali", the great "black" goddess. In the underworld she is supposed to sit in judgment on souls. This office, the similar name and the black hue [...] make her exceedingly like "Halja". And "Halja" is one of the oldest and commonest conceptions of our heathenism."
Grimm theorizes that the Helhest, a three legged-horse that roams the countryside "as a harbinger of plague and pestilence" in Danish folklore, was originally the steed of the goddess Hel, and that on this steed Hel roamed the land "picking up the dead that were her due." In addition, Grimm says that a wagon was once ascribed to Hel, with which Hel made journeys. Grimm says that Hel is an example of a "half-goddess;" "one who cannot be shown to be either wife or daughter of a god, and who stands in a dependent relation to higher divinities" and that "half-goddesses" stand higher than "half-gods" in Germanic mythology.
Hilda Ellis Davidson (1948) states that Hel "as a goddess" in surviving sources seems to belong to a genre of literary personification, that the word "hel" is generally "used simply to signify death or the grave," and that the word often appears as the equivalent to the English 'death,' which Davidson states "naturally lends itself to personification by poets." Davidson explains that "whether this personification has originally been based on a belief in a goddess of death called Hel is another question," but that she does not believe that the surviving sources give any reason to believe so. Davidson adds that, on the other hand, various other examples of "certain supernatural women" connected with death are to be found in sources for Norse mythology, that they "seem to have been closely connected with the world of death, and were pictured as welcoming dead warriors," and that the depiction of Hel "as a goddess" in "Gylfaginning" "might well owe something to these."
In a later work (1998), Davidson states that the description of Hel found in chapter 33 of "Gylfaginning" "hardly suggests a goddess." Davidson adds that "yet this is not the impression given in the account of Hermod's ride to Hel later in "Gylfaginning" (49)" and points out that here Hel "[speaks] with authority as ruler of the underworld" and that from her realm "gifts are sent back to Frigg and Fulla by Balder's wife Nanna as from a friendly kingdom." Davidson posits that Snorri may have "earlier turned the goddess of death into an allegorical figure, just as he made Hel, the underworld of shades, a place 'where wicked men go,' like the Christian Hell ("Gylfaginning" 3)." Davidson continues that:
Davidson further compares to early attestations of the Irish goddesses Badb (Davidson points to the description of Badb from "The Destruction of Da Choca's Hostel" where Badb is wearing a dusky mantle, has a large mouth, is dark in color, and has gray hair falling over her shoulders, or, alternatively, "as a red figure on the edge of the ford, washing the chariot of a king doomed to die") and The Morrígan. Davidson concludes that, in these examples, "here we have the fierce destructive side of death, with a strong emphasis on its physical horrors, so perhaps we should not assume that the gruesome figure of Hel is wholly Snorri's literary invention."
John Lindow states that most details about Hel, as a figure, are not found outside of Snorri's writing in "Gylfaginning", and says that when older skaldic poetry "says that people are 'in' rather than 'with' Hel, we are clearly dealing with a place rather than a person, and this is assumed to be the older conception," that the noun and place "Hel" likely originally simply meant "grave," and that "the personification came later." Rudolf Simek theorizes that the figure of Hel is "probably a very late personification of the underworld Hel," and says that "the first kennings using the goddess Hel are found at the end of the 10th and in the 11th centuries." Simek states that the allegorical description of Hel's house in "Gylfaginning" "clearly stands in the Christian tradition," and that "on the whole nothing speaks in favour of there being a belief in Hel in pre-Christian times." However, Simek also cites Hel as possibly appearing as one of three figures appearing together on Migration Period B-bracteates.
References.
</dl>

</doc>
<doc id="13667" url="http://en.wikipedia.org/wiki?curid=13667" title="Hawar Islands">
Hawar Islands

The Hawar Islands (Arabic: جزر حوار‎; transliterated: Juzur Ḩawār) is an archipelago of desert islands owned by Bahrain. They are situated off the west coast of Qatar in the Gulf of Bahrain of the Persian Gulf.
Description.
Despite their proximity to Qatar (they are only 1.1 km from the Qatari mainland whilst being 16 km from the main islands of Bahrain), the islands belong to Bahrain, having been a part of a dispute between Bahrain and Qatar which was resolved in 2001. The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain. The land area of the islands is approximately 52 km2.
The islands were listed as a Ramsar site in 1997. In 2002, the Bahraini government applied to have the islands recognised as a World Heritage Site due to their unique environment and habitat for endangered species; the application was ultimately unsuccessful. This site is home to many bird species, notably Socotra cormorants. There are small herds of Arabian oryx and sand gazelle on Hawar island and the seas around support a large population of dugong.
Although there are 36 islands in the group, many of the smaller islands are little more than sand or shingle accumulations on areas of exposed bedrock moulded by the ongoing processes of sedimentation and accretion. The WHS application named 8 major islands (see table below), which conforms to the description of the islands when first surveyed as consisting of 8 or 9 islands. It has often been described as an archipelago of 16 islands. Janan island, to the south of Hawar island, is not legally considered to be a part of the group and is owned by Qatar.
The islands used to be one of the settlements of the Bahraini branch of the Dawasir who settled there in the early 19th century. The islands were first surveyed in 1820, when they were called the Warden’s Islands, and two villages were recorded. They are now uninhabited, other than a police garrison and a hotel on the main island; access to all but Hawar island itself is severely restricted. Local fishermen are allowed to fish in adjacent waters and there is some recreational fishing and tourism on and around the islands. Fresh water has always been scarce; historically it was obtained by surface collection and even today, with the desalinisation plant, additional supplies have to be brought in.
List of islands.
Hawar archipelago.
By far the largest island is Hawar, which accounts for more than 41 km2 of the 52 km2 land area. Following in size are Suwād al Janūbīyah, Suwād ash Shamālīyah, Rubud Al Sharqiyah, Rubud Al Gharbiyah and Muhazwarah (Umm Hazwarah).
The following were not considered as part of the Hawar islands in the International Court of Justice (ICJ) judgment, being located between Hawar and the Bahrain Islands and not disputed by Qatar, but have been included in the Hawar archipelago by the Bahrain government as part of the 2002 World Heritage Site application.
Janan island.
Janan island, a small island south of Hawar island, was also considered in the 2001 ICJ judgment. Based on a previous agreement when both Qatar and Bahrain were under British protection, it was judged to be separate from the Hawar islands and so considered by the court separately. It was awarded to Qatar.

</doc>
<doc id="13669" url="http://en.wikipedia.org/wiki?curid=13669" title="Hans-Dietrich Genscher">
Hans-Dietrich Genscher

Hans-Dietrich Genscher (born 21 March 1927) is a German politician of the liberal Free Democratic Party (FDP). He served as Foreign Minister and Vice Chancellor of Germany from 1974 to 1992 (except for a two-week break in 1982), making him the longest-tenured holder of either post. In 1991, he was the chairman of the Organization for Security and Co-operation in Europe (OSCE).
Biography.
Early life.
Genscher was born on 21 March 1927 in Reideburg (Province of Saxony), now a part of Halle, in what later became East Germany. He was drafted to serve as a member of the Air Force Support Personnel ("Luftwaffenhelfer") at the age of 16. In 1945 he became a member of the Nazi Party. According to Genscher's statements, this happened through a collective application in his Wehrmacht unit and against his own intentions.
Genscher fought as a young man in the Wehrmacht at the end of the Second World War. In 1945, Genscher was a soldier in General Walther Wenck's 12th Army. He briefly became an American and British prisoner of war. Following World War II, he studied law and economics at the universities of Halle and Leipzig (1946–1949) and joined the East German Liberal Democratic Party (LDPD) in 1946.
Political career.
In 1952, Genscher fled to West Germany, where he joined the Free Democratic Party (FDP). He passed his second state examination in law in Hamburg in 1954 and became a solicitor in Bremen. From 1956 to 1959 he was a research assistant of the FDP parliamentary group in Bonn. From 1959 to 1965 he was the FDP group managing director, while from 1962 to 1964 he was National Secretary of the FDP.
In 1965 Genscher was elected on the North Rhine-Westphalian FDP list to the West German parliament and remained a member of parliament until his retirement in 1998. He was elected deputy national chairman in 1968. After serving in several party offices, he was appointed Minister of the Interior by Chancellor Willy Brandt, whose Social Democratic Party was in coalition with the FDP, in 1969; in 1974, he became foreign minister and Vice Chancellor.
From 1 October 1974 to 23 February 1985 he was Chairman of the FDP. It was during his tenure as party chairman that the FDP switched from being the junior member of social-liberal coalition to being the junior member of the 1982 coalition with the CDU/CSU. In 1985 he gave up the post of national chairman. After his resignation as Foreign Minister, Genscher was appointed honorary chairman of the FDP in 1992.
Minister of the Interior.
After the federal election of 1969 Genscher was instrumental in the formation of the social-liberal coalition and was on 22 October 1969 appointed as Minister of the Interior. 
In 1972, while Minister for the Interior, he rejected Israel's offer to send an Israeli special forces unit to Germany to deal with the Munich Olympics hostage crisis. A flawed rescue attempt by German police forces at Fürstenfeldbruck air base resulted in a bloody shootout, which left all eleven hostages, five terrorists, and one German policeman dead. Genscher's popularity with Israel declined further when he endorsed the release of the three captured attackers following the hijacking of a Lufthansa aircraft on 29 October 1972.
In the SPD-FDP coalition, he helped shape Brandt's policy of deescalation with the communist East, commonly known as "Ostpolitik", which was continued under Helmut Schmidt after Brandt's resignation in 1974.
Vice Chancellor and Federal Foreign Minister.
As Foreign Minister, Genscher stood for a policy of compromise between East and West, and developed strategies for an active policy of détente and the continuation of the East-West dialogue with the USSR. He was widey regarded a strong advocate of negotiated settlements to international problems.
Genscher was a major player in the negotiations on the text of the Helsinki Accords. In December 1976, the General Assembly of the United Nations accepted in New York Genscher's proposal of an anti-terrorism convention in New York, which was set among other things, to respond to demands from hostage-takers under any circumstances.
Genscher was one of the FDP's driving forces when, in 1982, the party switched sides from its coalition with the SPD to support the CDU/CSU in their Constructive vote of no confidence to have incumbent Helmut Schmidt replaced with opposition leader Helmut Kohl as Chancellor. The reason for this was the increase in the differences between the coalition partners, particularly in economic and social policy. Despite the great controversy that accompanied this switch, he remained one of the most popular politicians in West Germany.
At several points in his tenure, he has irritated the governments of the United States and other allies of Germany by appearing not to support Western initiatives fully. During the Cold War, his penchant to seek the middle ground at times exasperated United States policy-makers who wanted a more decisive, less equivocal Germany. They accused him of a quasi-neutralism that came to be known as "Genscherism". Fundamental to "Genscherism" was said to be the belief that Germany could play a role as a bridge between East and West without losing its status as a reliable NATO ally. In the 1980s, Genscher opposed the deployment of new short-range NATO missiles in Germany. At the time, the Reagan Administration questioned whether Germany was straying from the Western alliance and following a program of its own.
In 1988, Genscher appointed Jürgen Hellner as West Germany’s new ambassador to Libya, a post that had been vacant since the 1986 Berlin discotheque bombing that U.S. officials blamed on the government of Muammar Gaddafi.
Genscher’s proposals frequently set the tone and direction of foreign affairs among Western Europe's democracies. He was also an active participant in the further development of the European Union, taking an active part in the Single European Act Treaty negotiations in the mid-1980s, as well as the joint publication of the Genscher-Colombo plan with Italian Minister of Foreign Affairs Emilio Colombo which advocated further integration and deepening of relations in the European Union towards a more federal Europe.
Genscher retained his posts as foreign minister and vice chancellor through German reunification and until 1992 when he stepped down for health reasons.
Reunification efforts.
Genscher is most respected for his efforts that helped spell the end of the Cold War, in the late 1980s when Communist eastern European governments toppled, and which led to German reunification. During his time in office, he focused on maintaining stability and balance between the West and the Soviet bloc. From the beginning, he argued that the West should seek cooperation with Communist governments rather than treat them as implacably hostile; this policy was embraced by many Germans and other Europeans.
Genscher had great interest in European integration and the success of German reunification. He soon pushed for effective support of political reform processes in Poland and Hungary. For this purpose, he visited Poland to meet the chairman of Solidarity Lech Wałęsa as early as January 1980. Especially from 1987 he campaigned for an "active relaxation" policy response by the West to the Soviet efforts. In the years before German reunification, he made a point of maintaining strong ties with his birthplace Halle, which was regarded as significant by admirers and critics alike.
When thousands of East Germans sought refuge in West German embassies in Czechoslovakia and Poland, Genscher held discussions on the refugee crisis at the United Nations in New York with the foreign ministers of Czechoslovakia, Poland, East Germany and the Soviet Union in September 1989. Genscher’s 30 September 1989 speech from the balcony of the German embassy in Prague was an important milestone on the road to the end of the GDR. In the embassy courtyard thousands of East German citizens had assembled. They were trying to travel to West Germany, but were being denied permission to travel by the Czechoslovak government at the request of East Germany. He announced that he had reached an agreement with the Communist Czechoslovakian government that the refugees could leave: "We have come to you to tell you that today, your departure ..." (German: "Wir sind zu Ihnen gekommen, um Ihnen mitzuteilen, dass heute Ihre Ausreise ..."). After these words, the speech was drowned in cheers.
Genscher negotiated German reunification in 1990 with his counterpart from the GDR, Markus Meckel. In November 1990, Genscher and his Polish counterpart Krzysztof Skubiszewski signed the German-Polish Border Treaty on the establishment of the Oder–Neisse line as Poland's western border.
Post Reunification.
In 1991, Genscher successfully pushed for Germany’s recognition of the Republic of Croatia in the Croatian War of Independence shortly after the Serbian attack on Vukovar. After Croatia and Slovenia had declared independence, Genscher concluded that Yugoslavia could not be held together, and that republics that wanted to break from the Serbian-dominated federation deserved quick diplomatic recognition. He hoped that such recognition would stop the fighting. The rest of the European Union was subsequently pressured to follow suit soon afterward. The UN Secretary-General Javier Pérez de Cuéllar had warned the German Government, that a recognition of Slovenia and Croatia would lead to an increase in aggression in the former Yugoslavia .
At a meeting of the European Community’s foreign ministers in 1991, Genscher proposed to press for a war crimes trial for President Saddam Hussein of Iraq, accusing him of aggression against Kuwait, using chemical weapons against civilians and condoning genocide against the Kurds.
During the Gulf War, Genscher sought to deal with Iraq after other Western leaders had decided to go to war to force it out of Kuwait. Germany made a substantial financial contribution to the allied cause but, citing constitutional restrictions on the use of its armed forces, provided almost no military assistance. When, in the aftermath of the war, a far-reaching political debate broke out over how Germany should fulfill its global responsibilities, Genscher responded that if foreign powers expect Germany to assume greater responsibility in the world, they should give it a chance to express its views "more strongly" in the United Nations Security Council.
In 1992, Genscher, together with his Danish colleague Uffe Ellemann-Jensen, took the initiative to create the Council of the Baltic Sea States (CBSS) and the EuroFaculty.
More than half a century after Nazi leaders assembled their infamous exhibition "Degenerate Art," a sweeping condemnation of the work of the avant-garde, Genscher opened a re-creation of the show at the Altes Museum in March 1992, describing Nazi attempts to restrict artistic expression as "a step toward the catastrophe that produced the mass murder of European Jews and the war of extermination against Germany's neighbors." "The paintings in this exhibition have survived oppression and censorship," he asserted in his opening remarks. "They are not only a monument but also a sign of hope. They stand for the triumph of creative freedom over barbarism."
As a popular story on Genscher’s preferred method of shuttle diplomacy has it, "two Lufthansa jets crossed over the Atlantic, and Genscher was on both."
On 18 May 1992 Genscher retired at his own request from the federal government, which he had been member of for a total of 23 years. At the time, he was the world's longest-serving foreign minister and Germany's most popular politician. He had announced his decision three weeks earlier, on 27 April 1992. At that time he was Europe's longest-serving foreign minister. Genscher did not specify his reasons for quitting; however, he had suffered two heart attacks by that time. His resignation took effect in May, but he remained a member of parliament and continued to be influential in the Free Democratic Party.
Following Genscher’s resignation, Chancellor Helmut Kohl and FDP chairman Otto Graf Lambsdorff named Irmgard Schwaetzer, a former aide to Genscher, to be the new Foreign Minister. In a surprise decision, however, a majority of the FDP parliamentary group rejected her nomination and voted instead to name Justice Minister Klaus Kinkel to head the Foreign Ministry.
Activities after politics.
Having finished his political career, Genscher has been active as a lawyer and in international organizations.
In late 1992, Genscher was appointed chairman of a newly established donors’ board of the Berlin State Opera.
Between 1999 and 2010, Genscher was affiliated with the law firm Büsing, Müffelmann & Theye. He founded his own consulting firm, Hans-Dietrich Genscher Consult GmbH, in 2000. Between 2001 and 2013, he served as president of the German Council on Foreign Relations.
In 2001, Genscher headed an arbitration that ended a monthlong battle between German airline Lufthansa and its pilots' union and resulted in an agreement on increasing wages by more than 15 percent by the end of the following year.
In 2009 Genscher expressed public concern at Pope Benedict XVI's lifting of excommunication of the bishops of the Society of Saint Pius X. Genscher wrote in the "Mitteldeutsche Zeitung": "Poles can be proud of Pope John Paul II. At the last papal election, we said We are the pope! But please—not like this." He argued that Pope Benedict XVI was making a habit of offending non-Catholics. "This is a deep moral and political question. It is about respect for the victims of crimes against humanity", Genscher said.
On December 20, 2013, it was revealed that Genscher played a key role in coordinating the release and flight to Germany of Mikhail Khodorkovsky, the former head of Yukos. Once Russian President Vladimir Putin was re-elected in 2012, German Chancellor Angela Merkel instructed her officials to lobby for the president to meet Genscher. The subsequent negotiations involved two meetings between Genscher and Putin — one at Berlin Tegel Airport at the end of Putin’s first visit to Germany after he was re-elected in 2012, the other in Moscow. While keeping the chancellor informed, Khodorkovsky's attorneys and Genscher spent the ensuing months developing a variety of legal avenues that could allow Putin to release his former rival early, ranging from amendments to existing laws to clemency. When Khodorkovsky's mother was in a Berlin hospital with cancer in November 2013, Genscher passed a message to Khordorkovsky suggesting the prisoner should write a pardon letter to Putin emphasizing his mother's ill health. Following Putin’s pardoning of Khodorkovsky "for humanitarian reasons" in December 2013, a private plane provided by Genscher brought Khodorkovsky to Berlin for a family reunion at the Hotel Adlon.
Recognition (selection).
Genscher has been awarded honorary citizenship by his birthplace Halle (Saale) and the city of Berlin.

</doc>
<doc id="13675" url="http://en.wikipedia.org/wiki?curid=13675" title="Henry Ainsworth">
Henry Ainsworth

Henry Ainsworth (1571–1622) was an English Nonconformist clergyman and scholar.
Life.
He was born of a farming family of Swanton Morley, Norfolk. He was educated at St John's College, Cambridge, later moving to Caius College, and, after associating with the Puritan party in the Church, eventually joined the Separatists.
Driven abroad about 1593, he found a home in "a blind lane at Amsterdam", acting as "porter" to a bookseller, who, on discovering his knowledge of Hebrew, introduced him to other scholars. When part of the London church, of which Francis Johnson (then in prison) was pastor, reassembled in Amsterdam, Ainsworth was chosen as their doctor or teacher. In 1596 he drew up a confession of their faith, reissued in Latin in 1598 and dedicated to the various universities of Europe (including St Andrews, Scotland). Johnson joined his flock in 1597, and in 1604 he and Ainsworth composed "An Apology or Defence of such true Christians as are commonly but unjustly called Brownists".
Organizing the church was not easy and dissension was rife. Though often involved in controversy, Ainsworth was not arrogant, but was a steadfast and cultured champion of the principles represented by the early Congregationalists. Amid all the controversy, he steadily pursued his studies. The combination was so unique that some have mistaken him for two different individuals. (Confusion has also been occasioned through his friendly controversy with one John Ainsworth, who left the Anglican for the Roman Catholic church.)
In 1610 Ainsworth was forced reluctantly to withdraw, with a large part of their church, from Johnson and those who adhered to him. A difference of principle as to the church's right to revise its officers' decisions had been growing between them; Ainsworth taking the more Congregational view. In spirit he remained a man of peace.
He died in 1622 in Amsterdam.
Works.
In 1608 Ainsworth answered Richard Bernard's "The Separatist Schisme", but his greatest minor work in this field was his reply to John Smyth (commonly called "the Se-Baptist"), entitled "Defence of Holy Scripture, Worship and Ministry used in the Christian Churches separated from Antichrist, against the Challenges, Cavils and Contradictions of Mr Smyth" (1609).
His scholarly works include his "Annotations"—on "Genesis" (1616); "Exodus" (1617); "Leviticus" (1618); "Numbers" (1619); "Deuteronomy" (1619); "Psalms" (including a metrical version, 1612); and the "Song of Solomon" (1623). These were collected in folio in 1627. From the outset the "Annotations" took a commanding place, especially among continental scholars, establishing a scholarly tradition for English nonconformity.
His publication of Psalms, "The Book of Psalmes: Englished both in Prose and Metre with Annotations" (Amsterdam, 1612), which includes thirty-nine separate monophonic psalm tunes, constituted the Ainsworth Psalter, the only book of music brought to New England in 1620 by the Pilgrim settlers. Although its content was later reworked into the Bay Psalm Book, it had an important influence on the early development of American psalmody.
Ainsworth died in 1622, or early in 1623, for in that year was published his "Seasonable Discourse, or a Censure upon a Dialogue of the Anabaptists", in which the editor speaks of him as a departed worthy.

</doc>
<doc id="13677" url="http://en.wikipedia.org/wiki?curid=13677" title="Hindu">
Hindu

 
Hindu (  ) can refer to either a religious or cultural identity associated with the philosophical, religious and cultural systems that are indigenous to the Indian subcontinent. In common use today, it refers to an adherent of Hinduism. However, in the Constitution of India, the word "Hindu" has been used in places to denote persons professing any religion originated in India (i.e. Hinduism, Jainism, Buddhism or Sikhism). Further, the terms Hindu or Hindi are also used as a cultural identity to denote people living on the other side of the Indus river, thus poets like Iqbal, ministers like M.C.Chagla and organisations like the RSS used the terms Hindu and Hindi to represent any person living on the other side of the Indus river, irrespective of religion.
The word "Hindu" is derived (through Persian) from the Sanskrit word "Sindhu", the historic local name for the Indus River in the northwestern part of the Indian subcontinent (modern day Pakistan and Northern India). According to Gavin Flood, "The actual term "Hindu" first occurs as a Persian geographical term for the people who lived beyond the river Indus (Sanskrit: "Sindhu")". The term "Hindu" then was a geographical term and did not refer to a religion.
The term "Hindu" was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450). The Hindu religion ("dharma") was set in apposition with Islam ("turaka dharma") by poets such as Vidyapati, Kabir and Eknath.
16th- to 18th-century Bengali Gaudiya Vaishnava texts including "Chaitanya Charitamrita" and "Chaitanya Bhagavata" also made similar comparisons. 
Towards the end of the 18th century, the European merchants and colonists began to refer to the followers of Indian religions collectively as "Hindus". The term "Hinduism" was introduced into the English language in the 19th century to denote the religious, philosophical, and cultural traditions native to India.
With more than a billion adherents, Hinduism is the world's third largest religion after Christianity and Islam. The vast majority of Hindus, approximately 940 million, live in India. Other countries with large Hindu populations include Nepal, Bangladesh, Sri Lanka, Mauritius, Suriname, Guyana, Trinidad & Tobago, United States, Fiji, United Kingdom, Singapore, Canada and the island of Bali in Indonesia.
Etymology.
In origin, "Hinduš" was Old Persian name of the Indus River, cognate with Sanskrit word "Sindhu". By about 2nd - 1st century BCE, the term "Hein-tu" was used by Chinese, for referring to North Indian people. The Persian term was loaned into Arabic as "al-Hind" referring to the land of the people who live across river Indus, and into Greek as "Indos", whence ultimately English India.
History.
The notion of grouping the indigenous religions of India under a single umbrella term "Hindu" emerges as a result of various invasions in India bringing forth non-indigenous religions such as Islam to the Indian Subcontinent Numerous Muslim invaders, such as Nader Shah, Mahmud of Ghazni, Ahmad Shāh Abdālī, Muhammad Ghori, Babur and Aurangzeb, destroyed Hindu temples and persecuted Hindus; some, such as Akbar, were more tolerant. Hinduism underwent profound changes, in large part due to the influence of the prominent teachers Ramanuja, Madhva and Chaitanya. Followers of the Bhakti Movement moved away from the abstract concept of Brahman, which the philosopher Adi Shankara consolidated a few centuries before, with emotional, passionate devotion towards what they believed as the more accessible Avatars, especially Krishna and Rama.
Indology as an academic discipline of studying Indian culture from a European perspective was established in the 18th century by Sir William Jones and 19th century, by scholars such as Max Müller and John Woodroffe. They brought Vedic, Puranic and Tantric literature and philosophy to Europe and the United States. At the same time, societies such as the Brahmo Samaj and the Theosophical Society attempted to reconcile and fuse Abrahamic and Dharmic philosophies, endeavouring to institute societal reform. This period saw the emergence of movements which, while highly innovative, were rooted in indigenous tradition. They were based on the personalities and teachings of individuals, as with Ramakrishna and Ramana Maharshi. Prominent Hindu philosophers, including Aurobindo and Prabhupada (founder of ISKCON), translated, reformulated and presented Hinduism's foundational texts for contemporary audiences in new iterations, attracting followers and attention in India and abroad.
Others, such as Swami Vivekananda, Ramakrishna, Paramahansa Yogananda, Sri Chinmoy, B.K.S. Iyengar and Swami Rama, have also been instrumental in raising the profiles of Yoga and Vedanta in the West. Today modern movements, such as ISKCON and the Swaminarayan Faith, attract a large amount of followers across the world.
Definition.
The diverse set of religious beliefs, traditions and philosophies of the Hindus are the product of an amalgamation process that began with the decline of Buddhism in India (5th-8th Century), where traditions of Vedic Brahmanism and the mystical schools of Vedanta were combined with Shramana traditions and regional cults to give rise to the socio-religious and cultural sphere later described as "Hinduism".
Adi Shankara's commentaries on the Upanishads led to the rise of Advaita Vedanta, the most influential sub-school of Vedanta.
Hinduism continues to be divided in numerous sects and denominations, of which "Vaishnavism" and "Shaivism" are by far the most popular. Other aspects include folk and conservative Vedic Hinduism.
Since the 18th century, Hinduism has accommodated a host of new religious and reform movements, with Arya Samaj being one of the most notable Hindu revivalist organizations.
Due to the wide diversity in the beliefs, practices and traditions encompassed by Hinduism, there is no universally accepted definition on who a Hindu is, or even agreement on whether the term Hinduism represents a religious, cultural or socio-political entity. In 1995, Chief Justice P. B. Gajendragadkar was quoted in an Indian Supreme Court ruling:
Thus some scholars argue that the Hinduism is not a religion "per se" but rather a reification of a diverse set of traditions and practices by scholars who constituted a unified system and arbitrarily labeled it Hinduism. The usage may also have been necessitated by the desire to distinguish between "Hindus" and followers of other religions during the periodic census undertaken by the colonial British government in India. Other scholars, while seeing Hinduism as a 19th-century construct, view Hinduism as a response to British colonialism by Indian nationalists who forged a unified tradition centered on oral and written Sanskrit texts adopted as scriptures.
While Hinduism contains both "uniting and dispersing tendencies", it also has a common central thread of philosophical concepts (including dharma, moksha and samsara), practices (puja, bhakti etc.) and cultural traditions. These common elements originated (or were codified within) the Vedic, Upanishad and Puranic scriptures and epics. Thus a Hindu could:
The Republic of India is in the peculiar situation that the Supreme Court of India has repeatedly been called upon to define "Hinduism" because the Constitution of India, while it prohibits "discrimination of any citizen" on grounds of religion in article 15, article 30 foresees special rights for "All minorities, whether based on religion or language". As a consequence, religious groups have an interest in being recognized as distinct from the Hindu majority in order to qualify as a "religious minority". Thus, the Supreme Court was forced to consider the question whether Jainism is part of Hinduism in 2005 and 2006. In the 2006 verdict, the Supreme Court found that the "Jain Religion is indisputably not a part of the Hindu Religion".
In 1995, while considering the question "who are Hindus and what are the broad features of Hindu religion", the Supreme Court of India highlighted Bal Gangadhar Tilak's formulation of seven defining features of Hinduism:
Some thinkers have attempted to distinguish between the concept of Hinduism as a religion, and a Hindu as a member of a nationalist or socio-political class.
In Hindu nationalism, the term "Hindu" combines notions of geographical unity, common culture and common race. Thus, Veer Savarkar in his influential pamphlet "" defined a Hindu as a person who sees India "as his Fatherland as well as his Holy land, that is, the cradle land of his religion". This conceptualization of Hinduism, has led to establishment of Hindutva as the dominant force in Hindu nationalism over the last century.
Ethnic and cultural fabric.
Hinduism, its religious doctrines, traditions and observances are very typical and inextricably linked to the culture and demographics of India. Hinduism has one of the most ethnically diverse bodies of adherents in the world. It is hard to classify Hinduism as a religion because the framework, symbols, leaders and books of reference that make up some of the world's other religions are not uniquely identified in the case of Hinduism. As one of the oldest religions in the world, it is not clearly known exactly when it originated; some estimates put it around 5000 years old. Most commonly it can be seen as a "way of life" which gives rise to many other forms of religions.
Large tribes and communities indigenous to India are closely linked to the synthesis and formation of Hindu civilization. People of East Asian roots living in the states of north eastern India and Nepal were also a part of the earliest Hindu civilization. Immigration and settlement of people from Central Asia and people of Indo-Greek heritage have brought their own influence on Hindu society.
The roots of Hinduism in southern India, and among tribal and indigenous communities is just as ancient and fundamentally contributive to the foundations of the religious and philosophical system.
Ancient Hindu kingdoms arose and spread the religion and traditions across Southeast Asia, particularly Thailand, Nepal, Burma, Malaysia, Indonesia, Cambodia, Laos, Philippines, and what is now central Vietnam. A form of Hinduism particularly different from Indian roots and traditions is practiced in Bali, Indonesia, where Hindus form 90% of the population. Indian migrants have taken Hinduism and Hindu culture to South Africa, Fiji, Mauritius and other countries in and around the Indian Ocean, and in the nations of the West Indies and the Caribbean.
References.
Sources
</dl>

</doc>
<doc id="13678" url="http://en.wikipedia.org/wiki?curid=13678" title="Hernando de Alarcón">
Hernando de Alarcón

Hernando de Alarcón, a Spanish navigator of the 16th century, born in Trujillo, Extremadura, noted for having led an early expedition to the Baja California peninsula, meant to be coordinated with Francisco Vasquéz de Coronado's overland expedition, and for penetrating the Colorado River into California.
Little is known about Alarcón's life outside of his expedition in New Spain.
He set sail on May 9, 1540, with orders from the Spanish Viceroy Antonio de Mendoza to await at a certain point on the coast the arrival of an expedition by land under the command of Coronado. The meeting with Coronado was not effected, though Alarcón reached the appointed place and left letters, which were soon afterwards found by Melchior Diaz, another explorer.
Alarcón sailed to the head of the Gulf of California and completed the explorations begun by the Spanish explorer Francisco de Ulloa the preceding year. During this voyage Alarcón proved to his satisfaction that no open-water passage existed between the Gulf of California and the South Sea. Subsequently, on 26 September, he entered the Colorado River, which he named the "Buena Guia". He was the first European to ascend the river for a distance considerable enough to make important observations. On a second voyage, he probably proceeded past the present site of Yuma, Arizona. A map drawn by one of Alarcón's pilots is the earliest accurately detailed representation of the Gulf of California and the lower course of the Colorado River.
Alarcón is almost unique among the "conquistadores" in that he treated the Indians he met humanely, as opposed to behavior that was otherwise inhumane. Bernard de Voto, in his 1953 "Westward the Course of Empire", observed: "The Indians had an experience they were never to repeat: they were sorry to see these white men leave."
Alarcón wrote of his contact with the Yuma-speaking Indians along the Colorado. The information he compiled consisted of their practices in warfare, religion, curing and even sexual customs.

</doc>
<doc id="13679" url="http://en.wikipedia.org/wiki?curid=13679" title="Hakka cuisine">
Hakka cuisine

Hakka cuisine, or Kuhchia cuisine, is the cooking style of the Hakka people, who originated mainly from the southeastern Chinese provinces of Guangdong, Fujian, Jiangxi and Guangxi but may also be found in other parts of China and in countries with significant overseas Chinese communities. There are numerous restaurants in Hong Kong, Indonesia, Malaysia and Singapore serving Hakka cuisine. Hakka cuisine was listed in 2014 on the first Hong Kong Inventory of Intangible Cultural Heritage.
Notable dishes.
Hakka food also includes takes on other traditional Chinese dishes, just as other Chinese ethnic groups do. Some of the more notable dishes in Hakka cuisine are listed as follows:
Hakka cuisine in India.
In India and other regions with significant Indian populations, the locally known "Hakka cuisine" is actually an Indian adaptation of original Hakka dishes. This variation of Hakka cuisine is in reality, mostly Indian Chinese cuisine. It is called "Hakka cuisine" because in India many owners of restaurants that serve this cuisine are of Hakka origin. Typical dishes include 'chilli chicken' and 'Dong bei chow mein' (an Indianised version of real Dongbei cuisine), and these restaurants also serve traditional Indian dishes such as pakora. Being very popular in these areas, this style of cuisine is often mistakenly credited of being representative of Hakka cuisine in general, whereas the authentic style of Hakka cuisine is rarely known in these regions.

</doc>
<doc id="13680" url="http://en.wikipedia.org/wiki?curid=13680" title="Hunan cuisine">
Hunan cuisine

Hunan cuisine, also known as Xiang cuisine, consists of the cuisines of the Xiang River region, Dongting Lake, and western Hunan province in China. It is one of the Eight Great Traditions of Chinese cuisine and is well known for its hot spicy flavour, fresh aroma and deep colour. Common cooking techniques include stewing, frying, pot-roasting, braising, and smoking. Due to the high agricultural output of the region, ingredients for Hunan dishes are many and varied. 
History.
The history of the cooking skills employed in Hunan cuisine dates back many centuries. During the course of its history, Hunan cuisine assimilated a variety of local forms, eventually evolving into its own style. It now contains more than 4,000 dishes, such as fried chicken with Sichuan spicy sauce () and smoked pork with dried long green beans ().
Hunan cuisine consists of three primary styles:
Features.
Known for its liberal use of chili peppers, shallots and garlic, Hunan cuisine is known for being dry hot (干辣) or purely hot, as opposed to Sichuan cuisine, to which it is often compared. Sichuan cuisine is known for its distinctive mala (hot and numbing) seasoning and other complex flavour combinations, frequently employs Sichuan peppercorns along with chilies which are often dried, and utilises more dried or preserved ingredients and condiments. Hunan cuisine, on the other hand, is often spicier by pure chili content, contains a larger variety of fresh ingredients, and tends to be oilier. Another characteristic distinguishing Hunan cuisine from Sichuan cuisine is that, in general, Hunan cuisine uses smoked and cured goods in its dishes much more frequently.
Another feature of Hunan cuisine is that the menu changes with the seasons. In a hot and humid summer, a meal will usually start with cold dishes or a platter holding a selection of cold meats with chilies for opening the pores and keeping cool in the summer. In winter, a popular choice is the hot pot, thought to heat the blood in the cold months. A special hot pot called "yuanyang huoguo" () is notable for splitting the pot into two sides - a spicy one and a non-spicy one.

</doc>
<doc id="13681" url="http://en.wikipedia.org/wiki?curid=13681" title="Hyperinflation">
Hyperinflation

In economics, hyperinflation occurs when a country experiences very high and usually accelerating rates of inflation, rapidly eroding the real value of the local currency, and causing the population to minimize their holdings of the local money. The population normally switches to holding relatively stable foreign currencies. Under such conditions, the general price level within an economy increases rapidly as the official currency quickly loses real value. The value of economic items remains relatively more stable in terms of foreign currencies. However, relative prices do change over the course of the hyperinflationary period; for instance food prices in Germany during 1913–1923 rose 43% less than those of clothing, whereas they increased 80 times more than rents and 24 times more than the price index of shares.
Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices and in the supply of money, and the nominal cost of goods. But typically the general price level rises even more rapidly than the money supply since people try to get rid of the devaluing money as quickly as possible. The real stock of money, that is the amount of circulating money divided by the price level, decreases.
Hyperinflations are usually caused by large persistent government deficits financed primarily by money creation (rather than taxation or borrowing). As such, hyperinflation is often associated with wars, their aftermath, sociopolitical upheavals, or other crises that make it difficult for the government to tax the population. A sharp decrease in real tax revenue coupled with a strong need to maintain the status quo, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.
Definition.
In 1956, Phillip Cagan wrote "The Monetary Dynamics of Hyperinflation", the book often regarded as the first serious study of hyperinflation and its effects (though "The Economics of Inflation" by C. Bresciani-Turroni on the German hyperinflation was published in Italian in 1931). In his book, Cagan defined a hyperinflationary episode as starting in the month that the monthly inflation rate exceeds 50%, and as ending when the monthly inflation rate drops below 50% and stays that way for at least a year. Economists usually follow Cagan’s description that hyperinflation occurs when the monthly inflation rate exceeds 50%.
The International Accounting Standards Board has issued guidance on accounting rules in a hyperinflationary environment. It does not establish an absolute rule on when hyperinflation arises. Instead, it lists factors that indicate the existence of hyperinflation:
Causes.
There are a number of theories on the causes of high and/or hyper inflation. But nearly all hyperinflations have been caused by government budget deficits financed by money creation. After an analysis of 29 hyperinflations (following Cagan's definition) Bernholz concludes that at least 25 of them have been caused in this way. Moreover, a necessary condition for hyperinflation has been the existence of fiat money not convertible at a fixed parity into gold or silver. This is suggested by the fact that most known hyperinflations in history with some exceptions, such as the French hyperinflation of 1789-1796, occurred after the break-down of the gold standard during World War I. The French hyperinflation also took place after the introduction of an nonconvertible paper money, the assignats.
Supply shocks.
This theory, based on historical analysis, claims that past hyperinflations were caused by some sort of extreme negative supply shock, often but not always associated with wars, the breakdown of the communist system or natural disasters.
Money supply.
This theory claims that hyperinflation occurs when there is a continuing (and often accelerating) rapid increase in the amount of money that is not supported by a corresponding growth in the output of goods and services.
The price increases that result from the rapid money creation creates a vicious circle, requiring ever growing amounts of new money creation to fund government deficits. Hence both monetary inflation and price inflation proceed at a rapid pace. Such rapidly increasing prices cause widespread unwillingness of the local population to hold the local currency as it rapidly loses its buying power. Instead they quickly spend any money they receive, which increases the velocity of money flow; this in turn causes further acceleration in prices. This means that the increase in the price level is greater than that of the money supply. The real stock of money, M/P, decreases. Here M refers to the money stock and P to the price level.
This results in an imbalance between the supply and demand for the money (including currency and bank deposits), causing rapid inflation. Very high inflation rates can result in a loss of confidence in the currency, similar to a bank run. Usually, the excessive money supply growth results from the government being either unable or unwilling to fully finance the government budget through taxation or borrowing, and instead it finances the government budget deficit through the printing of money.
Governments have sometimes resorted to excessively loose monetary policy, as it allows a government to devalue its debts and reduce (or avoid) a tax increase. Inflation is effectively a regressive tax on the users of money, but less overt than levied taxes and is therefore harder to understand by ordinary citizens. Inflation can obscure quantitative assessments of the true cost of living, as published price indices only look at data in retrospect, so may increase only months later. Monetary inflation can become hyperinflation if monetary authorities fail to fund increasing government expenses from taxes, government debt, cost cutting, or by other means, because either
Theories of hyperinflation generally look for a relationship between seigniorage and the inflation tax. In both Cagan's model and the neo-classical models, a tipping point occurs when the increase in money supply or the drop in the monetary base makes it impossible for a government to improve its financial position. Thus when fiat money is printed, government obligations that are not denominated in money increase in cost by more than the value of the money created.
From this, it might be wondered why any rational government would engage in actions that cause or continue hyperinflation. One reason for such actions is that often the alternative to hyperinflation is either depression or military defeat. The root cause is a matter of more dispute. In both classical economics and monetarism, it is always the result of the monetary authority irresponsibly borrowing money to pay all its expenses. These models focus on the unrestrained seigniorage of the monetary authority, and the gains from the inflation tax.
In neo-classical economic theory, hyperinflation is rooted in a deterioration of the monetary base, that is the confidence that there is a store of value which the currency will be able to command later. In this model, the perceived risk of holding currency rises dramatically, and sellers demand increasingly high premiums to accept the currency. This in turn leads to a greater fear that the currency will collapse, causing even higher premiums. One example of this is during periods of warfare, civil war, or intense internal conflict of other kinds: governments need to do whatever is necessary to continue fighting, since the alternative is defeat. Expenses cannot be cut significantly since the main outlay is armaments. Further, a civil war may make it difficult to raise taxes or to collect existing taxes. While in peacetime the deficit is financed by selling bonds, during a war it is typically difficult and expensive to borrow, especially if the war is going poorly for the government in question. The banking authorities, whether central or not, "monetize" the deficit, printing money to pay for the government's efforts to survive. The hyperinflation under the Chinese Nationalists from 1939 to 1945 is a classic example of a government printing money to pay civil war costs. By the end, currency was flown in over the Himalayas, and then old currency was flown out to be destroyed.
Hyperinflation is regarded as a complex phenomenon and one explanation may not be applicable to all cases. However, in both of these models, whether loss of confidence comes first, or central bank seigniorage, the other phase is ignited. In the case of rapid expansion of the money supply, prices rise rapidly in response to the increased supply of money relative to the supply of goods and services, and in the case of loss of confidence, the monetary authority responds to the risk premiums it has to pay by "running the printing presses."
Nevertheless the immense acceleration process that occurs during hyperinflation (such as during the German hyperinflation of 1922/23) still remains unclear and unpredictable. The transformation of an inflationary development into the hyperinflation has to be identified as a very complex phenomenon, which could be a further advanced research avenue of the complexity economics in conjunction with research areas like mass hysteria, bandwagon effect, social brain and mirror neurons.
Models.
Since hyperinflation is visible as a monetary effect, models of hyperinflation center on the demand for money. Economists see both a rapid increase in the money supply and an increase in the velocity of money if the (monetary) inflating is not stopped. Either one, or both of these together are the root causes of inflation and hyperinflation. A dramatic increase in the velocity of money as the cause of hyperinflation is central to the "crisis of confidence" model of hyperinflation, where the risk premium that sellers demand for the paper currency over the nominal value grows rapidly. The second theory is that there is first a radical increase in the amount of circulating medium, which can be called the "monetary model" of hyperinflation. In either model, the second effect then follows from the first—either too little confidence forcing an increase in the money supply, or too much money destroying confidence.
In the "confidence model", some event, or series of events, such as defeats in battle, or a run on stocks of the specie which back a currency, removes the belief that the authority issuing the money will remain solvent—whether a bank or a government. Because people do not want to hold notes which may become valueless, they want to spend them. Sellers, realizing that there is a higher risk for the currency, demand a greater and greater premium over the original value. Under this model, the method of ending hyperinflation is to change the backing of the currency, often by issuing a completely new one. War is one commonly cited cause of crisis of confidence, particularly losing in a war, as occurred during Napoleonic Vienna, and capital flight, sometimes because of "contagion" is another. In this view, the increase in the circulating medium is the result of the government attempting to buy time without coming to terms with the root cause of the lack of confidence itself.
In the "monetary model", hyperinflation is a positive feedback cycle of rapid monetary expansion. It has the same cause as all other inflation: money-issuing bodies, central or otherwise, produce currency to pay spiralling costs, often from lax fiscal policy, or the mounting costs of warfare. When businesspeople perceive that the issuer is committed to a policy of rapid currency expansion, they mark up prices to cover the expected decay in the currency's value. The issuer must then accelerate its expansion to cover these prices, which pushes the currency value down even faster than before. According to this model the issuer cannot "win" and the only solution is to abruptly stop expanding the currency. Unfortunately, the end of expansion can cause a severe financial shock to those using the currency as expectations are suddenly adjusted. This policy, combined with reductions of pensions, wages, and government outlays, formed part of the Washington consensus of the 1990s.
Whatever the cause, hyperinflation involves both the supply and velocity of money. Which comes first is a matter of debate, and there may be no universal story that applies to all cases. But once the hyperinflation is established, the pattern of increasing the money stock, by whichever agencies are allowed to do so, is universal. Because this practice increases the supply of currency without any matching increase in demand for it, the price of the currency, that is the exchange rate, naturally falls relative to other currencies. Inflation becomes hyperinflation when the increase in money supply turns specific areas of pricing power into a general frenzy of spending quickly before money becomes worthless. The purchasing power of the currency drops so rapidly that holding cash for even a day is an unacceptable loss of purchasing power. As a result, no one holds currency, which increases the velocity of money, and worsens the crisis.
Because rapidly rising prices undermine the role of money as a store of value, people try to spend it on real goods or services as quickly as possible. Thus, the monetary model predicts that the velocity of money will increase as a result of an excessive increase in the money supply. At the point when money velocity and prices rapidly accelerate in a vicious circle, hyperinflation is out of control, because ordinary policy mechanisms, such as increasing reserve requirements, raising interest rates, or cutting government spending will be ineffective and be responded to by shifting away from the rapidly devalued money and towards other means of exchange.
During a period of hyperinflation, bank runs, loans for 24-hour periods, switching to alternate currencies, the return to use of gold or silver or even barter become common. Many of the people who hoard gold today expect hyperinflation, and are hedging against it by holding specie. There may also be extensive capital flight or flight to a "hard" currency such as the US dollar. This is sometimes met with capital controls, an idea which has swung from standard, to anathema, and back into semi-respectability. All of this constitutes an economy which is operating in an "abnormal" way, which may lead to decreases in real production. If so, that intensifies the hyperinflation, since it means that the amount of goods in "too much money chasing too few goods" formulation is also reduced. This is also part of the vicious circle of hyperinflation.
Once the vicious circle of hyperinflation has been ignited, dramatic policy means are almost always required. Simply raising interest rates is insufficient. Bolivia, for example, underwent a period of hyperinflation in 1985, where prices increased 12,000% in the space of less than a year. The government raised the price of gasoline, which it had been selling at a huge loss to quiet popular discontent, and the hyperinflation came to a halt almost immediately, since it was able to bring in hard currency by selling its oil abroad. The crisis of confidence ended, and people returned deposits to banks. The German hyperinflation (1919–November 1923) was ended by producing a currency based on assets loaned against by banks, called the Rentenmark. Hyperinflation often ends when a civil conflict ends with one side winning.
Although wage and price controls are sometimes used to control or prevent inflation, no episode of hyperinflation has been ended by the use of price controls alone, because price controls that force merchants to sell at prices far below their restocking costs result in shortages that cause prices to rise still further.
Nobel prize winner Milton Friedman said "We economists don't know much, but we do know how to create a shortage. If you want to create a shortage of tomatoes, for example, just pass a law that retailers can't sell tomatoes for more than two cents per pound. Instantly you'll have a tomato shortage. It's the same with oil or gas."
Effects.
Hyperinflation effectively wipes out the purchasing power of private and public savings, distorts the economy in favor of the hoarding of real assets, causes the monetary base, whether specie or hard currency, to flee the country, and makes the afflicted area anathema to investment.
But one of the most important characteristics of hyperinflation is the accelerating substitution of the inflating money by stable money, gold and silver in former times, but relatively stable foreign currencies after the breakdown of the gold or silver standards (Thiers' Law). If inflation is high enough all government regulations like heavy penalties and fines often combined with exchange controls cannot prevent this currency substitution. As a consequence the inflating currency is usually heavily undervalued compared to stable foreign money and in terms of purchasing power parity. As a consequence foreigners can live cheaply and buy at cheap prices in the countries hit by high inflation. It follows that governments who do not succeed to engineer in time a successful currency reform have finally to legalize the stable foreign currencies (or formerly gold and silver) which is threatening to fully substitute the inflating money. Otherwise their tax revenues including the inflation tax will approach zero. The last hyperinflation where this process could be observed, took place in Zimbabwe in the first decade of the 21st century. In this case the local money was mainly driven out by the US dollar and the South African rand.
Enactment of price controls to prevent discounting the value of paper money relative to gold, silver, hard currency, or commodities, fail to force acceptance of a paper money which lacks intrinsic value. If the entity responsible for printing a currency promotes excessive money printing, with other factors contributing a reinforcing effect, hyperinflation usually continues. Hyperinflation is generally associated with paper money, which can easily be used to increase the money supply: add more zeros to the plates and print, or even stamp old notes with new numbers. Historically, there have been numerous episodes of hyperinflation in various countries followed by a return to "hard money". Older economies would revert to hard currency and barter when the circulating medium became excessively devalued, generally following a "run" on the store of value.
Much attention on hyperinflation centers on the effect on savers whose investment becomes worthless. Academic economists seem not to have devoted much study on the (positive) effect on debtors. This may be due to the widespread perception that consistently saving a portion of one's income in monetary investments such as bonds or interest-bearing accounts is almost always a wise policy, and usually beneficial to the society of the savers. By contrast, incurring large or long-term debts (though sometimes unavoidable) is viewed as often resulting from irresponsibility or self-indulgence. Interest rate changes often cannot keep up with hyperinflation or even high inflation, certainly with contractually fixed interest rates. (For example, in the 1970s in the United Kingdom inflation reached 25% per annum, yet interest rates did not rise above 15%—and then only briefly—and many fixed interest rate loans existed). Contractually there is often no bar to a debtor clearing his long term debt with "hyperinflated-cash" nor could a lender simply somehow suspend the loan. "Early redemption penalties" were (and still are) often based on a penalty of "x" months of interest/payment; again no real bar to paying off what had been a large loan. In interwar Germany, for example, much private and corporate debt was effectively wiped out—certainly for those holding fixed interest rate loans.
Aftermath.
Hyperinflation is ended with drastic remedies, such as imposing the shock therapy of slashing government expenditures or altering the currency basis. One form this may take is dollarization, the use of a foreign currency (not necessarily the U.S. dollar) as a national unit of currency. An example was dollarization in Ecuador, initiated in September 2000 in response to a 75% loss of value of the Ecuadorian sucre in early 2000. But usually the "dollarization" takes place in spite of all efforts of the government to prevent it by exchange controls, heavy fines and penalties. The government has thus to try to engineer a successful currency reform stabilizing the value of the money. If it does not succeed with this reform the substitution of the inflating by stable money goes on. Thus it is not surprising that there exist at least seven historical cases in which the good (foreign) money did fully drive out the use of the inflating currency. In the end the government had to legalize the former, for otherwise its revenues would have fallen to zero.
Hyperinflation has always been a traumatic experience for the area which suffers it, and the next policy regime almost always enacts policies to prevent its recurrence. Often this means making the central bank very aggressive about maintaining price stability, as was the case with the German Bundesbank or moving to some hard basis of currency such as a currency board. Many governments have enacted extremely stiff wage and price controls in the wake of hyperinflation but this does not prevent further inflating of the money supply by its central bank, and always leads to widespread shortages of consumer goods if the controls are rigidly enforced.
Currency.
In countries experiencing hyperinflation, the central bank often prints money in larger and larger denominations as the smaller denomination notes become worthless. This can result in the production of some interesting banknotes, including those denominated in amounts of 1,000,000,000 or more.
One way to avoid the use of large numbers is by declaring a new unit of currency (an example being, instead of 10,000,000,000 dollars, a bank might set 1 new dollar = 1,000,000,000 old dollars, so the new note would read "10 new dollars.") An example of this would be Turkey's revaluation of the Lira on 1 January 2005, when the old Turkish lira (TRL) was converted to the New Turkish lira (TRY) at a rate of 1,000,000 old to 1 new Turkish Lira. While this does not lessen the actual value of a currency, it is called redenomination or revaluation and also happens over time in countries with standard inflation levels. During hyperinflation, currency inflation happens so quickly that bills reach large numbers before revaluation.
Some banknotes were stamped to indicate changes of denomination. This is because it would take too long to print new notes. By the time new notes were printed, they would be obsolete (that is, they would be of too low a denomination to be useful).
Metallic coins were rapid casualties of hyperinflation, as the scrap value of metal enormously exceeded the face value. Massive amounts of coinage were melted down, usually illicitly, and exported for hard currency.
Governments will often try to disguise the true rate of inflation through a variety of techniques. None of these actions address the root causes of inflation and if discovered, they tend to further undermine trust in the currency, causing further increases in inflation. Price controls will generally result in shortages and hoarding and extremely high demand for the controlled goods, causing disruptions of supply chains. Products available to consumers may diminish or disappear as businesses no longer find it sufficiently profitable (or may be operating at a loss) to continue producing and/or distributing such goods at the legal prices, further exacerbating the shortages.
There are also issues with computerized money-handling systems. In Zimbabwe, during the hyperinflation of the Zimbabwe dollar, many automated teller machines and payment card machines struggled with arithmetic overflow errors as customers required many billions and trillions of dollars at one time.
Hyperinflationary episodes.
Angola.
Angola experienced high inflation from 1991 to 1995. It was a result of exchange restrictions following the introduction of the novo kwanza (AON) to replace the original kwanza (AOK) in 1990. At the first months of 1991, the highest denomination was 50 000 AON. By 1994, the highest denomination was 500 000 kwanzas. In the 1995 currency reform, the readjusted kwanza (AOR) replaced the novo kwanza at the ratio of 1 000 AON to 1 AOR, but hyperinflation continued as further denominations of up to 5 000 000 AOR were issued. In the 1999 currency reform, the kwanza (AOA) was reintroduced at the ratio of 1 million AOR to 1 AOA. Currently, the highest denomination banknote is 2 000 AOA and the overall impact of hyperinflation was 1 AOA = 1 billion AOK.
Argentina.
Argentina went through steady inflation from 1975 to 1991. Democratically elected on December 10, 1983, President Alfonsin inherited a foreign debt crisis and record budget deficits, resulting in US$43 billion foreign debt, with Argentina's entire trade surplus going to debt service. The situation worsened markedly on "Black Tuesday," February 7, 1989, when the U.S. dollar gained around 40% against the austral. As a result, the World Bank recalled a large tranche of loans, sending the austral into the void and igniting hyperinflation by Election Day, May 14, 1989. By June 1989 social unrest was widespread and included instances of rioting and looting of supermarkets. At the beginning of 1975, the highest denomination was 1,000 pesos. In late 1976, the highest denomination was 5,000 pesos. In early 1979, the highest denomination was 10,000 pesos. By the end of 1981, the highest denomination was 1,000,000 pesos. In the 1983 currency reform, 1 peso argentino was exchanged for 10,000 pesos. In the 1985 currency reform, 1 austral was exchanged for 1,000 pesos argentinos. In the 1992 currency reform, 1 new peso was exchanged for 10,000 australes. The overall impact of hyperinflation: 1 (1992) peso = 100,000,000,000 pre-1983 pesos. Annual inflation hit 12,000% in 1989.
Armenia.
Armenia experienced high inflation and hyperinflation from January 1992 – December 1994. Its first episode of hyperinflation was due to the use of the Russian ruble after the dissolution of the Soviet Union. The second episode was due to an unstable Armenian dram.
Austria.
In 1922, inflation in Austria reached 1426%, and from 1914 to January 1923, the consumer price index rose by a factor of 11836, with the highest banknote in denominations of 500,000 krones.
Political ineptitude in Post WWI Austria comprised political expedience and Socialist benevolence. Thus, essentially all State enterprises ran at a loss and the number of State employees in Vienna, the capital of the post WWI republic was greater than that of the earlier monarchy, even though they served a population base nearly eight times smaller.
Observing the Austrian response to developing hyperinflation, fueled by selfishness and political ineptitude, including the hoarding of food and the speculation in foreign currencies, Owen S. Phillpotts, the Commercial Secretary at the British Legation in Vienna wrote: “The Austrians are like men on a ship who cannot manage it, and are continually signalling for help. While waiting, however, most of them begin to cut rafts, each for himself, out of the sides and decks. The ship has not yet sunk despite the leaks so caused, and those who have acquired stores of wood in this way may use them to cook their food, while the more seamanlike look on cold and hungry. The population lack courage and energy as well as patriotism”.
Azerbaijan.
Azerbaijan experienced inflation from January 1992 until December 1994. It first circulated the Russian ruble until August 1992 when it introduced the Azerbaijani manat.
Belarus.
Belarus experienced steady inflation from 1994 to 2002. In 1993, the highest denomination was 5,000 rublei. By 1999, it was 5,000,000 rublei. In the 2000 currency reform, the ruble was replaced by the new ruble at an exchange rate of 1 new ruble = 1,000 old rublei. The highest denomination in 2008 was 100,000 rublei, equal to 100,000,000 pre-2000 rublei. New 200,000 bill was introduced in 2012.
Bolivia.
Bolivia experienced its worst inflation between 1984 and 1986. Hyperinflation in Bolivia lasted from April 1984 to September 1985 and was the worst in the history of Latin America. The coalition government of Siles Suazo faced a perfect storm of financial and political challenges upon assuming power in October 1982. With GNP declining (real GNP fell 6.6% in 1982), historically high interest rates and declining commodity prices, international debt became unserviceable and access to international capital markets dried up. Additionally, the coalition government was effectively pressured by the left for social spending while lacking the political base to enact new taxes. Accelerating inflation caused a virtual collapse of the tax system, with taxes declining from 9% of GNP in 1981 to about 1.3% of GNP in the first half of 1985. Before 1984, the highest denomination was 1,000 Bolivian pesos. By 1985, the highest denomination was 10 million Bolivian pesos. In 1985, a Bolivian note for 1 million pesos was worth 55 cents in US dollars, one-thousandth of its exchange value of $5,000 less than three years previously. In the 1987 currency reform, the Peso Boliviano was replaced by the Boliviano at a rate of 1,000,000:1.
Bosnia and Herzegovina.
Bosnia and Herzegovina went through its worst inflation in 1992. In 1992, the highest denomination was 1,000 dinara. By 1993, the highest denomination was 100,000,000 dinara. In the Republika Srpska, the highest denomination was 10,000 dinara in 1992 and 10,000,000,000 dinara in 1993. 50,000,000,000 dinara notes were also printed in 1993 but never issued.
Brazil.
From 1967–1994, the base currency unit was shifted seven times to adjust for inflation in the final years of the Brazilian military dictatorship era. A 1967 cruzeiro was, in 1994, worth less than one trillionth of a US cent, after adjusting for multiple devaluations and note changes. In that same year, inflation reached a record 2,075.8%. A new currency called real was adopted in 1994, and hyperinflation was eventually brought under control. The "real" was also the currency in use until 1942; 1 (current) real is the equivalent of 2,750,000,000,000,000,000 of Brazil's first currency (called "réis" in Portuguese).
Bulgaria.
Bulgaria experienced its highest levels of inflation in February 1997. However, it had high inflation for many years before and after this episode. In order to curb inflation, Bulgaria implemented a currency board in July of that year. Inflation subsequently dropped almost immediately.
Starting 5 June 1999, Bulgarian currency was denominated having 1 new lev (BGN) equal 1000 old leva (BGL).
Chile.
In 1973, Chile had experienced hyperinflation that had hit 700 percent, at a time when the country, under high protectionist barriers, had no foreign reserves, and GDP was falling. Despite very severe government cuts and attempted monetary targeting under the Pinochet dictatorship inflation remained extremely high throughout the 1970s.
China.
As the first user of fiat currency, China has had an early history of troubles caused by hyperinflation. The Yuan Dynasty printed huge amounts of fiat paper money to fund their wars, and the resulting hyperinflation, coupled with other factors, led to its demise at the hands of a revolution. The Republic of China went through the worst inflation 1948–49. In 1947, the highest denomination was 50,000 yuan. By mid-1948, the highest denomination was 180,000,000 yuan. The 1948 currency reform replaced the yuan by the gold yuan at an exchange rate of 1 gold yuan = 3,000,000 yuan. In less than a year, the highest denomination was 10,000,000 gold yuan. In the final days of the civil war, the Silver Yuan was briefly introduced at the rate of 500,000,000 Gold Yuan. Meanwhile the highest denomination issued by a regional bank was 6,000,000,000 yuan (issued by Xinjiang Provincial Bank in 1949). After the renminbi was instituted by the new communist government, hyperinflation ceased with a revaluation of 1:10,000 old Renminbi in 1955. The overall impact of inflation was 1 Renminbi = 15,000,000,000,000,000,000 pre-1948 yuan.
Estonia.
Estonia experienced hyperinflation as a result of using the Russian ruble after the fall of the Soviet Union. However, it was the first post-Soviet union country to implement a currency reform by installing a currency board. It began circulation of the Estonian kroon in April 1992, ridding the country of high inflation.
France.
During the French Revolution and first Republic, the National Assembly issued bonds, some backed by seized church property, called Assignats. Napoleon replaced them with the franc in 1803, at which time the assignats were basically worthless.
Stephen D. Dillaye pointed out that one of the reasons for the failure was massive counterfeiting of the paper currency, “the Assignats” – largely through London – where, according to Dillaye: “Seventeen manufacturing establishments were in full operation in London, with a force of four hundred men devoted to the production of false and forged Assignats.” 
Free City of Danzig.
The Free City of Danzig went through its worst inflation in 1923. In 1922, the highest denomination was 1,000 Mark. By 1923, the highest denomination was 10,000,000,000 Mark.
Georgia.
Georgia went through its worst inflation in 1994. In 1993, the highest denomination was 100,000 coupons [kuponi]. By 1994, the highest denomination was 1,000,000 coupons. In the 1995 currency reform, a new currency, the lari, was introduced with 1 lari exchanged for 1,000,000 coupons.
Germany (Weimar Republic).
By November 1922 the gold value of money in circulation fell from £300 million before WWI to £20 million. The Reichsbank responded by the unlimited printing of notes, thereby accelerating the devaluation of the mark. In his report to London, Lord D'Abernon wrote: "In the whole course of history, no dog has ever run after its own tail with the speed of the Reichsbank." Germany went through its worst inflation in 1923. In 1922, the highest denomination was 50,000 Mark. By 1923, the highest denomination was 100,000,000,000,000 Mark. In December 1923 the exchange rate was 4,200,000,000,000 Marks to 1 US dollar. In 1923, the rate of inflation hit 3.25 × 106 percent per month (prices double every two days). Beginning on 20 November 1923, 1,000,000,000,000 old Marks were exchanged for 1 Rentenmark so that 4.2 Rentenmarks were worth 1 US dollar, exactly the same rate the Mark had in 1914.
Greece.
Greece went through its worst inflation in 1944. In 1942, the highest denomination was 50,000 drachmai. By 1944, the highest denomination was 100,000,000,000 drachmai. In the 1944 currency reform, 1 new drachma was exchanged for 50,000,000,000 drachmai. Another currency reform in 1953 replaced the drachma at an exchange rate of 1 new drachma = 1,000 old drachmai. The overall impact of hyperinflation: 1 (1953) drachma = 50,000,000,000,000 pre 1944 drachmai. The Greek monthly inflation rate reached 8.5 billion percent in October 1944.
Hungary, 1923–24.
The Treaty of Trianon and political instability between 1919 and 1924 led to a major inflation of Hungary's currency. In 1921, in an attempt to arrest Post WWI inflation, the national assembly of Hungary passed the Hegedüs reforms, including a 20% levy on bank deposits. This action precipitated a mistrust of banks by the public, especially the peasants, and resulted in a reduction savings and the amount of currency in circulation. Unable to tax adequately, the government resorted to printing money and by 1923 inflation in Hungary had reached 98% per month.
Hungary, 1945–46.
Hungary went through the worst inflation ever recorded in the world between the end of 1945 and July 1946. In 1944, the highest denomination was 1,000 pengő. By the end of 1945, it was 10,000,000 pengő. The highest denomination in mid-1946 was 100,000,000,000,000,000,000 pengő. A special currency the adópengő – or tax pengő – was created for tax and postal payments. The value of the adópengő was adjusted each day, by radio announcement. On 1 January 1946 one adópengő equaled one pengő. By late July, one adópengő equaled 2,000,000,000,000,000,000,000 or 2×1021 (2 sextillion) pengő. When the pengő was replaced in August 1946 by the forint, the total value of all Hungarian banknotes in circulation amounted to 1/1,000 of one US dollar. It is the most severe known incident of inflation recorded, peaking at 1.3 × 1016 percent per month (prices double every 15 hours). The overall impact of hyperinflation: On 18 August 1946, 400,000,000,000,000,000,000,000,000,000 or 4×1029 (four hundred quadrilliard on the long scale used in Hungary; four hundred octillion on short scale) pengő became 1 forint.
Kazakhstan.
Kazakhstan experienced two episodes of hyperinflation. The first was in 1992, as most post-Soviet Union countries experienced. The second episode was in November 1993 with the implementation of the Kazakhstani tenge.
Kyrgyzstan.
Kyrgyzstan experienced hyperinflation in Jan. 1992 following the dissolution of the Soviet Union.
Serbian Krajina.
The Republic of Serbian Krajina went through its worst inflation in 1993. In 1992, the highest denomination was 50,000 dinara. By 1993, the highest denomination was 50,000,000,000 dinara. Note that this unrecognized country was reincorporated into Croatia in 1995.
North Korea.
North Korea most likely experienced hyperinflation from December 2009 to mid-January 2011. Based on the price of rice, North Korea's hyperinflation peaked in mid-January 2010, but according to black market exchange-rate data, and calculations base on purchasing power parity, North Korea experienced its peak month of inflation in early March 2010. However, this data is unofficial and therefore must be treated with a degree of caution.
Nicaragua.
Nicaragua went through the worst inflation from 1986 to 1990. From 1943 to April 1971, one US dollar equalled 7 córdobas. From April 1971 – early 1978, one US dollar was worth 10 córdobas. In early 1986, the highest denomination was 10,000 córdobas. By 1987, it was 1,000,000 córdobas. In the 1988 currency reform, 1 new córdoba was exchanged for 1,000 old córdobas. The highest denomination in 1990 was 100,000,000 new córdobas. In the 1991 currency reform, 1 new "córdoba oro" was exchanged for 5,000,000 old "córdobas". The overall impact of hyperinflation: 1 (1991) córdoba = 5,000,000,000 pre-1988 córdobas.
Peru.
Peru experienced its worst inflation from 1988–1990. In the 1985 currency reform, 1 inti was exchanged for 1,000 soles. In 1986, the highest denomination was 1,000 intis. But in September 1988, monthly inflation went to 114%. In August 1990, monthly inflation was 397%. The highest denomination was 5,000,000 intis by 1991. In the 1991 currency reform, 1 nuevo sol was exchanged for 1,000,000 intis. The overall impact of hyperinflation: 1 nuevo sol = 1,000,000,000 (old) soles.
Philippines.
The Japanese government occupying the Philippines during the World War II issued fiat currencies for general circulation. The Japanese-sponsored Second Philippine Republic government led by Jose P. Laurel at the same time outlawed possession of other currencies, most especially "guerilla money." The fiat money was dubbed "Mickey Mouse Money" because it is similar to play money and is next to worthless. Survivors of the war often tell tales of bringing suitcase or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with Japanese-issued bills. In the early times, 75 Mickey Mouse pesos could buy one duck egg. In 1944, a box of matches cost more than 100 Mickey Mouse pesos.
In 1942, the highest denomination available was 10 pesos. Before the end of the war, because of inflation, the Japanese government was forced to issue 100, 500 and 1000 peso notes.
Poland, 1923–1924.
After Poland's independence in 1918, the country soon began experiencing extreme inflation. By 1921, prices had already risen 251 times above those of 1914, but in the following three years they rose by 988,223% with a peak rate in late 1923 of prices doubling every nineteen and a half days. At independence there was 8 marek per US dollar, but by 1923 the exchange rate was 6,375,000 marek (mkp) for 1 US dollar. The highest denomination was 10,000,000 mkp. In the 1924 currency reform there was a new currency introduced: 1 zloty = 1,800,000 mkp.
Poland, 1989–1990.
Poland experienced a second hyperinflation between 1989 and 1990. Developing inflation and shortages in Poland in the eighties were exacerbated by excessive and growing subsidies to state sector production. In this environment, in August 1989, the government "marketized" agriculture, resulting in the liberalization of retail food prices and profit margins and an immediate 40% jump in prices from July to August. The acceleration of inflation was fueled by wage and income indexation won by the Solidarity-led opposition and led to hyperinflation by October 1989. The highest denomination in 1989 was 200,000 zlotych. It was 1,000,000 zlotych in 1991 and 2,000,000 zlotych in 1992; the exchange rate was 9500 zlotych for 1 US dollar in January 1990 and 19600 zlotych at the end of August 1992. In the 1994 currency reform, 1 new zloty was exchanged for 10,000 old zlotych and 1 US$ exchange rate was ca. 2.5 zlotych (new).
Republika Srpska.
Republika Srpska was a breakaway region of Bosnia. As with Krajina, it pegged its currency, the Republika Srpska dinar, to that of Yugoslavia. Their bills were almost the same as Krajina's, but they issued fewer and did not issue currency after 1993.
Soviet Union / Russian Federation.
Between 1921 and 1922, inflation in the Soviet Union reached 213%.
In 1992, the first year of the post-Soviet economic reform, inflation levels went up to 2,520%. In 1993, the annual rate was 840%, and in 1994, 224%. The ruble devalued from about 40 r/$ in 1991 to about 5,000 r/$ in late 1997. In 1998, a denominated ruble was introduced at the exchange rate of 1 new ruble = 1,000 pre-1998 rubles. In the second half of the same year, ruble fell to about 30 r/$ as a result of financial crisis.
Taiwan.
As the Chinese Civil War reached its peak, Taiwan also suffered from the hyperinflation that ravaged China in the late 1940s. The largest denomination issued was a 1,000,000 Dollar Bearer's Cheque. Inflation was finally brought under control at introduction of New Taiwan Dollar in 15 June 1949 at rate of 40,000 old Dollar = 1 New Dollar.
Tajikistan.
Tajikistan experienced two episodes of hyperinflation. One episode following the break-up of the Soviet Union that lasted longer than many of its neighboring countries. The second episode was in 1995 following the circulation of their new ruble.
Turkmenistan.
Turkmenistan experienced two episodes of hyperinflation as well. Similar to Takjikistan it experienced one episode in 1992 and one in 1995 following the introduction of its currency, the manat.
Ukraine.
Ukraine experienced its worst inflation between 1992 and 1994. In 1992, the Ukrainian karbovanets was introduced, which was exchanged with the defunct Soviet ruble at a rate of 1 UAK = 1 SUR. Before 1993, the highest denomination was 1,000 karbovantsiv. By 1995, it was 1,000,000 karbovantsiv. In 1996, during the transition to the Hryvnya and the subsequent phase out of the karbovanets, the exchange rate was 100,000 UAK = 1 UAH. By some estimates, inflation for the entire calendar year of 1993 was 10,000% or higher, with retail prices reaching over 100 times their pre-1993 level by the end of the year.
 In 1996, it was taken out of circulation, and was replaced by the Hryvnya at an exchange rate of 100,000 karbovantsi = 1 Hryvnya (approx. USD 0.50 at that time, about USD 0.20 as of 2007).
Uzbekistan.
Like many of the post-Soviet Union countries, Uzbekistan also experienced hyperinflation following the dissolution of the Soviet Union in 1992.
Yugoslavia.
Yugoslavia went through a period of hyperinflation and subsequent currency reforms from 1989–1994. One of several regional conflicts accompanying the dissolution of Yugoslavia was the Bosnian War (1992–1995). The Belgrade government of Slobodan Milošević backed ethnic Serbian secessionist forces in the conflict, resulting in a United Nations boycott of Yugoslavia. The UN boycott collapsed an economy already weakened by regional war, with the projected monthly inflation rate accelerating to one million percent by December, 1993 (prices double every 2.3 days). The highest denomination in 1988 was 50,000 dinars. By 1989 it was 2,000,000 dinars. In the 1990 currency reform, 1 new dinar was exchanged for 10,000 old dinars. In the 1992 currency reform, 1 new dinar was exchanged for 10 old dinars. The highest denomination in 1992 was 50,000 dinars. By 1993, it was 10,000,000,000 dinars. In the 1993 currency reform, 1 new dinar was exchanged for 1,000,000 old dinars. However, before the year was over, the highest denomination was 500,000,000,000 dinars. In the 1994 currency reform, 1 new dinar was exchanged for 1,000,000,000 old dinars. In another currency reform a month later, 1 novi dinar was exchanged for 13 million dinars (1 novi dinar = 1 German mark at the time of exchange). The overall impact of hyperinflation: 1 novi dinar = 1 × 1027~1.3 × 1027 pre 1990 dinars. Yugoslavia's rate of inflation hit 5 × 1015 percent cumulative inflation over the time period 1 October 1993 and 24 January 1994. 
Zaire (now the Democratic Republic of the Congo).
Zaire went through a period of inflation between 1989 and 1996. In 1988, the highest denomination was 5,000 zaires. By 1992, it was 5,000,000 zaires. In the 1993 currency reform, 1 nouveau zaire was exchanged for 3,000,000 old zaires. The highest denomination in 1996 was 1,000,000 nouveaux zaires. In 1997, Zaire was renamed the Congo Democratic Republic and changed its currency to francs. 1 franc was exchanged for 100,000 nouveaux zaires. One post-1997 franc was equivalent to 3 × 1011 pre 1989 zaires.
Zimbabwe.
Hyperinflation in Zimbabwe was one of the few instances that resulted in the abandonment of the local currency. At independence in 1980, the Zimbabwe dollar (ZWD) was worth about USD 1.25. Afterwards, however, rampant inflation and the collapse of the economy severely devalued the currency. Inflation was steady before Robert Mugabe in 1998 began a program of land reforms that primarily focused on returning land taken from black natives during colonialization which led to disrupted food production and caused revenues from exports of food to plummet and foreign direct investment declined. The result was that to pay its expenditures Mugabe's government and Gideon Gono's Reserve Bank printed more and more notes with higher face values.
Hyperinflation began early in the 21st-century, reaching 624% in 2004. It fell back to low triple digits before surging to a new high of 1,730% in 2006. The Reserve Bank of Zimbabwe revalued on 1 August 2006 at a ratio of 1 000 ZWD to each second dollar (ZWN), but year-to-year inflation rose by June 2007 to 11,000% (versus an earlier estimate of 9,000%). Larger denominations were progressively issued:
Inflation by 16 July officially surged to 2,200,000% with some analysts estimating figures surpassing 9,000,000 percent. As of 22 July 2008 the value of the ZWN fell to approximately 688 billion per 1 USD, or 688 trillion pre-August 2006 Zimbabwean dollars.
On 1 August 2008, the Zimbabwe dollar was redenominated at the ratio of 1010 ZWN to each third dollar (ZWR). On 19 August 2008, official figures announced for June estimated the inflation over 11,250,000%. Zimbabwe's annual inflation was 231,000,000% in July (prices doubling every 17.3 days). By October 2008 Zimbawe was mired in hyperinflation with wages falling far behind inflation. In this dysfunctional economy hospitals and schools had chronic staffing problems, because many nurses and teachers could not afford bus fare to work. Most of the capital of Harare was without water because the authorities had stopped paying the bills to buy and transport the treatment chemicals. Desperate for foreign currency to keep the government functioning, Zimbabwe's central bank governor, Gideon Gono, sent runners into the streets with suitcases of Zimbabwean dollars to buy up American dollars and South African rand. For periods after July 2008, no official inflation statistics were released. Prof. Steve H. Hanke overcame the problem by estimating inflation rates after July 2008 and publishing the Hanke Hyperinflation Index for Zimbabwe. Prof. Hanke's HHIZ measure indicated that the inflation peaked at an annual rate of 89.7 sextillion percent (89,700,000,000,000,000,000,000%) in mid-November 2008. The peak monthly rate was 79.6 billion percent, which is equivalent to a 98% daily rate, or around percent yearly rate. At that rate, prices were doubling every 24.7 hours. Note that many of these figures should be considered mostly theoretic, since the hyperinflation did not proceed at that rate a whole year.
At its November 2008 peak, Zimbabwe's rate of inflation approached, but failed to surpass, Hungary's July 1946 world record. On 2 February 2009, the dollar was redenominated for the fourth time at the ratio of 1012 ZWR to 1 ZWL, only three weeks after the $100 trillion banknote was issued on 16 January, but hyperinflation waned by then as official inflation rates in USD were announced and foreign transactions were legalised, and on 12 April the dollar was abandoned in favour of using only foreign currencies. The overall impact of hyperinflation was 1 ZWL = 1025 ZWD.
Examples of high inflation.
Some countries experienced very high inflation, but did not reach hyperinflation, as defined as a "monthly" inflation rate of 50%.
Iraq.
Between 1987 and 1995 the Iraqi Dinar went from an official value of 0.306 Dinars/USD (or $3.26 USD per dinar, though the black market rate is thought to have been substantially lower) to 3000 Dinars/USD due to government printing of 10s of trillions of dinars starting with a base of only 10s of billions. That equates to approximately 315% inflation per year averaged over that eight-year period.
Mexico.
In spite of increased oil prices in the late 1970s (Mexico is a producer and exporter), Mexico defaulted on its external debt in 1982. As a result, the country suffered a severe case of capital flight and several years of hyperinflation and peso devaluation. On 1 January 1993, Mexico created a new currency, the "nuevo peso" ("new peso", or MXN), which chopped three zeros off the old peso, an inflation rate of 100,000% over the several years of the crisis. (One new peso was equal to 1,000 old MXP pesos).
Roman Egypt.
In Roman Egypt, where the best documentation on pricing has survived, the price of a measure of wheat was 200 drachmae in 276 AD, and increased to more than 2,000,000 drachmae in 334 AD, roughly 1,000,000% inflation in a span of 58 years.
Although the price increased by a factor of 10,000 over 58 years, the annual rate of inflation was only 17.2% compounded.
Romania.
Romania experienced hyperinflation in the 1990s. The highest denomination in 1990 was 100 lei and in 1998 was 100,000 lei. By 2000 it was 500,000 lei. In early 2005 it was 1,000,000 lei. In July 2005 the lei was replaced by the new leu at 10,000 old lei = 1 new leu. Inflation in 2005 was 9%. In July 2005 the highest denomination became 500 leu (= 5,000,000 old lei).
Vietnam.
Vietnam went through a period of chaotic and hyperinflation in the late 1980s, with inflation peaking at 774% in 1988, after the country's "price-wage-currency" reform package led by Mr Tran Phuong, then Deputy Prime Minister, had failed bitterly. Hyperinflation also characterizes the early stage of economic renovation, usually referred to as Doi Moi, in Vietnam.
United States.
During the Revolutionary War, when the Continental Congress authorized the printing of paper currency called continental currency, the monthly inflation rate reached a peak of 47 percent in November 1779 (Bernholz 2003: 48). These notes depreciated rapidly, giving rise to the expression "not worth a continental."
One cause of the inflation was counterfeiting by the British, who ran a press on HMS "Phoenix", moored in New York Harbour. The counterfeits were advertised and sold almost for the price of the paper they were printed on.
A second close encounter occurred during the U.S. Civil War, between January 1861 and April 1865, the Lerner Commodity Price Index of leading cities in the eastern Confederacy states increased from 100 to over 9,000. As the Civil War dragged on, the Confederate dollar had less and less value, until it was almost worthless by the last few months of the war. Similarly, the Union government inflated its greenbacks, with the monthly rate peaking at 40 percent in March 1864 (Bernholz 2003: 107).
Units of inflation.
Inflation rate is usually measured in percent per year. It can also be measured in percent per month or in price doubling time.
formula_1
formula_2
formula_3
formula_4
Often, at redenominations, three zeroes are cut from the bills. It can be read from the table that if the (annual) inflation is for example 100%, it takes 3.32 years to produce one more zero on the price tags, or 3 × 3.32 = 9.96 years to produce three zeroes. Thus can one expect a redenomination to take place about 9.96 years after the currency was introduced.
Further reading.
</dl>

</doc>
<doc id="13682" url="http://en.wikipedia.org/wiki?curid=13682" title="Herbert Hoover">
Herbert Hoover

Herbert Clark Hoover (August 10, 1874 – October 20, 1964) was the 31st President of the United States (1929–1933). He was a professional mining engineer, and was raised as a Quaker. A Republican, Hoover served as head of the U.S. Food Administration during World War I, and became internationally known for humanitarian relief efforts in war-time Belgium. As the United States Secretary of Commerce in the 1920s under Presidents Warren G. Harding and Calvin Coolidge, he promoted partnerships between government and business under the rubric "economic modernization". In the presidential election of 1928, Hoover easily won the Republican nomination, despite having no elected-office experience. Hoover is the most recent cabinet secretary to be elected President of the United States, as well as one of only two Presidents (along with William Howard Taft) elected without electoral experience or high military rank.
Hoover, a globally experienced engineer, believed strongly in the Efficiency Movement, which held that the government and the economy were riddled with inefficiency and waste, and could be improved by experts who could identify the problems and solve them. He also believed in the importance of volunteerism and of the role of individuals in society and the economy. Hoover, who had made a small fortune in mining, was the first of two Presidents to redistribute their salary (President Kennedy was the other; he donated all his paychecks to charity). When the Wall Street Crash of 1929 struck less than eight months after he took office, Hoover tried to combat the ensuing Great Depression with moderate government public works projects such as the Hoover Dam. The record tariffs imbedded in the Smoot-Hawley Tariff and aggressive increases in the top tax bracket from 25% to 63%, coupled with increases in corporate taxes, yielded a "balanced budget" in 1933, but the economy plummeted simultaneously and unemployment rates rose to afflict one in four American workers. This downward spiral set the stage for Hoover's defeat in 1932 by Democrat Franklin D. Roosevelt, who promised a New Deal. After Roosevelt assumed the Presidency in 1933, Hoover became a spokesman in opposition to the domestic and foreign policies of the New Deal. In 1947, President Harry S. Truman appointed Hoover to head the Hoover Commission, intended to foster greater efficiency throughout the federal bureaucracy. Most historians agree that Hoover's defeat in the 1932 election was caused primarily by the downward economic spiral, although his strong support for prohibition was also significant. Hoover is usually ranked lower than average among U.S. Presidents.
Family background and early life.
Herbert Hoover was born on August 10, 1874, in West Branch, Iowa, the first of his office born in that state and west of the Mississippi River. His father, Jesse Hoover (1849–1880), was a blacksmith and farm implement store owner, of German (Pfautz, Wehmeyer) and Swiss (Huber, Burkhart) ancestry. Jesse Hoover and his father Eli had moved to Iowa from Ohio twenty years previously. Hoover's mother, Hulda Randall Minthorn (1849–1884), was born in Norwich, Ontario, Canada, and was of English and Irish ancestry. Both of parents were Quakers.
At about age two "Bertie", as he was then called, contracted the croup. He was so ill that he was momentarily thought to have died, until he was resuscitated by his uncle, John Minthorn. As a child, he was often called by his father "my little stick in the mud", since he repeatedly was trapped in the mud while crossing an unpaved street. Hoover's family figured prominently in the town's public prayer life, due almost entirely to Hulda's role in her church. His father, noted by the local paper for his "pleasant, sunshiny disposition", died in 1880. After working to retire her husband's debts, retain their life insurance, and care for the children, his mother died in 1884, leaving Hoover (age nine), his older brother, and his younger sister as orphans. Fellow Quaker Lawrie Tatum was appointed as Hoover's guardian.
After a brief stay with one of his grandmothers in Kingsley, Iowa, Hoover lived the next 18 months with his uncle Allen Hoover in West Branch. In November 1885, he went to Newberg, Oregon, to live with his uncle Dr. John Minthorn, a physician and businessman whose own son had died the year before. The Minthorn household was considered cultured and educational, and imparted a strong work ethic. For two-and-a-half years, Hoover attended Friends Pacific Academy (now George Fox University), and then worked as an office assistant in his uncle's real estate office, the Oregon Land Company, in Salem, Oregon. Though he did not attend high school, Hoover attended night school and learned bookkeeping, typing and mathematics.
Hoover entered Stanford University in 1891, its inaugural year, after failing all the entrance exams (except mathematics) and then being tutored for the summer in Palo Alto. The first-year students were not required to pay tuition. Hoover claimed to be the very first student at Stanford, by virtue of having been the first person in the first class to sleep in the dormitory. While at the university, he was the student manager of both the baseball and football teams and was a part of the inaugural Big Game versus rival the University of California (Stanford won). Hoover graduated in 1895 with a degree in geology. He earned his way through four years of college working at various jobs on and off campus, including the Arkansas and United States Geological Survey. Throughout his tenure at Stanford, he was adamantly opposed to the fraternity system.
Mining engineer.
Australia.
Hoover went to Western Australia in 1897 as an employee of Bewick, Moreing & Co., a London-based gold mining company. His geological training and work experience were well suited for the firm's objectives. He worked at gold mines in Big Bell, Cue, Leonora, Menzies, and Coolgardie. Hoover first went to Coolgardie, then the center of the Western Australian goldfields, where he worked under Edward Hooper, a company partner. Conditions were harsh in these goldfields even though he got a $5,000 salary (equivalent to $100,000 today). In the Coolgardie and Murchison rangelands on the edge of the Great Victoria Desert, Hoover described the region as a land of "black flies, red dust and white heat". He served as a geologist and mining engineer while searching the Western Australian goldfields for investments. After being appointed as mine manager at the age of 23, he led a major program of expansion for the Sons of Gwalia gold mine at Gwalia, and brought in many Italian immigrants to cut costs and counter the union militancy of the Australian miners. He believed "the rivalry between the Italians and the other men was of no small benefit." He also described Italians as "fully 20 per cent superior" to other miners.
During his time at Gwalia, Hoover first met Fleury James Lyster, a pioneering metallurgist.
In Western Australia friends called Hoover "H.C." or the old nickname "Hail Columbia".
An open feud developed between Hoover and his boss Ernest Williams, with Hoover persuading four other mine managers to conspire against his rival. The firm's principals then offered Hoover a compelling promotion which would relocate him to China and also end the feud. Hoover then began to take stock of his private life, including contemplating a separation from his Stanford sweetheart, Lou Henry.
China and other global operations.
Hoover promptly sent a cable of proposal to her, and subsequently married Lou Henry, in 1899. She had been an Episcopalian and became a Quaker. The Hoovers had two sons, Herbert Charles Hoover (1903–1969) and Allan Henry Hoover (1907–1993). The family subsequently went to China. Hoover worked as chief engineer for the Chinese Bureau of Mines, and as general manager for the Chinese Engineering and Mining Corporation. Later he worked for Bewick, Moreing & Co. as the company's lead engineer. Hoover and his wife learned Mandarin Chinese while he worked in China and used it during his tenure at the White House when they wanted to foil eavesdroppers. Hoover made recommendations to improve the lot of the Chinese worker, seeking to end the practice of imposing long term servitude contracts and to institute reforms for workers based on merit. The Boxer Rebellion trapped the Hoovers in Tianjin in June 1900. For almost a month, the settlement was under fire, and both dedicated themselves to defense of their city. Hoover himself guided U.S. Marines around Tianjin during the battle, using his knowledge of the local terrain. Mrs. Hoover meanwhile devoted her efforts at the various hospitals and even wielded, and willingly and accurately deployed, a .38-caliber pistol.
Hoover was made a partner in Bewick, Moreing & Co. on December 18, 1901 and assumed responsibility for various Australian operations and investments. His initial compensation rose to $12,500 annually in addition to a 20% share of profits. The company eventually controlled at one point approximately 50% of gold production in Western Australia. In 1901, Hoover no longer lived in Australia, but he visited the country in 1902, 1903, 1905, and 1907 as an overseas investor.
Hoover was also a director of Chinese Engineering and Mining Corporation (CEMC) when it became a supplier of immigrant labor from Southeast Asia for South African mines. The first shipment of almost 2,000 workers arrived in Durban from Qinhuangdao in July 1904. By 1906, the total number of immigrant workers increased to 50,000, almost entirely recruited and shipped by CEMC. When the living and working conditions of the laborers became known, public opposition to the scheme grew and questions were asked in the British Parliament. The scheme was abandoned in 1911.
In August–September 1905, he founded the Zinc Corporation (eventually part of the Rio Tinto Group) with William Baillieu and others. The lead-silver ore produced at Broken Hill, New South Wales was rich in zinc. But the zinc could not be recovered due to "the Sulphide Problem", and was left in the tailings that remained after the silver and lead was extracted.
Zinc Corporation proposed to buy the tailings and extract the zinc by a new process. The froth flotation process was then being developed at Broken Hill, although the Zinc Corporation struggled to apply it. Hoover came to Broken Hill in 1907. So did Australian engineer Jim Lyster, whose "Lyster Process", enabled the Zinc Corporation to operate the world's first selective or differential flotation plant, from September 1912. Hoover's brother, Theodore J. Hoover, also came to Broken Hill.
"Broken Hill was one of the dreariest places in the world at this time. It lay in the middle of the desert, was unbelievably hot in summer, had no fresh water, no vegetation, and mountains of tailings blew into every crack with every wisp of wind." Despite these miserable conditions, Hoover and his associates became suppliers to world industry of zinc and other vital base minerals.
Sole proprietor.
In 1908, Hoover became an independent mining consultant, traveling worldwide until the outbreak of World War I in 1914. He left Bewick, Moreing & Co and, setting out on his own, eventually ended up with investments on every continent and offices in San Francisco, London, New York City, St. Petersburg, Paris and Mandalay, Burma. He had his second successful venture with the British firm Burma Corporation, again producing silver, lead, and zinc in large quantities at the Namtu Bawdwin Mine, where he caught malaria in 1907. While living in London, noting the American engineer's patriotic intensity, some British acquaintances referred to him as the "star-spangled Hoover". It recalled the nickname he had acquired in the Australian outback: "Hail Columbia" Hoover. The Bawdwin mine ultimately became the chief source of Hoover's fortune.
In his spare time, Hoover wrote. His lectures at Columbia and Stanford universities were published in 1909 as "Principles of Mining", which became a standard textbook. Hoover and his wife also published their English translation of the 1556 mining classic "De re metallica" in 1912. This translation from the Latin of Renaissance author Georgius Agricola is still the most important scholarly version and provides its historical context. It is still in print.
By 1914, Hoover was a wealthy man, with an estimated personal fortune of $4 million. He was once quoted as saying "If a man has not made a million dollars by the time he is forty, he is not worth much". By 1914, Hoover stood eventually to obtain what he later described as "a large fortune from these Russian industries, probably more than is good for anybody". Sixty-six years after opening the mine in 1897, Hoover still had a partial share in the Sons of Gwalia mine when it finally closed in 1963, just one year before the former President's death in New York City in 1964. The successful mine had yielded $55m in gold and $10m in dividends for investors. Herbert Hoover, acting as a main investor, financier, mining speculator, and organizer of men, played a major role in the important metallurgical developments that occurred in Broken Hill in the first decade of the twentieth century, developments that had a great impact on the mining and production of silver, lead, and zinc. In later years Hoover thought of himself and his associates as "engineering doctors to sick concerns", hence his reputation as the "Doctor of sick mines".
After World War II, Hoover's mining work in the Kyshtym area of Russia proved invaluable to American intelligence agencies. They had been unable to find detailed maps of the area, which contained the Soviets' first military plutonium production facility at Mayak, making knowledge of the area vital in the event of war with the Soviet Union. It was determined that Hoover had given extremely detailed maps of the area to Stanford University.
Humanitarian work.
Relief in Europe and Belgium.
When World War I began in August 1914, Hoover helped organize the return of around 120,000 Americans from Europe. He led 500 volunteers in distributing food, clothing, steamship tickets and cash. "I did not realize it at the moment, but on August 3, 1914, my career was over forever. I was on the slippery road of public life." Hoover liked to say that the difference between dictatorship and democracy was simple: dictators organize from the top down, democracies from the bottom up.
When Belgium faced a food crisis after being invaded by Germany in 1914, Hoover undertook an unprecedented relief effort with the Commission for Relief in Belgium (CRB). As chairman of the CRB, Hoover worked with the leader of the Belgian "Comité National de Secours et d'Alimentation" (CNSA), Émile Francqui, to feed the entire nation for the duration of the war. The CRB obtained and imported millions of tons of foodstuffs for the CNSA to distribute, and watched over the CNSA to make sure the German army didn't appropriate the food. The CRB became a veritable independent republic of relief, with its own flag, navy, factories, mills, and railroads. Private donations and government grants (78%) supplied an $11-million-a-month budget.
For the next two years, Hoover worked 14-hour days from London, administering the distribution of over two million tons of food to nine million war victims. In an early form of shuttle diplomacy, he crossed the North Sea forty times to meet with German authorities and persuade them to allow food shipments, becoming an international hero. The Belgian city of Leuven named a prominent square "Hooverplein" after him. At its peak, Hoover's American Relief Administration (ARA) fed 10.5 million people daily. Great Britain grew reluctant to support the CRB, preferring instead to emphasize Germany's obligation to supply the relief; Winston Churchill, whom Hoover intensely disliked, led a military faction that considered the Belgian relief effort "a positive military disaster".
During this time, Hoover made a strong impression on the American Ambassador to Great Britain, Walter Page. In a Memoranda dated December 30, 1916, Page wrote:
Mr. Herbert C. Hoover, Chairman of the Commission for Relief in Belgium, would, if opportunity should offer,
make a useful officer in the State Department. He is probably the only man living who has privately (i.e.,
without holding office) negotiated understandings with the British, French, German, Dutch, and Belgian governments.
He personally knows and has had direct dealings with these governments, and his transactions with them
have involved several hundred million dollars. He is a man of very considerable fortune—less than when the
war began, for tins relief work has cost him much. He was approached on behalf of the British Government
with the suggestion that if he would become a British subject the Government would be pleased to give him
an important executive post and with the hint that if he succeeded a title might await him. His answer was: "I'll
do what I can for you with pleasure; but I'll be damned if I'll give up my American citizenship—not on your life!"
Within the last six months two large financial organizations, each independently, have offered him $100,000 a year
to enter their service; and an industrial company offered him $100,000 "to start with." He declined them all.
When the Belgian relief work recently struck a financial snag, Hoover by telegraph got the promise of a loan in the
United States to the British and French governments for Belgian relief of $150,000,000 ! I do not know, but I think
he would be glad to turn his European experience to the patriotic use of our government. He is forty-two years
old, a graduate of Leland Stanford Jr. University.
U.S. Food Administration.
After the United States entered the war in April 1917, President Woodrow Wilson appointed Hoover to head the U.S. Food Administration, which was created under the Lever Food Control Act in 1917. This was a position he actively sought, though he later claimed it was thrust upon him. He was convinced from his Belgian work that centralization of authority was essential to any relief effort; he demanded, and got, great power albeit not as much as he sought. Hoover believed "food will win the war"; and beginning on September 29, this slogan was introduced and put into frequent use.
He carefully selected men to assist in the agency leadership – Alonzo Taylor (technical abilities), Robert Taft (political associations), Gifford Pinchot (agricultural influence) and Julius Barnes (business acumen). Hoover established set days for people to avoid eating specified foods and save them for soldiers' rations: meatless Mondays, wheatless Wednesdays, and "when in doubt, eat potatoes". This program helped reduce consumption of foodstuffs needed overseas and avoided rationing at home. It was dubbed "Hooverizing" by government publicists, in spite of Hoover's continual orders that publicity should not mention him by name. The agency employed a system of price controls and licensing requirements for suppliers to maximize production. Despite efforts to prevent it, some companies reaped great profits.
Post-war relief.
After the war, as a member of the Supreme Economic Council and head of the American Relief Administration, Hoover organized shipments of food for millions of starving people in Central Europe. He used a newly formed Quaker organization, the American Friends Service Committee, to carry out much of the logistical work in Europe.
Hoover provided aid to the defeated German nation after the war, as well as relief to famine-stricken Bolshevik-controlled areas of Russia in 1921, despite the opposition of Senator Henry Cabot Lodge and other Republicans. When asked if he was not thus helping Bolshevism, Hoover retorted, "Twenty million people are starving. Whatever their politics, they shall be fed!". The Russian famine of 1921–22 claimed 6 million people. In July 1922, Soviet author Maxim Gorky wrote to Hoover:
Your help will enter history as a unique, gigantic achievement, worthy of the greatest glory, which will long remain in the memory of millions of Russians whom you have saved from death.
At war's end, the "New York Times" named Hoover one of the "Ten Most Important Living Americans". Hoover confronted a world of political possibilities when he returned home in 1919. Democratic Party leaders saw him as a potential Presidential candidate, and President Wilson privately preferred Hoover as his successor. "There could not be a finer one," asserted Franklin D. Roosevelt, then a rising star from New York. Hoover briefly considered becoming a Democrat, but he believed that 1920 would be a Republican year. Also, Hoover confessed that he could not run for a party whose only member in his boyhood home had been the town drunk.
Hoover realized that he was in a unique position to collect information about the Great War and its aftermath. In 1919 he established the Hoover War Collection at Stanford University. He donated all the files of the Commission for Relief in Belgium, the U.S. Food Administration, and the American Relief Administration, and pledged $50,000 as an endowment. Scholars were sent to Europe to collect pamphlets, society publications, government documents, newspapers, posters, proclamations, and other ephemeral materials related to the war and the revolutions that followed it. The collection was later renamed the Hoover War Library and is now known as the Hoover Institution.
Secretary of Commerce.
Hoover rejected the Democratic Party's overtures in 1920. He had been a registered Republican before the war, though he had supported Theodore Roosevelt's "Bull Moose" Progressive Party in 1912. Now he declared himself a Republican and a candidate for the Presidency.
He placed his name on the ballot in the California state primary election, where he came close to beating popular Senator Hiram Johnson. But having lost in his home state, Hoover was not considered a serious contender at the convention. Even when it deadlocked for several ballots between Illinois Governor Frank Lowden and General Leonard Wood, few delegates seriously considered Hoover as a compromise choice. Although he had personal misgivings about the capability of the nominee, Warren G. Harding, Hoover publicly endorsed him and made two speeches for Harding.
After being elected, Harding rewarded Hoover for his support, offering to appoint him either Secretary of the Interior or Secretary of Commerce. Hoover ultimately chose Commerce. Commerce had existed for just eight years, since the division of the earlier Department of Commerce and Labor. Commerce was considered a minor Cabinet post, with limited and vaguely defined responsibilities.
Hoover aimed to change that, envisioning the Commerce Department as the hub of the nation's growth and stability. From Harding he demanded, and received, authority to coordinate economic affairs throughout the government. He created many sub-departments and committees, overseeing and regulating everything from manufacturing statistics, the census and radio, to air travel. In some instances he "seized" control of responsibilities from other Cabinet departments when he deemed that they were not carrying out their responsibilities well. Hoover became one of the most visible men in the country, often overshadowing Presidents Harding and Coolidge. Washington wags referred to Hoover as "the Secretary of Commerce... and Under-Secretary of Everything Else!"
As secretary and later as President, Hoover revolutionized relations between business and government. Rejecting the adversarial stance of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, he sought to make the Commerce Department a powerful service organization, empowered to forge cooperative voluntary partnerships between government and business. This philosophy is often called "associationalism".
Many of Hoover's efforts as Commerce Secretary centered on eliminating waste and increasing efficiency in business and industry. This included reducing labor losses from trade disputes and seasonal fluctuations, reducing industrial losses from accident and injury, and reducing the amount of crude oil spilled during extraction and shipping. One major achievement was to promote product standardizations. He energetically promoted international trade by opening offices overseas that gave advice and practical help to businessmen. Hoover was especially eager to promote Hollywood films overseas.
His "Own Your Own Home" campaign was a collaboration to promote ownership of single-family dwellings, with groups such as the Better Houses in America movement, the Architects' Small House Service Bureau, and the Home Modernizing Bureau. He worked with bankers and the savings and loan industry to promote the new long-term home mortgage, which dramatically stimulated home construction.
It has been suggested that Herbert Hoover was the best Secretary of Commerce in United States history. Hoover was the last President to have held a full cabinet position.
Radio conferences.
Hoover's radio conferences played a key role in the early organization, development and regulation of radio broadcasting. Prior to the Radio Act of 1927, the Secretary of Commerce was unable to deny radio licensing or reassign broadcast frequencies. With help from supporters Senator Dill and Representative White, Hoover brought the issue of radio control to the Senate floor. Hoover fought for more power to control the proliferation of licensed radio stations (which in 1927, stood at 732 stations). With help from Dill and White, Hoover promoted the Dill-White Bill which eventually would become the Radio Act of 1927. This act allowed the government to intervene and abolish radio stations that were deemed "non-useful" to the public. Hoover's attempts at regulating radio were not supported by all Congressmen, and he received much opposition from the Senate and from radio station owners. However, Hoover's contributions to regulate radio in its infancy heavily influenced the modern radio system.
Hoover contributed to major projects for navigation, irrigation of dry lands, electrical power, and flood control. As the new air transport industry developed, Hoover held a conference on aviation to promote codes and regulations. He became President of the American Child Health Organization, and he raised private funds to promote health education in schools and communities.
Although he continued to consider Harding ill-suited to be President, the two men nevertheless became friends. Hoover accompanied Harding on his final trip out West in 1923. It was Hoover who called for a specialist to tend to the ailing Chief Executive, and it was also Hoover who contacted the White House to inform them of the President's death. The Commerce Secretary headed the group of dignitaries accompanying Harding's body back to the capital.
By the end of Hoover's service as Secretary, he had raised the status of the Department of Commerce. This was reflected in its modern headquarters built during the Roosevelt Administration in the 1930s in the Federal Triangle in Washington, D.C.
Traffic conferences.
As Commerce Secretary, Hoover also hosted two national conferences on street traffic, in 1924 and 1926 (a third convened in 1930, during Hoover's presidency). Collectively the meetings were called the National Conference on Street and Highway Safety. Hoover's chief objective was to address the growing casualty toll of traffic accidents, but the scope grew and soon embraced motor vehicle standards, rules of the road, and urban traffic control. He left the invited interest groups to negotiate agreements among themselves, which were then presented for adoption by states and localities. Because automotive trade associations were the best organized, many of the positions taken by the conferences reflected their interests. The conferences issued a model Uniform Vehicle Code for adoption by the states, and a Model Municipal Traffic Ordinance for adoption by cities. Both were widely influential, promoting greater uniformity between jurisdictions and tending to promote the automobile's priority in city streets.
Mississippi flood.
The Great Mississippi Flood of 1927 broke the banks and levees of the lower Mississippi River in early 1927, resulting in flooding of millions of acres and leaving 1.5 million people displaced from their homes. Although such a disaster did not fall under the duties of the Commerce Department, the governors of six states along the Mississippi specifically asked for Herbert Hoover in the emergency. President Calvin Coolidge sent Hoover to mobilize state and local authorities, militia, army engineers, the Coast Guard, and the American Red Cross.
With a grant from the Rockefeller Foundation, Hoover set up health units to work in the flooded regions for a year. These workers stamped out malaria, pellagra, and typhoid fever from many areas. His work during the flood brought Herbert Hoover to the front page of newspapers almost everywhere, and he gained new accolades as a humanitarian. The great victory of his relief work, he stressed, was not that the government rushed in and provided all assistance; it was that much of the assistance available was provided by private citizens and organizations in response to his appeals. "I suppose I could have called in the Army to help", he said, "but why should I, when I only had to call upon Main Street."
The treatment of African-Americans during the disaster endangered Hoover's reputation as a humanitarian. Local officials brutalized black farmers and prevented them from leaving relief camps, aid intended for African-American sharecroppers was often given insteadto the landowners, and black men often were conscripted by locals into forced labor, sometimes at gun point. Knowing the potential damage to his presidential hopes if this became public, Hoover struck a deal with Robert Russa Moton, the prominent African-American successor to Booker T. Washington as president of the Tuskegee Institute. In exchange for keeping the sufferings of African-Americans quiet, Hoover promised unprecedented influence for African-Americans should he become president. Moton agreed, and following the accommodationist philosophy of Washington, he worked actively to conceal the information from the media.
Presidential election of 1928.
Republican primaries.
When President Calvin Coolidge announced in August 1927 that he would not seek a second full term of office in the 1928 presidential election, Hoover became the leading Republican candidate, despite the fact Coolidge was lukewarm on Hoover, often deriding his ambitious and popular Commerce Secretary as "Wonder Boy". Coolidge had been reluctant to choose Hoover as his successor; on one occasion he remarked that "for six years that man has given me unsolicited advice—all of it bad. I was particularly offended by his comment to 'shit or get off the pot'." Even so, Coolidge had no desire to split the party by publicly opposing the popular Commerce Secretary's nomination. The delegates did consider nominating Vice President Charles Dawes to be Hoover's running mate. But Coolidge (who hated Dawes) remarked that this would be "a personal affront" to him, and the convention selected Senator Charles Curtis of Kansas instead. His only real challenger was Frank Orren Lowden. Hoover received much favorable press coverage in the months leading up to the convention. Lowden's campaign manager complained that newspapers were full of "nothing but advertisements for Herbert Hoover and Fletcher's Castoria". Hoover's reputation, experience, and popularity coalesced to give him the nomination on the first ballot, with Senator Charles Curtis named as his running mate.
General election.
Hoover campaigned for efficiency and the Republican record of prosperity against Democrat Alfred E. Smith. Smith likewise was a proponent of efficiency earned as governor of New York. Both candidates were pro-business, and each promised to improve conditions for farmers, reform immigration laws, and maintain America's isolationist foreign policy. Where they differed was on the Volstead Act which outlawed the sale of liquor and beer. Smith was a "wet" who called for its repeal, whereas Hoover gave limited support for prohibition, calling it an "experiment noble in purpose". His use of "experiment" suggested it was not permanent. While Smith won extra support among Catholics in the big cities Smith was the target of intense anti-Catholicism from some Protestant communities, especially as Southern Baptists and German Lutherans. Overall the religious factor worked to the advantage of Hoover, although he took no part in it.
Historians agree that Hoover's national reputation and the booming economy, combined with deep splits in the Democratic Party over religion and prohibition, guaranteed his landslide victory with 58% of the vote. Hoover's appeal to southern white voters succeeded in cracking the "Solid South", winning the Democratic strongholds of Florida, North Carolina, Virginia, Texas and Tennessee; the Deep South continued to support Smith as the Democratic candidate. This was the first time that a Republican candidate for president had carried Texas. This outraged the black leadership, which largely broke from the Republican Party, and began seeking candidates who supported civil rights within the Democratic Party.
Presidency (1929–1933).
Hoover held a press conference on his first day in office, promising a "new phase of press relations". He asked the group of journalists to elect a committee to recommend improvements to the White House press conference. Hoover declined to use a spokesman, instead asking reporters to directly quote him and giving them handouts with his statements ahead of time. In his first 120 days in office, he held more regular and frequent press conferences than any other President, before or since. However, he changed his press policies after the 1929 stock market crash, screening reporters and greatly reducing his availability.
Lou Henry Hoover was an activist First Lady. She typified the new woman of the post–World War I era: intelligent, robust, and aware of multiple female possibilities.
White House physician Admiral Joel T. Boone invented the sport Hooverball to keep Hoover fit while in the White House. Hooverball is a combination of volleyball and tennis, played with a 6 lb medicine ball. Hoover and several staff members played it each morning, earning them the nickname "Medicine Ball Cabinet".
Policies.
On poverty, Hoover said that "Given the chance to go forward with the policies of the last eight years, we shall soon with the help of God, be in sight of the day when poverty will be banished from this nation", and promised, "We in America today are nearer to the final triumph over poverty than ever before in the history of any land," but within months, the Stock Market Crash of 1929 occurred, and the world's economy spiraled downward into the Great Depression.
Hoover entered office with a plan to reform the nation's regulatory system, believing that a federal bureaucracy should have limited regulation over a country's economic system. A self-described progressive and reformer, Hoover saw the presidency as a vehicle for improving the conditions of all Americans by encouraging public-private cooperation—what he termed "volunteerism". Hoover saw volunteerism as preferable to governmental coercion or intervention which he saw as opposed to the American ideals of individualism and self-reliance. Long before he had entered politics, he had denounced "laissez-faire" thinking.
Hoover expanded civil service coverage of Federal positions, canceled private oil leases on government lands, and by instructing the Justice Department and the Internal Revenue Service to pursue gangsters for tax evasion, he enabled the prosecution of mobster Al Capone. He appointed a commission that set aside 3,000,000 acres (12,000 km²) of national parks and 2,300,000 acres of national forests; advocated tax reduction for low-income Americans (not enacted); closed certain tax loopholes for the wealthy; doubled the number of veterans' hospital facilities; negotiated a treaty on St. Lawrence Seaway (which failed in the U.S. Senate); wrote a Children's Charter that advocated protection of every child regardless of race or gender; created an antitrust division in the Justice Department; required air mail carriers to adopt stricter safety measures and improve service; proposed federal loans for urban slum clearances (not enacted); organized the Federal Bureau of Prisons; reorganized the Bureau of Indian Affairs; instituted prison reform; proposed a federal Department of Education (not enacted); advocated $50-per-month pensions for Americans over 65 (not enacted); chaired White House conferences on child health, protection, homebuilding and home-ownership; began construction of the Boulder Dam (later renamed Hoover Dam); and signed the Norris–La Guardia Act that limited judicial intervention in labor disputes.
On November 19, 1928, Hoover embarked on a seven-week goodwill tour of several Latin American nations to outline his economic and trade policies to other nations in the Western Hemisphere. 
Foreign relations.
Following the release in 1930 of the Clark Memorandum, Hoover began formulating what would become Roosevelt's Good Neighbor policy. He began withdrawing American troops from Nicaragua and Haiti; he also proposed an arms embargo on Latin America and a one-third reduction of the world's naval power, which was called the Hoover Plan. The Roosevelt Corollary ceased being part of U.S. foreign policy. In response to the Japanese invasion of Manchuria, he and Secretary of State Henry Stimson outlined the Hoover–Stimson Doctrine which held that the United States would not recognize territories gained by force.
Hoover mediated between Chile and Peru to solve a conflict on the sovereignty of Arica and Tacna, that in 1883 by the Treaty of Ancón had been awarded to Chile for ten years, to be followed by a plebiscite that had never happened. By the Tacna–Arica compromise at the Treaty of Lima in 1929, Chile kept Arica, and Peru regained Tacna.
Civil rights.
Hoover seldom mentioned civil rights while he was President. He believed that African-Americans and other races could improve themselves with education and wanted the races assimilated into white culture.
Hoover attempted to appoint John J. Parker of North Carolina to the Supreme Court in 1930 to replace Edward Sanford. The NAACP claimed that Parker had made many court decisions against African-Americans, and they fought the nomination. The NAACP was successful in gaining Senator William Borah's support and the nomination was defeated by two votes (39-41) in the Senate.
First Lady Lou Hoover defied custom and invited the wife of Republican Oscar DePriest, the only African-American member in Congress, to tea at the White House. Booker T. Washington was the previous African-American to have dined at the White House, with Theodore Roosevelt in 1901.
Charles Curtis, the nation's first Native American Vice President, was from the Kaw tribe in Kansas. Hoover's humanitarian and Quaker reputation, along with Curtis as a vice-president, gave special meaning to his Indian policies. His Quaker upbringing influenced his views that Native Americans needed to achieve economic self-sufficiency. As President, he appointed Charles J. Rhoads as commissioner of Indian affairs. Hoover supported Rhoads' commitment to Indian assimilation and sought to minimize the federal role in Indian affairs. His goal was to have Indians acting as individuals (not as tribes) and to assume the responsibilities of citizenship granted with the Indian Citizenship Act of 1924.
Great Depression.
Hoover had long been a proponent of the concept that public-private cooperation was the way to achieve high long-term growth. Hoover feared that too much government intervention would undermine long-term individuality and self-reliance, which he considered essential to the nation's future. Both his ideals and the economy were put to the test with the onset of the Great Depression.
Although many people at the time and for decades afterwards denounced Hoover for taking a hands-off ("laissez-faire") approach to the Depression, a few historians emphasize how active he actually was. Hoover said he rejected Treasury Secretary Andrew Mellon's suggested "leave-it-alone" approach, and called many business leaders to Washington to urge them not to lay off workers or cut wages.
Libertarian economist Murray Rothbard argues that Hoover was actually the initiator of what came to be the New Deal. Hoover engaged in many unprecedented public works programs, including an increase in the Federal Buildings program of over $400 million and the establishment of the Division of Public Construction to spur public works planning. Hoover himself granted more subsidies to ship construction through the Federal Shipping Board and asked for a further $175 million appropriation for public works; this was followed in July 1930 with the expenditure of a giant $915 million public works program, including a Hoover Dam on the Colorado River. In the spring of 1930, Hoover acquired from Congress an added $100 million to continue the Federal Farm Board lending and purchasing policies. At the end of 1929, the FFB established a national wool cooperative-the National Wool Marketing Corporation (NWMC) made up of 30 state associations. The Board also established an allied National Wool Credit Corporation to handle finances. A total of $31.5 million in loans for wool were made by the FFB, of which $12.5 million were permanently lost; these massive agricultural subsidies were a precedent for the later Agricultural Adjustment Act. Hoover also advocated strong labor regulation law, including the enactment of the Bacon-Davis Act, requiring a maximum eight-hour day on construction of public buildings and the payment of at least the "prevailing wage" in the locality, as well as the Norris-LaGuardia Act in 1932. In the Banking sector, Hoover passed The Federal Home Loan Bank Act in July, 1932, establishing 12 district banks ruled by a Federal Home Loan Bank Board in a manner similar to the Federal Reserve System. $125 million capital was subscribed by the Treasury and this was subsequently shifted to the RFC. Hoover was also instrumental in passing the Glass-Steagall Act of 1932, allowing for prime rediscounting at the Federal Reserve, allowing further inflation of credit and bank reserves.
Lee Ohanian, from UCLA, argues that Hoover adopted pro-labor policies after the 1929 stock market crash that "accounted for close to two-thirds of the drop in the nation's gross domestic product over the two years that followed, causing what might otherwise have been a bad recession to slip into the Great Depression". This argument is at odds with the more Keynesian view of the causes of the Depression, and has been challenged as revisionist by J. Bradford DeLong of U.C. Berkeley.
Calls for greater government assistance increased as the U.S. economy continued to decline. He was also a firm believer in balanced budgets (as were most Democrats), and was unwilling to run a budget deficit to fund welfare programs. However, Hoover did pursue many policies in an attempt to pull the country out of depression. In 1929 he authorized the Mexican Repatriation program to help unemployed Mexican citizens return home. The program was largely a forced migration of approximately 500,000 people to Mexico, and continued until 1937. In June 1930, over the objection of many economists, Congress approved and Hoover reluctantly signed into law the Smoot–Hawley Tariff Act. The legislation raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. However, economic depression had spread worldwide, and Canada, France and other nations retaliated by raising tariffs on imports from the U.S. The result was to contract international trade, and worsen the Depression.
In 1931, Hoover issued the Hoover Moratorium, calling for a one-year halt in reparation payments by Germany to France and in the payment of Allied war debts to the United States. The plan was met with much opposition, especially from France, who saw significant losses to Germany during World War I. The Moratorium did little to ease economic declines. As the moratorium neared its expiration the following year, an attempt to find a permanent solution was made at the Lausanne Conference of 1932. A working compromise was never established, and by the start of World War II, reparations payments had stopped completely. Hoover in 1931 urged the major banks in the country to form a consortium known as the National Credit Corporation (NCC).
In the U.S. by 1932 unemployment had reached 24.9%, businesses defaulted on record numbers of loans, and more than 5,000 banks had failed. Hundreds of thousands of Americans found themselves homeless and began congregating in the numerous Hoovervilles (shanty towns) that sprang up in major cities.
Congress, desperate to increase federal revenue, enacted the Revenue Act of 1932, which was the largest peacetime tax increase in history. The Act increased taxes across the board, so that top earners were taxed at 63% on their net income. The 1932 Act also increased the tax on the net income of corporations from 12% to 13.75%.
The final attempt of the Hoover Administration to rescue the economy occurred in 1932 with the passage of the Emergency Relief and Construction Act, which authorized funds for public works programs and the creation of the Reconstruction Finance Corporation (RFC). The RFC's initial goal was to provide government-secured loans to financial institutions, railroads and farmers. The RFC had minimal impact at the time, but was adopted by President Franklin D. Roosevelt and greatly expanded as part of his New Deal.
Economy.
To pay for these and other government programs and to make up for revenue lost due to the Depression, in addition to the Revenue Act of 1932 Hoover agreed to roll back several tax cuts that his Administration had enacted on upper incomes. The estate tax was doubled and corporate taxes were raised by almost 15%. Also, a "check tax" was included that placed a 2-cent tax (over 30 cents in today's economy) on all bank checks. Economists William D. Lastrapes and George Selgin, conclude that the check tax was "an important contributing factor to that period's severe monetary contraction". Hoover also encouraged Congress to investigate the New York Stock Exchange, and this pressure resulted in various reforms.
Franklin D. Roosevelt blasted the Republican incumbent for spending and taxing too much, increasing national debt, raising tariffs and blocking trade, as well as placing millions on the government dole. Roosevelt attacked Hoover for "reckless and extravagant" spending, of thinking "that we ought to center control of everything in Washington as rapidly as possible". Roosevelt's running mate, John Nance Garner, accused the Republican of "leading the country down the path of socialism".
Bonus Army.
Thousands of World War I veterans and their families demonstrated and camped out in Washington, DC, during June 1932, calling for immediate payment of a bonus that had been promised by the World War Adjusted Compensation Act in 1924 for payment in 1945. Although offered money by Congress to return home, some members of the "Bonus army" remained. Washington police attempted to remove the demonstrators from their camp, but they were outnumbered and unsuccessful. Shots were fired by the police in a futile attempt to attain order, and two protesters were killed while many officers were injured. Hoover sent U.S. Army forces led by General Douglas MacArthur and helped by lower ranking officers Dwight D. Eisenhower and George S. Patton to stop a march. MacArthur, believing he was fighting a communist revolution, chose to clear out the camp with military force. In the ensuing clash, hundreds of civilians were injured. Hoover had sent orders that the Army was not to move on the encampment, but MacArthur chose to ignore the command. Hoover was incensed, but refused to reprimand MacArthur. The entire incident was another devastating negative for Hoover in the 1932 election. That led New York governor and Democratic presidential candidate Franklin Roosevelt to declare of Hoover: "There is nothing inside the man but jelly!"
1932 campaign.
Although Hoover had come to detest the presidency, he agreed to run again in 1932, not only as a matter of pride, but also because he feared that no other likely Republican candidate would deal with the depression without resorting to what Hoover considered dangerously radical measures.
Hoover was nominated by the Republicans for a second term. He had originally planned to make only one or two major speeches, and to leave the rest of the campaigning to proxies, but when polls showed the entire Republican ticket facing a resounding defeat at the polls, Hoover agreed to an expanded schedule of public addresses. In his nine major radio addresses Hoover primarily defended his administration and his philosophy. The apologetic approach did not allow Hoover to refute Democratic nominee Franklin Roosevelt's charge that he was personally responsible for the depression.
In his campaign trips around the country, Hoover was faced with perhaps the most hostile crowds of any sitting president. Besides having his train and motorcades pelted with eggs and rotten fruit, he was often heckled while speaking, and on several occasions, the Secret Service halted attempts to kill Hoover by disgruntled citizens, including capturing one man nearing Hoover carrying sticks of dynamite, and another already having removed several spikes from the rails in front of the President's train.
Osro Cobb, a leader of the Republican Party in Arkansas who became politically and personally close to Hoover, recalls:
President Hoover had become convinced that the Democrats deliberately were destroying the economy of the country and erecting roadblocks against every measure he offered to the Congress to restore balance to the economy ... all for the purpose of winning an election. Just a few weeks before the 1932 election, we were standing near a window in the Oval Office. His cigar was frayed and out, and he was in deep thought and obviously troubled. He turned aside and said that he had accepted a speaking engagement in Des Moines, Iowa, in three days and that the U.S. Secret Service had warned him that it had uncovered evidence of plots by radical elements to assassinate him if he kept it. Turmoil and uncertainty prevailed in the country, but there was absolutely no fear in his expression; to the contrary, there appeared to be an abundance of personal courage. Frankly, my heart went out to him, but I pointed out that fate and destiny played a part in the lives of all presidents and that I felt all possible precautions should be taken to protect him but that he should appear and make one of the greatest speeches of his administration. He smiled and said, "Osro, that's what I have already decided to do. Your concurrence is comforting." ...
Despite the late campaign endeavors, Hoover sustained a large defeat in the election, having procured only 39.7 percent of the popular vote to Roosevelt's 57.4 percent. Hoover's popular vote was reduced by 26 percentage points from his result in the 1928 election. In the electoral college he carried only Pennsylvania, Delaware, and four other Northeastern states to lose 59–472. The Democrats extended their control over the U.S. House and gained control of the U.S. Senate.
After the election, Hoover requested that Roosevelt retain the Gold standard as the basis of the US currency, and in effect, continue many of the Hoover Administration's economic policies. Roosevelt refused.
Supreme Court appointments.
Hoover appointed the following Justices to the Supreme Court of the United States:
Hoover broke party lines to appoint the Democrat Cardozo. He explained that he "was one of the ancient believers that the Supreme Court should have a strong minority of the opposition's party and that all appointments should be made from experienced jurists. When the vacancy came... [Hoover] canvassed all the possible Democratic jurists and immediately concluded that Justice Cardozo was the right man and appointed him."
Post-presidency.
Hoover departed from Washington in March 1933 with some bitterness, disappointed both that he had been repudiated by the voters and unappreciated for his best efforts. The Hoovers went first to New York City, where they stayed for a while in the Waldorf-Astoria Hotel. Later that spring, they returned to California to their Stanford residence. Hoover enjoyed returning to the men's clubs that he had long been involved with, including the Bohemian Club, the Pacific-Union Club, and the University Club in San Francisco.
Hoover liked to drive his car, accompanied by his wife or a friend (former Presidents did not get Secret Service protection until the 1960s), and drive on wandering journeys, visiting Western mining camps or small towns where he often went unrecognized, or heading up to the mountains, or deep into the woods, to go fishing in relative solitude. A year before his death, his own fishing days behind him, he published "Fishing For Fun—And To Wash Your Soul", the last of more than sixteen books in his lifetime.
Although many of his friends and supporters called upon Hoover to speak out against FDR's New Deal and to assume his place as the voice of the "loyal opposition", he refused to do so for many years after leaving the White House, and he largely kept himself out of the public spotlight until late in 1934. However, that did not stop rumors springing up about him, often fanned by Democratic politicians who found the former President to be a convenient scapegoat.
The relationship between Hoover and Roosevelt was one of the most severely strained in Presidential history. Hoover had little good to say about his successor. FDR, in turn, supposedly engaged in various petty official acts aimed at his predecessor, ranging from dropping him from the White House birthday greetings message list to having Hoover's name struck from the Hoover Dam on the Colorado River, which would officially be known only as Boulder Dam for many years to come.
In 1936, Hoover entertained hopes of receiving the Republican presidential nomination again, and thus facing Roosevelt in a rematch. However, although he retained strong support among some delegates, there was never much hope of his being selected. He publicly endorsed the nominee, Kansas Governor Alf Landon. But Hoover might as well have been the nominee, since the Democrats virtually ignored Landon, and they ran against the former President himself, constantly attacking him in speeches and warning that a Landon victory would put Hoover back in the White House as the secret power "behind the throne". Roosevelt won 46 of the 48 states, burying Landon in the Electoral College, and the Republican Party in Congress in another landslide.
Although Hoover's reputation was at its low point, circumstances began to rehabilitate his name and restore him to prominence. Roosevelt overreached on his Supreme Court packing plan, and a further financial recession in 1937 and 1938 tarnished his image of invincibility.
In 1939, former President Herbert Hoover became the first Honorary Chairman of Tolstoy Foundation in Valley Cottage, New York, served in this capacity until his death in 1964.
By 1940, Hoover was again being spoken of as the possible nominee of the party in the presidential election. Although he trailed in the polls behind Thomas Dewey, Arthur Vandenberg, and his own former protege, Robert A. Taft, he still had considerable first-ballot delegate strength, and it was believed that if the convention deadlocked between the leading candidates, the party might turn to him as its compromise. However, the convention nominated the utility company president Wendell Willkie, who had supported Roosevelt in 1932 but turned against him after the creation of the Tennessee Valley Authority forced him to sell his company. Hoover dutifully supported Willkie, although he despaired that the nominee endorsed a platform that, to Hoover, was little more than the New Deal in all but name.
The road to war and World War II.
Hoover visited 10 European countries in March 1938, the month of Nazi Germany's "Anschluss" of Austria, and stated "I do not believe a widespread war is at all probable in the near future. There is a general realization everywhere ... that civilization as we know it cannot survive another great war."
Like many, he initially believed that the European Allies would be able to contain Germany, and that Imperial Japan would not attack American interests in the Pacific.
Unlike Roosevelt's administration, Hoover was a vocal supporter of providing relief to countries in Nazi-occupied Europe. He was instrumental in creating the Commission for Polish Relief and Finnish Relief Fund.
When the Germans overran France and then had Britain held in a stalemate, many Americans saw Britain as on the verge of collapse. Nonetheless, Hoover declared that it would be folly for the United States to declare war on Germany and to rush to save the United Kingdom. Rather, he held, it was far wiser for this nation to devote itself to building up its own defenses, and to wash its hands of the mess in Europe. He called for a "Fortress America" concept, in which the United States, protected on the East and on the West by vast oceans patrolled by its Navy and its Air Corps (the USAAF), could adequately repel any attack on the Americas.
During a radio broadcast on June 29, 1941, one week after the Nazi invasion of the Soviet Union, Hoover disparaged any "tacit alliance" between the U.S. and the USSR by saying:
If we go further and join the war and we win, then we have won for Stalin the grip of communism on Russia... Again I say, if we join the war and Stalin wins, we have aided him to impose more communism on Europe and the world. At least we could not with such a bedfellow say to our sons that by making the supreme sacrifice, they are restoring freedom to the world. War alongside Stalin to impose freedom is more than a travesty. It is a tragedy.
When the United States entered the war following the December 7, 1941, Japanese attack on Pearl Harbor, Hoover swept aside all feelings of neutrality and called for total victory. He offered himself to the government in any capacity necessary, but the Roosevelt Administration did not call upon him to serve.
Post–World War II.
Following World War II, Hoover became friends with President Harry S. Truman. Hoover joked that they were for many years the sole members of the "trade union" of former Presidents (since Calvin Coolidge and Roosevelt were dead already). Because of Hoover's previous experience with Germany at the end of World War I, in 1946 President Truman selected the former president to tour Germany to ascertain the food status of the occupied nation. Hoover toured what was to become West Germany in Hermann Göring's old train coach and produced a number of reports critical of U.S. occupation policy. The economy of Germany had "sunk to the lowest level in a hundred years". He stated in one report:
There is the illusion that the New Germany left after the annexations can be reduced to a "pastoral state". It cannot be done unless we exterminate or move 25,000,000 people out of it.
On Hoover's initiative, a school meals program in the American and British occupation zones of Germany was begun on April 14, 1947. The program served 3,500,000 children aged six through 18. A total of 40,000 tons of American food was provided during the "Hooverspeisung" (Hoover meals).
In 1947, President Harry S. Truman appointed Hoover to a commission, which elected him chairman, to reorganize the executive departments. This became known as the Hoover Commission. He was appointed chairman of a similar commission by President Dwight D. Eisenhower in 1953. Both found numerous inefficiencies and ways to reduce waste. The government enacted most of the recommendations that the two commissions had made: 71% of the first commission's and 64% of the second commission's.
Throughout the Cold War, Hoover, always an opponent of Marxism, became even more outspokenly anti-Communist. However, he vehemently opposed American involvement in the Korean War, saying that "To commit the sparse ground forces of the non-communist nations into a land war against this communist land mass [in Asia] would be a war without victory, a war without a successful political terminal... that would be the graveyard of millions of American boys and the exhaustion of the United States."
Despite his advancing years, Hoover continued to work nearly full-time both on writing (among his literary works is "The Ordeal of Woodrow Wilson", a bestseller, and the first time one former President had ever written a biography about another), as well as overseeing the Hoover Institution at Stanford University, which housed not only his own professional papers, but also those of a number of other former high ranking governmental and military servants. He also threw himself into fund-raising for the Boys Clubs (now the Boys & Girls Clubs of America), which became his pet charity.
Final years and death.
From Coolidge's death in 1933 to Dwight D. Eisenhower's last day of serving the presidency in 1961, Hoover had been the only living Republican former president. In 1960, Hoover appeared at his final Republican National Convention. Since the 1948 convention, he had been feted as the guest of "farewell" ceremonies (the unspoken assumption being that the aging former President might not survive until the next convention). Joking to the delegates, he said, "Apparently, my last three good-byes didn't take." Although he lived to see the 1964 convention, ill health prevented him from attending. The Presidential nominee Barry Goldwater acknowledged Hoover's absence in his acceptance speech. In 1962, Hoover had a malignant intestinal tumor removed. Ten months later he had severe gastrointestinal bleeding and seemed terminally ill and frail, but his mind was clear and he maintained a great deal of correspondence. Although the illness would get worse over time, he refused to be hospitalized.
Hoover died following massive internal bleeding at the age of 90 in his New York City suite at 11:35 a.m. on October 20, 1964, 31 years, seven months, and sixteen days after leaving office. At the time of his death, he had the longest retirement of any President. Former President Jimmy Carter surpassed the length of Hoover's retirement on September 7, 2012. At the time of Hoover's death he was the second longest-lived president after John Adams; both were since surpassed by Gerald Ford, Ronald Reagan, George H. W. Bush, and Jimmy Carter. He had outlived by 20 years his wife, Lou Henry Hoover, who had died in 1944, and he was the last living member of the Coolidge administration. He also outlived both his successor Franklin D. Roosevelt, and Eleanor Roosevelt who died in 1945 and 1962, respectively. Hoover was the last President to be the only living ex-President until the death of Lyndon B. Johnson in 1973, thus making Richard Nixon the most recent President to be the only living ex-President. Hoover's time as only living President was also the shortest in history, at one day less than two months.
By the time of his death, he had rehabilitated his image. His birthplace in Iowa and an Oregon home where he lived as a child, became National Landmarks during his lifetime. His Rapidan fishing camp in Virginia, which he had donated to the government in 1933, is now a National Historic Landmark within the Shenandoah National Park. Hoover and his wife are buried at the Herbert Hoover Presidential Library and Museum in West Branch, Iowa. Hoover was honored with a state funeral, the last of three in a span of 12 months, coming as it did just after the deaths of President John F. Kennedy and General Douglas MacArthur. Former Chaplain of the Senate Frederick Brown Harris officiated. All three had two things in common: the commanding general of the Military District of Washington during those funerals was Army Major General Philip C. Wehle and the riderless horse was Black Jack, who also served in that role during Lyndon B. Johnson's funeral.
Writing.
Hoover began his magnum opus "Freedom Betrayed" in 1944 as part of a proposed autobiography. This turned into a significant work critiquing the foreign policy of the United States during the period from the 1930s to 1945. Essentially an attack on the statesmanship of Franklin D. Roosevelt, Hoover completed this work in his 90th year but it was not published until the historian George H. Nash took on the task of editing it. Significant themes are his belief that the western democratic powers should have let Nazi Germany and Soviet Russia assail and weaken each other, and opposition to the British guarantee of Poland's independence.
Heritage and memorials.
The Herbert Hoover Presidential Library and Museum is located in West Branch, Iowa next to the Herbert Hoover National Historic Site. The library is one of thirteen presidential libraries run by the National Archives and Records Administration. The Lou Henry and Herbert Hoover House, built in 1919 in Stanford, California, is now the official residence of the president of Stanford University, and a National Historic Landmark. Hoover's rustic rural presidential retreat, Rapidan Camp (also known as Camp Hoover) in the Shenandoah National Park, Virginia, has been restored and opened to the public. The Hoover Dam is named in his honor, as are numerous elementary, middle, and high schools across the United States.
On December 10, 2008, Hoover's great-granddaughter Margaret Hoover and Senate of Puerto Rico President Kenneth McClintock unveiled a life-sized bronze statue of Hoover at Puerto Rico's Territorial Capitol. The statue is one of seven honoring Presidents who have visited the United States territory during their term of office.
One line in the "All in the Family" theme song—an ironic exercise in pre–New Deal nostalgia—says "Mister, we could use a man like Herbert Hoover again".
The Belgian city of Leuven named a square in the city center after Hoover, honoring him for his work as chairman of the "Commission for Relief in Belgium" during World War I. The square is near the Central Library of the Catholic University of Leuven, where a bust of the president can be seen.
The Polish capital of Warsaw also has a square named after Hoover alongside the Royal Route leading to the Old Town.
George Burroughs Torrey painted a portrait of him.
The historic townsite of Gwalia, Western Australia contains the Sons of Gwalia Museum and the Hoover House Bed and Breakfast, the renovated and restored Mining Engineers residence that was the original residence of Herbert Hoover and where he stayed in subsequent visits to the mine during the first decade of the twentieth century.
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
