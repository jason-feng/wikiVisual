<doc id="19672" url="http://en.wikipedia.org/wiki?curid=19672" title="May 28">
May 28

May 28 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19673" url="http://en.wikipedia.org/wiki?curid=19673" title="MP3">
MP3

MPEG-1 or MPEG-2 Audio Layer III, more commonly referred to as MP3, is an audio coding format for digital audio which uses a form of lossy data compression. It is a common audio format for consumer audio streaming or storage, as well as a de facto standard of digital audio compression for the transfer and playback of music on most digital audio players.
The use of lossy compression is designed to greatly reduce the amount of data required to represent the audio recording and still sound like a faithful reproduction of the original uncompressed audio for most listeners. An MP3 file that is created using the setting of 128 kbit/s will result in a file that is about 1/11 the size of the CD file created from the original audio source. An MP3 file can also be constructed at higher or lower bit rates, with higher or lower resulting quality.
The compression works by reducing accuracy of certain parts of sound that are considered to be beyond the auditory resolution ability of most people. This method is commonly referred to as perceptual coding. It uses psychoacoustic models to discard or reduce precision of components less audible to human hearing, and then records the remaining information in an efficient manner.
MP3 was designed by the Moving Picture Experts Group (MPEG) as part of its MPEG-1 standard and later extended in the MPEG-2 standard. The first subgroup for audio was formed by several teams of engineers at Fraunhofer IIS, University of Hannover, AT&T-Bell Labs, Thomson-Brandt, CCETT, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III was approved as a committee draft of ISO/IEC standard in 1991, finalised in 1992 and published in 1993 (ISO/IEC 11172-3:1993). Backwards compatible MPEG-2 Audio (MPEG-2 Part 3) with additional bit rates and sample rates was published in 1995 (ISO/IEC 13818-3:1995).
History.
Development.
The MP3 lossy audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described a complete set of auditory curves regarding this phenomenon. Ernst Terhardt "et al." created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.
The psychoacoustic masking codec was first proposed in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, NJ, and M. A. Krasner both in the United States. Krasner was the first to publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the linear predictive coding (LPC) gain present in speech. Both Krasner and Schroeder built upon the work performed by Eberhard F. Zwicker in the areas of tuning and masking of critical bands, that in turn built on the fundamental research in the area from Bell Labs of Harvey Fletcher and his collaborators. A wide variety of (mostly perceptual) audio compression algorithms were reported in IEEE's refereed Journal on Selected Areas in Communications. That journal reported in February 1988 on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations.
The immediate predecessors of MP3 were "Optimum Coding in the Frequency Domain" (OCF), and Perceptual Transform Coding (PXFM). These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.
As a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music. He completed his doctoral work in 1989. MP3 is directly descended from OCF and PXFM, representing the outcome of the collaboration of Brandenburg - working as a postdoc at AT&T-Bell Labs with James D. Johnston ("JJ") of AT&T-Bell Labs - with the Fraunhofer Institut for Integrated Circuits, Erlangen, with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders. In 1990, Brandenburg became an assistant professor at Erlangen-Nuremberg. While there, he continued to work on music compression with scientists at the Fraunhofer Society (in 1993 he joined the staff of the Fraunhofer Institute).
The song "Tom's Diner" by Suzanne Vega was the first song used by Karlheinz Brandenburg to develop the MP3. Brandenburg adopted the song for testing purposes, listening to it again and again each time refining the scheme, making sure it did not adversely affect the subtlety of Vega's voice.
Standardization.
In 1991, there were only two proposals available that could be completely assessed for an MPEG audio standard: Musicam (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual Entropy Coding). The Musicam technique, as proposed by Philips (the Netherlands), CCETT (France) and Institut für Rundfunktechnik (Germany) was chosen due to its simplicity and error robustness, as well as its low computational power associated with the encoding of high quality compressed audio. The Musicam format, based on sub-band coding, was the basis of the MPEG Audio compression format (sampling rates, structure of frames, headers, number of samples per frame).
Much of its technology and ideas were incorporated into the definition of ISO MPEG Audio Layer I and Layer II and the filter bank alone into Layer III (MP3) format as part of the computationally inefficient hybrid filter bank. Under the chairmanship of Professor Musmann (University of Hannover) the editing of the standard was made under the responsibilities of Leon van de Kerkhof (Layer I) and Gerhard Stoll (Layer II).
ASPEC was the joint proposal of AT&T Bell Laboratories, Thomson Consumer Electronics, Fraunhofer Society and CNET. It provided the highest coding efficiency.
A working group consisting of Leon van de Kerkhof (The Netherlands), Gerhard Stoll (Germany), Leonardo Chiariglione (Italy), Yves-François Dehery (France), Karlheinz Brandenburg (Germany) and James D. Johnston (USA) took ideas from ASPEC, integrated the filter bank from Layer 2, added some of their own ideas and created MP3, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s.
All algorithms for MPEG-1 Audio Layer I, II and III were approved in 1991 and finalized in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3 (a.k.a. "MPEG-1 Audio" or "MPEG-1 Part 3"), published in 1993.
Further work on MPEG audio was finalized in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3 (a.k.a. "MPEG-2 Part 3" or backwards compatible "MPEG-2 Audio" or "MPEG-2 Audio BC"), originally published in 1995. MPEG-2 Part 3 (ISO/IEC 13818-3) defined additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III. The new sampling rates are exactly half that of those originally defined in MPEG-1 Audio. This reduction in sampling rate serves to cut the available frequency fidelity in half while likewise cutting the bitrate by 50%. 
MPEG-2 Part 3 also enhanced MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel.
An additional extension to "MPEG-2" is named "MPEG-2.5" audio, as MPEG-3 already had a different meaning. This extension was developed at Fraunhofer IIS, the registered MP3 patent holders. Like MPEG-2, MPEG-2.5 adds new sampling rates exactly half of that previously possible with MPEG-2. It thus widens the scope of MP3 to include human speech and other applications requiring only 25% of the frequency reproduction possible with MPEG-1. While not an ISO recognized standard, MPEG-2.5 is widely supported by both inexpensive and brand name digital audio players as well as computer software based MP3 encoders and decoders. 
A sample rate comparison between MPEG-1, 2 and 2.5 is given further down.
 MPEG-2.5 was not developed by MPEG and was never approved as an international standard. MPEG-2.5 is thus an unofficial or proprietary extension to the MP3 format.
Compression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampling rate of the input signal. Nevertheless, compression ratios are often published. They may use the Compact Disc (CD) parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit). Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term "compression ratio" for lossy encoders.
Karlheinz Brandenburg used a CD recording of Suzanne Vega's song "Tom's Diner" to assess and refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as "The mother of MP3". Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of noise artifacts unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model.
Going public.
A reference simulation software implementation, written in the C language and later known as "ISO 11172-5", was developed (in 1991–1996) by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, Layer 3). It was approved as a committee draft of ISO/IEC technical report in March 1994 and printed as document CD 11172-5 in April 1994. It was approved as a draft technical report (DTR/DIS) in November 1994, finalized in 1996 and published as international standard ISO/IEC TR 11172-5:1998 in 1998. The reference software in C language was later published as a freely available ISO standard. Working in non-real time on a number of operating systems, it was able to demonstrate the first real time hardware decoding (DSP based) of compressed audio. Some other real time implementation of MPEG Audio encoders were available for the purpose of digital broadcasting (radio DAB, television DVB) towards consumer receivers and set top boxes.
On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder called l3enc. The filename extension ".mp3" was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named ".bit"). With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives back in that time (~ 500–1000 MB) lossy compression was essential to store non-instrument based (see tracker and MIDI) music for
playback on computer.
As sound scholar Jonathan Sterne notes, "An Australian hacker acquired l3enc using a stolen credit card. The hacker then reverse-engineered the software, wrote a new user interface, and redistributed it for free, naming it "thank you Fraunhofer"".
Internet.
In the second half of '90s, MP3 files began to spread on the Internet. The popularity of MP3s began to rise rapidly with the advent of Nullsoft's audio player Winamp, released in 1997. In 1998, the first portable solid state digital audio player MPMan, developed by SaeHan Information Systems which is headquartered in Seoul, South Korea, was released and the Rio PMP300 was sold afterwards in 1998, despite legal suppression efforts by the RIAA.
In November 1997, the website mp3.com was offering thousands of MP3s created by independent artists for free. The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from CDs, which would have previously been nearly impossible. The first large peer-to-peer filesharing network, Napster, was launched in 1999.
The ease of creating and sharing MP3s resulted in widespread copyright infringement. Major record companies argued that this free sharing of music reduced sales, and called it "music piracy". They reacted by pursuing lawsuits against Napster (which was eventually shut down and later sold) and against individual users who engaged in file sharing.
Despite the popularity of the MP3 format, online music retailers often use other proprietary formats that are encrypted or obfuscated in order to make it difficult to use purchased music files in ways not specifically authorized by the record companies. Attempting to control the use of files in this way is known as Digital Rights Management. Record companies argue that this is necessary to prevent the files from being made available on peer-to-peer file sharing networks. This has other side effects, though, such as preventing users from playing back their purchased music on different types of devices. However, the audio content of these files can usually be converted into an unencrypted format. For instance, users are often allowed to burn files to audio CD, which requires conversion to an unencrypted audio format.
Unauthorized MP3 file sharing continues on next-generation peer-to-peer networks. Some authorized services, such as Beatport, Bleep, Juno Records, eMusic, Zune Marketplace, Walmart.com, Rhapsody, the recording industry approved re-incarnation of Napster, and Amazon.com sell unrestricted music in the MP3 format.
Encoding audio.
The MPEG-1 standard does not include a precise specification for an MP3 encoder, but does provide example psychoacoustic models, rate loop, and the like in the non-normative part of the original standard. At present, these suggested implementations are quite dated. Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information from the audio input. As a result, there are many different MP3 encoders available, each producing files of differing quality. Comparisons are widely available, so it is easy for a prospective user of an encoder to research the best choice. An encoder that is proficient at encoding at higher bit rates (such as LAME) is not necessarily as good at lower bit rates.
During encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples. If there is a transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of quantization noise accompanying the transient. (See psychoacoustics.)
Decoding audio.
Decoding, on the other hand, is carefully defined in the standard. Most decoders are "bitstream compliant", which means that the decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process).
Audio quality.
When performing lossy audio encoding, such as creating an MP3 file, there is a trade-off between the amount of space used and the sound quality of the result. Typically, the creator is allowed to set a bit rate, which specifies how many kilobits the file may use per second of audio. The higher the bit rate, the larger the compressed file will be, and, generally, the closer it will sound to the original file.
With too low a bit rate, compression artifacts (i.e., sounds that were not present in the original recording) may be audible in the reproduction. Some audio is hard to compress because of its randomness and sharp attacks. When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard. A sample of applause compressed with a relatively low bit rate provides a good example of compression artifacts.
Besides the bit rate of an encoded piece of audio, the quality of MP3 files also depends on the quality of the encoder itself, and the difficulty of the signal being encoded. As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders may feature quite different quality, even with identical bit rates. As an example, in a public listening test featuring two different MP3 encoders at about 128 kbit/s, one scored 3.66 on a 1–5 scale, while the other scored only 2.22.
Quality is dependent on the choice of encoder and encoding parameters.
The simplest type of MP3 file uses one bit rate for the entire file – this is known as Constant Bit Rate (CBR) encoding. Using a constant bit rate makes encoding simpler and faster. However, it is also possible to create files where the bit rate changes throughout the file. These are known as Variable Bit Rate (VBR) files. The idea behind this is that, in any piece of audio, some parts will be much easier to compress, such as silence or music containing only a few instruments, while others will be more difficult to compress. So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts. With some encoders, it is possible to specify a given quality, and the encoder will vary the bit rate accordingly. Users who know a particular "quality setting" that is transparent to their ears can use this value when encoding all of their music, and generally speaking not need to worry about performing personal listening tests on each piece of music to determine the correct bit rate.
Perceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones).
A test given to new students by Stanford University Music Professor Jonathan Berger showed that student preference for MP3-quality music has risen each year. Berger said the students seem to prefer the 'sizzle' sounds that MP3s bring to music.
An in-depth study of MP3 audio quality, sound artist and composer Ryan Maguire's project "The Ghost in the MP3" isolates the sounds lost during MP3 compression. In 2015, he released the track "moDernisT" (an anagram of "Tom's Diner"), composed exclusively from the sounds deleted during MP3 compression of the song "Tom's Diner", the mother of the MP3. A detailed account of the techniques used to isolate the sounds deleted during MP3 compression, along with the conceptual motivation for the project, was published in the 2014 Proceedings of the International Computer Music Conference.
Bit rate.
Several bit rates are specified in the MPEG-1 Audio Layer III standard: 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256 and 320 kbit/s, with available sampling frequencies of 32, 44.1 and 48 kHz. MPEG-2 Audio Layer III allows bit rates of 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160 kbit/s with sampling frequencies of 16, 22.05 and 24 kHz.
MPEG-2.5 Audio Layer III is restricted to bit rates of 8, 16, 24, 32, 40, 48, 56 and 64 kbit/s with sampling frequencies of 8, 11.025, and 12 kHz. Because of the Nyquist/Shannon theorem, frequency reproduction is always strictly less than half of the sampling frequency, and imperfect filters requires a larger margin for error (noise level versus sharpness of filter), so 8 kHz sampling rate limits the maximum frequency to 4 kHz, while 48 kHz maximum sampling rate limits an MP3 to 24 kHz sound reproduction.
A sample rate of 44.1 kHz is almost always used, because this is also used for CD audio, the main source used for creating MP3 files. A greater variety of bit rates are used on the Internet. The rate of 128 kbit/s is commonly used, at a compression ratio of 11:1, offering adequate audio quality in a relatively small space. As Internet bandwidth availability and hard drive sizes have increased, higher bit rates up to 320 kbit/s are widespread.
Uncompressed audio as stored on an audio-CD has a bit rate of 1,411.2 kbit/s, so the bitrates 128, 160 and 192 kbit/s represent compression ratios of approximately 11:1, 9:1 and 7:1 respectively.
Non-standard bit rates up to 640 kbit/s can be achieved with the LAME encoder and the freeformat option, although few MP3 players can play those files. According to the ISO standard, decoders are only required to be able to decode streams up to 320 kbit/s.
CBR.
Early MPEG Layer III encoders used what is now called Constant Bit Rate (CBR). The software was only able to use a uniform bitrate on all frames in an MP3 file.
ABR.
Later more sophisticated MP3 encoders were able to use the bit reservoir to target an
average bit rate selecting the encoding rate for each frame based on the complexity
of the sound in that portion of the recording.
VBR.
A more sophisticated MP3 encoder can produce variable bitrate audio.
MPEG audio may use bitrate switching on a per-frame basis, but only layer III decoders must support it. VBR is used when the goal is to achieve a fixed level of quality. The final file size of a VBR encoding is less predictable than with constant bitrate. Average bitrate is VBR implemented as a compromise between the two – the bitrate is allowed to vary for more consistent quality, but is controlled to remain near an average value chosen by the user, for predictable file sizes. Although an MP3 decoder must support VBR to be standards compliant, historically some decoders have bugs with VBR decoding, particularly before VBR encoders became widespread.
Layer III audio can also use a "bit reservoir", a partially full frame's ability to hold part of the next frame's audio data, allowing temporary changes in effective bitrate, even in a constant bitrate stream.
File structure.
Diagram of the structure of an MP3 file (MPEG version 2.5 not supported, hence 12 instead of 11 bits for MP3 Sync Word).
An MP3 file is made up of MP3 frames, which consist of a header and a data block. This sequence of frames is called an elementary stream. Due to the "byte reservoir", frames are not independent items and cannot usually be extracted on arbitrary frame boundaries. The MP3 Data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. The diagram shows that the MP3 Header consists of a sync word, which is used to identify the beginning of a valid frame. This is followed by a bit indicating that this is the MPEG standard and two bits that indicate that layer 3 is used; hence MPEG-1 Audio Layer 3 or MP3. After this, the values will differ, depending on the MP3 file. "ISO/IEC 11172-3" defines the range of values for each section of the header along with the specification of the header. Most MP3 files today contain ID3 metadata, which precedes or follows the MP3 frames, as noted in the diagram.
Design limitations.
There are several limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder.
Newer audio compression formats such as AAC, WMA Pro and Vorbis are generally free of a number of these limitations.
In technical terms, some limitations include:
Exploring these design limitations, "The Ghost in the MP3" is an ongoing project to isolate and make audible the sounds deleted during MP3 compression.
ID3 and other tags.
A "tag" in an audio file is a section of the file that contains metadata such as the title, artist, album, track number or other information about the file's contents. The MP3 standards do not define tag formats for MP3 files, nor is there a standard container format that would support metadata and obviate the need for tags.
However, several "de facto" standards for tag formats exist. As of 2010, the most widespread are ID3v1 and ID3v2, and the more recently introduced APEv2. These tags are normally embedded at the beginning or end of MP3 files, separate from the actual MP3 frame data. MP3 decoders either extract information from the tags, or just treat them as ignorable, non-MP3 junk data.
Playing & editing software often contains tag editing functionality, but there are also tag editor applications dedicated to the purpose.
Aside from metadata pertaining to the audio content, tags may also be used for DRM.
Volume normalization.
Since volume levels of different audio sources can vary greatly, due to the loudness war and other factors, it is sometimes desirable to adjust the playback volume of audio files such that a consistent average loudness is perceived. This normalization, while similar in purpose, is distinct from dynamic range compression.
ReplayGain is one standard for measuring and storing the loudness of an MP3 file in its metadata tag, enabling a ReplayGain-compliant player to automatically adjust the overall playback volume for each file. MP3Gain may be used to reversibly modify files based on ReplayGain measurements so that adjusted playback can be achieved on players without ReplayGain capability.
Licensing and patent issues.
Many organizations have claimed ownership of patents related to MP3 decoding or encoding. These claims have led to a number of legal threats and actions from a variety of sources, resulting in uncertainty about which patents must be licensed in order to create MP3 products without committing patent infringement in countries that allow software patents.
The initial near-complete MPEG-1 standard (parts 1, 2 and 3) was publicly available on 6 December 1991 as ISO CD 11172. In most countries, patents cannot be filed after prior art has been made public, and patents expire 20 years after the initial filing date, which can be up to 12 months later for filings in other countries. As a result, patents required to implement MP3 expired in most countries by December 2012, 21 years after the publication of ISO CD 11172.
An exception is the United States, where patents filed prior to 8 June 1995 expire 17 years after the publication date of the patent, but application extensions make it possible for a patent to issue much later than normally expected (see submarine patents). The various MP3-related patents expire on dates ranging from 2007 to 2017 in the U.S. Patents filed for anything disclosed in ISO CD 11172 a year or more after its publication are questionable. If only the known MP3 patents filed by December 1992 are considered, then MP3 decoding may be patent-free in the US by September 2015 when U.S. Patent expires which had a PCT filing in Oct 1992.
Technicolor (formerly called Thomson Consumer Electronics) claims to control MP3 licensing of the Layer 3 patents in many countries, including the United States, Japan, Canada and EU countries. Technicolor has been actively enforcing these patents.
MP3 license revenues generated about €100 million for the Fraunhofer Society in 2005.
In September 1998, the Fraunhofer Institute sent a letter to several developers of MP3 software stating that a license was required to "distribute and/or sell decoders and/or encoders". The letter claimed that unlicensed products "infringe the patent rights of Fraunhofer and Thomson. To make, sell and/or distribute products using the [MPEG Layer-3] standard and thus our patents, you need to obtain a license under these patents from us."
Sisvel S.p.A. and its U.S. subsidiary Audio MPEG, Inc. previously sued Thomson for patent infringement on MP3 technology, but those disputes were resolved in November 2005 with Sisvel granting Thomson a license to their patents. Motorola followed soon after, and signed with Sisvel to license MP3-related patents in December 2005.
In September 2006, German officials seized MP3 players from SanDisk's booth at the IFA show in Berlin after an Italian patents firm won an injunction on behalf of Sisvel against SanDisk in a dispute over licensing rights. The injunction was later reversed by a Berlin judge, but that reversal was in turn blocked the same day by another judge from the same court, "bringing the Patent Wild West to Germany" in the words of one commentator.
In February 2007, Texas MP3 Technologies sued Apple, Samsung Electronics and Sandisk in eastern Texas federal court, claiming infringement of a portable MP3 player patent that Texas MP3 said it had been assigned. Apple, Samsung, and Sandisk all settled the claims against them in January 2009.
Alcatel-Lucent has asserted several MP3 coding and compression patents, allegedly inherited from AT&T-Bell Labs, in litigation of its own. In November 2006, before the companies' merger, Alcatel sued Microsoft for allegedly infringing seven patents. On 23 February 2007, a San Diego jury awarded Alcatel-Lucent US $1.52 billion in damages for infringement of two of them. The court subsequently tossed the award, however, finding that one patent had not been infringed and that the other was not even owned by Alcatel-Lucent; it was co-owned by AT&T and Fraunhofer, who had licensed it to Microsoft, the judge ruled. That defense judgment was upheld on appeal in 2008. See Alcatel-Lucent v. Microsoft for more information.
Alternative technologies.
Other lossy formats exist. Among these, mp3PRO, AAC, and MP2 are all members of the same technological family as MP3 and depend on roughly similar psychoacoustic models. The Fraunhofer Gesellschaft owns many of the basic patents underlying these formats as well, with others held by Dolby Labs, Sony, Thomson Consumer Electronics, and AT&T. In addition, there are open source compression formats like Opus and Vorbis that are available free of charge and without any known patent restrictions.
Besides lossy compression methods, lossless codecs are a significant alternative to MP3 because they provide unaltered audio content, though with an increased file size compared to lossy compression. Lossless codecs include FLAC, a free lossless codec, Apple Lossless and others.

</doc>
<doc id="19674" url="http://en.wikipedia.org/wiki?curid=19674" title="May 15">
May 15

May 15 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19675" url="http://en.wikipedia.org/wiki?curid=19675" title="May 13">
May 13

May 13 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19676" url="http://en.wikipedia.org/wiki?curid=19676" title="May 14">
May 14

May 14 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19677" url="http://en.wikipedia.org/wiki?curid=19677" title="May 20">
May 20

May 20 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19679" url="http://en.wikipedia.org/wiki?curid=19679" title="Mary Rose">
Mary Rose

The Mary Rose was a carrack-type warship of the English Tudor navy of King Henry VIII. After serving for 33 years in several wars against France, Scotland, and Brittany and after being substantially rebuilt in 1536, she saw her last action on 19 July 1545. While leading the attack on the galleys of a French invasion fleet, she sank in the Solent, the straits north of the Isle of Wight. 
The wreck of the "Mary Rose" was rediscovered in 1971. It was salvaged in 1982 by the Mary Rose Trust, in one of the most complex and expensive projects in the history of maritime archaeology. The surviving section of the ship and thousands of recovered artefacts are of immeasurable value as a Tudor-era time capsule. The excavation and salvage of the "Mary Rose" was a milestone in the field of maritime archaeology, comparable in complexity and cost only to the raising of the Swedish 17th-century warship "Vasa" in 1961. 
The finds include weapons, sailing equipment, naval supplies and a wide array of objects used by the crew. Many of the artefacts are unique to the "Mary Rose" and have provided insights into topics ranging from naval warfare to the history of musical instruments. Since the mid-1980s, while undergoing conservation, the remains of the hull have been on display at the Portsmouth Historic Dockyard. An extensive collection of well-preserved artefacts is on display at the nearby Mary Rose Museum, built to display the reconstructed ship and its artefacts.
The "Mary Rose" was one of the largest ships in the English navy throughout more than three decades of intermittent war and was one of the earliest examples of a purpose-built sailing warship. She was armed with new types of heavy guns that could fire through the recently invented gun-ports. After being substantially rebuilt in 1536, she was also one of the earliest ships that could fire a broadside, although the line of battle tactics that employed it had not yet been developed. Several theories have sought to explain the demise of the "Mary Rose", based on historical records, knowledge of 16th-century shipbuilding, and modern experiments. The precise cause of her sinking is still unclear, because of conflicting testimonies and a lack of conclusive physical evidence. 
Historical context.
By the late 15th century, England was a relatively insignificant state on the periphery of Europe. The great victories against France in the Hundred Years' War were in the past; only the small enclave of Calais in northern France remained as a remnant of the vast continental holdings of the English kings. The War of the Roses—the civil war between the houses of York and Lancaster—had ended with Henry VII's establishment of the House of Tudor, the new ruling dynasty of England. The ambitious naval policies of Henry V were not continued by his successors, and from 1422 to 1509 only six ships were built for the crown. The marriage alliance between Anne of Brittany and Charles VIII of France in 1491, and his successor Louis XII in 1499, left England with a weakened strategic position on its southern flank. Despite this, Henry VII managed to maintain a comparatively long period of peace and a small but powerful core of a navy.
At the onset of the early modern period, the great European powers were France, the Holy Roman Empire and Spain. All three became involved in the War of the League of Cambrai in 1508. The conflict was initially aimed at the Republic of Venice but eventually turned against France. Through the Spanish possessions in the Low Countries, England had close economic ties with the Spanish Habsburgs, and it was the young Henry VIII's ambition to repeat the glorious martial endeavours of his predecessors. In 1509, six weeks into his reign, Henry married the Spanish princess Catherine of Aragon and joined the League, intent on certifying his historical claim as king of both England and France. By 1511 Henry was part of an anti-French alliance that included Ferdinand II of Aragon, Pope Julius II and Holy Roman emperor Maximilian.
The small navy that Henry VIII inherited from his father had only two sizeable ships, the carracks "Regent" and "Sovereign". Just months after his accession, two large ships were ordered: the "Mary Rose" and the "Peter Pomegranate" (later known as the "Peter" after being rebuilt in 1536) of about 500 and 450 tons respectively. Which king ordered the building of the "Mary Rose" is unclear; although construction began during Henry VIII's reign, the plans for naval expansion could have been in the making earlier. Henry VIII oversaw the project and he ordered additional large ships to be built, most notably the "Henry Grace a Dieu" ("Henry Grace of God"), or "Great Harry" at more than 1000 tons burthen. By the 1520s the English state had established a "de facto" permanent "Navy Royal", the organizational ancestor of the modern Royal Navy.
Construction.
The construction of the "Mary Rose" began in 1510 in Portsmouth and she was launched in July 1511. She was then towed to London and fitted with rigging and decking, and supplied with armaments. Other than the structural details needed to sail, stock and arm the "Mary Rose", she was also equipped with flags, banners and streamers (extremely elongated flags that were flown from the top of the masts) that were either painted or gilded.
Constructing a warship of the size of the "Mary Rose" was a major undertaking, requiring vast quantities of high-quality material. In the case of building a state-of-the-art warship, these materials were primarily oak. The total amount of timber needed for the construction can only be roughly calculated since only about one third of the ship still exists. One estimate for the number of trees is around 600 mostly large oaks, representing about 16 hectares (40 acres) of woodland. The huge trees that had been common in Europe and the British Isles in previous centuries were by the 16th century quite rare, which meant that timbers were brought in from all over southern England. The largest timbers used in the construction were of roughly the same size as those used in the roofs of the largest cathedrals in the high Middle Ages. An unworked hull plank would have weighed over 300 kg (660 lb), and one of the main deck beams would have weighed close to three-quarters of a tonne.
Naming.
The common explanation for the ship's name was that it was inspired by Henry VIII's favourite sister, Mary Tudor, and the rose as the emblem of the Tudors. According to historians David Childs, David Loades and Peter Marsden, no direct evidence of naming the ship after the King's sister exists. It was far more common at the time to give ships pious Christian names, a long-standing tradition in Western Europe, or to associate them with their royal patrons. Names like "Grace Dieu" (Grace of God) and "Holighost" (Holy Spirit) had been common since the 15th century and other Tudor navy ships had names like the "Regent" and "Three Ostrich Feathers" (referring to the crest of the Prince of Wales). The Virgin Mary is a more likely candidate for a namesake, and she was also associated with the mystic rose. The name of the sister ship of the "Mary Rose", the "Peter Pomegranate", is believed to have been named in honour of Saint Peter, and the badge of the Queen Catharine of Aragon, a pomegranate. According to Childs, Loades and Marsden, the two ships, which were built around the same time, were named in honour of the king and queen, respectively.
Design.
The "Mary Rose" was substantially rebuilt in 1536. The 1536 rebuilding turned a ship of 500 tons into one of 700 tons, and added an entire extra tier of broadside guns to the old carrack-style structure. By consequence, modern research is based mostly on interpretations of the concrete physical evidence of this version of the "Mary Rose". The construction of the original design from 1509 is less known.
The "Mary Rose" was built according to the carrack-style with high "castles" in the bow and stern with a low waist of open decking in the middle. The shape of the hull has a so-called tumblehome form and reflected the use of the ship as a platform for heavy guns. Above the waterline, the hull gradually narrows to compensate for the weight of the guns and to make boarding more difficult. Since only part of the hull has survived, it is not possible to determine many of the basic dimensions with any great accuracy. The moulded breadth, the widest point of the ship roughly above the waterline, was about 12 metres (39 ft) and the keel about 32 metres (105 ft), although the ship's overall length is uncertain.
The hull had four levels separated by three decks. The terminology for these in the 16th century was still not standardised so the terms used here are those that were applied by the Mary Rose Trust. The "hold" lay furthest down in the ship, right above the bottom planking below the waterline. This is where the kitchen, or galley, was situated and the food was cooked. Directly aft of the galley was the mast step, a rebate in the centre-most timber of the keelson, right above the keel, which supported the main mast, and next to it the main bilge pump. To increase the stability of the ship, the hold was where the ballast was placed and much of the supplies were kept. Right above the hold was the "orlop", the lowest deck. Like the hold it was partitioned and was also used as a storage area for everything from food to spare sails.
Above the orlop lay the "main deck" which housed the heaviest guns. The side of the hull on the main deck level had seven gunports on each side fitted with heavy lids that would have been watertight when closed. This was also the highest deck that was caulked and waterproof. Along the sides of the main deck there were cabins under the forecastle and sterncastle which have been identified as belonging to the carpenter, barber-surgeon, pilot and possibly also the master gunner and some of the officers. The top deck in the hull structure was the "upper deck" (or weather deck) which was exposed to the elements in the waist. It was a dedicated fighting deck without any known partitions and a mix of heavy and light guns. Over the open waist the upper deck was entirely covered with a coarse netting as a defence measure against boarding. Though very little of the upper deck has survived, it has been suggested that it housed the main living quarters of the crew underneath the sterncastle. A drainage located in this area has been identified as a possible "piss-dale", a general urinal to complement the regular toilets that would probably have been located in the bow.
The castles of the "Mary Rose" had additional decks, but since virtually nothing of them survives, their design has had to be reconstructed from historical records. Contemporary ships of equal size were consistently listed as having three decks in both castles. Although speculative, this layout is supported by the illustration in the Anthony Roll and the gun inventories.
During the early stages of excavation of the wreck, it was believed that the ship had originally been built with clinker (or clench) planking, a technique where the hull consisted of overlapping planks that bore the structural strength of the ship. Cutting gunports into a clinker-built hull would have meant weakening the ship's structural integrity, and it was assumed that she was later rebuilt to accommodate a hull with carvel edge-to-edge planking with a skeletal structure to support a hull perforated with gunports. Later examination indicates that the clinker planking is not present throughout the ship; only the outer structure of the sterncastle is built with overlapping planking, though not with a true clinker technique.
Sails and rigging.
Although only the lower fittings of the rigging survives, a 1514 inventory and the only known contemporary depiction of the ship from the Anthony Roll have been used to determine how the propulsion system of the "Mary Rose" was designed. Nine, or possibly ten, sails were flown from four masts and a bowsprit: the foremast and mainmast had two and three square sails respectively; the mizzen mast had a lateen sail and a small square sail and the bonaventure mizzen had at least one lateen sail, and possibly also a square sail, and the bowsprit flew a small square spritsail. According to the Anthony Roll illustration (see top of this section), the yards (the spars from which the sails were set) on the foremast and mainmast were also equipped with sheerhooks, twin curved blades sharpened on the inside, that were intended to cut an enemy ship's rigging during boarding actions.
The sailing capabilities of the "Mary Rose" were commented on by her contemporaries and were once even put to the test. In March 1513 a contest was arranged off The Downs, west of Kent, in which she raced against nine other ships. She won the contest, and Admiral Edward Howard described her enthusiastically as "the noblest ship of sayle [of any] gret ship, at this howr, that I trow [believe] be in Cristendom". Several years later, while sailing between Dover and The Downs, Vice-Admiral William Fitzwilliam noted that both the "Henry Grace à Dieu" and the "Mary Rose" performed very well, riding steadily in rough seas and that it would have been a "hard chose" between the two. Modern experts have been more sceptical to her sailing qualities, believing that ships at this time were almost incapable of sailing close against the wind, and describing the handling of the "Mary Rose" as being like "a wet haystack".
Armament.
The "Mary Rose" represented a transitional ship design in naval warfare. Since ancient times, war at sea had been fought much like that on land: with melee weapons and bows and arrows, but on floating wooden platforms rather than battlefields. Though the introduction of guns was a significant change, it only slowly changed the dynamics of ship-to-ship combat. As guns became heavier and able to take more powerful gunpowder charges, they needed to be placed lower in the ship, closer to the water line. Gunports cut in the hull of ships had been introduced as early as 1501, only about a decade before the "Mary Rose" was built. This made broadsides, coordinated volleys from all the guns on one side of a ship, possible for the first time in history, at least in theory. Naval tactics throughout the 16th century and well into the 17th century focused on countering the oar-powered galleys that were armed with heavy guns in the bow, facing forwards, which were aimed by turning the entire ship against its target. Combined with inefficient gunpowder and the difficulties inherent in firing accurately from moving platforms, this meant that boarding remained the primary tactic for decisive victory throughout the 16th century.
Bronze and iron guns.
As the "Mary Rose" was built and served during a period of rapid development of heavy artillery, her armament was a mix of old designs and innovations. The heavy armament was a mix of older-type wrought iron and cast bronze guns, which differed considerably in size, range and design. The large iron guns were made up of staves or bars welded into cylinders and then reinforced by shrinking iron hoops and breech loaded, from the back, and equipped with simpler gun-carriages made from hollowed-out elm logs with only one pair of wheels, or without wheels entirely. The bronze guns were cast in one piece and rested on four-wheel carriages which were essentially the same as those used until the 19th century. The breech-loaders were cheaper to produce and both easier and faster to reload, but could take less powerful charges than cast bronze guns. Generally, the bronze guns used cast iron shot and were more suited to penetrate hull sides while the iron guns used stone shot that would shatter on impact and leave large, jagged holes, but both could also fire a variety of ammunition intended to destroy rigging and light structure or injure enemy personnel.
The majority of the guns were small iron guns with short range that could be aimed and fired by a single person. The two most common are the "bases", breech-loading swivel guns, most likely placed in the castles, and "hailshot pieces", small muzzle-loaders with rectangular bores and fin-like protrusions that were used to support the guns against the railing and allow the ship structure to take the force of the recoil. Though the design is unknown, there were two "top pieces" in a 1546 inventory (finished after the sinking) which was probably similar to a base, but placed in one or more of the fighting tops.
The ship went through several changes in her armament throughout her career, most significantly accompanying her "rebuilding" in 1536 (see below), when the number of anti-personnel guns was reduced and a second tier of carriage-mounted long guns fitted. There are three inventories that list her guns, dating to 1514, 1540 and 1546. Together with records from the armoury at the Tower of London, these show how the configuration of guns changed as gun-making technology evolved and new classifications were invented. In 1514, the armament consisted mostly of anti-personnel guns like the larger breech-loading iron "murderers" and the small "serpentines", "demi-slings" and stone guns. Only a handful of guns in the first inventory were powerful enough to hole enemy ships, and most would have been supported by the ship's structure rather than resting on carriages. The inventories of both the "Mary Rose" and the Tower had changed radically by 1540. There were now the new cast bronze "cannons", "demi-cannons", "culverins" and "sakers" and the wrought iron "port pieces" (a name that indicated they fired through ports), all of which required carriages, had longer range and were capable of doing serious damage to other ships. The analysis of the 1514 inventory combined with hints of structural changes in the ship both indicate that the gunports on the main deck were indeed a later addition.
Various types of ammunition could be used for different purposes: plain spherical shot of stone or iron smashed hulls, spiked bar shot and shot linked with chains would tear sails or damage rigging, and canister shot packed with sharp flints produced a devastating shotgun effect. Trials made with replicas of culverins and port pieces showed that they could penetrate wood the same thickness of the "Mary Rose's" hull planking, indicating a stand-off range of at least 90 m (295 ft). The port pieces proved particularly efficient at smashing large holes in wood when firing stone shot and were a devastating anti-personnel weapon when loaded with flakes or pebbles.
Hand-held weapons.
To defend against being boarded, "Mary Rose" carried large stocks of melee weapons, including pikes and bills; 150 of each kind were stocked on the ship according to the Anthony Roll, a figure confirmed roughly by the excavations. Swords and daggers were personal possessions and not listed in the inventories, but the remains of both have been found in great quantities, including the earliest dated example of a British basket-hilted sword.
A total of 250 longbows were carried on board, and 172 of these have so far been found, as well as almost 4000 arrows, bracers (arm guards) and other archery-related equipment. Longbow archery in Tudor England was mandatory for all able adult men, and despite the introduction of field artillery and handguns, they were used alongside new missile weapons in great quantities. On the "Mary Rose", the longbows could only have been drawn and shot properly from behind protective panels in the open waist or from the top of the castles as the lower decks lacked sufficient headroom. There were several types of bows of various size and range. Lighter bows would have been used as "sniper" bows, while the heavier design could possibly have been used to shoot fire arrows.
The inventories of both 1514 and 1546 also list several hundred heavy darts and lime pots that were designed to be thrown onto the deck of enemy ships from the fighting tops, although no physical evidence of either of these weapon types has been identified. Of the 50 handguns listed in the Anthony Roll, the complete stocks of five matchlock muskets and fragments of another eleven have been found. They had been manufactured mainly in Italy, with some originating from Germany. Found in storage were several "gunshields", a rare type of firearm consisting of a wooden shield with a small gun fixed in the middle.
Crew.
Throughout her 33-year career, the crew of the "Mary Rose" changed several times and varied considerably in size. It would have a minimal skeleton crew of 17 men or fewer in peace time and when she was "in ordinary" (in reserve). The average wartime manning would have been about 185 soldiers, 200 sailors, 20–30 gunners and an assortment of other specialists such as surgeons, trumpeters and members of the admiral's staff, for a total of 400–450 men. When taking part in land invasions or raids, such as in the summer of 1512, the number of soldiers could have swelled to just over 400 for a combined total of more than 700. Even with the normal crew size of around 400, the ship was quite crowded, and with additional soldiers would have been extremely cramped.
Little is known of the identities of the men who served on the "Mary Rose", even when it comes to the names of the officers, who would have belonged to the gentry. Two admirals and four captains (including Edward and Thomas Howard, who served both positions) are known through records, as well as a few ship masters, pursers, master gunners and other specialists. Forensic science has been used by artists to create reconstructions of faces of eight crew members, and the results were publicized in May 2013. In addition, researchers have extracted DNA from remains in the hopes of identifying origins of crew, and potentially living descendants.
Of the vast majority of the crewmen, soldiers, sailors and gunners alike, nothing has been recorded. The only source of information for these men has been through osteological analysis of the human bones found at the wrecksite. An approximate composition of some of the crew has been conjectured based on contemporary records. The "Mary Rose" would have carried a captain, a master responsible for navigation, and deck crew. There would also have been a purser responsible for handling payments, a boatswain, the captain's second in command, at least one carpenter, a pilot in charge of navigation, and a cook, all of whom had one or more assistants (mates). The ship was also staffed by a barber-surgeon who tended to the sick and wounded, along with an apprentice or mate and possibly also a junior surgeon. The only positively identified person who went down with the ship was Vice-Admiral George Carew. McKee, Stirland and several other authors have also named Roger Grenville, father of Richard Grenville of the Elizabethan-era "Revenge", captain during the final battle, although the accuracy of the sourcing for this has been disputed by maritime archaeologist Peter Marsden.
The bones of a total of 179 people were found during the excavations of the "Mary Rose", including 92 "fairly complete skeletons", more or less complete collections of bones associated with specific individuals. Analysis of these has shown that crew members were all male, most of them young adults. Some were no more than 11–13 years old, and the majority (81%) under 30. They were mainly of English origin and, according to archaeologist Julie Gardiner, they most likely came from the West Country; many following their aristocratic masters into maritime service. There were also a few people from continental Europe. An eyewitness testimony right after the sinking refers to a survivor who was a Fleming, and the pilot may very well have been French. Analysis of oxygen isotopes in teeth indicates that some were also of southern European origin. In general they were strong, well-fed men, but many of the bones also reveal tell-tale signs of childhood diseases and a life of grinding toil. The bones also showed traces of numerous healed fractures, probably the result of on-board accidents.
There are no extant written records of the make-up of the broader categories of soldiers and sailors, but since the "Mary Rose" carried some 300 longbows and several thousand arrows there had to be a considerable proportion of longbow archers. Examination of the skeletal remains has found that there was a disproportionate number of men with a condition known as "os acromiale", affecting their shoulder blades. This condition is known among modern elite archery athletes and is caused by placing considerable stress on the arm and shoulder muscles, particularly of the left arm that is used to hold the bow to brace against the pull on the bowstring. Among the men who died on the ship it was likely that some had practised using the longbow since childhood, and served on board as specialist archers.
A group of six skeletons were found grouped close to one of the 2-tonne bronze culverins on the main deck near the bow. All but one of these crewmen (possibly a "powder monkey" not involved in heavy work) were strong, well-muscled men. They had all engaged in heavy pulling and pushing, indicated by fusing of parts of the spine and ossification, the growth of new bone, on several vertebrae. These have been tentatively classified as members of a complete gun crew, and all died at their battle station.
Military career.
First French war.
The "Mary Rose" first saw battle in 1512, in a joint naval operation with the Spanish against the French. The English were to meet the French and Breton fleets in the English Channel while the Spanish attacked them in the Bay of Biscay and then attack Gascony. The 35-year-old Sir Edward Howard was appointed Lord High Admiral in April and chose the "Mary Rose" as his flagship. His first mission was to clear the seas of French naval forces between England to the northern coast of Spain to allow for the landing of supporting troops near the French border at Fuenterrabia. The fleet consisted of 18 ships, among them the large ships the "Regent" and the "Peter Pomegranate", carrying over 5,000 men. Howard's expedition led to the capture of twelve Breton ships and a four-day raiding tour of Brittany where English forces successfully fought against local forces and burned numerous settlements.
The fleet returned to Southampton in June where it was visited by King Henry. In August the fleet sailed for Brest where it encountered a joint, but ill-coordinated, French-Breton fleet at the battle of St. Mathieu. The English with one of the great ships in the lead (according to Marsden the "Mary Rose") battered the French ships with heavy gunfire and forced them to retreat. The Breton flagship "Cordelière" put up a fight and was boarded by the 1,000-ton "Regent". By accident or through the unwillingness of the Breton crew to surrender, the powder magazine of the "Cordelière" caught fire and blew up in a violent explosion, setting fire to the "Regent" and eventually sinking her. About 180 English crew members saved themselves by throwing themselves into the sea and only a handful of Bretons survived, only to be captured. The captain of the "Regent", 600 soldiers and sailors, the High Admiral of France and the steward of the town of Morlaix were killed in the incident, making it the focal point of several contemporary chronicles and reports. On 11 August, the English burnt 27 French ships, captured another five and landed forces near Brest to raid and take prisoners, but storms forced the fleet back to Dartmouth in Devon and then to Southampton for repairs.
In the spring of 1513, the "Mary Rose" was once more chosen by Howard as the flagship for an expedition against the French. Before seeing action, she took part in a race against other ships where she was deemed to be one of the most nimble and the fastest of the great ships in the fleet (see details under "Sails and rigging"). On 11 April, Howard's force arrived off Brest only to see a small enemy force join with the larger force in the safety of Brest harbour and its fortifications. The French had recently been reinforced by a force of galleys from the Mediterranean, which sank one English ship and seriously damaged another. Howard landed forces near Brest, but made no headway against the town and was by now getting low on supplies. Attempting to force a victory, he took a small force of small oared vessels on a daring frontal attack on the French galleys on 25 April. Howard himself managed to reach the ship of French admiral, Prégent de Bidoux, and led a small party to board it. The French fought back fiercely and cut the cables that attached the two ships, separating Howard from his men. It left him at the mercy of the soldiers aboard the galley, who instantly killed him.
Demoralised by the loss of its admiral and seriously short of food, the fleet returned to Plymouth. Thomas Howard, elder brother of Edward, was assigned the new Lord Admiral, and was set to the task of arranging another attack on Brittany. The fleet was not able to mount the planned attack because of adverse winds and great difficulties in supplying the ships adequately and the "Mary Rose " took up winter quarters in Southampton. In August the Scots joined France in war against England, but were dealt a crushing defeat at the Battle of Flodden on 9 September 1513. A follow-up attack in early 1514 was supported by a naval force that included the "Mary Rose", but without any known engagements. The French and English mounted raids on each other throughout that summer, but achieved little, and both sides were by then exhausted. By autumn the war was over and a peace treaty was sealed by the marriage of Henry's sister, Mary, to French king Louis XII.
After the peace "Mary Rose" was placed in the reserves, "in ordinary". She was laid up for maintenance along with her sister ship the "Peter Pomegranate" in July 1514. In 1518 she received a routine repair and caulking, waterproofing with tar and oakum (old rope fibres) and was then assigned a small skeleton crew who lived on board the ship until 1522. She served briefly on a mission with other warships to "scour the seas" in preparation for Henry VIII's journey across the Channel to the summit with the French king Francis I at the Field of the Cloth of Gold in June 1520.
Second French war.
In 1522, England was once again at war with France because of a treaty with the Holy Roman Emperor Charles V. The plan was for an attack on two fronts with an English thrust in northern France. The "Mary Rose" participated in the escort transport of troops in June 1522, and by 1 July the Breton port of Morlaix was captured. The fleet sailed home and the "Mary Rose" berthed for the winter in Dartmouth. The war raged on until 1525 and saw the Scots join the French side. Though Charles Brandon came close to capturing Paris in 1523, there was little gained either against France or Scotland throughout the war. With the defeat of the French army and capture of Francis I by Charles V's forces at the Battle of Pavia on 24 February 1525, the war was effectively over without any major gains or major victories for the English side.
Maintenance and "in ordinary".
The "Mary Rose" was kept in reserve from 1522 to 1545. She was once more caulked and repaired in 1527 in a newly dug dock at Portsmouth and her longboat was repaired and trimmed. Little documentation about the "Mary Rose" between 1528 and 1539 exists. A document written by Thomas Cromwell in 1536 specifies that the "Mary Rose" and six other ships were "made new" during his service under the king, though it is unclear which years he was referring to and what "made new" actually meant. A later document from January 1536 by an anonymous author states that the "Mary Rose" and other ships were "new made", and dating of timbers from the ship confirms some type of repair being done in 1535 or 1536. This would have coincided with the controversial dissolution of the monasteries that resulted in a major influx of funds into the royal treasury. The nature and extent of this repair is unknown. Many experts, including Margaret Rule, the project leader for the raising of the Mary Rose, have assumed that it meant a complete rebuilding from clinker planking to carvel planking, and that it was only after 1536 that the ship took on the form that it had when it sank and that was eventually recovered in the 20th century. Marsden has speculated that it could even mean that the "Mary Rose" was originally built in a style that was closer to 15th-century ships, with a rounded, rather than square, stern and without the main deck gunports.
Third French war.
Henry's complicated marital situation and his high-handed dissolution of the monasteries angered the Pope and Catholic rulers throughout Europe, which increased England's diplomatic isolation. In 1544 Henry had agreed to attack France together with Emperor Charles V, and English forces captured Boulogne at great cost in September, but soon England was left in the lurch after Charles had achieved his objectives and brokered a separate peace.
In May 1545, the French had assembled a large fleet in the estuary of the Seine with the intent to land troops on English soil. The estimates of the size of the fleet varied considerably; between 123 and 300 vessels according to French sources; and up to 226 sailing ships and galleys according to the chronicler Edward Hall. In addition to the massive fleet, 50,000 troops were assembled at Havre de Grâce (modern-day Le Havre). An English force of 160 ships and 12,000 troops under Viscount Lisle was ready at Portsmouth by early June, before the French were ready to set sail, and an ineffective pre-emptive strike was made in the middle of the month. In early July the huge French force under the command of Admiral Claude d'Annebault set sail for England and entered the Solent unopposed with 128 ships on 16 July. The English had around 80 ships with which to oppose the French, including the flagship "Mary Rose". But since they had virtually no heavy galleys, the vessels that were at their best in sheltered waters like the Solent, the English fleet promptly retreated into Portsmouth harbour.
Battle of the Solent.
The English were becalmed in port and unable to manoeuvre. On 19 July 1545, the French galleys advanced on the immobilised English fleet, and initially threatened to destroy a force of 13 small galleys, or "rowbarges", the only ships that were able to move against them without a wind. The wind picked up and the sailing ships were able to go on the offensive before the oared vessels were overwhelmed. Two of the largest ships, the "Henry Grace Dieu" and the "Mary Rose", led the attack on the French galleys in the Solent.
Early in the battle something went wrong. While engaging the French galleys the "Mary Rose" suddenly heeled (leaned) heavily over to her starboard (right) side and water rushed in through the open gunports. The crew was powerless to correct the sudden imbalance, and could only scramble for the safety of the upper deck as the ship began to sink rapidly. As she leaned over, equipment, ammunition, supplies and storage containers shifted and came loose, adding to the general chaos. The massive port side brick oven in the galley collapsed completely and the huge 360-litre (90 gallon) copper cauldron was thrown onto the orlop deck above. Heavy guns came free and slammed into the opposite side, impeding escape or crushing men beneath them.
For those who were not injured or killed outright by moving objects, there was little time to reach safety, especially for the men who were manning the guns on the main deck or fetching ammunition and supplies in the hold. The companionways that connected the decks with one another would have become bottlenecks for fleeing men, something indicated by the positioning of many of the skeletons recovered from the wreck. What turned the sinking into a major tragedy in terms of lives lost was the anti-boarding netting that covered the upper decks in the waist (the midsection of the ship) and the sterncastle. With the exception of the men who were stationed in the tops in the masts, most of those who managed to get up from below deck were trapped under the netting; they would have been in view of the surface, and their colleagues above, but with little or no chance to break through, and were dragged down with the ship. Out of a crew of at least 400, fewer than 35 escaped, a catastrophic casualty rate of over 90%.
Causes of sinking.
Contemporary accounts.
Several accounts of the sinking have been preserved that describe the incident but the only confirmed eyewitness account is the testimony of a surviving Flemish crewman written down by the Holy Roman Emperor's ambassador François van der Delft in a letter dated 24 July. According to the unnamed Fleming, the ship had fired all of its guns of one side and was turning to present the guns on the other side to the enemy ship, when she was caught in a strong gust of wind, heeled and took in water through the open gunports. In a letter to William Paget dated 23 July former Lord High Admiral John Russel claimed that the ship had been lost because of "rechenes and great negligence". Three years after the sinking, the Hall's Chronicle gave the reason for the sinking as being caused by "to[o] much foly ... for she was laden with much ordinaunce, and the portes left open, which were low, & the great ordinaunce unbreached, so that when the ship should turne, the water entered, and sodainly she sanke."
Later accounts repeat the explanation that the ship heeled over while going about and that the ship was brought down because of the open gunports. A biography of Peter Carew, brother of George Carew, written by John Hooker sometime after 1575, gives the same reason for the sinking, but adds that insubordination among the crew was to blame. The biography claims that George Carew noted that the "Mary Rose" showed signs of instability as soon as her sails were raised. George's uncle Gawen Carew had passed by with his own ship the "Matthew Gonson" during the battle to inquire about the situation of his nephew's ship. In reply he was told "that he had a sorte of knaves whom he could not rule". Contrary to all other accounts, Martin du Bellay, a French cavalry officer who was present at the battle, stated that the "Mary Rose" had been sunk by French guns.
Modern theories.
The most common explanation for the sinking among modern historians is that the ship was unstable for a number of reasons. When a strong gust of wind hit the sails at a critical moment, the open gunports proved fatal, the ship flooded and quickly foundered. Coates offered a variant of this hypothesis, which explains why a ship which served for several decades without sinking, and which even fought in actions in the rough seas off Brittany, unexpectedly foundered: the ship had accumulated additional weight over the years in service and finally become unseaworthy. That the ship was turning after firing all the cannons on one side has been questioned by Marsden after examination of guns recovered in both the 19th and 20th centuries; guns from both sides were found still loaded. This has been interpreted to mean that something else could have gone wrong since it is assumed that an experienced crew would not have failed to secure the gunports before making a potentially risky turn.
The most recent surveys of the ship indicate that the ship was modified late in her career and has lent support to the idea that the "Mary Rose" was altered too much to be properly seaworthy. Marsden has suggested that the weight of additional heavy guns would have increased her draught so much that the waterline was less than one metre (c. 3 feet) from the gunports on the main deck.
Peter Carew's claim of insubordination has been given support by James Watt, former Medical Director-General of the Royal Navy, based on records of an epidemic of dysentery in Portsmouth which could have rendered the crew incapable of handling the ship properly, while historian Richard Barker has suggested that the crew actually knew that the ship was an accident waiting to happen, at which they balked and refused to follow orders. Marsden has noted that the Carew biography is in some details inconsistent with the sequence of events reported by both French and English eyewitnesses. It also reports that there were 700 men on board, an unusually high number. The distance in time to the event it describes may mean that it was embellished to add a dramatic touch. The report of French galleys sinking the "Mary Rose" as stated by Martin du Bellay has been described as "the account of a courtesan" by naval historian Maurice de Brossard. Du Bellay and his two brothers were close to king Francis I and du Bellay had much to gain from portraying the sinking as a French victory. English sources, even if biased, would have nothing to gain from portraying the sinking as the result of crew incompetence rather than conceding to a victory to the much-feared gun galleys.
Dominic Fontana, a geographer at the University of Portsmouth, has voiced support for du Bellay's version of the sinking based on the battle as it is depicted in the Cowdray Engraving, and modern GIS analysis of the modern scene of the battle. By plotting the fleets and calculating the conjectured final manoeuvres of the "Mary Rose", Fontana reached the conclusion that the ship had been hit low in the hull by the galleys and was destabilised after taking in water. He has interpreted the final heading of the ship straight due north as a failed attempt to reach the shallows at Spitbank only a few hundred metres away. This theory has been given partial support by Alexzandra Hildred, one of the experts who has worked with the Mary Rose, though she has suggested that the close proximity to Spitbank could also indicate that the sinking occurred while trying to make a hard turn to avoid running aground.
Experiments.
In 2000, the Channel 4 television programme "What Sank the Mary Rose" attempted to investigate the causes suggested for her sinking by means of experiments with scale models of the ship and metal weights to simulate the presence of troops on the upper decks. Initial tests showed that the ship was able to make the turn described by eyewitnesses without capsizing. In later tests, a fan was used to create a breeze similar to the one reported to have suddenly sprung up on the day of the sinking as the real "Mary Rose" went to make the turn. As the model made the turn, the breeze in the upper works forced it to heel more than at calm, forcing the main deck gun ports below the waterline and foundering the model within a few seconds. The sequence of events closely followed what eyewitnesses had reported, particularly the suddenness with which the ship sank.
History as a shipwreck.
A salvage attempt was ordered by Secretary of State William Paget only days after the sinking, and Charles Brandon, the king's brother-in-law, took charge of practical details. The operation followed the standard procedure for raising ships in shallow waters: strong cables were attached to the sunken ship and fastened to two empty ships, or hulks. At low tide, the ropes were pulled taut with capstans. When the high tide came in, the hulks rose and with them the wreck. It would then be towed into shallower water and the procedure repeated until the whole ship could be raised completely.
A list of necessary equipment was compiled by 1 August and included, among other things, massive cables, capstans, pulleys, and 40 pounds of tallow for lubrication. The proposed salvage team comprised 30 Venetian mariners and a Venetian carpenter with 60 English sailors to serve them. The two ships to be used as hulks were the "Jesus of Lübeck" and "Samson", each of 700 tons burthen and similar in size to the "Mary Rose". Brandon was so confident of success that he reassured the king that it would only be a matter of days before they could raise the "Mary Rose". The optimism proved unfounded. Since the ship had settled at a 60-degree angle to starboard much of it was stuck deep into the clay of the seabed. This made it virtually impossible to pass cables under the hull and required far more lifting power than if the ship had settled on a hard seabed. An attempt to secure cables to the main mast appears only to have resulted in its being snapped off.
The project was only successful in raising rigging, some guns and other items. At least two other salvage teams in 1547 and 1549 received payment for raising more guns from the wreck. Despite the failure of the first salvage operation, there was still lingering belief in the possibility of retrieving the "Mary Rose" at least until 1546, when she was presented as part of the illustrated list of English warships called the Anthony Roll. When all hope of raising the complete ship was finally abandoned is not known. It could have been after Henry VIII's death in January 1547 or even as late as 1549, when the last guns were brought up. The "Mary Rose" was remembered well into the reign of Elizabeth I, and according to one of the queen's admirals, William Monson (1569–1643), the wreck was visible from the surface at low tide in the late 16th century.
Deterioration.
After the sinking, the partially buried wreck created a barrier at a right angle against the currents of the Solent. Two scour pits, large underwater ditches, formed on either side of the wreck while silt and seaweed was deposited inside the ship. A deep but narrow pit formed on the upward tilting port side, while a shallower, broader pit formed on the starboard side, which had mostly been buried by the force of the impact. The abrasive actions of sand and silt carried by the currents and the activity of fungi, bacteria and wood-boring crustaceans and molluscs, such as the "teredo" "shipworm", began to break down the structure of the ship. Eventually the exposed wooden structure was weakened and gradually collapsed. The timbers and contents of the port side were deposited in the scour pits and the remaining ship structure, or carried off by the currents. Following the collapse of the exposed parts of the ship the site was levelled with the seabed and was gradually covered by layers of sediment, concealing most of the remaining structure. During the 16th century a hard layer of compacted clay and crushed shells formed over the ship, stabilising the site and sealing the Tudor-era deposits. Further layers of soft silt covered the site during the 18th and 19th centuries, but frequent changes in the tidal patterns and currents in the Solent occasionally exposed some of the timbers, leading to its accidental rediscovery in 1836 and aided in locating the wreck in 1971. After the ship had been salvaged it was determined that about 40% of the original structure had survived.
Rediscovery in 19th century.
In the summer of 1836, a group of five fishermen caught their nets on timbers protruding from the bottom of the Solent. They contacted a diver to help them remove the hindrance, and on 10 June, Henry Abbinett became the first person to see the "Mary Rose" in almost 300 years. Later, two other professional divers, John Deane and William Edwards, were employed. Using a recently invented rubber suit and metal diving helmet, Deane and Edwards began to examine the wreck and salvage items from it. Along with an assortment of timbers and wooden objects, including several longbows, they brought up several bronze and iron guns, which were sold to the Board of Ordnance for over £220. Initially, this caused a dispute between Deane (who had also brought in his brother Charles into the project), Abbinett and the fishermen who had hired them. The matter was eventually settled by allowing the fishermen a share of the proceeds from the sale of the first salvaged guns, while Deane received exclusive salvage rights at the expense of Abbinett. The wreck was soon identified as the "Mary Rose" from the inscriptions of one of the bronze guns manufactured in 1537.
The identification of the ship led to significant public interest in the salvage operation, and caused a great demand for the objects which were brought up. Though many of the objects could not be properly conserved at the time and subsequently deteriorated, many were documented with pencil sketches and watercolour drawings which survive to this day. John Deane ceased working on the wreck in 1836, but returned in 1840 with new, more destructive methods. With the help of condemned bomb shells filled with gunpowder acquired from the Ordnance Board he blasted his way into parts of the wreck. Fragments of bombs and traces of blasting craters were found during the modern excavations, but there was no evidence that Deane managed to penetrate the hard layer that had sealed off the Tudor levels. Deane reported retrieving a bilge pump and the lower part of the main mast, both of which would have been located inside the ship. The recovery of small wooden objects like longbows, suggest that Deane did manage to penetrate the Tudor levels at some point, though this has been disputed by the excavation project leader Margaret Rule. Newspaper reports on Deane's diving operations in October 1840 report that the ship was clinker built, but since the sterncastle is the only part of the ship with this feature, an alternative explanation has been suggested: Deane did not penetrate the hard shelly layer that covered most of the ship, but only managed to get into remains of the sterncastle that today no longer exist. Despite the rough handling by Deane the "Mary Rose" escaped the wholesale destruction by giant rakes and explosives that was the fate of other wrecks in the Solent.
Modern rediscovery.
The modern search for the "Mary Rose" was initiated by the Southsea branch of the British Sub-Aqua Club in 1965 as part of a project to locate shipwrecks in the Solent. The project was under the leadership of historian, journalist and amateur diver Alexander McKee. Another group led by Lieutenant-Commander Alan Bax of the Royal Navy, sponsored by the Committee for Nautical Archaeology in London, also formed a search team. Initially the two teams had differing views on where to find the wreck, but eventually joined forces. In February 1966 a chart from 1841 was found that marked the positions of the "Mary Rose" and several other wrecks. The charted position coincided with a trench (one of the scour pits) that had already been located by McKee's team, and a definite location was finally established at a position 3 km (1.9 mi) south of the entrance to Portsmouth Harbour () in water with a depth of 11 m (36 feet) at low tide. Diving on the site began in 1966 and a sonar scan by Harold Edgerton in 1967–68 revealed some type of buried feature. In 1970 a loose timber was located and on 5 May 1971, the first structural details of the buried hull were identified after they were partially uncovered by winter storms.
A major problem for the team from the start was that wrecksites in the UK lacked any legal protection from plunderers and treasure hunters. Sunken ships, once being moving objects, were legally treated as chattel and were awarded to those who could first raise them. The Merchant Shipping Act of 1894 also stipulated that any objects raised from a wreck should be auctioned off to finance the salvage operations, and there was nothing preventing anyone from "stealing" the wreck and making a profit. The problem was handled by forming an organisation, the Mary Rose Committee, aiming "to find, excavate, raise and preserve for all time such remains of the ship "Mary Rose" as may be of historical or archaeological interest".
To keep intruders at bay, the Committee arranged a lease of the seabed where the wreck lay from the Portsmouth authorities, thereby discouraging anyone from trespassing on the underwater property. In hindsight this was only a legalistic charade which had little chance of holding up in a court of law. In combination with secrecy as to the exact location of the wreck, it saved the project from interference. It was not until the passing of the Protection of Wrecks Act on 5 February 1973 that the "Mary Rose" was declared to be of national historic interest that enjoyed full legal protection from any disturbance by commercial salvage teams. Despite this, years after the passing of the 1973 act and the excavation of the ship, lingering conflicts with salvage legislation remained a threat to the "Mary Rose" project as "personal" finds such as chests, clothing and cooking utensils risked being confiscated and auctioned off.
Survey and excavation.
Following the discovery of the wreck in 1971, the project became known to the general public and received increasing media attention. This helped bring in more donations and equipment, primarily from private sources. By 1974 the Committee had representatives from the National Maritime Museum, the Royal Navy, the BBC and local organisations. In 1974 the project received royal patronage from Prince Charles, who participated in dives on the site. This attracted yet more publicity, and also more funding and assistance. The initial aims of the Mary Rose Committee were now more officially and definitely confirmed. The Committee had become a registered charity in 1974, which made it easier to raise funds, and the application for excavation and salvage had been officially approved by the UK government.
By 1978 the initial excavation work had uncovered a complete and coherent site with an intact ship structure and the orientation of the hull had been positively identified as being on an almost straight northerly heading with a 60-degree heel to starboard and a slight downward tilt towards the bow. As no records of English shipbuilding techniques used in vessels like the "Mary Rose" survive, excavation of the ship would allow for a detailed survey of her design and shed new light on the construction of ships of the era. A full excavation also meant removing the protective layers of silt that prevented the remaining ship structure from being destroyed through biological decay and the scouring of the currents; the operation had to be completed within a predetermined timespan of a few years or it risked irreversible damage. It was also considered desirable to recover and preserve the remains of the hull if possible. For the first time, the project was faced with the practical difficulties of actually raising, conserving and preparing the hull for public display.
To handle this new, considerably more complex and expensive task, it was decided that a new organisation was needed. The Mary Rose Trust, a limited charitable trust, with representatives from many organisations would handle the need for a larger operation and a large infusion of funds. In 1979 a new diving vessel was purchased to replace the previous 12 m (40 ft) catamaran "Roger Greenville" which had been used from 1971. The choice fell on the salvage vessel "Sleipner", the same craft that had been used as a platform for diving operations on the "Vasa". The project went from a team of only twelve volunteers working four months a year to over 50 individuals working almost around the clock nine months a year. In addition there were over 500 volunteer divers and a laboratory staff of about 70 that ran the shore base and conservation facilities. During the four diving seasons from 1979 to 1982 over 22,000 diving hours was spent on the site, an effort that amounted to 11.8-man years.
Salvage.
Raising the "Mary Rose" meant overcoming a number of delicate problems that had never been encountered before. The salvage of the Swedish warship "Vasa" 1959–61 was the only comparable precedent, but it had been a relatively straightforward operation since the hull was completely intact and rested upright on the seabed. It had been raised with basically the same methods as were in use in Tudor England: cables were slung under the hull and attached to two pontoons on either side of the ship which was then gradually raised and towed into shallower waters. Only one third of the "Mary Rose" was intact and she lay deeply embedded in mud. If the hull were raised in the traditional way, there was no guarantee that it would have enough structural strength to hold together out of water. Many suggestions for salvage were discarded, including the construction of a cofferdam around the wreck site, filling the ship with small buoyant objects (such as ping pong balls) or even pumping brine into the seabed and freezing it so that it would float and take the hull with it. After lengthy discussions it was decided in February 1980 that the hull would first be emptied of all its contents and strengthened with steel braces and frames. It would then be lifted to the surface with floating sheerlegs attached to nylon strops passing under the hull and transferred to a cradle. It was also decided that the ship would be recovered before the end of the diving season in 1982. If the wreck stayed uncovered any longer it risked irreversible damage from biological decay and tidal scouring.
During the last year of the operation, the massive scope of full excavation and salvage was beginning to take its toll on those closely involved in the project. In May 1981 Alexander McKee voiced concerns about the method chosen for the salvage and openly questioned Margaret Rule's position as excavation leader. McKee felt ignored in what he viewed as a project where he had always played a central role, both as the initiator of the search for the "Mary Rose" and other ships in the Solent, and as an active member throughout the diving operations. He had several supporters who all pointed to the risk of the project's turning into an embarrassing failure if the ship were damaged during salvage. To address these concerns it was suggested that the hull should be placed on top of a supporting steel cradle underwater. This would avoid the inherent risks of damaging the wooden structure if it were lifted out of the water without appropriate support. The idea of using nylon strops was also discarded in favour of drilling holes through the hull at 170 points and passing iron bolts through them to allow the attachment of wires connected to a lifting frame.
In the spring of 1982, after three intense seasons of archaeological underwater work, preparations began for the salvage. The operation soon ran into problems: early on there were difficulties with the custom-made lifting equipment; divers on the project belonging to the Royal Engineers had to be pulled because of the outbreak of the Falklands War; and the method of lifting the hull had to be considerably altered as late as June. After the frame was properly attached to the hull it was slowly jacked up on four legs straddling the wreck site to pull the ship off the seabed. The massive crane of the barge "Tog Mor" was then used to lift the frame and hull on to the specially designed cradle which was padded with water-filled bags. On the morning of 11 October 1982, the final lift of the entire package of cradle, hull and lifting frame began. At 9:03 the first timbers of the "Mary Rose" broke the surface in the presence of the salvaging team, Prince Charles and curious spectators on boats circling the site. A second set of bags under the hull was inflated with air to cushion the waterlogged wood and finally the whole package was transferred to the barge that would take the hull ashore. Though eventually successful, the salvage operation was close to floundering on two occasions; first when one of the supporting legs of the lifting frame was bent and had to be removed and later when a corner of the frame, with "an unforgettable crunch", slipped more than a metre (3 feet) and came close to crushing part of the hull.
Archaeology.
As one of the most ambitious and expensive projects in the history of maritime archaeology, the "Mary Rose" project broke new ground within this field in the UK. Besides becoming one of the first wrecks to be protected under the new Protection of Wrecks Act in 1973 it also created several new precedents. It was the first time that a British privately funded project was able to apply modern scientific standards fully and without having to auction off part of the findings to finance its activities; where previous projects often had to settle for just a partial recovery of finds, everything found in connection with the "Mary Rose" was recovered and recorded. The salvage made it possible to establish the first historic shipwreck museum in the UK to receive government accreditation and funding. The excavation of the "Mary Rose" wrecksite proved that it was possible to achieve a level of exactness in underwater excavations comparable to those on dry land.
Throughout the 1970s, the "Mary Rose" was meticulously surveyed, excavated and recorded with the latest methods within the field of maritime archaeology. Working in an underwater environment meant that principles of land-based archaeology did not always apply. Mechanical excavators, airlifts and suction dredges were used in the process of locating the wreck, but as soon as it began to be uncovered in earnest, more delicate techniques were employed. Many objects from the "Mary Rose" had been well preserved in form and shape, but many were quite delicate, requiring careful handling. Artefacts of all sizes were supported with soft packing material, such as old plastic ice cream containers, and some of the arrows that were "soft like cream cheese" had to be brought up in special styrofoam containers. The airlifts that sucked up clay, sand and dirt off-site or to the surface were still used, but with much greater precision since they could potentially disrupt the site. The many layers of sediment that had accumulated on the site could be used to date artefacts in which they were found, and had to be recorded properly. The various types of accretions and remnants of chemicals with artefacts were essential clues to objects that had long since broken down and disappeared, and needed to be treated with considerable care.
The excavation and salvage in the 1970s and early '80s meant that diving operations ceased, even though modern scaffolding and part of the bow were left on the seabed. The pressure on conservators to treat tens of thousands of artefacts and the high costs of conserving, storing and displaying the finds and the ship meant that there were no funds available for diving. In 2002, the UK Ministry of Defence announced plans to build two new aircraft carriers. Because of the massive size of the new vessels, the outlet from Portsmouth needed to be surveyed to make sure that they could sail no matter the tide. The planned route for the underwater channel ran close to the "Mary Rose" wrecksite, which meant that funding was supplied to survey and excavate the site once more. Even though the planned carriers were down-sized enough to not require alteration of Portsmouth outlet, the excavations had already exposed timbers and were completed in 2005. Among the most important finds was the ten-metre (32 feet) stem, the forward continuation of the keel, which provided more exact details about the original profile of the ship.
Finds.
Over 26,000 artefacts and pieces of timber were salvaged along with remains of about half the crew members, The faces of some crew members have been reconstructed. Analysis of the crew skeletons shows many had suffered malnutrition, and had evidence of rickets, scurvy, and other deficiency diseases was found. Crew members also developed arthritis through the stresses on their joints from heavy lifting and maritime life generally, and suffered bone fractures.
As the ship was intended to function as a floating, self-contained community, it was stocked with victuals (food and drink) that could sustain its inhabitants for extended periods of time. The casks used for storage on the "Mary Rose" have been compared with those from a wreck of a trade vessel from the 1560s and have revealed that they were of better quality, more robust and reliable, an indication that supplies for the Tudor navy were given high priority, and their requirements set a high standard for cask manufacturing at the time. As a miniature society at sea, the wreck of the "Mary Rose" held personal objects belonging to individual crew members. This included clothing, games, various items for spiritual or recreation use, or objects related to mundane everyday tasks such as personal hygiene, fishing and sewing. The master carpenter's chest, for example, contained a backgammon set, a book, three plates, a sundial, and a tankard, goods suggesting he was relatively wealthy.
The ship carried several skilled craftsmen and was equipped for handling both routine maintenance and repairing extensive battle damage. In and around one of the cabins on the main deck under the sterncastle, archaeologists found a "collection of woodworking tools ... unprecedented in its range and size", consisting of eight chests of carpentry tools. Along with loose mallets and tar pots used for caulking, this variety of tools belonged to one or several of the carpenters employed on the "Mary Rose".
Many of the cannons and other weapons from the "Mary Rose" have provided invaluable physical evidence about 16th-century weapon technology. The surviving gunshields are almost all from the "Mary Rose", and the four small cast iron hailshot pieces are the only known examples of this type of weapon.
Animal remains have been found in the wreck of the Mary Rose. These include the skeletons of a rat, a frog and a dog. The dog, a mongrel between eighteen months and two years in age, was found near the hatch to the ship's carpenter's cabin and is thought to have been brought aboard as a ratter. Nine barrels have been found to contain bones of cattle, indicating that they contained pieces of beef butchered and stored as ship's rations. In addition, the bones of pigs and fish, stored in baskets, have also been found.
Musical instruments.
Two fiddles, a bow, a still shawm or "doucaine", three three-hole pipes, and a tabor drum with a drumstick were found throughout the wreck. These would have been used for the personal enjoyment of the crew and to provide a rhythm to work on the rigging and turning the capstans on the upper decks. The tabor drum is the earliest known example of its kind and the drumstick of a previously unknown design. The tabor pipes are considerably longer than any known examples from the period. Their discovery proved that contemporary illustrations, previously viewed with some suspicion, were in fact accurate depictions of the instruments. Before the discovery of the "Mary Rose" shawm, an early predecessor to the oboe, instrument historians had been puzzled by reference to "still shawms", or "soft" shawms, that were said to have a sound that was less shrill than earlier shawms. The still shawm disappeared from the musical scene some time in the 16th century, and the instrument found on the "Mary Rose" is the only surviving example. A reproduction has been made and played. Combined with a pipe and tabor, it provides a "very effective bass part" that would have produced "rich and full sound, which would have provided excellent music for dancing on board ship". Only a few other fiddle-type instruments from the 16th century exist, but none of them of the type found on the "Mary Rose". Reproductions of both fiddles have been made, though less is known of their design than the shawm since the neck and strings were missing.
Navigation tools.
In the remains of a small cabin in the bow of the ship and in a few other locations around the wreck was found the earliest dated set of navigation instruments in Europe found so far: compasses, divider calipers, a stick used for charting, protractors, sounding leads, tide calculators and a logreel, an instrument for calculating speed. Several of these objects are not only unique in having such an early, definite dating, but also because they pre-date written records of their use; protractors would have reasonably been used to measure distance on maps, but sea charts are not known to have been used by English navigators during the first half of the 16th century, compasses were not depicted on English ships until the 1560s, and the first mention of a logreel is from 1574.
Barber-surgeon's cabin.
The cabin located on the main deck underneath the sterncastle is thought to have belonged to the barber-surgeon. He was a trained professional who saw to the health and welfare of the crew and acted as the medical expert on board. The most important of these finds were found in an intact wooden chest which contained over 60 objects relating to the barber-surgeon's medical practice: the wooden handles of a complete set of surgical tools and several shaving razors (although none of the steel blades had survived), a copper syringe for wound irrigation and treatment of gonorrhoea, and even a skilfully crafted feeding bottle for feeding incapacitated patients. More objects were found around the cabin, such as earscoops, shaving bowls and combs. With this wide selection of tools and medicaments the barber-surgeon, along with one or more assistants, could set bone fractures, perform amputations and deal with other acute injuries, treat a number of diseases and provide crew members with a minimal standard of personal hygiene.
Conservation.
Preservation of the "Mary Rose" and her contents was an essential part of the project from the start. Though many artefacts, especially those that were buried in silt, had been preserved, the long exposure to an underwater environment had rendered most of them sensitive to exposure to air after recovery. Archaeologists and conservators had to work in tandem from the start to prevent deterioration of the artefacts. After recovery, finds were placed in so-called passive storage, which would prevent any immediate deterioration before the active conservation which would allow them to be stored in an open-air environment. Passive storage depended on the type of material that the object was made of, and could vary considerably. Smaller objects from the most common material, wood, were sealed in polyethylene bags to preserve moisture. Timbers and other objects that were too large to be wrapped were stored in unsealed water tanks. Growth of fungi and microbes that could degrade wood were controlled by various techniques, including low-temperature storage, chemicals, and in the case of large objects, common pond snails that consumed wood-degrading organisms but not the wood itself.
Other organic materials such as leather, skin and textiles were treated similarly, by keeping them moist in tanks or sealed plastic containers. Bone and ivory was desalinated to prevent damage from salt crystallisation, as was glass, ceramic and stone. Iron, copper and copper alloy objects were kept moist in a sodium sesquicarbonate solution to prevent oxidisation and reaction with the chlorides that had penetrated the surface. Alloys of lead and pewter are inherently stable in the atmosphere and generally require no special treatment. Silver and gold were the only materials that required no special passive storage.
Conserving the hull of the Mary Rose was the most complicated and expensive task for the project. In 2002 a donation of £4.8 million from the Heritage Lottery Fund and equivalent monetary support from the Portsmouth City and Hampshire County Councils was needed to keep the work with conservation on schedule. During passive conservation, the ship structure could for practical reasons not be completely sealed, so instead it was regularly sprayed with filtered, recycled water that was kept at a temperature of 2 to 5 °C (35 to 41 °F) to keep it from drying out. Drying waterlogged wood that has been submerged for several centuries without appropriate conservation causes considerable shrinkage (20–50%) and leads to severe warping and cracking as water evaporates from the cellular structure of the wood. The substance polyethylene glycol (PEG) had been used before on archaeological wood, and was during the 1980s being used to conserve the "Vasa". After almost ten years of small-scale trials on timbers, an active three-phase conservation programme of the hull of the "Mary Rose" began in 1994. During the first phase, which lasted from 1994 to 2003, the wood was sprayed with low-molecular-weight PEG to replace the water in the cellular structure of the wood. From 2003 to 2010, a higher-molecular-weight PEG was used to strengthen the mechanical properties of the outer surface layers. The third phase will consist of a controlled air drying that will last three to five years, giving a final date of complete conservation of the "Mary Rose" no later than 2015.
Display.
After the decision to salvage the "Mary Rose," discussions ensued as to where she would eventually go on permanent display. The east end of Portsea Island at Eastney emerged as an early alternative, but was rejected because of parking problems and the distance from the dockyard where she was originally built. Placing the ship next to the famous flagship of Horatio Nelson, HMS "Victory", at Portsmouth Historic Dockyard was proposed in July 1981. A group called the Maritime Preservation Society even suggested Southsea Castle, where Henry VIII had witnessed the sinking, as a final resting place and there was widespread scepticism to the dockyard location. At one point a county councillor even threatened to withdraw promised funds if the dockyard site became more than an interim solution. As costs for the salvage project mounted, there was a debate in the Council chamber and in the local paper "The News" as to whether the money could be spent more appropriately. Although author David Childs writes that in the early 1980s "the debate was as a fiery one", the project was never seriously threatened because of the great symbolic importance of the "Mary Rose" to the naval history of both Portsmouth and England.
Since the mid-1980s, the hull of the "Mary Rose" has been kept in a covered dry dock while undergoing conservation. Although the hull has been open to the public for viewing, the need for keeping the ship saturated first with water and later a polyethylene glycol (PEG) solution has meant that visitors have been separated from the hull by a glass barrier. The specially built ship hall had been visited by over seven million visitors as of 2007, since it first opened on 4 October 1983, just under a year after it was successfully salvaged.
A separate Mary Rose Museum was housed in a structure called No. 5 Boathouse near the ship hall and was opened to the public on 9 July 1984. containing displays explaining the history of the ship and a small number of conserved artefacts, from entire bronze cannons to household items. In September 2009 the temporary "Mary Rose" display hall was closed to visitors to facilitate construction of the new £35 million museum building, which opened to the public on 31 May 2013.
The new Mary Rose Museum was designed by architects Wilkinson Eyre, Pringle Brandon Perkins+Will and built by construction firm Warings. The construction has been challenging because the museum has been built over the ship in the dry dock which is a listed monument. During construction of the museum, conservation of the hull continued inside a sealed "hotbox". In April 2013 the polyethylene glycol sprays were turned off and the process of controlled airdrying began. By 2016 the "hotbox" will be removed and for the first time since 1545, the ship will be revealed dry. This new museum displays most of the artefacts recovered from within the ship in context with the conserved hull. Since opening it has been visited by over 500,000 people.

</doc>
<doc id="19680" url="http://en.wikipedia.org/wiki?curid=19680" title="Mario Kart">
Mario Kart

Mario Kart (マリオカート, Mario Kāto) is a series of go-kart-style racing video games developed and published by Nintendo as spin-offs from its trademark "Super Mario" series. The first in the series, "Super Mario Kart", was launched in 1992 on the Super Nintendo Entertainment System to critical and commercial success.
There have been five "Mario Kart" games released for home consoles, three portable games, and three Namco co-developed arcade games, for a total of eleven. The latest title in the series, "Mario Kart 8", was released on Wii U in May 2014. The series has sold over 100 million copies worldwide to date.
History.
The first title in the "Mario Kart" series is "Super Mario Kart" and was released for the Super Nintendo Entertainment System in 1992. The development of the first game was overseen by Shigeru Miyamoto, the Japanese video game designer who helped create the original "Super Mario Bros.", as well as many other successful games for Nintendo. Darran Jones of NowGamer suggests that the original success of "Super Mario Kart" was the result of including characters previously seen in "Mario Bros." games, while also being a new type of racing game.
Gameplay.
In the "Mario Kart" series, players compete in go-kart races, controlling one of a selection of characters from the "Mario" franchise. Up to twelve characters can compete in each race.
One of the features of the series is the use of various power-up items obtained by driving into item boxes laid out on the course. These power-ups include mushrooms to give players a speed boost, Koopa Shells to be thrown at opponents, and banana peels that can be laid on the track as hazards. The type of weapon received from an item box is often random, though sometimes influenced by the player's current position in the race. For example, players lagging far behind may receive more powerful items while the leader will only receive small defensive items. Called rubber banding, this gameplay mechanism allows other players or computers a realistic chance to catch up to the leading player.
As the series has progressed, each new installment has introduced new elements in order to keep the gameplay fresh, such as new courses, box items and playable characters. These changes include:
Courses.
Many course themes recur throughout the series. Most are based on an existing area in the "Mario" series (Bowser's Castle being among the most prominent), but there are a number of courses that have not appeared elsewhere, but still belong in the Mushroom Kingdom, such as Rainbow Road. Each game in the series includes at least 16 original courses and up to 6 original battle arenas. Each game's tracks are divided into four "cups", or groups in which the player has to have the highest overall placing to win. Most courses can be done in three laps. The first game to feature courses from previous games was "Mario Kart: Super Circuit", which contained all of the tracks from the original Super Nintendo game. Starting with "Mario Kart DS", each entry in the series has featured 16 original courses and 16 "retro" tracks drawn from previous titles, spread across four cups each. In "Mario Kart 8", new tracks are available in two downloadable packages, eight for each package downloaded. This downloadable content includes courses based on other Nintendo titles, such as "The Legend of Zelda" and "Animal Crossing", making them the first "Mario Kart" courses to be set outside the "Mario" universe.
Modes of play.
Each installment features a variety of different modes. The following four modes recur most often in the series:
List of "Mario Kart" games.
At one point, there was also a game in the series planned for the Virtual Boy in 1995. Entitled "Super Mario Kart: Virtual Cup," it was likely to be the first sequel to "Super Mario Kart." The game was cancelled due to the Virtual Boy's failure, but was revealed in a 2000 issue of German gaming magazine "The Big N."
Other appearances.
Several "Mario Kart"-related items appear in the "Super Smash Bros." series, with "Super Smash Bros. Brawl" in particular featuring a Mario Circuit stage based on Figure-8 Circuit from "Mario Kart DS", "Super Smash Bros. for Nintendo 3DS" featuring a Rainbow Road stage based on its appearance in "Mario Kart 7", and "Super Smash Bros. for Wii U" featuring a Mario Circuit stage based on its appearance in "Mario Kart 8". Certain courses from the series have also appeared in "F-Zero X", "Fortune Street", and the "Mario & Sonic" series. Various items from the series can also be seen in games such as "Nintendogs" and "Animal Crossing".
Characters.
Notes.
</dl>
Merchandise.
The "Mario Kart" series has had a range of merchandise released.
Among them are a slot car racer series based on "Mario Kart DS", which comes with Mario and Donkey Kong figures, while Wario and Luigi are available separately. A line of radio-controlled karts have also been marketed, with are controlled by Game Boy Advance-shaped controllers, and feature Mario, Donkey Kong, and Yoshi. There are additional, larger karts that depict the same trio and are radio-controlled by a GameCube-shape controller.
Japanese figurines of Mario, Luigi, Peach, Toad, Yoshi, Wario, Donkey Kong, and Bowser are also available for purchase as well as for "Mario Kart 64", figures of Mario, Luigi, Wario, Bowser, Donkey Kong, and Yoshi were made by Toybiz. There are also Sound Drops inspired by "Mario Kart Wii" with eight sounds from the game. A land-line telephone featuring Mario holding a lightning bolt while seated in his kart, has also been marketed.
K'Nex released "Mario Kart Wii" sets, with Mario, Luigi, Yoshi, Donkey Kong, and Bowser in karts and bikes, as well as tracks from the game. "Mario Kart 7" K'Nex sets have also been released.
Nintendo's own customer rewards program Club Nintendo released merchandise from the series as well. These include a "Mario Kart 8" original soundtrack, a "Mario Kart Wii"-themed stopwatch, and gold trophies modeled after those in "Mario Kart 7". Before Club Nintendo, a "Mario Kart 64" soundtrack was offered by mail.
In 2014, McDonalds released "Mario Kart 8" toys with Happy Meals. They featured 8 of the characters in kart that were customizable with stickers.
Reception.
"Nintendo Power" listed the "Mario Kart" series as being one of the greatest multi-player experiences, citing the diversity in game modes as well as the entertainment value found.
"Guinness World Records" listed six records set by the "Mario Kart" series, including "First Console Kart Racing Game", "Best Selling Racing Game" and "Longest Running Kart Racing Franchise." "Guinness World Records" ranked the original "Super Mario Kart" number 1 on the list of top 50 console games of all time based on initial impact and lasting legacy.
The series has sold over 100 million copies. "Mario Kart Wii" is the best selling installment in the series, selling 35.53 million as of March 2014.

</doc>
<doc id="19683" url="http://en.wikipedia.org/wiki?curid=19683" title="Module (disambiguation)">
Module (disambiguation)

Module or modular may refer to the concept of modularity. It may also refer to:

</doc>
<doc id="19684" url="http://en.wikipedia.org/wiki?curid=19684" title="May 21">
May 21

May 21 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19685" url="http://en.wikipedia.org/wiki?curid=19685" title="Mythology">
Mythology

Mythology can refer to the collected myths of a group of people—their body of stories which they tell to explain nature, history, and customs—or to the study of such myths.
As a collection of such stories, mythology is a vital feature of every culture. Various origins for myths have been proposed, ranging from personification of nature, personification of natural phenomena to truthful or hyperbolic accounts of historical events, to explanations of existing ritual. Although the term is complicated by its implicit condescension, mythologizing is not just an ancient or primitive practice, as shown by contemporary mythopoeia such as urban legends and the expansive fictional mythoi created by fantasy novels and Japanese manga. A culture's collective mythology helps convey belonging, shared and religious experience, behavioural models, and moral and practical lessons.
As the study of myth, mythology dates back to antiquity. Rationalists in ancient Greece and China devised allegorical interpretations of their traditional stories. Rival classifications of the Greek myths by Euhemerus, Plato's "Phaedrus", and Sallustius were developed by the Neoplatonists and revived by Renaissance mythographers. Nineteenth-century comparative mythology reinterpreted myth as a primitive and failed counterpart of science (E. B. Tylor), a "disease of language" (Max Müller), or a misinterpretation of magical ritual (James Frazer).
Some recent approaches have rejected a conflict between the value of myth and rational thought, often viewing myths, rather than being merely inaccurate historical accounts, as expressions for understanding general psychological, cultural or societal truths.
Etymology.
The English term "mythology" predates the word "myth" by centuries. It appeared in the 15th century, borrowed whole from Middle French "mythologie". The word "mythology" "exposition of myths" comes from Middle French "mythologie", from Late Latin "mythologia", from Greek μυθολογία "mythologia" "legendary lore, a telling of mythic legends; a legend, story, tale," from μῦθος "mythos" "myth" and -λογία "-logia" "study." Both terms translated the subject of Fulgentius's 5th-century "Mythologiæ", which was concerned with the explication of Greek and Roman stories about their gods. Although the African author Fulgentius's conflation with the contemporary African saint Fulgentius is now questioned, the "Mythologiæ" explicitly treated its subject matter as allegories requiring interpretation and not as true events. (The word "mythología" [μυθολογία] appears in Plato but was a general term for "fiction" or "story-telling" of any kind, combining "mỹthos" [μῦθος, "narrative, fiction"] and "-logía" [-λογία, "discourse, able to speak about"].) From Lydgate until the 17th or 18th century, "mythology" was similarly used to mean a moral, a fable, an allegory, or a parable. From its earliest use in reference to a "collection" of traditional stories or beliefs, it has implied the falsehood of the stories being described; remaining associated with sacred tales of the Greeks and Romans, though, it came to be applied by analogy with similar bodies of traditional stories among other polytheistic cultures around the world. The Greek loanword "mythos" (pl. "mythoi") and Latinate "mythus" (pl. "mythi") both appeared in English before the first attestation of "myth" in 1830.
Terminology.
In present use, "mythology" usually refers to the "collected myths" of a group of people but may also mean the "study" of such myths. For example, Greek mythology, Roman mythology and Hittite mythology all describe the body of myths retold among those cultures, but landscape mythology describes the study of landscape used across various totemistic peoples. Alan Dundes defined myth as a sacred narrative which explains how the world and humanity evolved into their present form, "a story that serves to define the fundamental worldview of a culture by explaining aspects of the natural world and delineating the psychological and social practices and ideals of a society"; Bruce Lincoln defined it as "ideology in narrative form". Many scholars in other fields use the term "myth" in somewhat different ways; in a very broad sense, the word can refer to any traditional story or any popular misconception or imaginary entity. Because of this pejorative sense, some opt to return to the earlier "mythos", although its use was similarly pejorative and it now more commonly refers to its Aristotelian sense as a "plot point" or to a collective mythology, as in the worldbuilding of H.P. Lovecraft.
Mythology is now often sharply distinguished from didactic literature such as fables, but its relationship with other traditional stories such as legends and folktales is much more nebulous. The main characters in myths are usually gods, demigods, or supernatural humans, while legends generally feature humans as their main characters, but many exceptions or combinations exist, as in the "Iliad", "Odyssey", and "Aeneid". Myths are often endorsed by rulers and priests and closely linked to religion or spirituality. In fact, many societies group their myths, legends, and history together, considering myths to be true accounts of their remote past. Creation myths, particularly, take place in a primordial age when the world had not yet achieved its current form, but other myths explain how the society's customs, institutions and taboos were established and sanctified. A separate space is created for folktales, which are not considered true by the people who tell them. As stories spread to other cultures or as faiths change, however, myths can come to be considered folktales, sometimes even to the point of being reinterpreted as one, its divine characters recast as humans or as demihumans such as giants, elves, and faeries.
Origins.
Euhemerism.
One theory claims that myths are distorted accounts of real historical events. According to this theory, storytellers repeatedly elaborated upon historical accounts until the figures in those accounts gained the status of gods. For example, one might argue that the myth of the wind-god Aeolus evolved from a historical account of a king who taught his people to use sails and interpret the winds. Herodotus (5th century BC) and Prodicus made claims of this kind. This theory is named "euhemerism" after the mythologist Euhemerus (c.320 BC), who suggested that the Greek gods developed from legends about human beings.
Allegory.
Some theories propose that myths began as allegories. According to one theory, myths began as allegories for natural phenomena: Apollo represents the sun, Poseidon represents water, and so on. According to another theory, myths began as allegories for philosophical or spiritual concepts: Athena represents wise judgment, Aphrodite represents desire, etc. The 19th century Sanskritist Max Müller supported an allegorical theory of myth. He believed that myths began as allegorical descriptions of nature, but gradually came to be interpreted literally: for example, a poetic description of the sea as "raging" was eventually taken literally, and the sea was then thought of as a raging god.
Personification.
Some thinkers believe that myths resulted from the personification of inanimate objects and forces. According to these thinkers, the ancients worshipped natural phenomena such as fire and air, gradually coming to describe them as gods. For example, according to the theory of mythopoeic thought, the ancients tended to view things as persons, not as mere objects; thus, they described natural events as acts of personal gods, thus giving rise to myths.
Myth-ritual theory.
According to the myth-ritual theory, the existence of myth is tied to ritual. In its most extreme form, this theory claims that myths arose to explain rituals. This claim was first put forward by the biblical scholar William Robertson Smith. According to Smith, people begin performing rituals for some reason that is not related to myth; later, after they have forgotten the original reason for a ritual, they try to account for the ritual by inventing a myth and claiming that the ritual commemorates the events described in that myth. The anthropologist James Frazer had a similar theory. Frazer believed that primitive man starts out with a belief in magical laws; later, when man begins to lose faith in magic, he invents myths about gods and claims that his formerly magical rituals are religious rituals intended to appease the gods.
Functions of myth.
Mircea Eliade argued that one of the foremost functions of myth is to establish models for behavior and that myths may also provide a religious experience. By telling or reenacting myths, members of traditional societies detach themselves from the present and return to the mythical age, thereby bringing themselves closer to the divine.
Lauri Honko asserts that, in some cases, a society will reenact a myth in an attempt to reproduce the conditions of the mythical age. For example, it will reenact the healing performed by a god at the beginning of time in order to heal someone in the present. Similarly, Roland Barthes argues that modern culture explores religious experience. Because it is not the job of science to define human morality, a religious experience is an attempt to connect with a perceived moral past, which is in contrast with the technological present.
Joseph Campbell writes: "In the long view of the history of mankind, four essential functions of mythology can be discerned. The first and most distinctive – vitalizing all – is that of eliciting and supporting a sense of awe before the mystery of being." 
"The second function of mythology is to render a cosmology, an image of the universe that will support and be supported by this sense of awe before the mystery of the presence and the presence of a mystery." 
"A third function of mythology is to support the current social order, to integrate the individual organically with his group;"
"The fourth function of mythology is to initiate the individual into the order of realities of his own psyche, guiding him toward his own spiritual enrichment and realization."
In a later work Campbell explains the relationship of myth to civilization:
And yet the history of civilisation is not one of harmony.
Campbell gives his answer to the question: "what is the function of myth today"? in episode 2 of Bill Moyers's "The Power of Myth" series.
Study of mythology.
Historically, the important approaches to the study of mythology have been those of Vico, Schelling, Schiller, Jung, Freud, Lévy-Bruhl, Lévi-Strauss, Frye, the Soviet school, and the Myth and Ritual School.
Pre-modern theories.
The critical interpretation of myth goes back as far as the Presocratics. Euhemerus was one of the most important pre-modern mythologists. He interpreted myths as accounts of actual historical events, distorted over many retellings. Sallustius, for example, divides myths into five categories – theological, physical (or concerning natural laws), animastic (or concerning soul), material and mixed. This last being those myths which show the interaction between two or more of the previous categories and which, he says, are particularly used in initiations.
To ones who are even trying to change content of the myth according to probability would be found criticism in Plato "Phaedrus" (229d), in which Socrates says that it is the province of one who is "vehemently curious and laborious, and not entirely happy . . .".
Although Plato famously condemned poetic myth when discussing the education of the young in the "Republic", primarily on the grounds that there was a danger that the young and uneducated might take the stories of Gods and heroes literally, nevertheless he constantly refers to myths of all kinds throughout his writings. As Platonism developed in the phases commonly called 'middle Platonism' and neoplatonism, such writers as Plutarch, Porphyry, Proclus, Olympiodorus and Damascius wrote explicitly about the symbolic interpretation of traditional and Orphic myths.
Interest in polytheistic mythology revived in the Renaissance, with early works on mythography appearing in the 16th century, such as the "Theologia mythologica" (1532).Myths are not the same as fables, legends, folktales, fairy tales, anecdotes, or fiction, but the concepts may overlap. Notably, during the nineteenth century period of Romanticism, folktales and fairy tales were perceived as eroded fragments of earlier mythology (famously by the Brothers Grimm and Elias Lönnrot). Mythological themes are also very often consciously employed in literature, beginning with Homer. The resulting work may expressly refer to a mythological background without itself being part of a body of myths (Cupid and Psyche). The medieval romance in particular plays with this process of turning myth into literature. Euhemerism refers to the process of rationalization of myths, putting themes formerly imbued with mythological qualities into pragmatic contexts, for example following a cultural or religious paradigm shift (notably the re-interpretation of pagan mythology following Christianization).
Conversely, historical and literary material may acquire mythological qualities over time, for example the Matter of Britain referring to the legendary history of Great Britain, especially those focused on King Arthur and the knights of the Round Table, and the Matter of France, based on historical events of the fifth and eighth centuries, respectively, were first made into epic poetry and became partly mythological over the following centuries. "Conscious generation" of mythology has been termed mythopoeia by J. R. R. Tolkien[16], and was notoriously also suggested, very separately, by Nazi ideologist Alfred Rosenberg.
19th-century theories.
The first scholarly theories of myth appeared during the second half of the 19th century. In general, these 19th-century theories framed myth as a failed or obsolete mode of thought, often by interpreting myth as the primitive counterpart of modern science.
For example, E. B. Tylor interpreted myth as an attempt at a literal explanation for natural phenomena: unable to conceive of impersonal natural laws, early man tried to explain natural phenomena by attributing souls to inanimate objects, giving rise to animism. According to Tylor, human thought evolves through various stages, starting with mythological ideas and gradually progressing to scientific ideas. Not all scholars — not even all 19th century scholars — have agreed with this view. For example, Lucien Lévy-Bruhl claimed that "the primitive mentality is a condition of the human mind, and not a stage in its historical development."
Max Müller called myth a "disease of language". He speculated that myths arose due to the lack of abstract nouns and neuter gender in ancient languages: anthropomorphic figures of speech, necessary in such languages, were eventually taken literally, leading to the idea that natural phenomena were conscious beings, gods.
The anthropologist James Frazer saw myths as a misinterpretation of magical rituals; which were themselves based on a mistaken idea of natural law. According to Frazer, man begins with an unfounded belief in impersonal magical laws. When he realizes that his applications of these laws don't work, he gives up his belief in natural law, in favor of a belief in personal gods controlling nature — thus giving rise to religious myths. Meanwhile, man continues practicing formerly magical rituals through force of habit, reinterpreting them as reenactments of mythical events. Finally, Frazer contends, man realizes that nature does follow natural laws, but now he discovers their true nature through science. Here, again, science makes myth obsolete: as Frazer puts it, man progresses "from magic through religion to science".
Robert Segal asserts that by pitting mythical thought against modern scientific thought, such theories implied that modern man must abandon myth.
20th-century theories.
Many 20th-century theories of myth rejected the 19th-century theories' opposition of myth and science. In general, "twentieth-century theories have tended to see myth as almost anything but an outdated counterpart to science […] Consequently, moderns are not obliged to abandon myth for science."
Swiss psychologist Carl Jung (1873–1961) tried to understand the psychology behind world myths. Jung asserted that all humans share certain innate unconscious psychological forces, which he called archetypes. Jung believed that the similarities between the myths from different cultures reveals the existence of these universal archetypes.
Joseph Campbell believed that there were two different orders of mythology: myths that "are metaphorical of spiritual potentiality in the human being," and myths "that have to do with specific societies".
Joseph Campbell's major work is "The Masks of God I-IV". In the first volume, "Primitive Mythology", he outlines clearly his intention:
In his fourth volume however he coins the phrase, "creative mythology", which he explains as:
Claude Lévi-Strauss believed that myths reflect patterns in the mind and interpreted those patterns more as fixed mental structures — specifically, pairs of opposites (i.e. good/evil, compassionate/callous) — than as unconscious feelings or urges.
In his appendix to "Myths, Dreams and Mysteries", and in "The Myth of the Eternal Return", Mircea Eliade attributed modern man’s anxieties to his rejection of myths and the sense of the sacred.
In the 1950s, Roland Barthes published a series of essays examining modern myths and the process of their creation in his book "Mythologies".
Comparative mythology.
Comparative mythology is the systematic comparison of myths from different cultures. It seeks to discover underlying themes that are common to the myths of multiple cultures. In some cases, comparative mythologists use the similarities between different mythologies to argue that those mythologies have a common source. This common source may be a common source of inspiration (e.g. a certain natural phenomenon that inspired similar myths in different cultures) or a common "protomythology" that diverged into the various mythologies we see today.
Nineteenth-century interpretations of myth were often highly comparative, seeking a common origin for all myths. However, modern-day scholars tend to be more suspicious of comparative approaches, avoiding overly general or universal statements about mythology. One exception to this modern trend is Joseph Campbell's book "The Hero With a Thousand Faces" (1949), which claims that all hero myths follow the same underlying pattern. This theory of a "monomyth" is out of favor with the mainstream study of mythology.
Modern mythology.
In modern society, myth is often regarded as historical or obsolete. Many scholars in the field of cultural studies are now beginning to research the idea that myth has worked itself into modern discourses. Modern formats of communication allow for widespread communication across the globe, thus enabling mythological discourse and exchange among greater audiences than ever before. Various elements of myth can now be found in television, cinema and video games.
Although myth was traditionally transmitted through the oral tradition on a small scale, the technology of the film industry has enabled filmmakers to transmit myths to large audiences via film dissemination (Singer, "Mythmaking: Philosophy in Film", 3–6). In the psychology of Carl Jung, myths are the expression of a culture or society’s goals, fears, ambitions and dreams (Indick, "Classical Heroes in Modern Movies: Mythological Patterns of the Superhero", 93–95). Film is ultimately an expression of the society in which it was credited, and reflects the norms and ideals of the time and location in which it is created. In this sense, film is simply the evolution of myth. The technological aspect of film changes the way the myth is distributed, but the core idea of the myth is the same.
The basis of modern storytelling in both cinema and television lies deeply rooted in the mythological tradition. Many contemporary and technologically advanced movies often rely on ancient myths to construct narratives. The Disney Corporation is notorious among cultural study scholars for "reinventing" traditional childhood myths (Koven, "Folklore Studies and Popular Film and Television: A Necessary Critical Survey", 176–195). While many films are not as obvious as Disney fairy tales in respect to the employment of myth, the plots of many films are largely based on the rough structure of the myth. Mythological archetypes such as the cautionary tale regarding the abuse of technology, battles between gods, and creation stories are often the subject of major film productions. These films are often created under the guise of cyberpunk action movies, fantasy dramas, and apocalyptic tales. Although the range of narratives, as well as the medium in which it is being told is constantly increasing, it is clear that myth continues to be a pervasive and essential component of the collective imagination (Cormer, "Narrative." Critical Ideas in Television Studies, 47–59.)
Recent films such as "Clash of the Titans", "Immortals", or "Thor" continue the trend of mining traditional mythology in order to directly create a plot for modern consumption.
With the invention of modern myths such as urban legends, the mythological traditional will carry on to the increasing variety of mediums available in the 21st century and beyond. The crucial idea is that myth is not simply a collection of stories permanently fixed to a particular time and place in history, but an ongoing social practice within every society.

</doc>
<doc id="19688" url="http://en.wikipedia.org/wiki?curid=19688" title="Mind map">
Mind map

A mind map is a diagram used to visually organize information. A mind map is often created around a single concept, drawn as an image in the center of a blank landscape page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.
Mind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available.
Mind maps are considered to be a type of spider diagram. A similar concept in the 1970s was "idea sun bursting".
Origins.
Although the term "mind map" was first popularized by British popular psychology author and television personality Tony Buzan, the use of diagrams that visually "map" information using branching and radial maps traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, brainstorming, memory, visual thinking, and problem solving by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by Porphyry of Tyros, a noted thinker of the 3rd century, as he graphically visualized the concept categories of Aristotle. Philosopher Ramon Llull (1235–1315) also used such techniques.
The semantic network was developed in the late 1950s as a theory to understand human learning and developed further by Allan M. Collins and M. Ross Quillian during the early 1960s. Mind maps are similar in radial structure to concept maps, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.
Popularisation of the term "mind map".
Buzan's specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called "Use Your Head". In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.
Buzan says the idea was inspired by Alfred Korzybski's general semantics as popularized in science fiction novels, such as those of Robert A. Heinlein and A. E. van Vogt. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of cerebral hemispheres in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.
Mind map guidelines.
Buzan suggests the following guidelines for creating mind maps:
Uses.
As with other diagramming tools, mind maps can be used to generate, visualize, structure, and classify ideas, and as an aid to studying and organizing information, solving problems, making decisions, medicine and writing.
Mind maps have many applications in personal, family, educational, and business situations, including notetaking, brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a mnemonic technique, or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.
In addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance expert search systems, search engines and search and tag query recommender. To do so, mind maps can be analysed with classic methods of information retrieval to classify a mind map's author or documents that are linked from within the mind map.
Research.
Effectiveness - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science". Other studies also report positive effects through the use of mind maps. Farrand, Hussain, and Hennessy (2002) found that spider diagrams (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about concept mapping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions". The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.
Features of Mind Maps - Beel & Langer (2011) conducted a comprehensive analysis of the content of mind maps. They analysed 19,379 mind maps from 11,179 users of the mind mapping applications SciPlore MindMapping (aka Docear) and MindMeister. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps.
Automatic Creating of Mind Maps - There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. Rothenberger et al. extracted the main story of a text and presented it as mind map. And there is a patent about automatically creating sub-topics in mind maps.
Pen and Paper vs Computer - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.
Tools.
Mind-mapping software can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images. It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional note-taking.

</doc>
<doc id="19690" url="http://en.wikipedia.org/wiki?curid=19690" title="Machine gun">
Machine gun

A machine gun is a fully automatic mounted or portable firearm, usually designed to fire bullets in quick succession from an ammunition belt or magazine, typically at a rate of three to eighteen hundred rounds per minute.
Fully automatic weapons are generally categorized as submachine guns, assault rifles, machine guns, or autocannons. Submachine guns are hand-held automatic weapons for personal defense or short-range combat firing pistol-caliber rounds. A machine gun is often portable to a certain degree, but is generally used when attached to a mount or fired from the ground on a bipod or tripod, and generally fires rifle cartridges and is capable of sustained fire. Light machine guns are small enough to be fired hand-held, but are more effective when fired from a prone position. The difference between machine guns and autocannons is based on caliber, with autocannons using calibers larger than 16 mm., and whether the gun fires conventional bullets or explosive rounds. Guns firing large-caliber explosive rounds are generally considered either autocannons or automatic grenade launchers ("grenade machine guns"). In contrast to submachine guns and autocannons, machine guns (like rifles) tend to have a very high ratio of barrel length to caliber (a long barrel for a small caliber); indeed, a true machine gun is essentially a fully automatic rifle, and often the primary criterion for a machine gun as opposed to an automatic rifle is the presence of a quick-change barrel or other cooling system. Automatic rifles and (more commonly) assault rifles may be capable of fully automatic fire, but are not designed for sustained fire.
In United States gun law, "machine gun" is a technical term for any fully automatic firearm, and also for any component or part that will modify an existing firearm such that it functions as a fully automatic firearm.
Overview of modern automatic machine guns.
Unlike semi-automatic firearms, which require one trigger pull per round fired, a machine gun is designed to fire for as long as the trigger is held down. Nowadays the term is restricted to relatively heavy weapons fired from some sort of support rather than hand-held, able to provide continuous or frequent bursts of automatic fire for as long as ammunition lasts. Machine guns are normally used against unprotected or lightly protected personnel, or to provide suppressive fire.
Some machine guns have in practice sustained fire almost continuously for hours; other automatic weapons overheat after less than a minute of use. Because they become very hot, practically all machine guns fire from an open bolt, to permit air cooling from the breech between bursts. They also have either a barrel cooling system, or removable barrels which allow a hot barrel to be replaced.
Although subdivided into "light", "medium", "heavy" or "general-purpose", even the lightest machine guns tend to be substantially larger and heavier than other automatic weapons. Squad automatic weapons (SAW) are a variation of light machine gun and require only one operator (sometimes with an assistant to carry ammunition). Medium and heavy machine guns are either mounted on a tripod or on a vehicle; when carried on foot, the machine gun and associated equipment (tripod, ammunition, spare barrels) require additional crew members.
According to U.S. Army doctrine, a machine gun is distinguished from an automatic rifle by how it is used: a machine gun is a crew-served weapon, while an automatic rifle is used by a single person. While most weapons are designed to be used exclusively in one manner or the other, FM 3-22.68 "Crew-Served Machine Guns", describes how the M249 can be used either as a machine gun or as an automatic rifle: "Both the M249 automatic rifle and the M249 machine gun are identical, but its employment is different. The M249 automatic rifle is operated by an automatic rifleman, but its ammunition may be carried by other Soldiers within the squad or unit. The M249 machine gun is a crew-served weapon."
The majority of machine guns are belt-fed, although some light machine guns are fed from drum or box magazines, and some vehicle-mounted machine guns are hopper-fed.
Other automatic weapons are subdivided into several categories based on the size of the bullet used, and whether the cartridge is fired from a positively locked closed bolt, or a non-positively locked open bolt. Full automatic firearms using pistol-caliber ammunition are called machine pistols or submachine guns largely on the basis of size. Selective fire rifles firing a full-power rifle cartridge from a closed bolt are called automatic rifles or battle rifles, while rifles that fire an intermediate cartridge are called assault rifles. The difference in construction was driven by the difference in intended deployment. Automatic rifles (such as the Browning Automatic Rifle) were designed to be a high duty cycle arm for support of other troops, and were often made and deployed with quick change barrel assemblies to allow quick replacement of over heated barrels to allow for continued fire, and may have been operated by both the person actually firing the weapon as well as an additional crewman to assist in providing and caring for ammunition and the barrels, similar to a reduced version of a squad weapon. The assault rifle generally was made for a more intermittent duty cycle, and was designed to be easily carried and used by a single person.
Assault rifles are a compromise between the size and weight of a pistol-caliber submachinegun and a full size traditional automatic rifle by firing intermediate cartridges and allowing semi-automatic, burst, or full-automatic fire options (selective fire), often with two or more of these available on the rifle at once.
In certain states, like California, certain weapons that resemble true assault rifles, but are only semi-automatic (autoloading), are categorized as assault weapons and possession by civilians is generally illegal. Supporters of gun rights generally consider this application of the phrase "assault weapon" to be a misnomer and this term is in fact seldom used outside of the United States for these civilian firearms.
The machine gun's primary role in modern ground combat is to provide suppressive fire on an opposing force's position, forcing the enemy to take cover and reducing the effectiveness of his fire. This either halts an enemy attack or allows friendly forces to attack enemy positions with less risk.
Light machine guns usually have simple iron sights. A common aiming system is to alternate solid ("ball") rounds and tracer ammunition rounds (usually one tracer round for every four ball rounds), so shooters can see the trajectory and "walk" the fire into the target, and direct the fire of other soldiers.
Many heavy machine guns, such as the Browning M2 .50 caliber machine gun, are accurate enough to engage targets at great distances. During the Vietnam War, Carlos Hathcock set the record for a long-distance shot at 7382 ft (2250 m) with a .50 caliber heavy machine gun he had equipped with a telescopic sight. This led to the introduction of .50 caliber anti-materiel sniper rifles, such as the Barrett M82.
Operation.
All machine guns follow a cycle:
Cycle is repeated as long as the trigger is activated by operator.
Releasing the trigger resets the trigger mechanism by engaging a sear so the weapon stops firing with bolt carrier fully at the rear.
The operation is basically the same for all semi automatic or automatic weapons, regardless of the means of activating these mechanisms. Some examples:
Firing a machine gun produces great amounts of heat. In a worst-case scenario this may cause a cartridge to fire even when the trigger is not pulled, potentially leading to damage or causing the gun to cycle its action and keep firing until it has exhausted its ammunition supply or jammed. To prevent this, some kind of cooling system is required. Early heavy machine guns were often water-cooled; while very effective, the water also added considerable weight to an already bulky design. Air-cooled machine guns feature quick-change barrels, often carried by a crew member. The higher the rate of fire, the more often barrels must be changed and allowed to cool. To minimize this, most air-cooled guns are fired only in short bursts or at a reduced rate of fire. Some designs - such as the many variants of the MG42 - are capable of rates of fire in excess of 1500 rounds per minute.
In weapons where the round seats and fires at the same time, mechanical timing is essential for operator safety, to prevent the round from firing before it is seated properly. Machine guns are controlled by one or more mechanical sears. When a sear is in place, it effectively stops the bolt at some point in its range of motion. Some sears stop the bolt when it is locked to the rear. Other sears stop the firing pin from going forward after the round is locked into the chamber. Almost all machine guns have a "safety" sear, which simply keeps the trigger from engaging.
History.
It would not be until the mid-19th century that successful machine-gun designs came into existence. The key characteristic of modern machine guns, their relatively high rate of fire and more importantly machine (automatic) loading, came with the Model 1862 Gatling gun, which was adopted by the United States Navy. These weapons were still powered by hand; however, this changed with Hiram Maxim's idea of harnessing recoil energy to power reloading in his Maxim machine gun. Dr. Gatling also experimented with electric-motor-powered models; this externally powered machine reloading has seen use in modern weapons as well. The Vandenburg and Miltrailleuse volley (organ) gun concepts have been revived partially in the early 21st century in the form of electronically controlled, multibarreled volley guns. It is important to note that what exactly constitutes a machine gun, and whether volley guns are a type of machine gun, and to what extent some earlier types of devices are considered to be like machine guns, is a matter of debate in many cases and can vary depending which language and exact definition is used.
Early rapid-firing weapons.
The first known ancestors of multi-shot weapons were early revolvers made in Europe in the late 1500s. One is a shoulder-gun-length weapon made in Nuremberg, Germany, circa 1580. Another is a revolving arquebus, produced by Hans Stopler of Nuremberg in 1597.
Another large, early repeating was created by James Puckle, a London lawyer, who patented what he called "The Puckle Gun" on May 15, 1718. It was a design for a 1 in. (25.4 mm) caliber, flintlock revolver cannon able to fire 9 rounds before reloading, intended for use on ships. According to Puckle, it was able to fire round bullets at Christians and square bullets at Turks. While ahead of its time, foreshadowing the designs of revolvers, it was not adopted or produced.
In 1777, Philadelphia gunsmith Joseph Belton offered the Continental Congress a "new improved gun", which was capable of firing up to twenty shots in five seconds, automatically, and was capable of being loaded by a cartridge. Congress requested that Belton modify 100 flintlock muskets to fire eight shots in this manner, but rescinded the order when Belton's price proved too high.
In the early and mid-19th century, a number of rapid-firing weapons appeared which offered multi-shot fire, and a number of semi-automatic weapons as well as volley guns. Volley guns (such as the Mitrailleuse) and double barreled pistols relied on duplicating all parts of the gun. Pepperbox pistols did away with needing multiple hammers but used multiple barrels. Revolvers further reduced this to only needing a pre-prepared magazine using the same barrel and ignitions. However, like the Puckle gun, they were still only semiautomatic.
The Agar Gun, otherwise known as a "coffee-mill gun" because of its resemblance to a coffee mill, was invented by Wilson Agar at the beginning of the US Civil War. The weapon featured automatic loading through ammunition being loaded in a hopper above the weapon. The weapon featured a single barrel and fired through the turning of a hand crank. The weapon was demonstrated to President Lincoln in 1861. He was so impressed with the weapon that he purchased 10 on the spot for $1,500 apiece. The Union Army eventually purchased a total of 54 of the weapons. However, due to antiquated views of the Ordnance Department the weapons, like its more famous counterpart the Gatling Gun, saw only limited use.
The Gatling gun, patented in 1861 by Richard Jordan Gatling, was the first to offer controlled, sequential fire with automatic loading. The design's key features were machine loading of prepared cartridges and a hand-operated crank for sequential high-speed firing. It first saw very limited action in the American Civil War; it was subsequently improved and used in the Franco-Prussian war and North-West Rebellion. Many were sold to other armies in the late 19th century and continued to be used into the early 20th century, until they were gradually supplanted by Maxim guns. Early multi-barrel guns were approximately the size and weight of contemporary artillery pieces, and were often perceived as a replacement for cannon firing grapeshot or canister shot. The large wheels required to move these guns around required a high firing position which increased the vulnerability of their crews. Sustained firing of gunpowder cartridges generated a cloud of smoke making concealment impossible until smokeless powder became available in the late 19th century. Gatling guns were targeted by artillery they could not reach and their crews were targeted by snipers they could not see. The Gatling gun was used most successfully to expand European colonial empires by killing warriors of non-industrialized societies.
Gatlings were the first widely used rapid-fire guns and, due to their multiple barrels, could offer more sustained fire than the first generation of air-cooled, recoil-operated machine guns. The weight, complexity, and resulting cost of the multibarrel design meant recoil-operated weapons, which could be made lighter and cheaper, would supplant them. Recoil-operated machine guns were light enough to be moved by one man, were easier to move through rough terrain, and could be fired from a lower, protected position. It would be another 50 years before the concept was again used to allow extremely high rates of fire, such as in miniguns, and automatic aircraft cannon.
The first self-powered machine gun was invented in 1884 by Sir Hiram Maxim. The "Maxim gun" used the recoil power of the previously fired bullet to reload rather than being hand-powered, enabling a much higher rate of fire than was possible using earlier designs such as the Nordenfelt and Gatling weapons. Maxim also introduced the use of water cooling, via a water jacket around the barrel, to reduce overheating. Maxim's gun was widely adopted and derivative designs were used on all sides during the First World War. The design required fewer crew and was lighter and more usable than the Nordenfelt and Gatling guns. First World War combat experience greatly increased the importance of the machine gun. The United States Army issued four machine guns per regiment in 1912, but that allowance increased to 336 machine guns per regiment by 1919.
Heavy guns based on the Maxim such as the Vickers machine gun were joined by many other machine weapons, which mostly had their start in the early 20th century such as the Hotchkiss machine gun. Submachine guns (e.g., the German MP18) as well as lighter machine guns (the Chauchat, for example) saw their first major use in World War I, along with heavy use of large-caliber machine guns. The biggest single cause of casualties in World War I was actually artillery, but combined with wire entanglements, machine guns earned a fearsome reputation. The automatic mechanisms of machine guns were applied to handguns, giving rise to automatic pistols (and eventually machine pistols) such as the Borchardt (1890s) and later submachine guns (such as the Beretta 1918). Machine guns were mounted in aircraft for the first time in World War I. Firing through a moving propeller was solved in a variety of ways, including the interrupter gear, metal reinforcement of the propeller, or simply avoiding the problem with wing-mounted guns or having a pusher propeller.
Interwar era and World War II.
During the interwar years, many new designs were developed, such as the Browning M2 and the Thompson sub-machine gun, which, along with others, were used in World War II. The trend toward automatic rifles, light machine guns, and more powerful sub-machine guns resulted in a wide variety of firearms that combined characteristics of ordinary rifles and machine guns. The Cei-Rigotti (20th century), Fedorov Avtomat (1910s), AVS-36 Simonov (1930s), MP44, M2 Carbine, AK-47, and M16 have come to be known as assault rifles (after the German term "sturmgewehr"). Many aircraft were equipped with machine cannon, and similar cannon (nicknamed "Pom-pom guns") were used as antiaircraft weapons. The designs of Bofors of Sweden and Oerlikon of Switzerland were widely used by both sides and have greatly influenced similar weapons developed since then.
Germany developed during the interwar years the first widely used and successful general-purpose machine gun, the Maschinengewehr 34. The Maschinengewehr 42 was developed from it and was much cheaper to produce. The current GPMG of the German Army, the MG3, is a direct evolution of the MG42. Many other modern machine guns, including the US M60 and the FN MAG borrow elements of the design of the MG42.
Future.
Conventional machine-gun development has been slowed by the fact that existing machine-gun designs are adequate for most purposes, although significant developments are taking place with regard to caseless ammunition, antiarmor and antimissile weapons.
Electronically controlled machine guns with ultrahigh rates of fire may see use in some applications, although current small-caliber weapons of this type have found little use: they are too light for anti-vehicle use, but too heavy (especially with the need to carry a tactically useful amount of ammunition) for individual soldiers. The trend towards higher reliability and lower mass for a given power will probably continue. Another example is the six barreled, 4000 round per minute, XM214 "six pack" developed by General Electric. It has a complex power train and weighs 85 pounds, factors which may, in some circumstances, militate against its deployment.
Metal Storm has developed a new type of machine gun, with rates of fire up to 1.62 million rounds per minute. The distinguishing features of this technology are the absence of ammunition feed and casing ejection systems (the only moving parts are the projectiles), and the electronic ignition of the propellant charges.
Human interface.
The most common interface on machine guns is a pistol grip and trigger. On earlier manual machine guns, the most common type was a hand crank. On externally powered machine guns, such as miniguns, an electronic button or trigger on a joystick is commonly used. Light machine guns often have a butt stock attached, while vehicle and tripod mounted machine guns usually have spade grips. In the late 20th century, scopes and other complex optics became more common as opposed to the more basic iron sights.
Loading systems in early manual machine guns were often from a hopper of loose (un-linked) cartridges. Manual-operated volley guns usually had to be reloaded manually all at once (each barrel reloaded by hand). With hoppers, the rounds could often be added while the weapon was firing. This gradually changed to belt-fed types. Belts were either held in the open by the person, or in a bag or box. Some modern vehicle machine guns used linkless feed systems however.
Modern machine guns are commonly mounted in one of four ways. The first is a bipod – often these are integrated with the weapon. This is common on light machine guns and some medium machine guns. Another is a tripod, where the person holding it does not form a 'leg' of support. Medium and heavy machine guns usually use tripods. On ships and aircraft machine guns are usually mounted on a pintle mount – basically a steel post that is connected to the frame. Tripod and pintle mounts are usually used with spade grips. The last major mounting type is one that is disconnected from humans, as part of an armament system, such as a tank coaxial or part of aircraft's armament. These are usually electrically fired and have complex sighting systems, for example the US Helicopter Armament Subsystems.

</doc>
<doc id="19692" url="http://en.wikipedia.org/wiki?curid=19692" title="Monopoly (game)">
Monopoly (game)

Monopoly is a board game that originated in the United States in 1903 as a way to demonstrate the evils of land ownership. The current version was published by Parker Brothers in 1935. Subtitled "The Fast-Dealing Property Trading Game", the game is named after the economic concept of monopoly—the domination of a market by a single entity. It is now produced by the United States game and toy company Hasbro. Players move around the gameboard buying or trading properties, developing their properties with houses and hotels, and collecting rent from their opponents, with the goal being to drive them all into bankruptcy leaving one monopolist in control of the entire economy.
History.
Early history.
The history of "Monopoly" can be traced back to 1903, when American Elizabeth (Lizzie) J. Magie Phillips created a game through which she hoped to be able to explain the single tax theory of Henry George. It was intended as an educational tool to illustrate the negative aspects of concentrating land in private monopolies. Magie took out a patent in 1904. Her game, "The Landlord's Game", was self-published, beginning in 1906. A series of variant board games based on her concept was developed from 1906 through the 1930s that involved the buying and selling of land and the development of that land.
Origin.
By 1933, a variation on "The Landlord's Game" called "Monopoly" was the basis of the game sold by Parker Brothers, beginning on 6 February 1935. Several people, mostly in the Midwestern United States and near the East Coast, contributed to the game's design and evolution, and this is when the game's design took on the 4×10 space-to-a-side layout and familiar cards were produced. The original version of the game in this format was based on streets in Atlantic City, New Jersey. By the 1970s, the false notion that the game had been created solely by Charles Darrow had become popular folklore: it was printed in the game's instructions.
1936–70.
In 1936, Parker Brothers began licensing the game for sale outside the United States. In 1941, the British Secret Intelligence Service had John Waddington Ltd., the licensed manufacturer of the game in the United Kingdom, create a special edition for World War II prisoners of war held by the Nazis. Hidden inside these games were maps, compasses, real money, and other objects useful for escaping. They were distributed to prisoners by British secret service-created fake charity groups.
1970s–80s.
Economics professor Ralph Anspach published a game "Anti-Monopoly" in 1973, and was sued for trademark infringement by Parker Brothers in 1974. The case went to trial in 1976. Anspach won on appeals in 1979, as the 9th Circuit Court determined that the trademark "Monopoly" was generic, and therefore unenforceable. The United States Supreme Court declined to hear the case, allowing the appellate court ruling to stand. This decision was overturned by the passage of Public Law 98-620 in 1984. With that law in place, Parker Brothers and its parent companies (Hasbro) continue to hold valid trademarks for the game "Monopoly".
1990s–present.
A new wave of licensed products began in 1994, when Hasbro granted a license to USAopoly to begin publishing a San Diego Edition of "Monopoly", which has since been followed by over 100 more. Other licensees include Winning Moves Games (since 1995) and Winning Solutions, Inc. (since 2000) in the United States. Winning Moves also has offices in the UK, France, Germany and Australia, and other licensees include AH Media in The Netherlands, and Bestman Games in Nigeria.
Board.
The "Monopoly" gameboard consists of 40 spaces containing 28 properties (22 colored streets, four railway stations and two utilities), three Chance spaces, three Community Chest spaces, a Luxury Tax space, an Income Tax space, and the four corner squares: GO, (In) Jail/Just Visiting, Free Parking, and Go to Jail.
US versions.
There have been some changes to the board since the original. Not all of the Chance and Community Chest cards as printed in the 1935 patent were used in editions from 1936/1937 onwards, and graphics with the Mr. Monopoly character were added in that same timeframe. A graphic of a chest containing coins was added to the Community Chest spaces, as were the flat purchase prices of all of the properties. Traditionally, the Community Chest cards were yellow (although they sometimes were printed on blue stock) with no decoration or text on the back, and the Chance cards were orange, likewise with no text or decoration on the back.
Hasbro commissioned a major redesign to the US Standard Edition of the game in 2008. Among the changes: the colors of Mediterranean and Baltic Avenues (which changed from purple to brown), the colors of the GO square (which changed from red to black), the adoption of a flat $200 Income Tax (formerly the player's choice of $200 or 10% of their total holdings, which they may not calculate until after making their final decision), and increased $100 Luxury Tax amount (upped from $75). There were also changes to the Chance and Community Chest cards; for example, the "poor tax" and "grand opera opening" cards became "speeding fine" and "it is your birthday", respectively; though their effects remained the same, and the player must pay only $50 instead of $150 for the school tax. In addition, a player now gets $50 instead of $45 for sale of stock, and the Advance to Illinois Avenue card now has the added text concerning a player collecting $200 should they pass Go on the way there. Similar color and amount changes are used in the US Edition of the "Here and Now: World Edition" game, and are also used in the most recent versions of the McDonald's Monopoly promotion.
All of the Chance and Community Chest cards received a graphic upgrade in 2008 as part of the graphic refresh of the game. Mr. Monopoly's classic line illustration was also now usually replaced by renderings of a 3D Mr. Monopoly model. The backs of the cards have their respective symbols, with Community Chest cards in blue, and Chance cards in orange.
In the US versions shown below, the properties are named after locations in (or near) Atlantic City, New Jersey.
Atlantic City's Illinois Avenue was renamed Martin Luther King Jr. Blvd. in the 1980s. St. Charles Place no longer exists, as the now-defunct Showboat Casino Hotel was developed where it once ran.
Standard (American Edition) Monopoly board layout as of September 2008 
<table 
border="3" cellspacing="0" cellpadding="3" style="text-align:center;font:normal 8pt/9pt arial;width:400px;border-collapse:separate; background-color:#F0FFF0;">
<tr>
<tr>
</table>
A similar online vote was held in early 2015 for an updated version of the game. The resulting board should be released worldwide in late 2015. Lima, Peru won the vote and will hold the Boardwalk space.
Deluxe editions.
Hasbro sells a "Deluxe Edition", which is mostly identical to the classic edition but has wooden houses and hotels and gold-toned tokens, including one token in addition to the standard eleven, a railroad locomotive. Other additions to the "Deluxe Edition" include a card carousel, which holds the title deed cards, and money printed with two colors of ink.
In 1978, retailer Neiman Marcus manufactured and sold an all-chocolate edition of "Monopoly" through its "Christmas Wish Book" for that year. The entire set was edible, including the money, dice, hotels, properties, tokens and playing board. The set retailed for $600.
In 2000, the FAO Schwarz store in New York City sold a custom version called "One-Of-A-Kind Monopoly" for $100,000. This special edition comes in a locking attaché case made with Napolino leather and lined in suede, and features include:
The "Guinness Book of World Records" states that a set worth $2,000,000 and made of 23-carat gold, with rubies and sapphires atop the chimneys of the houses and hotels, is the most expensive "Monopoly" set ever produced. This set was designed by artist Sidney Mobell to honor the game's 50th anniversary in 1985.
References.
Notes
Further reading.
Bibliography 

</doc>
<doc id="19693" url="http://en.wikipedia.org/wiki?curid=19693" title="Max Steiner">
Max Steiner

Maximilian Raoul "Max" Steiner (May 10, 1888 – December 28, 1971) was an Austrian-born American composer of music for theatre and films. He was a child prodigy who conducted his first operetta when he was twelve and became a full-time professional, either composing, arranging or conducting, when he was fifteen.
He worked in England, then Broadway, and moved to Hollywood in 1929 where he became one of the first composers to write music scores for films. Steiner is referred to as "the father of film music" and is considered one of the greatest film score composers in the history of cinema. Along with such composers as Dimitri Tiomkin, Franz Waxman, Erich Wolfgang Korngold, Alfred Newman and Miklós Rózsa, Steiner played a major part in creating the tradition of writing music for films.
Steiner composed over 300 film scores with RKO and Warner Brothers, and was nominated for 24 Academy Awards, winning three: "The Informer" (1935), "Now, Voyager" (1942), and "Since You Went Away" (1944). Besides his Oscar-winning scores, some of Steiner's popular works include "King Kong" (1933), "Little Women" (1933), "Jezebel" (1938), "Casablanca" (1942), and the film score for which he is possibly best known, "Gone with the Wind" (1939).
He was also the first recipient of the Golden Globe Award for Best Original Score, which he won for his score to "Life with Father". Steiner was a frequent collaborator with some of the most famous film directors in history, including Michael Curtiz, John Ford and William Wyler, and scored many of the films with Errol Flynn, Bette Davis, Humphrey Bogart and Fred Astaire. Many of his film scores are available as separate soundtrack recordings.
Early years.
Steiner was born on May 10, 1888, in Austria-Hungary, the only child of a wealthy business and theatrical family of Jewish heritage. He was named after his paternal grandfather, Maximilian Steiner (1830–1880), credited with first persuading Johann Strauss, Jr. to write for the theater. He was also the influential manager of Vienna's historic Theater an der Wien, recognized for staging celebrated works of theatre, opera and symphony since 1801.
His father was Gabor Steiner (1858–1944), Viennese impresario, carnival exposition manager, and inventor, responsible for building the giant Ferris wheel in the Prater, known as the Wiener Riesenrad. Steiner's mother was a dancer in stage productions put on by his grandfather. His godfather was the composer Richard Strauss.
His parents sent Steiner to the Vienna University of Technology, but he expressed little interest in scholastic subjects. He then enrolled in the Imperial Academy of Music, where, due to his precocious musical talents and private tutoring by Robert Fuchs and Gustav Mahler, he completed a four-year course in only one year. He studied various instruments including piano, organ, violin, double bass, and trumpet. He also had courses in harmony, counterpoint, and composition. For his early achievement he was awarded a gold medal by the academy. Steiner credits his family for inspiring his early musical abilities, and writes of his father:
He produced Offenbach and Gilbert and Sullivan and all the others. When I was twelve he let me conduct an American operetta, "The Belle of New York", by Gustave Kerker. Kerker happened to be in Vienna at the time and he asked my parents if he could take me back to America with him as a Boy Wonder.
Beginning music career.
Steiner first entered the world of professional music when he was fifteen, writing and conducting the operetta, ""The Beautiful Greek Girl". He recalls how this first opera led to other shows in other countries, which eventually took him to London:
I conducted the opening night, and the production ended up running for a year. Out of that came offers to conduct other shows, a couple of which took me to Moscow and Hamburg. In 1906 I accepted an offer from the British impresario George Edwardes to go to London to conduct Lehar's "The Merry Widow", and that was the start of eight years in England for me. I conducted all kinds of musicals at Daly's Theatre, the Adelphi, the Hippodrome, the London Pavilion and the Blackpool Winter Garden.
During his years in England Steiner wrote and conducted both theater productions and symphonies. But in 1914 World War I started and he was interned as an enemy alien. Fortunately, he was befriended by the Duke of Westminster who was a fan of his, and was given exit papers to go to America, although his money was impounded. He arrived in New York City in December, 1914, with only $32 to his name.
Broadway music (1914–1929).
Steiner soon acquired employment and worked in New York for the next fifteen years as a musical director, arranger, orchestrator, and conductor of Broadway productions. They included operettas and musicals written by Victor Herbert, Jerome Kern, Vincent Youmans, and George Gershwin, among others. Steiner's credits include: "George White's Scandals" (1922), "Lady, Be Good" (1924), and "Rosalie" (1928). His final production on Broadway was in 1929, "Sons O' Guns".
During this period, when orchestrating and conducting Harry Tierney's "Rio Rita" in 1927, Tierney himself later requested that RKO Pictures in Hollywood hire Steiner to work in their music production departments. William LeBaron, RKO's head of production, traveled to New York to watch Steiner conduct and was "greatly impressed," writes biographer Tony Thomas. Steiner's thirty-five musicians each played several instruments, making his "elaborate orchestration sound even richer," adds Thomas. "Obviously, here was a man Hollywood could use."
Hollywood film music (1929–1971).
"Symphony of Six Million" (1932).
Steiner accepted their offer and moved to California in 1929. Soon after arriving, he orchestrated the film version of the musical "Rio Rita" and another musical, "Dixiana" (1930), for which he received his first screen credit as an orchestrator. Later that year LeBaron made him director of RKO's new music production department.
By the end of 1930, however, Hollywood studios, including RKO, cut back dramatically on producing musicals, which they had continually released during the 1920s. Steiner’s next film was a Western, "Cimarron" (1931), the first film for which he wrote an original composition. He then worked on "Bird of Paradise", putting to music almost the entire 85-minute film. Film historian Ronald Haver emphasizes the value of the music to the film:
To underline the exotic, languorous feeling of the visual, he wrote original material, weaving it in and out of his own arrangements of traditional Hawaiian melodies. His use of the vibraphone, marimbas, ukeleles, and the steel guitar created an ambience of the South Seas that was so popular it would soon become a cliché.:79
In 1932 Steiner was asked by a new producer at RKO, David O. Selznick, to try to improve a film he had just completed, but was still not satisfied with, "Symphony of Six Million" (1932). Selznick asked him, "Do you think you could put some music behind this thing? I think it might help it. Just do one reel." Steiner composed a short segment which Selznick liked so much, he asked him to compose the theme and underscoring for the entire picture. According to Haver, Selznick was more than satisfied with the film, feeling that it gave a realistic view of Jewish family life and tradition, and was "on his list of pictures he was proudest of.":75
The film became a career turning point, notes Thomas: “The real start of Steiner the film composer was "Symphony of Six Million"." Steiner explains the film’s significance to the film industry:
Music until then had not been used very much for underscoring—the producers were afraid the audience would ask ‘Where’s the music coming from?’ Unless they saw an orchestra or a radio or phonograph. But with this picture we proved scoring would work.
According to Thomas, “It was Steiner more than any other composer who pioneered the use of original composition as background scoring for films.” From then on, a third to half of the success of most films was “attributed to the extensive use of music.”
"King Kong" (1933).
The score for "King Kong" (1933) became Steiner's breakthrough and brought his name to everyone’s attention. Actor and musician Oscar Levant later called the film "a symphony accompanied by a movie," and an expression of Steiner's mastery of "illuminating action with sound." According to music critic and writer Bruce Eder, many critics at the time attributed a quarter of the film's success to the music.
The studio’s bosses were initially skeptical about the need for an original score, however, since they disliked the film’s contrived special effects, they let Steiner try to improve the film with music. Steiner recalls, "they didn't want to waste any more money on it and told me to use old tracks." Subsequently, King Kong producer Merian C. Cooper "came to me and asked me to score it to the best of my ability and that he would pay the cost of the orchestra." Steiner took advantage of this offer and used an eighty-piece orchestra, explaining: "It was made for music. It was the kind of film that allowed you to do anything and everything, from weird chords and dissonances to pretty melodies."
Thomas concludes it was a valuable improvement:
It was worth every penny because it was his score that literally makes that film work. As soon as the audience hears that three-note theme—those three massive, darkly orchestrated descending chords—it knows it is in for a fantastic experience.
Haver agrees, stating that “music like this had never been heard in a film before.” He adds:
There had never been a score so ambitious and so perfectly attuned to the visuals; Steiner’s music for "King Kong" was and is a landmark of film scoring, as much responsible for the success of the film as Cooper’s imagination and O’Brien’s gifted animation.:113
The film quickly made Steiner one of the most respected names in Hollywood. He continued on as RKOs music director for two more years, until 1936, during which time he composed, arranged and conducted another 55 films, from dramas to musicals. Among those were most of the Fred Astaire and Ginger Rogers dance musicals. He wrote a sonata used in Katharine Hepburn’s first film, "Bill of Divorcement" (1932).
RKO producers, including Selznick, often came to him when they had problems with films, treating him as if he were a “doctor,” notes Thomas. Steiner was asked to compose a needed score for "Of Human Bondage" (1934), which originally lacked music. He took that on, including adding musical touches to significant scenes, one of which conductor Leopold Stokowski called “a stroke of genius.” Director John Ford then called on him to score his film, “ "The Lost Patrol" (1934), which all the heads of RKO felt lacked enough “tension.” Steiner's composition was nominated for an Academy Award.
"The Informer" (1935).
Having now witnessed the value of music to films, Ford again hired Steiner to compose for his next film, ‘’ "The Informer" (1935), this time before he actually began production. Ford even asked his screenwriter to meet with Steiner during the writing phase. Ford’s preparation paid off, as the film was nominated for six Academy Awards and won four, including Steiner's first.
Producer David O. Selznick had set up his own production company in 1936 and “the only composer he wanted was Steiner,” notes Thomas. Steiner wrote the scores for Selznick's next three films.
Composing for Warner Brothers.
In April 1937, Steiner left RKO and signed a long-term contract with Warner Bros., but could continue to work for Selznick. The first of 140 films he would score for Warners was "The Charge of the Light Brigade" (1936). The film starred Errol Flynn and Olivia de Havilland, and over the coming years, Steiner would score fourteen more of Flynn’s pictures. Steiner also scored most of Bette Davis’s romantic dramas, eighteen in all. "When Bette Davis walked into a room, they wanted audiences to know that "Bette Davis" had walked into a room," writes composer Charles Strouse. Davis would later claim that "Max understood more about drama than any of us."
According to Eder, Warner Bros. made a special effort to have better music than the other studios. Jack Warner along with the other studio heads liked Steiner's scores and wanted to give their audiences "full value for their box office dollar." Eder writes:
That meant that, as movies were supposed to have better music than they did in the early thirties, Warner Bros. movies would have the best music. Steiner made it all possible—in fact, Steiner was one of the creative forces that made Warner Bros. fully competitive with better-heeled rivals such as M-G-M and 20th Century-Fox. M-G-M, in particular, may have had more working capital overall, and an awesome array of stars, . . . . but Warner Bros. had the good sense to hire Steiner, along with his fellow Viennese Erich Wolfgang Korngold. They made Warner Bros.' film scores sound as finely wrought and well constructed as M-G-M's best movies looked. By the middle-late 1930's, every studio wanted its own Max Steiner.
Steiner became a mainstay at Warner Bros., scoring 140 of their films over the next 30 years. He remained with Warners longer than any of his contemporaries. There are numerous soundtrack recordings of Steiner’s music, both as soundtracks, collections, and recordings by others. As late as 1959, for "A Summer Place", 71-year-old Steiner composed a theme which became one of Warner Brothers’ biggest hit-tunes for years and a re-recorded pop standard.
"Gone with the Wind" (1939).
In 1939, Steiner was borrowed from Warner Bros. by Selznick to compose the score for his next film, "Gone with the Wind" (1939), which became one of Steiner's most notable successes. Steiner was the only composer Selznick would consider for scoring the film, states Thomas. Despite 1939 being Steiner’s peak year for the number of scores he composed—twelve films in all—he was given only three months to do it. When the film was released, it was the longest film score ever composed, at nearly three hours. The composition consisted of 16 main themes and almost 300 musical segments. To meet the deadline, Steiner sometimes worked for 20-hours straight, taking Benzedrine pills to stay awake.
Selznick had asked Steiner only to use pre-existing classical music to help cut down on cost and time. But Steiner tried to convince him that filling the picture with swatches of classic concert music or popular works would not be as effective as an original score, which could be used to heighten the emotional content of scenes. In a letter to Steiner, Selznick expressed his opinion:
I am increasingly depressed by the prospect that we are not going to use the great classical music of the world for our score, . . . . I should like you to use, instead of two or three hours of original music, little original music . . . :227
Nevertheless, Steiner ignored his wishes and composed an entirely new score. The film went on to win 10 Academy Awards, although not for the best original score, which instead went to Herbert Stothart for the musical "The Wizard of Oz". The film’s theme song, however, "Tara’s Theme," is currently one of the “most easily recognizable motifs in the history of film music.” The score is ranked #2 by AFI as the second greatest American film score of all time. As a result, music and film historian Christopher Palmer claims it has "almost become a part of Americana."
Selznick’s opinion about using original scoring may have changed due to the overwhelming reaction to the film, nearly all of which contained Steiner’s music. In a letter he wrote a year later to the director of the Museum of Modern Art Film Library, Selznick now emphasizes the value of original film scores:
Important musicians are finally coming to recognize their importance; and more and more, really fine musicians are recognizing that scoring is a new form of musical art. . . . I believe that it is high time that someone gave encouragement to the training of musicians for the express purpose of scoring.:308
"Now Voyager" (1942).
Steiner received his next Oscar nomination for the 1940 film, "The Letter, his first of several collaborations with legendary director William Wyler. A further nomination followed the next year for "Sergeant York", and was also nominated for "Casablanca" (1942), which remains one of his most famous scores. He also composed two more Humphrey Bogart films besides "Casablanca", which is considered among Bogart’s best: "The Big Sleep" (1946) and "The Treasure of the Sierra Madre" (1948). In 1942, Steiner won his second Oscar for "Now, Voyager" (1942), one of his favorite scores, and received his third and final Oscar in 1944 with "Since You Went Away" (1944), set during World War II.
Palmer, however, feels that the “best Steiner is to be heard in those films whose tenor is light-years removed from the synthetic glamour and hyper-sophistication of the "Now Voyager" genre”, and explains why:
Steiner’s warm and spontaneous melodic gift and the naturally unsophisticated character of his musical personality lent themselves without any sense of strain to films dealing with the lyric and the homespun, tales of simple people in rural American settings.
He lists and describes some films whose music expresses the “beauties of the American landscape:” "This is Cinerama" (1952), with its natural settings such as the Grand Canyon, “Steiner’s music helps burn the moment into the memory;” the theme to "Spencer's Mountain" (1963), which Steiner composed when he was 75-years of age, “is imbued with that sense of childlike wonder which is the trademark of a perennially youthful creative imagination.” Palmer describes the effect of the score:
The main titles and their music are wedded to some splendid panoramic shots of the valley . . . this theme [with a musical quote from "America the Beautiful"] in its main-title setting reflects both the tranquility and beauty of lake, wood and mountain, and the peaceful temper of life in this uncorrupted area.
Westerns.
In "The Adventures of Mark Twain", and Westerns like "Dodge City" (1939) and "The Oklahoma Kid" (1939). Palmer notes that “their sense of grandeur may be romantic-European rather than authentic-American, but it is genuine nonetheless.” Even when he was in his seventies, ailing and near blind, he adds that Steiner’s compositions “revealed a freshness and fertility of invention,” which was “astonishing.” He would eventually write the scores for over twenty large-scale Westerns, most with epic-inspiring scores “about empire building and progress.” Thomas feels that "Dodge City", starring Errol Flynn and Olivia de Havilland, is a good example of Steiner’s handling of typical scenes of the Western genre:
Steiner bursts into an expansive, lilting, loping melody that bespeaks the glorious visual of the wagons and the horses and the cattle making their way across a handsome landscape. Throughout the whole film, whatever the drama or the comedy, the music picks up the picture and carries it.
Combining both Westerns and romance, Thomas asserts that for "They Died with Their Boots On" (1941), also starring Flynn and de Havilland, “Steiner’s love theme for the two is exquisite, perhaps his love theme par excellence . . .
Steiner reunited with director John Ford in 1956 to score "The Searchers", widely considered the greatest Western ever made. He returned to Warner-Bros in 1958 (although his contract ended in 1953) and scored several films, in addition a rare venture into television.[5] He continued to score films produced by Warner until the mid sixties.[6]
Steiner's pace slowed significantly in the mid-1950s, and he began freelancing. In 1954, RCA Victor asked Steiner to prepare and conduct an orchestral suite of music from "Gone with the Wind" for a special LP, which was later issued on CD. There are also acetates of Steiner conducting the Warner Brothers studio orchestra in music from some of his film scores.
In 1963, Steiner began writing his autobiography, which, although completed, was never published, and is the source of a few biographical errors concerning this composer. A copy of the manuscript resides with the rest of the Max Steiner Collection at Brigham Young University in Provo, Utah.
Methods of composing.
Steiner explains that in the early days of sound, producers avoided underscoring of music behind dialog, feeling that the audience would wonder where the music was coming from. As a result, he notes that “they began to add a little music here and there to support love scenes or silent sequences.” But in scenes where music might be expected, such as a night club, ballroom or theater, the orchestra fit in more naturally and was used often.
However, because half of the music was recorded on the set, Steiner says it led to a great deal of inconvenience and cost when scenes were later edited, as the score would often be ruined. As recording technology improved during this period, he was then able to record the music synced to the film and could change the score after the film was edited. Steiner explains his own typical method of scoring:
When a picture is finished and finally edited, it is turned over to me. Then I time it: not by stop watch, however, as many do. I have the film put through a special measuring machine and then a cue sheet created which gives me the exact time, to a split second, in which an action takes place, or a word is spoken. While these cue sheets are being made, I begin to work on themes for the different characters and scenes, but without regard to the required timing. During this period I also digest what I have seen, and try to plan the music for this picture.<br><br>
There may be a scene that is played a shade too slowly which I might be able to quicken with a little animated music; or, to a scene that is too fast, I may be able to give a little more feeling by using slower music. Or perhaps the music can clarify a character’s emotion, such as intense suffering, which is not demanded or fully revealed by a silent close-up.
Steiner also explains his own reasoning why he went against Selznick’s instruction to use classic music for "Gone With the Wind":
It is my conviction that familiar music, however popular, does not aid the underlying score of a dramatic picture. I believe that, while the American people are more musically minded than any other nation in the world, they are still not entirely familiar with all the old and new masters’ works. . . . Of course there are many in our industry who disagree with my viewpoint.
Palmer explains that scores from the classics were sometimes harmful to a picture, especially when they drew “unwanted attention to themselves by virtue of their familiarity, and gives examples of films like "2001 – A Space Odyssey", "The Sting" and "Manhattan", whose scores were easily recognized instead of having a preferred "subliminal" effect. Steiner, among the first to acknowledge “the need for an original, specially conceived dramatic musical support for each picture,” states Palmer led to this “major breakthrough” in movie production. He writes:
It was also a challenge of the first magnitude, for it offered musicians the opportunity to develop what was in effect a completely new and untried medium. . . . Somehow it happened; and in the work of Max Steiner—the father of them all—is contained a fair cross-section of the best and worst of Hollywood music. . . . One of Steiner’s most positive assets is his ability to crystallize the essence of a film in a single theme.
Steiner felt that knowing when to start and stop is the hardest part of proper scoring, as incorrect placement of music can speed up a scene meant to be slow and vice versa: "Knowing the difference is what makes a film composer." He also notes that many composers, contrary to his own technique, would fail to subordinate the music to the film:
I've always tried to subordinate myself to the picture. A lot of composers make the mistake of thinking of film as a concert platform on which they can show off. This is not the place. . . . If you get too decorative, you lose your appeal to the emotions. My theory is that the music should be felt rather than heard.
Character themes.
One of the important principles that guided Steiner whenever possible was his rule: "Every character should have a theme."
“As so often, with a few well-chosen brush strokes, Steiner creates a musical picture that tells us all we need to know about the character,” writes Palmer. Among the ways Steiner accomplished this, says music historian Kathryn Kalinak, was with “a high degree of direct synchronization between music and narrative action and the use of leitmotif," a constantly recurring musical phrase, as a structural framework.
Bette Davis's claim that "Max understood more about drama than any of us," is best exemplified by his score for "The Glass Menagerie" (1950), according to Palmer, who gives some examples of how the characters and the music worked together:
Another film which exemplifies the synchronizing of character and music is "The Fountainhead" (1949):
The character of Roark, an idealist architect (played by Gary Cooper):
Steiner’s theme for the hero is fraught with a true emotion and a genuine idealism and aspiration. It surges upward in ‘masculine’ style, whilst Roark’s mistress’s theme wends downwards in curves of typically feminine shapeliness. . . . He above, she traveling up in the workmen’s elevator: the music seems to draw them together in mutual fulfillment. . . . The score brings dignity and grandeur to the picture.”
Scene and situation themes.
In the same way that Steiner created a theme for each character in a film, Palmer shows how Steiner's music developed themes to express emotional aspects of general scenes, where the emotional content would have been missing without music. He gives a few examples:
Death.
Steiner died of congestive heart failure in Hollywood, aged 83. He is entombed in the Great Mausoleum at Forest Lawn Memorial Park Cemetery in Glendale, California.
AFI.
The American Film Institute respectively ranked Steiner's scores for "Gone with the Wind" (1939) and "King Kong" (1933) #2 and #13 on their list of the 25 greatest film scores. His scores for the following films were also nominated for the list:

</doc>
<doc id="19694" url="http://en.wikipedia.org/wiki?curid=19694" title="Mercury (planet)">
Mercury (planet)

Mercury is the smallest and closest to the Sun of the eight planets in the Solar System, with an orbital period of about 88 Earth days. Seen from Earth, it appears to move around its orbit in about 116 days, which is much faster than any other planet in the Solar System. It has no known natural satellites. The planet is named after the Roman deity Mercury, the messenger to the gods.
Because it has almost no atmosphere to retain heat, Mercury's surface experiences the greatest temperature variation of the planets in the Solar System, ranging from 100 K at night to 700 K during the day at some equatorial regions. The poles are constantly below 180 K. Mercury's axis has the smallest tilt of any of the Solar System's planets (about 1⁄30 of a degree), but it has the largest orbital eccentricity. At aphelion, Mercury is about 1.5 times as far from the Sun as it is at perihelion. Mercury's surface is heavily cratered and similar in appearance to the Moon, indicating that it has been geologically inactive for billions of years.
Mercury is gravitationally locked and rotates in a way that is unique in the Solar System. As seen relative to the fixed stars, it rotates on its axis exactly three times for every two revolutions it makes around the Sun. As seen from the Sun, in a frame of reference that rotates with the orbital motion, it appears to rotate only once every two Mercurian years. An observer on Mercury would therefore see only one day every two years.
Because Mercury orbits the Sun within Earth's orbit (as does Venus), it can appear in Earth's sky in the morning or the evening, but not in the middle of the night. Also, like Venus and the Moon, it displays a complete range of phases as it moves around its orbit relative to Earth. Although Mercury can appear as a bright object when viewed from Earth, its proximity to the Sun makes it more difficult to see than Venus. Two spacecraft have visited Mercury: "Mariner 10" flew by in the 1970s; and "MESSENGER", launched in 2004, orbited Mercury over 4,000 times in four years, before exhausting its fuel and crashing into the planet's surface on April 30, 2015.
Internal structure.
Mercury is one of four terrestrial planets in the Solar System, and is a rocky body like Earth. It is the smallest planet in the Solar System, with an equatorial radius of 2439.7 km. Mercury is also smaller—albeit more massive—than the largest natural satellites in the Solar System, Ganymede and Titan. Mercury consists of approximately 70% metallic and 30% silicate material. Mercury's density is the second highest in the Solar System at 5.427 g/cm3, only slightly less than Earth's density of 5.515 g/cm3. If the effect of gravitational compression were to be factored out, the materials of which Mercury is made would be denser, with an uncompressed density of 5.3 g/cm3 versus Earth's 4.4 g/cm3.
Mercury's density can be used to infer details of its inner structure. Although Earth's high density results appreciably from gravitational compression, particularly at the core, Mercury is much smaller and its inner regions are not as compressed. Therefore, for it to have such a high density, its core must be large and rich in iron.
Geologists estimate that Mercury's core occupies about 42% of its volume; for Earth this proportion is 17%. Research published in 2007 suggests that Mercury has a molten core. Surrounding the core is a 500–700 km mantle consisting of silicates. Based on data from the "Mariner 10" mission and Earth-based observation, Mercury's crust is believed to be 100–300 km thick. One distinctive feature of Mercury's surface is the presence of numerous narrow ridges, extending up to several hundred kilometers in length. It is believed that these were formed as Mercury's core and mantle cooled and contracted at a time when the crust had already solidified.
Mercury's core has a higher iron content than that of any other major planet in the Solar System, and several theories have been proposed to explain this. The most widely accepted theory is that Mercury originally had a metal-silicate ratio similar to common chondrite meteorites, thought to be typical of the Solar System's rocky matter, and a mass approximately 2.25 times its current mass. Early in the Solar System's history, Mercury may have been struck by a planetesimal of approximately 1/6 that mass and several thousand kilometers across. The impact would have stripped away much of the original crust and mantle, leaving the core behind as a relatively major component. A similar process, known as the giant impact hypothesis, has been proposed to explain the formation of the Moon.
Alternatively, Mercury may have formed from the solar nebula before the Sun's energy output had stabilized. It would initially have had twice its present mass, but as the protosun contracted, temperatures near Mercury could have been between 2,500 and 3,500 K and possibly even as high as 10,000 K. Much of Mercury's surface rock could have been vaporized at such temperatures, forming an atmosphere of "rock vapor" that could have been carried away by the solar wind.
A third hypothesis proposes that the solar nebula caused drag on the particles from which Mercury was accreting, which meant that lighter particles were lost from the accreting material and not gathered by Mercury. Each hypothesis predicts a different surface composition, and two space missions, "MESSENGER" and "BepiColombo", both will make observations to test them. "MESSENGER" has found higher-than-expected potassium and sulfur levels on the surface, suggesting that the giant impact hypothesis and vaporization of the crust and mantle did not occur because potassium and sulfur would have been driven off by the extreme heat of these events. The findings would seem to favor the third hypothesis; however, further analysis of the data is needed.
Surface geology.
Mercury's surface is similar in appearance to that of the Moon, showing extensive mare-like plains and heavy cratering, indicating that it has been geologically inactive for billions of years. Because knowledge of Mercury's geology has been based only on the 1975 "Mariner" flyby and terrestrial observations, it is the least understood of the terrestrial planets. As data from "MESSENGER" orbiter is processed, this knowledge will increase. For example, an unusual crater with radiating troughs has been discovered that scientists called "the spider". It later received the name Apollodorus.
Albedo features are areas of markedly different reflectivity, as seen by telescopic observation. Mercury has dorsa (also called "wrinkle-ridges"), Moon-like highlands, montes (mountains), planitiae (plains), rupes (escarpments), and valles (valleys).
Names for features on Mercury come from a variety of sources. Names coming from people are limited to the deceased. Craters are named for artists, musicians, painters, and authors who have made outstanding or fundamental contributions to their field. Ridges, or dorsa, are named for scientists who have contributed to the study of Mercury. Depressions or fossae are named for works of architecture. Montes are named for the word "hot" in a variety of languages. Plains or planitiae are named for Mercury in various languages. Escarpments or rupēs are named for ships of scientific expeditions. Valleys or valles are named for radio telescope facilities.
Mercury was heavily bombarded by comets and asteroids during and shortly following its formation 4.6 billion years ago, as well as during a possibly separate subsequent episode called the late heavy bombardment that came to an end 3.8 billion years ago. During this period of intense crater formation, the planet received impacts over its entire surface, facilitated by the lack of any atmosphere to slow impactors down. During this time the planet was volcanically active; basins such as the Caloris Basin were filled by magma, producing smooth plains similar to the maria found on the Moon.
Data from the October 2008 flyby of "MESSENGER" gave researchers a greater appreciation for the jumbled nature of Mercury's surface. Mercury's surface is more heterogeneous than either Mars's or the Moon's, both of which contain significant stretches of similar geology, such as maria and plateaus.
Impact basins and craters.
Craters on Mercury range in diameter from small bowl-shaped cavities to multi-ringed impact basins hundreds of kilometers across. They appear in all states of degradation, from relatively fresh rayed craters to highly degraded crater remnants. Mercurian craters differ subtly from lunar craters in that the area blanketed by their ejecta is much smaller, a consequence of Mercury's stronger surface gravity. According to IAU rules, each new crater must be named after an artist that was famous for more than fifty years, and dead for more than three years, before the date the crater is named.
The largest known crater is Caloris Basin, with a diameter of 1,550 km. The impact that created the Caloris Basin was so powerful that it caused lava eruptions and left a concentric ring over 2 km tall surrounding the impact crater. At the antipode of the Caloris Basin is a large region of unusual, hilly terrain known as the "Weird Terrain". One hypothesis for its origin is that shock waves generated during the Caloris impact traveled around the planet, converging at the basin's antipode (180 degrees away). The resulting high stresses fractured the surface. Alternatively, it has been suggested that this terrain formed as a result of the convergence of ejecta at this basin's antipode.
Overall, about 15 impact basins have been identified on the imaged part of Mercury. A notable basin is the 400 km wide, multi-ring Tolstoj Basin that has an ejecta blanket extending up to 500 km from its rim and a floor that has been filled by smooth plains materials. Beethoven Basin has a similar-sized ejecta blanket and a 625 km diameter rim. Like the Moon, the surface of Mercury has likely incurred the effects of space weathering processes, including Solar wind and micrometeorite impacts.
Plains.
There are two geologically distinct plains regions on Mercury. Gently rolling, hilly plains in the regions between craters are Mercury's oldest visible surfaces, predating the heavily cratered terrain. These inter-crater plains appear to have obliterated many earlier craters, and show a general paucity of smaller craters below about 30 km in diameter.
Smooth plains are widespread flat areas that fill depressions of various sizes and bear a strong resemblance to the lunar maria. Notably, they fill a wide ring surrounding the Caloris Basin. Unlike lunar maria, the smooth plains of Mercury have the same albedo as the older inter-crater plains. Despite a lack of unequivocally volcanic characteristics, the localisation and rounded, lobate shape of these plains strongly support volcanic origins. All the smooth plains of Mercury formed significantly later than the Caloris basin, as evidenced by appreciably smaller crater densities than on the Caloris ejecta blanket. The floor of the Caloris Basin is filled by a geologically distinct flat plain, broken up by ridges and fractures in a roughly polygonal pattern. It is not clear whether they are volcanic lavas induced by the impact, or a large sheet of impact melt.
One unusual feature of the planet's surface is the numerous compression folds, or rupes, that crisscross the plains. As the planet's interior cooled, it may have contracted and its surface began to deform, creating these features. The folds can be seen on top of other features, such as craters and smoother plains, indicating that the folds are more recent. Mercury's surface is flexed by significant tidal bulges raised by the Sun—the Sun's tides on Mercury are about 17 times stronger than the Moon's on Earth.
Volcanology.
Images obtained by the "MESSENGER" spacecraft have revealed evidence for pyroclastic flows on Mercury from low-profile shield volcanoes. "MESSENGER" data has helped identify 51 pyroclastic deposits on the surface, where 90% of them are found within impact craters. A study of the degradation state of the impact craters that host pyroclastic deposits suggests that pyroclastic activity occurred on Mercury over a prolonged interval.
A 'rimless depression’ inside the southwest rim of the Caloris Basin consists of at least nine overlapping volcanic vents, each individually up to 8 km in diameter. It is thus a 'compound volcano'. The vent floors are at a least 1 km below their brinks and they bear a closer resemblance to volcanic craters sculpted by explosive eruptions or modified by collapse into void spaces created by magma withdrawal back down into a conduit. The scientists could not quantify the age of the volcanic complex system, but reported that it could be of the order of a billion years.
Surface conditions and exosphere.
The surface temperature of Mercury ranges from 100 K to 700 K at the most extreme places: 0°N, 0°W, or 180°W. It never rises above 180 K at the poles,
due to the absence of an atmosphere and a steep temperature gradient between the equator and the poles. The subsolar point reaches about 700 K during perihelion (0°W or 180°W), but only 550 K at aphelion (90° or 270°W).
On the dark side of the planet, temperatures average 110 K.
The intensity of sunlight on Mercury's surface ranges between 4.59 and 10.61 times the solar constant (1,370 W·m−2).
Although the daylight temperature at the surface of Mercury is generally extremely high, observations strongly suggest that ice (frozen water) exists on Mercury. The floors of deep craters at the poles are never exposed to direct sunlight, and temperatures there remain below 102 K; far lower than the global average. Water ice strongly reflects radar, and observations by the 70-meter Goldstone telescope and the VLA in the early 1990s revealed that there are patches of high radar reflection near the poles. Although ice was not the only possible cause of these reflective regions, astronomers believed it was the most likely.
The icy regions are believed to contain about 1014–1015 kg of ice, and may be covered by a layer of regolith that inhibits sublimation. By comparison, the Antarctic ice sheet on Earth has a mass of about 4×1018 kg, and Mars's south polar cap contains about 1016 kg of water. The origin of the ice on Mercury is not yet known, but the two most likely sources are from outgassing of water from the planet's interior or deposition by impacts of comets.
Mercury is too small and hot for its gravity to retain any significant atmosphere over long periods of time; it does have a tenuous surface-bounded exosphere containing hydrogen, helium, oxygen, sodium, calcium, potassium and others. This exosphere is not stable—atoms are continuously lost and replenished from a variety of sources. Hydrogen and helium atoms probably come from the solar wind, diffusing into Mercury's magnetosphere before later escaping back into space. Radioactive decay of elements within Mercury's crust is another source of helium, as well as sodium and potassium. "MESSENGER" found high proportions of calcium, helium, hydroxide, magnesium, oxygen, potassium, silicon and sodium. Water vapor is present, released by a combination of processes such as: comets striking its surface, sputtering creating water out of hydrogen from the solar wind and oxygen from rock, and sublimation from reservoirs of water ice in the permanently shadowed polar craters. The detection of high amounts of water-related ions like O+, OH−, and H2O+ was a surprise. Because of the quantities of these ions that were detected in Mercury's space environment, scientists surmise that these molecules were blasted from the surface or exosphere by the solar wind.
Sodium, potassium and calcium were discovered in the atmosphere during the 1980–1990s, and are believed to result primarily from the vaporization of surface rock struck by micrometeorite impacts including presently from Comet Encke. In 2008, magnesium was discovered by "MESSENGER" probe. Studies indicate that, at times, sodium emissions are localized at points that correspond to the planet's magnetic poles. This would indicate an interaction between the magnetosphere and the planet's surface.
On November 29, 2012, NASA confirmed that images from "MESSENGER" had detected that craters at the north pole contained water ice. Sean C. Solomon was quoted in the "New York Times" as estimating the volume of the ice as large enough to "encase Washington, D.C., in a frozen block two and a half miles deep".
Magnetic field and magnetosphere.
Despite its small size and slow 59-day-long rotation, Mercury has a significant, and apparently global, magnetic field. According to measurements taken by "Mariner 10", it is about 1.1% the strength of Earth's. The magnetic-field strength at Mercury's equator is about 300 nT. Like that of Earth, Mercury's magnetic field is dipolar. Unlike Earth, Mercury's poles are nearly aligned with the planet's spin axis. Measurements from both the "Mariner 10" and "MESSENGER" space probes have indicated that the strength and shape of the magnetic field are stable.
It is likely that this magnetic field is generated by a dynamo effect, in a manner similar to the magnetic field of Earth. This dynamo effect would result from the circulation of the planet's iron-rich liquid core. Particularly strong tidal effects caused by the planet's high orbital eccentricity would serve to keep the core in the liquid state necessary for this dynamo effect.
Mercury's magnetic field is strong enough to deflect the solar wind around the planet, creating a magnetosphere. The planet's magnetosphere, though small enough to fit within Earth, is strong enough to trap solar wind plasma. This contributes to the space weathering of the planet's surface. Observations taken by the "Mariner 10" spacecraft detected this low energy plasma in the magnetosphere of the planet's nightside. Bursts of energetic particles were detected in the planet's magnetotail, which indicates a dynamic quality to the planet's magnetosphere.
During its second flyby of the planet on October 6, 2008, "MESSENGER" discovered that Mercury's magnetic field can be extremely "leaky". The spacecraft encountered magnetic "tornadoes" – twisted bundles of magnetic fields connecting the planetary magnetic field to interplanetary space – that were up to 800 km wide or a third of the radius of the planet. These "tornadoes" form when magnetic fields carried by the solar wind connect to Mercury's magnetic field. As the solar wind blows past Mercury's field, these joined magnetic fields are carried with it and twist up into vortex-like structures. These twisted magnetic flux tubes, technically known as flux transfer events, form open windows in the planet's magnetic shield through which the solar wind may enter and directly impact Mercury's surface.
The process of linking interplanetary and planetary magnetic fields, called magnetic reconnection, is common throughout the cosmos. It occurs in Earth's magnetic field, where it generates magnetic tornadoes as well. The "MESSENGER" observations show the reconnection rate is ten times higher at Mercury. Mercury's proximity to the Sun only accounts for about a third of the reconnection rate observed by "MESSENGER".
Orbit, rotation, and longitude.
Mercury has the most eccentric orbit of all the planets; its eccentricity is 0.21 with its distance from the Sun ranging from 46000000 to. It takes 87.969 Earth days to complete an orbit. The diagram on the right illustrates the effects of the eccentricity, showing Mercury's orbit overlaid with a circular orbit having the same semi-major axis. Mercury's higher velocity when it is near perihelion is clear from the greater distance it covers in each 5-day interval. In the diagram the varying distance of Mercury to the Sun is represented by the size of the planet, which is inversely proportional to Mercury's distance from the Sun. This varying distance to the Sun, combined with a 3:2 spin–orbit resonance of the planet's rotation around its axis, result in complex variations of the surface temperature.
This resonance makes a single day on Mercury last exactly two Mercury years, or about 176 Earth days.
Mercury's orbit is inclined by 7 degrees to the plane of Earth's orbit (the ecliptic), as shown in the diagram on the right. As a result, transits of Mercury across the face of the Sun can only occur when the planet is crossing the plane of the ecliptic at the time it lies between Earth and the Sun. This occurs about every seven years on average.
Mercury's axial tilt is almost zero, with the best measured value as low as 0.027 degrees. This is significantly smaller than that of Jupiter, which has the second smallest axial tilt of all planets at 3.1 degrees. This means that to an observer at Mercury's poles, the center of the Sun never rises more than 2.1 arcminutes above the horizon.
At certain points on Mercury's surface, an observer would be able to see the Sun rise about halfway, then reverse and set before rising again, all within the same Mercurian day. This is because approximately four Earth days before perihelion, Mercury's angular orbital velocity equals its angular rotational velocity so that the Sun's apparent motion ceases; closer to perihelion, Mercury's angular orbital velocity then exceeds the angular rotational velocity. Thus, to a hypothetical observer on Mercury, the Sun appears to move in a retrograde direction. Four Earth days after perihelion, the Sun's normal apparent motion resumes.
For the same reason, there are two points on Mercury's equator, 180 degrees apart in longitude, at either of which, around perihelion in alternate Mercurian years (once a Mercurian day), the Sun passes overhead, then reverses its apparent motion and passes overhead again, then reverses a second time and passes overhead a third time, taking a total of about 16 Earth-days for this entire process. In the other alternate Mercurian years, the same thing happens at the other of these two points. The amplitude of the retrograde motion is small, so the overall effect is that, for two or three weeks, the Sun is almost stationary overhead, and is at its most brilliant because Mercury is at perihelion, its closest to the Sun. This prolonged exposure to the Sun at its brightest makes these two points the hottest places on Mercury. Conversely, there are two other points on the equator, 90 degrees of longitude apart from the first ones, where the Sun passes overhead only when the planet is at aphelion in alternate years, when the apparent motion of the Sun in Mercury's sky is relatively rapid. These points, which are the ones on the equator where the apparent retrograde motion of the Sun happens when it is crossing the horizon as described in the preceding paragraph, receive much less solar heat than the first ones described above.
Mercury attains inferior conjunction (nearest approach to Earth) every 116 Earth days on average, but this interval can range from 105 days to 129 days due to the planet's eccentric orbit. Mercury can come as near as 82.2 Gm to Earth, and that is slowly declining: The next approach to within 82.1 Gm is in 2679, and to within 82 Gm in 4487, but it will not be closer to Earth than 80 Gm until AD 28,622. Its period of retrograde motion as seen from Earth can vary from 8 to 15 days on either side of inferior conjunction. This large range arises from the planet's high orbital eccentricity.
Longitude convention.
The longitude convention for Mercury puts the zero of longitude at one of the two hottest points on the surface, as described above. However, when this area was first visited, by Mariner 10, this zero meridian was in darkness, so it was impossible to select a feature on the surface to define the exact position of the meridian. Therefore, a small crater further west was chosen, called Hun Kal, which provides the exact reference point for measuring longitude. The center of Hun Kal defines the 20° West meridian. A 1970 International Astronomical Union resolution suggests that longitudes be measured positively in the westerly direction on Mercury. The two hottest places on the equator are therefore at longitudes 0°W and 180°W, and the coolest points on the equator are at longitudes 90°W and 270°W. However, the "MESSENGER" project uses an east-positive convention.
Spin–orbit resonance.
For many years it was thought that Mercury was synchronously tidally locked with the Sun, rotating once for each orbit and always keeping the same face directed towards the Sun, in the same way that the same side of the Moon always faces Earth. Radar observations in 1965 proved that the planet has a 3:2 spin–orbit resonance, rotating three times for every two revolutions around the Sun; the eccentricity of Mercury's orbit makes this resonance stable—at perihelion, when the solar tide is strongest, the Sun is nearly still in Mercury's sky.
The original reason astronomers thought it was synchronously locked was that, whenever Mercury was best placed for observation, it was always nearly at the same point in its 3:2 resonance, hence showing the same face. This is because, coincidentally, Mercury's rotation period is almost exactly half of its synodic period with respect to Earth. Due to Mercury's 3:2 spin–orbit resonance, a solar day (the length between two meridian transits of the Sun) lasts about 176 Earth days. A sidereal day (the period of rotation) lasts about 58.7 Earth days.
Simulations indicate that the orbital eccentricity of Mercury varies chaotically from nearly zero (circular) to more than 0.45 over millions of years due to perturbations from the other planets. This is thought to explain Mercury's 3:2 spin–orbit resonance (rather than the more usual 1:1), because this state is more likely to arise during a period of high eccentricity. Numerical simulations show that a future secular orbital resonant perihelion interaction with Jupiter may cause the eccentricity of Mercury's orbit to increase to the point where there is a 1% chance that the planet may collide with Venus within the next five billion years.
Advance of perihelion.
In 1859, the French mathematician and astronomer Urbain Le Verrier reported that the slow precession of Mercury's orbit around the Sun could not be completely explained by Newtonian mechanics and perturbations by the known planets. He suggested, among possible explanations, that another planet (or perhaps instead a series of smaller 'corpuscules') might exist in an orbit even closer to the Sun than that of Mercury, to account for this perturbation. (Other explanations considered included a slight oblateness of the Sun.) The success of the search for Neptune based on its perturbations of the orbit of Uranus led astronomers to place faith in this possible explanation, and the hypothetical planet was named Vulcan, but no such planet was ever found.
The perihelion precession of Mercury is 5600 arcseconds (1.5556°) per century relative to Earth, or 574.10±0.65 arcseconds per century relative to the inertial ICFR. Newtonian mechanics, taking into account all the effects from the other planets, predicts a precession of 5557 arcseconds (1.5436°) per century. In the early 20th century, Albert Einstein's general theory of relativity provided the explanation for the observed precession. The effect is small: just 42.98 arcseconds per century for Mercury; it therefore requires a little over twelve million orbits for a full excess turn. Similar, but much smaller, effects exist for other Solar System bodies: 8.62 arcseconds per century for Venus, 3.84 for Earth, 1.35 for Mars, and 10.05 for 1566 Icarus.
Observation.
Mercury's apparent magnitude varies between −2.6 (brighter than the brightest star Sirius) and about +5.7 (approximating the theoretical limit of naked-eye visibility). The extremes occur when Mercury is close to the Sun in the sky. Observation of Mercury is complicated by its proximity to the Sun, as it is lost in the Sun's glare for much of the time. Mercury can be observed for only a brief period during either morning or evening twilight.
Mercury can, like several other planets and the brightest stars, be seen during a total solar eclipse.
Like the Moon and Venus, Mercury exhibits phases as seen from Earth. It is "new" at inferior conjunction and "full" at superior conjunction. The planet is rendered invisible from Earth on both of these occasions because of its being obscured by the Sun's disk.
Mercury is technically brightest as seen from Earth when it is at a full phase. Although Mercury is farthest from Earth when it is full the greater illuminated area that is visible and the opposition brightness surge more than compensates for the distance. The opposite is true for Venus, which appears brightest when it is a crescent, because it is much closer to Earth than when gibbous.
Nonetheless, the brightest (full phase) appearance of Mercury is an essentially impossible time for practical observation, because of the extreme proximity of the Sun. Mercury is best observed at the first and last quarter, although they are phases of lesser brightness. The first and last quarter phases occur at greatest elongation east and west, respectively. At both of these times Mercury's separation from the Sun ranges anywhere from 17.9° at perihelion to 27.8° at aphelion. At greatest elongation west, Mercury rises at its earliest before the Sun, and at greatest elongation east, it sets at its latest after the Sun.
At tropical and subtropical latitudes, Mercury is more easily seen than at higher latitudes. In low latitudes and at the right times of year, the ecliptic intersects the horizon at a steep angle. When Mercury is vertically above the Sun in the sky and is at maximum elongation from the Sun (28 degrees), and when the Sun is 18 degrees below the horizon, so the sky is just completely dark, Mercury is 10 degrees above the horizon. This is the greatest angle of elevation at which Mercury can be seen in a completely dark sky.
At temperate latitudes, Mercury is more often easily visible from Earth's Southern Hemisphere than from its Northern Hemisphere. This is because Mercury's maximum possible elongations west of the Sun always occur when it is early autumn in the Southern Hemisphere, whereas its maximum possible eastern elongations happen during late winter in the Southern Hemisphere. In both of these cases, the angle Mercury strikes with the ecliptic is maximized, allowing it to rise several hours before the Sun in the former instance and not set until several hours after sundown in the latter in countries located at southern temperate zone latitudes, such as Argentina and South Africa.
An alternate method for viewing Mercury involves observing the planet during daylight hours when conditions are clear, ideally when it is at its point of greatest elongation. This allows the planet to be found easily, even when using telescopes with 8 cm apertures. Care must be taken to ensure the instrument isn't pointed directly towards the Sun because of the risk for eye damage. This method bypasses the limitation of twilight observing when the ecliptic is located at a low elevation (e.g. on autumn evenings).
Ground-based telescope observations of Mercury reveal only an illuminated partial disk with limited detail. The first of two spacecraft to visit the planet was "Mariner 10", which mapped about 45% of its surface from 1974 to 1975. The second is the "MESSENGER" spacecraft, which after three Mercury flybys between 2008 and 2009, attained orbit around Mercury on March 17, 2011, to study and map the rest of the planet.
The Hubble Space Telescope cannot observe Mercury at all, due to safety procedures that prevent its pointing too close to the Sun.
Because the shift of 0.15 revolutions in a year makes up a seven-year cycle (0.15 × 7 ≈ 1.0), in the seventh year Mercury follows almost exactly (earlier by 7 days) the sequence of phenomena it showed seven years before.
Observation history.
Ancient astronomers.
The earliest known recorded observations of Mercury are from the Mul.Apin tablets. These observations were most likely made by an Assyrian astronomer around the 14th century BC. The cuneiform name used to designate Mercury on the Mul.Apin tablets is transcribed as Udu.Idim.Gu\u4.Ud ("the jumping planet"). Babylonian records of Mercury date back to the 1st millennium BC. The Babylonians called the planet Nabu after the messenger to the gods in their mythology.
The ancient Greeks knew the planet as Στίλβων ("Stilbon"), meaning "the gleaming", Ἑρμάων ("Hermaon") and Ἑρμής ("Hermes"), a planetary name that is retained in modern Greek (Ερμής: "Ermis"). The Romans named the planet after the swift-footed Roman messenger god, Mercury (Latin "Mercurius"), which they equated with the Greek Hermes, because it moves across the sky faster than any other planet. The astronomical symbol for Mercury is a stylized version of Hermes' caduceus.
The Roman-Egyptian astronomer Ptolemy wrote about the possibility of planetary transits across the face of the Sun in his work "Planetary Hypotheses". He suggested that no transits had been observed either because planets such as Mercury were too small to see, or because the transits were too infrequent.
In ancient China, Mercury was known as "Chen Xing" (辰星), the Hour Star. It was associated with the direction north and the phase of water in the Wu Xing. Modern Chinese, Korean, Japanese and Vietnamese cultures refer to the planet literally as the "water star" (水星), based on the Five elements. Hindu mythology used the name Budha for Mercury, and this god was thought to preside over Wednesday. The god Odin (or Woden) of Germanic paganism was associated with the planet Mercury and Wednesday. The Maya may have represented Mercury as an owl (or possibly four owls; two for the morning aspect and two for the evening) that served as a messenger to the underworld.
In medieval Islamic astronomy, the Andalusian astronomer Abū Ishāq Ibrāhīm al-Zarqālī in the 11th century described the deferent of Mercury's geocentric orbit as being oval, like an egg or a pignon, although this insight did not influence his astronomical theory or his astronomical calculations. In the 12th century, Ibn Bajjah observed "two planets as black spots on the face of the Sun", which was later suggested as the transit of Mercury and/or Venus by the Maragha astronomer Qotb al-Din Shirazi in the 13th century. (Note that most such medieval reports of transits were later taken as observations of sunspots.)
In India, the Kerala school astronomer Nilakantha Somayaji in the 15th century developed a partially heliocentric planetary model in which Mercury orbits the Sun, which in turn orbits Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century.
Ground-based telescopic research.
The first telescopic observations of Mercury were made by Galileo in the early 17th century. Although he observed phases when he looked at Venus, his telescope was not powerful enough to see the phases of Mercury. In 1631, Pierre Gassendi made the first telescopic observations of the transit of a planet across the Sun when he saw a transit of Mercury predicted by Johannes Kepler. In 1639, Giovanni Zupi used a telescope to discover that the planet had orbital phases similar to Venus and the Moon. The observation demonstrated conclusively that Mercury orbited around the Sun.
A rare event in astronomy is the passage of one planet in front of another (occultation), as seen from Earth. Mercury and Venus occult each other every few centuries, and the event of May 28, 1737 is the only one historically observed, having been seen by John Bevis at the Royal Greenwich Observatory. The next occultation of Mercury by Venus will be on December 3, 2133.
The difficulties inherent in observing Mercury mean that it has been far less studied than the other planets. In 1800, Johann Schröter made observations of surface features, claiming to have observed 20 km high mountains. Friedrich Bessel used Schröter's drawings to erroneously estimate the rotation period as 24 hours and an axial tilt of 70°. In the 1880s, Giovanni Schiaparelli mapped the planet more accurately, and suggested that Mercury's rotational period was 88 days, the same as its orbital period due to tidal locking. This phenomenon is known as synchronous rotation. The effort to map the surface of Mercury was continued by Eugenios Antoniadi, who published a book in 1934 that included both maps and his own observations. Many of the planet's surface features, particularly the albedo features, take their names from Antoniadi's map.
In June 1962, Soviet scientists at the Institute of Radio-engineering and Electronics of the USSR Academy of Sciences led by Vladimir Kotelnikov became first to bounce radar signal off Mercury and receive it, starting radar observations of the planet. Three years later radar observations by Americans Gordon Pettengill and R. Dyce using 300-meter Arecibo Observatory radio telescope in Puerto Rico showed conclusively that the planet's rotational period was about 59 days. The theory that Mercury's rotation was synchronous had become widely held, and it was a surprise to astronomers when these radio observations were announced. If Mercury were tidally locked, its dark face would be extremely cold, but measurements of radio emission revealed that it was much hotter than expected. Astronomers were reluctant to drop the synchronous rotation theory and proposed alternative mechanisms such as powerful heat-distributing winds to explain the observations.
Italian astronomer Giuseppe Colombo noted that the rotation value was about two-thirds of Mercury's orbital period, and proposed that the planet's orbital and rotational periods were locked into a 3:2 rather than a 1:1 resonance. Data from Mariner 10 subsequently confirmed this view. This means that Schiaparelli's and Antoniadi's maps were not "wrong". Instead, the astronomers saw the same features during every "second" orbit and recorded them, but disregarded those seen in the meantime, when Mercury's other face was toward the Sun, because the orbital geometry meant that these observations were made under poor viewing conditions.
Ground-based optical observations did not shed much further light on Mercury, but radio astronomers using interferometry at microwave wavelengths, a technique that enables removal of the solar radiation, were able to discern physical and chemical characteristics of the subsurface layers to a depth of several meters. Not until the first space probe flew past Mercury did many of its most fundamental morphological properties become known. Moreover, recent technological advances have led to improved ground-based observations. In 2000, high-resolution lucky imaging observations were conducted by the Mount Wilson Observatory 1.5 meter Hale telescope. They provided the first views that resolved surface features on the parts of Mercury that were not imaged in the Mariner mission. Most of the planet has been mapped by the Arecibo radar telescope, with 5 km resolution, including polar deposits in shadowed craters of what may be water ice.
Research with space probes.
Reaching Mercury from Earth poses significant technical challenges, because it orbits so much closer to the Sun than Earth. A Mercury-bound spacecraft launched from Earth must travel over 91 million kilometers into the Sun's gravitational potential well. Mercury has an orbital speed of 48 km/s, whereas Earth's orbital speed is 30 km/s. Therefore, the spacecraft must make a large change in velocity (delta-v) to enter a Hohmann transfer orbit that passes near Mercury, as compared to the delta-v required for other planetary missions.
The potential energy liberated by moving down the Sun's potential well becomes kinetic energy; requiring another large delta-v change to do anything other than rapidly pass by Mercury. To land safely or enter a stable orbit the spacecraft would rely entirely on rocket motors. Aerobraking is ruled out because Mercury has a negligible atmosphere. A trip to Mercury requires more rocket fuel than that required to escape the Solar System completely. As a result, only two space probes have visited it so far. A proposed alternative approach would use a solar sail to attain a Mercury-synchronous orbit around the Sun.
"Mariner 10".
The first spacecraft to visit Mercury was NASA's "Mariner 10" (1974–1975). The spacecraft used the gravity of Venus to adjust its orbital velocity so that it could approach Mercury, making it both the first spacecraft to use this gravitational "slingshot" effect and the first NASA mission to visit multiple planets. Mariner 10 provided the first close-up images of Mercury's surface, which immediately showed its heavily cratered nature, and revealed many other types of geological features, such as the giant scarps that were later ascribed to the effect of the planet shrinking slightly as its iron core cools. Unfortunately, due to the length of Mariner 10's orbital period, the same face of the planet was lit at each of Mariner 10's close approaches. This made observation of both sides of the planet impossible, and resulted in the mapping of less than 45% of the planet's surface.
The spacecraft made three close approaches to Mercury, the closest of which took it to within 327 km of the surface. At the first close approach, instruments detected a magnetic field, to the great surprise of planetary geologists—Mercury's rotation was expected to be much too slow to generate a significant dynamo effect. The second close approach was primarily used for imaging, but at the third approach, extensive magnetic data were obtained. The data revealed that the planet's magnetic field is much like Earth's, which deflects the solar wind around the planet. The origin of Mercury's magnetic field is still the subject of several competing theories.
On March 24, 1975, just eight days after its final close approach, Mariner 10 ran out of fuel. Because its orbit could no longer be accurately controlled, mission controllers instructed the probe to shut down. Mariner 10 is thought to be still orbiting the Sun, passing close to Mercury every few months.
"MESSENGER".
A second NASA mission to Mercury, named "MESSENGER" (MErcury Surface, Space ENvironment, GEochemistry, and Ranging), was launched on August 3, 2004, from the Cape Canaveral Air Force Station aboard a Boeing Delta 2 rocket. It made a fly-by of Earth in August 2005, and of Venus in October 2006 and June 2007 to place it onto the correct trajectory to reach an orbit around Mercury. A first fly-by of Mercury occurred on January 14, 2008, a second on October 6, 2008, and a third on September 29, 2009. Most of the hemisphere not imaged by Mariner 10 was mapped during these fly-bys. The probe successfully entered an elliptical orbit around the planet on March 18, 2011. The first orbital image of Mercury was obtained on March 29, 2011. The probe finished a one-year mapping mission, and then entered a one-year extended mission into 2013. In addition to continued observations and mapping of Mercury, "MESSENGER" observed the 2012 solar maximum.
The mission was designed to clear up six key issues: Mercury's high density, its geological history, the nature of its magnetic field, the structure of its core, whether it has ice at its poles, and where its tenuous atmosphere comes from. To this end, the probe carried imaging devices that gathered much-higher-resolution images of much more of Mercury than Mariner 10, assorted spectrometers to determine abundances of elements in the crust, and magnetometers and devices to measure velocities of charged particles. Detailed measurements of tiny changes in the probe's velocity as it orbits will be used to infer details of the planet's interior structure. "MESSENGER"'s final maneuver was on April 24, 2015, which left it without fuel and an uncontrolled trajectory that inevitably led it to crash into Mercury's surface on April 30, 2015. The spacecraft's impact with Mercury occurred near 3:26 PM EDT on April 30, 2015, leaving a crater estimated to be 16 m in diameter.
"BepiColombo".
The European Space Agency is planning a joint mission with Japan called "BepiColombo", which will orbit Mercury with two probes: one to map the planet and the other to study its magnetosphere. Once launched in 2016, "BepiColombo" is expected to reach Mercury in 2024. It will release a magnetometer probe into an elliptical orbit, then chemical rockets will fire to deposit the mapper probe into a circular orbit. Both probes will operate for one terrestrial year. The mapper probe will carry an array of spectrometers similar to those on "MESSENGER", and will study the planet at many different wavelengths including infrared, ultraviolet, X-ray and gamma ray.
External links.
Listen to this article ()
This audio file was created from a revision of the "Mercury (planet)" article dated 2008-01-16, and does not reflect subsequent edits to the article. ()
More spoken articles
 Media related to at Wikimedia Commons

</doc>
<doc id="19701" url="http://en.wikipedia.org/wiki?curid=19701" title="Monty Python and the Holy Grail">
Monty Python and the Holy Grail

Monty Python and the Holy Grail is a 1975 British comedy film written and performed by the comedy group of Monty Python (Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin), and directed by Gilliam and Jones. It was conceived during the hiatus between the third and fourth series of their popular BBC television programme "Monty Python's Flying Circus".
In contrast to the group's first film, "And Now for Something Completely Different", a compilation of sketches from the first two television series, "Holy Grail" was composed of new material, and is therefore considered the first "proper" film by the group. It generally parodies the legend of King Arthur's quest to find the Holy Grail. The film was a success on its initial release, and Idle used the film as the inspiration for the 2005 Tony Award-winning musical "Spamalot".
The film was a box-office success, grossing the highest of any British film exhibited in the U.S. in 1975. It has remained popular since then, receiving critical acclaim. The film received a 97% "Fresh" rating on Rotten Tomatoes, with the consensus; "a cult classic as gut-bustingly hilarious as it is blithely ridiculous". In the US, the film was selected as the second best comedy of all time in the ABC special ""; in the UK, readers of "Total Film" magazine ranked the film the fifth greatest comedy film of all time, and a similar poll of Channel 4 viewers placed the film sixth (2000).
Plot.
In 932, King Arthur, along with his squire, Patsy, recruits his Knights of the Round Table: Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-As-Sir-Lancelot, and the aptly named Sir Not-appearing-in-this-film. On the way, Arthur battles the Black Knight. Arthur chops off the Black Knight's arms and legs but the Black Knight refuses to surrender despite being fatally wounded. The knights reach Camelot, but following a song-and-dance cutaway, Arthur decides not to enter, because "'tis a silly place". They are instructed by God to seek the Holy Grail.
Their first stop is a French-controlled castle where they believe the grail is held. After being insulted by a soldier with a strong French accent, they try sneaking into the castle in a Trojan Rabbit. This plan goes awry when they forget to hide inside it first. The rabbit is catapulted back at them with an assortment of animals and waste. Arthur decides the group should split-up to seek the grail. A modern-day historian serves as an on-camera documentary presenter, describing the Arthurian legends. He is abruptly killed by an unknown knight on horseback, triggering a police investigation.
Each of the knights encounters various perils on his quest. Arthur and Bedevere attempt to satisfy the strange requests of the dreaded Knights who say Ni. Sir Robin narrowly avoids a fight with the Three-Headed Giant by running away while the heads are arguing, causing embarrassment as his minstrel sings "Brave Sir Robin ran away". Sir Galahad is led by a grail-shaped beacon to Castle Anthrax, populated by women who wish to perform sexual favours for him, but he is rescued by Lancelot from the "perilous situation", somewhat against his will. Sir Lancelot then finds a note tied to an arrow, and after reading it assaults a wedding party at Swamp Castle. Believing them to be holding a lady against her will, he discovers that an effeminate prince sent the note.
The knights regroup with three new knights, Sir Gawain, Sir Ector, and Sir Bors, and a group of monks led by Brother Maynard. They travel to see Tim the Enchanter, who points them to caves where the location of the grail is written on the walls. To enter the caves the group must defeat the Killer Rabbit of Caerbannog. They do so using the Holy Hand Grenade of Antioch courtesy of Brother Maynard after the rabbit kills Gawain, Ector and Bors. They enter the cave and are attacked by the Legendary Black Beast of Aaaaarrrrrrggghhh, which devours Brother Maynard. Arthur and his Knights barely escape by virtue of the beast's animator suffering a fatal heart attack.
With their final destination known, the group travel to the Bridge of Death, where each knight is forced to answer three questions by the bridge-keeper before he may cross the Gorge of Eternal Peril. Lancelot easily answers his questions and crosses the bridge, but Robin and Galahad are confounded by difficult questions and thrown into the chasm. Arthur accidentally asks the bridge-keeper a question, and the bridge-keeper is thrown into the chasm himself for not knowing the answer.
Lancelot becomes separated from Arthur and Bedevere, and is later accused and arrested by police for the murder of the historian. Arthur and Bedevere travel to the Castle Aaargh, which they find occupied by the same French forces that insulted and drove them off earlier. The knights amass a large army and prepare to storm the castle, but just as they begin to charge, the modern police arrive on the scene. Arthur and Bedevere are arrested, and one of the officers covers the lens with his hand. The film breaks in the projector and runs out of the gate, putting an abrupt end to the film.
Production.
"Monty Python and the Holy Grail" was mostly shot on location in Scotland, particularly around Doune Castle, Glen Coe, and the privately owned Castle Stalker. The many castles seen throughout the film were either Doune Castle shot from different angles or hanging miniatures. There are several exceptions to this: the very first exterior shot of a castle at the beginning of the film is Kidwelly Castle in South Wales and the single exterior shot of the Swamp King castle during "Tale of Sir Lancelot" is Bodiam Castle in East Sussex – all subsequent shots of the exterior and interior of those scenes were filmed at Doune Castle.
At the start of the Tale of Sir Robin, there is a slow camera zoom in on rocky scenery (that in the voice over is described as 'the dark forest of Ewing'). This is actually a still photograph of The Gorge at Mount Buffalo National Park in Victoria, Australia. The production manager Julian Doyle stated in 2000 during an interview with Hotdog magazine that it was a still image filmed with candles underneath the frame (to give a heat haze). This was a low-cost method of achieving a convincing location effect.
On the DVD audio commentary, Cleese expresses irritation at a scene set in Castle Anthrax, where he says the focus was on technical aspects rather than comedy. In the scene where the knights were combating the Killer Rabbit, a real white rabbit was used. He was dyed with what was assumed to be a washable red colouring liquid in the shots after the battle. When filming wrapped, the rabbit's owner was dismayed to learn the dye could not be rinsed off. Gilliam described in an audio commentary that the owner of the rabbit was present and shooting was abruptly halted while the cast desperately attempted to clean the rabbit before the owner found out, an unsuccessful attempt. He also stated that he thought that, had they been more experienced in filmmaking, the crew would have just purchased a rabbit instead. Otherwise, the rabbit himself was unharmed. Also, the rabbit-bite effects were done by special puppetry by both Gilliam and SFX technician John Horton.
As chronicled in "The Life of Python", "The First 20 Years of Monty Python", and "The Pythons' Autobiography", it was revealed that Chapman was suffering from acrophobia, trembling, and bouts of forgetfulness during filming. These were the results of Chapman's long-standing alcohol addiction, and he decided from that moment on to remain "on an even keel" while the production continued. Nearly three years after "Holy Grail", Chapman vowed to quit drinking altogether (which he successfully achieved in December 1977).
Originally the knight characters were going to ride real horses, but after it became clear that the film's small budget precluded real horses, the Pythons decided that their characters would mime horse-riding while their porters trotted behind them banging coconut shells together. The joke was derived from the old-fashioned sound effect used by radio shows to convey the sound of hooves clattering. This was later referred to in the German release of the film, which translated the title as "Die Ritter der Kokosnuß" ("The Knights of the Coconut").
Soundtrack.
In addition to several songs written by Python regular Neil Innes, several pieces of music were licensed from De Wolfe Music Library. These include
Television broadcast.
The film had its television premiere 25 February 1977 on the CBS Late Movie. Reportedly, the Pythons were displeased to discover a number of edits were done by the network to reduce use of profanity and the showing of blood. The troupe pulled back the rights and thereafter had it broadcast in the United States only on PBS and later other channels such as IFC, where it runs uncut.
Home media.
The first DVD was released in 1999 and had only a non-anamorphic print, about two pages of production notes, and trailers for other Sony Pictures releases.
On 23 October 2001, the Special Edition DVD was released. Disc One includes two commentary tracks (featuring Idle, Palin, and Cleese in the first, Jones and Gilliam in the second), "Subtitles for People Who Don't Like the Film", consisting of lines taken from William Shakespeare's "Henry IV, Part 2", and a feature for the hard of hearing, where the menu is read aloud by Jones in a very loud voice.
As an extension of the group's penchant for never abiding to a generic formula, the film (if not already in progress) commences with a short subject named "Dentist on the Job" (Its American title is "Get on With It", a phrase spoken multiple times throughout "Holy Grail") After the opening credits, the projectionist (Jones) realises it is the wrong film and puts the correct one on, after he displays the "Please Wait" caption.
Additionally, there is a deleted scene where Galahad meets Dingo, during which she breaks out of character, turns to the camera and asks the audience if they should cut that scene. Her response is everyone yelling "GET ON WITH IT!" (including God).
Disc Two includes "Monty Python and the Holy Grail in Lego" (also known as "Lego Knights" or "It's Only a Model"), a "brickfilm" version of the "Camelot Song" as sung by Lego minifigures. It was created by Spite Your Face Productions on commission from the Lego Group and Python Pictures. The project was conceived by the original film's respective Producer and Co-Director, John Goldstone and Terry Gilliam. Disc Two also includes two scenes dubbed in Japanese and translated back through subtitles. "The Quest for the Holy Grail Locations", hosted by Palin and Jones, shows places in Scotland used for the setting titled as "England 932 A.D." (as well as the two Pythons purchasing a copy of their own script as a guide). Also included is a who's who page, advertising galleries, sing-alongs, and a small featurette about the proper use of a coconut (presented by Michael Palin).
The DVD release additionally included a copy of the screenplay, a limited-edition film cell/senitype, and limited-edition art cards; however, a few of the bonus features from the Extraordinarily Deluxe Edition were omitted.
A 35th anniversary edition on Blu-ray Disc was released in the US on 6 March 2012. Special features include "The Holy Book of Days," a second-screen experience that can be downloaded as an app on an iOS device and played with the Blu-ray to enhance its viewing; lost animation sequences with a new intro from animator Terry Gilliam; outtakes; and extended scenes with Python member and the movie's co-director Terry Jones.
Reaction and legacy.
"Monty Python and the Holy Grail" received critical acclaim and remains a cult classic to this day. The film received a 97% "Fresh" rating on Rotten Tomatoes based on 67 reviews; the consensus states: "A cult classic as gut-bustingly hilarious as it is blithely ridiculous, "Monty Python and the Holy Grail" has lost none of its exceedingly silly charm."
In 2000, readers of "Total Film" magazine voted "Holy Grail" the fifth greatest comedy film of all time. The next Python film, "Life of Brian", was ranked first. A similar poll of Channel 4 viewers in 2005 placed "Holy Grail" in sixth (with "Life of Brian" again topping the list). DigitalDreamDoor.com ranked "Holy Grail" the third greatest comedy film of all time.
In 2005 the film spawned a Tony Award-winning Broadway musical smash hit knockoff, "Spamalot". Written primarily by Idle, the show has more of an overarching plot and leaves out certain portions of the movie due to difficulties in rendering certain effects on stage. However, the same gags are there for the majority of the show.
In 2011, an ABC prime-time special, "", counted down the best films chosen by fans based on results of a poll conducted by ABC and "People". "Holy Grail" was selected as the second best comedy.
In 2013 the Pythons lost a legal case to Mark Forstater, the film producer, over royalties for the derivative work Spamalot. They owed a combined £800,000 in legal fees and back royalties to Forstater.
Further reading.
• Larsen, Darl. "A Book About the Film Monty Python and the Holy Grail: All the References From African Swallows to Zoot". Rowman & Littlefield, 2015. ISBN 9781442245532

</doc>
<doc id="19702" url="http://en.wikipedia.org/wiki?curid=19702" title="Mutation">
Mutation

In biology, a mutation is a permanent change of the nucleotide sequence of the genome of an organism, virus, or extrachromosomal DNA or other genetic elements. Mutations result from damage to DNA which is not repaired or to RNA genomes (typically caused by radiation or chemical mutagens), errors in the process of replication, or from the insertion or deletion of segments of DNA by mobile genetic elements. Mutations may or may not produce discernible changes in the observable characteristics (phenotype) of an organism. Mutations play a part in both normal and abnormal biological processes including: evolution, cancer, and the development of the immune system, including junctional diversity.
Mutation can result in several different types of change in sequences. Mutations in genes can either have no effect, alter the product of a gene, or prevent the gene from functioning properly or completely. Mutations can also occur in nongenic regions. One study on genetic variations between different species of "Drosophila" suggests that, if a mutation changes a protein produced by a gene, the result is likely to be harmful, with an estimated 70 percent of amino acid polymorphisms that have damaging effects, and the remainder being either neutral or weakly beneficial. Due to the damaging effects that mutations can have on genes, organisms have mechanisms such as DNA repair to prevent or correct (revert the mutated sequence back to its original state) mutations.
Description.
Mutations can involve the duplication of large sections of DNA, usually through genetic recombination. These duplications are a major source of raw material for evolving new genes, with tens to hundreds of genes duplicated in animal genomes every million years. Most genes belong to larger families of genes of shared ancestry. Novel genes are produced by several methods, commonly through the duplication and mutation of an ancestral gene, or by recombining parts of different genes to form new combinations with new functions.
Here, domains act as modules, each with a particular and independent function, that can be mixed together to produce genes encoding new proteins with novel properties. For example, the human eye uses four genes to make structures that sense light: three for color vision and one for night vision; all four arose from a single ancestral gene. Another advantage of duplicating a gene (or even an entire genome) is that this increases redundancy; this allows one gene in the pair to acquire a new function while the other copy performs the original function. Other types of mutation occasionally create new genes from previously noncoding DNA.
Changes in chromosome number may involve even larger mutations, where segments of the DNA within chromosomes break and then rearrange. For example, in the Homininae, two chromosomes fused to produce human chromosome 2; this fusion did not occur in the lineage of the other apes, and they retain these separate chromosomes. In evolution, the most important role of such chromosomal rearrangements may be to accelerate the divergence of a population into new species by making populations less likely to interbreed, thereby preserving genetic differences between these populations.
Sequences of DNA that can move about the genome, such as transposons, make up a major fraction of the genetic material of plants and animals, and may have been important in the evolution of genomes. For example, more than a million copies of the Alu sequence are present in the human genome, and these sequences have now been recruited to perform functions such as regulating gene expression. Another effect of these mobile DNA sequences is that when they move within a genome, they can mutate or delete existing genes and thereby produce genetic diversity.
Nonlethal mutations accumulate within the gene pool and increase the amount of genetic variation. The abundance of some genetic changes within the gene pool can be reduced by natural selection, while other "more favorable" mutations may accumulate and result in adaptive changes.
For example, a butterfly may produce offspring with new mutations. The majority of these mutations will have no effect; but one might change the color of one of the butterfly's offspring, making it harder (or easier) for predators to see. If this color change is advantageous, the chance of this butterfly's surviving and producing its own offspring are a little better, and over time the number of butterflies with this mutation may form a larger percentage of the population.
Neutral mutations are defined as mutations whose effects do not influence the fitness of an individual. These can accumulate over time due to genetic drift. It is believed that the overwhelming majority of mutations have no significant effect on an organism's fitness. Also, DNA repair mechanisms are able to mend most changes before they become permanent mutations, and many organisms have mechanisms for eliminating otherwise-permanently mutated somatic cells.
Beneficial mutations can improve reproductive success.
Causes.
Four classes of mutations are (1) spontaneous mutations (molecular decay), (2) mutations due to error prone replication bypass of naturally occurring DNA damage (also called error prone translesion synthesis), (3) errors introduced during DNA repair, and (4) induced mutations caused by mutagens. Scientists may also deliberately introduce mutant sequences through DNA manipulation for the sake of scientific experimentation.
Spontaneous mutation.
"Spontaneous mutations" on the molecular level can be caused by:
Error prone replication by-pass.
There is increasing evidence that the majority of spontaneously arising mutations are due to error prone replication (translesion synthesis) past a DNA damage in the template strand. As described in the article DNA damage (naturally occurring), naturally occurring DNA damages arise about 60,000 to 100,000 times per day per mammalian cell. In mice, the majority of mutations are caused by translesion synthesis. Likewise, in yeast, Kunz et al. found that more than 60% of the spontaneous single base pair substitutions and deletions were caused by translesion synthesis.
Errors introduced during DNA repair.
Although naturally occurring double-strand breaks occur at a relatively low frequency in DNA (see DNA damage (naturally occurring)) their repair often causes mutation. Non-homologous end joining (NHEJ) is a major pathway for repairing double-strand breaks. NHEJ involves removal of a few nucleotides to allow somewhat inaccurate alignment of the two ends for rejoining followed by addition of nucleotides to fill in gaps. As a consequence, NHEJ often introduces mutations.
Induced mutation.
"Induced mutations" on the molecular level can be caused by:-
Classification of mutation types.
By effect on structure.
The sequence of a gene can be altered in a number of ways. Gene mutations have varying effects on health depending on where they occur and whether they alter the function of essential proteins.
Mutations in the structure of genes can be classified as:
By effect on function.
See also Behavior mutation.
By effect on fitness.
In applied genetics, it is usual to speak of mutations as either harmful or beneficial.
Distribution of SHIBA effects.
In reality, viewing the fitness effects of mutations in these discrete categories is an oversimplification. Attempts have been made to infer the distribution of fitness effects (DFE) using mutagenesis experiments and theoretical models applied to molecular sequence data. Distribution of fitness effects, as used to determine the relative abundance of different types of mutations (i.e., strongly deleterious, nearly neutral or advantageous), is relevant to many evolutionary questions, such as the maintenance of genetic variation, the rate of genomic decay, the maintenance of outcrossing sexual reproduction as opposed to inbreeding and the evolution of sex and recombination. In summary, DFE plays an important role in predicting evolutionary dynamics. A variety of approaches have been used to study the distribution of fitness effects, including theoretical, experimental and analytical methods.
One of the earliest theoretical studies of the distribution of fitness effects was done by Motoo Kimura, an influential theoretical population geneticist. His neutral theory of molecular evolution proposes that most novel mutations will be highly deleterious, with a small fraction being neutral. Hiroshi Akashi more recently proposed a bimodal model for DFE, with modes centered around highly deleterious and neutral mutations. Both theories agree that the vast majority of novel mutations are neutral or deleterious and that advantageous mutations are rare, which has been supported by experimental results. One example is a study done on the distribution of fitness effects of random mutations in vesicular stomatitis virus. Out of all mutations, 39.6% were lethal, 31.2% were non-lethal deleterious, and 27.1% were neutral. Another example comes from a high throughput mutagenesis experiment with yeast. In this experiment it was shown that the overall distribution of fitness effects is bimodal, with a cluster of neutral mutations, and a broad distribution of deleterious mutations.
Though relatively few mutations are advantageous, those that are play an important role in evolutionary changes. Like neutral mutations, weakly selected advantageous mutations can be lost due to random genetic drift, but strongly selected advantageous mutations are more likely to be fixed. Knowing the distribution of fitness effects of advantageous mutations may lead to increased ability to predict the evolutionary dynamics. Theoretical work on the DFE for advantageous mutations has been done by John H. Gillespie and H. Allen Orr. They proposed that the distribution for advantageous mutations should be exponential under a wide range of conditions, which, in general, has been supported by experimental studies, at least for strongly selected advantageous mutations.
In general, it is accepted that the majority of mutations are neutral or deleterious, with rare mutations being advantageous; however, the proportion of types of mutations varies between species. This indicates two important points: first, the proportion of effectively neutral mutations is likely to vary between species, resulting from dependence on effective population size; second, the average effect of deleterious mutations varies dramatically between species. In addition, the DFE also differs between coding regions and non-coding regions, with the DFE of non-coding DNA containing more weakly selected mutations.
By inheritance.
In multicellular organisms with dedicated reproductive cells, mutations can be subdivided into germ line mutations, which can be passed on to descendants through their reproductive cells, and somatic mutations (also called acquired mutations), which involve cells outside the dedicated reproductive group and which are not usually transmitted to descendants.
A germline mutation gives rise to a "constitutional mutation" in the offspring, that is, a mutation that is present in every cell. A constitutional mutation can also occur very soon after fertilisation, or continue from a previous constitutional mutation in a parent.
The distinction between germline and somatic mutations is important in animals that have a dedicated germ line to produce reproductive cells. However, it is of little value in understanding the effects of mutations in plants, which lack dedicated germ line. The distinction is also blurred in those animals that reproduce asexually through mechanisms such as budding, because the cells that give rise to the daughter organisms also give rise to that organism´s germ line. 
A new mutation that was not inherited from either parent is called a "de novo" mutation.
Diploid organisms (e.g., humans) contain two copies of each gene — a paternal and a maternal allele. Based on the occurrence of mutation on each chromosome, we may classify mutations into three types.
A wildtype or homozygous non-mutated organism is one in which neither allele is mutated.
Nomenclature.
In order to categorize a mutation as such, the "normal" sequence must be obtained from the DNA of a "normal" or "healthy" organism (as opposed to a "mutant" or "sick" one), it should be identified and reported; ideally, it should be made publicly available for a straightforward nucleotide-by-nucleotide comparison, and agreed upon by the scientific community or by a group of expert geneticists and biologists, who have the responsibility of establishing the "standard" or so-called "consensus" sequence. This step requires a tremendous scientific effort. (See DNA sequencing.) Once the consensus sequence is known, the mutations in a genome can be pinpointed, described, and classified. The committee of the Human Genome Variation Society (HGVS) has developed the standard human sequence variant nomenclature, which should be used by researchers and DNA diagnostic centers to generate unambiguous mutation descriptions. In principle, this nomenclature can also be used to describe mutations in other organisms. The nomenclature specifies the type of mutation and base or amino acid changes.
Contribution of mutations.
The contribution of mutations is different in the tissues. This may be due to different mutation rates by cell division and the different number of cell divisions in each tissue.
Furthermore, knowing the mutational processes, mutation rates and the process of tissue development, can show the history of individual cells. For that, used cellular genome sequencing.
Mutation rates.
Mutation rates vary across species. Evolutionary biologists have theorized that higher mutation rates are beneficial in some situations, because they allow organisms to evolve and therefore adapt more quickly to their environments. For example, repeated exposure of bacteria to antibiotics, and selection of resistant mutants, can result in the selection of bacteria that have a much higher mutation rate than the original population (mutator strains).
According to one study, two children of different parents had 35 and 49 new mutations. Of them, in one case 92% were from the paternal germline, in another case, 64% were from the maternal germline.
Harmful mutations.
Changes in DNA caused by mutation can cause errors in protein sequence, creating partially or completely non-functional proteins. Each cell, in order to function correctly, depends on thousands of proteins to function in the right places at the right times. When a mutation alters a protein that plays a critical role in the body, a medical condition can result. A condition caused by mutations in one or more genes is called a genetic disorder. Some mutations alter a gene's DNA base sequence but do not change the function of the protein made by the gene. One study on the comparison of genes between different species of "Drosophila" suggests that if a mutation does change a protein, this will probably be harmful, with an estimated 70 percent of amino acid polymorphisms having damaging effects, and the remainder being either neutral or weakly beneficial. Studies have shown that only 7% of point mutations in non-coding DNA of yeast are deleterious and 12% in coding DNA are deleterious. The rest of the mutations are either neutral or slightly beneficial.
If a mutation is present in a germ cell, it can give rise to offspring that carries the mutation in all of its cells. This is the case in hereditary diseases. In particular, if there is a mutation in a DNA repair gene within a germ cell, humans carrying such germ-line mutations may have an increased risk of cancer. A list of 34 such germ-line mutations is given in the article DNA repair-deficiency disorder. An example of one is albinism. A mutation that occurs in the OCA1 or OCA2 gene. Individuals with this disorder are more prone to many types of cancers, other disorders and have impaired vision. On the other hand, a mutation may occur in a somatic cell of an organism. Such mutations will be present in all descendants of this cell within the same organism, and certain mutations can cause the cell to become malignant, and, thus, cause cancer.
A DNA damage can cause an error when the DNA is replicated, and this error of replication can cause a gene mutation that, in turn, could cause a genetic disorder. DNA damages are repaired by the DNA repair system of the cell. Each cell has a number of pathways through which enzymes recognize and repair damages in DNA. Because DNA can be damaged in many ways, the process of DNA repair is an important way in which the body protects itself from disease. Once a DNA damage has given rise to a mutation, the mutation cannot be repaired. DNA repair pathways can only recognize and act on "abnormal" structures in the DNA. Once a mutation occurs in a gene sequence it then has normal DNA structure and cannot be repaired.
Beneficial mutations.
Although mutations that cause changes in protein sequences can be harmful to an organism, on occasions the effect may be positive in a given environment. In this case, the mutation may enable the mutant organism to withstand particular environmental stresses better than wild-type organisms, or reproduce more quickly. In these cases a mutation will tend to become more common in a population through natural selection.
For example, a specific 32 base pair deletion in human CCR5 (CCR5-Δ32) confers HIV resistance to homozygotes and delays AIDS onset in heterozygotes. One possible explanation of the etiology of the relatively high frequency of CCR5-Δ32 in the European population is that it conferred resistance to the bubonic plague in mid-14th century Europe. People with this mutation were more likely to survive infection; thus its frequency in the population increased. This theory could explain why this mutation is not found in southern Africa, which remained untouched by bubonic plague. A newer theory suggests that the selective pressure on the CCR5 Delta 32 mutation was caused by smallpox instead of the bubonic plague.
Another example is Sickle-cell disease, a blood disorder in which the body produces an abnormal type of the oxygen-carrying substance hemoglobin in the red blood cells. One-third of all indigenous inhabitants of Sub-Saharan Africa carry the gene, because, in areas where malaria is common, there is a survival value in carrying only a single sickle-cell gene (sickle-cell trait). Those with only one of the two alleles of the sickle-cell disease are more resistant to malaria, since the infestation of the malaria plasmodium is halted by the sickling of the cells that it infests.
Prion mutations.
Prions are proteins and do not contain genetic material. However, prion replication has been shown to be subject to mutation and natural selection just like other forms of replication.
Somatic mutations.
A change in the genetic structure that is not inherited from a parent, and also not passed to offspring, is called a "somatic cell genetic mutation" or "acquired mutation".
When analyzing somatic mutations present in the cells of multicellular organisms, can know its origin and its past.
Cells with heterozygous mutations (one good copy of gene and one mutated copy) may function normally with the unmutated copy until the good copy has been spontaneously somatically mutated. This kind of mutation happens all the time in living organisms, but it is difficult to measure the rate. Measuring this rate is important in predicting the rate at which people may develop cancer.
Point mutations may arise from spontaneous mutations that occur during DNA replication. The rate of mutation may be increased by mutagens. Mutagens can be physical, such as radiation from UV rays, X-rays or extreme heat, or chemical (molecules that misplace base pairs or disrupt the helical shape of DNA). Mutagens associated with cancers are often studied to learn about cancer and its prevention.
Gain-of-function research.
The aim of gain-of-function (GOF) research is to genetically engineer increased transmissibility, virulence, or host range of pathogens. As such, it has been extremely controversial. As a "Nature" editorial put it in October 2014, "revelations over the past few months of serious violations and accidents at some of the leading biosafety containment labs in the United States has burst the hubris that some scientists, and their institutions, have in their perceived ability to work safely with dangerous pathogens." There is a current moratorium on such work in the United States.

</doc>
<doc id="19705" url="http://en.wikipedia.org/wiki?curid=19705" title="Microgyrus">
Microgyrus

A microgyrus is an area of the cerebral cortex that includes only four cortical layers instead of six.
Microgyria are believed by some to be part of the genetic lack of prenatal development which is a cause of, or one of the causes of, dyslexia.
Albert Galaburda of Harvard Medical School noticed that language centers in dyslexic brains showed microscopic flaws known as ectopias and microgyria (Galaburda "et al.", 2006, "Nature Neuroscience" 9(10): 1213-1217). Both affect the normal six-layer structure of the cortex. These flaws affect connectivity and functionality of the cortex in critical areas related to sound and visual processing. These and similar structural abnormalities may be the basis of the inevitable and hard to overcome difficulty in reading.

</doc>
<doc id="19708" url="http://en.wikipedia.org/wiki?curid=19708" title="Mercantilism">
Mercantilism

Mercantilism was an economic theory and practice, dominant in Europe from the 16th to the 18th century, that promoted governmental regulation of a nation's economy for the purpose of augmenting state power at the expense of rival national powers. It is the economic counterpart of political absolutism. Mercantilism includes a national economic policy aimed at accumulating monetary reserves through a positive balance of trade, especially of finished goods. Historically, such policies frequently led to war and also motivated colonial expansion. The Mercantilism theory varies in sophistication from one writer to another and has evolved over time. 
High tariffs, especially on manufactured goods, are an almost universal feature of mercantilism policy. Other policies have included:
Mercantilism in its simplest form is bullionism, but mercantilist writers have emphasized the circulation of money and reject hoarding. Their emphasis on monetary metals accords with current ideas regarding the money supply, such as the stimulative effect of a growing money supply. Specie concerns have since been rendered moot by fiat money and floating exchange rates. In time, the heavy emphasis on money was supplanted by industrial policy, accompanied by a shift in focus from the capacity to carry on wars to promoting general prosperity. Mature neomercantilist theory recommends selective high tariffs for "infant" industries or to promote the mutual growth of countries through national industrial specialization.
The term "mercantile system" was used by its foremost critic Adam Smith, but "mercantilism" had been used earlier by Mirabeau.
While many nations applied the theory, one exemplar was France, economically the most important state in Europe at the time. King Louis XIV followed the guidance of Jean Baptiste Colbert, his controller general of finances (1662–83). They were determined that the state should rule in the economic realm as it did in the diplomatic, and that the interests of the state as identified by the king were superior to those of merchants and everyone else. The goal of mercantilist economic policies was to build up the state, especially in an age of incessant warfare, and the state should look for ways to strengthen the economy and weaken foreign adversaries.
Influence.
Mercantilism was the dominant school of economic thought in Europe throughout the late Renaissance and early modern period (from the 15th to the 18th century). Mercantilism encouraged the many intra-European wars of the period and arguably fueled European expansion and imperialism – both in Europe and throughout the rest of the world – until the 19th century or early 20th century.
Evidence of mercantilistic practices appear in early modern Venice, Genoa, and Pisa regarding control of the Mediterranean trade of bullion. However, as a codified school, mercantilism's real birth is marked by the empiricism of the Renaissance, which first began to quantify large-scale trade accurately.
England began the first large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). An early statement on national balance of trade appeared in "Discourse of the Common Weal of this Realm of England", 1549: "We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them." The period featured various but often disjointed efforts by the court of Queen Elizabeth to develop a naval and merchant fleet capable of challenging the Spanish stranglehold on trade and of expanding the growth of bullion at home. Queen Elizabeth promoted the Trade and Navigation Acts in Parliament and issued orders to her navy for the protection and promotion of English shipping. A systematic and coherent explanation of balance of trade was made public through Thomas Mun's argument "England's Treasure by Forraign Trade, or the Balance of our Forraign Trade is The Rule of Our Treasure." It was written in the 1620s and published in 1664.
These efforts organized national resources sufficiently in the defense of England against the far larger and more powerful Spanish Empire, and in turn paved the foundation for establishing a global empire in the 19th century. The authors noted most for establishing the English mercantilist system include Gerard de Malynes and Thomas Mun, who first articulated the Elizabethan system, which in turn was then developed further by Josiah Child. Numerous French authors helped cement French policy around mercantilism in the 17th century. This French mercantilism was best articulated by Jean-Baptiste Colbert (in office, 1665–1683), though policy liberalised greatly under Napoleon.
In Europe, academic belief in mercantilism began to fade in the late 18th century, especially in Britain, in light of the arguments of Adam Smith and the classical economists. The repeal of the Corn Laws by Robert Peel symbolised the emergence of free trade as an alternative system.
Neomercantilism is a 20th-century economic policy that uses the ideas and methods of neoclassical economics. The new mercantilism has different goals and focuses on more rapid economic growth based on advanced technology. It promotes such policies as substitution state taxing, subsidizing, spending, and general regulatory powers for tariffs and quotas, and protection through the formation of supranational trading blocs. 
Theory.
Most of the European economists who wrote between 1500 and 1750 are today generally considered mercantilists; this term was initially used solely by critics, such as Mirabeau and Smith, but was quickly adopted by historians. Originally the standard English term was "mercantile system." The word "mercantilism" was introduced into English from German in the early 19th century.
The bulk of what is commonly called "mercantilist literature" appeared in the 1620s in Great Britain. Smith saw English merchant Thomas Mun (1571–1641) as a major creator of the mercantile system, especially in his posthumously published "Treasure by Foreign Trade" (1664), which Smith considered the archetype or manifesto of the movement. Perhaps the last major mercantilist work was James Steuart's "Principles of Political Economy" published in 1767.
"Mercantilist literature" also extended beyond England. Italy and France produced noted writers of mercantilist themes including Italy's Giovanni Botero (1544–1617) and Antonio Serra (1580–?); France's, Jean Bodin, Colbert and other physiocrats. Themes also existed in writers from the German historical school from List, as well as followers of the "American system" and British "free-trade imperialism," thus stretching the system into the 19th century. However, many British writers, including Mun and Misselden, were merchants, while many of the writers from other countries were public officials. Beyond mercantilism as a way of understanding the wealth and power of nations, Mun and Misselden are noted for their viewpoints on a wide range of economic matters.
The Austrian lawyer and scholar Philipp Wilhelm von Hornick, in his "Austria Over All, If She Only Will" of 1684, detailed a nine-point program of what he deemed effective national economy, which sums up the tenets of mercantilism comprehensively:
Other than Von Hornick, there were no mercantilist writers presenting an overarching scheme for the ideal economy, as Adam Smith would later do for classical economics. Rather, each mercantilist writer tended to focus on a single area of the economy. Only later did non-mercantilist scholars integrate these "diverse" ideas into what they called "mercantilism". Some scholars thus reject the idea of mercantilism completely, arguing that it gives "a false unity to disparate events". Smith saw the mercantile system as an enormous conspiracy by manufacturers and merchants against consumers, a view that has led some authors, especially Robert E. Ekelund and Robert D. Tollison to call mercantilism "a rent-seeking society". To a certain extent, mercantilist doctrine itself made a general theory of economics impossible. Mercantilists viewed the economic system as a zero-sum game, in which any gain by one party required a loss by another. Thus, any system of policies that benefited one group would by definition harm the other, and there was no possibility of economics being used to maximize the "commonwealth", or common good. Mercantilists' writings were also generally created to rationalize particular practices rather than as investigations into the best policies.
Mercantilist domestic policy was more fragmented than its trade policy. While Adam Smith portrayed mercantilism as supportive of strict controls over the economy, many mercantilists disagreed. The early modern era was one of letters patent and government-imposed monopolies; some mercantilists supported these, but others acknowledged the corruption and inefficiency of such systems. Many mercantilists also realized that the inevitable results of quotas and price ceilings were black markets. One notion mercantilists widely agreed upon was the need for economic oppression of the working population; laborers and farmers were to live at the "margins of subsistence". The goal was to maximize production, with no concern for consumption. Extra money, free time, or education for the "lower classes" was seen to inevitably lead to vice and laziness, and would result in harm to the economy.
Infinite growth.
The mercantilists saw a large population as a form of wealth which made possible the development of bigger markets and armies. The opposing doctrine of physiocracy predicted that mankind would outgrow its resources. The idea of mercantilism was to protect the markets, but it also helped to maintain the agriculture and those who were dependent upon it.
Origins.
Scholars debate over why mercantilism dominated economic ideology for 250 years. One group, represented by Jacob Viner, sees mercantilism as simply a straightforward, common-sense system whose logical fallacies remained opaque to people at the time, as they simply lacked the required analytical tools.
The second school, supported by scholars such as Robert B. Ekelund, portrays mercantilism not as a mistake, but rather as the best possible system for those who developed it. This school argues that rent-seeking merchants and governments developed and enforced mercantilist policies. Merchants benefited greatly from the enforced monopolies, bans on foreign competition, and poverty of the workers. Governments benefited from the high tariffs and payments from the merchants. Whereas later economic ideas were often developed by academics and philosophers, almost all mercantilist writers were merchants or government officials.
Monetarism offers a third explanation for mercantilism. European trade exported bullion to pay for goods from Asia, thus reducing the money supply and putting downward pressure on prices and economic activity. The evidence for this hypothesis is the lack of inflation in the British economy until the Revolutionary and Napoleonic wars when paper money came into vogue.
A fourth explanation lies in the increasing professionalisation and technification of the wars of the era, which turned the maintenance of adequate reserve funds (in the prospect of war) into a more and more expensive and eventually competitive business.
Mercantilism developed at a time of transition for the European economy. Isolated feudal estates were being replaced by centralized nation-states as the focus of power. Technological changes in shipping and the growth of urban centres led to a rapid increase in international trade.
Mercantilism focused on how this trade could best aid the states. Another important change was the introduction of double-entry bookkeeping and modern accounting. This accounting made extremely clear the inflow and outflow of trade, contributing to the close scrutiny given to the balance of trade.
Of course, the impact of the discovery of America cannot be ignored. New markets and new mines propelled foreign trade to previously inconceivable heights. The latter led to "the great upward movement in prices" and an increase in "the volume of merchant activity itself".
Prior to mercantilism, the most important economic work done in Europe was by the medieval scholastic theorists. The goal of these thinkers was to find an economic system compatible with Christian doctrines of piety and justice. They focused mainly on microeconomics and on local exchanges between individuals. Mercantilism was closely aligned with the other theories and ideas that began to replace the medieval worldview. This period saw the adoption of the very Machiavellian realpolitik and the primacy of the "raison d'état" in international relations. The mercantilist idea of all trade as a zero-sum game, in which each side was trying to best the other in a ruthless competition, was integrated into the works of Thomas Hobbes. The dark view of human nature also fit well with the Puritan view of the world, and some of the most stridently mercantilist legislation, such as the Navigation Ordinance of 1651, was enacted by the government of Oliver Cromwell.
Jean-Baptiste Colbert's work in seventeenth century France came to exemplify classical mercantilism. In the English-speaking world its ideas were criticized by Adam Smith with the publication of "The Wealth of Nations" in 1776 and later David Ricardo with his explanation of comparative advantage. Mercantilism was rejected by Britain and France by the mid-19th century. The British Empire embraced free-trade and used its power as the financial centre of the world to promote the same. The Guyanese historian Walter Rodney describes mercantilism as the period of the world-wide development of European commerce, which began in the fifteenth century with the voyages of Portuguese and Spanish explorers to Africa, Asia and the New World.
Policies.
Mercantilist ideas were the dominant economic ideology of all of Europe in the early modern period, and most states embraced it to a certain degree. Mercantilism was centred in England and France, and it was in these states that mercantilist polices were most often enacted.
France.
Mercantilism arose in France in the early 16th century soon after the monarchy had become the dominant force in French politics. In 1539, an important decree banned the importation of woolen goods from Spain and some parts of Flanders. The next year, a number of restrictions were imposed on the export of bullion.
Over the rest of the sixteenth century further protectionist measures were introduced. The height of French mercantilism is closely associated with Jean-Baptiste Colbert, finance minister for 22 years in the 17th century, to the extent that French mercantilism is sometimes called Colbertism. Under Colbert, the French government became deeply involved in the economy in order to increase exports. Protectionist policies were enacted that limited imports and favored exports. Industries were organized into guilds and monopolies, and production was regulated by the state through a series of over a thousand directives outlining how different products should be produced.
To encourage industry, foreign artisans and craftsmen were imported. Colbert also worked to decrease internal barriers to trade, reducing internal tariffs and building an extensive network of roads and canals. Colbert's policies were quite successful, and France's industrial output and economy grew considerably during this period, as France became the dominant European power. He was less successful in turning France into a major trading power, and Britain and the Netherlands remained supreme in this field.
Great Britain.
In England, mercantilism reached its peak during the Long Parliament government (1640–1660). Mercantilist policies were also embraced throughout much of the Tudor and Stuart periods, with Robert Walpole being another major proponent. In Britain, government control over the domestic economy was far less extensive than on the Continent, limited by common law and the steadily increasing power of Parliament. Government-controlled monopolies were common, especially before the English Civil War, but were often controversial.
With respect to its colonies, British mercantilism meant that the government and the merchants became partners with the goal of increasing political power and private wealth, to the exclusion of other empires. The government protected its merchants – and kept others out – by trade barriers, regulations, and subsidies to domestic industries in order to maximize exports from and minimize imports to the realm. The government had to fight smuggling – which became a favorite American technique in the 18th century to circumvent the restrictions on trading with the French, Spanish or Dutch. The goal of mercantilism was to run trade surpluses, so that gold and silver would pour into London. The government took its share through duties and taxes, with the remainder going to merchants in Britain. The government spent much of its revenue on a superb Royal Navy, which not only protected the British colonies but threatened the colonies of the other empires, and sometimes seized them. Thus the British Navy captured New Amsterdam (New York) in 1664. The colonies were captive markets for British industry, and the goal was to enrich the mother country.
British mercantilist writers were themselves divided on whether domestic controls were necessary. British mercantilism thus mainly took the form of efforts to control trade. A wide array of regulations was put in place to encourage exports and discourage imports. Tariffs were placed on imports and bounties given for exports, and the export of some raw materials was banned completely. The Navigation Acts expelled foreign merchants from England's domestic trade. The nation aggressively sought colonies and once under British control, regulations were imposed that allowed the colony to only produce raw materials and to only trade with Britain. This led to friction with the inhabitants of these colonies, and mercantilist policies (such as forbidding trade with other empires and controls over smuggling) were a major irritant leading to the American Revolution.
Mercantalism taught that trade was a zero-sum game with one country's gain equivalent to a loss sustained by the trading partner. Over all, however, mercantilist policies had a positive impact on Britain helping turn it into the world's dominant trader, and the global hegemon. One domestic policy that had a lasting impact was the conversion of "waste lands" to agricultural use. Mercantilists felt that to maximize a nation's power all land and resources had to be used to their utmost, and this era thus saw projects like the draining of The Fens.
Other countries.
The other nations of Europe also embraced mercantilism to varying degrees. The Netherlands, which had become the financial centre of Europe by being its most efficient trader, had little interest in seeing trade restricted and adopted few mercantilist policies. Mercantilism became prominent in Central Europe and Scandinavia after the Thirty Years' War (1618–1648), with Christina of Sweden, Jacob Kettler of Courland, Christian IV of Denmark being notable proponents. The Habsburg Holy Roman Emperors had long been interested in mercantilist policies, but the vast and decentralized nature of their empire made implementing such notions difficult.
Some constituent states of the empire did embrace Mercantilism, most notably Prussia, which under Frederick the Great had perhaps the most rigidly controlled economy in Europe. During the economic collapse of the seventeenth century Spain had little coherent economic policy, but French mercantilist policies were imported by Philip V with some success. Russia under Peter I (Peter the Great) attempted to pursue mercantilism, but had little success because of Russia's lack of a large merchant class or an industrial base.
Wars and imperialism.
Mercantilism was economic warfare and was well suited to an era of military warfare. Since the level of world trade was viewed as fixed, it followed that the only way to increase a nation's trade was to take it from another. A number of wars, most notably the Anglo-Dutch Wars and the Franco-Dutch Wars, can be linked directly to mercantilist theories. Most wars had other causes but they reinforced mercantilism by clearly defining the enemy, and justified damage to the enemy's economy.
Mercantilism fueled the imperialism of this era, as many nations expended significant effort to build new colonies that would be sources of gold (as in Mexico) or sugar (as in the West Indies), as well as becoming exclusive markets. European power spread around the globe, often under the aegis of companies with government-guaranteed monopolies in certain defined geographical regions, such as the Dutch East India Company or the British Hudson's Bay Company (operating in present-day Canada).
Criticisms.
Adam Smith and David Hume were the founding fathers of anti-mercantilist thought. A number of scholars found important flaws with mercantilism long before Adam Smith developed an ideology that could fully replace it. Critics like Hume, Dudley North, and John Locke undermined much of mercantilism, and it steadily lost favor during the 18th century.
In 1690, John Locke argued that prices vary in proportion to the quantity of money. Locke's "Second Treatise" also points towards the heart of the anti-mercantilist critique: that the wealth of the world is not fixed, but is created by human labor (represented embryonically by Locke's labor theory of value). Mercantilists failed to understand the notions of absolute advantage and comparative advantage (although this idea was only fully fleshed out in 1817 by David Ricardo) and the benefits of trade.
For instance, if Portugal was a more efficient producer of wine than England, yet in England cloth could be produced more efficiently than it could in Portugal. Thus if Portugal specialized in wine and England in cloth, "both" states would end up "better off" if they traded. This is an example of the reciprocal benefits of trade due to a comparative advantage. In modern economic theory, trade is "not" a zero-sum game of cutthroat competition because both sides can benefit.
Hume famously noted the impossibility of the mercantilists' goal of a constant positive balance of trade. As bullion flowed into one country, the supply would increase and the value of bullion in that state would steadily decline relative to other goods. Conversely, in the state exporting bullion, its value would slowly rise. Eventually it would no longer be cost-effective to export goods from the high-price country to the low-price country, and the balance of trade would reverse itself. Mercantilists fundamentally misunderstood this, long arguing that an increase in the money supply simply meant that everyone gets richer.
The importance placed on bullion was also a central target, even if many mercantilists had themselves begun to de-emphasize the importance of gold and silver. Adam Smith noted at the core of the mercantile system was the "popular folly of confusing wealth with money," bullion was just the same as any other commodity, and there was no reason to give it special treatment. More recently, scholars have discounted the accuracy of this critique. They believe Mun and Misselden were not making this mistake in the 1620s, and point to their followers Josiah Child and Charles Davenant, who, in 1699, wrote: "Gold and Silver are indeed the Measure of Trade, but that the Spring and Original of it, in all nations is the Natural or Artificial Product of the Country; that is to say, what this Land or what this Labour and Industry Produces." The critique that mercantilism was a form of rent-seeking has also seen criticism, as scholars such Jacob Viner in the 1930s point out that merchant mercantilists such as Mun understood that they would not gain by higher prices for English wares abroad.
The first school to completely reject mercantilism was the physiocrats, who developed their theories in France. Their theories also had several important problems, and the replacement of mercantilism did not come until Adam Smith published "The Wealth of Nations" in 1776. This book outlines the basics of what is today known as classical economics. Smith spends a considerable portion of the book rebutting the arguments of the mercantilists, though often these are simplified or exaggerated versions of mercantilist thought.
Scholars are also divided over the cause of mercantilism's end. Those who believe the theory was simply an error hold that its replacement was inevitable as soon as Smith's more accurate ideas were unveiled. Those who feel that mercantilism was rent-seeking hold that it ended only when major power shifts occurred. In Britain, mercantilism faded as the Parliament gained the monarch's power to grant monopolies. While the wealthy capitalists who controlled the House of Commons benefited from these monopolies, Parliament found it difficult to implement them because of the high cost of group decision making.
Mercantilist regulations were steadily removed over the course of the Eighteenth Century in Britain, and during the 19th century the British government fully embraced free trade and Smith's laissez-faire economics. On the continent, the process was somewhat different. In France, economic control remained in the hands of the royal family and mercantilism continued until the French Revolution. In Germany mercantilism remained an important ideology in the 19th and early 20th centuries, when the historical school of economics was paramount.
Legacy.
Adam Smith rejected the mercantilist focus on production, arguing that consumption was paramount to production. He added that mercantilism was popular among merchants because it was what is now called "rent seeking". However John Maynard Keynes argued that encouraging production was just as important as consumption, and he favoured the "new mercantilism". Keynes also noted that in the early modern period the focus on the bullion supplies was reasonable. In an era before paper money, an increase for bullion was one of the few ways to increase the money supply. Keynes said mercantilist policies generally improved both domestic and foreign investment. Domestic because the policies lowered the domestic rate of interest. And it increased investment by foreigners in the nation by tending to create a favorable balance of trade.
Keynes and other economists of the 20th century also realized the balance of payments is an important concern. Keynes also supported government intervention in the economy as necessity, as did mercantilism.
s of 2010[ [update]], the word "mercantilism" remains a pejorative term, often used to attack various forms of protectionism. The similarities between Keynesianism, and its successor ideas, with mercantilism have sometimes led critics to call them neo-mercantilism. Indeed, Paul Samuelson, writing within a Keynesian framework, defended mercantilism, writing: "With employment less than full and Net National Product suboptimal, all the debunked mercantilist arguments turn out to be valid."
Some other systems that do copy several mercantilist policies, such as Japan's economic system, are also sometimes called neo-mercantilist. In an essay appearing in the 14 May 2007 issue of "Newsweek", business columnist Robert J. Samuelson argued that China was pursuing an essentially mercantilist trade policy that threatened to undermine the post-World War II international economic structure.
Murray Rothbard, representing the Austrian School of economics, describes it this way:
Mercantilism, which reached its height in the Europe of the seventeenth and eighteenth centuries, was a system of statism which employed economic fallacy to build up a structure of imperial state power, as well as special subsidy and monopolistic privilege to individuals or groups favored by the state. Thus, mercantilism held exports should be encouraged by the government and imports discouraged.
In one area economists rejected Smith well before Keynes: in the use of data. Mercantilists, who were generally merchants or government officials, gathered vast amounts of trade data and used it extensively in their research and writing. William Petty, a strong mercantilist, is generally credited with being the first to use empirical analysis to study the economy. Smith rejected this, arguing that deductive reasoning from base principles was the proper method to discover economic truths. Today, many schools of economics accept that both methods are important.
In specific instances, protectionist mercantilist policies also had an important and positive impact on the state that enacted them. Adam Smith himself, for instance, praised the Navigation Acts as they greatly expanded the British merchant fleet, and played a central role in turning Britain into the naval and economic superpower from the 18th Century onward. Some economists thus feel that protecting infant industries, while causing short-term harm, can be beneficial in the long term.

</doc>
<doc id="19709" url="http://en.wikipedia.org/wiki?curid=19709" title="Meat Puppets">
Meat Puppets

Meat Puppets are an American rock band formed in January 1980, in Phoenix, Arizona. The group's original lineup was Curt Kirkwood (guitar/vocals), his brother Cris Kirkwood (bass guitar), and Derrick Bostrom (drums). The Kirkwood brothers met Bostrom while attending Brophy Prep High School in Phoenix. The three then moved to Tempe, Arizona (a Phoenix suburb and home to Arizona State University) where the Kirkwood brothers purchased two adjacent homes, one of which had a shed in the back where they regularly practiced.
One of the more notable groups on the roster of SST Records (who released most of their albums), Meat Puppets started as a punk rock band, but like most of their SST peers, Meat Puppets established their own unique style, blending punk with country and psychedelic rock, and featuring Curt's warbling vocals. Meat Puppets later gained significant exposure when the Kirkwood brothers served as guest musicians on Nirvana's MTV Unplugged performance in 1993. The band's 1994 album "Too High to Die" subsequently became their most successful release. The band broke up twice, in 1996 and 2002, but reunited again in 2006.
Meat Puppets have influenced various rock bands such as Nirvana, Soundgarden, Dinosaur Jr, Sebadoh and Pavement.
History.
Early career (1980–1990).
In the late 70's, drummer Derrick Bostrom played with guitarist Jack Knetzger in a band called Atomic Bomb Club, which began as a duo, but would come to include bassist Cris Kirkwood. The band played a few local shows and recorded some demos, but began to dissolve quickly thereafter. Derrick and Cris began rehearsing together with Cris' brother Curt Kirkwood by learning songs from Bostrom's collection of punk rock 45s. After briefly toying with the name "The Bastions of Immaturity", they settled on the name Meat Puppets in June, 1980 after a song by Curt of the same name which appears on their first album. Their early works were made up of hardcore punk, and attracted the attention of Joe Carducci as he was starting to work with legendary punk label SST Records. Carducci suggested they sign with the label, and the Meat Puppets released their first album "Meat Puppets" in 1982, which among several new originals and a pair of heavily skewed Doc Watson and Bob Nolan covers, featured the songs "The Gold Mine" and "Melons Rising", two tunes Derrick and Cris originally had written and performed as Atomic Bomb Club previously.
By the release of 1984's "Meat Puppets II", the bandmembers "were so sick of the hardcore thing," according to Bostrom. "We were really into pissing off the crowd." The band experimented with acid rock and country western sounds. While the album had been recorded in early 1983, the album's release was delayed for a year by SST. "Meat Puppets II" turned the band into one of the leading bands on SST Records, and along with the Violent Femmes, the Gun Club and others, helped establish the genre called "cow punk".
Meat Puppets II was followed by 1985's "Up on the Sun". The album's sound resembled the folk-rock of The Byrds more than punk, and some of the group's fans accused Meat Puppets of sounding dangerously like hippies and abandoning their punk roots. In keeping with their unconventional way of doing things, both Cris and Curt purposefully sang the entire album off key.
Over the next decade, Meat Puppets remained on SST and released a series of albums while touring relentlessly. Between tours they would regularly play small shows in bars around the Phoenix area such as "The Mason Jar" and "The Sun Club" in Tempe. After the release of "Out My Way" in 1986, however, the band was briefly sidelined by an accident when Curt's finger was broken after being slammed in their touring van's door. The accident delayed the band's next album, the psychedelic "Mirage", until the next year. The final result was considered their most polished sounding album to date.
Their next album, the heavier "Huevos", came out less than six months afterward, in late summer of 1987. In stark contrast to its predecessor, "Huevos" was recorded in a swift, fiery fashion, with many first takes, and minimal second guessing. These recordings were completed in only a matter of days, and along with a few drawings and one of Curt's paintings taken from the wall to serve as cover art (a dish of three boiled eggs, a green pepper, and a bottle of Tabasco sauce), were all sent to SST shortly before the band returned to the road en route to their next gig. Curt revealed in an interview that one of the reasons for the album being called Huevos (meaning 'eggs' in Spanish) was because of the multitude of first-takers on the record, as similarly eggs can only be used once.
"Monsters" was released in 1989, featuring a new sound with extended jams such as "Touchdown King" and "Flight of the Fire Weasel".
Major label career (1991–1995).
As numerous bands from the seminal SST label and other kindred punk-oriented indies had before them, Meat Puppets grappled with the decision to switch to a major label. Two years after their final studio recording for SST, 1989's Monsters the trio released its major-label debut, "Forbidden Places", on the indie-friendly London Records. "Forbidden Places" is now out of print.
In 1992 following his departure from the Red Hot Chili Peppers, guitarist John Frusciante auditioned for the band. Cris Kirkwood stated “He showed up with his guitar out of its case and barefoot. We were on a major label then, we just got signed, and those guys had blown up to where they were at and John needed to get out. John gets to our pad and we started getting ready to play and I said, ‘You want to use my tuner?’ He said, ‘No, I’ll bend it in.’ It was so far out. Then we jammed but it didn’t come to anything. Maybe he wasn’t in the right place and we were a tight little unit. It just didn’t quite happen but it could have worked." 
In late 1993, Meat Puppets achieved mainstream popularity when Nirvana's Kurt Cobain, who became a fan after seeing them open for Black Flag, invited Cris and Curt to join him on MTV Unplugged for acoustic performances of "Plateau", "Oh Me" and "Lake of Fire" (all originally from "Meat Puppets II"). The resulting album, "MTV Unplugged in New York," served as a swan song for Nirvana, as Cobain died 138 days after the concert. "Lake of Fire" became a cult favorite for its particularly wrenching vocal performance from Cobain. Subsequently, the Nirvana exposure and the strength of the single "Backwater" (their only charting single) helped lift Meat Puppets to new commercial heights. The band's studio return was 1994's "Too High To Die", produced by Butthole Surfers guitarist Paul Leary. The album featured "Backwater", a minor hit on alternative radio, and a hidden-track update of "Lake of Fire." "Too High To Die" earned the 'Pups a gold record (500,000 sold), outselling their previous records combined.
1995's "No Joke!" was the final album recorded by the original Meat Puppets lineup. Though the band's drug use included cocaine, heroin, LSD and many others, Cris' use of heroin and crack cocaine became so bad he rarely left his house except to obtain more drugs. At least two people (including his wife and one of his best friends) died of overdoses at his house in Tempe, AZ during this time. The Kirkwood brothers had always had a legendary appetite for illegal substances and during the tour to support "Too High To Die" with Stone Temple Pilots, the easy availability of drugs was too much for Cris. When it was over, he was severely addicted to cocaine.
First hiatus and reunion (1996–2001).
Derrick recorded a solo EP under the moniker "Today's Sounds" in 1996, and later on in 1999 took charge of re-issuing the Puppets' original seven records on Rykodisc as well as putting out their first live album, "Live in Montana." Curt formed a new band in Austin, TX called the Royal Neanderthal Orchestra, but they changed their name to Meat Puppets for legal reasons and released a promotional EP entitled "You Love Me" in 1999, "Golden Lies" in 2000 and "Live" in 2002. The line-up was Curt (voc/git), Kyle Ellison (voc/git), Andrew Duplantis (voc/bass) and Shandon Sahm (drums). Sahm's father was the legendary fiddler-singer-songwriter Doug Sahm of The Sir Douglas Quintet and Texas Tornados. The concluding track to "Classic Puppets" entitled "New Leaf" also dates from this incarnation of the band.
Break up (2002–2005).
Around 2002, the Meat Puppets dissolved as Curt had gone on to release albums with the groups Eyes Adrift and Volcano. In 2005, he released his first solo album entitled "Snow".
Bassist Cris was arrested in December 2003 for attacking a security guard at the main post office in downtown Phoenix, AZ with the guard's baton. The guard shot Kirkwood in the stomach at least twice during the melee, causing serious gunshot injuries requiring major surgery. Kirkwood was subsequently denied bail, the judge citing Kirkwood's previous drug arrests and probation violations. He eventually went to prison at the Arizona state prison in Florence, Arizona for felony assault. He was released in July 2005.
Derrick Bostrom began a web site for the band about six months before the original trio stopped working together. The site went through many different permutations before it was essentially mothballed in 2003. In late 2005, Bostrom revamped it once again, this time as a "blog" for his recollections and as a place to share pieces of Meat Puppets history.
Second reunion (2006–present).
On March 24, 2006, Curt Kirkwood polled fans at his MySpace page with a bulletin that asked: "Question for all ! Would the original line up of the Meat Puppets interest anyone ? Feedback is good — do you want a reunion!?" The response from fans was overwhelmingly positive within a couple of hours, leading to speculation of a full-blown Meat Puppets reunion in the near future. However, a post made by Derrick Bostrom on the official Meat Puppets site dismissed the notion.
In April 2006 "Billboard" reported that the Kirkwood brothers would reunite as the Meat Puppets without original drummer Derrick Bostrom. Although Primus drummer Tim Alexander was announced as Bostrom's replacement, the position was later filled by Ted Marcus. The new lineup recorded a new full-length album, "Rise to Your Knees", in mid-to-late 2006. The album was released by Anodyne Records on July 17, 2007.
On January 20, 2007, The Meat Puppets brothers performed two songs during an Army of Anyone concert, at La Zona Rosa in Austin, Texas. The first song was played with Curt Kirkwood and Cris Kirkwood along with Army of Anyone's Ray Luzier and Dean DeLeo. Then the second song was played with original members Curt and Cris Kirkwood and new Meat Puppets drummer Ted Marcus. This was in the middle of Army of Anyone's set, which they listed as "Meat Puppet Theatre" on the evening's set list. The band performed several new songs in March at the South by Southwest festival. On March 28, 2007, the band announced a West Coast tour through their MySpace page. This is the first tour with original bassist Cris in eleven years. The tour continued into the east coast and midwest later in 2007.
In 2008 they performed their classic second album live in its entirety at the ATP New York festival.
The band parted ways with Anodyne, signed to Megaforce and began recording new material in the winter of 2008. The resulting album, entitled "Sewn Together", was released on May 12, 2009.
In the summer of 2009 the band continued to tour across America. They appeared in Rochester Minnesota outside in front of over 5,000 fans, after playing Summerfest in Milwaukee, Wisconsin the night prior. The Meat Puppets performed at the 2009 Voodoo Music Experience in New Orleans over the Halloween weekend.
As of November 2009, Shandon Sahm is back as the drummer in the Meat Puppets, replacing Ted Marcus. The band was chosen by Animal Collective to perform the album 'Up on the Sun' live in its entirety at the All Tomorrow's Parties festival that they curated in May 2011.
The band's thirteenth studio album, entitled "Lollipop", was released on April 12, 2011. The Dandies supported the Meat Puppets on all European dates in 2011.
The Meat Puppets have played several gigs in their hometown since 2009, such as the Marquee show in June 2011 with Dead Confederate.
As of early 2011 Elmo Kirkwood, son of Curt Kirkwood and nephew of Cris Kirkwood, was touring regularly with the band playing rhythm guitar.
The Meat Puppets also contributed to Spin Magazine's exclusive album , playing Nirvana's Smells Like Teen Spirit.
In June 2012, a book titled "Too High to Die: Meet the Meat Puppets" by author Greg Prato was released, which featured all-new interviews with band members past and present and friends of the band, and covered the band's entire career.
In October 2012, it was announced that the group had just completed recording new songs. "Rat Farm", the band's 14th album, was released in April 2013.
In March 2013 the Meat Puppets played arguably their biggest gig since reunion, opening for Dave Grohl's Sound City Players at the SXSW Festival in Austin, TX 
In April 2014 the Meat Puppets completed a tour with The Moistboyz.

</doc>
<doc id="19710" url="http://en.wikipedia.org/wiki?curid=19710" title="List of mathematics competitions">
List of mathematics competitions

Mathematics competitions or mathematical olympiads are competitive events where participants sit a mathematics test. These tests may require multiple choice or numeric answers, or a detailed written solution or proof.
Online mathematics competitions.
Online Math Competitions are competitions that are completely free of charge and accessible to everyone on the internet
National mathematics olympiads.
Albania.
a) Olimpiada Kombetare e Matematikes
b) Olimpiada Mbarekombetare e Revistes Plus
Belgium.
French-speaking students from Belgium and Luxembourg can compete in the OMB (Olympiade Mathématique Belge) consisting of three categories:
Dutch-speaking students can compete in the VWO (Vlaamse Wiskunde Olympiade) and in Kangoeroe, with six categories:
University competitions include:
Brazil.
There are two national competitions in Brazil: the oldest one, OBM, dates back to 1979 and is open to all students from fifth grade to university.
The other one, OBMEP, was created in 2005 and is open to public school students from fifth grade to high school. In 2008 it counted with the participation of 18,3 million students on its first round.
There are also many regional competitions, usually open to all students of a given state.
Canada.
The Canadian Mathematics Olympiad (CMO) is the official competition whose top performers earn the right to represent Canada at the International Mathematical Olympiad (IMO). It runs each April. To be invited to write the CMO, students need to do very well on at least one of these:
 (since 1999) :
The New Pythagoreans Math Competition (http://www.school4math.ca) since 2014, Grades 1-12, hosted annually during May/June. It is an enthusiastic initiative to present math in an enjoyable way to students of all grades when thinking outside the box of the standard multiple choice question.
Canadian Math Kangaroo Contest (http://www.mathkangaroocanada.com) since 2001
International competitions hosted by (since 1969, past problems available):
Full Solutions:
Multiple Choice:
National competitions hosted by (since 2005):
Multiple Choice:
 (formerly MathCounts BC) is called MathChallengers since 2005. It is hosted by .(8th and 9th grade students)
National competitions hosted by (since 2009, past problems available, online contest):
 (since 1996) :
Colombia.
Web site: http://olimpia.uan.edu.co/olimpiadas/public/frameset.jsp
Greece.
Seen also 
Indonesia.
Singapore and Asian Schools Maths Olympiad Indonesia (SASMO Indonesia) http://www.mathsolympiads.org/indonesia
National Science Olympiad held at every level of elementary education, secondary and higher education
1. Educational Level Elementary Science Olympiad (Olimpiade Sains Nasional SD)
2. Science Olympiad Level Secondary Education (Olimpiade Sains Nasional SMP)
3. Higher Education Level Science Olympiad (Olimpiade Sains Nsional SMA)
Science Olympiad at higher education level or referred to the National Olympiad of Mathematics and Natural Sciences (ON MIPA) consists of 4 areas, namely Mathematics, Physics, Chemistry and Biology, held in 3 stages.
The first stage in college, the second phase in Kopertis and the third stage in the Directorate General of Higher Education.
Kenya.
Mangu National Mathematics Contest
( formerly Moi National Mathenatics Contest) which is held annually in the month of June at Mangu High School.
The test paper is out of 30.
To take the test, participating schools need to organise students in teams.
Each team must consist of 10 students(6 form four students and 4 form three students).
Several hundreds of schools attend the event with some schools presenting over four teams.
There are oral quizzes which are open to anyone. The first to solve and present the answer to these quizzes are awarded with instant goodies.
<http://www.elimu2.info/manguhighnew/content.php?pid=45>
Held annually at Alliance Girl's High School.
Both paper test and oral quizzes conducted.
<http://www.alliancegirlshigh.com/content.php?pid=36>
Macedonia.
"Official website (in Macedonian):" http://smm.org.mk/
Nigeria.
 Globe Mathematics Annual Quiz Competition Organized by Federal Ministry of Education for Unity colleges in Nigeria,around January to February.
Pakistan.
It is a Mathematics test based on Multiple Choice Questions.
PakTurk International Schools and Colleges have been successfully organizing the Inter Schools Maths Olympiad (ISMO) for the students of private as well as the State Schools across Pakistan since 2005. ISMO has become a national event and being conducted all over Pakistan.
This competition is conducted at different cities simultaneously. Attractive Cash Prizes along with Shields and Certificates are given away to the qualifying candidates. The first position holder (studying in class VIII) is awarded by the title "Al-Khwarizmi of Pakistan, of the year" after the name of Muḥammad ibn Mūsā al-Khwārizmī (Arabic: عَبْدَالله مُحَمَّد بِن مُوسَى اَلْخْوَارِزْمِي) (c. 780 - c. 850), a Persian Muslim mathematician, astronomer and geographer during the Abbasid Empire, a scholar in the House of Wisdom in Baghdad.
Links:
In 2006 almost 4,000,000 students from 41 countries played the game world-wide. The world "Kangaroo" center, which coordinates the competition in the various countries, was founded in 1994 in Paris. In Pakistan, the competition was first organized in 2005 by the Pakistan kangaroo Commission.
Peru.
It's the official Olympiad, organized by the Ministry of Education and the Peruvian Mathematical Society in 4 phases. The final phase takes place near to Lima usually in November.
Website available in Spanish: https://onemperu.wordpress.com/
Turkey.
http://matematik.fen.akdeniz.edu.tr/
United States.
Generally, registering for these contests is based on the grade level of math at which the student works rather than the age or the enrolled grade of the student. Also normally only competitions where the participants write a full proof are called Mathematical Olympiads.
Regional competitions.
See List of United States regional mathematics competitions.

</doc>
<doc id="19711" url="http://en.wikipedia.org/wiki?curid=19711" title="Michael Polanyi">
Michael Polanyi

Michael Polanyi, FRS (11 March 1891 – 22 February 1976) was a Hungarian-British polymath, who made important theoretical contributions to physical chemistry, economics, and philosophy. He argued that positivism supplies a false account of knowing, which if taken seriously undermines our highest achievements as human beings.
His wide-ranging research in physical science included chemical kinetics, x-ray diffraction, and adsorption of gases. He pioneered the theory of fibre diffraction analysis in 1921, and the dislocation theory of plastic deformation of ductile metals and other materials in 1934.
He emigrated to Germany, in 1926 becoming a chemistry professor at the Kaiser Wilhelm Institute in Berlin, and then in 1933 to England, becoming first a chemistry professor, and then a social sciences professor at the University of Manchester. Two of his chemistry pupils and his son won Nobel Prizes. He was elected to the Royal Society and the American Academy of Arts and Sciences. His contributions to the social sciences, for example his application of the concept of a polycentric spontaneous order, were developed in the context of his opposition to central planning.
Life.
Early life.
Polanyi, born Pollacsek Mihály in Budapest, was the fifth child of Mihály and Cecília Pollacsek (née Cecília Wohl), secular Jews from Ungvár (then in Hungary but now in Ukraine) and Vilnius in Lithuania, respectively. His father's family were entrepreneurs, while his mother's father was the chief rabbi of Vilnius. The family moved to Budapest and Magyarized their surname to Polányi. His father built much of the Hungarian railway system, but lost most of his fortune in 1899 when bad weather caused a railway building project to go over budget. He died in 1905. Cecília Polányi established a salon that was well known among Budapest's intellectuals, and continued until her death in 1939. His older brother was Karl Polanyi, the political economist and anthropologist, and his niece was Eva Zeisel, a world-renowned ceramist.
Education.
In 1909, after leaving the famous Budapest teacher-training secondary school (Mintagymnasium), he trained as a physician, obtaining a medical diploma in 1914. He was an active member of the Galilei Society. With the support of Ignác Pfeifer, professor of chemistry at the József Technical University of Budapest, he obtained a scholarship to study chemistry at the Technische Hochschule in Karlsruhe, Germany. In the First World War, he served in the Austro-Hungarian army as a medical officer, and was sent to the Serbian front. While on sick-leave in 1916, he wrote a PhD thesis on adsorption. His research, which was encouraged by Albert Einstein, was supervised by Gusztáv Buchböck, and in 1919 the University of Budapest awarded him a doctorate.
Career.
In October 1918, Mihály Károlyi established the Hungarian Democratic Republic, and Polanyi became Secretary to the Minister of Health. When Communists seized power in March 1919 he refused to serve in the Red Army and returned to medicine. When the Hungarian Soviet Republic was overthrown, Polanyi emigrated to Karlsruhe, and was invited by Fritz Haber to join the Kaiser Wilhelm Institut für Faserstoffchemie in Berlin. In 1923 Polanyi converted to Christianity, and in a Roman Catholic ceremony married Magda Elizabeth Kemeny. In 1926 he became the professorial head of department of the Institut für Physikalische Chemie und Elektrochemie. In 1929, Magda gave birth to their son John, who when he reached adulthood settled in Canada, and was awarded a Nobel Prize in chemistry in 1986. Their other son, George Polanyi, became a well-known British economist.
His experience of runaway inflation and high unemployment in Weimar Germany led Polanyi to become interested in economics. With the coming to power in 1933 of the Nazi party, he accepted a chair in physical chemistry at the University of Manchester. Two of his pupils, Eugene Wigner and Melvin Calvin went on to win a Nobel Prize. Because of his increasing interest in the social sciences, Manchester University created a new chair in Social Science (1948–58) for him.
In 1944 Polanyi was elected a member of the Royal Society, and on his retirement from the University of Manchester in 1958 he was elected a Senior Research Fellow at Merton College, Oxford. In 1962 he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.
Work.
Physical chemistry.
Polanyi's scientific interests were extremely diverse, including work in chemical kinetics, x-ray diffraction, and the adsorption of gases at solid surfaces. He is also well known for his potential adsorption theory, which was disputed for quite some time. In 1921, he laid the mathematical foundation of fibre diffraction analysis. In 1934, Polanyi, at about the same time as G. I. Taylor and Egon Orowan, realised that the plastic deformation of ductile materials could be explained in terms of the theory of dislocations developed by Vito Volterra in 1905. The insight was critical in developing the field of solid mechanics.
Freedom and community.
In 1936, as a consequence of an invitation to give lectures for the Ministry of Heavy Industry in the USSR, Polanyi met Bukharin, who told him that in socialist societies all scientific research is directed to accord with the needs of the latest Five Year Plan. Polanyi noted what had happened to the study of genetics in the Soviet Union once the doctrines of Trofim Lysenko had gained the backing of the State. Demands in Britain, for example by the Marxist John Desmond Bernal, for centrally planned scientific research led Polanyi to defend the claim that science requires free debate. Together with John Baker, he founded the influential Society for Freedom in Science.
In a series of articles, re-published in "The Contempt of Freedom" (1940) and "The Logic of Liberty" (1951), Polanyi claimed that co-operation amongst scientists is analogous to the way agents co-ordinate themselves within a free market. Just as consumers in a free market determine the value of products, science is a spontaneous order that arises as a consequence of open debate amongst specialists. Science (contrary to the claims of Bukharin) flourishes when scientists have the liberty to pursue truth as an end in itself:
"[S]cientists, freely making their own choice of problems and pursuing them in the light of their own personal judgment, are in fact co-operating as members of a closely knit organization."
"Such self-co-ordination of independent initiatives leads to a joint result which is unpremeditated by any of those who bring it about."
"Any attempt to organize the group ... under a single authority would eliminate their independent initiatives, and thus reduce their joint effectiveness to that of the single person directing them from the centre. It would, in effect, paralyse their co-operation."
He derived the phrase spontaneous order from Gestalt psychology, and it was adopted by the classical liberal economist Frederick Hayek, although the concept can be traced back to at least Adam Smith. Polanyi (unlike Hayek) argued that there are higher and lower forms of spontaneous order, and he asserted that defending scientific inquiry on utilitarian or sceptical grounds undermined the practice of science. He extends this into a general claim about free societies. Polanyi defends a free society not on the negative grounds that we ought to respect "private liberties", but on the positive grounds that "public liberties" facilitate our pursuit of objective ideals.
According to Polanyi, a free society that strives to be value-neutral undermines its own justification. But it is not enough for the members of a free society to believe that ideals such as truth, justice, and beauty, are objective, they also have to accept that they transcend our ability to wholly capture them. The objectivity of values must be combined with acceptance that all knowing is fallible.
In "Full Employment and Free Trade" (1948) Polanyi analyses the way money circulates around an economy, and in a monetarist analysis that, according to Paul Craig Roberts, was thirty years ahead of its time, he argues that a free market economy should not be left to be wholly self-adjusting. A central bank should attempt to moderate economic booms/busts via a strict/loose monetary policy.
All knowing is personal.
In his book "Science, Faith and Society" (1946), Polanyi set out his opposition to a positivist account of science, noting that it ignores the role personal commitments play in the practice of science. Polanyi was invited to give the prestigious Gifford Lectures in 1951-2 at Aberdeen. A revised version of his lectures were later published as "Personal Knowledge" (1958). In this book Polanyi claims that all knowledge claims (including those that derive from rules) rely on personal judgements. He denies that a scientific method can yield truth mechanically. All knowing, no matter how formalised, relies upon commitments. Polanyi argued that the assumptions that underlie critical philosophy are not only false, they undermine the commitments that motivate our highest achievements. He advocates a fiduciary post-critical approach, in which we recognise that we believe more than we can prove, and know more than we can say.
A knower does not stand apart from the universe, but participates personally within it. Our intellectual skills are driven by passionate commitments that motivate discovery and validation. According to Polanyi, a great scientist not only identifies patterns, but also chooses significant questions likely to lead to a successful resolution. Innovators risk their reputation by committing to a hypothesis. Polanyi cites the example of Copernicus, who declared that the Earth revolves around the Sun. He claims that Copernicus arrived at the Earth's true relation to the Sun not as a consequence of following a method, but via "the greater intellectual satisfaction he derived from the celestial panorama as seen from the Sun instead of the Earth." His writings on the practice of science influenced Thomas Kuhn and Paul Feyerabend.
Polanyi rejected the claim by British Empiricists that experience can be reduced into sense data, but he also rejects the notion that "indwelling" within (sometimes incompatible) interpretative frameworks traps us within them. Our tacit awareness connects us, albeit fallibly, with reality. It supplies us with the context within which our articulations have meaning. Contrary to the views of his colleague and friend Alan Turing, whose work at The University of Manchester prepared the way for the first modern computer, he denied that minds are reducible to collections of rules. His work influenced the critique by Hubert Dreyfus of "First Generation" Artificial Intelligence.
It was while writing "Personal Knowledge" that he identified the "structure of tacit knowing". He viewed it as his most important discovery. He claimed that we experience the world by integrating our subsidiary awareness into a focal awareness. In his later work, for example his Terry Lectures, later published as "The Tacit Dimension" (1966) he distinguishes between the phenomenological, instrumental, semantic, and ontological aspects of tacit knowing, as discussed (but not necessarily identified as such) in his previous writing.
Critique of reductionism.
In "Life's irreducible structure" (1968), Polanyi argues that the information contained in the DNA molecule is not reducible to the laws of physics and chemistry. Although a DNA molecule cannot exist without physical properties, these properties are constrained by higher-level ordering principles. In "Transcendence and Self-transcendence" (1970), Polanyi criticises the mechanistic world view that modern science inherited from Galileo.
Polanyi advocates emergence i.e. the claim that there are several levels of reality and of causality. He relies on the assumption that boundary conditions supply degrees of freedom that, instead of being random, are determined by higher-level realities, whose properties are dependent on but distinct from the lower level from which they emerge. An example of a higher-level reality functioning as a downward causal force is consciousness – intentionality – generating meanings – intensionality.
Mind is a higher-level expression of the capacity of living organisms for discrimination. Our pursuit of self-set ideals such as truth and justice transforms our understanding of the world. The reductionistic attempt to reduce higher-level realities into lower-level realities generates what Polanyi calls a moral inversion, in which the higher is rejected with moral passion. Polanyi identifies it as a pathology of the modern mind and traces its origins to a false conception of knowledge; although it is relatively harmless in the formal sciences, that pathology generates nihilism in the humanities. Polanyi considered Marxism an example of moral inversion. In Marxism, the State, ostensibly acting in accordance with the logic of history, is obliged to use its coercive powers in ways that disregard any appeals to morality.
Bibliography.
</dl>
External links.
</dl>

</doc>
<doc id="19712" url="http://en.wikipedia.org/wiki?curid=19712" title="Methanol">
Methanol

Methanol, also known as methyl alcohol, wood alcohol, wood naphtha or wood spirits, is a chemical with the formula CH3OH (often abbreviated MeOH). Methanol acquired the name "wood alcohol" because it was once produced chiefly as a byproduct of the destructive distillation of wood. Modern methanol is produced in a catalytic industrial process directly from carbon monoxide, carbon dioxide, and hydrogen.
Methanol is the simplest alcohol, and is a light, volatile, colorless, flammable liquid with a distinctive odor very similar to that of ethanol (drinking alcohol). However, unlike ethanol, methanol is highly toxic and unfit for consumption. At room temperature, it is a polar liquid, and is used as an antifreeze, solvent, fuel, and as a denaturant for ethanol. It is also used for producing biodiesel via transesterification reaction.
Methanol is produced naturally in the anaerobic metabolism of many varieties of bacteria, and is commonly present in small amounts in the environment. As a result, there is a small fraction of methanol vapor in the atmosphere. Over the course of several days, atmospheric methanol is oxidized with the help of sunlight to carbon dioxide and water.
Methanol also forms in abundant quantities in star forming regions of space, and is used in astronomy as a marker for such regions. It is detected through its spectral emission lines.
Methanol burns in oxygen, including open air, forming carbon dioxide and water:
Methanol ingested in large quantities is metabolized to formic acid or formate salts, which is poisonous to the central nervous system, and may cause blindness, coma, and death. Because of these toxic properties, methanol is frequently used as a denaturant additive for ethanol manufactured for industrial uses. This addition of methanol exempts industrial ethanol (commonly known as "denatured alcohol" or "methylated spirit") from liquor excise taxation in the US and some other countries.
Occurrence.
Human metabolite.
Methanol is poisonous to the central nervous system and may cause blindness, coma, and death. However, in small amounts, methanol is a natural endogenous compound found in normal, healthy human individuals, concluded by one study which found a mean of 4.5 ppm in the exhaled breath of the subjects. The mean endogenous methanol in humans of 0.45 g/d may be metabolized from pectin found in fruit; one kilogram of apple produces up to 1.4 g methanol.
Toxicity.
Methanol has a high toxicity in humans. If as little as 10 mL of pure methanol is ingested, for example, it can break down into formic acid, which can cause permanent blindness by destruction of the optic nerve, and 30 mL is potentially fatal, although the median lethal dose is typically 100 mL (3.4 fl oz) (i.e. 1–2 mL/kg body weight of pure methanol). Reference dose for methanol is 2 mg/kg/day. Toxic effects take hours to start, and effective antidotes can often prevent permanent damage. Because of its similarities in both appearance and odor to ethanol (the alcohol in beverages), it is difficult to differentiate between the two (such is also the case with denatured alcohol). However, there are cases of methanol resistance, such as that of Mike Malloy, who was the victim of a failed murder attempt by methanol in the early 1930s.
Methanol is toxic by two mechanisms. First, methanol (whether it enters the body by ingestion, inhalation, or absorption through the skin) can be fatal due to its CNS depressant properties in the same manner as ethanol poisoning. Second, in a process of toxication, it is metabolized to formic acid (which is present as the formate ion) via formaldehyde in a process initiated by the enzyme alcohol dehydrogenase in the liver. Methanol is converted to formaldehyde via alcohol dehydrogenase (ADH) and formaldehyde is converted to formic acid (formate) via aldehyde dehydrogenase (ALDH). The conversion to formate via ALDH proceeds completely, with no detectable formaldehyde remaining. Formate is toxic because it inhibits mitochondrial cytochrome c oxidase, causing the symptoms of hypoxia at the cellular level, and also causing metabolic acidosis, among a variety of other metabolic disturbances.
Methanol poisoning can be treated with fomepizole, or if unavailable, ethanol. Both drugs act to reduce the action of alcohol dehydrogenase on methanol by means of competitive inhibition, so it is excreted by the kidneys rather than being transformed into toxic metabolites. Further treatment may include giving sodium bicarbonate for metabolic acidosis, and hemodialysis or hemodiafiltration can be used to remove methanol and formate from the blood. Folinic acid or folic acid is also administered to enhance the metabolism of formate.
The initial symptoms of methanol intoxication include central nervous system depression, headache, dizziness, nausea, lack of coordination, and confusion. Sufficiently large doses can cause unconsciousness and death. The initial symptoms of methanol exposure are usually less severe than the symptoms resulting from the ingestion of a similar quantity of ethanol. Once the initial symptoms have passed, a second set of symptoms arises, 10 to as many as 30 hours after the initial exposure to methanol, including blurring or complete loss of vision, acidosis and putaminal hemorrhages, an uncommon but serious complication. These symptoms result from the accumulation of toxic levels of formate in the blood, and may progress to death by respiratory failure. Physical examination may show tachypnea, and ophthalmologic examination may show dilated pupils with hyperemia of the optic disc and retinal edema. Small amounts of methanol are produced by the metabolism of food and are generally harmless, being metabolized quickly and completely.
Ethanol is sometimes denatured (adulterated), and made poisonous, by the addition of methanol. The result is known as methylated spirit, "meths" (British use) or "metho" (Australian slang). These are not to be confused with "meth", a common abbreviation for methamphetamine, and an abbreviation for methadone in Britain.
Applications.
Methanol, a common laboratory solvent, is especially useful for HPLC, UV/VIS spectroscopy, and LCMS due to its low UV cutoff.
Feedstock.
The largest use of methanol by far is in making other chemicals. About 40% of methanol is converted to formaldehyde, and from there into products as diverse as plastics, plywood, paints, explosives, and permanent press textiles.
Also in the early 1970s, a methanol to gasoline process was developed by Mobil for producing gasoline ready for use in vehicles. One such industrial facility was built at Motunui in New Zealand in the 1980s. In the 1990s, large amounts of methanol were used in the United States to produce the gasoline additive methyl tert-butyl ether (MTBE). While MTBE is no longer marketed in the U.S., it is still widely used in other parts of the world. In addition to direct use as a fuel, methanol (or less commonly, ethanol) is used as a component in the transesterification of triglycerides to yield a form of biodiesel.
Other chemical derivatives of methanol include dimethyl ether, which has replaced chlorofluorocarbons as an aerosol spray propellant, and acetic acid. Dimethyl ether (DME) also can be blended with liquified petroleum gas (LPG) for home heating and cooking, and can be used as a diesel replacement for transportation fuel.
Methanol-to-Olefins/Methanol-to-Propylene (MTO/MTP), among others processes such as: Metathesis, Propane Dehydrogenation (PDH), High Severity FCC, and Olefins Cracking, is a new and novel lower-cost chemical process for on-purpose propylene production technology of high interest to the petrochemical marketplace, to supply the tight market for propylene.
The market became tight because of the ethane prices falling in the USA, due to the exploration of shale gas reserves. The low price ethylene produced from this raw material has given chemical producers in North America a feedstock advantage. Such change has put naphtha-fed steam crackers at a disadvantageous position, with many of them shutting down or revamping to use ethane as feedstock. Nevertheless, the propylene output rates from ethane-fed crackers are negligible.
Fuel for vehicles.
Methanol is used on a limited basis to fuel internal combustion engines. Pure methanol is required by rule to be used in Champcars, Monster Trucks, USAC sprint cars (as well as midgets, modifieds, etc.), and other dirt track series, such as World of Outlaws, and Motorcycle Speedway. Methanol is also used, as the primary fuel ingredient since the late 1940s, in the powerplants for radio control, control line and free flight airplanes (as methanol is required in the engines that primarily power them), cars and trucks, from such an engine's use of a platinum filament glow plug being able to ignite the methanol vapor through a catalytic reaction. Drag racers and mud racers, as well as heavily modified tractor pullers, also use methanol as their primary fuel source. Methanol is required with a supercharged engine in a Top Alcohol Dragster and, until the end of the 2006 season, all vehicles in the Indianapolis 500 had to run methanol. Mud racers have mixed methanol with gasoline with nitrous oxide to produce more power than mixing gasoline and nitrous oxide alone.
One of the potential drawbacks of using high concentrations of methanol (and other alcohols, such as ethanol) in fuel is the corrosivity to some metals of methanol, particularly to aluminium. Methanol, although a weak acid, attacks the oxide coating that normally protects the aluminum from corrosion:
The resulting methoxide salts are soluble in methanol, resulting in a clean aluminium surface, which is readily oxidized by dissolved oxygen. Also, the methanol can act as an oxidizer:
This reciprocal process effectively fuels corrosion until either the metal is eaten away or the concentration of CH3OH is negligible. Concerns with methanol's corrosivity have been addressed by using methanol-compatible materials, and fuel additives that serve as corrosion inhibitors.
When produced from wood or other organic materials, the resulting organic methanol (bioalcohol) has been suggested as renewable alternative to petroleum-based hydrocarbons. Low levels of methanol can be used in existing vehicles, with the use of proper cosolvents and corrosion inhibitors.
Methanol fuel has been proposed for ground transportation. The chief advantage of a methanol economy is that it could be adapted to present internal combustion engines with a minimum of modification in both engines and infrastructure to store and deliver liquid fuel.
Government policy.
The European Fuel Quality Directive allows up to 3% methanol with an equal amount of cosolvent to be blending in gasoline sold in Europe. China uses more than one billion gallons of methanol per year as a transportation fuel in both low level blends used in existing vehicles, and as high level blends in vehicles designed to accommodate the use of methanol fuels.
In the US in 2011, the Open Fuel Standard Act of 2011 was introduced in the US Congress to encourage car manufacturers to warrant their cars to burn methanol as a fuel in addition to gasoline and ethanol. The bill is being championed by the Open Fuel Standard Coalition.
Other applications.
Methanol is a traditional denaturant for ethanol, the product being known as "denatured alcohol" or "methylated spirit". This was commonly used during the Prohibition to discourage consumption of bootlegged liquor, and ended up causing several deaths.
Methanol is also used as a solvent, and as an antifreeze in pipelines and windshield washer fluid.
In some wastewater treatment plants, a small amount of methanol is added to wastewater to provide a carbon food source for the denitrifying bacteria, which convert nitrates to nitrogen to reduce the nitrification of sensitive aquifers.
During World War II, methanol was used as a fuel in several German military rocket designs, under the name M-Stoff, and in a roughly 50/50 mixture with hydrazine, known as C-Stoff.
Methanol was used as an automobile coolant antifreeze in the early 1900s.
Methanol is used as a denaturing agent in polyacrylamide gel electrophoresis.
Direct-methanol fuel cells are unique in their low temperature, atmospheric pressure operation, allowing them to be miniaturized to an unprecedented degree. This, combined with the relatively easy and safe storage and handling of methanol, may open the possibility of fuel cell-powered consumer electronics, such as for laptop computers and mobile phones.
Methanol is also a widely used fuel in camping and boating stoves. Methanol burns well in an unpressurized burner, so alcohol stoves are often very simple, sometimes little more than a cup to hold fuel. This lack of complexity makes them a favorite of hikers who spend extended time in the wilderness. Similarly, the alcohol can also be gelled to reduce risk of leaking or spilling, as with the brand "Sterno".
Methanol is mixed with water and injected into high performance diesel and gasoline engines for an increase of power and a decrease in intake air temperature in a process known as water methanol injection.
Energy carrier.
Methanol is also useful as an energy carrier. It is easier to store than hydrogen, burns cleaner than fossil fuels, and decomposes to water and carbon dioxide if spilled.
Safety in automotive fuels.
Pure methanol has been used in open wheel auto racing since the mid-1960s. Unlike petroleum fires, methanol fires can be extinguished with plain water. A methanol-based fire burns invisibly, unlike gasoline, which burns with a visible flame. If a fire occurs on the track, there is no flame or smoke to obstruct the view of fast approaching drivers, but this can also delay visual detection of the fire and the initiation of fire suppression. The decision to permanently switch to methanol in American IndyCar racing was a result of the devastating crash and explosion at the 1964 Indianapolis 500, which killed drivers Eddie Sachs and Dave MacDonald. In 2007 IndyCars switched to ethanol.
Methanol is readily biodegradable in both aerobic (oxygen present) and anaerobic (oxygen absent) environments. Methanol will not persist in the environment. The half-life for methanol in groundwater is just one to seven days, while many common gasoline components have half-lives in the hundreds of days (such as benzene at 10–730 days). Since methanol is miscible with water and biodegradable, it is unlikely to accumulate in groundwater, surface water, air or soil.
Production.
From synthesis gas.
Carbon monoxide and hydrogen react over a catalyst to produce methanol. Today, the most widely used catalyst is a mixture of copper, zinc oxide, and alumina first used by ICI in 1966. At 5–10 MPa (50–100 atm) and 250 C, it can catalyze the production of methanol from carbon monoxide and hydrogen with high selectivity (>99.8%):
It is worth noting that the production of synthesis gas from methane produces three moles of hydrogen gas for every mole of carbon monoxide, while the methanol synthesis consumes only two moles of hydrogen gas per mole of carbon monoxide. One way of dealing with the excess hydrogen is to inject carbon dioxide into the methanol synthesis reactor, where it, too, reacts to form methanol according to the equation:
Some chemists believe that the certain catalysts synthesize methanol using CO2 as an intermediary, and consuming CO only indirectly.
where the H2O byproduct is recycled via the water-gas shift reaction
This gives an overall reaction, which is the same as listed above.
From methane.
The direct catalytic conversion of methane to methanol using Cu-zeolites or other catalysts is an alternative process for the efficient production of methanol. This is currently an active field of research, however there are challenges including lifetime of the catalysts and use of affordable reactants to oxidise the methane.
From carbon dioxide.
Methanol has been generated directly from carbon dioxide in solution using copper oxide (CuO) nanorods coated by cuprous oxide (Cu2O) and energy from (simulated) sunlight. The process operated with 95% electrochemical efficiency and is claimed to be scalable to industrial size.
Feedstocks.
Production of synthesis gas.
Originally, synthesis gas for the production of methanol came from coal. Today, synthesis gas is most commonly produced from the methane component in natural gas, because natural gas contains hydrogen. Three processes are commercially practiced. At moderate pressures of 4 MPa (40 atm) and high temperatures (around 850 °C), methane reacts with steam on a nickel catalyst to produce syngas according to the chemical equation:
This reaction, commonly called steam-methane reforming or SMR, is endothermic, and the heat transfer limitations place limits on the size of and pressure in the catalytic reactors used. Methane can also undergo partial oxidation with molecular oxygen (at atmospheric pressure) to produce syngas, as the following equation shows:
This reaction is exothermic, and the heat given off can be used "in-situ" to drive the steam-methane reforming reaction. When the two processes are combined, it is referred to as "autothermal reforming".
The high pressures and high temperatures needed for steam-reforming require a greater capital investment in equipment than is needed for a simple partial-oxidation process; however, the energy-efficiency of steam-reforming is higher than for partial-oxidation, unless the waste-heat from partial-oxidation is used.
Stoichiometry adjustment.
Stoichiometry for methanol production requires the ratio of H2 / CO to equal 2. The "partial oxidation" process yields a ratio of 2, and the "steam reforming" process yields a ratio of 3.
The H2 / CO ratio can be lowered to some extent by the reverse water-gas shift reaction,
to provide the appropriate stoichiometry for methanol synthesis.
Alternate feedstock materials.
Although natural gas is the most economical and widely used feedstock for methanol production, many other feedstocks can be used to produce syngas via steam reforming. Steam-reformed coal is sometimes used as a feedstock for methanol production, particularly in China. In addition, mature technologies available for biomass gasification are being used for methanol production.
For instance, woody biomass can be gasified to water gas (a hydrogen-rich syngas), by introducing a blast of steam in a blast furnace. The water-gas / syngas can then be synthesized to methanol using standard methods. The net process is carbon neutral, since the CO2 byproduct is required to produce biomass via photosynthesis. Using a composition for wood of 50% carbon, 42% oxygen, 6% hydrogen we can represent wood with the formula C11H16O7 (we could also use C8H12O5). Then some combination of the following two formal reactions will occur:
Quality specifications and analysis.
Methanol for laboratory use.
Methanol is available commercially in various purity grades for fine chemicals:
1) “Synthesis” quality (corresponding to normal
commercial methanol)
2) Certified analytical quality
3) Extremely pure qualities for semiconductor manufacture
Commercial methanol.
In addition to laboratory grades, commercial methanol is generally classified according to ASTM purity grades A and AA. Methanol for chemical use normally corresponds to Grade AA. In addition to water, typical impurities include acetone (which is very difficult to separate by distillation) and ethanol. When methanol is delivered by ships or tankers used to transport
other substances, contamination by the previous cargo must be expected. Comparative ultraviolet spectroscopy has proved a convenient, quick test method for deciding whether a batch can be accepted and loaded.Traces of all chemicals derived from aromatic parent substances, as well as a large number of other compounds, can be detected. Further tests for establishing the quality of methanol include measurements of boiling point range, density, permanganate number, turbidity, color index, and acid number. More comprehensive tests include water determination according to the Karl Fischer method and gas chromatographic determination of byproducts. However, the latter is relatively expensive and time consuming because several injections using different columns and detectors must be made due to the variety of byproducts present.
History.
In their embalming process, the ancient Egyptians used a mixture of substances, including methanol, which they obtained from the pyrolysis of wood. Pure methanol, however, was first isolated in 1661 by Robert Boyle, when he produced it via the distillation of buxus (boxwood). It later became known as "pyroxylic spirit". In 1834, the French chemists Jean-Baptiste Dumas and Eugene Peligot determined its elemental composition.
They also introduced the word "methylene" to organic chemistry, forming it from Greek "methy" = "wine" + "hȳlē" = wood (patch of trees), with Greek language errors: "wood (substance)" (Greek "ξύλον", "xylon") was intended, and the components are in the wrong order for Greek. The term "methyl" was derived in about 1840 by back-formation from "methylene", and was then applied to describe "methyl alcohol". This was shortened to "methanol" in 1892 by the International Conference on Chemical Nomenclature. The suffix -yl used in organic chemistry to form names of carbon groups, was extracted from the word "methyl".
In 1923, the German chemists Alwin Mittasch and Mathias Pier, working for BASF, developed a means to convert synthesis gas (a mixture of carbon monoxide, carbon dioxide, and hydrogen) into methanol. A patent was filed 12 January 1926 (reference no. 1,569,775). This process used a chromium and manganese oxide catalyst, and required extremely vigorous conditions—pressures ranging from 50 to 220 atm, and temperatures up to 450 °C. Modern methanol production has been made more efficient through use of catalysts (commonly copper) capable of operating at lower pressures. The modern low pressure methanol (LPM) was developed by ICI in the late 1960s with the technology now owned by Johnson Matthey, which is a leading licensor of methanol technology.
Methanol is one of the most heavily traded chemical commodities in the world, with an estimated global demand of around 27 to 29 million metric tons. In recent years, production capacity has expanded considerably, with new plants coming on-stream in South America, China and the Middle East, the latter based on access to abundant supplies of methane gas. Even though nameplate production capacity (coal-based) in China has grown significantly, operating rates are estimated to be as low as 50 to 60%. No new production capacity is scheduled to come on-stream until 2015.
The main applications for methanol are the production of formaldehyde (used in construction and wooden boarding), acetic acid (basis for a.o. PET-bottles), MTBE (fuel component and replacement for the very volatile diethyl ether) and more recently for the formation of methyl esters in the production of bio-diesel. In China, demand is expected to grow exponentially, not only caused by a growing internal market of the traditional applications, but accelerated by new applications, such as direct blending (with gasoline), Methanol-To-Olefins (e.g. propylene) and DME. Methanol can also be used to produce gasoline.
The use of methanol as a motor fuel received attention during the oil crises of the 1970s due to its availability, low cost, and environmental benefits. By the mid-1990s, over 20,000 methanol "flexible fuel vehicles" capable of operating on methanol or gasoline were introduced in the U.S. In addition, low levels of methanol were blended in gasoline fuels sold in Europe during much of the 1980s and early-1990s. Automakers stopped building methanol FFVs by the late-1990s, switching their attention to ethanol-fueled vehicles. While the methanol FFV program was a technical success, rising methanol pricing in the mid- to late-1990s during a period of slumping gasoline pump prices diminished the interest in methanol fuels.
In 2006, astronomers using the MERLIN array of radio telescopes at Jodrell Bank Observatory discovered a large cloud of methanol in space, 288 billion miles across.

</doc>
<doc id="19714" url="http://en.wikipedia.org/wiki?curid=19714" title="Milk">
Milk

Milk is a white liquid produced by the mammary glands of mammals. It is the primary source of nutrition for young mammals before they are able to digest other types of food. Early-lactation milk contains colostrum, which carries the mother's antibodies to its young and can reduce the risk of many diseases. Milk contains many other nutrients and the carbohydrate lactose. 
As an agricultural product, milk is extracted from mammals during or soon after pregnancy and is used as food for humans. Worldwide, dairy farms produced about 730 million tonnes of milk in 2011, from 260 million dairy cows. India is the world's largest producer and consumer of milk, yet neither exports nor imports it. New Zealand, the European Union's 28 member states, Australia, and the United States are the world's largest exporters of milk and milk products. China and Russia are the world's largest importers of milk and milk products.
Throughout the world, there are more than six billion consumers of milk and milk products. Over 750 million people live within dairy farming households.
Types of consumption.
There are two distinct types of milk consumption: a natural source of nutrition for all infant mammals and a food product for humans of all ages that is derived from other animals.
Nutrition for infant mammals.
In almost all mammals, milk is fed to infants through breastfeeding, either directly or by expressing the milk to be stored and consumed later. The early milk from mammals is called colostrum. Colostrum contains antibodies that provide protection to the newborn baby as well as nutrients and growth factors. The makeup of the colostrum and the period of secretion varies from species to species.
For humans, the World Health Organization recommends exclusive breastfeeding for six months and breastfeeding in addition to other food for at least two years. In some cultures it is common to breastfeed children for three to five years, and the period may be longer.
Fresh goats' milk is sometimes substituted for breast milk. This introduces the risk of the child developing electrolyte imbalances, metabolic acidosis, megaloblastic anemia, and a host of allergic reactions.
Food product for humans.
In many cultures of the world, especially the West, humans continue to consume milk beyond infancy, using the milk of other animals (especially cattle, goats and sheep) as a food product. Initially, the ability to digest milk was limited to children as adults did not produce lactase, an enzyme necessary for digesting the lactose in milk. Milk was therefore converted to curd, cheese and other products to reduce the levels of lactose. Thousands of years ago, a chance mutation spread in human populations in Europe that enabled the production of lactase in adulthood. This allowed milk to be used as a new source of nutrition which could sustain populations when other food sources failed. Milk is processed into a variety of dairy products such as cream, butter, yogurt, kefir, ice cream, and cheese. Modern industrial processes use milk to produce casein, whey protein, lactose, condensed milk, powdered milk, and many other food-additives and industrial products.
Whole milk, butter and cream have high levels of saturated fat. The sugar, lactose, is found only in milk, forsythia flowers, and a few tropical shrubs. The enzyme needed to digest lactose, lactase, reaches its highest levels in the small intestine after birth and then begins a slow decline unless milk is consumed regularly. Those groups who do continue to tolerate milk, however, often have exercised great creativity in using the milk of domesticated ungulates, not only of cattle, but also sheep, goats, yaks, water buffalo, horses, reindeer and camels. The largest producer and consumer of cattle and buffalo milk in the world is India.
Terminology.
The term "milk" is also used for white colored, non-animal beverages resembling milk in color and texture (milk substitutes) such as soy milk, rice milk, almond milk, and coconut milk. In addition, a substance secreted by pigeons to feed their young is called "crop milk" and bears some resemblance to mammalian milk, although it is not consumed as a milk substitute. Dairy relates to milk and milk production, e.g. dairy products. Milk can be synthesized in a laboratory, from water, fatty acids and proteins.
Evolution of lactation.
The mammary gland is thought to have derived from apocrine skin glands. It has been suggested that the original function of lactation (milk production) was keeping eggs moist. Much of the argument is based on monotremes (egg-laying mammals). The original adaptive significance of milk secretions may have been nutrition or immunological protection. This secretion gradually became more copious and accrued nutritional complexity over evolutionary time.
History.
Humans first learned to regularly consume the milk of other mammals following the domestication of animals during the Neolithic Revolution or the development of agriculture. This development occurred independently in several places around the world from as early as 9000–7000 BC in Southwest Asia to 3500–3000 BC in the Americas. The most important dairy animals—cattle, sheep and goats—were first domesticated in Southwest Asia, although domestic cattle had been independently derived from wild aurochs populations several times since. Initially animals were kept for meat, and archaeologist Andrew Sherratt has suggested that dairying, along with the exploitation of domestic animals for hair and labor, began much later in a separate secondary products revolution in the fourth millennium BC. Sherratt's model is not supported by recent findings, based on the analysis of lipid residue in prehistoric pottery, that shows that dairying was practiced in the early phases of agriculture in Southwest Asia, by at least the seventh millennium BC.
From Southwest Asia domestic dairy animals spread to Europe (beginning around 7000 BC but not reaching Britain and Scandinavia until after 4000 BC), and South Asia (7000–5500 BC). The first farmers in central Europe and Britain milked their animals. Pastoral and pastoral nomadic economies, which rely predominantly or exclusively on domestic animals and their products rather than crop farming, were developed as European farmers moved into the Pontic-Caspian steppe in the fourth millennium BC, and subsequently spread across much of the Eurasian steppe. Sheep and goats were introduced to Africa from Southwest Asia, but African cattle may have been independently domesticated around 7000–6000 BC. Camels, domesticated in central Arabia in the fourth millennium BC, have also been used as dairy animals in North Africa and the Arabian Peninsula. The earliest Egyptian records of burn treatments describe burn dressings using milk from mothers of male babies. In the rest of the world (i.e., East and Southeast Asia, the Americas and Australia) milk and dairy products were historically not a large part of the diet, either because they remained populated by hunter-gatherers who did not keep animals or the local agricultural economies did not include domesticated dairy species. Milk consumption became common in these regions comparatively recently, as a consequence of European colonialism and political domination over much of the world in the last 500 years.
In the Middle Ages, milk was called the "virtuous white liquor" because alcoholic beverages were more reliable than water.
Industrialization.
The growth in urban population coupled with the expansion of the railway network in the mid-19th century, brought about a revolution in milk production and supply. Individual railway firms began transporting milk from rural areas to London from the 1840s and 1850s. Possibly the first such instance was in 1846, when St Thomas's Hospital in Southwark contracted with milk suppliers outside London to provide milk by rail. The Great Western Railway was an early and enthusiastic adopter, and began to transport milk into London from Maidenhead in 1860, despite much criticism. By 1900, the company was transporting over 25 million gallons annually. The milk trade grew slowly through the 1860s, but went through a period of extensive, structural change in the 1870s and 1880s.
Urban demand began to grow, as consumer purchasing power increased and milk became regarded as a required daily commodity. Over the last three decades of the 19th century, demand for milk in most parts of the country doubled, or in some cases, tripled. Legislation in 1875 made the adulteration of milk illegal - this combined with a marketing campaign to change the image of milk. The proportion of rural imports by rail as a percentage of total milk consumption in London grew from under 5% in the 1860s to over 96% by the early 20th century. By that point, the supply system for milk was the most highly organized and integrated of any food product.
The first glass bottle packaging for milk was used in the 1870s. The first company to do so may have been the New York Dairy Company in 1877. The Express Dairy Company in England began glass bottle production in 1880. In 1884, Hervey Thatcher, an American inventor from New York, invented a glass milk bottle, called 'Thatcher's Common Sense Milk Jar', which was sealed with a waxed paper disk. Later, in 1932, plastic-coated paper milk cartons were introduced commercially.
In 1863, French chemist and biologist Louis Pasteur invented pasteurization, a method of killing harmful bacteria in beverages and food products. He developed this method while on summer vacation in Arbois, to remedy the frequent acidity of the local wines. He found out experimentally that it is sufficient to heat a young wine to only about 50 - for a brief time to kill the microbes, and that the wine could be nevertheless properly aged without sacrificing the final quality. In honor of Pasteur, the process became known as "pasteurization". Pasteurization was originally used as a way of preventing wine and beer from souring. Commercial pasteurizing equipment was produced in Germany in the 1880s, and the process had been adopted in Copenhagen and Stockholm by 1885.
Sources of milk.
The females of all mammal species can by definition produce milk, but cow's milk dominates commercial production. In 2011, FAO estimates 85% of all milk worldwide was produced from cows.
Human milk is not produced or distributed industrially or commercially; however, human milk banks collect donated human breastmilk and redistribute it to infants who may benefit from human milk for various reasons (premature neonates, babies with allergies, metabolic diseases, etc.) but who cannot breastfeed.
In the Western world, cow's milk is produced on an industrial scale and is by far the most commonly consumed form of milk. Commercial dairy farming using automated milking equipment produces the vast majority of milk in developed countries. Dairy cattle such as the Holstein have been bred selectively for increased milk production. About 90% of the dairy cows in the United States and 85% in Great Britain are Holsteins. Other dairy cows in the United States include Ayrshire, Brown Swiss, Guernsey, Jersey and Milking Shorthorn (Dairy Shorthorn).
Sources aside from cows.
Aside from cattle, many kinds of livestock provide milk used by humans for dairy products. These animals include buffalo, goat, sheep, camel, donkey, horse, reindeer and yak. The first four respectively produced about 11%, 2%, 1.4% and 0.2% of all milk worldwide in 2011.
In Russia and Sweden, small moose dairies also exist.
According to the US National Bison Association, American bison (also called American buffalo) are not milked commercially; however, various sources report cows resulting from cross-breeding bison and domestic cattle are good milk producers, and have been used both during the European settlement of North America and during the development of commercial Beefalo in the 1970s and 1980s.
Production worldwide.
In 2009, the largest producer of milk and milk products was the European Union followed by India, the United States, China, Germany, Brazil, and Russia. All European Union members together produced about 138 million tonnes of milk in 2011.
Increasing affluence in developing countries, as well as increased promotion of milk and milk products, has led to a rise in milk consumption in developing countries in recent years. In turn, the opportunities presented by these growing markets have attracted investments by multinational dairy firms. Nevertheless, in many countries production remains on a small scale and presents significant opportunities for diversification of income sources by small farms. Local milk collection centers, where milk is collected and chilled prior to being transferred to urban dairies, are a good example of where farmers have been able to work on a cooperative basis, particularly in countries such as India.
Production yields.
FAO reports Israel dairy farms are the most productive in the world, with a yield of 12,546 kg milk per cow per year. This survey over 2001 and 2007 was conducted by ICAR (International Committee for Animal Recording) across 17 developed countries. The survey found that the average herd size in these developed countries increased from 74 to 99 cows per herd between 2001 to 2007. A dairy farm had an average of 19 cows per herd in Norway, and 337 in New Zealand. Annual milk production in the same period increased from 7,726 to 8,550 kg per cow in these developed countries. The lowest average production was in New Zealand at 3,974 kg per cow. The milk yield per cow depended on production systems, nutrition of the cows, and only to a minor extent different genetic potential of the animals. What the cow ate made the most impact on the production obtained. New Zealand cows with the lowest yield per year grazed all year, in contrast to Israel with the highest yield where the cows ate in barns with an energy-rich mixed diet.
The milk yield per cow in the United States, the world's largest cow milk producer, was 9,954 kg per year in 2010. In contrast, the milk yields per cow in India and China – the second and third largest producers – were respectively 1,154 kg and 2,282 kg per year.
Price.
It was reported in 2007 that with increased worldwide prosperity and the competition of bio-fuel production for feed stocks, both the demand for and the price of milk had substantially increased worldwide. Particularly notable was the rapid increase of consumption of milk in China and the rise of the price of milk in the United States above the government subsidized price. In 2010 the Department of Agriculture predicted farmers would receive an average of $1.35 per US gallon of cow's milk (35 cents per liter), which is down 30 cents per gallon from 2007 and below the break-even point for many cattle farmers.
Grading.
In the United States, there are two grades of milk, with grade A primarily used for direct sales and consumption in stores, and grade B used for indirect consumption, such as in cheese making or other processing.
The differences between the two grades are defined in the Wisconsin administrative code for Agriculture, Trade, and Consumer Protection, chapter 60. Grade B generally refers to milk that is cooled in milk cans, which are immersed in a bath of cold flowing water that typically is drawn up from an underground water well rather than using mechanical refrigeration.
Physical and chemical properties of milk.
Milk is an emulsion or colloid of butterfat globules within a water-based fluid that contains dissolved carbohydrates and protein aggregates with minerals. Because it is produced as a food source for the young, all of its contents provide benefits for its growth. The principal requirements are energy (lipids, lactose, and protein), biosynthesis of non-essential amino acids supplied by proteins (essential amino acids and amino groups), essential fatty acids, vitamins and inorganic elements, and water.
Lipids.
Initially milk fat is secreted in the form of a fat globule surrounded by a membrane. Each fat globule is composed almost entirely of triacylglycerols and is surrounded by a membrane consisting of complex lipids such as phospholipids, along with proteins. These act as emulsifiers which keep the individual globules from coalescing and protect the contents of these globules from various enzymes in the fluid portion of the milk. Although 97–98% of lipids are triacylglycrols, small amounts of di- and monoacylglycerols, free cholesterol and cholesterol esters, free fatty acids, and phospholipids are also present. Unlike protein and carbohydrates, fat composition in milk varies widely in the composition due to genetic, lactational, and nutritional factor difference between different species.
Like composition, fat globules vary in size from less than 0.2 to about 15 micrometers in diameter between different species. Diameter may also vary between animals within a species and at different times within a milking of a single animal. In unhomogenized cow's milk, the fat globules have an average diameter of two to four micrometers and with homogenization, average around 0.4 micrometers. The fat-soluble vitamins A, D, E, and K along with essential fatty acids such as linoleic and linolenic acid are found within the milk fat portion of the milk.
Proteins.
Normal bovine milk contains 30–35 grams of protein per liter of which about 80% is arranged in casein micelles.
Caseins.
The largest structures in the fluid portion of the milk are "casein micelles": aggregates of several thousand protein molecules with superficial resemblance to a surfactant micelle, bonded with the help of nanometer-scale particles of calcium phosphate. Each casein micelle is roughly spherical and about a tenth of a micrometer across. There are four different types of casein proteins: αs1-, αs2-, β-, and κ-caseins. Collectively, they make up around 76–86% of the protein in milk, by weight. Most of the casein proteins are bound into the micelles. There are several competing theories regarding the precise structure of the micelles, but they share one important feature: the outermost layer consists of strands of one type of protein, k-casein, reaching out from the body of the micelle into the surrounding fluid. These kappa-casein molecules all have a negative electrical charge and therefore repel each other, keeping the micelles separated under normal conditions and in a stable colloidal suspension in the water-based surrounding fluid.
Milk contains dozens of other types of proteins beside the caseins including enzymes. These other proteins are more water-soluble than the caseins and do not form larger structures. Because the proteins remain suspended in the whey left behind when the caseins coagulate into curds, they are collectively known as "whey proteins". Whey proteins make up approximately 20% of the protein in milk, by weight. Lactoglobulin is the most common whey protein by a large margin.
Salts, minerals, and vitamins.
Minerals or milk salts, are traditional names for a variety of cations and anions within bovine milk. Calcium, phosphate, magnesium, sodium, potassium, citrate, and chlorine are all included as minerals and they typically occur at concentration of 5–40 mM. The milk salts strongly interact with casein, most notably calcium phosphate. It is present in excess and often, much greater excess of solubility of solid calcium phosphate. In addition to calcium, milk is a good source of many other vitamins. Vitamins A, B6, B12, C, D, K, E, thiamine, niacin, biotin, riboflavin, folates, and pantothenic acid are all present in milk.
Calcium phosphate structure.
For many years the most accepted theory of the structure of a micelle was that it was composed of spherical casein aggregates, called submicelles, that were held together by calcium phosphate linkages. However, there are two recent models of the casein micelle that refute the distinct micellular structures within the micelle.
The first theory attributed to de Kruif and Holt, proposes that nanoclusters of calcium phosphate and the phosphopeptide fraction of beta-casein are the centerpiece to micellular structure. Specifically in this view, unstructured proteins organize around the calcium phosphate giving rise to their structure and thus no specific structure is formed.
The second theory proposed by Horne, the growth of calcium phosphate nanoclusters begins the process of micelle formation but is limited by binding phosphopeptide loop regions of the caseins. Once bound, protein-protein interactions are formed and polymerization occurs, in which K-casein is used as an end cap, to form micelles with trapped calcium phosphate nanoclusters.
Some sources indicate that the trapped calcium phosphate is in the form of Ca9(PO4)6;
whereas, others say it is similar to the structure of the mineral brushite CaHPO4 -2H2O.
Carbohydrates and miscellaneous contents.
Milk contains several different carbohydrate including lactose, glucose, galactose, and other oligosaccharides. The lactose gives milk its sweet taste and contributes approximately 40% of whole cow's milk's calories. Lactose is a disaccharide composite of two simple sugars, glucose and galactose. Bovine milk averages 4.8% anhydrous lactose, which amounts to about 50% of the total solids of skimmed milk. Levels of lactose are dependent upon the type of milk as other carbohydrates can be present at higher concentrations that lactose in milks.
Other components found in raw cow's milk are living white blood cells, mammary gland cells, various bacteria, and a large number of active enzymes.
Appearance.
Both the fat globules and the smaller casein micelles, which are just large enough to deflect light, contribute to the opaque white color of milk. The fat globules contain some yellow-orange carotene, enough in some breeds (such as Guernsey and Jersey cattle) to impart a golden or "creamy" hue to a glass of milk. The riboflavin in the whey portion of milk has a greenish color, which sometimes can be discerned in skimmed milk or whey products. Fat-free skimmed milk has only the casein micelles to scatter light, and they tend to scatter shorter-wavelength blue light more than they do red, giving skimmed milk a bluish tint.
Processing.
In most Western countries, centralized dairy facilities process milk and products obtained from milk (dairy products), such as cream, butter, and cheese. In the US, these dairies usually are local companies, while in the Southern Hemisphere facilities may be run by very large nationwide or trans-national corporations (such as Fonterra).
Pasteurization.
Pasteurization is used to kill harmful microorganisms by heating the milk for a short time and then immediately cooling it. The standard high temperature short time (HTST) process produces a 99.999% reduction in the number of bacteria in milk, rendering it safe to drink for up to three weeks if continually refrigerated. Dairies print expiration dates on each container, after which stores remove any unsold milk from their shelves.
A side effect of the heating of pasteurization is that some vitamin and mineral content is lost. Soluble calcium and phosphorus decrease by 5%, thiamin and vitamin B12 by 10%, and vitamin C by 20%. Because losses are small in comparison to the large amount of the two B-vitamins present, milk continues to provide significant amounts of thiamin and vitamin B12. The loss of vitamin C is not nutritionally significant, as milk is not an important dietary source of vitamin C.
A newer process, ultrapasteurization or ultra-high temperature treatment (UHT), heats the milk to a higher temperature for a shorter amount of time. This extends its shelf life and allows the milk to be stored unrefrigerated because of the longer lasting sterilization effect.
Microfiltration.
Microfiltration is a process that partially replaces pasteurization and produces milk with fewer microorganisms and longer shelf life without a change in the taste of the milk. In this process, cream is separated from the whey and is pasteurized in the usual way, but the whey is forced through ceramic microfilters that trap 99.9% of microorganisms in the milk (as compared to 99.999% killing of microorganisms in standard HTST pasteurization). The whey then is recombined with the pasteurized cream to reconstitute the original milk composition.
Creaming and homogenization.
Upon standing for 12 to 24 hours, fresh milk has a tendency to separate into a high-fat cream layer on top of a larger, low-fat milk layer. The cream often is sold as a separate product with its own uses. Today the separation of the cream from the milk usually is accomplished rapidly in centrifugal cream separators. The fat globules rise to the top of a container of milk because fat is less dense than water. The smaller the globules, the more other molecular-level forces prevent this from happening. In fact, the cream rises in cow's milk much more quickly than a simple model would predict: rather than isolated globules, the fat in the milk tends to form into clusters containing about a million globules, held together by a number of minor whey proteins. These clusters rise faster than individual globules can. The fat globules in milk from goats, sheep, and water buffalo do not form clusters as readily and are smaller to begin with, resulting in a slower separation of cream from these milks.
Milk often is homogenized, a treatment that prevents a cream layer from separating out of the milk. The milk is pumped at high pressures through very narrow tubes, breaking up the fat globules through turbulence and cavitation. A greater number of smaller particles possess more total surface area than a smaller number of larger ones, and the original fat globule membranes cannot completely cover them. Casein micelles are attracted to the newly exposed fat surfaces. Nearly one-third of the micelles in the milk end up participating in this new membrane structure. The casein weighs down the globules and interferes with the clustering that accelerated separation. The exposed fat globules are vulnerable to certain enzymes present in milk, which could break down the fats and produce rancid flavors. To prevent this, the enzymes are inactivated by pasteurizing the milk immediately before or during homogenization.
Homogenized milk tastes blander but feels creamier in the mouth than unhomogenized. It is whiter and more resistant to developing off flavors. Creamline (or cream-top) milk is unhomogenized. It may or may not have been pasteurized. Milk that has undergone high-pressure homogenization, sometimes labeled as "ultra-homogenized," has a longer shelf life than milk that has undergone ordinary homogenization at lower pressures.
Nutrition and health.
The composition of milk differs widely among species. Factors such as the type of protein; the proportion of protein, fat, and sugar; the levels of various vitamins and minerals; and the size of the butterfat globules, and the strength of the curd are among those that may vary. For example:
Donkey and horse milk have the lowest fat content, while the milk of seals and whales may contain more than 50% fat.
Cow's milk.
These compositions vary by breed, animal, and point in the lactation period.
The protein range for these four breeds is 3.3% to 3.9%, while the lactose range is 4.7% to 4.9%.
Milk fat percentages may be manipulated by dairy farmers' stock diet formulation strategies. Mastitis infection can cause fat levels to decline.
Nutritional value.
Processed cow's milk was formulated to contain differing amounts of fat during the 1950s. One cup (250 ml) of 2%-fat cow's milk contains 285 mg of calcium, which represents 22% to 29% of the daily recommended intake (DRI) of calcium for an adult. Depending on its age, milk contains 8 grams of protein, and a number of other nutrients (either naturally or through fortification) including:
The amount of calcium from milk that is absorbed by the human body is disputed. Calcium from dairy products has a greater bioavailability than calcium from certain vegetables, such as spinach, that contain high levels of calcium-chelating agents, but a similar or lesser bioavailability than calcium from low-oxalate vegetables such as kale, broccoli, or other vegetables in the "Brassica" genus.
Recommended consumption.
The U.S. federal government document "Dietary Guidelines for Americans, 2010" recommends consumption of three glasses of fat-free or low-fat milk for adults and children 9 and older (less for younger children) per day. This recommendation is disputed by some health researchers who call for more study of the issue, given that there are other sources for calcium and vitamin D. The researchers also claim that the recommendations have been unduly influenced by the American dairy industry, and that whole milk may be better for health due to its increased ability to satiate hunger.
Medical research.
There is recent evidence suggesting consumption of milk is effective at promoting muscle growth. Some studies have suggested that conjugated linoleic acid, which can be found in dairy products, is an effective supplement for reducing body fat.
Lactose intolerance.
Lactose, the disaccharide sugar component of all milk, must be cleaved in the small intestine by the enzyme, lactase, in order for its constituents, galactose and glucose, to be absorbed. The production of the enzyme lactase declines significantly after weaning in all mammals. Consequently, many humans become unable to digest lactose properly as they mature. There is a great deal of variance, with some individuals reacting badly to even small amounts of lactose, some able to consume moderate quantities, and some able to consume large quantities of milk and other dairy products without problems. The gene in humans that controls lactase production, and hence lactose tolerance and intolerance, is labeled C/T-13910. An individual who consumes milk without producing sufficient lactase may suffer diarrhea, intestinal gas, cramps and bloating, as the undigested lactose travels through the gastrointestinal tract and serves as nourishment for intestinal microflora that excretes gas in processes known as fermentation and anaerobic respiration.
It is estimated that 30 to 50 million Americans are lactose intolerant, including 75% of Native Americans and African Americans, and 90% of Asian Americans. Lactose intolerance is less common among those descended from northern Europeans. Other genetic groups that have a lower prevalence of lactose intolerance are the Tuareg of the Sahara, the Fulani of the West African Sahel, and the Beja and Kabbabish of Sudan, as well as possibly the Tutsi population of the Uganda–Rwanda area. Another locus of lactose tolerance is in Northern India.
Lactose intolerance is a natural process and there is no reliable way to prevent or reverse it. 
Having lactose intolerance doesn't mean saying goodbye to dairy forever. Some dairy foods are easier to digest than others, including yogurt and aged cheese. Lactose-intolerant people vary in how much lactose they can tolerate, but dairy and aged cheese are easier to digest because processing has already broken down some of the lactose. If you take supplemental lactase, which is an enzyme that breaks down lactose, you can generally enjoy these foods and other dairy products without experiencing the unpleasant side effects. 
Possible harms.
Some studies suggest that milk consumption may increase the risk of suffering from certain health problems. Cow's milk allergy (CMA) is an immunologically mediated adverse reaction, rarely fatal, to one or more cow's milk proteins. Milk from any mammal contains amino acids and microRNA which influence the drinker's metabolism and growth; this "programming" is beneficial for milk's natural consumers, namely infants of the same species as the milk producer, but post-infancy and trans-species milk consumption affects the mTORC1 metabolic pathway and may promote diseases of civilization such as obesity and diabetes.
Milk contains casein, a substance that breaks down in the human stomach to produce casomorphin, an opioid peptide. In the early 1990s it was hypothesized that casomorphin can cause or aggravate autism spectrum disorders, and casein-free diets are widely promoted. Studies supporting these claims have had significant flaws, and the data are inadequate to guide autism treatment recommendations.
A study demonstrated that men who drink a large amount of milk and consume dairy products were at a slightly increased risk of developing Parkinson's disease; the effect for women was smaller. The reason behind this is not fully understood, and it also remains unclear why there is less of a risk for women.
The most recent assessment by the World Cancer Research Fund and the American Institute for Cancer Research found that most individual epidemiological studies showed increased risk of prostate cancer with increased intake of milk or dairy products. "Meta-analysis of cohort data produced evidence of a clear dose-response relationship between advanced/aggressive cancer risk with milk intake, and between all prostate cancer risk and milk and dairy products." Possible mechanisms proposed included inhibition of the conversion of vitamin D to its active metabolite, 1,25- dihydroxy vitamin D3 by calcium (which some evidence suggests increases cell proliferation in the prostate), and elevation of levels of insulin-like growth factor-1 (IGF-1). Several sources suggest a correlation between high calcium intake from milk, in particular, and prostate cancer, consistent with a calcium/vitamin D based mechanism. Overall, the WCRF/AICR panel concluded that "The evidence is inconsistent from both cohort and case-control studies. There is limited evidence suggesting that milk and dairy products are a cause of prostate cancer."
Medical studies also have shown a possible link between milk consumption and the exacerbation of diseases such as Crohn's disease, Hirschsprung's disease–mimicking symptoms in babies with existing cow's milk allergies, and the aggravation of Behçet's disease.
Flavored milk in US schools.
Milk must be offered at every meal if a United States school district wishes to get reimbursement from the federal government. A quarter of the largest school districts in the US offer rice or soy milk and almost 17% of all US school districts offer lactose-free milk. Seventy-one percent of the milk served in US school cafeterias is flavored, causing some school districts to propose a ban because flavored milk has added sugars. (Though some flavored milk products use artificial sweeteners instead.) The Boulder, Colorado, school district banned flavored milk in 2009 and instead installed a dispenser that keeps the milk colder.
Bovine growth hormone supplementation.
Since November 1993, recombinant bovine somatotropin (rbST), also called rBGH, has been sold to dairy farmers with FDA approval. Cows produce bovine growth hormone naturally, but some producers administer an additional recombinant version of BGH which is produced through a genetically engineered E. coli because it increases milk production. Bovine growth hormone also stimulates liver production of insulin-like growth factor 1 (IGF1). Monsanto has stated that both of these compounds are harmless given the levels found in milk and the effects of pasteurization.
On June 9, 2006, the largest milk processor in the world and the two largest supermarkets in the United States – Dean Foods, Wal-Mart, and Kroger – announced that they are "on a nationwide search for rBGH-free milk." Milk from cows given rBST may be sold in the United States, and the FDA stated that no significant difference has been shown between milk derived from rBST-treated and that from non-rBST-treated cows. Milk that advertises that it comes from cows not treated with rBST, is required to state this finding on its label.
Cows receiving rBGH supplements may more frequently contract an udder infection known as mastitis. Problems with mastitis have led to Canada, Australia, New Zealand, and Japan banning milk from rBST treated cows. Mastitis, among other diseases, may be responsible for the fact that levels of white blood cells in milk vary naturally.
rBGH is also banned in the European Union.
Criticism.
Vegans and some other vegetarians do not consume milk for reasons mostly related to animal rights and environmental concerns. They may object to features of dairy farming including the necessity of keeping dairy cows pregnant, the killing of almost all the male offspring of dairy cows (either by disposal soon after birth, for veal production, or for beef), the routine separation of mother and calf soon after birth, other perceived inhumane treatment of dairy cattle, and culling of cows after their productive lives.
Some have criticized the American government's promotion of milk consumption. The main concern is the financial interest that the American government has taken in the dairy industry, promoting milk as the best source of calcium. All United States schools that are a part of the federally funded National School Lunch Act are required by the federal government to provide milk for all students. The Office of Dietary Supplements recommends that healthy adults between ages 19 and 50 get about 1,000 mg of calcium per day, but studies show that the human body can only retain about 550 mg of calcium per day. Milk also contains more excess calories, sugar, and fat than many other sources of calcium.
There is also some skepticism of the idea that large doses of calcium provide for healthier bones and teeth. This is a commonly held belief, but there have been some studies that show there is "no connection between the intake of calcium (in any form) and the reduced risk of bone fractures in women aged 34-71 years". Another study suggests that calcium supplements do not contribute to bone gain when a daily intake of 800 mg is surpassed.
The latest studies have found that milk consumption may not only be unhelpful, but also be detrimental. In 2011, The Journal of Bone and Mineral Research published a meta-analysis examining whether milk consumption might protect against hip fracture in middle-aged and older adults. Studies could find no association between drinking milk and lower rates of fractures. In 2014, JAMA Pediatrics published a report after monitoring almost 100,000 men and women for more than two decades. Subjects were asked to report on how much milk they had consumed as teenagers, and were followed to see if there is any association with a reduced chance of hip fractures later in life, it found there was not any.
A study published in "The BMJ" that followed more than 45,000 men and 61,000 women in Sweden age 39 and older had similar results. Milk consumption in adults was associated with no protection for men, and an increased risk of fractures in women. It was also associated with an increased risk of death in both sexes.
Varieties and brands.
Milk products are sold in a number of varieties based on types/degrees of
Milk preserved by the UHT process does not need to be refrigerated before opening and has a longer shelf life than milk in ordinary packaging. It is typically sold unrefrigerated in the UK, US, Europe, Latin America, and Australia.
Reduction or elimination of lactose.
Lactose-free milk can be produced by passing milk over lactase enzyme bound to an inert carrier. Once the molecule is cleaved, there are no lactose ill effects. Forms are available with reduced amounts of lactose (typically 30% of normal), and alternatively with nearly 0%. The only noticeable difference from regular milk is a slightly sweeter taste due to the generation of glucose by lactose cleavage. It does not, however, contain more glucose, and is nutritionally identical to regular milk.
Finland, where approximately 17% of the Finnish-speaking population has hypolactasia, has had "HYLA" (acronym for "hydrolysed lactose") products available for many years. Lactose of low-lactose level cow's milk products, ranging from ice cream to cheese, is enzymatically hydrolysed into glucose and galactose. The ultra-pasteurization process, combined with aseptic packaging, ensures a long shelf life. In 2001, Valio launched a lactose-free milk drink that is not sweet like HYLA milk but has the fresh taste of ordinary milk. Valio patented the chromatographic separation method to remove lactose. Valio also markets these products in Sweden, Estonia, Belgium, and the United States, where the company says ultrafiltration is used.
In the UK, where an estimated 15% of the population are affected by lactose intolerance, Lactofree produces milk, cheese, and yogurt products that contain only 0.03% lactose.
To aid digestion in those with lactose intolerance, milk with added bacterial cultures such as "Lactobacillus acidophilus" ("acidophilus milk") and bifidobacteria ("a/B milk") is available in some areas. Another milk with "Lactococcus lactis" bacteria cultures ("cultured buttermilk") often is used in cooking to replace the traditional use of naturally soured milk, which has become rare due to the ubiquity of pasteurization, which also kills the naturally occurring Lactococcus bacteria.
Additives and flavoring.
In areas where the cattle (and often the people) live indoors, commercially sold milk commonly has vitamin D added to it to make up for lack of exposure to UVB radiation.
Reduced-fat milks often have added vitamin A palmitate to compensate for the loss of the vitamin during fat removal; in the United States this results in reduced fat milks having a higher vitamin A content than whole milk.
Milk often has flavoring added to it for better taste or as a means of improving sales. Chocolate milk has been sold for many years and has been followed more recently by strawberry milk and others. Some nutritionists have criticized flavored milk for adding sugar, usually in the form of high-fructose corn syrup, to the diets of children who are already commonly obese in the US.
Distribution.
Due to the short shelf life of normal milk, it used to be delivered to households daily in many countries; however, improved refrigeration at home, changing food shopping patterns because of supermarkets, and the higher cost of home delivery mean that daily deliveries by a milkman are no longer available in most countries.
Australia and New Zealand.
In Australia and New Zealand, prior to "metrification", milk was generally distributed in 1 pint (568ml) glass bottles. In Australia and in Ireland there was a government funded "free milk for school children" program, and milk was distributed at morning recess in 1/3 pint bottles. With the conversion to metric measures, the milk industry were concerned that the replacement of the pint bottles with 500ml bottles would result in a 13.6% drop in milk consumption; hence, all pint bottles were recalled and replaced by 600 mL bottles. With time, due to the steadily increasing cost of collecting, transporting, storing and cleaning glass bottles, they were replaced by cardboard cartons. A number of designs were used, including a tetrahedron which could be close-packed without waste space, and could not be knocked over accidentally. (slogan: No more crying over spilt milk.) However, the industry eventually settled on a design similar to that used in the United States.
Milk is now available in a variety of sizes in cardboard cartons (250 mL, 375 mL, 600 mL, 1 liter and 1.5 liters) and plastic bottles (1, 2 and 3 liters). A significant addition to the marketplace has been "long-life" milk (UHT), generally available in 1 and 2 liter rectangular cardboard cartons. In urban and suburban areas where there is sufficient demand, home delivery is still available, though in suburban areas this is often 3 times per week rather than daily. Another significant and popular addition to the marketplace has been flavored milks – for example, as mentioned above, Farmers Union Iced Coffee outsells Coca-Cola in South Australia.
India.
In rural India, milk is home delivered, daily, by local milkmen carrying bulk quantities in a metal container, usually on a bicycle. In other parts of metropolitan India, milk is usually bought or delivered in plastic bags or cartons via shops or supermarkets.
Pakistan.
In Pakistan, milk is supplied in jugs. Milk has been a staple food, especially among the pastoral tribes in this country.
United Kingdom.
Since the late 1990s, milk-buying patterns have changed drastically in the UK. The classic milkman, who travels his local milk round (route) using a milk float (often battery powered) during the early hours and delivers milk in 1 pint glass bottles with aluminium foil tops directly to households, has almost disappeared. Two of the main reasons for the decline of UK home deliveries by milkmen are household refrigerators (which lessen the need for daily milk deliveries) and private car usage (which has increased supermarket shopping). Another factor is that it is cheaper to purchase milk from a supermarket than from home delivery. In 1996, more than 2.5 billion liters of milk were still being delivered by milkmen, but by 2006 only 637 million liters (13% of milk consumed) was delivered by some 9,500 milkmen. By 2010, the estimated number of milkmen had dropped to 6,000. Assuming that delivery per milkman is the same as it was in 2006, this means milkmen deliveries now only account for 6–7% of all milk consumed by UK households (6.7 billion liters in 2008/2009).
Almost 95% of all milk in the UK is thus sold in shops today, most of it in plastic bottles of various sizes, but some also in milk cartons. Milk is hardly ever sold in glass bottles in UK shops.
United States.
In the United States, glass milk bottles have been replaced mostly with milk cartons and plastic jugs. Gallons of milk are almost always sold in jugs, while half gallons and quarts may be found in both paper cartons and plastic jugs, and smaller sizes are almost always in cartons.
The "half pint" .5 USpt milk carton is the traditional unit as a component of school lunches, though some companies have replaced that unit size with a plastic bottle, which is also available at retail in 6- and 12-pack size.
Packaging.
Glass milk bottles are now rare. Most people purchase milk in bags, plastic bottles, or plastic-coated paper cartons. Ultraviolet (UV) light from fluorescent lighting can alter the flavor of milk, so many companies that once distributed milk in transparent or highly translucent containers are now using thicker materials that block the UV light.
Milk comes in a variety of containers with local variants:
Practically everywhere, condensed milk and evaporated milk are distributed in metal cans, 250 and 125 mL paper containers and 100 and 200 mL squeeze tubes, and powdered milk (skim and whole) is distributed in boxes or bags.
Spoilage and fermented milk products.
When raw milk is left standing for a while, it turns "sour". This is the result of fermentation, where lactic acid bacteria ferment the lactose in the milk into lactic acid. Prolonged fermentation may render the milk unpleasant to consume. This fermentation process is exploited by the introduction of bacterial cultures (e.g. "Lactobacilli sp., Streptococcus sp., Leuconostoc sp.", etc.) to produce a variety of fermented milk products. The reduced pH from lactic acid accumulation denatures proteins and causes the milk to undergo a variety of different transformations in appearance and texture, ranging from an aggregate to smooth consistency. Some of these products include sour cream, yogurt, cheese, buttermilk, viili, kefir, and kumis. "See Dairy product" for more information.
Pasteurization of cow's milk initially destroys any potential pathogens and increases the shelf life, but eventually results in spoilage that makes it unsuitable for consumption. This causes it to assume an unpleasant odor, and the milk is deemed non-consumable due to unpleasant taste and an increased risk of food poisoning. In raw milk, the presence of lactic acid-producing bacteria, under suitable conditions, ferments the lactose present to lactic acid. The increasing acidity in turn prevents the growth of other organisms, or slows their growth significantly. During pasteurization, however, these lactic acid bacteria are mostly destroyed.
In order to prevent spoilage, milk can be kept refrigerated and stored between 1 and 4 degrees Celsius in bulk tanks. Most milk is pasteurized by heating briefly and then refrigerated to allow transport from factory farms to local markets. The spoilage of milk can be forestalled by using ultra-high temperature (UHT) treatment. Milk so treated can be stored unrefrigerated for several months until opened but has a characteristic "cooked" taste. Condensed milk, made by removing most of the water, can be stored in cans for many years, unrefrigerated, as can evaporated milk. The most durable form of milk is powdered milk, which is produced from milk by removing almost all water. The moisture content is usually less than 5% in both drum- and spray-dried powdered milk.
Language and culture.
The importance of milk in human culture is attested to by the numerous expressions embedded in our languages, for example, "the milk of human kindness". In ancient Greek mythology, the goddess Hera spilled her breast milk after refusing to feed Heracles, resulting in the Milky Way.
In many African and Asian countries, butter is traditionally made from fermented milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.
Holy books have also mentioned milk. The Bible contains references to the 'Land of Milk and Honey'. In the Qur'an, there is a request to wonder on milk as follows: 'And surely in the livestock there is a lesson for you, We give you to drink of that which is in their bellies from the midst of digested food and blood, pure milk palatable for the drinkers.'(16-The Honeybee, 66). The Ramadan fast is traditionally broken with a glass of milk and dates.
Abhisheka is conducted by Hindu and Jain priests, by pouring libations on the image of a deity being worshipped, amidst the chanting of mantras. Usually offerings such as milk, yogurt, ghee, honey may be poured among other offerings depending on the type of abhishekam being performed.
To milk someone, in the vernacular of many English-speaking countries, is to take advantage of the person.
The word "milk" has had many slang meanings over time. In the 19th century, milk was used to describe a cheap alcoholic drink made from methylated spirits mixed with water. The word was also used to mean defraud, to be idle, to intercept telegrams addressed to someone else, and a weakling or 'milksop'. In the mid-1930s, the word was used in Australia meaning to siphon gas from a car.
Other uses.
Besides serving as a beverage or source of food, milk has been described as used by farmers and gardeners as an organic fungicide and foliage fertilizer. Diluted milk solutions have been demonstrated to provide an effective method of preventing powdery mildew on grape vines, while showing it is unlikely to harm the plant.

</doc>
<doc id="19715" url="http://en.wikipedia.org/wiki?curid=19715" title="Miss Congeniality (film)">
Miss Congeniality (film)

 
Miss Congeniality is a 2000 comedy film directed by Donald Petrie, written by Marc Lawrence, Katie Ford and Caryn Lucas, starring Sandra Bullock, Michael Caine, Benjamin Bratt and Candice Bergen.
Plot.
The film opens with a flashback to Gracie Hart's childhood. Gracie (Mary Ashleigh Green) is in a school playground where one boy is bullying another. She steps in and beats up the bully to help the victim (a boy she likes). But instead of being grateful, he feels humiliated at being rescued "by a girl," and gets angry at her. She promptly punches him in the nose and then sulks alone in the playground.
Fast-forward to the present day, and Gracie Hart (Sandra Bullock) is an FBI Special Agent. In the middle of a sting operation against Russian mobsters, the target starts to choke on some food, and Gracie disobeys her superior's command in order to stop the target from choking. As a result, a member of her squad gets shot, and she is reprimanded by being reassigned to a desk job.
Soon afterwards, the bureau learns that there has been a bomb threat against the 75th annual Miss United States beauty pageant in San Antonio, Texas, which the bureau determines came from the notorious domestic terrorist known as "The Citizen". Gracie's partner, Special Agent Eric Matthews (Benjamin Bratt), is chosen to lead the mission to stop the attack. Despite Eric's position as the lead agent in charge, it is apparent that Gracie is the more capable agent, as Eric is unsure of how to proceed and takes credit for Gracie's suggestions. One of Gracie's suggestions is to put an agent undercover at the pageant. Gracie's male colleagues then proceed to have a bit of fun running the digital images of female (and male) agents through a website meant for children to play dress up with their dolls. Despite the immature and chauvinistic actions of the agents, Eric recognizes that Gracie is the female FBI agent best qualified for the undercover job.
Beauty pageant coach Victor Melling (Michael Caine), whose reputation was ruined after his last contestant criticized his methods, teaches the tomboyish Gracie how to dress, walk, and act like a contestant. She is not used to such behavior, however, and sees the pageant and its participants as "outdated and antifeminist". Eventually though Victor and Gracie come to respect each other's strengths. Representing New Jersey as "Gracie Lou Freebush", Gracie impresses the audience by playing the glass harp and demonstrating self-defense techniques during the talent competition. She also unexpectedly becomes friends with the other contestants, in particular Miss Rhode Island, Cheryl Frasier (Heather Burns)
Several suspects emerge, including the corrupt competition director and former pageant winner Kathy Morningside (Candice Bergen); her unpleasant assistant Frank Tobin (Steve Monroe); the veteran pageant MC Stan Fields (William Shatner) who, like Kathy, is being replaced with a younger person; and last but not least Cheryl, who appears to have a history as a radical animal rights activist. While Gracie takes Cheryl and some of the other girls out partying, in an attempt to seek out more information about Cheryl's past through "girl talk", The Citizen is arrested elsewhere on an unrelated charge. After hearing what some of the other contestants said about Kathy Morningside's past (she won the pageant only after the original winner "mysteriously" contracted food poisoning), Gracie begins to suspect her, and is worried about the safety of the girls. Her boss thinks that her suspicions are groundless and that the pageant is out of danger now that the Citizen has been arrested. He calls off the mission. Gracie opts to turn in her badge and gun and continue the investigation alone. Eric initially fails to support Gracie and is about to return to the bureau when he figures out that Gracie's suspicions must be correct, and returns to help her.
In the final round of the pageant, Gracie earns second place while Cheryl becomes Miss United States. Gracie realizes that Kathy Morningside and Frank Tobin — who unbeknownst to anyone (except Victor) is Kathy's son — impersonated the Citizen and made the threat. Because she was going to be fired after the pageant was over, Kathy and her son were planning to murder the newly crowned pageant winner on stage by putting an explosive device in her tiara. However, Gracie and Eric save the day and arrest Kathy and her son. Gracie's fellow contestants choose her as "Miss Congeniality," and, as the story ends, a romance blooms between Gracie and Eric.
Production.
The story is set in New York City and San Antonio. Scenes showing the exterior of the St. Regis Hotel, and a few street scenes, were shot on location in New York, and the Alamo and River Walk scenes were shot on location in San Antonio. However, most of the movie was actually shot in Austin, Texas: scenes depicting the interior of the St. Regis were shot in Austin's Driskill Hotel; the pageant scenes were shot at the Bass Concert Hall at the University of Texas at Austin; and scenes depicting the pageant contestants in their hotel rooms were shot in the Omni Austin at South Park.
Box office and reception.
The film was the fifth highest grossing film in North America on its opening weekend, making $13,853,686 USD; it had a 5% increase in earnings the following week—enough to make the film reach #3; and overall it was a box office hit, grossing more than $106 million in the United States, and more than $212.7 million worldwide. It was nominated for several awards, including two Golden Globes: Sandra Bullock earned a nod for Best Performance by an Actress in a Motion Picture - Comedy/Musical, and Bosson's "One in a Million" was nominated for Best Original Song in a Motion Picture. The film has a 42% rating at Rotten Tomatoes and a metascore of 43 at Metacritic, signifying mixed or average reviews.
Home media.
The film's first DVD edition, released in 2001, included two audio commentaries, some deleted scenes, the theatrical trailer, and two documentaries about the making of the film. A deluxe-edition DVD, released in In 2005, featured different cover art for the DVD and menu. It contained the same features as the other DVD version plus a quiz hosted by William Shatner and a sneak peek at the upcoming sequel. In 2009, a double DVD edition was released that included the sequel, ".
Sequel.
A sequel, ", was released on March 24, 2005. The film starred Sandra Bullock, Regina King, Enrique Murciano, William Shatner, Ernie Hudson, Heather Burns, Diedrich Bader, and Treat Williams. The sequel was less successful, critically and commercially, earning only $101,393,569.

</doc>
<doc id="19716" url="http://en.wikipedia.org/wiki?curid=19716" title="Magnetism">
Magnetism

Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field, which acts on other currents and magnetic moments. Every material is influenced to some extent by a magnetic field. The most familiar effect is on permanent magnets, which have persistent magnetic moments caused by ferromagnetism. Most materials do not have permanent moments. Some are attracted to a magnetic field (paramagnetism); others are repulsed by a magnetic field (diamagnetism); others have a more complex relationship with an applied magnetic field (spin glass behavior and antiferromagnetism). Substances that are negligibly affected by magnetic fields are known as "non-magnetic" substances. These include copper, aluminium, gases, and plastic. Pure oxygen exhibits magnetic properties when cooled to a liquid state.
The magnetic state (or phase) of a material depends on temperature and other variables such as pressure and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change.
History.
Aristotle attributed the first of what could be called a scientific discussion on magnetism to Thales of Miletus, who lived from about 625 BC to about 545 BC. Around the same time, in ancient India, the Indian surgeon, Sushruta, was the first to make use of the magnet for surgical purposes.
In ancient China, the earliest literary reference to magnetism lies in a 4th-century BC book named after its author, "The Master of Demon Valley" (鬼谷子): "The lodestone makes iron come or it attracts it." The earliest mention of the attraction of a needle appears in a work composed between AD 20 and 100 ("Louen-heng"): "A lodestone attracts a needle." The Chinese scientist Shen Kuo (1031–1095) was the first person to write of the magnetic needle compass and that it improved the accuracy of navigation by employing the astronomical concept of true north "(Dream Pool Essays", AD 1088), and by the 12th century the Chinese were known to use the lodestone compass for navigation. They sculpted a directional spoon from lodestone in such a way that the handle of the spoon always pointed south.
Alexander Neckam, by 1187, was the first in Europe to describe the compass and its use for navigation. In 1269, Peter Peregrinus de Maricourt wrote the "Epistola de magnete", the first extant treatise describing the properties of magnets. In 1282, the properties of magnets and the dry compass were discussed by Al-Ashraf, a Yemeni physicist, astronomer, and geographer.
In 1600, William Gilbert published his "De Magnete, Magneticisque Corporibus, et de Magno Magnete Tellure" ("On the Magnet and Magnetic Bodies, and on the Great Magnet the Earth"). In this work he describes many of his experiments with his model earth called the terrella. From his experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses pointed north (previously, some believed that it was the pole star (Polaris) or a large magnetic island on the north pole that attracted the compass).
An understanding of the relationship between electricity and magnetism began in 1819 with work by Hans Christian Ørsted, a professor at the University of Copenhagen, who discovered more or less by accident that an electric current could influence a compass needle. This landmark experiment is known as Ørsted's Experiment. Several other experiments followed, with André-Marie Ampère, who in 1820 discovered that the magnetic field circulating in a closed-path was related to the current flowing through the perimeter of the path; Carl Friedrich Gauss; Jean-Baptiste Biot and Félix Savart, both of whom in 1820 came up with the Biot–Savart law giving an equation for the magnetic field from a current-carrying wire; Michael Faraday, who in 1831 found that a time-varying magnetic flux through a loop of wire induced a voltage, and others finding further links between magnetism and electricity. James Clerk Maxwell synthesized and expanded these insights into Maxwell's equations, unifying electricity, magnetism, and optics into the field of electromagnetism. In 1905, Einstein used these laws in motivating his theory of special relativity, requiring that the laws held true in all inertial reference frames.
Electromagnetism has continued to develop into the 21st century, being incorporated into the more fundamental theories of gauge theory, quantum electrodynamics, electroweak theory, and finally the standard model.
Sources of magnetism.
Magnetism, at its root, arises from two sources:
Ordinarily, the enormous number of electrons in a material are arranged such that their magnetic moments (both orbital and intrinsic) cancel out. This is due, to some extent, to electrons combining into pairs with opposite intrinsic magnetic moments as a result of the Pauli exclusion principle (see "electron configuration"), or combining into filled subshells with zero net orbital motion. In both cases, the electron arrangement is so as to exactly cancel the magnetic moments from each electron. Moreover, even when the electron configuration "is" such that there are unpaired electrons and/or non-filled subshells, it is often the case that the various electrons in the solid will contribute magnetic moments that point in different, random directions, so that the material will not be magnetic.
However, sometimes—either spontaneously, or owing to an applied external magnetic field—each of the electron magnetic moments will be, on average, lined up. Then the material can produce a net total magnetic field, which can potentially be quite strong.
The magnetic behavior of a material depends on its structure, particularly its electron configuration, for the reasons mentioned above, and also on the temperature. At high temperatures, random thermal motion makes it more difficult for the electrons to maintain alignment.
Materials.
Diamagnetism.
Diamagnetism appears in all materials, and is the tendency of a material to oppose an applied magnetic field, and therefore, to be repelled by a magnetic field. However, in a material with paramagnetic properties (that is, with a tendency to enhance an external magnetic field), the paramagnetic behavior dominates. Thus, despite its universal occurrence, diamagnetic behavior is observed only in a purely diamagnetic material. In a diamagnetic material, there are no unpaired electrons, so the intrinsic electron magnetic moments cannot produce any bulk effect. In these cases, the magnetization arises from the electrons' orbital motions, which can be understood classically as follows:
Note that this description is meant only as an heuristic; a proper understanding requires a quantum-mechanical description.
Note that all materials undergo this orbital response. However, in paramagnetic and ferromagnetic substances, the diamagnetic effect is overwhelmed by the much stronger effects caused by the unpaired electrons.
Paramagnetism.
In a paramagnetic material there are "unpaired electrons", i.e. atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.
Ferromagnetism.
A ferromagnet, like a paramagnetic substance, has unpaired electrons. However, in "addition" to the electrons' intrinsic magnetic moment's tendency to be parallel to an "applied field", there is also in these materials a tendency for these magnetic moments to orient parallel to "each other" to maintain a lowered-energy state. Thus, even in the absence of an applied field, the magnetic moments of the electrons in the material spontaneously line up parallel to one another.
Every ferromagnetic substance has its own individual temperature, called the Curie temperature, or Curie point, above which it loses its ferromagnetic properties. This is because the thermal tendency to disorder overwhelms the energy-lowering due to ferromagnetic order.
Ferromagnetism only occurs in a few substances; the common ones are iron, nickel, cobalt, their alloys, and some alloys of rare earth metals.
Magnetic domains.
The magnetic moment of atoms in a ferromagnetic material cause them to behave something like tiny permanent magnets. They stick together and align themselves into small regions of more or less uniform alignment called magnetic domains or Weiss domains. Magnetic domains can be observed with a magnetic force microscope to reveal magnetic domain boundaries that resemble white lines in the sketch. There are many scientific experiments that can physically show magnetic fields.
When a domain contains too many molecules, it becomes unstable and divides into two domains aligned in opposite directions so that they stick together more stably as shown at the right.
When exposed to a magnetic field, the domain boundaries move so that the domains aligned with the magnetic field grow and dominate the structure (dotted yellow area) as shown at the left. When the magnetizing field is removed, the domains may not return to an unmagnetized state. This results in the ferromagnetic material's being magnetized, forming a permanent magnet.
When magnetized strongly enough that the prevailing domain overruns all others to result in only one single domain, the material is magnetically saturated. When a magnetized ferromagnetic material is heated to the Curie point temperature, the molecules are agitated to the point that the magnetic domains lose the organization and the magnetic properties they cause cease. When the material is cooled, this domain alignment structure spontaneously returns, in a manner roughly analogous to how a liquid can freeze into a crystalline solid.
Antiferromagnetism.
In an antiferromagnet, unlike a ferromagnet, there is a tendency for the intrinsic magnetic moments of neighboring valence electrons to point in "opposite" directions. When all atoms are arranged in a substance so that each neighbor is 'anti-aligned', the substance is antiferromagnetic. Antiferromagnets have a zero net magnetic moment, meaning no field is produced by them. Antiferromagnets are less common compared to the other types of behaviors, and are mostly observed at low temperatures. In varying temperatures, antiferromagnets can be seen to exhibit diamagnetic and ferrimagnetic properties.
In some materials, neighboring electrons want to point in opposite directions, but there is no geometrical arrangement in which "each" pair of neighbors is anti-aligned. This is called a spin glass, and is an example of geometrical frustration.
Ferrimagnetism.
Like ferromagnetism, ferrimagnets retain their magnetization in the absence of a field. However, like antiferromagnets, neighboring pairs of electron spins like to point in opposite directions. These two properties are not contradictory, because in the optimal geometrical arrangement, there is more magnetic moment from the sublattice of electrons that point in one direction, than from the sublattice that points in the opposite direction.
Most ferrites are ferrimagnetic. The first discovered magnetic substance, magnetite, is a ferrite and was originally believed to be a ferromagnet; Louis Néel disproved this, however, after discovering ferrimagnetism.
Superparamagnetism.
When a ferromagnet or ferrimagnet is sufficiently small, it acts like a single magnetic spin that is subject to Brownian motion. Its response to a magnetic field is qualitatively similar to the response of a paramagnet, but much larger.
Electromagnet.
An "electromagnet" is a type of magnet whose magnetism is produced by the flow of electric current. The magnetic field disappears when the current ceases.
Magnetism, electricity, and special relativity.
As a consequence of Einstein's theory of special relativity, electricity and magnetism are fundamentally interlinked. Both magnetism lacking electricity, and electricity without magnetism, are inconsistent with special relativity, due to such effects as length contraction, time dilation, and the fact that the magnetic force is velocity-dependent. However, when both electricity and magnetism are taken into account, the resulting theory (electromagnetism) is fully consistent with special relativity. In particular, a phenomenon that appears purely electric or purely magnetic to one observer may be a mix of both to another, or more generally the relative contributions of electricity and magnetism are dependent on the frame of reference. Thus, special relativity "mixes" electricity and magnetism into a single, inseparable phenomenon called electromagnetism, analogous to how relativity "mixes" space and time into spacetime.
All observations on electromagnetism apply to what might be considered to be primarily magnetism, e.g. perturbations in the magnetic field are necessarily accompanied by a nonzero electric field, and propagate at the speed of light.
Magnetic fields in a material.
In a vacuum,
where μ0 is the vacuum permeability.
In a material, 
The quantity μ0M is called "magnetic polarization".
If the field H is small, the response of the magnetization M in a diamagnet or paramagnet is approximately linear:
the constant of proportionality being called the magnetic susceptibility. If so,
In a hard magnet such as a ferromagnet, M is not proportional to the field and is generally nonzero even when H is zero (see Remanence).
Magnetic force.
The phenomenon of magnetism is "mediated" by the magnetic field. An electric current or magnetic dipole creates a magnetic field, and that field, in turn, imparts magnetic forces on other particles that are in the fields.
Maxwell's equations, which simplify to the Biot–Savart law in the case of steady currents, describe the origin and behavior of the fields that govern these forces. Therefore magnetism is seen whenever electrically charged particles are in motion—for example, from movement of electrons in an electric current, or in certain cases from the orbital motion of electrons around an atom's nucleus. They also arise from "intrinsic" magnetic dipoles arising from quantum-mechanical spin.
The same situations that create magnetic fields—charge moving in a current or in an atom, and intrinsic magnetic dipoles—are also the situations in which a magnetic field has an effect, creating a force. Following is the formula for moving charge; for the forces on an intrinsic dipole, see magnetic dipole.
When a charged particle moves through a magnetic field B, it feels a Lorentz force F given by the cross product:
where 
Because this is a cross product, the force is perpendicular to both the motion of the particle and the magnetic field. It follows that the magnetic force does no work on the particle; it may change the direction of the particle's movement, but it cannot cause it to speed up or slow down. The magnitude of the force is
where formula_8 is the angle between v and B.
One tool for determining the direction of the velocity vector of a moving charge, the magnetic field, and the force exerted is labeling the index finger "V", the middle finger "B", and the thumb "F" with your right hand. When making a gun-like configuration, with the middle finger crossing under the index finger, the fingers represent the velocity vector, magnetic field vector, and force vector, respectively. See also right hand rule.
Magnetic dipoles.
A very common source of magnetic field shown in nature is a dipole, with a "South pole" and a "North pole", terms dating back to the use of magnets as compasses, interacting with the Earth's magnetic field to indicate North and South on the globe. Since opposite ends of magnets are attracted, the north pole of a magnet is attracted to the south pole of another magnet. The Earth's North Magnetic Pole (currently in the Arctic Ocean, north of Canada) is physically a south pole, as it attracts the north pole of a compass.
A magnetic field contains energy, and physical systems move toward configurations with lower energy. When diamagnetic material is placed in a magnetic field, a "magnetic dipole" tends to align itself in opposed polarity to that field, thereby lowering the net field strength. When ferromagnetic material is placed within a magnetic field, the magnetic dipoles align to the applied field, thus expanding the domain walls of the magnetic domains.
Magnetic monopoles.
Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry "magnetic charge" analogous to electric charge. Despite systematic searches since 1931, as of 2010[ [update]], they have never been observed, and could very well not exist.
Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.
Certain grand unified theories predict the existence of monopoles which, unlike elementary particles, are solitons (localized energy packets). The initial results of using these models to estimate the number of monopoles created in the big bang contradicted cosmological observations—the monopoles would have been so plentiful and massive that they would have long since halted the expansion of the universe. However, the idea of inflation (for which this problem served as a partial motivation) was successful in solving this problem, creating models in which monopoles existed but were rare enough to be consistent with current observations.
Quantum-mechanical origin of magnetism.
In principle all kinds of magnetism originate (similar to superconductivity) from specific quantum-mechanical phenomena (e.g. Mathematical formulation of quantum mechanics, in particular the chapters on spin and on the Pauli principle).
A successful model was developed already in 1927, by Walter Heitler and Fritz London, who derived quantum-mechanically, how hydrogen molecules are formed from hydrogen atoms, i.e. from the atomic hydrogen orbitals formula_9 and formula_10 centered at the nuclei "A" and "B", see below. That this leads to magnetism is not at all obvious, but will be explained in the following.
According to the Heitler-London theory, so-called two-body molecular formula_11-orbitals are formed, namely the resulting orbital is:
Here the last product means that a first electron, r1, is in an atomic hydrogen-orbital centered at the second nucleus, whereas the second electron runs around the first nucleus. This "exchange" phenomenon is an expression for the quantum-mechanical property that particles with identical properties cannot be distinguished. It is specific not only for the formation of chemical bonds, but as we will see, also for magnetism, i.e. in this connection the term exchange interaction arises, a term which is essential for the origin of magnetism, and which is stronger, roughly by factors 100 and even by 1000, than the energies arising from the electrodynamic dipole-dipole interaction.
As for the "spin function" formula_13, which is responsible for the magnetism, we have the already mentioned Pauli's principle, namely that a symmetric orbital (i.e. with the + sign as above) must be multiplied with an antisymmetric spin function (i.e. with a − sign), and "vice versa". Thus:
I.e., not only formula_15 and formula_10 must be substituted by "α" and "β", respectively (the first entity means "spin up", the second one "spin down"), but also the sign + by the − sign, and finally ri by the discrete values "s"i (= ±½); thereby we have formula_17 and formula_18. The "singlet state", i.e. the − sign, means: the spins are "antiparallel", i.e. for the solid we have antiferromagnetism, and for two-atomic molecules one has diamagnetism. The tendency to form a (homoeopolar) chemical bond (this means: the formation of a "symmetric" molecular orbital, i.e. with the + sign) results through the Pauli principle automatically in an "antisymmetric" spin state (i.e. with the − sign). In contrast, the Coulomb repulsion of the electrons, i.e. the tendency that they try to avoid each other by this repulsion, would lead to an "antisymmetric" orbital function (i.e. with the − sign) of these two particles, and complementary to a "symmetric" spin function (i.e. with the + sign, one of the so-called "triplet functions"). Thus, now the spins would be "parallel" (ferromagnetism in a solid, paramagnetism in two-atomic gases).
The last-mentioned tendency dominates in the metals iron, cobalt and nickel, and in some rare earths, which are "ferromagnetic". Most of the other metals, where the first-mentioned tendency dominates, are "nonmagnetic" (e.g. sodium, aluminium, and magnesium) or "antiferromagnetic" (e.g. manganese). Diatomic gases are also almost exclusively diamagnetic, and not paramagnetic. However, the oxygen molecule, because of the involvement of π-orbitals, is an exception important for the life-sciences.
The Heitler-London considerations can be generalized to the Heisenberg model of magnetism (Heisenberg 1928).
The explanation of the phenomena is thus essentially based on all subtleties of quantum mechanics, whereas the electrodynamics covers mainly the phenomenology.
Living things.
Some organisms can detect magnetic fields, a phenomenon known as magnetoception. Magnetobiology studies magnetic fields as a medical treatment; fields naturally produced by an organism are known as biomagnetism.
Further reading.
</dl>

</doc>
<doc id="19719" url="http://en.wikipedia.org/wiki?curid=19719" title="Filter (mathematics)">
Filter (mathematics)

In mathematics, a filter is a special subset of a partially ordered set. For example, the power set of some set, partially ordered by set inclusion, is a filter. Filters appear in order and lattice theory, but can also be found in topology whence they originate. The dual notion of a filter is an ideal.
Filters were introduced by Henri Cartan in 1937 and subsequently used by Bourbaki in their book "Topologie Générale" as an alternative to the similar notion of a net developed in 1922 by E. H. Moore and H. L. Smith.
Motivation.
Intuitively, a filter on a partially ordered set ("poset") contains those elements that are large enough to satisfy some criterion. For example, if "x" is an element of the poset, then the set of elements that are above "x" is a filter, called the principal filter at "x". (Notice that if "x" and "y" are incomparable elements of the poset, then neither of the principal filters at "x" and "y" is contained in the other one.)
Similarly, a filter on a set contains those subsets that are sufficiently large to contain "something". For example, if the set is the real line and "x" is one of its points, then the family of sets that contain "x" in their interior is a filter, called the filter of neighbourhoods of "x". (Notice that the "thing" in this case is slightly larger than "x", but it still doesn't contain any other specific point of the line.) 
The mathematical notion of filter provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, general topology and logic.
General definition.
A subset "F" of a partially ordered set ("P",≤) is a filter if the following conditions hold:
A filter is proper if it is not equal to the whole set "P". This condition is sometimes added to the definition of a filter.
While the above definition is the most general way to define a filter for arbitrary posets, it was originally defined for lattices only. In this case, the above definition can be characterized by the following equivalent statement:
A subset "F" of a lattice ("P",≤) is a filter, if and only if it is an upper set that is closed under finite intersection (infima or meet), i.e., for all "x", "y" in "F", we find that "x" ∧ "y" is also in "F".
The smallest filter that contains a given element "p" is a principal filter and "p" is a principal element in this situation. The principal filter for "p" is just given by the set {"x" in "P" | "p" ≤ "x"} and is denoted by prefixing "p" with an upward arrow: formula_1.
The dual notion of a filter, i.e. the concept obtained by reversing all ≤ and exchanging ∧ with ∨, is ideal. Because of this duality, the discussion of filters usually boils down to the discussion of ideals. Hence, most additional information on this topic (including the definition of maximal filters and prime filters) is to be found in the article on ideals. There is a separate article on ultrafilters.
Filter on a set.
A special case of a filter is a filter defined on a set. Given a set "S", a partial ordering ⊆ can be defined on the powerset P("S") by subset inclusion, turning (P("S"),⊆) into a lattice. Define a filter "F" on "S" as a nonempty subset of P("S") with the following properties:
The first two properties imply that a filter on a set has the finite intersection property. Note that with this definition, a filter on a set is indeed a filter; in fact, it is a proper filter. Because of this, sometimes this is called a proper filter on a set; however, the adjective "proper" is generally omitted and considered implicit. The only nonproper filter on "S" is P("S").
A filter base (or filter basis) is a subset "B" of P("S") with the following properties:
Given a filter base "B", the filter generated or spanned by "B" is defined as the minimum filter containing "B". It is the family of all the subsets of "S" which contain some set of "B". Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.
If "B" and "C" are two filter bases on "S", one says "C" is finer than "B" (or that "C" is a refinement of "B") if for each "B"0 ∈ "B", there is a "C"0 ∈ "C" such that "C"0 ⊆ "B"0. If also "B" is finer than "C", one says that they are equivalent filter bases.
For any subset "T" of P("S") there is a smallest (possibly nonproper) filter "F" containing "T", called the filter generated or spanned by "T". It is constructed by taking all finite intersections of "T", which then form a filter base for "F". This filter is proper if and only if any finite intersection of elements of "T" is non-empty, and in that case we say that "T" is a filter subbase.
Filters in model theory.
For any filter "F" on a set "S", the set function defined by
is finitely additive — a "measure" if that term is construed rather loosely. Therefore the statement
can be considered somewhat analogous to the statement that φ holds "almost everywhere". That interpretation of membership in a filter is used (for motivation, although it is not needed for actual "proofs") in the theory of ultraproducts in model theory, a branch of mathematical logic.
Filters in topology.
In topology and analysis, filters are used to define convergence in a manner similar to the role of sequences in a metric space.
In topology and related areas of mathematics, a filter is a generalization of a net. Both nets and filters provide very general contexts to unify the various notions of limit to arbitrary topological spaces. 
A sequence is usually indexed by the natural numbers, which are a totally ordered set. Thus, limits in first-countable spaces can be described by sequences. However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a directed set. Filters can be thought of as sets built from multiple nets. Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.
Neighbourhood bases.
Let "X" be a topological space and "x" a point of "X".
Convergent filter bases.
Let "X" be a topological space and "x" a point of "X".
Indeed:
(i) implies (ii): if "F" is a filter base satisfying the properties of (i), then the filter associated to "F" satisfies the properties of (ii).
(ii) implies (iii): if "U" is any open neighborhood of "x" then by the definition of convergence "U" contains an element of "F"; since also "Y" is an element of "F", 
"U" and "Y" have nonempty intersection. 
(iii) implies (i): Define formula_9. Then "F" is a filter base satisfying the properties of (i).
Clustering.
Let "X" be a topological space and "x" a point of "X".
Properties of a topological space.
Let "X" be a topological space. 
Functions on topological spaces.
Let formula_11, formula_12 be topological spaces. Let formula_13 be a filter base on formula_11 and formula_15 be a function. The image of formula_13 under formula_17 is formula_18 is the set formula_19. The image formula_18 forms a filter base on formula_12. (Do not confuse the x element of B and x point in X!)
Cauchy filters.
Let formula_26 be a metric space.
More generally, given a uniform space "X", a filter "F" on "X" is called Cauchy filter if for every entourage "U" there is an "A" ∈ "F" with ("x,y") ∈ "U" for all "x,y" ∈ "A". In a metric space this agrees with the previous definition. "X" is said to be complete if every Cauchy filter converges. Conversely, on a uniform space every convergent filter is a Cauchy filter. Moreover, every cluster point of a Cauchy filter is a limit point.
A compact uniform space is complete: on a compact space each filter has a cluster point, and if the filter is Cauchy, such a cluster point is a limit point. Further, a uniformity is compact if and only if it is complete and totally bounded.
Most generally, a Cauchy space is a set equipped with a class of filters declared to be Cauchy. These are required to have the following properties:
The Cauchy filters on a uniform space have these properties, so every uniform space (hence every metric space) defines a Cauchy space.

</doc>
<doc id="19722" url="http://en.wikipedia.org/wiki?curid=19722" title="Metallurgy">
Metallurgy

Metallurgy is a domain of materials science and engineering that studies the physical and chemical behavior of metallic elements, their intermetallic compounds, and their mixtures, which are called alloys. Metallurgy is also the technology of metals: the way in which science is applied to the production of metals, and the engineering of metal components for use in products for consumers and manufacturers. The production of metals involves the processing of ores to extract the metal they contain, and the mixture of metals, sometimes with other elements, to produce alloys. Metallurgy is distinguished from the craft of metalworking, although metalworking relies on metallurgy, as medicine relies on medical science, for technical advancement. 
Metallurgy is subdivided into "ferrous metallurgy" (sometimes also known as black metallurgy) and "non-ferrous metallurgy" or "colored metallurgy". Ferrous metallurgy involves processes and alloys based on iron while non-ferrous metallurgy involves processes and alloys based on other metals. The production of ferrous metals accounts for 95 percent of world metal production.
Etymology and pronunciation.
The word was originally an alchemist's term for the extraction of metals from minerals, the ending "-urgy" signifying a process, especially manufacturing: it was discussed in this sense in the 1797 Encyclopaedia Britannica. In the late 19th century it was extended to the more general scientific study of metals, alloys, and related processes. The roots of "metallurgy" derive from Ancient Greek: μεταλλουργός, "metallourgós", "worker in metal", from μέταλλον, "métallon", "metal" + ἔργον, "érgon", "work". In English, the /meˈtælədʒi/ pronunciation is the more common one in the UK and Commonwealth. The /ˈmetələrdʒi/ pronunciation is the more common one in the USA, and is the first-listed variant in various American dictionaries (e.g., "Merriam-Webster Collegiate", "American Heritage").
History.
The earliest recorded metal employed by humans appears to be gold which can be found free or "native." Small amounts of natural gold have been found in Spanish caves used during the late Paleolithic period, "c." 40,000 BC.
Silver, copper, tin and meteoric iron can also be found in native form, allowing a limited amount of metalworking in early cultures. Egyptian weapons made from meteoric iron in about 3000 BC were highly prized as "daggers from heaven." 
Certain metals, notably tin, lead and (at a higher temperature) copper, can be recovered from their ores by simply heating the rocks in a fire, a process known as smelting. The first evidence of this extractive metallurgy dates from the 5th and 6th millennium BC and was found in the archaeological sites of Majdanpek, Yarmovac and Plocnik, all three in Serbia. To date, the earliest evidence of copper smelting is found at the Belovode site, including a copper axe from 5500 BC belonging to the Vinča culture. Other signs of early metals are found from the third millennium BC in places like Palmela (Portugal), Los Millares (Spain), and Stonehenge (United Kingdom). However, as often happens with the study of prehistoric times, the ultimate beginnings cannot be clearly ascertained and new discoveries are both continuous and ongoing. 
These first metals were single ones or as found. About 3500 BC, it was discovered that by combining copper and tin, a superior metal could be made, an alloy called bronze, representing a major technological shift which began the Bronze Age.
The extraction of iron from its ore into a workable metal is much more difficult than for copper or tin. The process appears to have been invented by the Hittites in about 1200 BC, beginning the Iron Age. The secret of extracting and working iron was a key factor in the success of the Philistines.
Historical developments in ferrous metallurgy can be found in a wide variety of past cultures and civilizations. This includes the ancient and medieval kingdoms and empires of the Middle East and Near East, ancient Iran, ancient Egypt, ancient Nubia, and Anatolia (Turkey), Ancient Nok, Carthage, the Greeks and Romans of ancient Europe, medieval Europe, ancient and medieval China, ancient and medieval India, ancient and medieval Japan, amongst others. Many applications, practices, and devices associated or involved in metallurgy were established in ancient China, such as the innovation of the blast furnace, cast iron, hydraulic-powered trip hammers, and double acting piston bellows.
A 16th century book by Georg Agricola called "De re metallica" describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. Agricola has been described as the "father of metallurgy".
Extraction.
Extractive metallurgy is the practice of removing valuable metals from an ore and refining the extracted raw metals into a purer form. In order to convert a metal oxide or sulfide to a purer metal, the ore must be reduced physically, chemically, or electrolytically.
Extractive metallurgists are interested in three primary streams: feed, concentrate (valuable metal oxide/sulfide), and tailings (waste). After mining, large pieces of the ore feed are broken through crushing and/or grinding in order to obtain particles small enough where each particle is either mostly valuable or mostly waste. Concentrating the particles of value in a form supporting separation enables the desired metal to be removed from waste products.
Mining may not be necessary if the ore body and physical environment are conducive to leaching. Leaching dissolves minerals in an ore body and results in an enriched solution. The solution is collected and processed to extract valuable metals.
Ore bodies often contain more than one valuable metal. Tailings of a previous process may be used as a feed in another process to extract a secondary product from the original ore. Additionally, a concentrate may contain more than one valuable metal. That concentrate would then be processed to separate the valuable metals into individual constituents.
Alloys.
Common engineering metals include aluminium, chromium, copper, iron, magnesium, nickel, titanium and zinc. These are most often used as alloys. Much effort has been placed on understanding the iron-carbon alloy system, which includes steels and cast irons. Plain carbon steels (those that contain essentially only carbon as an alloying element) are used in low cost, high strength applications where weight and corrosion are not a problem. Cast irons, including ductile iron are also part of the iron-carbon system.
Stainless steel or galvanized steel are used where resistance to corrosion is important. Aluminium alloys and magnesium alloys are used for applications where strength and lightness are required.
Copper-nickel alloys (such as Monel) are used in highly corrosive environments and for non-magnetic applications. Nickel-based superalloys like Inconel are used in high temperature applications such as gas turbine, turbochargers, pressure vessel, and heat exchangers. For extremely high temperatures, single crystal alloys are used to minimize creep.
Production.
In production engineering, metallurgy is concerned with the production of metallic components for use in consumer or engineering products. This involves the production of alloys, the shaping, the heat treatment and the surface treatment of the product. The task of the metallurgist is to achieve balance between material properties such as cost, weight, strength, toughness, hardness, corrosion, fatigue resistance, and performance in temperature extremes. To achieve this goal, the operating environment must be carefully considered. In a saltwater environment, ferrous metals and some aluminium alloys corrode quickly. Metals exposed to cold or cryogenic conditions may endure a ductile to brittle transition and lose their toughness, becoming more brittle and prone to cracking. Metals under continual cyclic loading can suffer from metal fatigue. Metals under constant stress at elevated temperatures can creep.
Metalworking processes.
Metals are shaped by processes such as:
Cold-working processes, in which the product’s shape is altered by rolling, fabrication or other processes while the product is cold, can increase the strength of the product by a process called work hardening. Work hardening creates microscopic defects in the metal, which resist further changes of shape.
Various forms of casting exist in industry and academia. These include sand casting, investment casting (also called the "lost wax process"), die casting, and continuous casting.
Heat treatment.
Metals can be heat-treated to alter the properties of strength, ductility, toughness, hardness and/or resistance to corrosion. Common heat treatment processes include annealing, precipitation strengthening, quenching, and tempering. The annealing process softens the metal by heating it and then allowing it to cool very slowly, which gets rid of stresses in the metal and makes the grain structure large and soft-edged so that when the metal is hit or stressed it dents or perhaps bends, rather than breaking; it is also easier to sand, grind, or cut annealed metal. Quenching is the process of cooling a high-carbon steel very quickly after heating, thus "freezing" the steel's molecules in the very hard martensite form, which makes the metal harder. There is a balance between hardness and toughness in any steel; the harder the steel, the less tough or impact-resistant it is, and the more impact-resistant it is, the less hard it is. Tempering relieves stresses in the metal that were caused by the hardening process; tempering makes the metal less hard while making it better able to sustain impacts without breaking. 
Often, mechanical and thermal treatments are combined in what are known as thermo-mechanical treatments for better properties and more efficient processing of materials. These processes are common to high-alloy special steels, super alloys and titanium alloys.
Plating.
Electroplating is a chemical surface-treatment technique. It involves bonding a thin layer of another metal such as gold, silver, chromium or zinc to the surface of the product. It is used to reduce corrosion as well as to improve the product's aesthetic appearance.
Thermal spraying.
Thermal spraying techniques are another popular finishing option, and often have better high temperature properties than electroplated coatings.
Microstructure.
Metallurgists study the microscopic and macroscopic properties using metallography, a technique invented by Henry Clifton Sorby. In metallography, an alloy of interest is ground flat and polished to a mirror finish. The sample can then be etched to reveal the microstructure and macrostructure of the metal. The sample is then examined in an optical or electron microscope, and the image contrast provides details on the composition, mechanical properties, and processing history.
Crystallography, often using diffraction of x-rays or electrons, is another valuable tool available to the modern metallurgist. Crystallography allows identification of unknown materials and reveals the crystal structure of the sample. Quantitative crystallography can be used to calculate the amount of phases present as well as the degree of strain to which a sample has been subjected.

</doc>
<doc id="19723" url="http://en.wikipedia.org/wiki?curid=19723" title="MUMPS">
MUMPS

MUMPS ("Massachusetts General Hospital Utility Multi-Programming System") or alternatively M, is a general-purpose computer programming language that provides ACID (Atomic, Consistent, Isolated, and Durable) transaction processing. Its most unique and differentiating feature is its "built-in" database, enabling high-level access to disk storage using simple symbolic program variables and subscripted arrays, similar to the variables used by most languages to access main memory.
The M database is a key-value database engine optimized for high-throughput transaction processing. As such it is in the class of "schema-less", "schema-free," or NoSQL databases. Internally, M stores data in multidimensional hierarchical sparse arrays (also known as key-value nodes, sub-trees, or associative memory). Each array may have up to 32 subscripts, or dimensions. A scalar can be thought of as an array element with zero subscripts. Nodes with varying numbers of subscripts (including one node with no subscripts) can freely co-exist in the same array.
Perhaps the most unusual aspect of the M language is the notion that the database is accessed through variables, rather than queries or retrievals. This means that accessing volatile memory and non-volatile storage use the same basic syntax, enabling a function to work on either local (volatile) or global (non-volatile) variables. Practically, this provides for extremely high performance data access.
Originally designed in 1966 for the healthcare industry, M continues to be used today by many large hospitals and banks to provide high-throughput transaction data processing.
History.
Genesis.
MUMPS was developed by Neil Pappalardo and colleagues in Dr. Octo Barnett's animal lab at the Massachusetts General Hospital (MGH) in Boston during 1966 and 1967. The original MUMPS system was, like Unix a few years later, built on a spare DEC PDP-7. Octo Barnett and Neil Pappalardo were also involved with MGH's planning for a Hospital Information System, obtained a backward compatible PDP-9, and began using MUMPS in the admissions cycle and laboratory test reporting. MUMPS was then an interpreted language, yet even then, incorporated a hierarchical database file system to standardize interaction with the data.
Some aspects of MUMPS can be traced from Rand Corporation's JOSS through BBN's TELCOMP and STRINGCOMP. The MUMPS team deliberately chose to include portability between machines as a design goal. Another feature, not widely supported for machines of the era, in operating systems or in computer hardware, was multitasking, which was also built into the language itself.
The portability was soon useful, as MUMPS was shortly adapted to a DEC PDP-15, where it lived for some time. MUMPS was developed with the support of a government research grant, and so MUMPS was released to the public domain (no longer a requirement for grants), and was soon ported to a number of other systems including the popular DEC PDP-8, the Data General Nova and the DEC PDP-11 and the Artronix PC12 minicomputer. Word about MUMPS spread mostly through the medical community, and by the early 1970s was in widespread use, often being locally modified for their own needs.
1970s.
By the early 1970s, there were many and varied implementations of MUMPS on a range of hardware platforms. The most widespread was DEC's MUMPS-11 on the PDP-11, and MEDITECH's MIIS. In 1972, many MUMPS users attended a conference which standardized the then-fractured language, and created the MUMPS Users Group and MUMPS Development Committee (MDC) to do so. These efforts proved successful; a standard was complete by 1974, and was approved, on September 15, 1977, as ANSI standard, X11.1-1977. At about the same time DEC launched DSM-11 (Digital Standard MUMPS) for the PDP-11. This quickly dominated the market, and became the reference implementation of the time. Also, InterSystems sold ISM-11 for the PDP-11 (which was identical to DSM-11).
1980s.
During the early 1980s several vendors brought MUMPS-based platforms that met the ANSI standard to market. The most significant were:
Other companies developed important MUMPS implementations:
This period also saw considerable MDC activity. The second revision of the ANSI standard for MUMPS (X11.1-1984) was approved on November 15, 1984.
Current users of MUMPS applications.
The U.S. Department of Veterans Affairs (formerly the Veterans Administration) was one of the earliest major adopters of the MUMPS language. Their development work (and subsequent contributions to the free MUMPS application codebase) was an influence on many medical users worldwide. In 1995, the Veterans Affairs' patient Admission/Tracking/Discharge system, Decentralized Hospital Computer Program (DHCP) was the recipient of the Computerworld Smithsonian Award for best use of Information Technology in Medicine. In July 2006, the Department of Veterans Affairs (VA) / Veterans Health Administration (VHA) was the recipient of the Innovations in American Government Award presented by the Ash Institute of the John F. Kennedy School of Government at Harvard University for its extension of DHCP into the Veterans Health Information Systems and Technology Architecture (VistA). Nearly the entire VA hospital system in the United States, the Indian Health Service, and major parts of the Department of Defense CHCS hospital system use MUMPS databases for clinical data tracking.
Large companies currently using MUMPS include AmeriPath (part of Quest Diagnostics), Care Centric, Epic, Coventry Healthcare, EMIS, Partners HealthCare (including Massachusetts General Hospital), MEDITECH, GE Healthcare (formerly IDX Systems and Centricity), and Sunquest Information Systems (formerly Misys Healthcare). Many reference laboratories, such as DASA, Quest Diagnostics, and Dynacare, use MUMPS software written by or based on Antrim Corporation code. Antrim was purchased by Misys Healthcare (now Sunquest Information Systems) in 2001.
MUMPS is widely used in financial applications. MUMPS gained an early following in the financial sector, and MUMPS applications are in use at many banks and credit unions. It is used by Ameritrade, the largest online trading service in the US with over 12 billion transactions per day, as well as by the Bank of England and Barclays Bank, among others.
Since 2005, the use of MUMPS has been either in the form of GT.M or InterSystems Caché. The latter is being aggressively marketed by InterSystems and has had success in penetrating new markets, such as telecommunications, in addition to existing markets. The European Space Agency announced on May 13, 2010 that it will use the InterSystems Caché database to support the Gaia mission. This mission aims to map the Milky Way with unprecedented precision.
Overview.
MUMPS is a language intended for and designed to build database applications. Secondary language features were included to help programmers make applications using minimal computing resources. The original implementations were interpreted, though modern implementations may be fully or partially compiled. Individual "programs" run in memory "partitions". Early MUMPS memory partitions were limited to 2048 bytes so aggressive abbreviation greatly aided multi-programming on severely resource limited hardware, because more than one MUMPS job could fit into the very small memories extant in hardware at the time. The ability to provide multi-user systems was another language design. The Multi-Programming in the acronym of language name point to this. Even the earliest machines running MUMPS supported multiple jobs running at the same time. With the change from mini-computers to micro-computers a few years later, even a "single user PC" with a single 8-bit CPU with 16K or 64K of memory could support multiple users, running dumb terminals in command line mode (without any trace of a graphical user interface).
Since memory was tight originally, the language design for MUMPS valued very terse code. Thus, every MUMPS command or function name could be abbreviated from one to three letters in length, e.g. Quit (exit program) as Q, $P = $Piece function, R = Read command, $TR = $Translate function. Spaces and end-of-line markers are significant in MUMPS because line scope promoted the same terse language design. Hence, an entire line of program code could express the same idea a small number of characters that other programming languages might easily take 5 to 10 times as many characters to express. Abbreviation was a common feature of languages designed in this period (e.g., FOCAL-69, early BASICs such as Tiny BASIC, etc.). An unfortunate side effect of this coupled with the early need to write minimalist code was that MUMPS programmers routinely did not comment code and used extensive abbreviations, meaning that even an expert MUMPS programmer could not just skim through a page of code to see its function but would have to analyze it line by line.
Database interaction is transparently built into the language. The MUMPS language provides a hierarchical database made up of persistent sparse arrays, which is implicitly "opened" for every MUMPS application. All variable names prefixed with the caret character ("^") use permanent (instead of RAM) storage, will maintain their values after the application exits, and will be visible to (and modifiable by) other running applications. Variables using this shared and permanent storage are called "Globals" in MUMPS, because the scoping of these variables is "globally available" to all jobs on the system. The more recent and more common use of the name "global variables" in other languages is a more limited scoping of names, coming from the fact that unscoped variables are "globally" available to any programs running in the same process, but not shared among multiple processes. The MUMPS Storage mode (i.e. Globals stored as persistent sparse arrays), gives the MUMPS database the characteristics of a document-oriented database.
All variable names which are not prefixed with caret character ("^") are temporary and private. Like global variables, they also have a hierarchical storage model, but are only "locally available" to a single job, thus they are called "locals". Both "globals" and "locals" can have child nodes (called "subscripts" in MUMPS terminology). Subscripts are not limited to numerals—any ASCII character or group of characters can be a subscript identifier. While this is not uncommon for modern languages such as Perl or JavaScript, it was a highly unusual feature in the late 1970s. This capability was not universally implemented in MUMPS systems before the 1984 ANSI standard, as only canonically numeric subscripts were required by the standard to be allowed. Thus, the variable named 'Car' can have subscripts "Door", "Steering Wheel" and "Engine", each of which can contain a value and have subscripts of their own. The variable ^Car("Door") could have a nested variable subscript of "Color" for example. Thus, you could say
to modify a nested child node of ^Car. In MUMPS terms, "Color" is the 2nd subscript of the variable ^Car (both the names of the child-nodes and the child-nodes themselves are likewise called subscripts). Hierarchical variables are similar to objects with properties in many object oriented languages. Additionally, the MUMPS language design requires that all subscripts of variables are automatically kept in sorted order. Numeric subscripts (including floating-point numbers) are stored from lowest to highest. All non-numeric subscripts are stored in alphabetical order following the numbers. In MUMPS terminology, this is "canonical order". By using only non-negative integer subscripts, the MUMPS programmer can emulate the arrays data type from other languages. Although MUMPS does not natively offer a full set of DBMS features such as mandatory schemas, several DBMS systems have been built on top of it that provide application developers with flat-file, relational and network database features.
Additionally, there are built-in operators which treat a delimited string (e.g., comma-separated values) as an array. Early MUMPS programmers would often store a structure of related information as a delimited string, parsing it after it was read in; this saved disk access time and offered considerable speed advantages on some hardware.
MUMPS has no data types. Numbers can be treated as strings of digits, or strings can be treated as numbers by numeric operators ("coerced", in MUMPS terminology). Coercion can have some odd side effects, however. For example, when a string is coerced, the parser turns as much of the string (starting from the left) into a number as it can, then discards the rest. Thus the statement codice_1 is evaluated as codice_2 in MUMPS.
Other features of the language are intended to help MUMPS applications interact with each other in a multi-user environment. Database locks, process identifiers, and atomicity of database update transactions are all required of standard MUMPS implementations.
In contrast to languages in the C or Wirth traditions, some space characters between MUMPS statements are significant. A single space separates a command from its argument, and a space, or newline, separates each argument from the next MUMPS token. Commands which take no arguments (e.g., codice_3) require two following spaces. The concept is that one space separates the command from the (nonexistent) argument, the next separates the "argument" from the next command. Newlines are also significant; an codice_4, codice_3 or codice_6 command processes (or skips) everything else till the end-of-line. To make those statements control multiple lines, you must use the codice_7 command to create a code block.
"Hello, World!" example.
A simple Hello world program in MUMPS might be:
and would be run from the MUMPS command line with the command 'codice_8'. Since MUMPS allows commands to be strung together on the same line, and since commands can be abbreviated to a single letter, this routine could be made more compact:
The 'codice_9' after the text generates a newline. The 'codice_10' is not strictly necessary at the end of a function like this, but is good programming practice in case other functions are added below 'codice_11' later.
Summary of key language features.
The following summary seeks to give programmers familiar with other languages a feeling for what MUMPS is like. This is not a formal language specification, and many features and qualifiers have been omitted for brevity. ANSI X11.1-1995 gives a complete, formal description of the language; an annotated version of this standard is available online.
Data types: There is one universal datatype, which is implicitly coerced to string, integer, or floating-point datatypes as context requires.
Booleans (called "truthvalues" in MUMPS): In IF commands and other syntax that has expressions evaluated as conditions, any string value is evaluated as a numeric value, and if that is a nonzero value, then it is interpreted as True. codice_12 yields 1 if a is less than b, 0 otherwise.
Declarations: None. All variables are dynamically created at the first time a value is assigned.
Lines: are important syntactic entities, unlike their status in languages patterned on C or Pascal. Multiple statements per line are allowed and are common. The scope of any IF, ELSE, and FOR command is "the remainder of current line."
Case sensitivity: Commands and intrinsic functions are case-insensitive. In contrast, variable names and labels are case-sensitive. There is no special meaning for upper vs. lower-case and few widely followed conventions. The percent sign (%) is legal as first character of variables and labels.
Postconditionals: execution of almost all commands can be controlled by following it with a colon and a truthvalue expression. codice_13 sets A to "FOO" if N is less than 10; codice_14 performs PRINTERR if N is greater than 100. This construct provides a conditional whose scope is less than a full line.
Abbreviation: You can abbreviate nearly all commands and native functions to one, two, or three characters.
Reserved words: None. Since MUMPS interprets source code by context, there is no need for reserved words. You may use the names of language commands as variables. There has been no contest such as the International Obfuscated C Code Contest for MUMPS, despite the potential of examples such as the following, perfectly legal, MUMPS code:
MUMPS can be made more obfuscated by using the contracted operator syntax, as shown in this terse example derived from the example above:
Arrays: are created dynamically, stored as B-trees, are sparse (i.e. use almost no space for missing nodes), can use any number of subscripts, and subscripts can be strings or numeric (including floating point). Arrays are always automatically stored in sorted order, so there is never any occasion to sort, pack, reorder, or otherwise reorganize the database. Built in functions such as $DATA, $ORDER, $NEXT(deprecated) and $QUERY functions provide efficient examination and traversal of the fundamental array structure, on disk or in memory.
Local arrays: variable names not beginning with caret (i.e. "^") are stored in memory by process, are private to the creating process, expire when the creating process terminates. The available storage depends on implementation. For those implementations using partitions, it is limited to the partition size, (A small partition might be 32K). For other implementations, it may be several megabytes.
Global arrays: codice_15. These are stored on disk, are available to all processes, and are persistent when the creating process terminates. Very large globals (for example, hundreds of gigabytes) are practical and efficient in most implementations. This is MUMPS' main "database" mechanism. It is used instead of calling on the operating system to create, write, and read files.
Indirection: in many contexts, codice_16 can be used, and effectively substitutes the contents of VBL into another MUMPS statement. codice_17 sets the variable ABC to 123. codice_18 performs the subroutine named REPORT. This substitution allows for lazy evaluation and late binding as well as effectively the operational equivalent of "pointers" in other languages.
Piece function: This breaks variables into segmented pieces guided by a user specified separator string (sometimes called a "delimiter"). Those who know awk will find this familiar. codice_19 means the "third caret-separated piece of STRINGVAR." The piece function can also appear as an assignment (SET command) target.
codice_20 yields "std".
After
codice_21 causes X to become "office@world.std.com" (note that $P is equivalent to $PIECE and could be written as such).
Order function: This function treats its input as a structure, and finds the next index that exists which has the same structure except for the last subscript. It returns the sorted value that is ordered after the one given as input. (This treats the array reference as a content-addressable data rather than an address of a value)
codice_22 yields 6, codice_23 yields 10, codice_24 yields 10, codice_25 yields 15, codice_26 yields "".
Here, the argument-less "For" repeats until stopped by a terminating "Quit". This line prints a table of i and stuff(i) where i is successively 6, 10, and 15.
For iterating the database, the Order function returns the next key to use.
Multi-User/Multi-Tasking/Multi-Processor: MUMPS supports multiple simultaneous users and processes even when the underlying operating system does not (e.g., MS-DOS). Additionally, there is the ability to specify an environment for a variable, such as by specifying a machine name in a variable (as in codice_27), which can allow you to access data on remote machines.
For a thorough listing of the rest of the MUMPS commands, operators, functions and special variables, see these online resources:
Criticism.
The Daily WTF, a site devoted to sharing programming "horror stories", includes a sharply critical article describing one programmer's experience with MUMPS's syntax and features.
"MUMPS" vs. "M" naming debate.
While of little interest to those outside the MUMPS/M community, this topic has been contentious there.
All of the following positions can be, and have been, supported by knowledgeable people at various times:
Some of the contention arose in response to strong M advocacy on the part of one commercial interest, InterSystems, whose chief executive disliked the name MUMPS and felt that it represented a serious marketing obstacle. Thus, favoring M to some extent became identified as alignment with InterSystems. The dispute also reflected rivalry between organizations (the M Technology Association, the MUMPS Development Committee, the ANSI and ISO Standards Committees) as to who determines the "official" name of the language. Some writers have attempted to defuse the issue by referring to the language as "M[UMPS]", square brackets being the customary notation for optional syntax elements. A leading authority, and the author of an open source MUMPS implementation, Professor Kevin O'Kane, uses only 'MUMPS'.
The most recent standard (ISO/IEC 11756:1999, re-affirmed on 25 June 2010), still mentions both M and MUMPS as officially accepted names.

</doc>
<doc id="19726" url="http://en.wikipedia.org/wiki?curid=19726" title="Mercury (programming language)">
Mercury (programming language)

 
Mercury is a functional logic programming language geared towards real-world applications. It was initially developed at the University Of Melbourne Computer Science department under the supervision of Zoltan Somogyi. The first version was developed by Fergus Henderson, Thomas Conway and Zoltan Somogyi and was released on April 8, 1995.
Mercury is a purely declarative logic language. It is related to both Prolog and Haskell. It features a strong, static, polymorphic type system, as well as a strong mode and determinism system.
The official implementation, the Melbourne Mercury Compiler, is available for most Unix platforms, including Mac OS X, as well as for Microsoft Windows.
Overview.
Mercury is based on the logic programming language Prolog. It has the same syntax, and the same basic concepts such as the SLD resolution algorithm. It can be viewed as a pure subset of Prolog with strong types and modes. As such, it is often compared to its predecessor, both in terms of features, and run-time efficiency.
The language is designed with software engineering principles in mind. Unlike the original implementations of Prolog, it has a separate compilation phase, rather than being directly interpreted, which allows a much wider range of errors to be caught before running a program. It features a strict static type and mode system and a module system.
Due to the use of information obtained at compile time (such as type and mode information), programs written in Mercury typically perform significantly faster than equivalent programs written in Prolog. Its authors claim that Mercury is the fastest logic language in the world, by a wide margin.
Mercury is a purely declarative language, unlike Prolog, since it lacks "extra-logical" Prolog statements such as "cut" and imperative I/O. This enables advanced static analysis and program optimization, including compile-time garbage collection, but can make certain programming constructs (such as a switch over a number of options, with a default) harder to express. (Note that while Mercury does allow impure functionality, this serves primarily as a way of calling foreign language code. All impure code must be explicitly marked.) Operations which would typically be impure (such as input/output) are expressed using pure constructs in Mercury using linear types, by threading a dummy "world" value through all relevant code.
Notable programs written in Mercury include the Mercury compiler itself and the Prince XML formatter. Mission Critical IT , a software company, has also been using Mercury since 2000 to develop enterprise applications
and its Ontology-Driven software development platform ODASE.
Back-ends.
Mercury has several back-ends, which means it is possible to compile Mercury code into the following languages:
Production level:
Beta quality:
Alpha quality (may not work well, or even be completely broken):
Past back-ends:
Mercury also features a foreign language interface, allowing code in other languages (depending on the chosen back-end) to be linked with Mercury code. The following foreign languages are possible:
Other languages can then be interfaced to by calling them from these languages. However, this means that foreign language code may need to be written several times for the different backends, otherwise portability between backends will be lost.
The most commonly used back-end is the original low-level C back-end.
Examples.
Hello World:
Calculating the 10th Fibonacci number (in the most obvious way):
Release schedule.
The Mercury project has a 6 monthly release cycle. Releases are named according to the year and month of the release.
The current stable release is 14.01.1 (September 2014).
Previously releases were numbered 0.12, 0.13, etc. and the period between stable releases was very large (3 years).
There is also a snapshot release consisting of the latest features and bug fixes added to the last stable release.

</doc>
<doc id="19727" url="http://en.wikipedia.org/wiki?curid=19727" title="Michael Faraday">
Michael Faraday

Michael Faraday FRS (22 September 1791 – 25 August 1867) was an English scientist who contributed to the fields of electromagnetism and electrochemistry. His main discoveries include those of electromagnetic induction, diamagnetism and electrolysis.
Although Faraday received little formal education, he was one of the most influential scientists in history. It was by his research on the magnetic field around a conductor carrying a direct current that Faraday established the basis for the concept of the electromagnetic field in physics. Faraday also established that magnetism could affect rays of light and that there was an underlying relationship between the two phenomena. He similarly discovered the principle of electromagnetic induction, diamagnetism, and the laws of electrolysis. His inventions of electromagnetic rotary devices formed the foundation of electric motor technology, and it was largely due to his efforts that electricity became practical for use in technology.
As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented an early form of the Bunsen burner and the system of oxidation numbers, and popularised terminology such as anode, cathode, electrode, and ion. Faraday ultimately became the first and foremost Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a lifetime position.
Faraday was an excellent experimentalist who conveyed his ideas in clear and simple language; his mathematical abilities, however, did not extend as far as trigonometry or any but the simplest algebra. James Clerk Maxwell took the work of Faraday and others, and summarized it in a set of equations that is accepted as the basis of all modern theories of electromagnetic phenomena. On Faraday's uses of the lines of force, Maxwell wrote that they show Faraday "to have been in reality a mathematician of a very high order – one from whom the mathematicians of the future may derive valuable and fertile methods." The SI unit of capacitance is named in his honour: the farad.
Albert Einstein kept a picture of Faraday on his study wall, alongside pictures of Isaac Newton and James Clerk Maxwell. Physicist Ernest Rutherford stated; "When we consider the magnitude and extent of his discoveries and their influence on the progress of science and of industry, there is no honour too great to pay to the memory of Faraday, one of the greatest scientific discoverers of all time".
Personal life.
Early life.
Faraday was born in Newington Butts, which is now part of the London Borough of Southwark, but which was then a suburban part of Surrey. His family was not well off. His father, James, was a member of the Glassite sect of Christianity. James Faraday moved his wife and two children to London during the winter of 1790 from Outhgill in Westmorland, where he had been an apprentice to the village blacksmith. Michael was born the autumn of that year. The young Michael Faraday, who was the third of four children, having only the most basic school education, had to educate himself. At fourteen he became the apprentice to George Riebau, a local bookbinder and bookseller in Blandford Street. During his seven-year apprenticeship he read many books, including Isaac Watts' "The Improvement of the Mind", and he enthusiastically implemented the principles and suggestions contained therein. At this time he also developed an interest in science, especially in electricity. Faraday was particularly inspired by the book "Conversations on Chemistry" by Jane Marcet.
Adult life.
In 1812, at the age of twenty, and at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist Humphry Davy of the Royal Institution and Royal Society, and John Tatum, founder of the City Philosophical Society. Many of the tickets for these lectures were given to Faraday by William Dance, who was one of the founders of the Royal Philharmonic Society. Faraday subsequently sent Davy a three-hundred-page book based on notes that he had taken during these lectures. Davy's reply was immediate, kind, and favourable. In 1813, when Davy damaged his eyesight in an accident with nitrogen trichloride, he decided to employ Faraday as an assistant. Coincidentally one of the Royal Institution's assistants, John Payne, was sacked, and Sir Humphry Davy was asked to find a replacement; thus he appointed Faraday as Chemical Assistant at the Royal Institution on 1 March 1813. Very soon Davy entrusted Faraday with preparation of nitrogen trichloride samples, and they both became injured in an explosion of this very sensitive substance. 
In the class-based English society of the time, Faraday was not considered a gentleman. When Davy set out on a long tour of the continent in 1813–15, his valet did not wish to go. Instead, Faraday went as Davy's scientific assistant, and was asked to act as Davy's valet until a replacement could be found in Paris. Faraday was forced to fill the role of valet as well as assistant throughout the trip. Davy's wife, Jane Apreece, refused to treat Faraday as an equal (making him travel outside the coach, eat with the servants, etc.), and made Faraday so miserable that he contemplated returning to England alone and giving up science altogether. The trip did, however, give him access to the scientific elite of Europe and exposed him to a host of stimulating ideas.
Faraday married Sarah Barnard (1800–1879) on 12 June 1821. They met through their families at the Sandemanian church, and he confessed his faith to the Sandemanian congregation the month after they were married. They had no children.
Faraday was a devout Christian; his Sandemanian denomination was an offshoot of the Church of Scotland. Well after his marriage, he served as deacon and for two terms as an elder in the meeting house of his youth. His church was located at Paul's Alley in the Barbican. This meeting house was relocated in 1862 to Barnsbury Grove, Islington; this North London location was where Faraday served the final two years of his second term as elder prior to his resignation from that post. Biographers have noted that "a strong sense of the unity of God and nature pervaded Faraday's life and work."
Later life.
In June 1832, the University of Oxford granted Faraday a Doctor of Civil Law degree (honorary). During his lifetime, he was offered a knighthood in recognition for his services to science, which he turned down on religious grounds, believing it was against the word of the Bible to accumulate riches and pursue worldly reward, stating he preferred to remain "plain Mr Faraday to the end". He twice refused to become President of the Royal Society. He was elected a foreign member of the Royal Swedish Academy of Sciences in 1838, and was one of eight foreign members elected to the French Academy of Sciences in 1844.
Faraday suffered a nervous breakdown in 1839 but eventually returned to his electromagnetic investigations. In 1848, as a result of representations by the Prince Consort, Faraday was awarded a grace and favour house in Hampton Court in Middlesex, free of all expenses or upkeep. This was the Master Mason's House, later called Faraday House, and now No. 37 Hampton Court Road. In 1858 Faraday retired to live there.
When asked by the British government to advise on the production of chemical weapons for use in the Crimean War (1853–1856), Faraday refused to participate citing ethical reasons.
Faraday died at his house at Hampton Court on 25 August 1867, aged 75. He had previously turned down burial in Westminster Abbey, but he has a memorial plaque there, near Isaac Newton's tomb. Faraday was interred in the dissenters' (non-Anglican) section of Highgate Cemetery. 
Scientific achievements.
Chemistry.
Faraday's earliest chemical work was as an assistant to Humphry Davy. Faraday was specifically involved in the study of chlorine; he discovered two new compounds of chlorine and carbon. He also conducted the first rough experiments on the diffusion of gases, a phenomenon that was first pointed out by John Dalton. The physical importance of this phenomenon was more fully revealed by Thomas Graham and Joseph Loschmidt. Faraday succeeded in liquefying several gases, investigated the alloys of steel, and produced several new kinds of glass intended for optical purposes. A specimen of one of these heavy glasses subsequently became historically important; when the glass was placed in a magnetic field Faraday determined the rotation of the plane of polarisation of light. This specimen was also the first substance found to be repelled by the poles of a magnet.
Faraday invented an early form of what was to become the Bunsen burner, which is in practical use in science laboratories around the world as a convenient source of heat.
Faraday worked extensively in the field of chemistry, discovering chemical substances such as benzene (which he called bicarburet of hydrogen) and liquefying gases such as chlorine. The liquefying of gases helped to establish that gases are the vapours of liquids possessing a very low boiling point and gave a more solid basis to the concept of molecular aggregation. In 1820 Faraday reported the first synthesis of compounds made from carbon and chlorine, C2Cl6 and C2Cl4, and published his results the following year. Faraday also determined the composition of the chlorine clathrate hydrate, which had been discovered by Humphry Davy in 1810. Faraday is also responsible for discovering the laws of electrolysis, and for popularizing terminology such as anode, cathode, electrode, and ion, terms proposed in large part by William Whewell.
Faraday was the first to report what later came to be called metallic nanoparticles. In 1847 he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal. This was probably the first reported observation of the effects of quantum size, and might be considered to be the birth of nanoscience.
Electricity and magnetism.
Faraday is best known for his work regarding electricity and magnetism. His first recorded experiment was the construction of a voltaic pile with seven ha'penny coins, stacked together with seven disks of sheet zinc, and six pieces of paper moistened with salt water. With this pile he decomposed sulphate of magnesia (first letter to Abbott, 12 July 1812).
In 1821, soon after the Danish physicist and chemist Hans Christian Ørsted discovered the phenomenon of electromagnetism, Davy and British scientist William Hyde Wollaston tried, but failed, to design an electric motor. Faraday, having discussed the problem with the two men, went on to build two devices to produce what he called "electromagnetic rotation". One of these, now known as the homopolar motor, caused a continuous circular motion that was engendered by the circular magnetic force around a wire that extended into a pool of mercury wherein was placed a magnet; the wire would then rotate around the magnet if supplied with current from a chemical battery. These experiments and inventions formed the foundation of modern electromagnetic technology. In his excitement, Faraday published results without acknowledging his work with either Wollaston or Davy. The resulting controversy within the Royal Society strained his mentor relationship with Davy and may well have contributed to Faraday’s assignment to other activities, which consequently prevented his involvement in electromagnetic research for several years.
From his initial discovery in 1821, Faraday continued his laboratory work, exploring electromagnetic properties of materials and developing requisite experience. In 1824, Faraday briefly set up a circuit to study whether a magnetic field could regulate the flow of a current in an adjacent wire, but he found no such relationship. This experiment followed similar work conducted with light and magnets three years earlier that yielded identical results. During the next seven years, Faraday spent much of his time perfecting his recipe for optical quality (heavy) glass, borosilicate of lead, which he used in his future studies connecting light with magnetism. In his spare time, Faraday continued publishing his experimental work on optics and electromagnetism; he conducted correspondence with scientists whom he had met on his journeys across Europe with Davy, and who were also working on electromagnetism. Two years after the death of Davy, in 1831, he began his great series of experiments in which he discovered electromagnetic induction, recording in his laboratory diary on 28 October 1831 he was; "making many experiments with the great magnet of the Royal Society".
Faraday's breakthrough came when he wrapped two insulated coils of wire around an iron ring, and found that upon passing a current through one coil a momentary current was induced in the other coil. This phenomenon is now known as mutual induction. The iron ring-coil apparatus is still on display at the Royal Institution. In subsequent experiments, he found that if he moved a magnet through a loop of wire an electric current flowed in that wire. The current also flowed if the loop was moved over a stationary magnet. His demonstrations established that a changing magnetic field produces an electric field; this relation was modelled mathematically by James Clerk Maxwell as Faraday's law, which subsequently became one of the four Maxwell equations, and which have in turn evolved into the generalization known today as field theory. Faraday would later use the principles he had discovered to construct the electric dynamo, the ancestor of modern power generators and the electric motor.
In 1832, he completed a series of experiments aimed at investigating the fundamental nature of electricity; Faraday used "static", batteries, and "animal electricity" to produce the phenomena of electrostatic attraction, electrolysis, magnetism, etc. He concluded that, contrary to the scientific opinion of the time, the divisions between the various "kinds" of electricity were illusory. Faraday instead proposed that only a single "electricity" exists, and the changing values of quantity and intensity (current and voltage) would produce different groups of phenomena.
Near the end of his career, Faraday proposed that electromagnetic forces extended into the empty space around the conductor. This idea was rejected by his fellow scientists, and Faraday did not live to see the eventual acceptance of his proposition by the scientific community. Faraday's concept of lines of flux emanating from charged bodies and magnets provided a way to visualize electric and magnetic fields; that conceptual model was crucial for the successful development of the electromechanical devices that dominated engineering and industry for the remainder of the 19th century.
Diamagnetism.
In 1845, Faraday discovered that many materials exhibit a weak repulsion from a magnetic field: a phenomenon he termed diamagnetism.
Faraday also discovered that the plane of polarization of linearly polarized light can be rotated by the application of an external magnetic field aligned with the direction in which the light is moving. This is now termed the Faraday effect. He wrote in his notebook, "I have at last succeeded in "illuminating a magnetic curve" or "line of force" and in "magnetising a ray of light"".
Later on in his life, in 1862, Faraday used a spectroscope to search for a different alteration of light, the change of spectral lines by an applied magnetic field. The equipment available to him was, however, insufficient for a definite determination of spectral change. Pieter Zeeman later used an improved apparatus to study the same phenomenon, publishing his results in 1897 and receiving the 1902 Nobel Prize in Physics for his success. In both his 1897 paper and his Nobel acceptance speech, Zeeman made reference to Faraday's work.
Faraday cage.
In his work on static electricity, Faraday's ice pail experiment demonstrated that the charge resided only on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields emanating from them cancel one another. This shielding effect is used in what is now known as a Faraday cage.
Royal Institution and public service.
Faraday had a long association with the Royal Institution of Great Britain. He was appointed Assistant Superintendent of the House of the Royal Institution in 1821. He was elected a member of the Royal Society in 1824. In 1825, he became Director of the Laboratory of the Royal Institution. Six years later, in 1833, Faraday became the first Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a position to which he was appointed for life without the obligation to deliver lectures. His sponsor and mentor was John 'Mad Jack' Fuller, who created the position at the Royal Institution for Faraday.
Beyond his scientific research into areas such as chemistry, electricity, and magnetism at the Royal Institution, Faraday undertook numerous, and often time-consuming, service projects for private enterprise and the British government. This work included investigations of explosions in coal mines, being an expert witness in court, and along with two engineers from Chance Brothers c.1853, the preparation of high-quality optical glass, which was required by Chance for its lighthouses. In 1846, together with Charles Lyell, he produced a lengthy and detailed report on a serious explosion in the colliery at Haswell County Durham, which killed 95 miners. Their report was a meticulous forensic investigation and indicated that coal dust contributed to the severity of the explosion. The report should have warned coal owners of the hazard of coal dust explosions, but the risk was ignored for over 60 years until the Senghenydd Colliery Disaster of 1913.
As a respected scientist in a nation with strong maritime interests, Faraday spent extensive amounts of time on projects such as the construction and operation of light houses and protecting the bottoms of ships from corrosion. His workshop still stands at Trinity Buoy Wharf above the Chain and Buoy Store, next to London's only lighthouse where he carried out the first experiments in electric lighting for lighthouses.
Faraday was also active in what would now be called environmental science, or engineering. He investigated industrial pollution at Swansea and was consulted on air pollution at the Royal Mint. In July 1855, Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames, which resulted in an oft-reprinted cartoon in Punch. (See also The Great Stink.)
Faraday assisted with the planning and judging of exhibits for the Great Exhibition of 1851 in London. He also advised the National Gallery on the cleaning and protection of its art collection, and served on the National Gallery Site Commission in 1857.
Education was another of Faraday's areas of service; he lectured on the topic in 1854 at the Royal Institution, and in 1862 he appeared before a Public Schools Commission to give his views on education in Great Britain. Faraday also weighed in negatively on the public's fascination with table-turning, mesmerism, and seances, and in so doing chastised both the public and the nation's educational system.
Before his famous Christmas lectures, Faraday delivered chemistry lectures for the City Philosophical Society from 1816 to 1818 in order to refine the quality of his lectures. Between 1827 and 1860 at the Royal Institution in London, Faraday gave a series of nineteen Christmas lectures for young people, a series which continues today. The objective of Faraday’s Christmas lectures was to present science to the general public in the hopes of inspiring them and generating revenue for the Royal Institution. They were notable events on the social calendar among London’s gentry. Over the course of several letters to his close friend Benjamin Abbott, Faraday outlined his recommendations on the art of lecturing: Faraday wrote “a flame should be lighted at the commencement and kept alive with unremitting splendour to the end”. His lectures were joyful and juvenile, he delighted in filling soap bubbles with various gasses (in order to determine whether or not they are magnetic) in front of his audiences and marveled at the rich colors of polarized lights, but the lectures were also deeply philosophical. In his lectures he urged his audiences to consider the mechanics of his experiments: “you know very well that ice floats upon water . . . Why does the ice float? Think of that, and philosophise”. His subjects included:
Commemorations.
A statue of Faraday stands in Savoy Place, London, outside the Institution of Engineering and Technology. Also in London, the Michael Faraday Memorial, designed by brutalist architect Rodney Gordon and completed in 1961, is at the Elephant & Castle gyratory system, near Faraday's birthplace at Newington Butts. Faraday School is located on Trinity Buoy Wharf where his workshop still stands above the Chain and Buoy Store, next to London's only lighthouse.
Faraday Gardens is a small park in Walworth, London, not far from his birthplace at Newington Butts. This park lies within the local council ward of Faraday in the London Borough of Southwark. Michael Faraday Primary school is situated on the Aylesbury Estate in Walworth.
A building at London South Bank University, which houses the institute's electrical engineering departments is named the Faraday Wing, due to its proximity to Faraday's birthplace in Newington Butts. A hall at Loughborough University was named after Faraday in 1960. Near the entrance to its dining hall is a bronze casting, which depicts the symbol of an electrical transformer, and inside there hangs a portrait, both in Faraday's honour. An eight-story building at the University of Edinburgh's science & engineering campus is named for Faraday, as is a recently built hall of accommodation at Brunel University, the main engineering building at Swansea University, and the instructional and experimental physics building at Northern Illinois University. The former UK Faraday Station in Antarctica was named after him.
Streets named for Faraday can be found in many British cities (e.g., London, Fife, Swindon, Basingstoke, Nottingham, Whitby, Kirkby, Crawley, Newbury, Swansea, Aylesbury and Stevenage) as well as in France (Paris), Germany (Berlin-Dahlem, Hermsdorf), Canada (Quebec; Deep River, Ontario; Ottawa, Ontario), and the United States (Reston, Virginia).
A Royal Society of Arts blue plaque, unveiled in 1876, commemorates Faraday at 48 Blandford Street in London's Marylebone district. From 1991 until 2001, Faraday's picture featured on the reverse of Series E £20 banknotes issued by the Bank of England. He was portrayed conducting a lecture at the Royal Institution with the magneto-electric spark apparatus. In 2002, Faraday was ranked number 22 in the BBC's list of the 100 Greatest Britons following a UK-wide vote.
The Faraday Institute for Science and Religion derives its name from the scientist, who saw his faith as integral to his scientific research. The logo of the Institute is also based on Faraday's discoveries. It was created in 2006 by a $2,000,000 grant from the John Templeton Foundation to carry out academic research, to foster understanding of the interaction between science and religion, and to engage public understanding in both these subject areas.
Faraday's life and contributions to electromagnetics was the principal topic of the tenth episode of the 2014 American science documentary series, "", which was broadcast on Fox and the National Geographic Channel.
Bibliography.
Faraday's books, with the exception of "Chemical Manipulation", were collections of scientific papers or transcriptions of lectures. Since his death, Faraday's diary has been published, as have several large volumes of his letters and Faraday's journal from his travels with Davy in 1813–1815.

</doc>
<doc id="19728" url="http://en.wikipedia.org/wiki?curid=19728" title="Marriage">
Marriage

Marriage, also called matrimony or wedlock, is a socially or ritually recognized union or legal contract between spouses that establishes rights and obligations between them, between them and their children, and between them and their in-laws. The definition of marriage varies according to different cultures, but it is principally an institution in which interpersonal relationships, usually sexual, are acknowledged. In some cultures, marriage is recommended or considered to be compulsory before pursuing any sexual activity. When defined broadly, marriage is considered a cultural universal.
Individuals may marry for several reasons, including legal, social, libidinal, emotional, financial, spiritual, and religious purposes. Who they marry may be influenced by socially determined rules of incest, prescriptive marriage rules, parental choice and individual desire. In some areas of the world, arranged marriage, child marriage, polygamy, and sometimes forced marriage, may be practiced as a cultural tradition. Conversely, such practices may be outlawed and penalized in parts of the world out of concerns for women's rights and because of international law. In developed parts of the world, there has been a general trend towards ensuring equal rights within marriage for women and legally recognizing the marriages of interracial, interfaith, and same-gender couples. Oftentimes, these trends have been motivated by a desire to establish equality and uphold human rights.
Marriage can be recognized by a state, an organization, a religious authority, a tribal group, a local community or peers. It is often viewed as a contract. Civil marriage, which does not exist in some countries, is marriage without religious content carried out by a government institution in accordance with the marriage laws of the jurisdiction, and recognised as creating the rights and obligations intrinsic to matrimony. Marriages can be performed in a secular civil ceremony or in a religious setting via a wedding ceremony. The act of marriage usually creates normative or legal obligations between the individuals involved, and any offspring they may produce. In terms of legal recognition, most sovereign states and other jurisdictions limit marriage to opposite-sex couples and a diminishing number of these permit polygyny, child marriages, and forced marriages. Over the twentieth century, a growing number of countries and other jurisdictions have lifted bans on and have established legal recognition for interracial marriage, interfaith marriage, and most recently, same-sex marriage. Some cultures allow the dissolution of marriage through divorce or annulment. In some areas, child marriages and polygamy may occur in spite of national laws against the practice.
Since the late twentieth century, major social changes in Western countries have led to changes in the demographics of marriage, with the age of first marriage increasing, fewer people marrying, and more couples choosing to cohabit rather than marry. For example, the number of marriages in Europe decreased by 30% from 1975 to 2005. As marriage has become less common, it has, proportionately, become an upper-middle-class and upper-class phenomenon.
Historically, in most cultures, married women had very few rights of their own, being considered, along with the family's children, the property of the husband; as such, they could not own or inherit property, or represent themselves legally (see for example coverture). In Europe, the United States, and other places in the developed world, beginning in the late 19th century and lasting through the 21st century, marriage has undergone gradual legal changes, aimed at improving the rights of the wife. These changes included giving wives legal identities of their own, abolishing the right of husbands to physically discipline their wives, giving wives property rights, liberalizing divorce laws, providing wives with reproductive rights of their own, and requiring a wife's consent when sexual relations occur. These changes have occurred primarily in Western countries. In the 21st century, there continue to be controversies regarding the legal status of married women, legal acceptance of or leniency towards violence within marriage (especially sexual violence), traditional marriage customs such as dowry and bride price, forced marriage, marriageable age, and criminalization of consensual behaviors such as premarital and extramarital sex.
Etymology.
The word "marriage" derives from Middle English "mariage", which first appears in 1250–1300 CE. This in turn is derived from Old French "marier" (to marry) and ultimately Latin "marītāre" meaning to provide with a husband or wife and "marītāri" meaning to get married. The adjective "marīt-us -a, -um" meaning matrimonial or nuptial could also be used in the masculine form as a noun for "husband" and in the feminine form for "wife."
The related word "matrimony" derives from the Old French word "matremoine" which appears around 1300 CE and ultimately derives from Latin "mātrimōnium" which combines the two concepts "mater" meaning "mother" and the suffix -"monium" signifying "action, state, or condition." "
Definitions.
Anthropologists have proposed several competing definitions of marriage in an attempt to encompass the wide variety of marital practices observed across cultures. Even within Western culture, "definitions of marriage have careened from one extreme to another and everywhere in between" (as Evan Gerstmann has put it).
Relation recognized by custom or law.
In "The History of Human Marriage" (1922), Edvard Westermarck defined marriage as "a more or less durable connection between male and female lasting beyond the mere act of propagation till after the birth of the offspring." In "The Future of Marriage in Western Civilization" (1936), he rejected his earlier definition, instead provisionally defining marriage as "a relation of one or more men to one or more women that is recognized by custom or law".
Legitimacy of offspring.
The anthropological handbook "Notes and Queries" (1951) defined marriage as "a union between a man and a woman such that children born to the woman are the recognized legitimate offspring of both partners." In recognition of a practice by the Nuer of Sudan allowing women to act as a husband in certain circumstances (the Ghost marriage), Kathleen Gough suggested modifying this to "a woman and one or more other persons."
In an analysis of marriage among the Nayar, a polyandrous society in India, Gough found that the group lacked a husband role in the conventional sense; that unitary role in the west was divided between a non-resident "social father" of the woman's children, and her lovers who were the actual procreators. None of these men had legal rights to the woman's child. This forced Gough to disregard sexual access as a key element of marriage and to define it in terms of legitimacy of offspring alone: marriage is "a relationship established between a woman and one or more other persons, which provides a child born to the woman under circumstances not prohibited by the rules of relationship, is accorded full birth-status rights common to normal members of his society or social stratum."
Economic anthropologist Duran Bell has criticized the legitimacy-based definition on the basis that some societies do not require marriage for legitimacy. He argued that a legitimacy-based definition of marriage is circular in societies where illegitimacy has no other legal or social implications for a child other than the mother being unmarried.
Collection of rights.
Edmund Leach criticized Gough's definition for being too restrictive in terms of recognized legitimate offspring and suggested that marriage be viewed in terms of the different types of rights it serves to establish. In 1955 article in "Man", Leach argued that no one definition of marriage applied to all cultures. He offered a list of ten rights associated with marriage, including sexual monopoly and rights with respect to children, with specific rights differing across cultures. Those rights, according to Leach, included:
Right of sexual access.
In a 1997 article in "Current Anthropology", Duran Bell describes marriage as "a relationship between one or more men (male or female) in severalty to one or more women that provides those men with a demand-right of sexual access within a domestic group and identifies women who bear the obligation of yielding to the demands of those specific men." In referring to "men in severalty", Bell is referring to corporate kin groups such as lineages which, in having paid brideprice, retain a right in a woman's offspring even if her husband (a lineage member) deceases (Levirate marriage). In referring to "men (male or female)", Bell is referring to women within the lineage who may stand in as the "social fathers" of the wife's children born of other lovers. (See Nuer "Ghost marriage")
Types of marriage.
Monogamy.
Monogamy is a form of marriage in which an individual has only one spouse during their lifetime or at any one time (serial monogamy).
Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found a strong correlation between intensive plough agriculture, dowry and monogamy. This pattern was found in a broad swath of Eurasian societies from Japan to Ireland. The majority of Sub-Saharan African societies that practice extensive hoe agriculture, in contrast, show a correlation between "Bride price," and polygamy. A further study drawing on the Ethnographic Atlas showed a statistical correlation between increasing size of the society, the belief in "high gods" to support human morality, and monogamy.
In the countries which do not permit polygamy, a person who marries in one of those countries a person while still being lawfully married to another commits the crime of bigamy. In all cases, the second marriage is considered legally null and void. Besides the second and subsequent marriages being void, the bigamist is also liable to other penalties, which also vary between jurisdictions.
Serial monogamy.
Governments that support monogamy, may allow easy divorce. In a number of Western countries divorce rates approach 50%. Those who remarry do so on average 3 times. Divorce and remarriage can thus result in "serial monogamy", i.e. multiple marriages but only one legal spouse at a time. This can be interpreted as a form of plural mating, as are those societies dominated by female-headed families in the Caribbean, Mauritius and Brazil where there is frequent rotation of unmarried partners. In all, these account for 16 to 24% of the "monogamous" category.
Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, remains an active part of her "ex-husband's" life, as they may be tied together by transfers of resources (alimony, child support), or shared child custody. Bob Simpson notes that in the British case, serial monogamy creates an "extended family" – a number of households tied together in this way, including mobile children (possible ex's may include an ex-wife, an ex-brother-in-law, etc., but not an "ex-child"). These "unclear families" do not fit the mould of the monogamous nuclear family. As a series of connected households, they come to resemble the polygynous model of separate households maintained by mothers with children, tied by a male to whom they are married or divorced.
Polygamy.
Polygamy is a marriage which includes more than two partners. When a man is married to more than one wife at a time, the relationship is called polygyny, and there is no marriage bond between the wives; and when a woman is married to more than one husband at a time, it is called polyandry, and there is no marriage bond between the husbands. If a marriage includes multiple husbands and wives, it can be called group marriage.
A molecular genetic study of global human genetic diversity argued that sexual polygyny was typical of human reproductive patterns until the shift to sedentary farming communities approximately 10,000 to 5,000 years ago in Europe and Asia, and more recently in Africa and the Americas. As noted above, Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found that the majority of Sub-Saharan African societies that practice extensive hoe agriculture show a correlation between "Bride price," and polygamy. A survey of other cross-cultural samples has confirmed that the absence of the plough was the only predictor of polygamy, although other factors such as high male mortality in warfare (in non-state societies) and pathogen stress (in state societies) had some impact.
Marriages are classified according to the number of legal spouses an individual has. The suffix "-gamy" refers specifically to the number of spouses, as in bi-gamy (two spouses, generally illegal in most nations), and poly-gamy (more than one spouse).
Societies show variable acceptance of polygamy as a cultural ideal and practice. According to the Ethnographic Atlas, of 1,231 societies noted, 186 were monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. However, as Miriam Zeitzen writes, social tolerance for polygamy is different from the practice of polygamy, since it requires wealth to establish multiple households for multiple wives. The actual practice of polygamy in a tolerant society may actually be low, with the majority of aspirant polygamists practicing monogamous marriage. Tracking the occurrence of polygamy is further complicated in jurisdictions where it has been banned, but continues to be practiced ("de facto polygamy").
Zeitzen also notes that Western perceptions of African society and marriage patterns are biased by "contradictory concerns of nostalgia for traditional African culture versus critique of polygamy as oppressive to women or detrimental to development." Polygamy has been condemned as being a form of human rights abuse, with concerns arising over domestic abuse, forced marriage, and neglect. The vast majority of the world's countries, including virtually all of the world's developed nations, do not permit polygamy. There have been calls for the abolition of polygamy in developing countries.
Polygyny.
Although a society may be classified as polgynous, not all marriages in it necessarily are; monogamous marriages may in fact predominate. It is to this flexibility that Anthropologist Robin Fox attributes its success as a social support system: "This has often meant – given the imbalance in the sex ratios, the higher male infant mortality, the shorter life span of males, the loss of males in wartime, etc. – that often women were left without financial support from husbands. To correct this condition, females had to be killed at birth, remain single, become prostitutes, or be siphoned off into celibate religious orders. Polygynous systems have the advantage that they can promise, as did the Mormons, a home and family for every woman."
Nonetheless, polygyny is a gender issue which offers men asymmetrical benefits. In some cases, there is a large age discrepancy (as much as a generation) between a man and his youngest wife, compounding the power differential between the two. Tensions not only exist "between" genders, but also "within" genders; senior and junior men compete for wives, and senior and junior wives in the same household may experience radically different life conditions, and internal hierarchy. Several studies have suggested that the wive's relationship with other women, including co-wives and husband's female kin, are more critical relationships than that with her husband for her productive, reproductive and personal achievement. In some societies, the co-wives are relatives, usually sisters, a practice called "sororal polygyny"; the pre-existing relationship between the co-wives is thought to decrease potential tensions within the marriage.
Fox argues that "the major difference between polygyny and monogamy could be stated thus: while plural mating occurs in both systems, under polygyny several unions may be recognized as being legal marriages while under monogamy only one of the unions is so recognized. Often, however, it is difficult to draw a hard and fast line between the two."
As polygamy in Africa is increasingly subject to legal limitations, a variant form of "de facto" (as opposed to legal or "de jure") polygyny is being practised in urban centres. Although it does not involve multiple (now illegal) formal marriages, the domestic and personal arrangements follow old polygynous patterns. The de facto form of polygyny is found in other parts of the world as well (including some Mormon sects and Muslim families in the United States).
In some societies such as the Lovedu of South Africa, or the Nuer of the Sudan, aristocratic women may become female 'husbands.' In the Lovedu case, this female husband may take a number of polygamous wives. This is not a lesbian relationship, but a means of legitimately expanding a royal lineage by attaching these wives' children to it. The relationships are considered polygynous, not polyandrous, because the female husband is in fact assuming masculine gendered political roles.
Religious groups have differing views on the legitimacy of polygyny. It is allowed in Islam and Confucianism, though in most areas today it is uncommon. Judaism, Christianity and Hinduism have allowed polygyny in the past, but it is prohibited today.
Polyandry.
Polyandry is notably more rare than polygyny, though less rare than the figure commonly cited in the "Ethnographic Atlas" (1980) which listed only those polyandrous societies found in the Himalayan Mountains. More recent studies have found 53 societies outside the 28 found in the Himalayans which practice polyandry. It is most common in egalitarian societies marked by high male mortality or male absenteeism. It is associated with "partible paternity", the cultural belief that a child can have more than one father.
The explanation for polyandry in the Himalayan Mountains is related to the scarcity of land; the marriage of all brothers in a family to the same wife ("fraternal polyandry") allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this was prevented through the social practice of impartible inheritance (the dis-inheriting of most siblings, some of whom went on to become celibate monks and priests).
Plural marriage.
Group marriage (also known as "multi-lateral marriage") is a form of polyamory in which more than two persons form a family unit, with all the members of the group marriage being considered to be married to all the other members of the group marriage, and all members of the marriage share parental responsibility for any children arising from the marriage. No country legally condones group marriages, neither under the law nor as a common law marriage, but historically it has been practiced by some cultures of Polynesia, Asia, Papua New Guinea and the Americas – as well as in some intentional communities and alternative subcultures such as the Oneida Perfectionists in up-state New York. Of the 250 societies reported by the American anthropologist George P. Murdock in 1949, only the Caingang of Brazil had any group marriages at all.
Child marriage.
A child marriage is a marriage where one or both spouses are under the age of 18. It is related to child betrothal and teenage pregnancy. Child marriage was common throughout history but is today condemned by international human rights organizations. Child marriages are often arranged between the families of the future bride and groom, sometimes as soon as the girl is born. Child marriages can also occur in the context of marriage by abduction.
While child marriage is observed for both boys and girls, the overwhelming majority of child spouses are girls. In many cases, only one marriage-partner is a child, usually the female, due to the importance placed upon female virginity. Causes of child marriage include poverty, bride price, dowry, laws that allow child marriages, religious and social pressures, regional customs, fear of remaining unmarried, and perceived inability of women to work for money.
Today, child marriages are widespread in parts of the world; being most common in South Asia and sub-Saharan Africa, with more than half of the girls in some countries in those regions being married before 18.
The incidence of child marriage has been falling in most parts of the world. In developed countries child marriage is outlawed or restricted.
Girls who marry before 18 are at greater risk of becoming victims of domestic violence, than those who marry later, especially when they are married to a much older man.
Same-sex and third gender marriages.
As noted above, several kinds of same gendered, non-sexual marriages exist in some lineage based societies; this section relates to same gendered sexual unions. However, some cultures include third gender (two-spirited or transgendered) individuals, such as the berdache of the Zuni in New Mexico; is the marriage between a berdache and a man a "same sex marriage"? We'wha, one of the most revered Zuni elders (an Ihamana, spiritual leader) served as an emissary of the Zuni to Washington, where he met President Grover Cleveland. We'wha had a husband who was generally recognized as such.
While it is a relatively new practice to grant same-sex couples the same form of legal marital recognition as commonly granted to mixed-sex couples, there is some history of recorded same-sex unions around the world. Ancient Greek same-sex relationships were like modern companionate marriages, unlike their different-sex marriages in which the spouses had few emotional ties, and the husband had freedom to engage in outside sexual liaisons. The Theodosian Code ("C. Th." 9.7.3) issued in 438 CE imposed severe penalties or death on same-sex relationships but the exact intent of the law and its relation to social practice is unclear, as only a few examples of same-sex marriage in that culture exist. Same-sex unions were celebrated in some regions of China, such as Fujian.
Temporary marriages.
Several cultures have practiced temporary and conditional marriages. Examples include the Celtic practice of handfasting and fixed-term marriages in the Muslim community. Pre-Islamic Arabs practiced a form of temporary marriage that carries on today in the practice of Nikah Mut'ah, a fixed-term marriage contract. The Islamic prophet Muhammad sanctioned a temporary marriage – sigheh in Iran and muta'a in Iraq – which can provide a legitimizing cover for sex workers. The same forms of temporary marriage have been used in Egypt, Lebanon and Iran to make the donation of a human ova legal for in vitro fertilization; a woman cannot, however, use this kind of marriage to obtain a sperm donation. Muslim controversies related to Nikah Mut'ah have resulted in the practice being confined mostly to Shi'ite communities.
The matrilineal Mosuo of China practice what they call "walking marriage".
Cohabitation.
In some jurisdictions cohabitation, in certain circumstances, may constitute a common-law marriage, an unregistered partnership, or otherwise provide the unmarried partners with various rights and responsibilities; and in some countries the laws recognize cohabitation in lieu of institutional marriage for taxation and social security benefits. This is the case, for example, in Australia. Cohabitation may be an option pursued as a form of resistance to traditional institutionalized marriage. However, in this context, some nations reserve the right to define the relationship as marital, or otherwise to regulate the relation, even if the relation has not been registered with the state or a religious institution.
Conversely, institutionalized marriages may not involve cohabitation. In some cases couples living together do not wish to be recognized as married. This may occur because pension or alimony rights are adversely affected; because of taxation considerations; because of immigration issues, or for other reasons. Such marriages have also been increasingly common in Beijing. Guo Jianmei, director of the center for women's studies at Beijing University, told a Newsday correspondent, "Walking marriages reflect sweeping changes in Chinese society." A "walking marriage" refers to a type of temporary marriage formed by the Mosuo of China, in which male partners live elsewhere and make nightly visits. A similar arrangement in Saudi Arabia, called misyar marriage, also involves the husband and wife living separately but meeting regularly.
Partner selection.
There is wide cross-cultural variation in the social rules governing the selection of a partner for marriage. There is variation in the degree to which partner selection is an individual decision by the partners or a collective decision by the partners' kin groups, and there is variation in the rules regulating which partners are valid choices.
The United Nations World Fertility Report of 2003 reports that 89% of all people get married before age forty-nine. The percent of women and men who marry before age forty-nine drops to nearly 50% in some nations and reaches 100% in other nations.
In other cultures with less strict rules governing the groups from which a partner can be chosen the selection of a marriage partner may involve either the couple going through a selection process of courtship or the marriage may be arranged by the couple's parents or an outside party, a matchmaker.
Social status.
Some people want to marry a person with higher or lower status than them. Others want to marry people who have similar status. In many societies women marry men who are of higher social status. There are marriages where each party has sought a partner of similar status. There are other marriages in which the man is older than the woman.
The incest taboo, exogamy and endogamy.
Societies have often placed restrictions on marriage to relatives, though the degree of prohibited relationship varies widely. Marriages between parents and children, or between full siblings, with few exceptions, have been considered incest and forbidden. However, marriages between more distant relatives have been much more common, with one estimate being that 80% of all marriages in history have been between second cousins or closer. This proportion has fallen dramatically, but still more than 10% of all marriages are believed to be between first and second cousins. In the United States, such marriages are now highly stigmatized, and laws ban most or all first-cousin marriage in 30 states. Specifics vary: in South Korea, historically it was illegal to marry someone with the same last name and same ancestral line.
An Avunculate marriage is a marriage that occurs between an uncle and his niece or between an aunt and her nephew. Such marriages are illegal in most countries due to incest restrictions. However a small number of countries have legalized it, including Argentina, Australia, Austria, Malaysia, and Russia.
In various societies the choice of partner is often limited to suitable persons from specific social groups. In some societies the rule is that a partner is selected from an individual's own social group – endogamy, this is often the case in class and caste based societies. But in other societies a partner must be chosen from a different group than one's own – exogamy, this may be the case in societies practicing totemic religion where society is divided into several exogamous totemic clans, such as most Aboriginal Australian societies. In other societies a person is expected to marry their cross-cousin, a woman must marry her father's sister's son and a man must marry his mother's brother's daughter – this is often the case if either a society has a rule of tracing kinship exclusively through patrilineal or matrilineal descent groups as among the Akan people of West Africa. Another kind of marriage selection is the levirate marriage in which widows are obligated to marry their husband's brother, mostly found in societies where kinship is based on endogamous clan groups.
Religion has commonly weighed in on the matter of which relatives, if any, are allowed to marry. Relations may be by consanguinity or affinity, meaning by blood or by marriage. On the marriage of cousins, Catholic policy has evolved from initial acceptance, through a long period of general prohibition, to the contemporary requirement for a dispensation. Islam has always allowed it, while Hindu strictures vary widely.
Prescriptive marriage.
In a wide array of lineage-based societies with a classificatory kinship system, potential spouses are sought from a specific class of relative as determined by a prescriptive marriage rule. This rule may be expressed by anthropologists using a "descriptive" kinship term, such as a "man's mother's brother's daughter" (also known as a "cross-cousin"). Such descriptive rules mask the participant's perspective: a man should marry a woman from his mother's lineage. Within the society's kinship terminology, such relatives are usually indicated by a specific term which sets them apart as potentially marriageable. Pierre Bourdieu notes, however, that very few marriages ever follow the rule, and that when they do so, it is for "practical kinship" reasons such as the preservation of family property, rather than the "official kinship" ideology.
Insofar as regular marriages following prescriptive rules occur, lineages are linked together in fixed relationships; these ties between lineages may form political alliances in kinship dominated societies. French structural anthropologist Claude Lévi-Strauss developed alliance theory to account for the "elementary" kinship structures created by the limited number of prescriptive marriage rules possible.
A pragmatic (or 'arranged') marriage is made easier by formal procedures of family or group politics. A responsible authority sets up or encourages the marriage; they may, indeed, engage a professional matchmaker to find a suitable spouse for an unmarried person. The authority figure could be parents, family, a religious official, or a group consensus. In some cases, the authority figure may choose a match for purposes other than marital harmony.
Forced marriage.
A forced marriage is a marriage in which one or both of the parties is married against their will. Forced marriages continue to be practiced in parts of the world, especially in South Asia and Africa. The line between forced marriage and consensual marriage may become blurred, because the social norms of these cultures dictate that one should never oppose the desire of one's parents/relatives in regard to the choice of a spouse; in such cultures it is not necessary for violence, threats, intimidation etc. to occur, the person simply "consents" to the marriage even if he/she doesn't want it, out of the implied social pressure and duty. The customs of bride price and dowry, that exist in parts of the world, can lead to buying and selling people into marriage.
In some societies, ranging from Central Asia to the Caucasus to Africa, the custom of bride kidnapping still exists, in which a woman is captured by a man and his friends. Sometimes this covers an elopement, but sometimes it depends on sexual violence. In previous times, "raptio" was a larger-scale version of this, with groups of women captured by groups of men, sometimes in war; the most famous example is The Rape of the Sabine Women, which provided the first citizens of Rome with their wives.
Other marriage partners are more or less imposed on an individual. For example, widow inheritance provides a widow with another man from her late husband's brothers.
In rural areas of India, child marriage is practiced, with parents often arranging the wedding, sometimes even before the child is born. This practice was made illegal under the Child Marriage Restraint Act of 1929.
Companionate marriage.
Eva Illouz argues that the 'love' (companionate) marriage emerged at the same time as the rise of capitalism in European and American society. 'Love' is not the raw emotion that Western representations make it out to be, but a cultural construction shaped by the social and economic conditions of modern industrial society. Industrialization weakened the ties between extended families, and made the nuclear family the norm. In this view, love is a culturally constructed label for physiological arousal that is shrouded in cultural symbols that situate the emerging relationship within a particular set of cultural expectations – one of which leads to marriage as an institution. These cultural expectations are shaped by a number of cultural industries, such as advertising, film and television, and the 'wedding industry'. Until the turn of the twentieth century, marriage was viewed as one of the most important financial decisions of one's life, determined in large part by property transfers such as dowry (or dower), and romantic love was viewed as disruptive of the rational economic decision making needed. Under the development of capitalism, this changed:
Romantic love, then, precedes capitalism per se but articulates two leitmotifs that will later resonate with capitalism's central ideological themes. One concerns the sovereignty of the individual vis à vis the group, such sovereignty being affirmed in illicit sexual choices and in the lovers' refusal to conform to the rules of endogamy set by the group. The other concerns the distinction central to bourgeois ideology between interest and sentiments, selfishness and selflessness, embodied respectively in the public and private spheres. In this division, romantic love asserts the privilege of sentiments over social and economic interests, of gratuity over profit, of abundance over the deprivations caused by accumulation. In proclaiming the supremacy of human relationships governed by the disinterested gift of oneself, love not only celebrates the fusion of individual souls and bodies but also opens the possibility of an alternative social order. Love thus projects an aura of transgression and both promises and demands a better world.
Economic considerations.
The financial aspects of marriage vary between cultures and have changed over time.
In some cultures, dowries and bridewealth continue to be required today. In both cases, the financial arrangements are usually made between the groom (or his family) and the bride's family; with the bride often not being involved in the negotiations, and often not having a choice in whether to participate in the marriage.
In Early Modern Britain, the social status of the couple was supposed to be equal. After the marriage, all the property (called "fortune") and expected inheritances of the wife belonged to the husband.
Dowry.
A dowry is "a process whereby parental property is distributed to a daughter at her marriage (i.e. "inter vivos") rather than at the holder's death ("mortis causa")… A dowry establishes some variety of conjugal fund, the nature of which may vary widely. This fund ensures her support (or endowment) in widowhood and eventually goes to provide for her sons and daughters."
In some cultures, especially in South Asia, in countries such as India, Bangladesh, Pakistan, Sri Lanka and Nepal, dowries continue to be expected. In India, nearly 7,000 women were killed in 2001 over dowries, and activists believe that figures represent a third of the actual number of such murders. Dowry related violence is a problem in several places (see dowry deaths), and, in response to violent incidents regarding the practice, several jurisdictions have enacted laws restricting or banning dowry (see Dowry law in India). In Nepal, dowry has been made illegal in 2009. Some authors believe that the giving and receiving of dowry reflects the status and even the effort to climb high in social hierarchy.
Dower.
Direct Dowry contrasts with bridewealth, which is paid by the groom or his family to the bride's parents, and with indirect dowry (or dower), which is property given to the bride herself by the groom at the time of marriage and which remains under her ownership and control.
In the Jewish tradition, the rabbis in ancient times insisted on the marriage couple entering into a prenuptial agreement, called a "ketubah". Besides other things, the "ketubah" provided for an amount to be paid by the husband in the event of a divorce or his estate in the event of his death. This amount was a replacement of the biblical dower or bride price, which was payable at the time of the marriage by the groom to the father of the bride. This innovation was put in place because the biblical bride price created a major social problem: many young prospective husbands could not raise the bride price at the time when they would normally be expected to marry. So, to enable these young men to marry, the rabbis, in effect, delayed the time that the amount would be payable, when they would be more likely to have the sum. It may also be noted that both the dower and the "ketubah" amounts served the same purpose: the protection for the wife should her support cease, either by death or divorce. The only difference between the two systems was the timing of the payment. It is the predecessor to the wife's present-day entitlement to maintenance in the event of the breakup of marriage, and family maintenance in the event of the husband not providing adequately for the wife in his will. Another function performed by the "ketubah" amount was to provide a disincentive for the husband contemplating divorcing his wife: he would need to have the amount to be able to pay to the wife.
Morning gifts, which might also be arranged by the bride's father rather than the bride, are given to the bride herself; the name derives from the Germanic tribal custom of giving them the morning after the wedding night. She might have control of this morning gift during the lifetime of her husband, but is entitled to it when widowed. If the amount of her inheritance is settled by law rather than agreement, it may be called dower. Depending on legal systems and the exact arrangement, she may not be entitled to dispose of it after her death, and may lose the property if she remarries. Morning gifts were preserved for centuries in morganatic marriage, a union where the wife's inferior social status was held to prohibit her children from inheriting a noble's titles or estates. In this case, the morning gift would support the wife and children. Another legal provision for widowhood was jointure, in which property, often land, would be held in joint tenancy, so that it would automatically go to the widow on her husband's death.
Islamic tradition has similar practices. A 'mahr', either immediate or deferred, is the woman's portion of the groom's wealth (divorce) or estate (death). These amounts are usually set on the basis of the groom's own and family wealth and incomes, but in some parts these are set very high so as to provide a disincentive for the groom exercising the divorce, or the husband's family 'inheriting' a large portion of the estate, especially if there are no male offspring from the marriage. In some countries, including Iran, the mahr or alimony can amount to more than a man can ever hope to earn, sometimes up to US$1,000,000 (4000 official Iranian gold coins). If the husband cannot pay the mahr, either in case of a divorce or on demand, according to the current laws in Iran, he will have to pay it by installments. Failure to pay the mahr might even lead to imprisonment.
Bridewealth.
Bridewealth is a common practice in parts of South-East Asia (Thailand, Cambodia), parts of Central Asia, and in much of sub-Saharan Africa. It is also known as brideprice although this has fallen in disfavor as it implies the purchase of the bride. Bridewealth is the amount of money or property or wealth paid by the groom or his family to the parents of a woman upon the marriage of their daughter to the groom. In anthropological literature, bride price has often been explained as payment made to compensate the bride's family for the loss of her labor and fertility. In some cases, bridewealth is a means by which the groom's family's ties to the children of the union are recognized.
Taxation.
In some countries a married person or couple benefits from various taxation advantages not available to a single person. For example, spouses may be allowed to average their combined incomes. This is advantageous to a married couple with disparate incomes. To compensate for this, countries may provide a "higher" tax bracket for the averaged income of a married couple. While income averaging might still benefit a married couple with a stay-at-home spouse, such averaging would cause a married couple with roughly equal personal incomes to pay more total tax than they would as two single persons. In the United States, this is called the marriage penalty.
When the rates applied by the tax code are not based income averaging, but rather on the "sum" of individuals' incomes, higher rates will usually apply to each individual in a two-earner households in a progressive tax systems. This is most often the case with high-income taxpayers and is another situation called a marriage penalty.
Conversely, when progressive tax is levied on the individual with no consideration for the partnership, dual-income couples fare much better than single-income couples with similar household incomes. The effect can be increased when the welfare system treats the same income as a shared income thereby denying welfare access to the non-earning spouse. Such systems apply in Australia and Canada, for example.
Post-marital residence.
In many Western cultures, marriage usually leads to the formation of a new household comprising the married couple, with the married couple living together in the same home, often sharing the same bed, but in some other cultures this is not the tradition. Among the Minangkabau of West Sumatra, residency after marriage is matrilocal, with the husband moving into the household of his wife's mother. Residency after marriage can also be patrilocal or avunculocal. In these cases, married couples may not form an independent household, but remain part of an extended family household.
Early theories explaining the determinants of postmarital residence connected it with the sexual division of labor. However, to date, cross-cultural tests of this hypothesis using worldwide samples have failed to find any significant relationship between these two variables. However, Korotayev's tests show that the female contribution to subsistence does correlate significantly with matrilocal residence in general. However, this correlation is masked by a general polygyny factor.
Although an increase in the female contribution to subsistence tends to lead to matrilocal residence, it also tends simultaneously to lead to general non-sororal polygyny which effectively destroys matrilocality. If this polygyny factor is controlled (e.g., through a multiple regression model), division of labor turns out to be a significant predictor of postmarital residence. Thus, Murdock's hypotheses regarding the relationships between the sexual division of labor and postmarital residence were basically correct, though the actual relationships between those two groups of variables are more complicated than he expected.
There has been a trend toward the neolocal residence in western societies.
Marriage law.
Marriage laws refer to the legal requirements which determine the validity of a marriage, which vary considerably between countries.
Rights and obligations.
A marriage bestows rights and obligations on the married parties, and sometimes on relatives as well, being the sole mechanism for the creation of affinal ties (in-laws). These may include, depending on jurisdiction:
These rights and obligations vary considerably between societies, and between groups within society. These might include arranged marriages, family obligations, the legal establishment of a nuclear family unit, the legal protection of children and public declaration of commitment.
Property regime.
In many countries today, each marriage partner has the choice of keeping his or her property separate or combining properties. In the latter case, called community property, when the marriage ends by divorce each owns half. In lieu of a will or trust, property owned by the deceased generally is inherited by the surviving spouse.
In some legal systems, the partners in a marriage are "jointly liable" for the debts of the marriage. This has a basis in a traditional legal notion called the "Doctrine of Necessities" whereby a husband was responsible to provide necessary things for his wife. Where this is the case, one partner may be sued to collect a debt for which they did not expressly contract. Critics of this practice note that debt collection agencies can abuse this by claiming an unreasonably wide range of debts to be expenses of the marriage. The cost of defense and the burden of proof is then placed on the non-contracting party to prove that the expense is not a debt of the family. The respective maintenance obligations, both during and eventually after a marriage, are regulated in most jurisdictions; alimony is one such method.
Marriage restrictions.
Marriage is an institution that is historically filled with restrictions. From age, to race, to social status, to consanguinity, to gender, restrictions are placed on marriage by society for reasons of benefiting the children, passing on healthy genes, maintaining cultural values, or because of prejudice and fear. Almost all cultures that recognize marriage also recognize adultery as a violation of the terms of marriage.
Age.
Most jurisdictions set a minimum age for marriage, that is, a person must attain a certain age to be legally allowed to marry. This age may depend on circumstances, for instance exceptions from the general rule may be permitted if the parents of a young person express their consent and/or if a court decides that said marriage is in the best interest of the young person (often this applies in cases where a girl is pregnant). Although most age restrictions are in place in order to prevent children from being forced into marriages, especially to much older partners – marriages which can have negative education and health related consequences, and lead to child sexual abuse and other forms of violence – such child marriages remain common in parts of the world. According to the UN, child marriages are most common in rural sub-Saharan Africa and South Asia. The ten countries with the highest rates of child marriage are: Niger (75%), Chad, Central African Republic, Bangladesh, Guinea, Mozambique, Mali, Burkina Faso, South Sudan, and Malawi.
Race.
Laws banning "race-mixing" were enforced in certain North American jurisdictions from 1691 until 1967, in Nazi Germany (The Nuremberg Laws) from 1935 until 1945, and in South Africa during most part of the Apartheid era (1949–1985). All these laws primarily banned marriage between persons of different racially or ethnically defined groups, which was termed "amalgamation" or "miscegenation" in the U.S. The laws in Nazi Germany and many of the U.S. states, as well as South Africa, also banned sexual relations between such individuals.
In the United States, laws in some but not all of the states prohibited the marriage of whites and blacks, and in many states also the intermarriage of whites with Native Americans or Asians. In the U.S., such laws were known as anti-miscegenation laws. From 1913 until 1948, 30 out of the then 48 states enforced such laws. Although an "Anti-Miscegenation Amendment" to the United States Constitution was proposed in 1871, in 1912–1913, and in 1928, no nationwide law against racially mixed marriages was ever enacted. In 1967, the United States Supreme Court unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws are unconstitutional. With this ruling, these laws were no longer in effect in the remaining 16 states that still had them.
The Nazi ban on interracial marriage and interracial sex was enacted in September 1935 as part of the Nuremberg Laws, the "Gesetz zum Schutze des deutschen Blutes und der deutschen Ehre" (The Law for the Protection of German Blood and German Honour). The Nuremberg Laws classified Jews as a race, and forbade marriage and extramarital sexual relations at first with people of Jewish descent, but was later ended to the "Gypsies, Negroes or their bastard offspring" and people of "German or related blood". Such relations was marked as "Rassenschande" (lit. "race-disgrace") and could be punished by imprisonment (usually followed by deportation to a concentration camp) and even by death.
South Africa under apartheid also banned interracial marriage. The Prohibition of Mixed Marriages Act of 1949 prohibited marriage between persons of different races, and the Immorality Act of 1950 made sexual relations with a person of a different race a criminal offence.
Same-sex marriage recognition.
The first laws in modern times recognizing same-sex marriage were enacted during the first decade of the 21st century. As of March 2015, seventeen countries (Argentina, Belgium, Brazil, Canada, Denmark, France, Iceland, Luxembourg, the Netherlands, New Zealand, Norway, Portugal, South Africa, Spain, Sweden, the United Kingdom and Uruguay) and several sub-national jurisdictions (parts of Mexico and a majority of the U.S. states) allow same-sex couples to marry. Finland has enacted a law to legalize same-sex marriage which will come into force in March 2017. Bills allowing legal recognition of same-sex marriage have been proposed, are pending, or have passed at least one legislative house in Austria, Australia, Chile, Germany, Ireland, Slovenia, Switzerland, Taiwan and Venezuela, as well as in the legislatures of several sub-national jurisdictions (parts of Australia, Mexico, and the United States).
Introduction of same-sex marriage laws has varied by jurisdiction, being variously accomplished through a legislative change to marriage laws, a court ruling based on constitutional guarantees of equality, or by direct popular vote (via a ballot initiative or a referendum). The recognition of same-sex marriage is a political, social, civil rights and religious issue in many nations, and debates continue to arise over whether same-sex couples should be allowed marriage, be required to hold a different status (a civil union), or be denied recognition of such rights. Allowing same-gender couples to legally marry is considered to be one of the most important of all LGBT rights.
It is a relatively new practice that same-sex couples are being granted the same form of legal marital recognition available to mixed-sexed couples. In the United States, the 1996 Defense of Marriage Act (DOMA) explicitly defined marriage for the purposes of federal law as between a man and a woman and allowed states to ignore same-sex marriages from other states. Section 3 of the law was struck down in United States v. Windsor (2013) which prevented the Federal Government form recognizing same-sex marriage. Section 2, which allows states to not recognize same-sex marriage in other states is still in effect. As of June 2013, thirty-six US states currently define marriage as between a man and a woman. Three of those states have statutory language that pre-dates DOMA (enacted before 1996) defining marriage as such. Thirty states have defined marriage in their constitutions. Arizona defeated a constitutional amendment defining marriage as only between a man and a woman (2006), but it subsequently passed one in 2008. In 2012, Minnesota defeated a similar amendment, and enacted legislation to legalize same-sex marriages in May 2013.
Number of spouses in a marriage.
Polygyny is widely practiced in mostly Muslim and African countries. In the Middle Eastern region, Israel, Turkey and Tunisia are notable exceptions.
In most other jurisdictions, polygamy is illegal. For example, In the United States, polygamy is illegal in all 50 states.
Over a century ago, citizens of the self-governing territory of what is present-day Utah were forced by the United States federal government to abandon the practice of polygamy through the vigorous enforcement of several Acts of Congress and eventually complied. The Church of Jesus Christ of Latter-day Saints formally abolished the practice in 1890, in a document labeled 'The Manifesto'. Among American Muslims, a small minority of around 50,000 to 100,000 people are estimated to live in families with a husband maintaining an illegal polygamous relationship.
Several countries such as India and Sri Lanka, permit only their Islamic citizens to practice polygny. Some Indians have converted to Islam in order to bypass such legal restrictions. Predominantly Christian nations usually do not allow polygamous unions, with a handful of exceptions being the Republic of the Congo, Uganda, and Zambia. Myanmar (frequently referred to as Burma) is also the only predominately Buddhist nation to allow for civil polygynous marriages, though such is rarely tolerated by the Burmese population.
State recognition.
In various jurisdictions, a civil marriage may take place as part of the religious marriage ceremony, although they are theoretically distinct. Some jurisdictions allow civil marriages in circumstances which are notably not allowed by particular religions, such as same-sex marriages or civil unions.
The opposite case may happen as well. Partners may not have full juridical acting capacity and churches may have less strict limits than the civil jurisdictions. This particularly applies to minimum age, or physical infirmities.
It is possible for two people to be recognised as married by a religious or other institution, but not by the state, and hence without the legal rights and obligations of marriage; or to have a civil marriage deemed invalid and sinful by a religion. Similarly, a couple may remain married in religious eyes after a civil divorce.
Marriage license, civil ceremony and registration.
A marriage is usually formalized at a wedding or marriage ceremony. The ceremony may be officiated either by a religious official, by a government official or by a state approved celebrant. In various European and some Latin American countries, any religious ceremony must be held separately from the required civil ceremony. Some countries – such as Belgium, Bulgaria, France, the Netherlands, Romania and Turkey – require that a civil ceremony take place before any religious one. In some countries – notably the United States, Canada, the United Kingdom, the Republic of Ireland, Norway and Spain – both ceremonies can be held together; the officiant at the religious and civil ceremony also serving as agent of the state to perform the civil ceremony. To avoid any implication that the state is "recognizing" a religious marriage (which is prohibited in some countries) – the "civil" ceremony is said to be taking place at the same time as the religious ceremony. Often this involves simply signing a register during the religious ceremony. If the civil element of the religious ceremony is omitted, the marriage ceremony is not recognized as a marriage by government under the law.
Some countries, such as Australia, permit marriages to be held in private and at any location; others, including England and Wales, require that the civil ceremony be conducted in a place open to the public and specially sanctioned by law for the purpose. In England, the place of marriage formerly had to be a church or register office, but this was extended to any public venue with the necessary licence. An exception can be made in the case of marriage by special emergency license (UK: licence), which is normally granted only when one of the parties is terminally ill. Rules about where and when persons can marry vary from place to place. Some regulations require one of the parties to reside within the jurisdiction of the register office (formerly parish).
Each religious authority has rules for the manner in which marriages are to be conducted by their officials and members. Where religious marriages are recognised by the state, the officiator must also conform with the law of the jurisdiction.
Common-law marriage.
In a small number of jurisdictions marriage relationships may be created by the operation of the law alone. Unlike the typical ceremonial marriage with legal contract, wedding ceremony, and other details, a common-law marriage may be called "marriage by habit and repute (cohabitation)." A de facto common-law marriage without a license or ceremony is legally binding in some jurisdictions but has no legal consequence in others.
Civil unions.
A "civil union", also referred to as a "civil partnership", is a legally recognized form of partnership similar to marriage. Beginning with Denmark in 1989, civil unions under one name or another have been established by law in several countries in order to provide same-sex couples rights, benefits, and responsibilities similar (in some countries, identical) to opposite-sex civil marriage. In some jurisdictions, such as Brazil, New Zealand, Uruguay, Ecuador, France and the U.S. states of Hawaii and Illinois, civil unions are also open to opposite-sex couples.
"Marriage of convenience".
Sometimes people marry to take advantage of a certain situation, sometimes called a marriage of convenience or a sham marriage. For example, according to one publisher of information about "green card" marriages, "Every year over 450,000 United States citizens marry foreign-born individuals and petition for them to obtain a permanent residency (Green Card) in the United States." While this is likely an overestimate, in 2003 alone 184,741 immigrants were admitted to the U.S. as spouses of U.S. citizens. More were admitted as fiancés of US citizens for the purpose of being married within 90 days. Regardless of the number of people entering the US to marry a US citizen, it does not indicate the number of these marriages that are convenience marriages, which number could include some of those with the motive of obtaining permanent residency, but also include people who are US citizens. One example would be to obtain an inheritance that has a marriage clause. Another example would be to save money on health insurance or to enter a health plan with preexisting conditions offered by the new spouse's employer. Other situations exist, and, in fact, all marriages have a complex combination of conveniences motivating the parties to marry. A marriage of convenience is one that is devoid of normal reasons to marry. In certain countries like Singapore sham marriages like these are punishable criminal offences.
Contemporary legal and human rights criticisms of marriage.
People have proposed arguments against marriage for reasons that include political, philosophical and religious criticisms; concerns about the divorce rate; individual liberty and gender equality; questioning the necessity of having a personal relationship sanctioned by government or religious authorities; or the promotion of celibacy for religious or philosophical reasons.
Power and gender roles.
Feminist theory approaches opposite-sex marriage as an institution traditionally rooted in patriarchy that promotes male superiority and power over women. This power dynamic conceptualizes men as "the provider operating in the public sphere" and women as "the caregivers operating within the private sphere". "Theoretically, women ... [were] defined as the property of their husbands ... The adultery of a woman was always treated with more severity than that of a man." "[F]eminist demands for a wife's control over her own property were not met [in parts of Britain] until ... [laws were passed in the late 19th century]."
Traditional marriage imposed an obligation of the wife to be sexually available for her husband and an obligation of the husband to provide material/financial support for the wife. Numerous philosophers, feminists and other academic figures have commented on this throughout history, condemning the hypocrisy of legal and religious authorities in regard to sexual issues; pointing to the lack of choice of a woman in regard to controlling her own sexuality; and drawing parallels between marriage, an institution promoted as sacred, and prostitution, widely condemned and vilified (though often tolerated as a "necessary evil"). Mary Wollstonecraft, in the 18th century, described marriage as "legal prostitution". Emma Goldman wrote in 1910: "To the moralist prostitution does not consist so much in the fact that the woman sells her body, but rather that she sells it out of wedlock". Bertrand Russell in his book Marriage and Morals wrote that:"Marriage is for woman the commonest mode of livelihood, and the total amount of undesired sex endured by women is probably greater in marriage than in prostitution." Angela Carter in Nights at the Circus wrote: "What is marriage but prostitution to one man instead of many?"
Some critics object to what they see as propaganda in relation to marriage – from the government, religious organizations, the media – which aggressively promote marriage as a solution for all social problems; such propaganda includes, for instance, marriage promotion in schools, where children, especially girls, are bombarded with positive information about marriage, being presented only with the information prepared by authorities.
The performance of dominant gender roles by men and submissive gender roles by women influence the power dynamic of a marriage. In some American households, women internalize gender role stereotypes and often assimilate into the role of "wife", "mother", and "caretaker" in conformity to societal norms and their male partner. Author bell hooks states "within the family structure, individuals learn to accept sexist oppression as 'natural' and are primed to support other forms of oppression, including heterosexist domination." "[T]he cultural, economic, political and legal supremacy of the husband" was "[t]raditional ... under English law". This patriarchal dynamic is contrasted with a conception of egalitarian or Peer Marriage in which power and labour are divided equally, and not according to gender roles.
In the US, studies have shown that, despite egalitarian ideals being common, less than half of respondents viewed their opposite-sex relationships as equal in power, with unequal relationships being more commonly dominated by the male partner. Studies also show that married couples find the highest level of satisfaction in egalitarian relationships and lowest levels of satisfaction in wife dominate relationships. In recent years, egalitarian or Peer Marriages have been receiving increasing focus and attention politically, economically and culturally in a number of countries, including the United States.
Sex outside of marriage.
Different societies demonstrate variable tolerance of extramarital sex. The Standard Cross-Cultural Sample describes the occurrence of extramarital sex by gender in over 50 pre-industrial cultures. The occurrence of extramarital sex by men is described as "universal" in 6 cultures, "moderate" in 29 cultures, "occasional" in 6 cultures, and "uncommon" in 10 cultures. The occurrence of extramarital sex by women is described as "universal" in 6 cultures, "moderate" in 23 cultures, "occasional" in 9 cultures, and "uncommon" in 15 cultures. Three studies using nationally representative samples in the United States found that between 10–15% of women and 20–25% of men engage in extramarital sex.
Many of the world's major religions look with disfavor on sexual relations outside marriage. There are non-secular states that sanction criminal penalties for sexual intercourse before marriage. Sexual relations by a married person with someone other than his/her spouse is known as adultery. Adultery is considered in many jurisdictions to be a crime and grounds for divorce.
In some countries, such as Saudi Arabia, Pakistan, Afghanistan, Iran, Kuwait, Maldives, Morocco, Oman, Mauritania, United Arab Emirates, Sudan, Yemen, any form of sexual activity outside marriage is illegal.
In some parts of the world, women and girls accused of having sexual relations outside marriage are at risk of becoming victims of honor killings committed by their families. In 2011 several people were sentenced to death by stoning after being accused of adultery in Iran, Somalia, Afghanistan, Sudan, Mali and Pakistan. Practices such as honor killings and stoning continue to be supported by mainstream politicians and other officials in some countries. In Pakistan, after the 2008 Balochistan honour killings in which five women were killed by tribesmen of the Umrani Tribe of Balochistan, Pakistani Federal Minister for Postal Services Israr Ullah Zehri defended the practice; he said: ""These are centuries-old traditions, and I will continue to defend them. Only those who indulge in immoral acts should be afraid"."
Marriage and sexual violence.
An issue that is a serious concern regarding marriage and which has been the object of international scrutiny is that of sexual violence within marriage. Throughout much of the history, in most cultures, sex in marriage was considered a 'right', that could be taken by force (often by a man from a women), if 'denied'. As the concept of human rights started to develop in the 20th century, and with the arrival of second wave feminism, such views have become less widely held.
The legal and social concept of marital rape has developed in most industrialized countries in the mid to late 20th century; in many other parts of the world it is not recognized as a form of abuse, socially or legally. Several countries in Eastern Europe and Scandinavia made marital rape illegal before 1970, and other countries in Western Europe and the English-speaking Western World outlawed it in the 1980s and 1990s. In England and Wales, marital rape was made illegal in 1991. Although marital rape is being increasingly criminalized in developing countries too, cultural, religious, and traditional ideologies about "conjugal rights" remain very strong in many parts of the world; and even in many countries that have adequate laws against rape in marriage these laws are rarely enforced.
Apart from the issue of rape committed against one's spouse, marriage is, in many parts of the world, closely connected with other forms of sexual violence: in some places, like Morocco, unmarried girls and women who are raped are often forced by their families to marry their rapist. Because being the victim of rape and losing virginity carry extreme social stigma, and the victims are deemed to have their "reputation" tarnished, a marriage with the rapist is arranged. This is claimed to be in the advantage of both the victim – who does not remain unmarried and doesn't lose social status – and of the rapist, who avoids punishment. In 2012, after a Moroccan 16-year-old girl committed suicide after having been forced by her family to marry her rapist and enduring further abuse by the rapist after they married, there have been protests from activists against this practice which is common in Morocco.
In some societies, the very high social and religious importance of marital fidelity, especially female fidelity, has as result the criminalization of adultery, often with harsh penalties such as stoning or flogging; as well as leniency towards punishment of violence related to infidelity (such as honor killings). In the 21st century, criminal laws against adultery have become controversial with international organizations calling for their abolition. Opponents of adultery laws argue that these laws are a major contributor to discrimination and violence against women, as they are enforced selectively mostly against women; that they prevent women from reporting sexual violence; and that they maintain social norms which justify violent crimes committed against women by husbands, families and communities. A Joint Statement by the United Nations Working Group on discrimination against women in law and in practice states that "Adultery as a criminal offence violates women's human rights". Some human rights organizations argue that the criminalization of adultery also violates internationally recognized protections for private life, as it represents an arbitrary interference with an individual's privacy, which is not permitted under international law.
Marriage laws, human rights and the global status of women.
The laws surrounding marriage in many countries have come under international scrutiny because they contradict international standards of human rights; institutionalize violence against women, child marriage and forced marriage; require the permission of a husband for his wife to work in a paid job, sign legal documents, file criminal charges against someone, sue in civil court etc.; sanction the use by husbands of violence to "discipline" their wives; and discriminate against women in divorce.
Such things were legal even in many Western countries until recently: for instance, in France, married women obtained the right to work without their husband's permission in 1965, and in West Germany women obtained this right in 1977 (by comparison women in East Germany had much more rights). In Spain, during Franco's era, a married woman needed her husband's consent, referred to as the "permiso marital", for almost all economic activities, including employment, ownership of property, and even traveling away from home; the "permiso marital" was abolished in 1975.
An absolute submission of a wife to her husband is accepted as natural in many parts of the world, for instance surveys by UNICEF have shown that the percentage of women aged 15–49 who think that a husband is justified in hitting or beating his wife under certain circumstances is as high as 90% in Afghanistan and Jordan, 87% in Mali, 86% in Guinea and Timor-Leste, 81% in Laos, 80% in Central African Republic. Detailed results from Afghanistan show that 78.4% of women agree with a beating if the wife "goes out
without telling him [the husband]" and 76.2% agree "if she argues with him".
Throughout history, and still today in many countries, laws have provided for mitigating circumstances, partial or complete defenses, for men who killed their wifes due to adultery, with such acts often being seen as crimes of passion and being covered by legal defenses such as provocation or defense of family honor.
Dowry and bridewealth.
In recent years, the customs of dowry and bride price have received international criticism for inciting conflicts between families and clans; contributing to violence against women; promoting materialism; increasing property crimes (where men steal goods such as cattle in order to be able to pay the bride price); and making it difficult for poor people to marry. African women's rights campaigners advocate the abolishing of bride price, which they argue is based on the idea that women are a form of property which can be bought. Bride price has also been criticized for contributing to child trafficking as impoverished parents sell their young daughters to rich older men. A senior Papua New Guinea police officer has called for the abolishing of bride price arguing that it is one of the main reasons for the mistreatment of women in that country. The opposite practice of dowry has been linked to a high level of violence (see dowry deaths) and to crimes such as extortion.
Children born outside marriage.
Historically, and still in many countries, children born outside marriage suffered severe social stigma and discrimination. In England and Wales, such children were known as bastards and whoresons.
There are significant differences between world regions in regard to the social and legal position of non-marital births, ranging from being fully accepted and uncontroversial to being severely stigmatized and discriminated.
The 1975 European Convention on the Legal Status of Children Born out of Wedlock protects the rights of children born to unmarried parents. The convention states, among others, that,: ""The father and mother of a child born out of wedlock shall have the same obligation to maintain the child as if it were born in wedlock" and that "A child born out of wedlock shall have the same right of succession in the estate of its father and its mother and of a member of its father's or mother's family, as if it had been born in wedlock"."
While in most Western countries legal inequalities between children born inside and outside marriage have largely been abolished, this is not the case in some parts of the world.
The legal status of an unmarried father differs greatly from country to country.
Without voluntary formal recognition of the child by the father, in most cases there is a need of due process of law in order to establish paternity. In some countries however, unmarried cohabitation of a couple for a specific period of time does create a presumption of paternity similar to that of formal marriage. This is the case in Australia. Under what circumstances can a paternity action be initiated, the rights and responsibilities of a father once paternity has been established (whether he can obtain parental responsibility and weather he can be forced to support the child) as well as the legal position of a father who voluntarily acknowledges the child, vary widely by jurisdiction. A special situation arises when a married woman has a child by a man other than her husband. Some countries, such as Israel, refuse to accept a legal challenge of paternity in such a circumstance, in order to avoid the stigmatization of the child (see Mamzer, a concept under Jewish law). In 2010, the European Court of Human Rights ruled in favor of a German man who had fathered twins with a married woman, granting him right of contact with the twins, despite the fact that the mother and her husband had forbidden him to see the children.
The steps that an unmarried father must take in order to obtain rights to his child vary by country. In some countries (such as the UK – since 2003 in England and Wales, 2006 in Scotland, and 2002 in Northern Ireland) it is sufficient for the father to be listed on the birth certificate for him to have parental rights; in other countries, such as Ireland, simply being listed on the birth certificate does not offer any rights, additional legal steps must be taken (if the mother agrees, the parents can both sign a "statutory declaration", but if the mother does not agree, the father has to apply to court).
Children born outside marriage have become more common, and in some countries, the majority. Recent data from Latin America showed figures for non-marital childbearing to be 74% for Colombia, 69% for Peru, 68% for Chile, 66% for Brazil, 58% for Argentina, 55% for Mexico. In 2012, in the European Union, 40% of births were outside marriage, and in the United States, in 2013, the figure was similar, at 40.6%.
In the United Kingdom 47.6% of births were to unmarried women in 2012; in Ireland the figure was 35.1%.
Some married couples choose not to have children. Others are unable to have children because of infertility or other factors preventing conception or the bearing of children. In some cultures, marriage imposes an "obligation" on women to bear children. In northern Ghana, for example, payment of bridewealth signifies a woman's requirement to bear children, and women using birth control face substantial threats of physical abuse and reprisals.
Marriage and religion.
Among the precepts of mainstream religions are found, as a rule, unequivocal prescriptions for marriage, establishing both rituals and rules of conduct.
Abrahamic religions.
Bahá'í.
In the Bahá'í Faith marriage is encouraged and viewed as a mutually strengthening bond, but is not obligatory. A Bahá'í marriage requires the couple to choose each other, and then the consent of all living parents.
Christianity.
Christian marriages are based upon the teachings of Jesus Christ and the Apostle Paul. Today many Christian denominations regard marriage as a sacrament, a sacred institution, or a covenant, but this wasn't the case before marriage was officially recognized as a sacrament at the 1184 Council of Verona. Before then, no specific ritual was prescribed for celebrating a marriage: "Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime."
In the decrees on marriage of the Council of Trent (twenty-fourth session from 1563) the validity of marriage was made dependent upon the wedding taking place before a priest and two witnesses, although the lack of a requirement for parental consent ended a debate that had proceeded from the 12th century. In the case of a divorce, the right of the innocent party to marry again was denied so long as the other party was alive, even if the other party had committed adultery.
The Christian Church performed marriages in the vestibule prior to the 16th century, when the emphasis was on the marriage contract and betrothal. Then the ceremony was moved inside the sacristy.
Christians often marry for religious reasons ranging from following the biblical injunction for a "man to leave his father and mother and cleave to his wife, and the two shall become one," to obeying Canon Law stating marriage between baptized persons is a sacrament.
Divorce and remarriage while generally not encouraged are regarded differently by each Christian denomination. Most Protestant Churches allow people to marry again after a divorce. The Eastern Orthodox Church allows divorce for a limited number of reasons, and in theory, but usually not in practice, requires that a marriage after divorce be celebrated with a penitential overtone. In the Roman Catholic Church, marriage can be ended by an annulment where the Church for special reasons regards it as never having taken place.
 "'Then the Lord God made a woman from the rib he had taken out of the man, and he brought her to the man. The man said, "This is now bone of my bones and flesh of my flesh; she shall be called 'woman, ' for she was taken out of man." For this reason a man will leave his father and mother and be united to his wife, and they will become one flesh.
 "'...So they are no longer two, but one. Therefore what God has joined together, let man not separate."
 — Jesus
Catholics, Eastern Orthodox Christians and many Anglicans consider marriage termed "holy matrimony" to be an expression of divine grace, termed a "sacrament" or "mystery". In Western ritual, the ministers of the sacrament are the husband and wife themselves, with a bishop, priest, or deacon merely witnessing the union on behalf of the church, and adding a blessing. In Eastern ritual churches, the bishop or priest functions as the actual minister of the Sacred Mystery (Eastern Orthodox deacons may not perform marriages). Western Christians commonly refer to marriage as a vocation, while Eastern Christians consider it an ordination and a martyrdom, though the theological emphases indicated by the various names are not excluded by the teachings of either tradition. Marriage is commonly celebrated in the context of a Eucharistic service (a nuptial Mass or Divine Liturgy). The sacrament of marriage is indicative of the relationship between Christ and the Church.
The Roman Catholic tradition of the 12th and 13th centuries defined marriage as a sacrament ordained by God, signifying the mystical marriage of Christ to his Church.
The matrimonial covenant, by which a man and a woman establish between themselves a partnership of the whole of life, is by its nature ordered toward the good of the spouses and the procreation and education of offspring; this covenant between baptized persons has been raised by Christ the Lord to the dignity of a sacrament.
The mutual love between man and wife becomes an image of the eternal love with which God loves humankind. The celebration of marriage between two Catholics normally takes place during the public liturgical celebration of the Holy Mass, because of its sacramental connection with the unity of the Paschal mystery of Christ (Communion). Sacramental marriage confers a perpetual and exclusive bond between the spouses. By its nature, the institution of marriage and conjugal love is ordered to the procreation and upbringing of offspring. Marriage creates rights and duties in the Church between the spouses and towards their children: "[e]ntering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment."
According to current Catholic legislation governing marriage, the essential properties of marriage are unity and indissolubility; in Christian marriage they acquire a distinctive firmness by reason of the sacrament. Divorce is not recognized, but annulments predicated upon previously existing impediments may be granted. Offspring resulting from annulled relationships are considered legitimate. Remarried persons divorced from a living, lawful spouse are not separated from the Church, but they cannot receive Eucharistic communion.
For Protestant denominations, the purposes of marriage include intimate companionship, rearing children and mutual support for both husband and wife to fulfill their life callings. Protestants are generally not opposed to the use of birth control and consider marital sexual pleasure to be a gift of God.
Most Reformed Christians would deny the elevation of marriage to the status of a sacrament, nevertheless it is considered a covenant between spouses before God.cf.
Since the sixteenth century, five competing models of marriage in have shaped Protestant marriage and legal tradition:
Once the process of marriage was secularized into a private contract, the role of churches substantially diminished for Protestants. This culminated in the second half of the 20th century with nearly all Protestants permitting divorce and remarriage.
Members of The Church of Jesus Christ of Latter-day Saints (LDS Church) believe that "marriage between a man and a woman is ordained of God and that the family is central to the Creator's plan for the eternal destiny of His children." The LDS believe that marriage between a man and a woman can last beyond death and into eternity. Marriages between LDS couples are normally solemnized in LDS temples, though they regard civil marriages just as valid as temple marriages.
Christian attitudes to same-sex marriage.
While most Christian denominations do not currently perform same-sex marriages, some do, such as Unitarian Universalist, Metropolitan Community Church, Quaker, United Church of Canada, and United Church of Christ congregations, and some Anglican dioceses, for example. Same-sex marriage is recognized by various religious denominations.
Islam.
Islam also commends marriage, with the age of marriage being whenever the individuals feel ready, financially and emotionally.
In Islam, polygyny is allowed while polyandry is not, with the specific limitation that a man can have no more than four legal wives at any one time and an unlimited number of female slaves as concubines, with the requirement that the man is able and willing to partition his time and wealth equally among the respective wives.
For a Muslim wedding to take place, the bridegroom and the guardian of the bride ("wali") must both agree on the marriage. Should the guardian disagree on the marriage, it may not legally take place. If the "wali" of the girl her father or paternal grandfather, he has the right to force her into marriage even against her proclaimed will, if it is her first marriage. A guardian who is allowed to force the bride into marriage is called "wali mujbir".
From an Islamic (Sharia) law perspective, the minimum requirements and responsibilities in a Muslim marriage are that the groom provide living expenses (housing, clothing, food, maintenance) to the bride, and in return, the bride's main responsibility is raising children to be proper Muslims. All other rights and responsibilities are to be decided between the husband and wife, and may even be included as stipulations in the marriage contract before the marriage actually takes place, so long as they do not go against the minimum requirements of the marriage.
In Sunni Islam and Ahmadiyya Islam, marriage must take place in the presence of at least two reliable witnesses, with the consent of the guardian of the bride and the consent of the groom. Following the marriage, the couple may consummate the marriage. To create an 'urf marriage, it is sufficient that a man and a woman indicate an intention to marry each other and recite the requisite words in front of a suitable Muslim. The wedding party usually follows but can be held days, or months later, whenever the couple and their families want to, however there can be no concealment of the marriage as it is regarded as public notification due to the requirement of witnesses.
In Shia Islam, marriage may take place without the presence of witnesses as is often the case in temporary mut'ah marriage (prohibited in Sunni Islam), but with the consent of both the bride and the groom. Following the marriage they may consummate their marriage.
Judaism.
In Judaism, marriage is based on the laws of the Torah and is a contractual bond between a man and a woman in which the woman dedicates herself to be the exclusive woman of a single man. This contract is called Kiddushin. Though procreation is not the sole purpose, a Jewish marriage is also expected to fulfill the commandment to have children. The main focus centers around the relationship between the husband and wife. Kabbalistically, marriage is understood to mean that the husband and wife are merging into a single soul. This is why a man is considered "incomplete" if he is not married, as his soul is only one part of a larger whole that remains to be unified.
The Hebrew Bible (Christian Old Testament) describes a number of marriages, including those of Isaac (), Jacob() and Samson (). Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Hebrew Bible. Today Ashkenazi Jews are prohibited to take more than one wife because of a ban instituted on this by Gershom ben Judah (Died 1040).
Among ancient Hebrews, marriage was a domestic affair and not a religious ceremony; the participation of a priest or rabbi was not required.
Betrothal ("erusin"), which refers to the time that this binding contract is made, is distinct from marriage itself ("nissu'in"), with the time between these events varying substantially.
In biblical times, a wife was regarded as chattel, belonging to her husband; the descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and men with more than one wife were expected to ensure that they continue to give the first wife food, clothing, and marital rights.
Since a wife was regarded as property, her husband was originally free to divorce her for any reason, at any time. Divorcing a woman against her will was also banned by Gershom ben Judah. A divorced couple were permitted to get back together, unless the wife had married someone else after her divorce.
Hinduism.
Hinduism sees marriage as a sacred duty that entails both religious and social obligations. Old Hindu literature in Sanskrit gives many different types of marriages and their categorization ranging from "Gandharva Vivaha" (instant marriage by mutual consent of participants only, without any need for even a single third person as witness) to normal (present day) marriages, to "Rakshasa Vivaha" ("demoniac" marriage, performed by abduction of one participant by the other participant, usually, but not always, with the help of other persons). In India and generally in South Asia, arranged marriages, the spouse's parents or an older family member choose the partner, are still predominant in comparison with so called love marriages until nowadays. The Hindu Widow's Remarriage Act 1856 empowers a Hindu widow to remarry.
Sati, the practice of a widow immolating herself on her husband's funeral pyre, was officially outlawed by India's British rulers in 1829. The last sati incident allegedly ouccured in Rajasthan in 1987 when 18-year-old Roop Kanwar allegedly committed sati. A court order ruled in 2004 that no such incident had occurred and acquitted all accused.
Buddhism.
The Buddhist view of marriage considers marriage a secular affair and thus not a sacrament. Buddhists are expected to follow the civil laws regarding marriage laid out by their respective governments. Gautama Buddha, being a kshatriya was required by Shakyan tradition to pass a series of tests to prove himself as a warrior, before he was allowed to marry.
Sikhism.
In a Sikh marriage, the couple walks around the "Guru Granth Sahib" holy book four times, and a holy man recites from it in the kirtan style. The ceremony is known as 'Anand Karaj' and represents the holy union of two souls united as one.
Wicca.
Wiccan marriages are commonly known as handfastings. Although handfastings vary for each Wiccan they often involve honoring Wiccan gods. Sex is considered a pious and sacred activity.
Marriage and health.
Marriage, like other close relationships, exerts considerable influence on health. Married people experience lower morbidity and mortality across such diverse health threats as cancer, heart attacks, and surgery. Research on marriage and health is part of the broader study of the benefits of social relationships. Social ties provide people with a sense of identity, purpose, belonging and support. Simply being married, as well as the quality of one's marriage, has been linked to diverse measures of health.
The health-protective effect of marriage is stronger for men than women. Marital status — the simple fact of being married — confers more health benefits to men than women. Women's health is more strongly impacted than men's by marital conflict or satisfaction, such that unhappily married women do not enjoy better health relative to their single counterparts. Most research on marriage and health has focused on heterosexual couples, and more work is needed to clarify the health impacts of same-sex marriage.
Divorce and annulment.
In most societies, the death of one of the partners terminates the marriage, and in monogamous societies this allows the other partner to remarry, though sometimes after a waiting or mourning period.
In some societies, a marriages can be annulled, when an authority declares that a marriage never happened. Jurisdictions often have provisions for void marriages or voidable marriages.
A marriage may also be terminated through divorce. Countries that have relatively recently legalized divorce are Italy (1970), Portugal (1975), Brazil (1977), Spain (1981), Argentina (1987), Paraguay (1991), Colombia (1991), Ireland (1996), Chile (2004) and Malta (2011). As of 2012, the Philippines and the Vatican City are the only jurisdictions which do not allow divorce (this is currently under discussion in Philippines).)
After divorce, one spouse may have to pay alimony. Laws concerning divorce and the ease with which a divorce can be obtained vary widely around the world. After a divorce or an annulment, the people concerned are free to remarry (or marry).
A statutory right of two married partners to mutually consent to divorce was enacted in western nations in the mid-20th century. In the United States no-fault divorce was first enacted in California in 1969 and the final state to legalize it was New York in 1989.
About 45% of marriages in Britain and, according to a 2009 study, 46% of marriages in the U.S. end in divorce.
History of marriage.
Ancient world.
Many cultures have legends concerning the origins of marriage. The way in which a marriage is conducted and its rules and ramifications has changed over time, as has the institution itself, depending on the culture or demographic of the time.
A wife was seen as being of high value, and was therefore, usually, carefully looked after. Early nomadic communities in the middle east practised a form of marriage known as "beena", in which a wife would own a tent of her own, within which she retains complete independence from her husband; this principle appears to survive in parts of early Israelite society, as some early passages of the Bible appear to portray certain wives as each owning a tent as a personal possession (specifically, Jael, Sarah, and Jacob's wives).
The last chapter of Book of Proverbs show the characteristics of a virtuous woman.
The husband too, is indirectly implied to have some responsibilities to his wife. The Covenant Code orders "If he take him another; her food, her clothing, and her duty of marriage, shall he not diminish(or lessen)". If the husband does not provide the first wife with these things, she is to be divorced, without cost to her. The Talmud interprets this as a requirement for a man to provide food and clothing to, and have sex with, each of his wives. However, "duty of marriage" is also interpreted as whatever one does as a married couple, which is more than just sexual activity. And the term diminish, which means to lessen, shows the man must treat her as if he was not married to another.
As a polygynous society, the Israelites did not have any laws which imposed marital fidelity on men. However, the prophet Malachi states that none should be faithless to the wife of his youth and that God hates divorce. Adulterous married women,adulterous betrothed women, and the men who slept with them however, were subject to the death penalty by the biblical laws against adultery According to the Priestly Code of the Book of Numbers, if a pregnant woman was suspected of adultery, she was to be subjected to the Ordeal of Bitter Water, a form of trial by ordeal, but one that took a miracle to convict. The literary prophets indicate that adultery was a frequent occurrence, despite their strong protests against it, and these legal strictnesses.
In Ancient Greece, no specific civil ceremony was required for the creation of a marriage – only mutual agreement and the fact that the couple must regard each other as husband and wife accordingly. Men usually married when they were in their 20s and women in their teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a young girl ensured ample time for her to bear children, as life expectancies were significantly lower during this period. Married Greek women had few rights in ancient Greek society and were expected to take care of the house and children. Time was an important factor in Greek marriage. For example, there were superstitions that being married during a full moon was good luck and, according to Robert Flacelière, Greeks married in the winter. Inheritance was more important than feelings: a woman whose father dies without male heirs could be forced to marry her nearest male relative – even if she had to divorce her husband first.
There were several types of marriages in ancient Roman society. The traditional ("conventional") form called "conventio in manum" required a ceremony with witnesses and was also dissolved with a ceremony. In this type of marriage, a woman lost her family rights of inheritance of her old family and gained them with her new one. She now was subject to the authority of her husband. There was the free marriage known as "sine manu". In this arrangement, the wife remained a member of her original family; she stayed under the authority of her father, kept her family rights of inheritance with her old family and did not gain any with the new family. The minimum age of marriage for girls was 12.
Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, according to Tacitus:
The youths partake late of the pleasures of love, and hence pass the age of puberty unexhausted: nor are the virgins hurried into marriage; the same maturity, the same full growth is required: the sexes unite equally matched and robust; and the children inherit the vigor of their parents.
Where Aristotle had set the prime of life at 37 years for men and 18 for women, the Visigothic Code of law in the 7th century placed the prime of life at twenty years for both men and women, after which both presumably married. Tacitus states that ancient Germanic brides were on average about 20 and were roughly the same age as their husbands.
Europe.
From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of Antioch writing around 110 to bishop Polycarp of Smyrna exhorts, "[I]t becomes both men and women who marry, to form their union with the approval of the bishop, that their marriage may be according to God, and not after their own lust."
In the 12th century, women were obligated to take the name of their husbands and starting in the second half of the 16th century parental consent along with the church's consent was required for marriage.
With few local exceptions, until 1545, Christian marriages in Europe were by mutual consent, declaration of intention to marry and upon the subsequent physical union of the parties. The couple would promise verbally to each other that they would be married to each other; the presence of a priest or witnesses was not required. This promise was known as the "verbum." If freely given and made in the present tense (e.g., "I marry you"), it was unquestionably binding; if made in the future tense ("I will marry you"), it would constitute a betrothal.
In 1552 a wedding took place in Zufia, Navarre, between Diego de Zufia and Mari-Miguel following the custom as it was in the realm since the Middle Ages, but the man denounced the marriage on the grounds that its validity was conditioned to "riding" her ("si te cabalgo, lo cual dixo de bascuence (...) balvin yo baneça aren senar içateko"). The tribunal of the kingdom rejected the husband's claim, validating the wedding, but the husband appealed to the tribunal in Zaragoza, and this institution annulled the marriage. According to the Charter of Navarre, the basic union consisted of a civil marriage with no priest required and at least two witnesses, and the contract could be broken using the same formula. The Church in turn lashed out at those who got married twice or thrice in a row while their formers spouses were still alive. In 1563 the Council of Trent, twenty-fourth session, required that a valid marriage must be performed by a priest before two witnesses.
One of the functions of churches from the Middle Ages was to register marriages, which was not obligatory. There was no state involvement in marriage and personal status, with these issues being adjudicated in ecclesiastical courts. During the Middle Ages marriages were arranged, sometimes as early as birth, and these early pledges to marry were often used to ensure treaties between different royal families, nobles, and heirs of fiefdoms. The church resisted these imposed unions, and increased the number of causes for nullification of these arrangements. As Christianity spread during the Roman period and the Middle Ages, the idea of free choice in selecting marriage partners increased and spread with it.
In Medieval Western Europe, later marriage and higher rates of definitive celibacy (the so-called "European marriage pattern") helped to constrain patriarchy at its most extreme level. For example, Medieval England saw marriage age as variable depending on economic circumstances, with couples delaying marriage until the early twenties when times were bad and falling to the late teens after the Black Death, when there were labor shortages; by appearances, marriage of adolescents was not the norm in England. Where the strong influence of classical Celtic and Germanic cultures (which were not rigidly patriarchal) helped to offset the Judaeo-Roman patriarchal influence, in Eastern Europe the tradition of early and universal marriage (often in early adolescence) as well as traditional Slavic patrilocal custom led to a greatly inferior status of women at all levels of society.
The average age of marriage for most of Northwestern Europe from 1500 to 1800 was around 25 years of age; as the Church dictated that both parties had to be at least 21 years of age to marry without the consent of their parents, the bride and groom were roughly the same age, with most brides in their early twenties and most grooms two or three years older, and a substantial number of women married for the first time in their thirties and forties, particularly in urban areas, with the average age at first marriage rising and falling as circumstances dictated. In better times, more people could afford to marry earlier and thus fertility rose and conversely marriages were delayed or foregone when times were bad, thus restricting family size; after the Black Death, the greater availability of profitable jobs allowed more people to marry young and have more children, but the stabilization of the population in the 16th century meant less job opportunities and thus more people delaying marriages.
As part of the Protestant Reformation, the role of recording marriages and setting the rules for marriage passed to the state, reflecting Martin Luther's view that marriage was a "worldly thing". By the 17th century, many of the Protestant European countries had a state involvement in marriage.
In England, under the Anglican Church, marriage by consent and cohabitation was valid until the passage of Lord Hardwicke's Act in 1753. This act instituted certain requirements for marriage, including the performance of a religious ceremony observed by witnesses.
As part of the Counter-Reformation, in 1563 the Council of Trent decreed that a Roman Catholic marriage would be recognized only if the marriage ceremony was officiated by a priest with two witnesses. The Council also authorized a Catechism, issued in 1566, which defined marriage as, "The conjugal union of man and woman, contracted between two qualified persons, which obliges them to live together throughout life."
In the early modern period, John Calvin and his Protestant colleagues reformulated Christian marriage by enacting the Marriage Ordinance of Geneva, which imposed "The dual requirements of state registration and church consecration to constitute marriage" for recognition.
In England and Wales, Lord Hardwicke's Marriage Act 1753 required a formal ceremony of marriage, thereby curtailing the practice of Fleet Marriage, an irregular or a clandestine marriage. These were clandestine or irregular marriages performed at Fleet Prison, and at hundreds of other places. From the 1690s until the Marriage Act of 1753 as many as 300,000 clandestine marriages were performed at Fleet Prison alone. The Act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The Act did not apply to Jewish marriages or those of Quakers, whose marriages continued to be governed by their own customs.
In England and Wales, since 1837, civil marriages have been recognized as a legal alternative to church marriages under the Marriage Act 1836. In Germany, civil marriages were recognized in 1875. This law permitted a declaration of the marriage before an official clerk of the civil administration, when both spouses affirm their will to marry, to constitute a legally recognized valid and effective marriage, and allowed an optional private clerical marriage ceremony.
In contemporary English common law, a marriage is a voluntary contract by a man and a woman, in which by agreement they choose to become husband and wife. Edvard Westermarck proposed that "the institution of marriage has probably developed out of a primeval habit".
As of 2000, the average marriage age range was 25–44 years for men and 22–39 years for women.
China.
The mythological origin of Chinese marriage is a story about Nüwa and Fu Xi who invented proper marriage procedures after becoming married. In ancient Chinese society, people of the same surname are supposed to consult with their family trees prior marriage to reduce the potential risk of unintentional incest. Marriaging to one's maternal relatives was generally not thought of as incest, families sometimes intermarried from one generation to another. Over time, Chinese people became more geographically mobile. Individuals remained members of their biological families. When a couple died, the husband and the wife were buried separately in the respective clans' graveyard. In a maternal marriage, a male would become a son-in-law who lived in the wife's home.
The New Marriage Law of 1950 radically changed Chinese marriage traditions, enforcing monogamy, equality of men and women, and choice in marriage; arranged marriages were the most common type of marriage in China until then. Starting October 2003, it became legal to marry or divorce without authorization from the couple's work units. Although people with infectious diseases such as AIDS may now marry; marriage is still illegal for the mentally ill.

</doc>
<doc id="19731" url="http://en.wikipedia.org/wiki?curid=19731" title="Midgard">
Midgard

Midgard (an anglicised form of Old Norse "Miðgarðr"; Old English "Middangeard", Swedish "Midgård", Old Saxon "Middilgard", Old High German "Mittilagart", Gothic "Midjun-gards"; literally "middle enclosure") is the name for the world (in the sense of oikoumene) inhabited by and known to humans in early Germanic cosmology, and specifically one of the Nine Worlds in Norse mythology.
Etymology.
This name occurs in Old Norse literature as "Miðgarðr". In Old Saxon "Heliand" it appears as "Middilgard" and in Old High German poem "Muspilli" it appears as "Mittilagart". The Gothic form "Midjungards" is attested in the Gospel of Luke as a translation of the Greek word οἰκουμένη. The word is present in Old English epic and poetry as "Middangeard"; later transformed to "Middellærd" or "Mittelerde" ("Middle-earth") in Middle English literature.
All these forms are from a Common Germanic "*midja-gardaz" ("*meddila-", "*medjan-"), a compound of "*midja-" "middle" and "*gardaz" "yard, enclosure".
In early Germanic cosmology, the term stands alongside "world" (Old English "weorold", Old Saxon "werold", Old High German "weralt", Old Frisian "warld" and Old Norse "verǫld"), from a Common Germanic compound "*wira-alđiz" literally the "age of men".
Old Norse.
Midgard is a realm in Norse mythology. It is one of the Nine Worlds of North Mythology - the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim - the land of ice - to the north and Muspelheim - the land of fire - to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.
In Norse mythology, "Miðgarðr" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of Manheimr, the "home of men", a word used to refer to the entire world. The gods slew the giant Aurgelmir (Ymir) (the first created being) and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Aurgelmir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.
According to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plain of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea, only to rise again, fertile and green when the cycle repeats and the creation begins again.
Although most surviving instances of the word refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:
The Danish and Swedish form "Midgård" or "Midgaard", the Norwegian "Midgard" or "Midgård", as well as the Icelandic form "Miðgarður", all derive from the Old Norse term.
Old and Middle English.
The name "middangeard" occurs half a dozen times in the Old English epic poem Beowulf, and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.
The concept of Midgard occurs many times in Middle English. The association with "earth" (OE "eorðe") in Middle English "middellærd", "middelerde" is by popular etymology; the continuation of "geard" "enclosure" is "yard". An early example of this transformation is from the Ormulum:
The usage of "Middle-earth" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his "The Lord of the Rings" and other fantasy works; he was originally inspired by the references to "middangeard" and "Éarendel" in the Old English poem "Crist".
Old High German.
"Mittilagart" is mentioned in the 9th century Old High German "Muspilli" (v. 54) meaning "the world" as opposed to the sea and the heavens:

</doc>
<doc id="19734" url="http://en.wikipedia.org/wiki?curid=19734" title="Malcolm Fraser">
Malcolm Fraser

John Malcolm Fraser AC, CH, GCL (; 21 May 1930 – 20 March 2015) was an Australian politician who was the 22nd Prime Minister of Australia and the Leader of the Liberal Party from 1975 to 1983.
Elected to the Australian Parliament seat of Wannon in 1955 at the age of 25, Fraser was appointed to the Cabinet in 1966. After rising to become Minister for Defence in 1969, he was regarded as a contender for the leadership of the Liberal Party following their defeat in 1972, but he lost that contest to Billy Snedden. Fraser challenged Snedden in 1975 and was elected Leader of the Liberal Party, becoming the Leader of the Opposition.
Fraser was appointed as caretaker prime minister on 11 November 1975 by the Governor-General of Australia, Sir John Kerr, following the controversial dismissal of the Whitlam Government in which he played a key role. He went on to win the largest parliamentary majority as a proportion of seats in Australian political history at the subsequent election. After two further election victories in 1977 and 1980, he was defeated by the Bob Hawke-led Australian Labor Party in 1983 and left parliament shortly after.
Fraser was the last Liberal Party Prime Minister to practise Keynesian economics. In retirement, Fraser became involved in international relief and humanitarian aid issues and, domestically, as a forthright liberal voice for human rights. Shortly after Tony Abbott won the 2009 Liberal Party leadership spill, Fraser ended his Liberal Party membership, stating the party was "no longer a liberal party but a conservative party".
On 20 March 2015, Fraser died at the age of 84 after a brief illness.
Early life and education.
John Malcolm Fraser was born on 21 May 1930 in Toorak, Victoria, to a family with a history of involvement in politics and the pastoral industry. His grandfather, Simon Fraser, Sr., emigrated from Nova Scotia in 1853, becoming a successful pastoralist and speculator, and later a member of the Victorian Parliament, the Federation Conventions and the Australian Senate. Fraser's uncle, Simon Fraser, Jr., was a noted sportsman who rowed for Australia at the 1912 Olympics in Stockholm, and also played Australian rules football for Essendon and University in the Victorian Football League.
Fraser's father, John Neville Fraser, had been educated at the University of Melbourne, and was a pastoralist at Moulamein in the western Riverina region of New South Wales, and later at a property called "Nareen Station" in Nareen, near Hamilton in the Western District of Victoria. Fraser's mother, Una Woolf, was of Jewish descent on her father's side, which Fraser did not discover until he was an adult.
Fraser grew up on the family's pastoral properties and was educated at Tudor House School, near Moss Vale in New South Wales, Glamorgan, now part of Geelong Grammar School, and Melbourne Grammar School, before completing a degree in Philosophy, Politics and Economics at Magdalen College, Oxford in 1952. While at Oxford, Fraser was a classmate and friend of future Canadian Prime Minister John Turner.
Early political career.
Upon returning to Australia from Oxford, Fraser contested the seat of Wannon in 1954 for the Liberal Party, losing to Labor incumbent Don McLeod by just 17 votes. However, a redistribution made Wannon a notionally Liberal seat, and McLeod announced his retirement shortly before the election in 1955. Fraser subsequently won the seat with a majority of more than 5,000 on a swing of 8.5 per cent. Aged just 25, he was the youngest Member of Parliament; he would continue to represent Wannon until his retirement in 1983.
Cabinet minister.
After more than a decade on the backbench, Fraser was appointed to the Cabinet by the prime minister, Harold Holt, in 1966. As Minister for the Army he presided over the controversial Vietnam War conscription program. Under the new prime minister, John Gorton, he became Minister for Education and Science and in 1969 was promoted to Minister for Defence, a particularly challenging post at the time, given the height of Australia's involvement in the Vietnam War and the protests against it.
In March 1971, Fraser abruptly resigned from the Cabinet in protest at what he called Gorton's "interference in (his) ministerial responsibilities". This precipitated a series of events which eventually led to the downfall of Gorton and his replacement as prime minister by William McMahon. Gorton never forgave Fraser for the role he played in his downfall and to the day he died in 2002 could not bear to be in the same room with him. McMahon immediately reappointed Fraser to the Cabinet, returning him to his old position of Minister for Education and Science. When the Liberals were defeated at the 1972 election by the Labor Party under Gough Whitlam, McMahon resigned and Fraser became Shadow Minister for Labour under Billy Snedden.
Role in the dismissal.
After Snedden was defeated in 1974, Fraser unsuccessfully challenged him for the leadership of the Liberal Party in November. Despite surviving the challenge, Snedden's position in opinion polls continued to decline and he was unable to get the better of Whitlam in the Parliament. Fraser again challenged Snedden on 21 March 1975, this time succeeding and becoming Leader of the Liberal Party and Leader of the Opposition.
Following a series of ministerial scandals engulfing the Whitlam Government later that year, Fraser began to instruct Coalition senators to delay the government's budget bills, with the objective of forcing an early election that he believed he would win. After several months of political deadlock, during which time the government secretly explored methods of obtaining supply funding outside the Parliament, the governor-general, Sir John Kerr, controversially dismissed Whitlam as prime minister on 11 November 1975. Fraser was immediately sworn in as caretaker prime minister on the condition that he end the political deadlock and call an immediate election.
On 19 November 1975, shortly after the election had been called, a letter bomb was sent to Fraser, but it was intercepted and defused before it reached him. Similar devices were sent to the governor-general and the Premier of Queensland, Joh Bjelke-Petersen.
Prime Minister of Australia.
At the 1975 election, Fraser led the Liberal-Country Party Coalition to a landslide victory. The Coalition won 30 seats from Labor to gain a 55-seat majority, which remains to date the largest in Australian history. Fraser subsequently led the Coalition to a second victory in 1977, with only a very small decrease in their vote. The Liberals actually won a majority in their own right, something that even Robert Menzies had not been able to achieve. Although Fraser thus had no need for the support of the (National) Country Party to govern, he retained the formal Coalition between the two parties.
Fraser quickly dismantled some of the programs of the Whitlam Government, such as the Ministry of the Media, and made major changes to the universal health insurance system Medibank. He initially maintained Whitlam's levels of tax and spending, but real per-person tax and spending soon began to increase. He did manage to rein in inflation, which had soared under Whitlam. His so-called "Razor Gang" implemented stringent budget cuts across many areas of the Commonwealth Public Sector, including the Australian Broadcasting Corporation (ABC).
Fraser practiced Keynesian economics during his time as Prime Minister, in part demonstrated by running budget deficits throughout his term as Prime Minister. He was the Liberal Party's last Keynesian Prime Minister. Though he had long been identified with the Liberal Party's right wing, he did not carry out the radically conservative program that his political enemies had predicted, and that some of his followers wanted. Fraser's relatively moderate policies particularly disappointed the treasurer, John Howard, as well as other ministers who were strong adherents of emerging free market neo-liberal economics, and therefore detractors of Keynesian economics. The government's economic record was marred by rising double-digit unemployment and double-digit inflation, creating "stagflation", caused in part by the ongoing effects of the 1973 oil crisis.
Fraser was particularly active in foreign policy as prime minister. He supported the Commonwealth in campaigning to abolish apartheid in South Africa and refused permission for the aircraft carrying the Springbok rugby team to refuel on Australian territory en route to their controversial 1981 tour of New Zealand. However, an earlier tour by the South African ski boat angling team was allowed to pass through Australia on the way to New Zealand in 1977 and the transit records were suppressed by Cabinet order.
Fraser also strongly opposed white minority rule in Rhodesia. During the 1979 Commonwealth Conference, Fraser, together with his Nigerian counterpart, convinced the newly elected British prime minister, Margaret Thatcher, to withhold recognition of the internal settlement Zimbabwe Rhodesia government; Thatcher had earlier promised to recognise it. Subsequently, the Lancaster House Agreement was signed and Robert Mugabe was elected leader of an independent Zimbabwe at the inaugural 1980 election. Duncan Campbell, a former deputy secretary of the Department of Foreign Affairs and Trade has stated that Fraser was "the principal architect" in the ending of white minority rule. The President of Tanzania, Julius Nyerere, said that he considered Fraser's role "crucial in many parts" and the President of Zambia, Kenneth Kaunda, called his contribution "vital".
Under Fraser, Australia recognised Indonesia's annexation of East Timor, although many East Timorese refugees were granted asylum in Australia. Fraser was also a strong supporter of the United States and supported the boycott of the 1980 Summer Olympics in Moscow. However, although he persuaded some sporting bodies not to compete, Fraser did not try to the prevent the Australian Olympic Committee sending a team to the Moscow Games.
Fraser also surprised his critics over immigration policy; according to 1977 Cabinet documents, the Fraser Government adopted a formal policy for "a humanitarian commitment to admit refugees for resettlement". Fraser's aim was to expand immigration from Asian countries and allow more refugees to enter Australia. He was a firm supporter of multiculturalism and established a government-funded multilingual radio and television network, the Special Broadcasting Service (SBS), building on their first radio stations which had been established under the Whitlam Government.
Despite Fraser's support for SBS, his government imposed stringent budget cuts on the national broadcaster, the ABC, which came under repeated attack from the Coalition for alleged "left-wing bias" and "unfair" coverage on their TV programs, including "This Day Tonight" and "Four Corners", and on the ABC's new youth-oriented radio station Double Jay. One result of the cuts was a plan to establish a national youth radio network, of which Double Jay was the first station. The network was delayed for many years and did not come to fruition until the 1990s. Fraser also legislated to give Indigenous Australians control of their traditional lands in the Northern Territory, but resisted imposing land rights laws on conservative state governments.
At the 1980 election, Fraser saw his majority more than halved, from 48 seats to 21. The Coalition also lost control of the Senate. Despite this, Fraser remained ahead of Labor leader Bill Hayden in opinion polls. However, in 1982 the economy experienced a sharp and unexpected recession, and a protracted scandal over tax-avoidance schemes run by some high-profile Liberals also began to hurt the Government.
In April 1981, the Minister for Industrial Relations, Andrew Peacock, resigned from the Cabinet, accusing Fraser of "constant interference in his portfolio". Fraser, however, had accused former prime minister John Gorton of the same thing a decade earlier. Peacock subsequently challenged Fraser for the leadership; although Fraser defeated Peacock, these events left him politically weakened.
By early 1982, the popular former ACTU President, Bob Hawke, who had entered Parliament in 1980, was polling well ahead of both Fraser and the Labor Leader, Bill Hayden, on the question of who voters would rather see as prime minister. Fraser was well aware of the infighting this caused between Hayden and Hawke and had planned to call a snap election in autumn 1982, preventing the Labor Party changing leaders. These plans were derailed when Fraser suffered a severe back injury. Shortly after recovering from his injury, the Liberal Party narrowly won a by-election in the marginal seat of Flinders in January 1983. The failure of the Labor Party to win the seat convinced Fraser that he would be able to win an election against Hayden. As leadership tensions began to grow in the Labor Party throughout January, Fraser subsequently resolved to call a double dissolution election at the earliest opportunity, hoping to capitalise on Labor's disunity and prevent them from replacing Hayden.
On 3 February 1983, Fraser arranged to visit the Governor-General of Australia, Ninian Stephen, intending to call a surprise election. By coincidence and without any knowledge of Fraser's plans, Hayden resigned as Labor Leader just two hours before Fraser travelled to Government House. This meant that the considerably more popular Bob Hawke was able to replace him at almost exactly the same time that the writs were issued for the election. Although Fraser reacted to the move by saying he looked forward to "knock[ing] two Labor Leaders off in one go" at the forthcoming election, Labor immediately surged in the opinion polls.
At the election on 5 March the Coalition was heavily defeated, suffering a 24-seat swing, the worst defeat of a non-Labor government since Federation. Fraser immediately announced his resignation as Liberal leader and formally resigned as prime minister on 11 March 1983; he retired from Parliament two months later. To date, he is the last non-interim prime minister from a rural seat.
Retirement.
In retirement Fraser served as Chairman of the UN Panel of Eminent Persons on the Role of Transnational Corporations in South Africa 1985, as Co-Chairman of the Commonwealth Group of Eminent Persons on South Africa in 1985–86, and as Chairman of the UN Secretary-General's Expert Group on African Commodity Issues in 1989–90. He was a distinguished international fellow at the American Enterprise Institute from 1984–86. Fraser helped to establish the foreign aid group CARE organisation in Australia and became the agency's international president in 1991, and worked with a number of other charitable organisations. In 2006, he was appointed Professorial Fellow at the Asia Pacific Centre for Military Law, and in October 2007 he presented his inaugural professorial lecture, "Finding Security in Terrorism's Shadow: The importance of the rule of law".
Memphis trousers affair.
On 14 October 1986, Fraser, then the Chairman of the Commonwealth Eminent Persons Group, was found in the foyer of the Admiral Benbow Inn, a seedy Memphis hotel, wearing only a pair of underpants and confused as to where his trousers were. The hotel was an establishment popular with prostitutes and drug dealers. Though it was rumoured at the time that the former Prime Minister had been with a prostitute, his wife stated that Fraser had no recollection of the events and that she believes it more likely that he was the victim of a practical joke by his fellow delegates.
Estrangement from the Liberal Party.
In 1993, Fraser made a bid for the Liberal Party presidency but withdrew at the last minute following opposition to his bid, which was raised due to Fraser being critical of then Liberal leader John Hewson for losing the election earlier that year.
After 1996, Fraser was critical of the Howard Coalition government over foreign policy issues, particularly John Howard's alignment with the foreign policy of the Bush administration, which Fraser saw as damaging Australian relationships in Asia. He opposed Howard's policy on asylum-seekers, campaigned in support of an Australian Republic and attacked what he perceived as a lack of integrity in Australian politics, together with former Labor prime minister Gough Whitlam, finding much common ground with his predecessor and his successor Bob Hawke, another republican.
The 2001 election continued his estrangement from the Liberal Party. Many Liberals criticised the Fraser years as "a decade of lost opportunity" on deregulation of the Australian economy and other issues. In early 2004, a Young Liberal convention in Hobart called for Fraser's life membership of the Liberal Party to be ended.
In 2006, Fraser criticised Howard Liberal government policies on areas such as refugees, terrorism and civil liberties, and that "if Australia continues to follow United States policies, it runs the risk of being embroiled in the conflict in Iraq for decades, and a fear of Islam in the Australian community will take years to eradicate". Fraser claimed that the way the Howard government handled the David Hicks, Cornelia Rau and Vivian Solon cases was questionable.
On 20 July 2007, Fraser sent an open letter to members of the large activist group GetUp!, encouraging members to support GetUp's campaign for a change in policy on Iraq including a clearly defined exit strategy. Fraser stated: "One of the things we should say to the Americans, quite simply, is that if the United States is not prepared to involve itself in high-level diplomacy concerning Iraq and other Middle East questions, our forces will be withdrawn before Christmas."
After the defeat of the Howard government at the 2007 federal election, Fraser claimed Howard approached him in a corridor, following a cabinet meeting in May 1977 regarding Vietnamese refugees, and said: "We don't want too many of these people. We're doing this just for show, aren't we?" The claims were made by Fraser in an interview to mark the release of the 1977 cabinet papers. Howard, through a spokesman, denied making the comment.
In October 2007 Fraser gave a speech to Melbourne Law School on terrorism and "the importance of the rule of law," which Liberal MP Sophie Mirabella 
condemned in January 2008, claiming errors and "either intellectual sloppiness or deliberate dishonesty", and claimed that he tacitly supported Islamic fundamentalism, that he should have no influence on foreign policy, and claimed his stance on the war on terror had left him open to caricature as a "frothing-at-the-mouth leftie".
Shortly after Tony Abbott won the 2009 Liberal Party leadership spill, Fraser ended his Liberal Party membership, stating the party was "no longer a liberal party but a conservative party".
Later political activity.
In December 2011, Fraser was highly critical of the Australian government's decision (also supported by the Liberal Party Opposition) to permit the export of uranium to India, relaxing the Fraser government's policy of banning sales of uranium to countries that are not signatories of the Nuclear Non-Proliferation Treaty.
In 2012, Fraser criticised the basing of U.S. military forces in Australia. In 2014, speaking on the Russian RT television network, he criticised the concept of American exceptionalism and U.S. foreign policy.
In late 2012, Fraser wrote a foreword for "The Journal Jurisprudence" where he openly criticised the current state of human rights in Australia and the Western World. "It is a sobering thought that in recent times, freedoms hard won through centuries of struggle, in the United Kingdom and elsewhere have been whittled away. In Australia alone we have laws that allow the secret detention of the innocent. We have had a vast expansion of the power of intelligence agencies. In many cases the onus of proof has been reversed and the justice that once prevailed as been gravely diminished."
In July 2013, Fraser endorsed Australian Greens Senator Sarah Hanson-Young for re-election in a television advertisement, stating she had been a "reasonable and fair-minded voice".
Fraser's books include "Malcolm Fraser: The Political Memoirs" (with Margaret Simons – The Miegunyah Press, 2010) and "Dangerous Allies" (Melbourne University Press, 2014), which warns of “strategic dependence” on the United States.
Death.
On 20 March 2015, his office announced that Fraser had died in the early hours of the morning, noting that he had suffered a brief illness. An obituary noted that there had been "greater appreciation of the constructive and positive nature of his post-prime ministerial contribution" as his retirement years progressed. He was survived by his wife Tamara "Tamie" Beggs, whom he had married in 1956, and their four children, Mark, Angela, Hugh and Phoebe.
Fraser was given a state funeral at Scots' Church in Melbourne on 27 March 2015.
Legacy.
In 2004, Fraser designated the University of Melbourne the official custodian of his personal papers and library to create the Malcolm Fraser Collection at the University of Melbourne.
Bibliography.
</dl>

</doc>
<doc id="19735" url="http://en.wikipedia.org/wiki?curid=19735" title="Macquarie University">
Macquarie University

Macquarie University is a public research university based in Sydney, Australia, in the suburb of Macquarie Park. Founded in 1964 by the New South Wales Government, it was the third university to be established in the metropolitan area of Sydney.
The university's five faculties, the Macquarie University Hospital and the Macquarie Graduate School of Management are located on the university's main campus in suburban Sydney. The Macquarie City Campus is located in the Sydney central business district.
The university is the first in Australia to fully align its degree system with the Bologna Accord.
History.
20th century.
The idea of founding a third university in Sydney was flagged in the early 1960s when the New South Wales Government formed a committee of enquiry into higher education to deal with a perceived emergency in university enrollments in New South Wales. During this enquiry, the Senate of the University of Sydney put in a submission which highlighted ‘the immediate need to establish a third university in the metropolitan area’. After much debate a future campus location was selected in what was then a semi-rural part of North Ryde, and it was decided that the future university be named after Lachlan Macquarie, an important early governor of the colony of New South Wales.
Macquarie University was formally established in 1964 with the passage of the Macquarie University Act 1964 by the New South Wales parliament. The university was designed in the Brutalist style and developed by the renowned town planner Walter Abraham who also oversaw the next 20 years of planning and development for the university. A committee appointed to advise the state government on the establishment of the new university at North Ryde nominated Abraham as the architect-planner. The fledgling Macquarie University Council decided that planning for the campus would be done within the university, rather than by consultants, and this led to the establishment of the architect-planners office.
The first Vice-Chancellor of Macquarie University, Alexander George Mitchell, was selected by the University Council which met for the first time on 17 June 1964. Members of the first university council included: Colonel Sir Edward Ford OBE, David Paver Mellor, Rae Else-Mitchell QC and Sir Walter Scott.
The university first opened to students on 6 March 1967 with more students than anticipated. The Australian Universities Commission had allowed for 510 effective full-time students (EFTS) but Macquarie had 956 enrolments and 622 EFTS. Between 1968 and 1969, enrolment at Macquarie increased dramatically with an extra 1200 EFTS, with 100 new academic staff employed. 1969 also saw the establishment of the Macquarie Graduate School of Management (MGSM).
Macquarie grew during the seventies and eighties with rapid expansion in courses offered, student numbers and development of the site. In 1972, the university established the Macquarie Law School, the third law school in Sydney. In their book "Liberality of Opportunity", Bruce Mansfield and Mark Hutchinson describe the founding of Macquarie University as ‘an act of faith and a great experiment’. An additional topic considered in this book is the science reform movement of the late 1970s that resulted in the introduction of a named science degree, thus facilitating the subsequent inclusion of other named degrees in addition to the traditional BA. Alternative views on this topic are given by famous British-Australian physicist John Ward and laser physicist Frank Duarte.
The first Vice-Chancellor of Macquarie University was Alexander George Mitchell, who held the position until December 1975, when he was replaced by Edwin Webb, who served until 1986.
In 1990 the university absorbed the Institute of Early Childhood Studies of the Sydney College of Advanced Education, under the terms of the Higher Education (Amalgamation) Act 1989.
Di Yerbury was appointed Vice-Chancellor in 1986, and was the first female Vice-Chancellor in Australia. Professor Yerbury held the position of Vice-Chancellor for just under 20 years.
21st century.
Professor Steven Schwartz replaced Di Yerbury at the beginning of 2006. Yerbury's departure was attended with much controversy, including a "bitter dispute" with Schwartz, disputed ownership of university artworks worth $13 million and Yerbury's salary package. In August 2006, Professor Schwartz expressed concern about the actions of Yerbury in a letter to university auditors. Yerbury strongly denied any wrongdoing and claimed the artworks were hers.
During 2007, Macquarie University restructured its student organisation after an audit raised questions about management of hundreds of thousands of dollars in funds by student organisations At the centre of the investigation was Victor Ma, president of the Macquarie University Students' Council, who was previously involved in a high-profile case of student election fixing at the University of Sydney.
The university Council resolved to immediately remove Ma from his position. Vice-Chancellor Schwartz cited an urgent need to reform Macquarie's main student bodies.
However, Ma strongly denied any wrongdoing and labelled the controversy a case of ‘character assassination’.
The Federal Court ordered on 23 May 2007 that Macquarie University Union Ltd be wound up.
Following the dissolution of Macquarie University Union Ltd, the outgoing student organisation was replaced with a new wholly owned subsidiary company of the university, known as U@MQ Ltd. The new student organisation originally lacked a true student representative union; however, following a complete review and authorisation from the university Council, a new student union known as Macquarie University Students Association (MUSRA) was established in 2009.
Within the first few hundred days of Schwartz's instatement as Vice-Chancellor, the 'Macquarie@50' strategic plan was launched, which positioned the university to enhance research, teaching, infrastructure and academic rankings by the university's 50th anniversary in 2014. Included in the university's plans for the future was the establishment of a sustainability office in order to more effectively manage environmental and social development at Macquarie. As part of this campaign, in 2009 Macquarie became the first Fair Trade accredited university in Australia. The beginning of 2009 also saw the introduction of a new logo for the university which retained the Sirius Star, present on both the old logo and the university crest, but now 'embedded in a stylised lotus flower'. In accordance with the university by-law, the crest continues to be used for formal purposes and is displayed on university testamurs. The by-law also prescribes the university's motto, taken from Chaucer: 'And gladly teche'.
In 2013, the university became the first in Australia to fully align its degree system with the Bologna Accord.
Governance.
The university is governed by a 17-member Council.
The is the governing authority of the university under the "Macquarie University Act 1989". The Council takes primary responsibility for the control and management of the affairs of the University, and is empowered to make by-laws and rules relating to how the University is managed. Members of the Council include the University Vice-Chancellor, Academic and non-academic staff, the Vice President of the Academic Senate and a student representative. The Council is chaired by The Chancellor of the University.
The is the primary academic body of the university. It has certain powers delegated to it by Council, such as the approving of examination results and the completion of requirements for the award of degrees. At the same time, it makes recommendations to the Council concerning all changes to degree rules, and all proposals for new awards. While the Academic Senate is an independent body, it is required to make recommendations to the university Council in relation to matters outside its delegated authority.
Macquarie's current Vice-Chancellor, Dr Bruce Dowton, took over from Professor Schwartz in September 2012. Prior to his appointment Professor Dowton served as a senior medical executive having held a range of positions in university, healthcare and consulting organisations. He also served as a pediatrician at the Massachusetts General Hospital for Children, and as Clinical Professor of Pediatrics at Harvard Medical School. There have been five Vice-Chancellors in the university’s history.
Campus.
Main campus.
Macquarie University's main campus is located about 16 km north-west of the Sydney CBD and is set on 126 hectares of rolling lawns and natural bushland. Located within the high-technology corridor of Sydney's north-west and in close proximity to Macquarie Park and its surrounding industries, Macquarie's location has been crucial in its development as a relatively research intensive university. The university is straddled between the suburbs of North Ryde and the later developed technology and industry focused Macquarie Park; however, the campus has its own postcode, 2109. The M2 Motorway runs parallel to the northern boundary of the campus and is accessible to traffic from the university.
Prior to the development of the campus, most of the site was cultivated with peach orchards, market gardens and poultry farms. The university’s first architect-planner was Walter Abraham, one of the first six administrators appointed to Macquarie University.
 As the site adapted from its former rural use to a busy collegiate environment, he implemented carefully designed planting programs across the campus. Abraham established a grid design comprising lots of 300 sqft running north-south, with the aim of creating a compact academic core. The measure of 300 ft was seen as one minute's walk, and grid design reflected the aim of having a maximum walk of 10 minutes between any two parts of the university. The main east-west walkway that runs from the Macquarie University Research Park through to the arts faculty buildings, was named Wally's Walk in recognition of Walter Abraham's contribution to the development of the university.
Apart from its centres of learning, the campus features the Macquarie University Research Park, museums, art galleries, a sculpture park, an observatory, a sport and aquatic centre and also the private Macquarie University Hospital.
Macquarie became the first university in Australia to own and operate a private medical facility in 2010 when it opened a $300 million hospital on its North Ryde campus. The hospital is first and only private not-for-profit teaching hospital on an Australian university campus. The Macquarie University Hospital is located to the north of the main campus area are the university sports grounds. It comprises 183 beds, 12 operating theatres, 2 cardiac and vascular angiography suites. The hospital is co-located with the university's Australian School of Advanced Medicine.
The Macquarie University Research Park is a privately funded Research and Development Park located on campus and is home to companies including Dow Corning, Goodman Fielder, Nortel Networks, OPSM and Siemens.
Cochlear Headquarters, located on the southern edge of the campus, is the global headquarters for Cochlear Limited, manufacturers of cochlear implants.
Located on the western side of the campus is the Macquarie University Sport and Aquatic Centre. Previously a sports hall facility, the complex was renovated and reopened in 2007 with the addition of the new gym and aquatic centre. It houses a 50 metre FINA-compliant outdoor pool and a 25 metre indoor pool. The complex also contains a gymnasium and squash, badminton, basketball, volleyball and netball courts.
The Macquarie University Observatory was originally constructed in 1978 as a research facility but, since 1997, has been accessible to the public through its Public Observing Program.
Library.
The library houses over 1.8 million items and uses the Library of Congress Classification System. The library features several collections including a Rare Book Collection, a Palaeontology Collection and the Brunner Collection of Egyptological materials. Macquarie University operated two libraries during the transition. The old library in building C7A closed at the end of July 2011, and the new library in building C3C became fully operational on 1 August 2011. The new library was the first university library in Australia to possess an Automated Storage and Retrieval System (ASRS). The ASRS consists of an environmentally controlled vault with metal bins storing the items; robotic cranes retrieve an item on request and deliver it to the service desk for collection.
Residential colleges.
Macquarie University has two residential colleges on its campus, Dunmore Lang College and Robert Menzies College, both founded in 1972. In addition to these residential colleges is the Macquarie University Village which contains over 890 rooms inside multiple two storey townhouses and apartment block.
Macquarie University railway station.
Macquarie University is served by the Macquarie University railway station, which opened in 2009. The underground station is on the Sydney Trains network. The station is served by eight trains per hour for most of the day. There is also a bus interchange within the campus that provides close to 800 bus services daily.
Sydney central business district campus.
The Macquarie City Campus is located on York Street in the Sydney CBD, above Wynyard train station. The city campus offered foundation studies, three undergraduate and six postgraduate degrees until 2015. Macquarie is seeking new premises near the current location of the city campus after the university decided to manage the city campus itself from 2016.
Academics.
The university currently comprises 35 departments within five faculties:
Research centres, schools and institutes that are affiliated with the university:
Macquarie University’s Australian Hearing Hub is partnered with Cochlear. Cochlear Headquarters are on campus. The Australian Hearing Hub includes the head office of Australian Hearing.
The Australian Research Institute for Environment and Sustainability is a research centre that promotes change for environmental sustainability, is affiliated with the University and is located on its campus.
Access Macquarie Limited was established in 1989 as the commercial arm of the university. It facilitates and supports the commercial needs of industry, business and government organisations seeking to utilise the academic expertise of the broader University community.
Admissions.
The Sydney Institute of Business and Technology operates on the Macquarie University campus, offering Foundation Studies (Pre-University) and University-level Diplomas. Upon successful completion of a SIBT Diploma, students enter the appropriate Bachelor Degree as a second year student.
The Centre for Macquarie English is the English-language centre that offers a range of specialised, direct entry English programmes that are approved by Macquarie University.
Research.
The university positions itself as being research intensive. In 2012, 85% of Macquarie's broad fields of research was rated 'at or above world standard' in the Excellence in Research for Australia 2012 National report. The university is within the top 3 universities in Australia for the number of peer reviewed publications produced per academic staff member.
Researchers at Macquarie University, David Skellern and Neil Weste, and the Commonwealth Scientific and Industrial Research Organisation helped develop Wi-Fi. David Skellern has been a major donor to the University through the Skellern Family Trust. Macquarie University's linguistics department developed the Macquarie Dictionary. The dictionary is regarded as the standard reference on Australian English.
Macquarie University has a research partnership with the University of Hamburg in Germany and Fudan University in China. They offer dual and joint degree programs and engage in joint research.
University rankings.
Macquarie is ranked in the top 40 universities in the Asia-Pacific region and within Australia's top nine universities according to the Academic Ranking of World Universities, the U.S. News & World Report Rankings and the QS World University Rankings. Macquarie is ranked just outside the top eight in Australia (Group of Eight universities) in most international rankings, or has ranked within the the top eight in Australia in international rankings. In 2011-2012 CWTS Leiden Ranking Macquarie was ranked 4th in Australia. Macquarie University was ranked in 2014: 239th in the world (9th in Australia) in the Academic Ranking of World Universities, 254th in the world (9th in Australia) in the QS World University Rankings, and 301-350 in the world in the Times Higher Education World University Rankings.
Macquarie was the highest ranked university in Australia under the age of 50 and was ranked 18th in the world (prior to its golden jubilee in 2014), according to the QS World University Rankings.
Macquarie University was ranked among the top 50 universities in the world for linguistics (43rd), psychology (48th) and earth and marine sciences (48th), and was ranked in the top 5 nationally for philosophy and earth and marine sciences, according to the 2014 QS World University Rankings.
Macquarie ranked 67th in the world for Arts and Humanities (equal 5th in Australia), according to the 2015 Times Higher Education rankings by subject. Arts and Humanities is Macquarie's best discipline area in rankings. Macquarie was one of four non-Group of Eight universities ranked in the top 100 universities in the world in particular discipline areas.
The Macquarie Graduate School of Management is one of the oldest business schools in Australia. In 2011, The Economist ranked MGSM 73th in the world, 7th in Asia Pacific and 1st in Sydney/New South Wales. It was ranked 13th in the Asia-Pacific, according to QS Global 200 Business Schools Report for 2013-14. In 2014, The Economist ranked MGSM 5th in the Asia-Pacific, 3rd in Australia, 1st in Sydney/New South Wales and 49th in the world. It was the highest ranked business school in Australia and was ranked 68th in the world in the 2015 Financial Times MBA ranking.
Macquarie University is ranked first in environmental sciences and ecology research within Australia and New Zealand, and is ranked 14th in the world, according to Times Higher Education.
Students.
Macquarie is the fourth largest university in Sydney (38,753 students in 2013). The university has the largest student exchange programme in Australia. 
In 2012, 9,802 students from Asia were enrolled at Macquarie University (Sydney campuses and offshore programs in China, Hong Kong, Korea and Singapore).
Campus Life manages the university’s non-academic services: food and retail, sport and recreation, student groups, child care, and entertainment.
The Global Leadership Program (GLP) is a student organisation and program that is undertaken by a large proportion of Macquarie Students. All students at the university are encouraged to undertake the program to enhance leadership skills, cross cultural understanding and international awareness. Upon completion of the GLP, students receive a formal notation on their academic transcript.
Macquarie University has its own community radio station on campus, 2SER FM. The station is jointly owned by Macquarie University and University of Technology, Sydney.
Macquarie University students celebrate Conception Day each year since 1969 to – according to legend – commemorate the date of conception of Lachlan Macquarie, as his birthday fell at the wrong time of year for a celebration. Conception Day is traditionally held on the last day of classes before the September mid-semester break.
Notable alumni and staff.
Alumni include Rhodes and John Monash Scholars and several Fulbright Scholars.
Notable alumni include: Australian politician and former Lord Mayor of Brisbane, Jim Soorley; Australian basketball player, Lauren Jackson; Australian swimmer, Ian Thorpe; Australian water polo player, Holly Lincoln-Smith; three founding members of the Australian children's musical group The Wiggles (Murray Cook, Anthony Field, Greg Page); New Zealand conservationist, Pete Bethune.
Notable alumni in science include: Australian scientist Barry Brook, American physicist Frank Duarte, and Australian scientist Cathy Foley. Alumni notable in the business world include: Australian edge fund manager Greg Coffey, Australian businesswoman Catherine Livingstone and Australian venture capitalist Larry R. Marshall. 
Notable faculty members include: Australian writer and four time Miles Franklin Award winner, Thea Astley; Hungarian Australian mathematician, Esther Szekeres; Australian mathematican, Neil Trudinger; Australian environmentalist and activist, Tim Flannery; British physicist, Paul Davies; British-Australian physicist, John Clive Ward; Israeli-Australian physicist, José Enrique Moyal; Australian linguist, Geoffrey Hull.
Four Macquarie University academics were included in The World’s Most Influential Minds 2014 report by Thomson Reuters, which identified the most highly cited researchers of the last 11 years.

</doc>
<doc id="19736" url="http://en.wikipedia.org/wiki?curid=19736" title="Muspelheim">
Muspelheim

In Norse mythology, Muspelheim (Old Norse: "Múspellsheimr"), also called Muspell (Old Norse: "Múspell"), is a realm of fire. This realm is one of the Nine Worlds, ruled by Surtr with his consort Sinmara in some accounts. The denizens of Muspelheim were usually referred to as the eldjötnar ("Fire Giants") in Norse tradition, though they were also identified by other epithets in Eddic poetry, such as the Múspellssynir (or Múspellsmegir — "sons of Muspell") and the Rjúfendr (from "rjúfa" — "to break, tear asunder", "Destroyers of Doomsday"). Both of these terms sometimes described an entirely separate mythological species that dwelled alongside or in place of the eldjötnar within this fiery realm. Muspelheim is fire; and the land to the North, Niflheim, is ice. The two mixed and created water from the melting ice in Ginnungagap. The sun and the stars originate from Muspelheim.
According to the Ragnarök prophecies in Snorri Sturluson's Gylfaginning, the first part of his Prose Edda, the sons of Muspell will break the Bifröst bridge, signaling the end of times:
 In the midst of this clash and din the heavens are rent in twain, and the sons of Muspell come riding through the opening. Surtr rides first, and before him and after him flames burning fire. He has a very good sword, which shines brighter than the sun. As they ride over Bifrost it breaks to pieces, as has before been stated. The sons of Muspel direct their course to the plain which is called Vigrid... The sons of Muspel have there effulgent bands alone by themselves.
The etymology of "Muspelheim" is uncertain, but may come from "Mund-spilli", "world-destroyers", "wreck of the world".
In popular culture.
The realm of Muspelheim appears as 'Muspel' in Gene Wolfe's book series, "The Wizard Knight". It is inhabited by the dragon 'Setr', as a close parallel to Surtr.
"Muspel" is the name of a German colony in Africa in the alternate-reality novel "The Afrika Reich" by Guy Saville.
Muspelheim is the name of a reoccurring boss ship in the "" series.
A glimpse of Muspelheim is seen in the Marvel Studios film .
The realm is also referenced in the "Agents of S.H.I.E.L.D." episode "Repairs" when technician Tobias Ford is trapped between cosmic realms following the explosion of a StatiCorp Particle Acceleration Complex. He repeatedly traverses between Earth and another plane of existence, described by Tobias as "Hell." When Fitz and Simmons generate digital scans of the portal generated by the explosion, a fiery, mountain-filled realm can be seen that shares a significant resemblance to Muspelheim.
Múspelheim appears in the science fiction novel "Resonance: Ragnarok: Volume Three" (Kindle Location 3488) by John Meaney
In the Video Game "Valkyrie Profile", Muspelheim is mentioned within the magic words that compose the great Magic "Ifreet Caress": "I invoke the rites of fiery Muspelheim, now give up thy soul to inferno's embrace! Ifreet Caress!"
In the videogame "La Mulana 2", Múspelheim (spelled "Muspellheim") is an area within the ruins Eg'Lana and is known as the "Blazing Frost Canopy", a land that's both frozen and on fire. This technically makes it both Múspelheim and Niflheim in one.
In the Japanese light novel "Mahouka Koukou no Rettousei", Muspelheim is the name of a magic spell that creates a field where immense heat, capable of creating plasma, is generated.
In the Amon Amarth song "Destroyer of the Universe", Muspelheim is mentioned in the first verse.
In the 2014 video game Bayonetta 2 Muspelheim serves as the location of challenge rooms within portals found throughout levels, however the realm is not represented like it is described in Norse Mythology but instead like the typical depictions of Judeo-Christian "Heaven" and "Hell".
In Heroine's Quest, Muspell (actually eldjötnar) appear as enemies.

</doc>
<doc id="19737" url="http://en.wikipedia.org/wiki?curid=19737" title="Maxwell's equations">
Maxwell's equations

Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862. 
The equations have two major variants. The "microscopic" set of Maxwell's equations uses total charge and total current, including the complicated charges and currents in materials at the atomic scale; it has universal applicability but may be infeasible to calculate. The "macroscopic" set of Maxwell's equations defines two new auxiliary fields that describe large-scale behaviour without having to consider these atomic scale details, but it requires the use of parameters characterizing the electromagnetic properties of the relevant materials.
The term "Maxwell's equations" is often used for other forms of Maxwell's equations. For example, space-time formulations are commonly used in high energy and gravitational physics. These formulations, defined on space-time rather than space and time separately, are manifestly compatible with special and general relativity. In quantum mechanics and analytical mechanics, versions of Maxwell's equations based on the electric and magnetic potentials are preferred.
Since the mid-20th century, it has been understood that Maxwell's equations are not exact laws of the universe, but are a classical approximation to the more accurate and fundamental theory of quantum electrodynamics. In most cases, though, quantum deviations from Maxwell's equations are immeasurably small. Exceptions occur when the particle nature of light is important or for very strong electric fields.
Formulation in terms of electric and magnetic fields.
The powerful and most widely familiar form of Maxwell's equations, whose formulation is due to Oliver Heaviside, in the vector calculus formalism, is used throughout unless otherwise explicitly stated.
Symbols in bold represent vector quantities, and symbols in "italics" represent scalar quantities, unless otherwise indicated.
The equations introduce the electric field E, a vector field, and the magnetic field B, a pseudovector field, where each generally have time-dependence. The sources of these fields are electric charges and electric currents, which can be expressed as local densities namely charge density "ρ" and current density J. A separate law of nature, the Lorentz force law, describes how the electric and magnetic field act on charged particles and currents. A version of this law was included in the original equations by Maxwell but, by convention, is no longer.
In the electric-magnetic field formulation there are four equations. Two of them describe how the fields vary in space due to sources, if any; electric fields emanating from electric charges in Gauss's law, and magnetic fields as closed field lines "not due to magnetic monopoles" in Gauss's law for magnetism. The other two describe how the fields "circulate" around their respective sources; the magnetic field "circulates" around electric currents and time varying electric fields in Ampère's law with Maxwell's addition, while the electric field "circulates" around time varying magnetic fields in Faraday's law.
The precise formulation of Maxwell's equations depends on the precise definition of the quantities involved. Conventions differ with the unit systems, because various definitions and dimensions are changed by absorbing dimensionful factors like the speed of light "c". This makes constants come out differently.
Conventional formulation in SI units.
The equations in this section are given in the convention used with SI units. Other units commonly used are Gaussian units based on the cgs system, Lorentz–Heaviside units (used mainly in particle physics), and Planck units (used in theoretical physics). See below for the formulation with Gaussian units.
where the universal constants appearing in the equations are 
In the differential equations, a "local" description of the fields, 
The sources are taken to be 
In the integral equations, a description of the fields within a region of space, 
Here "fixed" means the volume or surface do not change in time. Although it is possible to formulate Maxwell's equations with time-dependent surfaces and volumes, this is not actually necessary: the equations are correct and complete with time-independent surfaces. The sources are correspondingly the total amounts of charge and current within these volumes and surfaces, found by integration. 
The "total charge or current" refers to including free and bound charges, or free and bound currents. These are used in the macroscopic formulation below.
Relationship between differential and integral formulations.
The differential and integral formulations of the equations are mathematically equivalent, by the divergence theorem in the case of Gauss's law and Gauss's law for magnetism, and by the Kelvin–Stokes theorem in the case of Faraday's law and Ampère's law. Both the differential and integral formulations are useful. The integral formulation can often be used to simplify and directly calculate fields from symmetric distributions of charges and currents. On the other hand, the differential formulation is a more natural starting point for calculating the fields in more complicated (less symmetric) situations, for example using finite element analysis.
Flux and divergence.
The "fields emanating from the sources" can be inferred from the surface integrals of the fields through the closed surface ∂Ω, defined as the electric flux formula_7formula_8 and magnetic flux formula_7formula_10, as well as their respective divergences ∇ · E and ∇ · B. These surface integrals and divergences are connected by the divergence theorem.
Circulation and curl.
The "circulation of the fields" can be interpreted from the line integrals of the fields around the closed curve ∂Σ:
where dℓ is the differential vector element of "path length" tangential to the path/curve, as well as their curls:
These line integrals and curls are connected by Stokes' theorem, and are analogous to quantities in classical fluid dynamics: the circulation of a fluid is the line integral of the fluid's flow velocity field around a closed loop, and the vorticity of the fluid is the curl of the velocity field.
Time evolution.
The "dynamics" or "time evolution of the fields" is due to the partial derivatives of the fields with respect to time:
These derivatives are crucial for the prediction of field propagation in the form of electromagnetic waves. Since the surface is taken to be time-independent, we can make the following transition in Faraday's law:
see differentiation under the integral sign for more on this result.
Conceptual descriptions.
Gauss's law.
Gauss's law describes the relationship between a static electric field and the electric charges that cause it: The static electric field points away from positive charges and towards negative charges. In the field line description, electric field lines begin only at positive electric charges and end only at negative electric charges. 'Counting' the number of field lines passing through a closed surface, therefore, yields the total charge (including bound charge due to polarization of material) enclosed by that surface divided by dielectricity of free space (the vacuum permittivity). More technically, it relates the electric flux through any hypothetical closed "Gaussian surface" to the enclosed electric charge.
Gauss's law for magnetism.
Gauss's law for magnetism states that there are no "magnetic charges" (also called magnetic monopoles), analogous to electric charges. Instead, the magnetic field due to materials is generated by a configuration called a dipole. Magnetic dipoles are best represented as loops of current but resemble positive and negative 'magnetic charges', inseparably bound together, having no net 'magnetic charge'. In terms of field lines, this equation states that magnetic field lines neither begin nor end but make loops or extend to infinity and back. In other words, any magnetic field line that enters a given volume must somewhere exit that volume. Equivalent technical statements are that the sum total magnetic flux through any Gaussian surface is zero, or that the magnetic field is a solenoidal vector field.
Faraday's law.
The Maxwell-Faraday's equation version of Faraday's law describes how a time varying magnetic field creates ("induces") an electric field. This dynamically induced electric field has closed field lines just as the magnetic field, if not superposed by a static (charge induced) electric field. This aspect of electromagnetic induction is the operating principle behind many electric generators: for example, a rotating bar magnet creates a changing magnetic field, which in turn generates an electric field in a nearby wire.
Ampère's law with Maxwell's addition.
Ampère's law with Maxwell's addition states that magnetic fields can be generated in two ways: by electrical current (this was the original "Ampère's law") and by changing electric fields (this was "Maxwell's addition").
Maxwell's addition to Ampère's law is particularly important: it shows that not only does a changing magnetic field induce an electric field, but also a changing electric field induces a magnetic field. Therefore, these equations allow self-sustaining "electromagnetic waves" to travel through empty space (see electromagnetic wave equation).
The speed calculated for electromagnetic waves, which could be predicted from experiments on charges and currents, exactly matches the speed of light; indeed, light "is" one form of electromagnetic radiation (as are X-rays, radio waves, and others). Maxwell understood the connection between electromagnetic waves and light in 1861, thereby unifying the theories of electromagnetism and optics.
Vacuum equations, electromagnetic waves and speed of light.
In a region with no charges ("ρ" = 0) and no currents (J = 0), such as in a vacuum, Maxwell's equations reduce to:
Taking the curl (∇×) of the curl equations, and using the curl of the curl identity ∇×(∇×X) = ∇(∇·X) − ∇2X we obtain the wave equations
which identify
with the speed of light in free space. In materials with relative permittivity "εr" and relative permeability "μr", the phase velocity of light becomes
which is usually less than "c".
In addition, E and B are mutually perpendicular to each other and the direction of wave propagation, and are in phase with each other. A sinusoidal plane wave is one special solution of these equations. Maxwell's equations explain how these waves can physically propagate through space. The changing magnetic field creates a changing electric field through Faraday's law. In turn, that electric field creates a changing magnetic field through Maxwell's addition to Ampère's law. This perpetual cycle allows these waves, now known as electromagnetic radiation, to move through space at velocity "c".
"Microscopic" versus "macroscopic".
The "microscopic" variant of Maxwell's equation expresses the electric E field and the magnetic B field in terms of the "total charge" and total "current" present including the charges and currents at the atomic level. It is sometimes called the general form of Maxwell's equations or "Maxwell's equations in a vacuum". The macroscopic variant of Maxwell's equation is equally general, however, with the difference being one of bookkeeping.
"Maxwell's macroscopic equations", also known as Maxwell's equations in matter, are more similar to those that Maxwell introduced himself.
Unlike the "microscopic" equations, the "macroscopic" equations separate out the bound charge "Q"b and current "I"b to obtain equations that depend only on the free charges "Q"f and currents "I"f. This factorization can be made by splitting the total electric charge and current as follows:
Correspondingly, the total current density J splits into free Jf and bound Jb components, and similarly the total charge density "ρ" splits into free "ρ"f and bound "ρ"b parts.
The cost of this factorization is that additional fields, the displacement field D and the magnetizing field H, are defined and need to be determined. Phenomenological constituent equations relate the additional fields to the electric field E and the magnetic B-field, often through a simple linear relation.
For a detailed description of the differences between the microscopic ("total" charge and current including material contributes or in air/vacuum)
and macroscopic ("free" charge and current; practical to use on materials) variants of Maxwell's equations, see below.
Bound charge and current.
When an electric field is applied to a dielectric material its molecules respond by forming microscopic electric dipoles – their atomic nuclei move a tiny distance in the direction of the field, while their electrons move a tiny distance in the opposite direction. This produces a "macroscopic" "bound charge" in the material even though all of the charges involved are bound to individual molecules. For example, if every molecule responds the same, similar to that shown in the figure, these tiny movements of charge combine to produce a layer of positive bound charge on one side of the material and a layer of negative charge on the other side. The bound charge is most conveniently described in terms of the polarization P of the material, its dipole moment per unit volume. If P is uniform, a macroscopic separation of charge is produced only at the surfaces where P enters and leaves the material. For non-uniform P, a charge is also produced in the bulk.
Somewhat similarly, in all materials the constituent atoms exhibit magnetic moments that are intrinsically linked to the angular momentum of the components of the atoms, most notably their electrons. The connection to angular momentum suggests the picture of an assembly of microscopic current loops. Outside the material, an assembly of such microscopic current loops is not different from a macroscopic current circulating around the material's surface, despite the fact that no individual charge is traveling a large distance. These "bound currents" can be described using the magnetization M.
The very complicated and granular bound charges and bound currents, therefore, can be represented on the macroscopic scale in terms of P and M which average these charges and currents on a sufficiently large scale so as not to see the granularity of individual atoms, but also sufficiently small that they vary with location in the material. As such, "Maxwell's macroscopic equations" ignore many details on a fine scale that can be unimportant to understanding matters on a gross scale by calculating fields that are averaged over some suitable volume.
Auxiliary fields, polarization and magnetization.
The "definitions" (not constitutive relations) of the auxiliary fields are:
where P is the polarization field and M is the magnetization field which are defined in terms of microscopic bound charges and bound currents respectively. The macroscopic bound charge density "ρ"b and bound current density Jb in terms of polarization P and magnetization M are then defined as
If we define the free, bound, and total charge and current density by
and use the defining relations above to eliminate D, and H, the "macroscopic" Maxwell's equations reproduce the "microscopic" equations.
Constitutive relations.
In order to apply 'Maxwell's macroscopic equations', it is necessary to specify the relations between displacement field D and the electric field E, as well as the magnetizing field H and the magnetic field B. Equivalently, we have to specify the dependence of the polarisation P (hence the bound charge) and the magnetisation M (hence the bound current) on the applied electric and magnetic field. The equations specifying this response are called constitutive relations. For real-world materials, the constitutive relations are rarely simple, except approximately, and usually determined by experiment. See the main article on constitutive relations for a fuller description.
For materials without polarisation and magnetisation ("vacuum"), the constitutive relations are
for scalar constants "ε"0 and "μ"0. Since there is no bound charge, the total and the free charge and current are equal.
More generally, for linear materials the constitutive relations are
where "ε" is the permittivity and "μ" the permeability of the material. Even the linear case can have various complications, however.
Even more generally, in the case of non-linear materials (see for example nonlinear optics), D and P are not necessarily proportional to E, similarly B is not necessarily proportional to H or M. In general D and H depend on both E and B, on location and time, and possibly other physical quantities.
In applications one also has to describe how the free currents and charge density behave in terms of E and B possibly coupled to other physical quantities like pressure, and the mass, number density, and velocity of charge-carrying particles. E.g., the original equations given by Maxwell (see History of Maxwell's equations) included Ohms law in the form
Equations in Gaussian units.
Gaussian units are a popular system of units, that is part of the centimetre–gram–second system of units (cgs). When using cgs units it is conventional to use a slightly different definition of electric field Ecgs = "c"−1 ESI. This implies that the modified electric and magnetic field have the same units (in the SI convention this is not the case: e.g. for EM waves in vacuum, |E|SI, making dimensional analysis of the equations different). Then it uses a unit of charge defined in such a way that the permittivity of the vacuum "ε"0 = 1/(4"πc"), hence "μ"0 = 4"π"/"c".
Using these different conventions, the Maxwell equations become:
Alternative formulations.
Following is a summary of some of the numerous other ways to write the microscopic Maxwell's equations, showing they can be formulated using different points of view and mathematical formalisms that describe the same physics. Often, they are also called the Maxwell equations. The direct space–time formulations make manifest that the Maxwell equations are relativistically invariant (in fact studying the hidden symmetry of the vector calculus formulation was a major source of inspiration for relativity theory). In addition, the formulation using potentials was originally introduced as a convenient way to solve the equations but with all the observable physics contained in the fields. The potentials play a central role in quantum mechanics, however, and act quantum mechanically with observable consequences even when the fields vanish (Aharonov–Bohm effect). See the main articles for the details of each formulation. SI units are used throughout.
where
Other formulations include the geometric algebra formulation and a matrix representation of Maxwell's equations. Historically, a quaternionic formulation was used.
Solutions.
Maxwell's equations are partial differential equations that relate the electric and magnetic fields to each other and to the electric charges and currents. Often, the charges and currents are themselves dependent on the electric and magnetic fields via the Lorentz force equation and the constitutive relations. These all form a set of coupled partial differential equations, which are often very difficult to solve. In fact, the solutions of these equations encompass all the diverse phenomena in the entire field of classical electromagnetism. A thorough discussion is far beyond the scope of the article, but some general notes follow.
Like any differential equation, boundary conditions and initial conditions are necessary for a unique solution. For example, even with no charges and no currents anywhere in spacetime, many solutions to Maxwell's equations are possible, not just the obvious solution E = B = 0. Another solution is E = constant, B = constant, while yet other solutions have electromagnetic waves filling spacetime. In some cases, Maxwell's equations are solved through infinite space, and boundary conditions are given as asymptotic limits at infinity. In other cases, Maxwell's equations are solved in just a finite region of space, with appropriate boundary conditions on that region: For example, the boundary could be an artificial absorbing boundary representing the rest of the universe, or periodic boundary conditions, or (as with a waveguide or cavity resonator) the boundary conditions may describe the walls that isolate a small region from the outside world.
Jefimenko's equations (or the closely related Liénard–Wiechert potentials) are the explicit solution to Maxwell's equations for the electric and magnetic fields created by any given distribution of charges and currents. It assumes specific initial conditions to obtain the so-called "retarded solution", where the only fields present are the ones created by the charges. Jefimenko's equations are not so helpful in situations when the charges and currents are themselves affected by the fields they create.
Numerical methods for differential equations can be used to approximately solve Maxwell's equations when an exact solution is impossible. These methods usually require a computer, and include the finite element method and finite-difference time-domain method. For more details, see Computational electromagnetics.
Maxwell's equations "seem" overdetermined, in that they involve six unknowns (the three components of E and B) but eight equations (one for each of the two Gauss's laws, three vector components each for Faraday's and Ampere's laws). (The currents and charges are not unknowns, being freely specifiable subject to charge conservation.) This is related to a certain limited kind of redundancy in Maxwell's equations: It can be proven that any system satisfying Faraday's law and Ampere's law "automatically" also satisfies the two Gauss's laws, as long as the system's initial condition does. Although it is possible to simply ignore the two Gauss's laws in a numerical algorithm (apart from the initial conditions), the imperfect precision of the calculations can lead to ever-increasing violations of those laws. By introducing dummy variables characterizing these violations, the four equations become not overdetermined after all. The resulting formulation can lead to more accurate algorithms that take all four laws into account.
Limitations for a theory of electromagnetism.
While Maxwell's equations (along with the rest of classical electromagnetism) are extraordinarily successful at explaining and predicting a variety of phenomena, they are not exact, but approximations. In some special situations, they can be noticeably inaccurate. Examples include extremely strong fields (see Euler–Heisenberg Lagrangian) and extremely short distances (see vacuum polarization). Moreover, various phenomena occur in the world even though Maxwell's equations predict them to be impossible, such as "nonclassical light" and quantum entanglement of electromagnetic fields (see quantum optics). Finally, any phenomenon involving individual photons, such as the photoelectric effect, Planck's law, the Duane–Hunt law, single-photon light detectors, etc., would be difficult or impossible to explain if Maxwell's equations were exactly true, as Maxwell's equations do not involve photons. For the most accurate predictions in all situations, Maxwell's equations have been superseded by quantum electrodynamics.
Variations.
Popular variations on the Maxwell equations as a classical theory of electromagnetic fields are relatively scarce because the standard equations have stood the test of time remarkably well.
Magnetic monopoles.
Maxwell's equations posit that there is electric charge, but no magnetic charge (also called magnetic monopoles), in the universe. Indeed, magnetic charge has never been observed (despite extensive searches) and may not exist. If they did exist, both Gauss's law for magnetism and Faraday's law would need to be modified, and the resulting four equations would be fully symmetric under the interchange of electric and magnetic fields.
Historical publications.
The developments before relativity:

</doc>
<doc id="19738" url="http://en.wikipedia.org/wiki?curid=19738" title="Metrization theorem">
Metrization theorem

In topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_1 is said to be metrizable if there is a metric 
such that the topology induced by "d" is formula_3. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.
Properties.
Metrizable spaces inherit all topological properties from metric spaces. For example, they are Hausdorff paracompact spaces (and hence normal and Tychonoff) and first-countable. However, some properties of the metric, such as completeness, cannot be said to be inherited. This is also true of other structures linked to the metric. A metrizable uniform space, for example, may have a different set of contraction maps than a metric space to which it is homeomorphic.
Metrization theorems.
One of the first widely-recognized metrization theorems was Urysohn's metrization theorem. This states that every Hausdorff second-countable regular space is metrizable. So, for example, every second-countable manifold is metrizable. (Historical note: The form of the theorem shown here was in fact proved by Tychonoff in 1926. What Urysohn had shown, in a paper published posthumously in 1925, was that every second-countable "normal" Hausdorff space is metrizable). The converse does not hold: there exist metric spaces that are not second countable, for example, an uncountable set endowed with the discrete metric. The Nagata–Smirnov metrization theorem, described below, provides a more specific theorem where the converse does hold.
Several other metrization theorems follow as simple corollaries to Urysohn's Theorem. For example, a compact Hausdorff space is metrizable if and only if it is second-countable.
Urysohn's Theorem can be restated as: A topological space is separable and metrizable if and only if it is regular, Hausdorff and second-countable. The Nagata–Smirnov metrization theorem extends this to the non-separable case. It states that a topological space is metrizable if and only if it is regular, Hausdorff and has a σ-locally finite base. A σ-locally finite base is a base which is a union of countably many locally finite collections of open sets. For a closely related theorem see the Bing metrization theorem.
Separable metrizable spaces can also be characterized as those spaces which are homeomorphic to a subspace of the Hilbert cube formula_4, i.e. the countably infinite product of the unit interval (with its natural subspace topology from the reals) with itself, endowed with the product topology. 
A space is said to be locally metrizable if every point has a metrizable neighbourhood. Smirnov proved that a locally metrizable space is metrizable if and only if it is Hausdorff and paracompact. In particular, a manifold is metrizable if and only if it is paracompact.
Examples.
The group of unitary operators formula_5 on a separable Hilbert space formula_6 endowed
with the strong operator topology is metrizable (see Proposition II.1 in ).
Examples of non-metrizable spaces.
Non-normal spaces cannot be metrizable; important examples include
The real line with the lower limit topology is not metrizable. The usual distance function is not a metric on this space because the topology it determines is the usual topology, not the lower limit topology. This space is Hausdorff, paracompact and first countable.
The long line is locally metrizable but not metrizable; in a sense it is "too long".
References.
"This article incorporates material from on PlanetMath, which is licensed under the ."

</doc>
<doc id="19739" url="http://en.wikipedia.org/wiki?curid=19739" title="Martin Agricola">
Martin Agricola

Martin Agricola (6 January 1486 – 10 June 1556) was a German composer of Renaissance music and a music theorist.
Biography.
Agricola was born in Schwiebus in Lower Silesia.
From 1524 until his death he lived at Magdeburg, where he occupied the post of teacher or cantor in the Protestant school. The senator and music-printer Georg Rhau, of Wittenberg, was a close friend of Agricola, whose theoretical works, providing valuable material concerning the change from the old to the new system of notation, he published.
Among Agricola's other theoretical works is "Musica instrumentalis deudsch" (1529), a study of musical instruments, and one of the most important works in early organology; and one of the earliest books on the Rudiments of music.
Agricola was also the first to harmonize in four parts Martin Luther's chorale, "Ein feste Burg".

</doc>
<doc id="19740" url="http://en.wikipedia.org/wiki?curid=19740" title="Max August Zorn">
Max August Zorn

Max August Zorn (]; June 6, 1906 – March 9, 1993) was a German mathematician. He was an algebraist, group theorist, and numerical analyst. He is best known for Zorn's lemma, a powerful tool in set theory that is applicable to a wide range of mathematical constructs such as vector spaces, ordered sets, etc. Zorn's lemma was first postulated by Kazimierz Kuratowski in 1922, and then independently by Zorn in 1935.
Life and career.
Zorn was born in Krefeld, Germany. He attended the University of Hamburg. He received his Ph.D. in April 1930 for a thesis on alternative algebras. He married Alice Schlottau and they had one son, Jens, and one daughter, Liz. Jens (born June 19, 1931) is an emeritus professor of physics at the University of Michigan and an accomplished sculptor. Max Zorn's grandson Eric Zorn is a columnist for the "Chicago Tribune".
Max Zorn was appointed as an assistant at the University of Halle. However, he did not have the opportunity to work there for long since he was forced to leave Germany in 1933 because of the Nazi policies. He emigrated to the U.S. and was appointed a Sterling Fellow at Yale University. After that, he moved to UCLA and remained until 1946. He left UCLA to become a professor at Indiana University. He held this position from 1946 until he retired in 1971. He was thesis advisor for Israel Nathan Herstein.
Zorn died in Bloomington, Indiana, United States, in March 1993, of congestive heart failure, according to his obituary in "The New York Times".

</doc>
<doc id="19745" url="http://en.wikipedia.org/wiki?curid=19745" title="Main (river)">
Main (river)

The Main (]) is a river in Germany. With a length of 527 km (including the White Main: 574 km), it is the longest right tributary of the Rhine, and the longest river lying entirely in Germany.
Geography.
The Main River flows through the German states of Bavaria, Baden-Württemberg (forming the border with Bavaria for some distance) and Hesse. Its basin competes with the Danube for water; as a result, many of its boundaries are identical with those of the European Watershed.
The Main begins near Kulmbach in Franconia at the joining of its two headstreams, the Red Main ("Roter Main") and the White Main ("Weißer Main"). The Red Main originates in the Franconian Jura mountain range, 50 km in length, and runs through Creussen and Bayreuth. The White Main originates in the mountains of the Fichtelgebirge; it is 41 km long. In its upper and middle section it runs through the valleys of the German Highlands. Its lower section crosses the Lower Main Lowlands (Hanau-Seligenstadt Basin and northern Upper Rhine Plain) to Wiesbaden, where it discharges into the Rhine. Major tributaries of the Main are the Regnitz, the Franconian Saale, the Tauber, and the Nidda.
The name derives from the Latin "Moenus" or "Menus", and is not related to the name of the city Mainz (Latin: "Moguntiacum").
Navigation.
The Main is navigable for shipping from its mouth at the Rhine close to Mainz for 396 km to Bamberg. Since 1992, the Main has been connected to the Danube via the Rhine-Main-Danube Canal and the highly regulated Altmühl river. The Main has been canalized with 34 large locks (300 ×) to allow CEMT class V (110 ×) vessels to navigate the total length of the river. The 16 locks in the adjacent Rhine-Main-Danube Canal and the Danube itself are of the same dimensions.
Tributaries.
Tributaries from source to mouth:
Ports and municipalities.
Around Frankfurt are several large inland ports. Because the river is rather narrow on many of the upper reaches, navigation with larger vessels and push convoys requires great skill.
The largest cities along the Main are Frankfurt am Main and Würzburg. The Main also passes the following towns and cities: Burgkunstadt, Lichtenfels, Bad Staffelstein, Eltmann, Haßfurt, Schweinfurt, Volkach, Kitzingen, Marktbreit, Ochsenfurt, Karlstadt, Gemünden, Lohr, Marktheidenfeld, Wertheim, Miltenberg, Obernburg, Erlenbach/Main, Aschaffenburg, Seligenstadt, Hainburg, Hanau, Offenbach, Hattersheim, Flörsheim, and Rüsselsheim.
The river has gained enormous importance as a vital part of European "Corridor VII", the inland waterway link from the North Sea to the Black Sea.
Main line.
In a historical and political sense, the Main line is referred to as the northern border of Southern Germany, with its predominantly Catholic population. The river roughly marked the southern border of the North German Federation, established in 1867 under Prussian leadership as the predecessor of the German Empire.
The river course also corresponds with the Speyer line isogloss between Central and Upper German dialects, sometimes mocked as "Weißwurstäquator".
Recreation.
The Main-Radweg is a major German bicycle path running along the Main River. It is approximately 600 km long and was the first long-distance bicycle path to be awarded 5 stars by the General German Bicycle Club ADFC in 2008. It starts from either Creußen or Bischofsgrün and ends in Mainz.

</doc>
<doc id="19747" url="http://en.wikipedia.org/wiki?curid=19747" title="Marcus Vipsanius Agrippa">
Marcus Vipsanius Agrippa

Agrippina
Marcus Vipsanius Agrippa (; 23 October or November 64/63 BC – 12 BC) was a Roman statesman, general and architect. He was a close friend, son-in-law, and lieutenant to Augustus and was responsible for the construction of some of the most beautiful buildings in the history of Rome and for important military victories, most notably at the Battle of Actium against the forces of Mark Antony and Cleopatra. As a result of these victories Octavian became the first Roman Emperor, adopting the name of Augustus. Agrippa assisted Augustus in making Rome a city of marble and renovating aqueducts to give all Romans, from every social class, access to the highest quality public services. He was responsible for the creation of many baths, porticoes and gardens and was once thought to have commissioned the construction of the Pantheon. Agrippa was also father-in-law to the second Emperor Tiberius, maternal grandfather to Caligula, and maternal great-grandfather to the Emperor Nero.
Early life.
Agrippa was born between 23 October and 23 November in 64–62 BC, in an uncertain location. His father was perhaps called Lucius Vipsanius Agrippa. He had an elder brother whose name was also Lucius Vipsanius Agrippa, and a sister named Vipsania Polla. The family had not been prominent in Roman public life. However, Agrippa was about the same age as Octavian (the future emperor Augustus), and the two were educated together and became close friends. Despite Agrippa's association with the family of Julius Caesar, his elder brother chose another side in the civil wars of the 40s BC, fighting under Cato against Caesar in Africa. When Cato's forces were defeated, Agrippa's brother was taken prisoner but freed after Octavian interceded on his behalf.
It is not known whether Agrippa fought against his brother in Africa, but he probably served in Caesar's campaign of 46–45 BC against Gnaeus Pompeius, which culminated in the Battle of Munda. Caesar regarded him highly enough to send him with Octavius in 45 BC to study in Apollonia with the Macedonian legions, while Caesar consolidated his power in Rome. It was in the fourth month of their stay in Apollonia that the news of Julius Caesar's assassination in March 44 BC reached them. Agrippa and another friend, Quintus Salvidienus Rufus, advised Octavius to march on Rome with the troops from Macedonia, but Octavius decided to sail to Italy with a small retinue. After his arrival, he learned that Caesar had adopted him as his legal heir. Octavius at this time took Caesar's name, but is referred to by modern historians as "Octavian" during this period.
Rise to Power.
After Octavian's return to Rome, he and his supporters realised they needed the support of legions. Agrippa helped Octavian to levy troops in Campania. Once Octavian had his legions, he made a pact with Mark Antony and Lepidus, legally established in 43 BC as the Second Triumvirate. Octavian and his consular colleague Quintus Pedius arranged for Caesar's assassins to be prosecuted in their absence, and Agrippa was entrusted with the case against Gaius Cassius Longinus. It may have been in the same year that Agrippa began his political career, holding the position of Tribune of the Plebs, which granted him entry to the Senate.
In 42 BC, Agrippa probably fought alongside Octavian and Antony in the Battle of Philippi. After their return to Rome, he played a major role in Octavian's war against Lucius Antonius and Fulvia Antonia, respectively the brother and wife of Mark Antony, which began in 41 BC and ended in the capture of Perusia in 40 BC. However, Salvidienus remained Octavian's main general at this time. After the Perusine war, Octavian departed for Gaul, leaving Agrippa as urban praetor in Rome with instructions to defend Italy against Sextus Pompeius, an opponent of the Triumvirate who was now occupying Sicily. In July 40, while Agrippa was occupied with the Ludi Apollinares that were the praetor's responsibility, Sextus began a raid in southern Italy. Agrippa advanced on him, forcing him to withdraw. However, the Triumvirate proved unstable, and in August 40 both Sextus and Antony invaded Italy (but not in an organized alliance). Agrippa's success in retaking Sipontum from Antony helped bring an end to the conflict. Agrippa was among the intermediaries through whom Antony and Octavian agreed once more upon peace. During the discussions Octavian learned that Salvidienus had offered to betray him to Antony, with the result that Salvidienus was prosecuted and either executed or committed suicide. Agrippa was now Octavian's leading general.
In 39 or 38 BC, Octavian appointed Agrippa governor of Transalpine Gaul, where in 38 he put down a rising of the Aquitanians. He also fought the Germanic tribes, becoming the next Roman general to cross the Rhine after Julius Caesar. He was summoned back to Rome by Octavian to assume the consulship for 37 BC. He was well below the usual minimum age of 43, but Octavian had suffered a humiliating naval defeat against Sextus Pompey and needed his friend to oversee the preparations for further warfare. Agrippa refused the offer of a triumph for his exploits in Gaul – on the grounds, says Dio, that he thought it improper to celebrate during a time of trouble for Octavian. Since Sextus Pompeius had command of the sea on the coasts of Italy, Agrippa's first care was to provide a safe harbour for his ships. He accomplished this by cutting through the strips of land which separated the Lacus Lucrinus from the sea, thus forming an outer harbour, while joining the lake Avernus to the Lucrinus to serve as an inner harbor. The new harbor-complex was named Portus Julius in Octavian's honour. Agrippa was also responsible for technological improvements, including larger ships and an improved form of grappling hook. About this time, he married Caecilia Pomponia Attica, daughter of Cicero's friend Titus Pomponius Atticus.
In 36 BC, Octavian and Agrippa set sail against Sextus. The fleet was badly damaged by storms and had to withdraw; Agrippa was left in charge of the second attempt. Thanks to superior technology and training, Agrippa and his men won decisive victories at Mylae and Naulochus, destroying all but seventeen of Sextus' ships and compelling most of his forces to surrender. Octavian, with his power increased, forced the triumvir Lepidus into retirement and entered Rome in triumph. Agrippa received the unprecedented honour of a naval crown decorated with the beaks of ships; as Dio remarks, this was "a decoration given to nobody before or since".
Life in public service.
Agrippa participated in smaller military campaigns in 35 and 34 BC, but by the autumn of 34 he had returned to Rome. He rapidly set out on a campaign of public repairs and improvements, including renovation of the aqueduct known as the Aqua Marcia and an extension of its pipes to cover more of the city. Through his actions after being elected in 33 BC as one of the aediles (officials responsible for Rome's buildings and festivals), the streets were repaired and the sewers were cleaned out, while lavish public spectacles were put on. Agrippa signalled his tenure of office by effecting great improvements in the city of Rome, restoring and building aqueducts, enlarging and cleansing the Cloaca Maxima, constructing baths and porticos, and laying out gardens. He also gave a stimulus to the public exhibition of works of art. It was unusual for an ex-consul to hold the lower-ranking position of aedile, but Agrippa's success bore out this break with tradition. As emperor, Augustus would later boast that "he had found the city of brick but left it of marble", thanks in part to the great services provided by Agrippa under his reign.
Antony and Cleopatra.
Agrippa was again called away to take command of the fleet when the war with Antony and Cleopatra broke out. He captured the strategically important city of Methone at the southwest of the Peloponnese, then sailed north, raiding the Greek coast and capturing Corcyra (modern Corfu). Octavian then brought his forces to Corcyra, occupying it as a naval base. Antony drew up his ships and troops at Actium, where Octavian moved to meet him. Agrippa meanwhile defeated Antony's supporter Quintus Nasidius in a naval battle at Patrae. Dio relates that as Agrippa moved to join Octavian near Actium, he encountered Gaius Sosius, one of Antony's lieutenants, who was making a surprise attack on the squadron of Lucius Tarius, a supporter of Octavian. Agrippa's unexpected arrival turned the battle around.
As the decisive battle approached, according to Dio, Octavian received intelligence that Antony and Cleopatra planned to break past his naval blockade and escape. At first he wished to allow the flagships past, arguing that he could overtake them with his lighter vessels and that the other opposing ships would surrender when they saw their leaders' cowardice. Agrippa objected that Antony's ships, although larger, could outrun Octavian's if they hoisted sails, and that Octavian ought to fight now because Antony's fleet had just been struck by storms. Octavian followed his friend's advice.
On September 2, 31 BC, the Battle of Actium was fought. Octavian's victory, which gave him the mastery of Rome and the empire, was mainly due to Agrippa. As a token of signal regard, Octavian bestowed upon him the hand of his niece Claudia Marcella Major in 28 BC. He also served a second consulship with Octavian the same year. In 27 BC, Agrippa held a third consulship with Octavian, and in that year, the senate also bestowed upon Octavian the imperial title of Augustus.
In commemoration of the Battle of Actium, Agrippa built and dedicated the building that served as the Roman Pantheon before its destruction in 80AD. Emperor Hadrian used Agrippa's design to build his own Pantheon, which survives in Rome. The inscription of the later building, which was built around 125, preserves the text of the inscription from Agrippa's building during his third consulship. The years following his third consulship, Agrippa spent in Gaul, reforming the provincial administration and taxation system, along with building an effective road system and aqueducts.
Late life.
Agrippa's friendship with Augustus seems to have been clouded by the jealousy of Augustus' nephew Marcus Claudius Marcellus, which was probably fomented by the intrigues of Livia, the third wife of Augustus, who feared his influence over her husband. Traditionally it is said the result of such jealousy was that Agrippa left Rome, ostensibly to take over the governorship of eastern provinces – a sort of honourable exile, but he only sent his legate to Syria, while he himself remained at Lesbos and governed by proxy, though he may have been on a secret mission to negotiate with the Parthians about the return of the Roman legions standards which they held. On the death of Marcellus, which took place within a year of his exile, he was recalled to Rome by Augustus, who found he could not dispense with his services. However, if one places the events in the context of the crisis in 23 BC it seems unlikely that, when facing significant opposition and about to make a major political climb down, the emperor Augustus would place a man in exile in charge of the largest body of Roman troops. What is far more likely is that Agrippa's 'exile' was actually the careful political positioning of a loyal lieutenant in command of a significant army as a backup plan in case the settlement plans of 23 BC failed and Augustus needed military support. Moreover, after 23 BC as part of what became known as Augustus' "Second Constitutional Settlement", Agrippa's constitutional powers were greatly increased to provide the Principate of Augustus with greater constitutional stability by providing for a political heir or replacement for Augustus if he were to succumb to his habitual ill health or was assassinated. In the course of the year proconsular imperium, similar to Augustus' power, was conferred upon Agrippa for five years. The exact nature of the grant is uncertain but it probably covered Augustus' imperial provinces, east and west, perhaps lacking authority over the provinces of the Senate. That was to come later, as was the jealously guarded tribunicia potestas, or powers of a tribune of the plebeians. These great powers of state are not usually heaped upon a former exile. 
It is said that Maecenas advised Augustus to attach Agrippa still more closely to him by making him his son-in-law. He accordingly induced him to divorce Marcella and marry his daughter Julia the Elder by 21 BC, the widow of Marcellus, equally celebrated for her beauty, abilities, and her shameless profligacy. In 19 BC, Agrippa was employed in putting down a rising of the Cantabrians in Hispania (Cantabrian Wars). 
In 18 BC, Agrippa's powers were even further increased to almost match those of Augustus. That year his proconsular imperium was augmented to cover the provinces of the Senate. More than that, he was finally granted tribunicia potestas, or powers of a tribune of the plebeians. As was the case with Augustus, Agrippa’s grant of tribunician powers was conferred without his having to actually hold that office. These powers were considerable, giving him veto power over the acts of the Senate or other magistracies, including those of other tribunes, and the power to present laws for approval by the People. Just as important, a tribune’s person was sacrosanct, or sacred, meaning that any person who harmfully touched them or impeded their actions, including political acts, could lawfully be killed. After the grant of these powers Agrippa was, on paper, almost as powerful as Augustus was. However, there was no doubt that Augustus was the man in charge. 
Agrippa was appointed governor of the eastern provinces a second time in 17 BC, where his just and prudent administration won him the respect and good-will of the provincials, especially from the Jewish population. Agrippa also restored effective Roman control over the Cimmerian Chersonnese (Crimean Peninsula) during his governorship.
Agrippa’s last public service was his beginning of the conquest of the upper Danube River region, which would become the Roman province of Pannonia in 13 BC. He died at Campania in 12 BC at the age of 51. His posthumous son, Marcus Vipsanius Agrippa Postumus, was named in his honor. Augustus honoured his memory by a magnificent funeral and spent over a month in mourning. Augustus personally oversaw all of Agrippa's children’s educations. Although Agrippa had built a tomb for himself, Augustus had Agrippa's remains placed in Augustus' own mausoleum.
Legacy.
Agrippa was also known as a writer, especially on the subject of geography. Under his supervision, Julius Caesar's dream of having a complete survey of the Empire made was carried out. Agrippa constructed a circular chart, which was later engraved on marble by Augustus, and afterwards placed in the colonnade built by his sister Polla. Amongst his writings, an autobiography, now lost, is referenced.
Marriages and issue.
Agrippa had several children through his three marriages:
Descendants.
Through his numerous children, Agrippa would become ancestor to many subsequent members of the Julio-Claudian dynasty, whose position he helped to attain, as well as many other reputed Romans.
There have been some attempts to assign further descendants to a number of the aforementioned figures, including two lines of Asinii descended from Gaius Asinius Pollio and Marcus Asinius Agrippa respectively. A daughter (and further descendants) named Rubellia Bassa to Julia, who may have been a daughter of Gaius Rubellius Blandus by an earlier marriage. And, finally, a series of descendants from Junia Lepida and her husband, Gaius Cassius Longinus. However, all of these lines of descent are extremely hypothetical and lack any evidence to support a connection to the descendants of Agrippa.
Agrippa in popular culture.
Drama.
Agrippa is a character in William Shakespeare's play "Antony and Cleopatra".
A fictional version of Agrippa in his later life played a prominent role in the 1976 BBC Television series "I, Claudius". Agrippa was portrayed as a much older man, though he would have only been 39 years old at the time of the first episode (24/23 BC). He was played by John Paul.
Agrippa is the main character in Paul Naschy's 1980 film "Los cántabros", played by Naschy himself. It is a highly fictionalized version of the Cantabrian Wars in which Agrippa is depicted as the lover of the sister of Cantabrian leader Corocotta.
Agrippa appears in several film versions of the life of Cleopatra. He is normally portrayed as an old man rather than a young one. Among the people to portray him are Philip Locke, Alan Rowe and Andrew Keir.
Agrippa is also one of the principal characters in the British/Italian joint project "" (2003) featuring flashbacks between Augustus and Julia about Agrippa, which shows him in his youth on serving in Caesar's army up until his victory at Actium and the defeat of Cleopatra. He is portrayed by Ken Duken. In the 2005 series "Empire" the young Agrippa (played by Christopher Egan) becomes Octavian's sidekick after saving him from an attempted poisoning.
Marcus Agrippa, a highly fictional character based on Marcus Vipsanius Agrippa's early life, is part of the BBC-HBO-RAI television series "Rome". He is played by Allen Leech. He describes himself as the grandson of a slave. The series creates a romantic relationship between Agrippa and Octavian's sister Octavia Minor, for which there is no historical evidence.
Literature.
Agrippa is a main character in the early part of Robert Graves novel "I, Claudius". He is a main character in the later two novels of Colleen McCullough's Masters of Rome series. He is a featured character of prominence and importance in the historical fiction novel "Cleopatra's Daughter" by Michelle Moran. He also features prominently in John Edward Williams' historical novel Augustus.
Video games.
A heavily fictionalized version of Agrippa is one of the playable characters (the other being an equally fictionalized Augustus) in the video game "Shadow of Rome". There, Agrippa is sentenced to become a gladiator after his father was wrongly sentenced for assassinating Caesar. Agrippa's goal is to stay alive as a gladiator for as long as possible, while Augustus acts as an infiltrator who slowly exposes the conspiracy against Caesar. Eventually, Augustus is able to prove Vipsanius' innocence and both of them are pardoned. Then a civil war breaks out, because the direct successor was outraged by exposure of the conspiracy. Agrippa and Augustus fight against Antonius.

</doc>
<doc id="19757" url="http://en.wikipedia.org/wiki?curid=19757" title="Mariotto Albertinelli">
Mariotto Albertinelli

Mariotto di Bigio di Bindo Albertinelli (October 13, 1474 – November 5, 1515) was a High Renaissance Italian painter of the Florentine school, closely involved with Fra Bartolomeo and influenced by Raphael.
Biography.
Mariotto Albertinelli was born in Florence. As a 12-year-old boy, he became a pupil of Cosimo Rosselli, and a fellow-pupil with Fra Bartolomeo with whom he formed such an intimate brotherly rapport that in 1494 the two started their own studio in Florence. Vasari's opinion was that Mariotto was not so well grounded in drawing as Bartolomeo, and he tells that, to improve his hand he had taken to drawing the antiquities in the Medici garden, where he was encouraged by Madonna Alfonsina, the mother of Lorenzo de' Medici.
When the Medici were temporarily banished in 1494, he returned to his friend, whose manner he copied so assiduously, according to Vasari, that his works were taken for Baccio's. When, in the wake of Savonarola's morality campaign, Baccio joined the Dominican order as Fra Bartolomeo in 1500 and gave up painting, Albertinelli, beside himself with the loss, would have joined him; but, spurred by his success in completing an unfinished "Last Judgment" of Bartolomeo's, he resolved to carry on alone. Among his many students were Jacopo da Pontormo, Innocenzo di Pietro Francucci da Imola and Giuliano Bugiardini.
Mariotto was a most restless person and carnal in the affairs of love and apt to the art of living, and, taking a dislike to the studies and brain-wracking necessary to painting, being also often stung by the tongues of other painters, as is their way, he resolved to give himself to a less laborious and more jovial profession, and so opened the most lovely hostelry outside the Porta San Gallo, and at the sign of the Dragon at the Ponte Vecchio a tavern and inn. This life he led for many months, saying that he had taken up an art that was without muscles, foreshortening or perspective and, better still, without faultfinding, and that the art that he had given up imitated flesh and blood, but this one created flesh and blood; in this if you had good wine you heard yourself praised, but in that every day you were blamed. But at last the low life became an annoyance to him, and, filled with remorse, he returned to painting.
Albertinelli's paintings bear the imprint of Perugino's sense of volumes in space and perspective, Fra Bartolomeo's coloring, the landscape portrayal of Flemish masters like Memling, and Leonardo's Sfumato technique. His chief paintings are in Florence, notably his masterpiece, the "Visitation" (1503) at the Uffizi ("illustrated right").

</doc>
<doc id="19758" url="http://en.wikipedia.org/wiki?curid=19758" title="Beijing cuisine">
Beijing cuisine

Beijing cuisine (), also known as Jing cuisine () or Mandarin cuisine and Peiping cuisine (北平菜) in Taiwan, because that was the city's name during the Republican era of China, is the cuisine of Beijing.
Background.
As Beijing has been the capital of China for centuries, its cuisine is influenced by culinary traditions from all over China, but the style that has the greatest influence on Beijing cuisine is that of the eastern coastal province of Shandong. Beijing cuisine has itself, in turn, also greatly influenced other Chinese cuisines, particularly the cuisine of Liaoning, the Chinese imperial cuisine, and the Chinese aristocrat cuisine.
Another tradition that influenced Beijing cuisine (as well as influenced by the latter itself) is the Chinese imperial cuisine that originated from the "Emperor's Kitchen" (), which referred to the cooking facilities inside the Forbidden City, where thousands of cooks from different parts of China showed their best culinary skills to please the imperial family and officials. Therefore, it is sometimes difficult to determine the actual origin of a dish as the term "Mandarin" is generalised and refers not only to Beijing, but other provinces as well. However, some generalisation of Beijing cuisine can be characterised as follows: Foods that originated in Beijing are often snacks rather than main courses, and they are typically sold by small shops or street vendors. There is emphasis on dark soy paste, sesame paste, sesame oil, and scallions, and fermented tofu is often served as a condiment. In terms of cooking techniques, methods relating to different ways of frying are often used. There is less emphasis on rice as an accompaniment as compared to many other regions in China, as local rice production in Beijing is limited by the relatively dry climate.
Dishes in Beijing cuisine that are served as main courses are mostly from other Chinese cuisines, and some of the following in particular have been central to the formation of Beijing cuisine.
Huaiyang cuisine has been praised since ancient times in China, and it was a general practice for an official travelling to Beijing to take up a new post to bring along with him a chef specialising in Huaiyang cuisine. When these officials had completed their terms in the capital and returned to their native provinces, most of the chefs they brought along often remained in Beijing. They opened their own restaurants or were hired by wealthy locals. The imperial clan of the Ming Dynasty, the House of Zhu, who had ancestry from Jiangsu, also contributed greatly in introducing Huaiyang cuisine to Beijing when the capital was moved from Nanjing to Beijing in the 15th century, because the imperial kitchen was mainly Huaiyang style. The element of traditional Beijing culinary and gastronomical cultures of enjoying artistic performances such as Beijing opera while dining directly developed from the similar practice in the culture of Jiangsu and Huaiyang cuisines. 
Chinese Islamic cuisine is another important component of Beijing cuisine, and was first prominently introduced when Beijing became the capital of the Yuan Dynasty. However, the most significant contribution to the formation of Beijing cuisine came from Shandong cuisine, as most chefs from Shandong came to Beijing en masse during the Qing Dynasty. Unlike the earlier two cuisines, which were brought by the ruling class such as nobles, aristocrats and bureaucrats, and then spread to the general populace, the introduction of Shandong cuisine begun with serving the general populace, with much wider market segment, from wealthy merchants to the working class.
History.
The Qing Dynasty was a major period in the formation of Beijing cuisine. Before the Boxer Rebellion, the foodservice establishments in Beijing were strictly stratified by the foodservice guild. Each category of the establishment was specifically based on its ability to provide for a particular segment of the market. The top ranking foodservice establishments served nobles, aristocrats, and wealthy merchants and landlords, while lower ranking foodservice establishments served the populace of lower financial and social status. It was during this period when Beijing cuisine gained fame and became recognised by the Chinese culinary society, and the stratification of the foodservice was one of its most obvious characteristics as part of its culinary and gastronomic cultures during this first peak of its formation.
The official stratification was an integral part of the local culture of Beijing and it was not finally abolished officially after the end of the Qing Dynasty, which resulted in the second peak in the formation of Beijing cuisine. Meals previously offered to nobles and aristocrats was made available to anyone who can afford them instead of being restricted only to the upper class. As chefs freely switched between jobs offered by different foodservice establishments, they brought their skills that further enriched and developed Beijing cuisine. Though the stratification of food services in Beijing was no longer effected by imperial laws, the structure more or less remained despite continuous weakening due to the financial background of the local clientele. The different classes are listed in the following subsections.
Zhuang.
Foodservice establishments with names ending with the Chinese character "zhuang" (), or "zhuang zihao" (), were the top-ranking foodservice establishments, not only in providing foods, but entertainment as well. The form of entertainment provided was usually Beijing opera, and foodservice establishments of this class always had long-term contracts with a Beijing opera troupe to perform onsite. Moreover, foodservice establishments of this class would always have long-term contracts with famous performers (such as national-treasure-class performers) to perform onsite, though not on a daily basis. Foodservice establishments of this category did not accept any different customers on a walk-in basis, but instead, only accepted customers who came as a group and ordered banquets by appointment, and the banquets provided by foodservice establishments of this category often included most, if not all tables, at the site. The bulk of the business of foodservice of this category, however, was catering at customers’ homes or other locations, and such catering was often for birthdays, marriages, funerals, promotions and other important celebrations and festivals. When catering, these foodservice establishments not only provided what was on the menu, but fulfilled customers’ requests.
Foodservice establishments categorised as "leng zhuangzi" () lacked any rooms to host banquets, and thus their business was purely catering.
Tang.
Foodservice establishments with names ending with the Chinese character "tang" (), or "tang zihao" (), are similar to foodservice establishments with names ending with the Chinese character "zhuang", but the business of these second-class foodservice establishments were generally evenly divided among onsite banquet hosting and catering (at customers’ homes). Foodservice establishments of this class would also have long-term contracts with Beijing opera troupes to perform onsite, but they did not have long-term contracts with famous performers (such as national-treasure-class performers) to perform onsite on regular basis; however these top performers would still perform at foodservice establishments of this category occasionally. In terms of catering at the customers’ sites, foodservice establishments of this category often only provided dishes strictly according to their menu, and would not provide any dishes that were not on the menu.
Ting.
Foodservice establishments with names ending with the Chinese character "ting" (), or "ting zihao" () are foodservice establishments which had more business in onsite banquet hosting than catering at customers’ homes. For onsite banquet hosting, entertainment was still provided, but foodservice establishments of this category did not have long-term contracts with Beijing opera troupes, so that performers varied from time to time, and top performers usually did not perform here or at any lower-ranking foodservice establishments. For catering, different foodservice establishments of this category were incapable of handling significant catering on their own, but generally had to combine resources with other foodservice establishments of the same ranking (or lower) to do the job.
Yuan.
Foodservice establishments with names ending with the Chinese character "yuan" (), or "yuan zihao" () did nearly all their business in hosting banquets onsite. Entertainment was not provided on a regular basis, but there were stages built onsite for Beijing opera performers. Instead of being hired by the foodservice establishments like in the previous three categories, performers at foodservice establishments of this category were usually contractors who paid the foodservice establishment to perform and split the earnings according to a certain percentage. Occasionally, foodservice establishments of this category would be called upon to help cater at customers’ homes, and like foodservice establishments with names ending with the Chinese character "ting", they could not do the job on their own but had to work with others, never taking the lead as foodservice establishments with names ending with the Chinese character "ting" could.
Lou.
Foodservice establishments with names ending with the Chinese character "lou" (), or "lou zihao" () did the bulk of their business hosting banquets onsite by appointment. In addition, a smaller portion of the business was in serving different customers onsite on a walk-in basis. Occasionally, when catering at customers’ homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for.
Ju.
Foodservice establishments with names ending with the Chinese character "ju" (), or "ju zihao" () generally divided their business evenly into two areas: serving different customers onsite on a walk-in basis, and hosting banquets by appointment for customers who came as one group. Occasionally, when catering at the customers’ homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for, just like foodservice establishments with names ending with the Chinese character "lou". However, unlike those establishments, which always cooked their specialty dishes on location, foodservice establishment of this category would either cook on location or simply bring the already-cooked food to the location.
Zhai.
Foodservice establishments with names ending with the Chinese character "zhai" (), or "zhai zihao" () were mainly in the business of serving different customers onsite on a walk-in basis, but a small portion of their income did come from hosting banquets by appointment for customers who came as one group. Just like foodservice establishments with names ending with the Chinese character "ju", when catering at customers’ homes, foodservice establishments of this category would also only provide the few specialty dishes they are famous for, but they would mostly bring the already-cooked dishes to the location, and would only cook on location occasionally.
Fang.
Foodservice establishments with names ending with the Chinese character "fang" (), or "fang zihao" (). Foodservice establishments of this category generally did not offer the service of hosting banquets made by appointment for customers who came as one group, but instead, often only offered to serve different customers onsite on a walk-in basis. Foodservice establishments of this category or lower would not be called upon to perform catering at the customers’ homes for special events.
Guan.
Foodservice establishments with names ending with the Chinese character "guan" (), or "guan zihao" (). Foodservice establishments of this category mainly served different customers onsite on a walk-in basis, and in addition, a portion of the income would be earned from selling to-goes.
Dian.
Foodservice establishments with names ending with the Chinese character "dian" (), or "dian zihao" (). Foodservice establishments of this category had their own place, like all previous categories, but serving different customers to dine onsite on a walk-in basis only provided half of the overall income, while the other half came from selling to-goes.
Pu.
Foodservice establishments with name ending with the Chinese character "pu" (), or "pu zihao" (). Foodservice establishments of this category ranked next to the last, and they were often named after the owners' last names. Foodservice establishments of this category had fixed spots of business for having their own places, but not as large as those belonging to the category of "dian", and thus did not have tables, but only seats for customers. As a result, the bulk of the income of foodservice establishments of this category was from selling to-goes, while income earned from customers dining onsite only provided a small portion of the overall income.
Tan.
Foodservice establishments with names ending with the Chinese character "tan" (), or "tan zihao" (). The lowest ranking foodservice establishments without any tables, and selling to-goes was the only form of business. In addition to name the food stand after the owners’ last name or the food sold, these food stands were also often named after the owners’ nicknames.
Restaurants known for Beijing cuisine.
Numerous traditional restaurants in Beijing are credited with great contributions in the formation of Beijing cuisine, but many of them have gone out of business as time went by. However, some of them managed to survive until today, and some of them are:

</doc>
<doc id="19760" url="http://en.wikipedia.org/wiki?curid=19760" title="Manichaeism">
Manichaeism

Manichaeism (;
in Modern Persian آیین مانی "Āyin e Māni"; ) was a major religion that was founded by the Iranian prophet Mani (in Persian: مانی, Syriac: ܡܐܢܝ, Latin: Manichaeus or Manes) ( 216–276 AD) in the Sasanian Empire.
Manichaeism taught an elaborate dualistic cosmology describing the struggle between a good, spiritual world of light, and an evil, material world of darkness. Through an ongoing process which takes place in human history, light is gradually removed from the world of matter and returned to the world of light whence it came. Its beliefs were based on local Mesopotamian gnostic and religious movements.
Manichaeism was quickly successful and spread far through the Aramaic-Syriac speaking regions. It thrived between the third and seventh centuries, and at its height was one of the most widespread religions in the world. Manichaean churches and scriptures existed as far east as China and as far west as the Roman Empire. It was briefly the main rival to Christianity in the competition to replace classical paganism. Manichaeism survived longer in the east than in the west, and it appears to have finally faded away after the 14th century in southern China, contemporary to the decline in China of the Church of the East – see Ming Dynasty. While most of Mani's original writings have been lost, numerous translations and fragmentary texts have survived.
An adherent of Manichaeism is called, especially in older sources, a "Manichee, "or more recently "Manichaean." By extension, the term "manichean" is widely applied (often intended as an insult) as an adjective to a philosophy of moral dualism, according to which a moral course of action involves a clear (or simplistic) choice between good and evil, or as a noun to people who hold such a view.
History.
Life of Mani.
Mani, an Arsacid Persian by birth, was born 216 AD in Mesopotamia (Iraq), which was ruled by Persia, then within the Sassanid Empire province of Asuristan. According to the Cologne Mani-Codex, Mani's parents were members of the religious sect of Elcesaites. The king of Persia (Bahram I) put him to death in 274 or 277.
Mani composed seven writings, six of which were written in Syriac Aramaic. The seventh, the Shabuhragan, was written by Mani in Middle Persian and presented by him to the contemporary King of Sassanid Persia, Shapur I, in the Persian capital of Ctesiphon. Although there is no proof Shapur I was a Manichaean, he tolerated the spread of Manicheanism and refrained from persecuting it in his empire's boundaries. According to one tradition it was Mani himself who invented the unique version of the Syriac script called Manichaean script, which was used in all of the Manichaean works written within the Persian Empire, whether they were in Syriac or Middle Persian, and also for most of the works written within the Uyghur Empire. The primary language of Babylon (and the administrative and cultural language of the Sassanid Empire) at that time was Eastern Middle Aramaic, which included three main dialects: Judeo-Aramaic (the language of the Talmud), Mandaean Aramaic (the language of the Mandaean religion), and Syriac Aramaic, which was the language of Mani, as well as of the Syriac Christians.
While Manichaeism was spreading, existing religions such as Christianity and Zoroastrianism were gaining social and political influence. Although having fewer adherents, Manichaeism won the support of many high-ranking political figures. With the assistance of the Persian Empire, Mani began missionary expeditions. After failing to win the favour of the next generation of Persian royalty, and incurring the disapproval of the Zoroastrian clergy, Mani is reported to have died in prison awaiting execution by the Persian Emperor Bahram I. The date of his death is estimated at AD 276–277.
Influences.
Mani believed that the teachings of Buddha, Zoroaster, and Jesus were incomplete, and that his revelations were for the entire world, calling his teachings the "Religion of Light." Manichaean writings indicate that Mani received revelations when he was 12 and again when he was 24, and over this time period he grew dissatisfied with the Elcesaite sect he was born into. Mani began preaching at an early age and was possibly influenced by contemporary Babylonian-Aramaic movements such as Mandaeanism, and Aramaic translations of Jewish apocalyptic writings similar to those found at Qumran (such as the book of Enoch literature), and by the Syriac dualist-gnostic writer Bardaisan (who lived a generation before Mani). With the discovery of the Mani-Codex, it also became clear that he was raised in a Jewish-Christian baptism sect, the Elcesaites, and was influenced by their writings as well. According to biographies preserved by Ibn al-Nadim and the Persian polymath al-Biruni, he allegedly received a revelation as a youth from a spirit, whom he would later call his "Twin" (Aramaic "Tauma" (תאומא), from which is also derived the name of the apostle Thomas, the "twin"), his "Syzygos" (Greek for "partner", in the Cologne Mani-Codex), his Double, his Protective Angel or "Divine Self". It taught him truths which he developed into a religion. His "divine" Twin or true Self brought Mani to Self-realization. He claimed to be the "Paraclete of the Truth", as promised in the New Testament.
Manichaeism's views on Jesus are described by historians:
"Jesus in Manichaeism possessed three separate identities: (1) Jesus the Luminous, (2) Jesus the Messiah and (3) Jesus patibilis (the suffering Jesus). (1) As Jesus the Luminous... his primary role was as supreme revealer and guide and it was he who woke Adam from his slumber and revealed to him the divine origins of his soul and its painful captivity by the body and mixture with matter. Jesus the Messiah was a historical being who was the prophet of the Jews and the forerunner of Mani. However, the Manicheans believed he was wholly divine. He never experienced human birth as notions of physical conception and birth filled the Manichaeans with horror and the Christian doctrine of virgin birth was regarded as equally obscene. Since he was the light of the world, where was this light, they asked, when he was in the womb of the Virgin? (2) Jesus the Messiah was truly born at his baptism as it was on that occasion that the Father openly acknowledged his sonship. The suffering, death and resurrection of this Jesus were in appearance only as they had no salvific value but were an exemplum of the suffering and eventual deliverance of the human soul and a prefiguration of Mani’s own martyrdom. (3) The pain suffered by the imprisoned Light-Particles in the whole of the visible universe, on the other hand, was real and immanent. This was symbolized by the "mystic placing of the Cross whereby the wounds of the passion of our souls are set forth". On this mystical Cross of Light was suspended the Suffering Jesus (Jesus patibilis) who was "the life and salvation of Man." This mystica cruxificio was present in every tree, herb, fruit, vegetable and even stones and the soil. This constant and universal suffering of the captive soul is exquisitely expressed in one of the Coptic Manichaean psalms" 
Historians also note that Mani declared himself to be an "apostle of Jesus Christ" Manichaean tradition is also noted to have claimed that Mani was the reincarnation of different religious figures from Buddha, Lord Krishna, Zoroaster, and Jesus.
"Other than incorporating the symbols and doctrine of dominant religious traditions, Manichaeism also incorporated the symbols and deities of indigenous traditions, in particular the Hindu deity Ganesha into its fold, demonstrated by the image available in the article, Manichaean art and calligraphy by Hans-Joachim Klimkeit. Mani was allegedly claiming to be the reincarnation of the Buddha, Lord Krishna, Zoroaster and Jesus depending on the context in which he was carrying out his preachings. Such strategic claims fostered a spirit of toleration among the Manicheans and the other religious communities and this particular feature greatly assisted them in gaining the approval of authorities to practice in different regions along the Silk Road." 
Academics also note that since much of what is known about Manichaeism comes from later 10th and 11th Century AD Islamic historians like Al-Biruni and especially the Shia Muslim Persian historian Ibn al-Nadim (and his work Fihrist), "Islamic authors ascribed to Mani the claim to be the Seal of the Prophets." This topic is discussed by an Israeli academic, Guy G. Stroumsa.
Another source of Mani's scriptures was original Aramaic writings relating to the book of Enoch literature (see the Book of Enoch and the Second Book of Enoch), as well as an otherwise unknown section of the book of Enoch called the "Book of Giants". This book was quoted directly, and expanded on by Mani, becoming one of the original six Syriac writings of the Manichaean Church. Besides brief references by non-Manichaean authors through the centuries, no original sources of "The Book of Giants" (which is actually part six of the "Book of Enoch") were available until the 20th century.
Scattered fragments of both the original Aramaic "Book of Giants" (which were analyzed and published by Józef Milik in 1976) and of the Manichaean version of the same name (analyzed and published by W.B. Henning in 1943) were found with the discovery in the twentieth century of the Dead Sea Scrolls in the Judaean Desert and the Manichaean writings of the Uyghur Manichaean kingdom in Turpan. Henning wrote in his analysis of them:
It is noteworthy that Mani, who was brought up and spent most of his life in a province of the Persian empire, and whose mother belonged to a famous Parthian family, did not make any use of the Iranian mythological tradition. There can no longer be any doubt that the Iranian names of Sām, Narīmān, etc., that appear in the Persian and Sogdian versions of the Book of the Giants, did not figure in the original edition, written by Mani in the Syriac language.
From a careful reading of the Enoch literature and the Book of Giants, alongside the description of the Manichaean myth, it becomes clear that the "Great King of Honor" of this myth (a being that sits as a guard to the world of light at the seventh of ten heavens in the Manichaean myth,) is identical with the King of Honor sitting on the heavenly throne in the Enoch literature. In the Aramaic book of Enoch, in the Qumran writings in general, and in the original Syriac section of Manichaean scriptures quoted by Theodore bar Konai, he is called "malka raba de-ikara" (the great king of honor).
Mani was also influenced by writings of the Assyrian gnostic Bardaisan (154–222), who like Mani, wrote in Syriac, and presented a dualistic interpretation of the world in terms of light and darkness, in combination with elements from Christianity.
Noting Mani's travels to the Kushan Empire (several religious paintings in Bamiyan are attributed to him) at the beginning of his proselytizing career, Richard Foltz postulates Buddhist influences in Manichaeism:
Buddhist influences were significant in the formation of Mani's religious thought. The transmigration of souls became a Manichaean belief, and the quadripartite structure of the Manichaean community, divided between male and female monks (the "elect") and lay followers (the "hearers") who supported them, appears to be based on that of the Buddhist sangha.
The Kushan monk Lokakṣema began translating Pure Land Buddhist texts into Chinese in the century prior to Mani arriving there, and the Chinese texts of Manichaeism are full of uniquely Buddhist terms taken directly from these Chinese Pure Land scriptures, including the term "Pure land" (淨土 Jìngtǔ) itself. However, the central object of veneration in Pure Land Buddhism, Amitābha, the Buddha of Infinite Light, does not appear in Chinese Manichaeism, and seems to have been replaced by another deity.
Later history.
Manichaeism continued to spread with extraordinary speed through both the east and west. It reached Rome through the apostle Psattiq by AD 280, who was also in Egypt in 244 and 251. It was flourishing in the Fayum area of Egypt in AD 290. Manichaean monasteries existed in Rome in 312 AD during the time of the Christian Pope Miltiades.
The spread and success of Manichaeism were seen as a threat to other religions, and it was widely persecuted in Hellenistic, Christian, Zoroastrian, Islamic, and Buddhist cultures.
In 291, persecution arose in the Persian empire with the murder of the apostle Sisin by Bahram II, and the slaughter of many Manichaeans. In AD 296, Diocletian decreed against the Manichaeans: "We order that their organizers and leaders be subject to the final penalties and condemned to the fire with their abominable scriptures", resulting in many martyrdoms in Egypt and North Africa (see "Diocletian Persecution"). By AD 354, Hilary of Poitiers wrote that the Manichaean faith was a significant force in southern Gaul. In AD 381 Christians requested Theodosius I to strip Manichaeans of their civil rights. He issued a decree of death for Manichaean monks in AD 382.
Augustine of Hippo (AD 354–430) converted to Christianity from Manichaeism, in the year 387. This was shortly after the Roman Emperor Theodosius I had issued a decree of death for Manichaeans in AD 382 and shortly before he declared Christianity to be the only legitimate religion for the Roman Empire in 391. According to his "Confessions," after nine or ten years of adhering to the Manichaean faith as a member of the group of "hearers", Augustine became a Christian and a potent adversary of Manichaeism (which he expressed in writing against his Manichaean opponent Faustus of Mileve), seeing their beliefs that knowledge was the key to salvation as too passive and not able to effect any change in one's life.
I still thought that it is not we who sin but some other nature that sins within us. It flattered my pride to think that I incurred no guilt and, when I did wrong, not to confess it... I preferred to excuse myself and blame this unknown thing which was in me but was not part of me. The truth, of course, was that it was all my own self, and my own impiety had divided me against myself. My sin was all the more incurable because I did not think myself a sinner. ("Confessions, Book V, Section 10")
Some modern scholars have suggested that Manichaean ways of thinking influenced the development of some of Augustine's ideas, such as the nature of good and evil, the idea of hell, the separation of groups into elect, hearers, and sinners, and the hostility to the flesh and sexual activity.
How Manichaeism may have influenced Christianity continues to be debated. Manichaeism may have influenced the Bogomils, Paulicians, and Cathars. However, these groups left few records, and the link between them and Manichaeans is tenuous. Regardless of its accuracy the charge of Manichaeism was levelled at them by contemporary orthodox opponents, who often tried to make contemporary heresies conform to those combatted by the church fathers. Whether the dualism of the Paulicians, Bogomils, and Cathars and their belief that the world was created by a Satanic demiurge were due to influence from Manichaeism is impossible to determine. The Cathars apparently adopted the Manichaean principles of church organization. Priscillian and his followers may also have been influenced by Manichaeism. The Manichaeans preserved many apocryphal Christian works, such as the Acts of Thomas, that would otherwise have been lost.
Manichaeism maintained a sporadic and intermittent existence in the west (Mesopotamia, Africa, Spain, France, North Italy, the Balkans) for a thousand years, and flourished for a time in the land of its birth (Persia) and even further east in Northern India, Western China, and Tibet. While it had long been thought that Manichaeism arrived in China only at the end of the seventh century, a recent archaeological discovery demonstrated that it was already known there in the second half of the sixth century.
Some Sodgians in Central Asia believed in the religion. Uyghur ruler Khagan Boku Tekin (AD 759–780) converted to the religion in 763 after a 3 days discussion with its preachers, the Babylonia headquarters sent high rank clerics to Uyghur, and Manichaeism remained the state religion for about a century before the collapse of the Uyghur empire in 840. In the east it spread along trade routes as far as Chang'an, the capital of the Tang Dynasty in China. In the ninth century, it is reported that the Muslim Caliph Al-Ma'mun tolerated a community of Manichaeans. However, al-Mahdi persecuted the Manichaeans, establishing an inquisition to root out their "heresy", even resorting to outright massacre against them. In the Song and Yuan dynasties of China remnants of Manichaeanism continued to leave a legacy contributing to sects such as the Red Turbans.
Manichaeism claimed to present the complete version of teachings that were corrupted and misinterpreted by the followers of its predecessors Adam, Zoroaster, Buddha and Jesus. Accordingly, as it spread, it adapted new deities from other religions into forms it could use for its scriptures. Its original Aramaic texts already contained stories of Jesus. When they moved eastward and were translated into Iranian languages, the names of the Manichaean deities (or angels) were often transformed into the names of Zoroastrian yazatas. Thus "Abbā dəRabbūṯā " ("The Father of Greatness", the highest Manichaean deity of Light), in Middle Persian texts might either be translated literally as "pīd ī wuzurgīh", or substituted with the name of the deity "Zurwān". Similarly, the Manichaean primal figure "Nāšā Qaḏmāyā " "The Original Man" was rendered "Ohrmazd Bay", after the Zoroastrian god Ahura Mazda. This process continued in Manichaeism's meeting with Chinese Buddhism, where, for example, the original Aramaic "karia" (the "call" from the world of Light to those seeking rescue from the world of Darkness), becomes identified in the Chinese scriptures with Guan Yin (觀音 or Avalokitesvara in Sanskrit, literally, "watching/perceiving sounds [of the world]", the Chinese Bodhisattva of Compassion).
Persecution and extinction.
In 732 Emperor Xuanzong of Tang banned local conversion to the religion. In 843 Emperor Wuzong of Tang gave the order to kill all Manichaean clerics as part of his campaign against Buddhism and other religions, and over half died. Caliph Abdullah al-Mahdi Billah also killed thousands of Manichaeans and Al-Muqtadir killed so many that Ibn al-Nadim knew only 5 Manichaeans in Baghdad, the headquarters of the religion.
Later movements accused of "Neo-Manichaeism".
During the Middle Ages, several movements emerged which were collectively described as "Manichaean" by the Catholic Church, and persecuted as Christian heresies through the establishment, in 1184, of the Inquisition. They included the Cathar churches of Western Europe. Other groups sometimes referred to as "neo-Manichaean" were the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria. An example of this usage can be found in the published edition of the Latin Cathar text, the "Liber de duobus principiis" ("Book of the Two Principles"), which was described as "Neo-Manichaean" by its publishers. As there is no presence of Manichaean mythology or church terminology in the writings of these groups, there has been some dispute among historians as to whether these groups were descendants of Manichaeism.
Present day.
Some sites are preserved in Xinjiang and Fujian in China. The Cao'an temple is the only fully intact Manichaean building,:256–257 though it later became associated with Buddhism. Several small groups claim to continue to practice this faith.
Teachings and beliefs.
General.
Mani's teaching dealt with the origin of evil, by addressing a theoretical part of the problem of evil by denying the omnipotence of God and postulating two opposite powers. Manichaean theology taught a dualistic view of good and evil. A key belief in Manichaeism is that the powerful, though not omnipotent good power (God) was opposed by the semi-eternal evil power (Satan). Humanity, the world and the soul are seen as the byproduct of the battle between God's proxy, Primal Man, and Satan. The human person is seen as a battleground for these powers: the soul defines the person, but it is under the influence of both light and dark. This contention plays out over the world as well as the human body—neither the Earth nor the flesh were seen as intrinsically evil, but rather possessed portions of both light and dark. Natural phenomena (such as rain) were seen as the physical manifestation of this spiritual contention. Therefore, the Manichaean worldview explained the existence of evil with a flawed creation which God took no role in forming but rather was the result of Satan striking out against God.
Cosmogony.
Manichaeism presented an elaborate description of the conflict between the spiritual world of light and the material world of darkness. The beings of both the world of darkness and the world of light have names. There are numerous sources for the details of the Manichaean belief. There are two portions of Manichaean scriptures that are probably the closest thing to the original Manichaean writings in their original languages that will ever be available. These are the Syriac-Aramaic quotation by the Nestorian Christian Theodore bar Konai, in his Syriac "Book of Scholia" ("Ketba de-Skolion", eighth century), and the Middle Persian sections of Mani's Shabuhragan discovered at Turpan (a summary of Mani's teachings prepared for Shapur I). These two sections are probably the original Syriac and Middle Persian written by Mani.
From these and other sources, it is possible to derive an almost complete description of the detailed Manichaean vision (a complete list of Manichaean deities is outlined below). According to Mani, the unfolding of the universe takes place with three "creations":
The First Creation: Originally, good and evil existed in two completely separate realms, one the "World of Light", ruled by the "Father of Greatness" together with his five "Shekhinas" (divine attributes of light), and the other the "World of Darkness", ruled by the "King of Darkness". At a certain point, the "Kingdom of Darkness" notices the "World of Light", becomes greedy for it and attacks it. The "Father of Greatness", in the first of three "creations" (or "calls"), calls to the "Mother of Life", who sends her son "Original Man" ("Nāšā Qaḏmāyā" in Aramaic), to battle with the attacking powers of Darkness, which include the "Demon of Greed". The "Original Man" is armed with five different shields of light (reflections of the five "Shekhinas"), which he loses to the forces of darkness in the ensuing battle, described as a kind of "bait" to trick the forces of darkness, as the forces of darkness greedily consume as much light as they can. When the "Original Man" comes to, he is trapped among the forces of darkness.
The Second Creation: Then the "Father of Greatness" begins the "Second Creation", calling to the "Living Spirit", who calls to his five sons, and sends a call to the "Original Man" ("Call" then becomes a Manichaean deity). An answer ("Answer" becomes another Manichaean deity) then returns from the "Original Man" to the "World of Light". The "Mother of Life", the "Living Spirit", and his five sons begin to create the universe from the bodies of the evil beings of the "World of Darkness", together with the light that they have swallowed. Ten heavens and eight earths are created, all consisting of various mixtures of the evil material beings from the "World of Darkness" and the swallowed light. The sun, moon, and stars are all created from light recovered from the "World of Darkness". The waxing and waning of the moon is described as the moon filling with light, which passes to the sun, then through the Milky Way, and eventually back to the "World of Light".
The Third Creation: Great demons (called "archons" in bar-Khonai's account) are hung out over the heavens, and then the "Father of Greatness" begins the "Third Creation". Light is recovered from out of the material bodies of the male and female evil beings and demons, by causing them to become sexually aroused in greed, towards beautiful images of the beings of light, such as the "Third Messenger" and the "Virgins of Light". However, as soon as the light is expelled from their bodies and falls to the earth (some in the form of abortions – the source of fallen angels in the Manichaean myth), the evil beings continue to swallow up as much of it as they can to keep the light inside of them. This results eventually in the evil beings swallowing huge quantities of light, copulating, and producing Adam and Eve. The "Father of Greatness" then sends the "Radiant Jesus" to awaken Adam, and to enlighten him to the true source of the light that is trapped in his material body. Adam and Eve, however, eventually copulate, and produce more human beings, trapping the light in bodies of mankind throughout human history. The appearance of the Prophet Mani was another attempt by the "World of Light" to reveal to mankind the true source of the spiritual light imprisoned within their material bodies.
Outline of the beings and events in the Manichaean mythos.
Beginning with the time of its creation by Mani, the Manichaean religion had a detailed description of deities and events that took place within the Manichaean scheme of the universe. In every language and region that Manichaeism spread to, these same deities reappear, whether it is in the original Syriac quoted by Theodore bar Konai, or the Latin terminology given by Saint Augustine from Mani's "Epistola Fundamenti", or the Persian and Chinese translations found as Manichaeism spread eastward. While the original Syriac retained the original description which Mani created, the transformation of the deities through other languages and cultures produced incarnations of the deities not implied in the original Syriac writings. This process began in Mani's lifetime, with "The Father of Greatness", for example, being translated into Middle Persian as Zurvan, a Zoroastrian supreme being.
Organization and religious practices.
Organization of the Manichaean Church.
The Manichaean Church was divided into "Elect" – those who had taken upon themselves the vows of Manicheaism, and "Hearers" – those who had not, but still participated in the Church. The terms for these divisions were already common since the days of early Christianity. In the Chinese writings, the Middle Persian and Parthian terms are transcribed phonetically (instead of being translated into Chinese).
The Bema Fest.
The most important religious observance of the Manichaeans was the Bema Fest, observed annually:
The Bema was originally, in the Syriac Christian churches, a seat placed in the middle of the nave on which the bishop would preside and from which the Gospel would be read. In the Manichean places of worship, the throne was a five-stepped altar, covered by precious cloths, symbolizing the five classes of the hierarchy. The top of the Bema was always empty, as it was the seat of Mani. The Bema was celebrated at the vernal equinox, was preceded by fasts, and symbolized the passion of Mani, thus it was strictly parallel to the Christian Easter.
While it is often presumed that the Bema seat was empty, there is some evidence from the Coptic Manichaean "Bema Psalms", that the Bema seat may have actually contained a copy of Mani's picture book, the Arzhang.
Primary sources.
Mani wrote either seven or eight books, which contained the teachings of the religion. Only scattered fragments and translations of the originals remain.
The original six Syriac writings are not preserved, although their Syriac names have been. There are also fragments and quotations from them. A long quotation, preserved by the eighth-century Nestorian Christian author Theodore bar Konai, shows that in the original Syriac Aramaic writings of Mani there was no influence of Iranian or Zoroastrian terms. The terms for the Manichaean deities in the original Syriac writings are in Aramaic. The adaptation of Manichaeism to the Zoroastrian religion appears to have begun in Mani's lifetime however, with his writing of the Middle Persian Shabuhragan, his book dedicated to the King Shapuhr. In it, there are mentions of Zoroastrian deities such as Ohrmazd, Ahriman, and Az. Manichaeism is often presented as a Persian religion, mostly due to the vast number of Middle Persian, Parthian, and Soghdian (as well as Turkish) texts discovered by German researchers near Turpan, in the Xinjiang (Chinese Turkestan) province of China, during the early 1900s. However, from the vantage point of its original Syriac descriptions (as quoted by Theodore bar Khonai and outlined below), Manichaeism may be better described as a unique phenomenon of Aramaic Babylonia, occurring in proximity to two other new Aramaic religious phenomena, Talmudic Judaism and Babylonian Mandaeism, which were also appearing in Babylonia in roughly the third century AD.
The original, but now lost, six sacred books of Manichaeism were composed in Syriac Aramaic, and translated into other languages to help spread the religion. As they spread to the east, the Manichaean writings passed through Middle Persian, Parthian, Sogdian, Tocharian and ultimately Uyghur and Chinese translations. As they spread to the west, they were translated into Greek, Coptic, and Latin.
Henning describes how this translation process evolved and influenced the Manichaeans of Central Asia:
Beyond doubt, Sogdian was the national language of the Majority of clerics and propagandists of the Manichaean faith in Central Asia. Middle Persian (= Pārsīg), and to a lesser degree, Parthian (= Pahlavānīg), occupied the position held by Latin in the medieval church. The founder of Manichaeism had employed Syriac (his own language) as his medium, but conveniently he had written at least one book in Middle Persian, and it is likely that he himself had arranged for the translation of some or all of his numerous writings from Syriac into Middle Persian. Thus the Eastern Manichaeans found themselves entitled to dispense with the study of Mani’s original writings, and to continue themselves to reading the Middle Persian edition; it presented small difficulty to them to acquire a good knowledge of the Middle Persian language, owing to its affinity with Sogdian.
Later works.
In later centuries, as Manichaeism passed through eastern Persian speaking lands and arrived at the Uyghur Empire (回鹘帝国), and eventually the Uyghur kingdom of Turpan (destroyed around 1335), Middle Persian and Parthian prayers ("āfrīwan" or "āfurišn") and the Parthian hymn-cycles (the "Huwīdagmān" and "Angad Rōšnan" created by Mar Ammo) were added to the Manichaean writings. A translation of a collection of these produced the Manichaean Chinese Hymnscroll (the 摩尼教下部贊, which Lieu translates as "Hymns for the Lower Section [i.e. the Hearers] of the Manichaean Religion"). In addition to containing hymns attributed to Mani, it contains prayers attributed to Mani's earliest disciples, including Mār Zaku, Mār Ammo and Mār Sīsin. Another Chinese work is a complete translation of the "Sermon of the Light Nous", presented as a discussion between Mani and his disciple Adda.
Critical and polemic sources.
Until discoveries in the 1900s of original sources, the only sources for Manichaeism were descriptions and quotations from non-Manichaean authors, either Christian, Muslim, Buddhist or Zoroastrian. While often criticizing Manichaeism, they also quoted directly from Manichaean scriptures. This enabled Isaac de Beausobre, writing in the 18th century, to create a comprehensive work on Manichaeism, relying solely on anti-Manichaean sources. Thus quotations and descriptions in Greek and Arabic have long been known to scholars, as have the long quotations in Latin by Saint Augustine, and the extremely important quotation in Syriac by Theodore bar Khonai.
Patristic depictions of Mani and Manchæeism.
Eusebius commented as follows:
 "The error of the Manichees, which commenced at this time."
 — In the mean time, also, that madman Manes, (Mani is of Persian or semetic origin) as he was called, well agreeing with his name, for his demoniacal heresy, armed himself by the perversion of his reason, and at the instruction of Satan, to the destruction of many. He was a barbarian in his life, both in speech and conduct, but in his nature as one possessed and insane. Accordingly, he attempted to form himself into a Christ, and then also proclaimed himself to be the very paraclete and the Holy Spirit, and with all this was greatly puffed up with his madness. Then, as if he were Christ, he selected twelve disciples, the partners of his new religion, and after patching together false and ungodly doctrines, collected from a thousand heresies long since extinct, he swept them off like a deadly poison, from Persia, upon this part of the world. Hence the impious name of the Manicheans spreading among many, even to the present day. Such then was the occasion of this knowledge, as it was falsely called, that sprouted up in these times.
Acta Archelai.
An example of how inaccurate some of these accounts could be is seen in the account of the origins of Manichaeism contained in the "Acta Archelai". This was a Greek anti-manichaean work written before 348, most well known in its Latin version, which was regarded as an accurate account of Manichaeism until the end of the 19th century:
In the time of the Apostles there lived a man named Scythianus, who is described as coming 'from Scythia,' and also as being 'a Saracen by race' ('ex genere Saracenorum'). He settled in Egypt, where he became acquainted with 'the wisdom of the Egyptians,' and invented the religious system which was afterwards known as Manichaeism. Finally he emigrated to Palestine, and, when he died, his writings passed into the hands of his sole disciple, a certain Terebinthus. The latter betook himself to Babylonia, assumed the name of Budda, and endeavoured to propagate his master's teaching. But he, like Scythianus, gained only one disciple, who was an old woman. After a while he died, in consequence of a fall from the roof of a house, and the books which he had inherited from Scythianus became the property of the old woman, who, on her death, bequeathed them to a young man named Corbicius, who had been her slave. Corbicius thereupon changed his name to Manes, studied the writings of Scythianus, and began to teach the doctrines which they contained, with many additions of his own. He gained three disciples, named Thomas, Addas, and Hermas. About this time the son of the Persian king fell ill, and Manes undertook to cure him; the prince, however, died, whereupon Manes was thrown into prison. He succeeded in escaping, but eventually fell into the hands of the king, by whose order he was flayed, and his corpse was hung up at the city gate.
A. A. Bevan, who quoted this story, commented that it 'has no claim to be considered historical.'
View of Judaism in the "Acta Archelai".
According to Hegemonius' portrayal of Mani, the devil god which created the world was the Jewish Jehovah. Hegemonius reports that Mani said, "It is the Prince of Darkness who spoke with Moses, the Jews and their priests. Thus the Christians, the Jews, and the Pagans are involved in the same error when they worship this God. For he leads them astray in the lusts he taught them." He goes on to state: "Now, he who spoke with Moses, the Jews, and the priests he says is the archont of Darkness, and the Christians, Jews, and pagans (ethnic) are one and the same, as they revere the same god. For in his aspirations he seduces them, as he is not the god of truth. And so therefore all those who put their hope in the god who spoke with Moses and the prophets have (this in store for themselves, namely) to be bound with him, because they did not put their hope in the god of truth. For that one spoke with them (only) according to their own aspirations."
Central Asian and Iranian primary sources.
In the early 1900s, original Manichaean writings started to come to light when German scholars led by Albert Grünwedel, and then by Albert von Le Coq, began excavating at Gaochang, the ancient site of the Manichaean Uyghur Kingdom near Turpan, in Chinese Turkestan (destroyed around AD 1300). While most of the writings they uncovered were in very poor condition, there were still hundreds of pages of Manichaean scriptures, written in three Iranian languages (Middle Persian, Parthian, and Sogdian) and old Turkish. These writings were taken back to Germany, and were analyzed and published at the Preußische Akademie der Wissenschaften in Berlin, by Le Coq and others, such as Friedrich W. K. Müller and Walter Bruno Henning. While the vast majority of these writings were written in a version of the Syriac script known as Manichaean script, the German researchers, perhaps for lack of suitable fonts, published most of them using Hebrew letters (which could easily be substituted for the 22 Syriac letters).
Perhaps the most comprehensive of these publications was "Manichaeische Dogmatik aus chinesischen und iranischen Texten" ("Manichaean Dogma from Chinese and Iranian texts"), by Ernst Waldschmidt and Wolfgang Lentz, published in Berlin in 1933. More than any other research work published before or since, this work printed, and then discussed, the original key Manichaean texts in the original scripts, and consists chiefly of sections from Chinese texts, and Middle Persian and Parthian texts transcribed with Hebrew letters. (After the Nazi party gained power in Germany, the Manichaean writings continued to be published during the 1930s, but the publishers no longer used Hebrew letters, instead transliterating the texts into Latin letters.)
Coptic primary sources.
Additionally, in 1930, German researchers in Egypt found a large body of Manichaean works in Coptic. Though these were also damaged, hundreds of complete pages survived and, beginning in 1933, were analyzed and published in Berlin before World War II, by German scholars such as Hans Jakob Polotsky. Some of these Coptic Manichaean writings were lost during the war.
Chinese primary sources.
After the success of the German researchers, French scholars visited China and discovered what is perhaps the most complete set of Manichaean writings, written in Chinese. These three Chinese writings, all found at the Caves of the Thousand Buddhas among the Dunhuang manuscripts, and all written before the 9th century, are today kept in London, Paris, and Beijing. Some of the scholars involved with their initial discovery and publication were Édouard Chavannes, Paul Pelliot, and Aurel Stein. The original studies and analyses of these writings, along with their translations, first appeared in French, English, and German, before and after World War II. The complete Chinese texts themselves were first published in Tokyo, Japan in 1927, in the Taisho Tripitaka, volume 54. While in the last thirty years or so they have been republished in both Germany (with a complete translation into German, alongside the 1927 Japanese edition), and China, the Japanese publication remains the standard reference for the Chinese texts.
Greek life of Mani, Cologne codex.
In Egypt a small codex was found and became known through antique dealers in Cairo. It was purchased by the University of Cologne in 1969. Two of its scientists, Henrichs and Koenen, produced the first edition known since as the Cologne Mani-Codex, which was published in four articles in the "Zeitschrift für Papyrologie und Epigraphik". The ancient papyrus manuscript contained a Greek text describing the life of Mani. Thanks to this discovery, much more is known about the man who founded one of the most influential world religions of the past.
Figurative use.
The terms "Manichaean" and "Manichaeism" are sometimes used figuratively as a synonym of the more general term "dualist" with respect to a philosophy or outlook. They are often used to suggest that the world view in question simplistically reduces the world to a struggle between good and evil. For example, Jean-Paul Sartre in the essay "Anti-Semite and Jew" referred to the antisemitic world view as "a form of Manichaeism", since "it explains the course of the world by the struggle of the principle of good with the principle of evil" (the "principle of evil" being equated with the Jews). Similarly, Zbigniew Brzezinski used the phrase "Manichaean paranoia" in reference to U.S. President George W. Bush's world view (in the The Daily Show with Jon Stewart, March 14, 2007); Brzezinski elaborated that he meant "the notion that he (Bush) is leading the forces of good against the empire of evil".
References.
Books and articles.
</dl>

</doc>
<doc id="19761" url="http://en.wikipedia.org/wiki?curid=19761" title="Moroccan cuisine">
Moroccan cuisine

Moroccan cuisine is influenced by Morocco's interactions and exchanges with other cultures and nations over the centuries. Moroccan cuisine is typically a mix of Mediterranean, Arabic, Andalusian and berber cuisine. The cooks in the royal kitchens of Fes, Meknes, Marrakech, Rabat and Tetouan created the basis for what is known as Moroccan cuisine today.
Ingredients.
Morocco produces a large range of Mediterranean fruits and vegetables and even some tropical ones. Common meats include beef, mutton and lamb, chicken and seafood, which serve as a base for the cuisine. Characteristic flavorings include lemon pickle, cold-pressed, unrefined olive oil and dried fruits.
Spices and other flavorings.
Spices.
Spices are used extensively in Moroccan food. Although spices have been imported to Morocco through the arabs for thousands of years, many ingredients — like saffron from Talaouine, mint and olives from Meknes, and oranges and lemons from Fez — are home-grown. Common spices include:
Herbs.
Common herbs include:
Structure of meals.
A typical lunch meal begins with a series of hot and cold salads, followed by a tagine or Dwaz. Bread is eaten with every meal. Often, for a formal meal, a lamb or chicken dish is next, followed by couscous topped with meat and vegetables. A cup of sweet mint tea usually ends the meal. Moroccans either eat with fork, knife and spoon or with their hands using bread as a utensil depending on the dish served. The consumption of pork and alcohol is not common due to religious restrictions.
Main dishes.
The main Moroccan dish most people are familiar with is couscous, the old national delicacy. Beef is the most commonly eaten red meat in Morocco, usually eaten in a Tagine with a wide selection of vegetables. Chicken is also very commonly used in Tagines, or roasted. One of the most appreciated local dishes is the Tagine of Chicken, fries and olives. 
Lamb is also heavily consumed, and since Moroccan sheep breeds store most of their fat in their tails, Moroccan lamb does not have the pungent flavour that Western lamb and mutton have. 
Since Morocco lies on two coasts the Atlantic and the Mediterranean, Moroccan cuisine has ample seafood dishes. European pilchard is widely and heavily consumed due to its abundance and quality, hence Morocco is the first producer of this kind of species globally. 
Among the most famous Moroccan dishes are Couscous, Pastilla (also spelled Basteeya or Bestilla), Tajine, Tanjia and Harira, a typical heavy soup, eaten during winter to warm up and is usually served for dinner, it is typical eaten with plain bread or with dates. The latter is especially used during the month of Ramadan. 
A big part of the daily meal is bread. Bread in Morocco is principally from durum wheat semolina known as khobz. Bakeries are very common throughout Morocco and fresh bread is a staple in every city, town and village. The most common is whole grain coarse ground or white flour bread or Baguette. There are also a number of flat breads and pulled unleavened pan-fried breads. Morocco is also the world’s first consumer of bread.
In addition, there are dried salted meats and salted preserved meats such as kliia/khlia and "g'did" which are used to flavor tagines or used in "el ghraif" a folded savory Moroccan pancake"
Salads.
Salads include both raw and cooked vegies, served either hot or cold. Cold salads include "zaalouk," an aubergine and tomato mixture, and taktouka (a mixture of tomatoes, green peppers, garlic and spices) characteristic of the cities of Taza and Fes, in the Atlas.
Desserts.
Usually, seasonal fruits rather than cooked desserts are served at the close of a meal. A common dessert is "kaab el ghzal" ("gazelle's horns"), a pastry stuffed with almond paste and topped with sugar. Another is "Halwa chebakia", pretzel-shaped dough deep-fried, soaked in honey and sprinkled with sesame seeds. Halwa Shebakia are cookies eaten during the month of Ramadan. Coconut fudge cakes, 'Zucre Coco', are popular also.
Beverages.
The most popular drink is green tea with mint. Traditionally, making good mint tea in Morocco is considered an art form and the drinking of it with friends and family is often a daily tradition. The pouring technique is as crucial as the quality of the tea itself. Moroccan tea pots have long, curved pouring spouts and this allows the tea to be poured evenly into tiny glasses from a height. For the best taste, glasses are filled in two stages. The Moroccans traditionally like tea with bubbles, so while pouring they hold the teapot high above the glasses. Finally, the tea is accompanied with hard sugar cones or lumps.
Snacks and fast food.
Selling fast food in the street has long been a tradition, and the best example is Djemaa el Fna square in Marrakech. Starting in the 1980s, new snack restaurants started serving "Bocadillo" (a Spanish for a sandwich. Though the composition of a bocadillo varies by region, it is usually a baguette filled with salad and a choice of meats, Mozarella, fish (usually tuna), or omelette.
Dairy product shops locally called Mhlaba, are very prevalent all around the country. Those dairy stores generally offer all types of dairy products, juices, and local delicacies such as (Bocadillos, Msemen and Harcha).
In the late 1990s, several multinational fast-food franchises opened restaurants in major cities.
Moroccan food abroad.
Couscous is one of the most popular North African dishes globally. Markets, stores and restaurants in Europe, especially in France and lately the United Kingdom, feature lamb tajines, bastilla, and couscous.
Paula Wolfert, prolific American author of nine cookbooks (two on Moroccan cuisine), helped enable Moroccan-Americans to enjoy their native cuisine with ease. "Couscous and Other Good Food from Morocco" was published in 1973 and is still in print; it was added to the James Beard Hall of Fame in 2008. Her "Food of Morocco" came out in 2011 and won the 2012 James Beard Award for Best International Cookbook. Wolfert appeared on the "Martha Stewart Show" to demonstrate cooking in clay.
Raised between Fez and San Sebastian, chef Najat Kaanache has served as an unofficial culinary ambassador of Morocco, sharing Moroccan flavors and cooking techniques with many of the world's top chefs during her pilgrimage through the best restaurant kitchens of Spain, Denmark, the Netherlands and the US.

</doc>
<doc id="19763" url="http://en.wikipedia.org/wiki?curid=19763" title="Martin Van Buren">
Martin Van Buren

Martin Van Buren (Dutch: "Maarten van Buren"  ; December 5, 1782 – July 24, 1862) was the eighth President of the United States (1837–1841). A member of the Democratic Party, he served in a number of senior roles, including eighth Vice President (1833–1837) and Secretary of State (1829–1831), both under Andrew Jackson. Van Buren's inability as president to deal with the economic chaos of the Panic of 1837 and with the surging Whig Party led to his defeat for re-election in 1840.
Of Dutch descent, Van Buren learned early how to coordinate multiple ethnic and political groups. A meticulous dresser, he could mingle in upper class society as well as in saloon environments such as the tavern his father ran. A delegate to a political convention at age 18, he quickly moved from local to state politics, gaining fame both as a political organizer and an accomplished lawyer. Elected to the Senate by the state legislature in 1821, Van Buren supported William H. Crawford for president in 1824, but by 1828 had come to support General Andrew Jackson. Van Buren was a major supporter and organizer for Jackson in the 1828 election. Jackson was elected, and made Van Buren Secretary of State.
During Jackson's eight years as president, Van Buren was a key advisor, and built the organizational structure for the coalescing Democratic Party, particularly in New York. In 1831, Jackson gave him a recess appointment as American minister to Britain, but Van Buren's nomination was rejected by the Senate, cutting short his service in London. He was successful in the jockeying to become Jackson's picked successor, and was elected vice president in 1832. Van Buren faced several Whig opponents in his presidential bid in 1836, and was elected.
As president, he was blamed for the depression of 1837; hostile newspapers called him "Martin Van Ruin." He attempted to cure the economic problems by keeping control of federal funds in an Independent Treasury, rather than in state banks, but Congress did not pass this until 1840. In foreign affairs, he did not want the United States to annex Texas, an act which John Tyler would achieve eight years after Van Buren's initial rejection. Between the bloodless Aroostook War and the "Caroline" Affair, relations with Britain and its colonies in Canada also proved to be strained.Van Buren was voted out of office in 1840, losing to Whig candidate William Henry Harrison. He was the leading candidate for the Democratic nomination in 1844, but lost to James K. Polk, who went on to win the election. In the 1848 election Van Buren ran unsuccessfully as the candidate of the Free Soil Party. Van Buren supported fellow Democrats Franklin Pierce (1852), James Buchanan (1856), and Stephen A. Douglas (1860) for the presidency, but his increasingly anti-slavery views and support for the Union led him to support Abraham Lincoln's policies after the start of the American Civil War. Van Buren's health began to fail in 1861, and he died in July 1862 at the age of seventy-nine. Although he served in many high offices, his most lasting achievement was as a political organizer who built the modern Democratic Party and guided it to dominance in the new Second Party System.
Early life and education.
Born on December 5, 1782, Martin Van Buren was the first president to be born after the United States declared independence. He was born in the village of Kinderhook, New York, on December 5, 1782, about 20 mi south of Albany on the Hudson River. In the era before the steamboat, Kinderhook was an isolated village, and most of the townsfolk, including the Van Burens, were of Dutch descent and spoke Dutch at home. Martin Van Buren was the only president who spoke English as a second language. Van Buren descended from Cornelis Maessen of the town of Buren in the Netherlands, who had come to America in 1631 and purchased a plot of land on Manhattan Island; his son, Martin Cornelisen, took the surname Van Buren.
The future president's father, Abraham Van Buren (1737–1817), was a farmer who owned a Kinderhook inn as well as six slaves. Abraham Van Buren supported the American Revolution and later the Jeffersonian Republicans. Martin Van Buren's mother was Maria Hoes Van Alen Van Buren (1747–1818). Martin's mother had been married to Johannes Van Alen; he died and left her with two boys and a girl. In 1776, she married Abraham Van Buren. By his mother's first marriage, Van Buren had one half-sister and two half-brothers, including James I. Van Alen, who practiced law with Van Buren for a time and served as a Federalist member of Congress (1807–1809). Van Buren had four full siblings from his parents's marriage: Dirckie "Derike" Van Buren (1777–1865), Jannetje "Hannah" Van Buren (1780-1838), Lawrence Van Buren (1786–1868), and Abraham Van Buren (1788–1836).
Van Buren received a basic education at the village schoolhouse and briefly studied Latin at the Kinderhook Academy and at Washington Seminary in Claverack. His formal education ended before he reached 14, when he began reading law in 1796 at the office of Peter Silvester and his son Francis, prominent Federalist attorneys in Kinderhook.
Van Buren was small in stature; as an adult he was 5 feet 6 inches tall, and often referred to as "Little Van." When he first began his legal studies, he often presented an unkempt appearance in rough, homespun clothing. It was the Silvesters who suggested that Van Buren could improve his professional prospects by dressing fashionably and taking care in how he appeared in public; he heeded the advice and patterned his clothing, appearance, bearing and conduct after theirs. After six years under the Silvesters, the elder Silvester and Democratic-Republican political figure John Peter Van Ness suggested that Van Buren's political leanings made it a good idea for him to complete his education with a Democratic-Republican attorney. Accepting this advice, he spent a final year of apprenticeship in the New York City office of John Van Ness's brother William P. Van Ness, a political lieutenant of Aaron Burr. Van Buren was admitted to the bar in 1803.
Van Buren married Hannah Hoes, his childhood sweetheart and first cousin once removed, on February 21, 1807, in Catskill, New York. Like Van Buren, she was raised in a Dutch home; she spoke primarily Dutch, and spoke English with a distinct accent. The couple had five sons and one daughter: Abraham (1807–1873) a graduate of West Point and career military officer; John (1810–1866), graduate of Yale and Attorney General of New York; Martin, Jr. (1812–1855), secretary to his father and editor of his father's papers until a premature death from tuberculosis; Winfield Scott (born and died in 1814); and Smith Thompson (1817–1876), an editor and special assistant to his father while president. Their daughter was stillborn. After 12 years of marriage, Hannah Van Buren contracted tuberculosis and died on February 5, 1819, at the age of 35. Martin Van Buren never remarried.
Early political career.
Van Buren had been active in politics from at least the age of 17 when he attended a party convention in Troy, New York, where he worked successfully to secure for John Peter Van Ness the Democratic-Republican Party nomination in a special election for the 6th Congressional District seat. He formed a law partnership with his half-brother James I. Van Alen, and once established in his practice, he became financially secure enough to increase his focus on politics. He was an early supporter of Aaron Burr, and allied himself with the George Clinton faction of the Democratic-Republican Party. Van Buren supported Daniel D. Tompkins for Governor over incumbent Morgan Lewis in 1807. Tompkins won, and his allies were a majority in the state legislature. As a result, Van Buren was appointed Surrogate of Columbia County, New York, replacing Van Alen, who had supported Lewis. Van Buren served as Surrogate from 1808 until 1813, when the Federalist Party obtained a majority in the state legislature and replaced him.
Van Buren was a member of the New York State Senate from 1812 to 1820, and joined the opposition party in 1813. (The opposition party were Democratic-Republicans who fought DeWitt Clinton for control of the Democratic-Republican Party in New York.) Van Buren served as New York Attorney General from 1815 to 1819. He replaced William Floyd as a presidential elector in 1820, and voted for James Monroe and Daniel D. Tompkins.
Though he never served in the military, during the War of 1812 Van Buren worked in the State Senate to pass war measures, including bills to expand the New York Militia and increase soldier pay. In addition, he was a special judge advocate appointed to serve as one of the prosecutors of William Hull during Hull's court-martial following the surrender of Detroit.
At first he opposed DeWitt Clinton's plan for the Erie Canal, but he supported it when the Bucktails (the name given to the anti-DeWitt Clinton Democratic-Republicans) were able to gain a majority on the Erie Canal Commission, and he supported a bill that raised money for the canal through the sale of state bonds.
In 1817, Van Buren's connection with so-called "machine politics" started when he created the first political organization encompassing all of New York, the Bucktails. The Bucktails became a successful movement that emphasized party loyalty and used it to capture and control many patronage posts throughout New York. Van Buren gained the nickname of "Little Magician" for the skill with which he exploited what came to be called the "spoils system". Van Buren served as a member of the 1820 state constitutional convention, where he favored expanded voting rights, but opposed universal suffrage and tried to maintain property requirements for voting.
He was the leading figure in the Albany Regency, a group of Bucktail leaders who for more than a generation dominated the politics of New York and influenced national politics. The Regency, together with other political organizations such as Tammany Hall, played a major role in expanding the spoils system and making it a recognized and accepted procedure. He was the prime architect of the first nationwide political party: the Jacksonian Democrats or Democratic Party, which evolved from the Democratic-Republicans and relied on party loyalty and patronage to prevent contentious sectional issues, including tariffs and slavery, from becoming national crises. In Van Buren's words, "Without strong national political organizations, there would be nothing to moderate the prejudices between free and slaveholding states." As had James Madison and other Democratic Party organizers who favored states' rights and local control, Van Buren was struggling to find an institutional solution to the Constitution's seeming inability to prevent concentration of power in an administrative republic.
Early in his life Van Buren owned a slave, a man named Tom who served as his personal valet. Tom ran away in 1814 and eventually settled in Canada, with Van Buren making no effort to locate him. In 1824 Tom was found to be living in Worcester, Massachusetts. Since he still legally owned Tom (under New York's gradual emancipation law, slavery was scheduled to be completely abolished in the state in 1827), Van Buren agreed to sell Tom to the finder, a resident of Rensselaer County, for $50 provided that he could be captured without violence. The finder could not make such a guarantee, and his willingness to pay was lessened by the knowledge that Tom would be emancipated in less than three years even if he was re-enslaved, so Tom remained free, as Van Buren probably intended.
U.S. Senate and national politics.
In February 1821, Martin Van Buren was elected a U.S. Senator from New York, defeating the incumbent Nathan Sanford who ran as the Clintonian candidate. Van Buren at first favored internal improvements, such as road repairs and canal construction, proposing a constitutional amendment in 1824 to authorize such undertakings, but changed his position the following year. He voted for the tariffs of 1824 and 1828, and then gradually abandoned this protectionist position, later coming out for tariffs "for revenue only."
In the presidential election of 1824, Van Buren supported William H. Crawford and received the electoral vote of Georgia for Vice President. None of the presidential candidates—Crawford, John Quincy Adams, Andrew Jackson, or Henry Clay—had received a majority of the electoral college votes, so the choice fell to the United States House of Representatives. The House had to choose from among the top three candidates, so Clay was eliminated. Van Buren had originally hoped to block John Quincy Adams by denying him the state of New York, which was divided between supporters of Crawford and Adams. However, Representative Stephen Van Rensselaer swung New York to Adams. Adams won, and appointed Clay as Secretary of State. Because Clay had supported Adams in the House election, Jackson and Crawford supporters alleged corruption. After the House contest, Van Buren shrewdly kept out of the controversy which followed, and began looking forward to 1828. He switched his support early from Crawford, whose ill health after a stroke had made him a less than viable candidate, to Andrew Jackson, who had won the popular vote in 1824. Jackson was angered to see the presidency go to Adams after he received fewer votes than Jackson, and Jackson eagerly looked forward to a rematch.
Always notably courteous in his treatment of opponents, Van Buren showed no bitterness toward either Adams or Henry Clay, and he voted for Clay's confirmation as Secretary of State, notwithstanding Jackson's "corrupt bargain" charge. At the same time, he opposed the Adams-Clay plans for internal infrastructure improvements (roads, canals, bridges etc.) and declined to support U.S. participation in the Congress of Panama. As chair of the Judiciary Committee, he brought forward a number of measures for the improvement of judicial procedure, including one (not adopted), which would have required a super-majority vote by the United States Supreme Court to declare a law unconstitutional. in May 1826, Van Buren joined with Senator Thomas Hart Benton in reporting on patronage in the executive branch, going against his own use of the spoils system to propose unsuccessfully that Presidents not be able to remove officeholders at will, and that Presidents report to Congress on the reasons why dismissed holders of federal positions had been removed. The 1828 "Tariff of Abominations" was recognized as his work. Since Democrats, especially Southerners, were generally opposed to tariffs that increased the price of manufactured goods from the North but did not benefit the raw materials produced in the South, Van Buren could normally have been expected to oppose tariffs. Political observers of the time viewed Van Buren's efforts to pass the 1828 tariff as part of the campaign to elect Jackson as President. Anticipating that most Southerners would vote for Andrew Jackson no matter who else was running, Van Buren intended the tariff proposed by Jackson's Northern Democratic supporters in Congress to attract to Jackson's candidacy Northern voters, who generally favored high tariffs to protect the manufactured goods they produced. Van Buren voted in favor, later adopting the cover story that he had done so only in response to instructions from the New York State Legislature. Most Democrats, especially Southerners, continued to oppose tariffs after 1828. Van Buren's political opponents in the Democratic Party used his 1828 vote against him for years afterwards to prevent him from obtaining Southern support for his candidacies.
Van Buren was not an orator, but his more important speeches show careful preparation and his opinions carried weight; the oft-repeated charge that he refrained from declaring himself on crucial questions is hardly borne out by an examination of his senatorial career. In February 1827, he was re-elected to the Senate by a large majority. He became one of the recognized managers of the Jackson campaign, and his tour of Virginia, the Carolinas, and Georgia in the spring of 1827 won support for Jackson from Crawford. Martin Van Buren sought to reorganize and unify "the old Republican party" behind Jackson. At the state level, Jackson's committee chairs would split up the responsibilities around the state and organize volunteers at the local level. "Hurra Boys" would plant hickory trees (in honor of Jackson's nickname, "Old Hickory") or hand out hickory sticks at rallies. In 1828 Van Buren ran for Governor of New York in an effort to use his personal popularity to bolster Jackson's chances of carrying New York in the presidential election. Jackson defeated Adams handily, leading the pro-Adams "New York American" to editorialize "Organization is the secret of victory. By the want of it we have been overthrown." Van Buren won his election, and resigned from the Senate to start the gubernatorial term, which began on January 1, 1829.
Martin Van Buren's tenure as New York governor is the second shortest on record. While his term was short, he did manage to pass the Bank Safety Fund Law (an early form of deposit insurance) through the Legislature.
Jackson Cabinet.
On March 5, 1829, President Jackson appointed Van Buren Secretary of State, an office which probably had been assured to him before the 1828 elections, and Van Buren resigned the governorship on March 12. He was succeeded in the governorship by his Lieutenant Governor, Enos T. Throop, a member of the Regency. As Secretary of State, Van Buren took care to keep on good terms with the Kitchen Cabinet, the group of politicians who acted as Jackson's advisers. He sometimes opposed Jackson in the matter of removing political appointees from office to replace them with Jackson loyalists, but also saw to the replacement of Postmasters in New York with Van Buren loyalists.
He won the lasting regard of Jackson by his courtesies to Peggy Eaton, wife of Secretary of War John H. Eaton, with whom the wives of the cabinet officers led by Vice President Calhoun's wife, Floride Calhoun had refused to associate in the Petticoat Affair.
No serious diplomatic questions arose during Van Buren's tenure, but he achieved several notable successes, including the settlement of long-standing claims against France, winning reparations for property that had been seized during the Napoleonic Wars. He reached an agreement with the British to open trade with the British West Indies colonies. In addition, Van Buren completed a treaty with the Ottoman Empire that gained American merchants access to the Black Sea. Items on which he did not achieve success included settling the Maine-New Brunswick boundary dispute with Great Britain, gaining settlement of the U.S. claim to the Oregon Country, concluding a commercial treaty with Russia, and persuading Mexico to sell Texas.
Van Buren also advised Jackson informally on matters of domestic policy. In the controversy over the Bank of the United States, he sided with Jackson. He also sided with Jackson on the Indian Removal Act. After the breach between Jackson and Calhoun, which culminated with the Nullification Crisis, Van Buren's position as one of Jackson's primary political supporters and policy advisors clearly marked him as the most prominent candidate for the vice presidency in 1832, and Jackson's most likely successor in 1836.
Vice-Presidency.
In December 1829, Jackson had already made known his wish that Van Buren receive the 1832 vice presidential nomination. In April 1831, Van Buren resigned as Secretary of State during the Petticoat affair, giving Jackson the opportunity to end the dispute by requesting other resignations so he could reorganize his cabinet. Van Buren did not leave office until June, and continued to play a part in the Kitchen Cabinet. In August 1831 Jackson gave Van Buren a recess appointment as Minister to the Court of St. James (Britain) and he arrived in London in September. He was cordially received, but in February, he learned that his nomination had been rejected by the Senate on January 25, 1832. The rejection was attributed by the Senate to Van Buren's instructions while Secretary of State to Louis McLane, the American minister to Britain. Van Buren's instructions, which concerned the opening of the West Indies trade, supposedly repudiated the foreign policy of Jackson's predecessors, which the Senate claimed was a breach of decorum. In fact, the rejection of Van Buren was the work of Calhoun. Calhoun opposed Van Buren, believing that Van Buren had attempted to keep him from becoming vice president. Calhoun also opposed Van Buren for his role in the Petticoat Affair and his work on the 1828 tariff. When the vote on Van Buren's nomination was taken, enough Democrats refrained from voting to produce a tie, thus giving Calhoun, in his role as presiding officer, the ability to cast a vote. He voted no, and so achieved "vengeance" on Van Buren.
Calhoun was elated, convinced that he had ended Van Buren's career. "It will kill him dead, sir, kill him dead. He will never kick, sir, never kick," Calhoun exclaimed to a friend within earshot of Thomas Hart Benton. In fact, Calhoun's move backfired by making Van Buren seem the victim of petty politics, thus raising him in both Jackson's regard and the esteem of others in the Democratic Party. Far from ending Van Buren's career, Calhoun's action gave greater impetus to Van Buren's candidacy for vice president.
After a brief tour of Europe, Van Buren reached New York on July 5, 1832. The May 1832 Democratic National Convention, the party's first, had nominated him for vice president on the Jackson ticket. Van Buren's nomination was not as strongly supported as Jackson's, particularly among southerners who recalled his work on the tariff in 1828, but he somewhat placated southerners by denying the right of Congress to abolish slavery in the District of Columbia without the consent of the slave states.
The Jackson-Van Buren ticket won the 1832 election, and Van Buren took office as Vice President in March 1833. During his time in office Van Buren continued to be one of Jackson's primary advisors and confidants, and accompanied Jackson on his tour of the northeastern United States in 1833. Jackson's confidence in Van Buren was further demonstrated after Jackson named Benjamin F. Butler, Van Buren's political ally and former law partner, to serve as Attorney General, and John Forsyth, another Van Buren ally, to serve as Secretary of State.
Van Buren's support of Jackson in the Nullification Crisis and the decision not to recharter the Second Bank of the United States made him a target of Jackson's most vocal opponents. Van Buren was threatened with violence, including explicit threats from Senator George Poindexter of Mississippi, which caused Van Buren to carry pistols for self-defense. However he also demonstrated both the willingness and the ability to work with his opponents, cooperating with Clay and Calhoun (now a Senator) to pass the compromise Tariff of 1833, which helped end the Nullification Crisis.
During one contentious debate on the bank issue, Van Buren presided over the Senate as Clay spoke passionately about the harm he believed Jackson's bank policy would cause. Directing his remarks to Van Buren, Clay asked rhetorically whether Van Buren would approach Jackson and persuade him to change his mind. After Clay concluded, observers wondered how Van Buren would respond. Rather than answering directly, Van Buren descended from the rostrum and asked Clay if he could borrow a pinch of Clay's snuff. Caught off guard, Clay reflexively handed over his snuff box. Van Buren took a pinch, bowed to Clay, and left the chamber, both reducing the effect of Clay's remarks and preventing tension from escalating, as would have happened if Van Buren had attempted to reply to Clay.
Election of 1836.
In the election of 1832, the Jackson-Van Buren ticket won by a landslide. With Jackson not running in 1836, he was determined to make Van Buren his successor in order to continue the Jackson administration's policies.
Van Buren was unanimously nominated by the 1835 Democratic National Convention at Baltimore, Maryland. On the issue of slavery, Van Buren moved to obtain the support of southerners by assuring them that he opposed abolitionism and that he supported leaving slavery alone in the states where it already existed. On the issue of the national bank, Van Buren made clear that he opposed rechartering the Bank of the United States. To demonstrate his good faith on the slavery issue, Van Buren cast the tie-breaking vote in the Senate in favor of engrossing a bill to subject abolitionist literature in the mail to state laws, thus ensuring that its circulation would be prohibited in the south.
Martin Van Buren's competitors in the 1836 election were the Whigs, who ran several regional candidates in hopes of sending the election to the House of Representatives, where each state delegation would have one vote and the Whigs would stand a better chance of winning. William Henry Harrison hoped to receive the support of the Western voters, Daniel Webster had strength in New England, and Hugh Lawson White and Willie Person Mangum had support in the South. Van Buren won the election easily, with 170 electoral votes to 73 for Harrison, 26 for White, 14 for Webster and 11 for Mangum.
Twentieth Century etymologist Allen Walker Read published research asserting the wide usage of the phrase "O.K." (okay) -- "Old Kinderhook"—started during the presidential campaign and subsequent presidency of Martin Van Buren.
Presidency 1837–1841.
Policies.
 BEP engraved portrait of Van Buren as President. 
Martin Van Buren announced his intention "to follow in the footsteps of his illustrious predecessor", and retained all but one of Jackson's cabinet. Van Buren had few economic tools to deal with the Panic of 1837. The Panic was followed by a five-year depression. Banks failed and unemployment reached record highs. Some modern economists have argued that the Panic was caused by the bank policies of the Jackson administration, with the power to create money being distributed into decentralized banks, most of which would then continue to cause a massive inflationary bubble.
Van Buren advocated lower tariffs and free trade, and by doing so maintained support of the South for the Democratic Party. He succeeded in setting up a system of bonds for the national debt. His party was so split that his 1837 proposal for an "Independent Treasury" system did not pass until 1840. It gave the Treasury control of all federal funds and had a legal tender clause that required (by 1843) all payments to be made in specie, but it further inflamed public opinion on both sides.
In a bold step, Van Buren reversed Andrew Jackson's policies and sought peace at home, as well as abroad. Instead of settling a financial dispute between American citizens and the Mexican government by force, Van Buren wanted to seek a diplomatic solution. In August 1837, Van Buren denied Texas' formal request to join the United States, again prioritizing sectional harmony over territorial expansion. This action particularly angered pro-slavery leaders, who thought he did not want to admit Texas as a slave state per the Missouri Compromise.
In the case of the ship "Amistad", Van Buren sided with the Spanish Government to return the kidnapped slaves. Van Buren oversaw the movement of Cherokee, Choctaw, Creek, Chickasaw and Seminole tribes from Georgia, Tennessee, Alabama, and South Carolina to the Oklahoma territory, executing the orders passed under Jackson. To help secure Florida, Van Buren also continued the Second Seminole War, which had begun while Jackson was in office. Fighting was not resolved until 1842, after Van Buren had left office.
In 1839, Joseph Smith, Jr., the founder of the Latter Day Saint movement visited Van Buren to plead for the U.S. to help roughly 20,000 Mormon settlers of Independence, Missouri, who were forced from the state during the 1838 Mormon War there. The Governor of Missouri, Lilburn Boggs, had issued an executive order on October 27, 1838, known as the "Extermination Order". It authorized troops to use force against Mormons to "exterminate or drive [them] from the state". In 1839, after moving to Illinois, Smith and his party appealed to members of Congress and to President Van Buren to intercede for the Mormons. According to Smith's grandnephew, Van Buren said to Smith, "Your cause is just, but I can do nothing for you; if I take up for you I shall lose the vote of Missouri".
Slavery.
Though he did vote against the admission of Missouri as a slave state, and though he would be the nominated presidential candidate of the Free Soil Party, an anti-slavery political party, in 1848, there was no ambiguity in his position on the abolition of slavery during his term of office. Van Buren considered slavery morally wrong but sanctioned by the Constitution. When it came to the issue of slavery in D.C. and slavery in the United States, he was against its abolition, and said so in his Inaugural Address in 1837: "I believed it a solemn duty fully to make known my sentiments in regard to it [slavery], and now, when every motive for misrepresentation has passed away, I trust that they will be candidly weighed and understood.
"I must go into the Presidential chair the inflexible and uncompromising opponent of every attempt on the part of Congress to abolish slavery in the District of Columbia against the wishes of the slaveholding States, and also with a determination equally decided to resist the slightest interference with it in the States where it exists." Slavery would be abolished in the District of Columbia on April 18, 1862.
Judicial appointments.
Supreme Court.
Van Buren appointed two Justices to the Supreme Court of the United States:
Van Buren appointed eight other federal judges, all to United States district courts.
Some sources incorrectly state that Van Buren appointed John Catron to the Supreme Court. Catron was appointed by Jackson on Jackson's last day in office, and confirmed a few days later, after Van Buren's term had begun.
Election of 1840.
Van Buren took the blame for hard times, as Whigs ridiculed him as "Martin Van Ruin". Van Buren's rather elegant personal style was also an easy target for Whig attacks, such as the Gold Spoon Oration. State elections of 1837 and 1838 were disastrous for the Democrats, and the partial economic recovery in 1838 was offset by a second commercial crisis in that year. Nevertheless, Van Buren controlled his party and was unanimously renominated by the Democrats in 1840. The revolt against Democratic rule led to the election of William Henry Harrison, the Whig candidate. Van Buren once mentioned his relief upon leaving office: "As to the presidency, the two happiest days of my life were those of my entrance upon the office and my surrender of it."
Later life.
On the expiration of his term, Van Buren returned to his estate, Lindenwald in Kinderhook, where he planned his return to the White House. He seemed likely to be nominated by the Democrats in 1844, but in April of that year a Van Buren letter to William H. Hammett was made public. In it, Van Buren opposed the immediate annexation of Texas, but said that he would support annexation once the state of war between Texas and Mexico was resolved. Van Buren's opposition to immediate annexation cost him the support of pro-slavery Democrats; he began the Democratic National Convention with a majority of the delegates, but with no southern support he could not reach the two-thirds threshold required for nomination. His name was withdrawn after eight ballots, and a dark horse, James K. Polk, received the nomination and went on to win the presidency.
Van Buren was increasingly opposed to slavery, and his original attempts to accommodate pro-slavery southerners gave way over time to acceptance of anti-slavery positions including opposing slavery's expansion into newly organized western states. In 1848, he was nominated for President by two minor parties, first by the "Barnburner" faction of the Democratic Party in New York, then by the Free Soil Party, with whom the "Barnburners" coalesced. The Barnburners and Free Soilers opposed Democratic nominee Lewis Cass, who opposed the Wilmot Proviso and was otherwise seen as friendly to slavery. In addition, Van Buren, who had been denied the 1844 nomination by Cass supporters despite having begun the convention with a majority of delegates, likely ran in order to exact a measure of revenge by denying Cass the presidency. Van Buren won no electoral votes, but finished second to Whig nominee Zachary Taylor in New York, taking enough votes from Cass to give the state—and perhaps the election—to Taylor.
Unlike many anti-slavery Democrats of the 1840s and 1850s, who later joined the Republican Party, Van Buren and most of his followers remained in the Democratic fold, including his son John Van Buren and Samuel J. Tilden, who later served as Governor of New York and was the Democratic nominee for President in 1876. Van Buren supported Franklin Pierce for President in 1852, and James Buchanan in 1856, though he later opposed the Buchanan administration's efforts to accommodate the southern states when they threatened secession.
In the election of 1860, he supported Stephen A. Douglas, the candidate of northern Democrats, and helped create a fusion ticket in New York of Democratic electors pledged to both Douglas and John C. Breckinridge, but Abraham Lincoln carried New York and every northern state except New Jersey. Once the American Civil War began, Van Buren made public his support for the Union, and supported Abraham Lincoln's efforts to prevent the southern states from seceding. In April, 1861 former President Pierce wrote to the other living former Presidents and asked them to consider meeting in order to use their stature and influence to propose a negotiated end to the war. Pierce asked Van Buren to use his role as the senior living ex-President to issue a formal call. Van Buren's reply suggested that Buchanan should be the one to call the meeting, since he was the former President who had served most recently, and nothing more resulted from Pierce's proposal.
Van Buren's health began to fail later in 1861, and he was bedridden with pneumonia during the fall and winter of 1861-1862. He did not recover, and died of bronchial asthma and heart failure at his Lindenwald estate in Kinderhook at 2:00 a.m. on July 24, 1862 at the age of 79. He is buried in the Kinderhook Reformed Dutch Church Cemetery, as are his wife Hannah, his parents, and his son Martin Van Buren, Jr.
Memorials.
Counties.
Counties are named for Martin Van Buren in Michigan, Iowa, Arkansas, and Tennessee. Cass County, Missouri was originally named for Van Buren, and was renamed in 1849 to honor Lewis Cass.
Cities and towns.
Cities and towns named for Van Buren include:
Arkansas: Van Buren, Arkansas.
Indiana:<br>Van Buren, Indiana<br>Van Buren Township, Clay County, Indiana<br>Van Buren Township, Brown County, Indiana<br>Van Buren Township, Monroe County, Indiana<br>Van Buren Township, Grant County, Indiana<br>Van Buren Township, Pulaski County, Indiana<br>Van Buren Township, Fountain County, Indiana<br>Van Buren Township, LaGrange County, Indiana<br>Van Buren Township, Madison County, Indiana<br>Van Buren Township, Kosciusko County, Indiana<br>Van Buren Township, Daviess County, Indiana<br>Van Buren Township, Shelby County, Indiana.
In addition, Van Buren Township in LaPorte County, Indiana was later merged with Noble Township.
Iowa: Van Buren Township, Jackson County, Iowa, Van Buren Township, Lee County, Iowa.
Louisiana: Van Buren, Livingston Parish. The original parish seat, it is now abandoned.
Maine: Van Buren, Maine.
Michigan: Van Buren Charter Township, Michigan and Martin, Michigan. In addition, the now-defunct village of Martinsville in Sumpter Township was named for him.
Missouri: Van Buren, Missouri.
Minnesota: Van Buren Township, St. Louis County, Minnesota.
Mississippi: Van Buren, Mississippi (defunct).
New York: Van Buren, New York.
Ohio:<br>Van Buren (a village in Hancock County)<br>Van Buren Township, Shelby County, Ohio. This township started to be populated by white settlers in the early 1830s. It was incorporated in 1835, and its government organized in 1841.<br>Van Buren Township, Putnam County, Ohio. Originally part of Blanchard Township, it was surveyed in 1821, became home to its first white settlers in 1835, and was organized in 1843.<br>Van Buren Township, Darke County, Ohio<br>Van Buren Township, Hancock County, Ohio.
Tennessee: Van Buren, Hardeman County (unincorporated). Established in 1831, this unincorporated populated area is located at the intersection of Van Buren and Lake Hardeman Roads, and shares a ZIP code with Hickory Valley.
Wisconsin: Van Buren, Grant County. In 1841 this unincorporated area was combined with unincorporated areas named for Lafayette and Osceola to form the incorporated town of Potosi.
State parks.
Van Buren State Park and Van Buren Trail State Park in Michigan, and Ohio's Van Buren State Park and its Van Buren Lake are named for him.
Mountains.
Mount Van Buren on the Palmer Land portion of Antarctica was named for Martin Van Buren.
Ships.
The "USS Van Buren", a United States Navy schooner in service from 1839 to 1847 was also named for Martin Van Buren.
In popular culture.
During the 1988 campaign for President, George H. W. Bush, a Yale University graduate and member of the Skull and Bones secret society, was attempting to become the first sitting Vice President to win election to the Presidency since Van Buren. In the comic strip "Doonesbury" artist Garry Trudeau depicted members of Skull and Bones as attempting to rob Van Buren's grave, apparently intending to use the relics in a ritual that would aid Bush in the election.
On the television show "Seinfeld", the episode "The Van Buren Boys" is about a fictional street gang that admires Van Buren and bases its rituals and symbols on him, including the hand sign of eight fingers pointing up. Eight fingers signifies Van Buren, the eighth President.
Martin Van Buren was portrayed by Nigel Hawthorne in the 1997 historical drama film "Amistad".
In an early scene of the film "Two Faces of January", the main characters – American expatriates in Athens – encounter an American tourist and discover that she is a Van Buren descendant. They then argue over whether Martin Van Buren was the seventh or eighth President.
The "USS Van Buren" is a fictional Navy aircraft carrier named for President Van Buren which has appeared in the television show "".
References.
Sources
Further reading
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="19765" url="http://en.wikipedia.org/wiki?curid=19765" title="Melbourne Cricket Ground">
Melbourne Cricket Ground

The Melbourne Cricket Ground (MCG) is an Australian sports stadium located in Yarra Park, Melbourne, Victoria, and is home to the Melbourne Cricket Club. It is the 12th-largest stadium in the world, the largest in Australia, the largest in the Southern Hemisphere, the largest cricket ground by capacity, and has the tallest light towers of any sporting venue. The MCG is within walking distance of the city centre and is served by the Richmond railway station, Richmond, and the Jolimont railway station, East Melbourne. It is part of the Melbourne Sports and Entertainment Precinct.
Internationally, the MCG is remembered as the centrepiece stadium of the 1956 Summer Olympics, the 2006 Commonwealth Games and two Cricket World Cups: 1992 and 2015. The open-air stadium is also famous for its role in the development of international cricket; it was the venue for the world's first Test cricket match in 1877, between Australia and England. The annual Boxing Day Test match is one of the MCG's most popular events. Referred to as "the spiritual home of Australian rules football", it hosts AFL matches in the winter, with at least one game (though usually more) held there each round. The stadium fills to capacity for the AFL Grand Final in late September.
The MCG, also known as "The G", has hosted other major events, including International rules football matches between Australia and Ireland, international rugby union matches, State of Origin series (rugby league), FIFA World Cup qualifiers and international friendly matches, serves as the finish line for the Melbourne Marathon and also hosts major rock concerts.
Until the 1970s, more than 120,000 people sometimes crammed into the venue – the record crowd standing at around 130,000 for a Billy Graham evangelistic crusade in 1959, followed by 121,696 for the 1970 VFL Grand Final. Grandstand redevelopments and occupational health and safety legislation have now limited the maximum seating capacity to approximately 95,000 with an additional 5000 standing room capacity, bringing the total capacity to 100,024.
The MCG is listed on the Victorian Heritage Register and was included on the Australian National Heritage List on 26 December 2005.
Early history.
Founded in November 1838 the Melbourne Cricket Club (MCC) selected the current MCG site in 1853 after previously playing at several grounds around Melbourne. The club’s first game was against a military team at the Old Mint site, at the corner of William and Latrobe Streets. Burial Hill (now Flagstaff railway station) became its home ground in January 1839, but the area was already set aside for Botanical Gardens and the club was moved on in October 1846, to an area on the south bank of the Yarra about where the Herald and Weekly Times building is today. The area was subject to flooding, forcing the club to move again, this time to a ground in South Melbourne.
It was not long before the club was forced out again, this time because of the expansion of the railway. The South Melbourne ground was in the path of Victoria’s first steam railway line from Melbourne to Sandridge (now Port Melbourne). Governor La Trobe offered the MCC a choice of three sites; an area adjacent to the existing ground, a site at the junction of Flinders and Spring Streets or a ten-acre (about 4 hectares) section of the Government Paddock at Richmond next to Richmond Park.
This last option, which is now Yarra Park, had been used by Aborigines until 1835. Between 1835 and 1853 it was an agistment area for colonial troopers’ horses. In 1850 it was part of a 200 acre stretch set aside for public recreation extending from Governor La Trobe’s Jolimont Estate to the Yarra River. By 1853 it had become a busy promenade for Melbourne residents.
An MCC sub-committee chose the Richmond Park option because it was level enough for cricket but sloped enough to prevent inundation. That ground was located where the Richmond, or outer, end of the current MCG is now.
At the same time the Richmond Cricket Club was given occupancy rights to six acres (2.4 hectares) for another cricket ground on the eastern side of the Government Paddock.
At the time of the land grant the Government stipulated that the ground was to be used for cricket and cricket only. This condition remained until 1933 when the State Government allowed the MCG’s uses to be broadened to include other purposes when not being used for cricket.
In 1863 a corridor of land running diagonally across Yarra Park was granted to the Hobson’s Bay Railway and divided Yarra Park from the river. The area closest to the river was also developed for sporting purposes in later years including Olympic venues in 1956.
Stadium development.
The first grandstand at the MCG was the original wooden members’ stand built in 1854, while the first public grandstand was a 200-metre long 6000-seat temporary structure built in 1861. Another grandstand seating 2000, facing one way to the cricket ground and the other way to the park where football was played, was built in 1876 for the 1877 visit of James Lillywhite's English cricket team. It was during this tour that the MCG hosted the world's first Test match.
In 1881 the original members' stand was sold to the Richmond Cricket Club for £55. A new brick stand, considered at the time to be the world’s finest cricket facility, was built in its place. The foundation stone was laid by Prince George of Wales and Prince Albert Victor on 4 July and the stand opened in December that year. It was also in 1881 that a telephone was installed at the ground, and the wickets and goal posts were changed from an east-west orientation to north-south. In 1882 a scoreboard was built which showed details of the batsman's name and how he was dismissed.
When the Lillywhite tour stand burnt down in 1884 it was replaced by a new stand which seated 450 members and 4500 public. In 1897, second-storey wings were added to ‘The Grandstand’, as it was known, increasing capacity to 9,000. In 1900 it was lit with electric light.
More stands were built in the early 20th century. An open wooden stand was on the south side of the ground in 1904 and the 2084-seat Grey Smith Stand (known as the New Stand until 1912) was erected for members in 1906. The 4000-seat Harrison Stand on the ground’s southern side was built in 1908 followed by the 8000-seat Wardill Stand in 1912. In the 15 years after 1897 the stand capacity at the ground increased to nearly 20,000.
In 1927 the second brick members’ stand was replaced at a cost of £60,000. The Harrison and Wardill Stands were demolished in 1936 to make way for the Southern Stand which was completed in 1937. The Southern Stand seated 18,200 under cover and 13,000 in the open and was the main public area of the MCG. It was where the famous Bay 13 was, the MCG’s equivalent to The Hill at the Sydney Cricket Ground.
The Northern Stand, also known as the Olympic Stand, was built to replace the old Grandstand for the 1956 Olympic Games. Ten years later, the Grey Smith Stand and the open concrete stand next to it were replaced by the Western Stand; the Duke of Edinburgh laid a foundation stone for the Western Stand on 3 March 1967, and it was completed in 1968. In 1986, the Western Stand was renamed the Ponsford Stand in honour of Victorian batsman Bill Ponsford.
The MCG was the home of Australia’s first full colour video scoreboard, which replaced the old scoreboard in 1982, located on Level 4 of the Ponsford Stand, with a second video screen added in 1994 almost directly opposite, on Level 4 of the Olympic stand. In 1985, light towers were installed at the ground, allowing for night football and day-night cricket games.
In 1988 inspections of the old Southern Stand found concrete cancer and provided the opportunity to replace the increasingly run-down 50-year-old facility. The projected cost of $100 million was outside what the Melbourne Cricket Club could afford so the Victorian Football League took the opportunity to part fund the project in return for a 30-year deal to share the ground. The new Great Southern Stand was completed in 1992, in time for the 1992 Cricket World Cup, at a final cost of $150 million.
The 1928 Members' stand, the 1956 Olympic stand and the 1968 Ponsford stand were demolished one by one between late 2003 to 2005 and replaced with a new structure in time for the 2006 Commonwealth Games. Despite now standing as a single unbroken stand, the individual sections retain the names of Ponsford, Olympic and Members Stands. The redevelopment cost exceeded A$400 million and pushed the ground's capacity over the 100,000 mark. Since redevelopment, the highest attendance was the 2010 Grand Final of the AFL with 100,016.
From 2011 until 2013, the Victorian Government and the Melbourne Cricket Club funded a $55 million refurbishment of the facilities of Great Southern Stand, including renovations to entrance gates and ticket outlets, food and beverage outlets, public concourses, toilets, function rooms, replacement of seats on level 2, etc.; the grandstand itself will not be substantially modified as part of the refurbishment. In 2013, it was announced that the Video screens originally installed as part of the Northern Stand redevelopment, would be removed and replaced with new scoreboards that are over 2 times larger than the former screens. After the 2013 AFL season concluded, the old screens were dismantled and the new screens were assembled. The new scoreboards were unveiled to the public on 16 December 2013 before the first cricket game with the new screens 4 days later.
Prior to the 2015 ICC Cricket World Cup the scrolling advertising units around the oval fence were dismantled and was replaced with LED units across the oval fence and just above the level 1 seating area.
A panoramic west view of the MCG in August 2009 during an AFL match between Essendon and Hawthorn.
Cricket.
Early years.
The first cricket match was played on 30 September 1854.
The first inter-colonial cricket match to be played at the MCG was between Victoria and New South Wales in March 1856. Victoria had played Tasmania (then known as Van Diemen's Land) as early as 1851 but the Victorians had included two professionals in the 1853 team upsetting the Tasmanians and causing a cooling of relations between the two colonies. To replace the disgruntled Tasmanians the Melbourne Cricket Club issued a challenge to play any team in the colonies for £1000. Sydney publican William Tunks accepted the challenge on behalf of New South Wales although the Victorians were criticised for playing for money. Ethics aside, New South Wales could not afford the £1000 and only managed to travel to Melbourne after half the team’s travel cost of £181 was put up by Sydney barrister Richard Driver.
The game eventually got under way on 26 March 1856. The Victorians, stung by criticism over the £1000 stake, argued over just about everything; the toss, who should bat first, whether different pitches should be used for the different innings and even what the umpires should wear.
Victoria won the toss but New South Wales captain George Gilbert successfully argued that the visiting team should decide who bats first. The MCG was a grassless desert and Gilbert, considering players fielded without boots, promptly sent Victoria into bat. Needing only 16 to win in the final innings, New South Wales collapsed to be 5 for 5 before Gilbert’s batting saved the game and the visitors won by three wickets.
In subsequent years conditions at the MCG improved but the ever-ambitious Melburnians were always on the lookout for more than the usual diet of club and inter-colonial games. In 1861, Felix William Spiers and Christopher Pond, the proprietors of the Cafe de Paris in Bourke Street and caterers to the MCC, sent their agent, W.B. Mallam, to England to arrange for a cricket team to visit Australia.
Mallam found a team and, captained by Heathfield Stephenson, it arrived in Australia on Christmas Eve 1861 to be met by a crowd of more than 3000 people. The team was taken on a parade through the streets wearing white-trimmed hats with blue ribbons given to them for the occasion. Wherever they went they were mobbed and cheered by crowds to the point where the tour sponsors had to take them out of Melbourne so that they could train undisturbed.
Their first game was at the MCG on New Year’s Day 1862, against a Victorian XVIII. The Englishmen also wore coloured sashes around their waists to identify each player and were presented with hats to shade them from the sun. Some estimates put the crowd at the MCG that day at 25,000. It must have been quite a picture with a new 6000 seat grandstand, coloured marquees ringing the ground and a carnival outside. Stephenson said that the ground was better than any in England. The Victorians however, were no match for the English at cricket and the visitors won by an innings and 96 runs.
Over the four days of the ‘test’ more than 45,000 people attended and the profits for Speirs and Pond from this game alone was enough to fund the whole tour. At that time it was the largest number of people to ever watch a cricket match anywhere in the world. Local cricket authorities went out of their way to cater for the needs of the team and the sponsors. They provided grounds and sponsors booths without charge and let the sponsors keep the gate takings. The sponsors however, were not so generous in return. They quibbled with the Melbourne Cricket Club about paying £175 for damages to the MCG despite a prior arrangement to do so.
The last match of the tour was against a Victorian XXII at the MCG after which the English team planted an elm tree outside the ground.
Following the success of this tour, a number of other English teams also visited in subsequent years. George Parr’s side came out in 1863–64 and there were two tours by sides led by W.G. Grace. The fourth tour was led by James Lillywhite.
First Test match.
Up until the fourth tour in 1877, led by Lillywhite, touring teams had played first-class games against the individual colonial sides, but Lillywhite felt that his side had done well enough against New South Wales to warrant a game against an All Australian team.
When Lillywhite headed off to New Zealand he left Melbourne cricketer John Conway to arrange the match for their return. Conway ignored the cricket associations in each colony and selected his own Australian team, negotiating directly with the players. Not only was the team he selected of doubtful representation but it was also probably not the strongest available as some players had declined to take part for various reasons. Demon bowler Fred Spofforth refused to play because wicket-keeper Billy Murdoch was not selected. Paceman Frank Allan was at Warnambool Agricultural Show and Australia’s best all-rounder Edwin Evans could not get away from work. In the end only five Australian-born players were selected.
The same could be said for Lillywhite’s team which, being selected from only four counties, meant that some of England’s best players did not take part. In addition, the team had a rough voyage back across the Tasman Sea and many members had been seasick. The game was due to be played on 15 March, the day after their arrival, but most had not yet fully recovered. On top of that, wicket-keeper Ted Pooley was still in a New Zealand prison after a brawl in a Christchurch pub.
England was nonetheless favourite to win the game and the first ever Test match began with a crowd of only 1000 watching. The Australians elected Dave Gregory from New South Wales as Australia’s first ever captain and on winning the toss he decided to bat.
Charles Bannerman scored an unbeaten 165 before retiring hurt. Sydney Cricket Ground curator, Ned Gregory, playing in his one and only Test for Australia, scored Test cricket’s first duck. Australia racked up 245 and 104 while England scored 196 and 108 giving Australia victory by 45 runs. The win hinged on Bannerman’s century and a superb bowling performance by Tom Kendall who took of 7 for 55 in England’s second innings.
A fortnight later there was a return game, although it was really more of a benefit for the English team. Australia included Spofforth, Murdoch and T.J.D. Cooper in the side but this time the honours went to England who won by four wickets.
Two years later Lord Harris brought another England team out and during England’s first innings in the Test at the MCG, Fred Spofforth took the first hat-trick in Test cricket. He bagged two hauls of 6 for 48 and 7 for 62 in Australia’s ten wicket win.
Later cricket.
On Boxing Day 1866 an Indigenous Australian cricket team played at the MCG with 11,000 spectators against an MCC team. A few players in that match were in a later team that toured England in 1868. Some also played in three other matches at the ground before 1869.
By the 1880s the tradition of England-Australia cricket tours was well established, with a total of eight Tests having been played, five of them at the MCG, two at the Sydney Cricket Ground and one at The Oval in London. In 1882, England lost to a visiting Australian team in England for the first time. The match was played at The Oval in August on what was said to be a difficult pitch. Australian bowler Fred Spofforth decimated the English batting after a shocking start by the Australians and the result was a nailbiting finish in which Australia won by seven runs – still one of the closest finishes in Test cricket history. The defeat was widely recorded in the English press and a mock obituary was published in "The Sporting Times", lamenting the death of English cricket and noted that "the body will be cremated and the ashes taken to Australia".
Later that year, the Honourable Ivo Bligh led a team of eight amateurs and four professionals to Australia to recover them, with the first two matches of the tour played at the MCG. The first being a timeless match (as was the custom in those days) that commenced on 30 December. On New Year's Day the attendance was 23,000, and Australia won the match by nine wickets in three days. The second match commenced on 19 January 1883 and was won comfortably by England by an innings and 27 runs.
Two further matches were played by the tourists in Sydney, with the first being won by England and the second by Australia. The second Sydney match was subsequently deemed to not be of "Test" status, so England had won with the series and had "recovered The Ashes" as Bligh had set out to do. A group of Melbourne women presented Bligh with a small urn and the Ashes tradition was then firmly established.
Donald Bradman's record at the MCG is an average of 128 runs in 17 innings. In the 11 Tests that he played there, he made at least one century in nine of them.
Australia’s highest first class score was posted at the MCG when Victoria made 1107 against New South Wales in 1926–27. Jack Ryder scored 295 for the Vics and hit six sixes in the process.
Highlights and lowlights.
One of the most sensational incidents in Test cricket occurred at the MCG during the Melbourne test of the 1954–55 England tour of Australia. Big cracks had appeared in the pitch during a very hot Saturday’s play and on the rest day Sunday, groundsman Jack House watered the pitch to close them up. This was illegal and the story was leaked by The Age newspaper. The teams agreed to finish the match and England won by 128 runs after Frank Tyson took 7 for 27 in the final innings.
An incident in the second Test of the 1960–61 series involved the West Indies player Joe Solomon being given out after his hat fell on the stumps after being bowled at by Richie Benaud. The crowd sided with the West Indies over the Australians.
Not only was the first Test match played at the MCG, the first One Day International match was also played there, on 5 January 1971, between Australia and England. Australia won the 40-over match by 5 wickets. The next ODI was played on August 1972, some 19 months later.
In March 1977, the Australian Cricket Board assembled 218 of the surviving 224 Australia-England players for a Test match to celebrate 100 years of Test cricket between the two nations. The match was the idea of former Australian bowler and MCC committee member Hans Ebeling who had been responsible for developing the cricket museum at the MCG.
The match had everything. England’s Derek Randall scored 174, Australia’s Rod Marsh also got a century, Lillee took 11 wickets, and David Hookes, in his first test, smacked five fours in a row off England captain Tony Greig’s bowling. Rick McCosker who opened for Australia suffered a fractured jaw after being hit by a sharply rising delivery. He left the field but came back in the second innings with his head swathed in bandages. Australia won the match by 45 runs, exactly the same margin as the first test in 1877.
An incident occurred in 1981 when Indian batsmen Sunil Gavaskar and Chetan Chauhan walked off the field in a test against Australia. Gavaskar was unhappy with the umpire’s decision to give him out lbw.
Another incident occurred on 1 February 1981 at the end of a one-day match between Australia and New Zealand. New Zealand, batting second, needed six runs off the last ball of the day to tie the game. Australian captain, Greg Chappell instructed his brother Trevor, who was bowling the last over, to send the last ball down underarm to prevent the New Zealand batsman, Brian McKechnie, from hitting the ball for six. Although not entirely in the spirit of the game, an underarm delivery was quite legal, so long as the arm was kept straight. The Laws of cricket have since been changed to prevent such a thing happening again. The incident has long been a sore point between Australia and New Zealand. Chappell’s decision was taken against the advice of his vice-captain Rod Marsh and other senior players. On the surface it seems baffling. McKechnie was a tailender who had just come to the crease. His chances of hitting his first ball for six on the vast MCG were apparently nil and even if he did manage to get it over the fence New Zealand would not win but only draw the game. However, the series was tied and draw would mean both teams would have to front up again for another match. Chappell wanted the game and the series finished to give his players a rest.
In February and March 1985 the Benson & Hedges World Championship of Cricket was played at the MCG, a One Day International tournament involving all of the then Test match playing countries to celebrate 150 years of the Australian state of Victoria. Some matches were also played at Sydney Cricket Ground.
The MCG hosted the historic 1992 Cricket World Cup final between Pakistan and England with a crowd of more than 87,000. Pakistan won the match after sterling all-round performance by Wasim Akram who scored 33 runs and picked up 3 crucial wickets to make Pakistan cricket world champions for the first and as yet only time.
During the 1995 Boxing Day Test at the MCG, Australian umpire Darrell Hair called Sri Lankan spin bowler Muttiah Muralitharan for throwing the ball, rather than bowling it, seven times during the match. The other umpires did not call him once and this caused a controversy, although he was later called for throwing by other umpires seven other times in different matches.
The MCG is known for its great atmosphere, much of which is generated in the infamous Bay 13, situated almost directly opposite to the members stand. In the late 1980s, the crowd at Bay 13 would often mimic the warm up stretches performed by Merv Hughes. In a 1999 One-Day International, the behaviour of Bay 13 was so bad that Shane Warne, donning a helmet for protection, had to enter the ground from his dressing rooms and tell the crowd to settle down at the request of opposing England captain Alec Stewart.
The MCG hosted 3 pool games as part of the 2015 ICC Cricket World Cup as well as a quarter-final, and then the final on March 29. The MCG became the second cricket ground to host more than one Cricket World Cup final, having hosted the 1992 as well. The only other ground is Lords Cricket Ground, which has hosted four finals in 1975,1979,1983,1999.
Australian rules football.
Origins.
Despite being called the Melbourne Cricket Ground, the stadium has been and continues to be used much more often for Australian rules football. Spectator numbers for football are larger than for any other sport in Australia, and it makes more money for the MCG than any of the other sports played there.
Although the Melbourne Cricket Club members were instrumental in founding Australian Rules Football, there were understandable concerns in the early days about the damage that might be done to the playing surface if football was allowed to be played at the MCG. Therefore, football games were often played in the parklands next to the cricket ground, and this was the case for the first documented football match to be played at the ground.
Headmasters of Australian schools following the trend in British public schools, introducing the idea of muscular Christianity through sport, and football in particular. Two headmasters, Dr John E. Bromby of Melbourne Church of England Grammar and St. Kilda Grammar’s William C. Northcott, arranged a game of football between their schools in 1858. The game was played on the open area just north of the Melbourne Cricket Ground on 31 July, the week of Bryant’s published offer, and was won by St Kilda Grammar.
The next game was organised between Scotch College and Melbourne Grammar a week later. The game began on Saturday, 7 August and bore little resemblance to Australian Football as it is played today. There were no written rules and no umpires. The field was a paddock with no boundaries and trees for goal posts. About 40 players on each side kicked a round leather ball inflated by a pigs bladder. The game commenced about midday, probably with a place kick, and the rules at that time said that the winner was the first team to score two goals. By 5 pm it was getting dark and as both teams had scored only one goal they all had to come back two weeks later on 21 August for the next instalment. Again neither scored and a fortnight later on 4 September both teams returned to continue playing. Again there was no score. The match was declared a draw and that was the end of the first football season. A plaque in Yarra Park, outside the current MCG, commemorates this match between Scotch College and Melbourne Grammar as the first Australian Rules Football match.
It wasn’t until 1869 that football was played on the MCG proper, a trial game involving a police team. It was not for another ten years, in 1879, after the formation of the Victorian Football Association, that the first official match was played on the MCG and the cricket ground itself became a regular venue for football. Night matches were even played that year using specially erected light towers.
In the early years, the MCG was the home ground of Melbourne Football Club, Australia’s oldest club, established in 1859 by the founder of the game itself, Thomas Wills. Melbourne won five Challenge Cup premierships during the 1870s using the MCG as its home ground.
The first of nearly 2500 Victorian Football League/Australian Football League games to be played at the MCG was on 15 May 1897, with Melbourne beating Geelong 64 to 19.
Several Australian Football League (AFL) clubs later joined Melbourne as in using the MCG as their home ground: Richmond (1965), North Melbourne (1985), Essendon (1992), Collingwood (started moving in 1994, became full-time tenant in 2000) and Hawthorn (2000).
Finals and Grand Finals.
The VFL/AFL Grand Final has been played at the MCG every season since 1902, except in 1924, when no Grand Final was held because of the season's round-robin finals format (it hosted three of the six games in the Finals Series), 1942–1945, when the ground was used by the military during World War II; and in 1991, as the construction of the Great Southern Stand had temporarily reduced the ground’s capacity below that of Waverley Park. All three Grand Final Replays have been played at the MCG.
Before the ground was fully seated, the Grand Final could draw attendances above 110,000. The record for the highest attendance in the history of both the venue and the sport was set in the 1970 VFL Grand Final, with 121,696 in attendance.
Since being fully seated, Grand Final attendances are typically between 95,000 and 100,000, with the record of 100,016 attending the first 2010 AFL Grand Final (which ended in a draw, requiring a replay).
In the modern era, most finals games held in Melbourne have been played at the MCG. Under the current contract, ten finals (excluding the Grand Final) must be played at the MCG over a five-year period. Under previous contracts, the MCG was entitled to host at least one match in each week of the finals, which on two occasions required a non-Victorian club to play a "home" final in Victoria.
The MCG and the VFL/AFL.
For many years the VFL had an uneasy relationship with the MCG trustees and the Melbourne Cricket Club. Both needed the other, but resented the dependence. The VFL made the first move which brought things to a head by beginning the development of VFL Park at Mulgrave in the 1960s as its own home ground and as a potential venue for future grand finals. Then in 1983, president of the VFL, Alan Aylett started to pressure the MCG Trust to give the VFL a greater share of the money it made from using the ground for football.
In March 1983 the MCG trustees met to consider a submission from Aylett. Aylett said he wanted the Melbourne Cricket Club’s share of revenue cut from 15 per cent to 10 per cent. He threatened to take the following day’s opening game of the season, Collingwood vs Melbourne, away from the MCG. The money was held aside until an agreement could be reached.
Different deals, half deals and possible deals were done over the years, with the Premier of Victoria, John Cain, even becoming involved. Cain was said to have promised the VFL it could use the MCG for six months of the year and then hand it back to the MCC, but this never eventuated, as the MCG Trust did not approve it. In the mid-1980s, a deal was done where the VFL was given its own members area in the Southern Stand.
Against this background of political manoeuvring, in 1985 North Melbourne became the third club to make the MCG its home ground. In the same year, North played in the first night football match at the MCG for almost 110 years, against Collingwood on 29 March 1985.
In 1986, only a month after Ross Oakley had taken over as VFL Commissioner, VFL executives met with the MCC and took a big step towards resolving their differences. Changes in the personnel at the MCC also helped. In 1983 John Lill was appointed secretary and Don Cordner its president.
Shortly after the Southern Stand opened in 1992, the Australian Football League moved its headquarters into the complex. The AFL assisted with financing the new stand and came to an agreement that ensures at least 45 AFL games are played at the MCG each year, including the Grand Final in September. Another 45 days of cricket are also played there each year and more than 3.5 million spectators come to watch every year.
As of the end of 2011, Matthew Richardson holds the records for having scored the most goals on the MCG, and Kevin Bartlett holds the record for playing the most matches at the MCG. Two players have scored 14 goals for an AFL or VFL game in one match at the MCG: Gary Ablett, Sr. in 1989 and 1993, and John Longmire in 1990.
Before an AFL match between Richmond and Carlton on 27 August 1999, the city end scoreboard caught on fire due to an electrical fault, causing the start of play to be delayed by half an hour.
A panoramic view of the Melbourne Cricket Ground from level 4 of the Northern Stand, First game of the 2010 AFL Season between Richmond and Carlton
World War II.
During World War II, the government requisitioned the MCG for military use. From 1942 until 1945 it was occupied by (in order): the US Army Air Forces, the Royal Australian Air Force, the US Marine Corps and again the RAAF. Over the course of the war, more than 200,000 personnel were barracked at the MCG. From April to October 1942, the US Army’s Fifth Air Force occupied the ground, naming it "Camp Murphy", in honor of officer Colonel William Murphy, a senior USAAF officer killed in Java. In 1943 the MCG was home to the legendary First Regiment of the First Division of the United States Marine Corp. The First Marine Corp were the heroes of the battle of Guadalcanal (and later Okinawa) used the "cricket grounds", as the marines referred to it, to rest and recuperate. On 14 March 1943 the marines hosted a giant "get together" of US and Australian troops on the arena.
In 1977, Melbourne Cricket Club president Sir Albert Chadwick and Congressional Medal of Honour winner Colonel Mitchell Page unveiled a commemorative plaque recognizing the Americans' time at the ground.
In the 2010 TV miniseries, "The Pacific", episode 3, members of the US Marines are shown to be camped in the war-era MCG.
Olympic Games.
The MCG’s most famous moment in history was as the main stadium for the 1956 Olympic Games. The MCG was only one of seven possible venues, including the Melbourne Showgrounds, for the Games’ main arena. The MCG was the Federal Government’s preferred venue but there was resistance from the MCC. The inability to decide on the central venue nearly caused the Games to be moved from Melbourne. Prime Minister Robert Menzies recognised the potential embarrassment to Australia if this happened and organised a three-day summit meeting to thrash things out. Attending was Victorian Premier John Cain, the Prime Minister, deputy opposition leader Arthur Calwell, all State political leaders, civic leaders, Olympic officials and trustees and officials of the MCC. Convening the meeting was no small effort considering the calibre of those attending and that many of the sports officials were only part-time amateurs.
As 22 November, the date of the opening ceremony, drew closer, Melbourne was gripped ever more tightly by Olympic fever. At 3 pm the day before the opening ceremony, people began to line up outside the MCG gates. That night the city was paralysed by a quarter of a million people who had come to celebrate.
The MCG's capacity was increased by the new Olympic (or Northern) Stand, and on the day itself 103,000 people filled the stadium to capacity. A young up and coming distance runner was chosen to carry the Olympic torch into the stadium for the opening ceremony.
Although Ron Clarke had a number of junior world records for distances of 1500 m, one mile (1.6 km) and two miles (3 km), he was relatively unknown in 1956. Perhaps the opportunity to carry the torch inspired him because he went on to have a career of exceptional brilliance and was without doubt the most outstanding runner of his day. At one stage he held the world record for every distance from two miles (3 km) to 20 km. His few failures came in Olympic and Commonwealth Games competition. Although favourite for the gold at Tokyo in 1964 he was placed ninth in the 5,000 metres race and the marathon and third in the 10,000 metres. He lost again in the 1966 Commonwealth Games and in 1968 at altitude in Mexico he collapsed at the end of the 10 km race.
On that famous day in Melbourne in 1956 the torch spluttered and sparked, showering Clarke with hot magnesium, burning holes in his shirt. When he dipped the torch into the cauldron it burst into flame singeing him further. In the centre of the ground, John Landy, the fastest miler in the world, took the Olympic oath and sculler Merv Wood carried the Australian flag.
The Melbourne Games also saw the high point of Australian female sprinting with Betty Cuthbert winning three gold medals at the MCG. She won the 100 m and 200 m and anchored the winning 4 x 100 m team. Born in Merrylands in Sydney’s west she was a champion schoolgirl athlete and had already broken the world record for the 200 m just before the 1956 Games. She was to be overshadowed by her Western Suburbs club member, the Marlene Matthews. When they got to the Games, Matthews was the overwhelming favourite especially for the 100 m a distance over which Cuthbert had beaten her just once.
Both Matthews and Cuthbert won their heats with Matthews setting an Olympic record of 11.5 seconds in hers. Cuthbert broke that record in the following heat with a time of 11.4 seconds. The world record of 11.3 was held by another Australian, Shirley Strickland who was eliminated in her heat. In the final Matthews felt she got a bad start and was last at the 50 metre mark. Cuthbert sensed Isabella Daniels from the USA close behind her and pulled out a little extra to win Australia’s first gold at the Games in a time of 11.5 seconds, Matthews was third. The result was repeated in the 200 m final. Cuthbert won her second gold breaking Marjorie Jackson’s Olympic record. Mathews was third again.
By the time the 1956 Olympics came around, Shirley Strickland was a mother of 31 years of age but managed to defend her 80 m title, which she had won in Helsinki four years before, winning gold and setting a new Olympic record.
The sensational incident of the track events was the non-selection of Marlene Matthews in the 4 x 100 m relay. Matthews trained with the relay team up until the selection was made but Cuthbert, Strickland, Fleur Mellor and Norma Croker were picked for the team. There was outrage at the selection which increased when Matthews went on to run third in both the 100 m and 200 m finals. Personally she was devastated and felt that she had been overlooked for her poor baton change. Strickland was disappointed with the way Matthews was treated and maintained it was an opinion held in New South Wales that she had baton problems. One of the selectors, Doris Magee from NSW, said that selecting Matthews increased the risk of disqualification at the change. But Cuthbert maintained that the selectors made the right choice saying that Fleur Mellor was fresh, a specialist relay runner and was better around the curves than Matthews.
The men did not fare so well. The 4 x 400 m relay team, including later IOC Committee member Kevan Gosper, won silver. Charles Porter also won silver in the high jump. Hec Hogan won bronze in the 100 m to become the first Australian man to win a medal in a sprint since the turn of the century and despite injury John Landy won bronze in the 1500 m. Allan Lawrence won bronze in the 10,000 m event.
Apart from athletics, the stadium was also used for the soccer finals, the hockey finals, the Opening and Closing Ceremonies, and an exhibition game of baseball between the Australian National Team and a US armed services team at which an estimated crowd of 114,000 attended. This was the Guinness World Record for the largest attendance for any baseball game, which stood until a 29 March 2008 exhibition game between the Boston Red Sox and Los Angeles Dodgers at the Los Angeles Coliseum (also a former Olympic venue) drawing 115,300.
The MCG was also used for another demonstration sport, Australian Rules. The Olympics being an amateur competition meant that only amateurs could play in the demonstration game. A combined team of amateurs from the VFL and VFA were selected to play a state team from the Victorian Amateur Football Association (VAFA). The game was played 7 December 1956 with the VAFA side, wearing white jumpers, green collars and the Olympic rings on their chests, winning easily 81 to 55.
The MCG’s link with its Olympic past continues to this day. Within its walls is the IOC-endorsed Australian Gallery of Sport and Olympic Museum.
Forty-four years later at the 2000 Summer Olympics in Sydney, the Grounds served as host to several football preliminaries, making it one of a few venues ever used for more than one Olympics. It is quite possible the ground may be one of the first to be used in three Olympic games, as Melbourne's Lord Mayor Robert Doyle, and Premier Ted Ballieu are considering bidding for either the 2024 or 2028 Olympic games, using the MCG as a selling point.
Commonwealth Games.
The Opening and Closing Ceremonies of the 2006 Commonwealth Games were held at the MCG, as well as athletics events during the games. The games began on 15 March and ended on 26 March.
Rugby union.
The first game of Rugby Union to be played on the ground was on Saturday, 29 June 1878, when the Waratah Club of Sydney played Carlton Football Club in a return of the previous year’s contests in Sydney where the clubs had competed in both codes of football. The match, watched by a crowd of between 6,000 and 7,000 resulted in a draw; one goal and one try being awarded to each team.
The next Rugby match was held on Wednesday 29 June 1881, when the Wanderers, a team organised under the auspices of the Melbourne Cricket Club, played a team representing a detached Royal Navy squadron then visiting Melbourne. The squadron team won by one goal and one try to nil.
It was not until 19 August 1899 that the MCG was again the venue for a Union match, this time Victoria v the British Lions (as they were later to be called). During the preceding week the Victorians had held several trial and practice matches there, as well as several training sessions, despite which they were defeated
30-0 on the day before a crowd of some 7,000.
Nine years later, on Monday, 10 August 1908, Victoria was again the host, this time to the Australian team en route to Great Britain and soon to be dubbed the First Wallabies. Despite being held on a working day some 1,500 spectators attended to see the visitors win by 26-6.
On Saturday, 6 July 1912 the MCG was the venue, for the only time ever, of a match between two Victorian Rugby Union clubs, Melbourne and East Melbourne, the former winning 9-5 in what was reported to be ‘... one of the finest exhibitions of the Rugby game ever seen in Victoria.’ It was played before a large crowd as a curtain raiser to a State Rules match against South Australia.
On Saturday 18 June 1921, in another curtain raiser, this time to a Melbourne-Fitzroy League game, a team representing Victoria was soundly beaten 51-0 by the South African Springboks in front of a crowd of 11,214 
It was nine years later, on Saturday 13 September 1930, that the British Lions returned to play Victoria, again before a crowd of 7,000, this time defeating the home side 41-36, a surprisingly narrow winning margin. 
The first post war match at the MCG was on 21 May 1949 when the NZ Maoris outclassed a Southern States side 35-8 before a crowd of close to 10,000. A year later, on 29 July 1950, for the first and only time, Queensland travelled to Victoria to play an interstate match, defeating their hosts 31-12 before a crowd of 7,479. 
In the following year the MCG was the venue for a contest between the New Zealand All Blacks and an Australian XV . This was on 30 June 1951 before some 9,000 spectators and resulted in a convincing 56-11 win for the visitors.
Union did not return the MCG until the late ‘nineties, for several night time Test matches, both Australia v New Zealand All Blacks. The first, on Saturday 26 July 1997, being notable for an attendance of 90,119, the visitors winning 33-18 and the second, on Saturday 11 July 1998, for a decisive victory to Australia of 24-16. The two met again at the MCG on 30 June 2007, the hosts again winning, this time by 20 points to 15. 
Rugby league.
Rugby league was first played at the ground on 15 August 1914, with the New South Wales team losing to England 15–21.
The first ever State of Origin match at the MCG (and second in Melbourne) was Game II of the 1994 series, and the attendance of 87,161 set a new record rugby league crowd in Australia. The MCG was also the venue for Game II of the 1995 State of Origin series and drew 52,994, the most of any game that series. The third game of the 1997 State of Origin series, which, due to the Super League war only featured Australian Rugby League-signed players, was played there too, but only attracted 25,105, the lowest in a series that failed to attract over 35,000 to any game.
The Melbourne Storm played two marquee games at the MCG in 2000. This was the first time that they had played outside of their normal home ground of Olympic Park Stadium which holds 18,500 people.
Their first game was held on 3 March 2000 against the St. George Illawarra Dragons in a rematch of the infamous 1999 Grand Final. Anthony Mundine said they were 'not worthy premiers' and the Storm responded by running in 12 tries to two and winning 70–10 in front of 23,239 fans. This was their biggest crowd they had played against until 33,427 turned up to the 2007 Preliminary Final at Docklands Stadium. The record home and away crowd record has also been overhauled, when a match at Docklands in 2010 against St George attracted 25,480 spectators.
Their second game attracted only 15,535 spectators and was up against the Cronulla Sharks on 24 June 2000. Once again, the Storm won 22–16.
It was announced in June 2014 that the ground will host its first State of Origin match since 1997, with Game II of the 2015 series to be played at the venue.
Soccer.
On 9 February 2006, the then Victorian premier Steve Bracks and Football Federation Australia chairman Frank Lowy announced that the MCG would host a world class football event each year from 2006 until 2009 inclusive. The announcement came as the game gained further popularity in the country following the qualification for the 2006 FIFA World Cup in Germany.
The agreement sees an annual fixture at the MCG, beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103, before Australia left to contest in the World Cup finals. Australia beat Greece 1–0. The Socceroos also hosted a match in 2007 against Argentina, losing 1–0, as well as 2010 FIFA World Cup qualification matches in 2009 against Asian Football heavyweights (Japan) which attracted 81,872 fans as Australia beat Japan 2–1 via 2 Tim Cahill trademark headers after falling behind 1–0 late in the 1st half.
In 2010 it was announced that as a warm up to the 2010 FIFA World Cup which the Australians had qualified for, they would play fellow qualified nation New Zealand on 24 May at the MCG.
Other matches played at the MCG include the following:
Tennis.
In 1878 the Melbourne Cricket Club’s Lawn Tennis Committee laid an asphalt court at the MCG and Victoria’s first game of tennis was played there. A second court of grass was laid in 1879 and the first Victorian Championship played on it in 1880. The first inter-colonial championship was played in 1883 and the first formal inter-state match between NSW and Victoria played in 1884 with Victoria winning.
In 1889 the MCC arranged for tennis to be played at the then Warehousemen’s Cricket Ground, now the Albert Ground, at Albert Park, rather than the MCG.
Cycling.
It was at the MCG in 1869 that Australia’s first bicycle race was held. The event was for velocipedes, crude wooden machines with pedals on the front wheels. In 1898 the Austral Wheel Race was held at the MCG attracting a crowd of 30,000 to see cyclists race for a total of £200 in prize money.
VFL/AFL Records.
"Last updated: 1 October 2013"
Statues.
Outside of the MCG are statues of famous Australian athletes donated by Tattersalls and known as the "Parade of Champions", including many Australian rules football and cricket legends.
They include:
The MCC, in association with Australia Post, commissioned five statues for the "Australia Post Avenue of Legends". Three have been erected so far:
A statue is also planned for cricket player Neil Harvey.
External links.
class="wikitable succession-box collapsible autocollapse" style="margin: 0 auto 0 auto; font-size:95%;clear:both;"
Events and tenants

</doc>
<doc id="19766" url="http://en.wikipedia.org/wiki?curid=19766" title="Marshall Plan">
Marshall Plan

The Marshall Plan (officially the European Recovery Program, ERP) was an American initiative to aid Europe, in which the United States gave $17 billion (approximately $120 billion in current dollar value) in economic support to help rebuild European economies after the end of World War II. The plan was in operation for four years beginning in April 1948. The goals of the United States were to rebuild war-devastated regions, remove trade barriers, modernize industry, make Europe prosperous again, and prevent the spread of communism. The Marshall Plan required a lessening of interstate barriers, a dropping of many petty regulations constraining business, and encouraged an increase in productivity, labour union membership, as well as the adoption of modern business procedures.
The Marshall Plan aid was divided amongst the participant states roughly on a per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The largest recipient of Marshall Plan money was the United Kingdom (receiving about 26% of the total), followed by France (18%) and West Germany (11%). Some 18 European countries received Plan benefits. Although offered participation, the Soviet Union refused Plan benefits, and also blocked benefits to Eastern Bloc countries, such as East Germany and Poland. The United States provided similar aid programs in Asia, but they were not called "Marshall Plan".
The initiative is named after Secretary of State George Marshall. The plan had bipartisan support in Washington, where the Republicans controlled Congress and the Democrats controlled the White House with Harry S. Truman as president. The Plan was largely the creation of State Department officials, especially William L. Clayton and George F. Kennan, with help from Brookings Institution, as requested by Senator Arthur H. Vandenberg, chairman of the Senate Foreign Relations Committee. Marshall spoke of an urgent need to help the European recovery in his address at Harvard University in June 1947. 
The phrase "equivalent of the Marshall Plan" is often used to describe a proposed large-scale economic rescue program.
Development and deployment.
The reconstruction plan, developed at a meeting of the participating European states, was drafted on June 5, 1947. It offered the same aid to the Soviet Union and its allies, but they refused to accept it, as to do so would be to allow a degree of US control over the Communist economies. In fact, the Soviet Union even prevented its satellite states (i.e. East Germany, Poland, etc.) from accepting. Secretary Marshall became convinced that Stalin had absolutely no interest in helping restore economic health in Western Europe. President Harry Truman signed the Marshall Plan on April 3, 1948, granting $5 billion in aid to 16 European nations. During the four years that the plan was operational, US donated $13 billion in economic and technical assistance to help the recovery of the European countries that had joined in the Organization for European Economic Co-operation. In 2013, the equivalent sum reflecting currency inflation since 1948 totalled roughly $148 billion. The $13 billion was in the context of a US GDP of $258 billion in 1948, and was on top of $13 billion in American aid to Europe between the end of the war and the start of the Plan that is counted separately from the Marshall Plan. The Marshall Plan was replaced by the Mutual Security Plan at the end of 1951.
The ERP addressed each of the obstacles to postwar recovery. The plan looked to the future, and did not focus on the destruction caused by the war. Much more important were efforts to modernize European industrial and business practices using high-efficiency American models, reducing artificial trade barriers, and instilling a sense of hope and self-reliance.
By 1952, as the funding ended, the economy of every participant state had surpassed pre-war levels; for all Marshall Plan recipients, output in 1951 was at least 35% higher than in 1938. Over the next two decades, Western Europe enjoyed unprecedented growth and prosperity, but economists are not sure what proportion was due directly to the ERP, what proportion indirectly, and how much would have happened without it. A common American interpretation of the program's role in European recovery is the one expressed by Paul Hoffman, head of the Economic Cooperation Administration, in 1949, when he told Congress that Marshall aid had provided the "critical margin" on which other investment needed for European recovery depended. The Marshall Plan was one of the first elements of European integration, as it erased trade barriers and set up institutions to coordinate the economy on a continental level—that is, it stimulated the total political reconstruction of western Europe.
Belgian economic historian Herman Van der Wee concludes the Marshall Plan was a "great success":
Wartime destruction.
By the end of World War II, much of Europe was devastated. Sustained aerial bombardment during the war had badly damaged most major cities, and industrial facilities were especially hard-hit. The region's trade flows had been thoroughly disrupted; millions were in refugee camps living on aid from United Nations Relief and Rehabilitation Administration and other agencies. Food shortages were severe, especially in the harsh winter of 1946–1947. From July 1945 Through June 1946, the United States shipped 16.5 million tons of food, primarily wheat, to Europe and Japan. It amounted to 1/6 of the American food supply, and provided 35 trillion calories, enough to provide 400 calories a day for one year to 300 million people.
Especially damaged was transportation infrastructure, as railways, bridges, and docks had been specifically targeted by air strikes, while much merchant shipping had been sunk. Although most small towns and villages had not suffered as much damage, the destruction of transportation left them economically isolated. None of these problems could be easily remedied, as most nations engaged in the war had exhausted their treasuries in its execution.
The only major powers whose infrastructure had not been significantly harmed in World War II were the United States and Canada. They were much more prosperous than before the war but exports were a small factor in their economy. Much of the Marshall Plan aid would be used by the Europeans to buy manufactured goods and raw materials from the United States and Canada.
Initial post-war events.
Slow recovery.
Europe's economies were recovering slowly, as unemployment and food shortages led to strikes and unrest in several nations. In 1947 the European economies were still well below their pre-war levels and were showing few signs of growth. Agricultural production was 83% of 1938 levels, industrial production was 88%, and exports only 59%. In Britain the situation was not as severe.
In Germany in 1945–46 housing and food conditions were bad, as the disruption of transport, markets and finances slowed a return to normality. In the West, bombing had destroyed 5,000,000 houses and apartments, and 12,000,000 refugees from the east had crowded in. Food production was only two-thirds of the pre-war level in 1946–48, while normal grain and meat shipments no longer arrived from the East. Furthermore, the large shipments of food stolen from occupied nations during the war no longer reached Germany. Industrial production fell more than half and reached pre-war levels only at the end of 1949.
While Germany struggled to recover from the destruction of the War, the recovery effort began in June of 1948, moving on from emergency relief. The currency reform in 1948 was headed by the military government and helped Germany to restore stability by encouraging production. The reform revalued old currency and deposits and introduced new currency. Taxes were also reduced and Germany prepared to remove economic roadblocks.
During the first three years of occupation of Germany the UK and US vigorously pursued a military disarmament program in Germany, partly by removal of equipment but mainly through an import embargo on raw materials, part of the Morgenthau Plan approved by President Franklin D. Roosevelt.
Nicholas Balabkins concludes that "as long as German industrial capacity was kept idle the economic recovery of Europe was delayed." By July 1947 Washington realized that economic recovery in Europe could not go forward without the reconstruction of the German industrial base, deciding that an "orderly, prosperous Europe requires the economic contributions of a stable and productive Germany." In addition, the strength of Moscow-controlled communist parties in France and Italy worried Washington.
In the view of the State Department under President Harry S. Truman, the United States needed to adopt a definite position on the world scene or fear losing credibility. The emerging doctrine of containment (as opposed to rollback) argued that the United States needed to substantially aid non-communist countries to stop the spread of Soviet influence. There was also some hope that the Eastern European nations would join the plan, and thus be pulled out of the emerging Soviet bloc, but that did not happen.
In January 1947, Truman appointed retired General George Marshall as Secretary of State. In July 1947 Marshall scrapped Joint Chiefs of Staff Directive 1067 implemented as part of the Morgenthau Plan under the personal supervision of Roosevelt's treasury secretary Henry Morgenthau, Jr., which had decreed "take no steps looking toward the economic rehabilitation of Germany [or] designed to maintain or strengthen the German economy." Thereafter, JCS 1067 was supplanted by JCS 1779, stating that "an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The restrictions placed on German heavy industry production were partly ameliorated; permitted steel production levels were raised from 25% of pre-war capacity to a new limit placed at 50% of pre-war capacity.
With a Communist insurgency threatening Greece, and Britain financially unable to continue its aid, the President announced his Truman Doctrine on 12 March 1947, "to support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures", with an aid request for consideration and decision, concerning Greece and Turkey. Also in March 1947, former US President Herbert Hoover, in one of his reports from Germany, argued for a change in US occupation policy, amongst other things stating:
"There is the illusion that the New Germany left after the annexations can be reduced to a 'pastoral state' (Morgenthau's vision). It cannot be done unless we exterminate or move 25,000,000 people out of it."
Hoover further noted that, "The whole economy of Europe is interlinked with German economy through the exchange of raw materials and manufactured goods. The productivity of Europe cannot be restored without the restoration of Germany as a contributor to that productivity." Hoover's report led to a realization in Washington that a new policy was needed; "almost any action would be an improvement on current policy." In Washington, the Joint Chiefs declared that the "complete revival of German industry, particularly coal mining" was now of "primary importance" to American security.
The United States was already spending a great deal to help Europe recover. Over $14 billion was spent or loaned during the postwar period through the end of 1947, and is not counted as part of the Marshall Plan. Much of this aid was designed to restore infrastructure and help refugees. Britain, for example, received an emergency loan of $3.75 billion.
The United Nations also launched a series of humanitarian and relief efforts almost wholly funded by the United States. These efforts had important effects, but they lacked any central organization and planning, and failed to meet many of Europe's more fundamental needs. Already in 1943, the United Nations Relief and Rehabilitation Administration (UNRRA) was founded to provide relief to areas liberated from Germany. UNRRA provided billions of dollars of rehabilitation aid, and helped about 8 million refugees. It ceased operations in the Displaced persons camp of Europe in 1947; many of its functions were transferred to several UN agencies.
Soviet negotiations.
After Marshall's appointment in January 1947, administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets in their occupied zone. The Soviets took a punitive approach, pressing for a delay rather than an acceleration in economic rehabilitation, demanding unconditional fulfillment of all prior reparation claims, and pressing for progress toward nationwide socioeconomic transformation.
After six weeks of negotiations, Molotov rejected all of the American and British proposals. Molotov also rejected the counter-offer to scrap the British-American "Bizonia" and to include the Soviet zone within the newly constructed Germany. Marshall was particularly discouraged after personally meeting with Stalin to explain that the United States could not possibly abandon its position on Germany, while Stalin expressed little interest in a solution to German economic problems.
Marshall's speech.
After the adjournment of the Moscow conference following six weeks of failed discussions with the Soviets regarding a potential German reconstruction, the United States concluded that a solution could not wait any longer.
To clarify the US's position, a major address by Secretary of State George Marshall was planned. Marshall gave the address to the graduating class of Harvard University on June 5, 1947. Standing on the steps of Memorial Church in Harvard Yard, he offered American aid to promote European recovery and reconstruction. The speech described the dysfunction of the European economy and presented a rationale for US aid.
The modern system of the division of labor upon which the exchange of products is based is in danger of breaking down. ... Aside from the demoralizing effect on the world at large and the possibilities of disturbances arising as a result of the desperation of the people concerned, the consequences to the economy of the United States should be apparent to all. It is logical that the United States should do whatever it is able to do to assist in the return of normal economic health to the world, without which there can be no political stability and no assured peace. Our policy is not directed against any country, but against hunger, poverty, desperation and chaos. Any government that is willing to assist in recovery will find full co-operation on the part of the USA.
Its purpose should be the revival of a working economy in the world so as to permit the emergence of political and social conditions in which free institutions can exist.
Marshall was convinced that economic stability would provide political stability in Europe. He offered aid, but the European countries had to organize the program themselves.
The speech, written by Charles Bohlen, contained virtually no details and no numbers. More a proposal than a plan, it was a challenge to European leaders to cooperate and coordinate. It asked Europeans to create their own plan for rebuilding Europe, indicating the United States would then fund this plan. The administration felt that the plan would likely be unpopular among many Americans, and the speech was mainly directed at a European audience. In an attempt to keep the speech out of American papers journalists were not contacted, and on the same day Truman called a press conference to take away headlines. In contrast, Dean Acheson, an Under Secretary of State, was dispatched to contact the European media, especially the British media, and the speech was read in its entirety on the BBC.
Rejection by the Soviets.
British Foreign Secretary Ernest Bevin heard Marshall's radio broadcast speech and immediately contacted French Foreign Minister Georges Bidault to begin preparing a quick European response to (and acceptance of) the offer. The two agreed that it would be necessary to invite the Soviets as the other major allied power. Marshall's speech had explicitly included an invitation to the Soviets, feeling that excluding them would have been a sign of distrust. State Department officials, however, knew that Stalin would almost certainly not participate, and that any plan that would send large amounts of aid to the Soviets was unlikely to be approved by Congress.
Initial reactions.
While the Soviet ambassador in Washington suspected that the Marshall Plan could lead to the creation of an anti-Soviet bloc, Stalin was open to the offer. He directed that—in negotiations to be held in Paris regarding the aid—countries in the Eastern Bloc should not reject economic conditions being placed upon them. Stalin only changed his outlook when he learned that (a) credit would only be extended under conditions of economic cooperation and,(b) aid would also be extended to Germany in total, an eventuality which Stalin thought would hamper the Soviets' ability to exercise influence in western Germany.
Initially, Stalin maneuvered to kill the Plan, or at least hamper it by means of destructive participation in the Paris talks regarding conditions. He quickly realized, however, that this would be impossible after Molotov reported—following his arrival in Paris in July 1947—that conditions for the credit were non-negotiable. Looming as just as large a concern was the Czechoslovak eagerness to accept the aid, as well as indications of a similar Polish attitude.
Stalin suspected a possibility that these Eastern Bloc countries might defy Soviet directives not to accept the aid, potentially causing a loss of control of the Eastern Bloc. In addition, the most important condition was that every country choosing to take advantage of the plan would need to have its economic situation independently assessed—a level of scrutiny to which the Soviets could not agree. Bevin and Bidault also insisted that any aid be accompanied by the creation of a unified European economy, something incompatible with the strict Soviet command economy.
Compulsory Eastern Bloc rejection.
Soviet Foreign Minister Vyacheslav Molotov left Paris, rejecting the plan. Thereafter, statements were made suggesting a future confrontation with the West, calling the United States both a "fascizing" power and the "center of worldwide reaction and anti-Soviet activity," with all U.S.-aligned countries branded as enemies. The Soviets also then blamed the United States for communist losses in elections in Belgium, France and Italy months earlier, in the spring of 1947. It claimed that "marshallization" must be resisted and prevented by any means, and that French and Italian communist parties were to take maximum efforts to sabotage the implementation of the Plan. In addition, Western embassies in Moscow were isolated, with their personnel being denied contact with Soviet officials.
On July 12, a larger meeting was convened in Paris. Every country of Europe was invited, with the exceptions of Spain (a World War II neutral that had sympathized with Axis powers) and the small states of Andorra, San Marino, Monaco, and Liechtenstein. The Soviet Union was invited with the understanding that it would likely refuse. The states of the future Eastern Bloc were also approached, and Czechoslovakia and Poland agreed to attend. In one of the clearest signs of Soviet control over the region, the Czechoslovakian foreign minister, Jan Masaryk, was summoned to Moscow and berated by Stalin for thinking of joining the Marshall Plan. Polish Prime minister Józef Cyrankiewicz was rewarded by Stalin for the Polish rejection of the Plan. Russia rewarded Poland with a lucrative five-year trade agreement, the equivalent of 450 million 1948 dollars ($4.4 billion in 2014 dollars) in credit, 200,000 tons of grain, heavy machinery, and factories.
The Marshall Plan participants were not surprised when the Czechoslovakian and Polish delegations were prevented from attending the Paris meeting. The other Eastern European states immediately rejected the offer. Finland also declined in order to avoid antagonizing the Soviets (see also Finlandization). The Soviet Union's "alternative" to the Marshall plan, which was purported to involve Soviet subsidies and trade with western Europe, became known as the Molotov Plan, and later, the COMECON. In a 1947 speech to the United Nations, Soviet deputy foreign minister Andrei Vyshinsky said that the Marshall Plan violated the principles of the United Nations. He accused the United States of attempting to impose its will on other independent states, while at the same time using economic resources distributed as relief to needy nations as an instrument of political pressure.
Szklarska Poręba meeting.
In late September, the Soviet Union called a meeting of nine European Communist parties in southwest Poland. A Communist Party of the Soviet Union (CPSU) report was read at the outset to set the heavily anti-Western tone, stating now that "international politics is dominated by the ruling clique of the American imperialists" which have embarked upon the "enslavement of the weakened capitalist countries of Europe". Communist parties were to struggle against the U.S. presence in Europe by any means necessary, including sabotage. The report further claimed that "reactionary imperialist elements throughout the world, particularly in the U.S.A., in Britain and France, had put particular hope on Germany and Japan, primarily on Hitlerite Germany—first as a force most capable of striking a blow at the Soviet Union".
Referring to the Eastern Bloc, the report stated that "the Red Army's liberating role was complemented by an upsurge of the freedom-loving peoples' liberation struggle against the fascist predators and their hirelings." It argued that "the bosses of Wall Street" were "tak[ing] the place of Germany, Japan and Italy". The Marshall Plan was described as "the American plan for the enslavement of Europe". It described the world now breaking down "into basically two camps—the imperialist and antidemocratic camp on the one hand, and the antiimperialist and democratic camp on the other".
Although the Eastern Bloc countries except Czechoslovakia had immediately rejected Marshall Plan aid, Eastern Bloc communist parties were blamed for permitting even minor influence by non-communists in their respective countries during the run up to the Marshall Plan. The meeting's chair, Andrei Zhdanov, who was in permanent radio contact with the Kremlin from whom he received instructions, also castigated communist parties in France and Italy for collaboration with those countries' domestic agendas. Zhdanov warned that if they continued to fail to maintain international contact with Moscow to consult on all matters, "extremely harmful consequences for the development of the brother parties' work" would result.
Italian and French communist leaders were prevented by party rules from pointing out that it was actually Stalin who had directed them not to take opposition stances in 1944. The French communist party, as others, was then to redirect its mission to "destroy capitalist economy" and that the Soviet Communist Information Bureau (Cominform) would take control of the French Communist Party's activities to oppose the Marshall Plan. When they asked Zhdanov if they should prepare for armed revolt when they returned home, he did not answer. In a follow-up conversation with Stalin, he explained that an armed struggle would be impossible and that the struggle against the Marshall Plan was to be waged under the slogan of national independence.
Negotiations.
Turning the plan into reality required negotiations among the participating nations, and to get the plan through the United States Congress. Sixteen nations met in Paris to determine what form the American aid would take, and how it would be divided. The negotiations were long and complex, with each nation having its own interests. France's major concern was that Germany not be rebuilt to its previous threatening power. The Benelux countries (Belgium, Netherlands and Luxemburg), despite also suffering under the Nazis, had long been closely linked to the German economy and felt their prosperity depended on its revival. The Scandinavian nations, especially Sweden, insisted that their long-standing trading relationships with the Eastern bloc nations not be disrupted and that their neutrality not be infringed.
The United Kingdom insisted on special status as a longstanding belligerent during the war, concerned that if it were treated equally with the devastated continental powers it would receive virtually no aid. The Americans were pushing the importance of free trade and European unity to form a bulwark against communism. The Truman administration, represented by William L. Clayton, promised the Europeans that they would be free to structure the plan themselves, but the administration also reminded the Europeans that implementation depended on the plan's passage through Congress. A majority of Congress members were committed to free trade and European integration, and were hesitant to spend too much of the money on Germany. However, before the Marshall Plan was in effect, France, Austria, and Italy needed immediate aid. On December 17, 1947, the United States agreed to give $40 million to France, Austria, China, and Italy.
Agreement was eventually reached and the Europeans sent a reconstruction plan to Washington. In the document the Europeans asked for $22 billion in aid. Truman cut this to $17 billion in the bill he put to Congress. The plan encountered sharp opposition in Congress, mostly from the portion of the Republican Party led by Robert A. Taft that advocated a more isolationist policy and was weary of massive government spending. The plan also had opponents on the left, Henry A. Wallace notably among them. Wallace saw the plan as a subsidy for American exporters and sure to polarize the world between East and West.
Wallace, the former vice president and secretary of agriculture, mockingly called this the "Martial Plan", arguing that it was just another step towards war. However, opposition against the Marshall Plan was greatly reduced by the shock of the Communist coup in Czechoslovakia in February 1948. Soon after, a bill granting an initial $5 billion passed Congress with strong bipartisan support. Congress would eventually allocate $12.4 billion in aid over the four years of the plan.
On March 17, 1948, President Harry S. Truman addressed European security and condemned the Soviet Union before a hastily convened Joint Session of Congress. Attempting to contain spreading Soviet influence in Eastern Europe, Truman asked Congress to restore a peacetime military draft and to swiftly pass the Economic Cooperation Act, the name given to the Marshall Plan. Of the Soviet Union Truman said, "The situation in the world today is not primarily the result of the natural difficulties which follow a great war. It is chiefly due to the fact that one nation has not only refused to cooperate in the establishment of a just and honorable peace but—even worse—has actively sought to prevent it."
Members of the Republican-controlled 80th Congress (1947–1949) were skeptical. "In effect, he told the Nation that we have lost the peace, that our whole war effort was in vain.", noted Representative Frederick Smith of Ohio. Others thought he had not been forceful enough to contain the USSR. "What [Truman] said fell short of being tough", noted Representative Eugene Cox, a Democrat from Georgia, "there is no prospect of ever winning Russian cooperation." Despite its reservations, the 80th Congress implemented Truman's requests, further escalating the Cold War with the USSR.
Truman signed the Economic Cooperation Act into law on April 3, 1948; the Act established the Economic Cooperation Administration (ECA) to administer the program. ECA was headed by economic cooperation administrator Paul G. Hoffman. In the same year, the participating countries (Austria, Belgium, Denmark, France, West Germany, the United Kingdom, Greece, Iceland, Ireland, Italy, Luxembourg, the Netherlands, Norway, Sweden, Switzerland, Turkey, and the United States) signed an accord establishing a master financial-aid-coordinating agency, the Organization for European Economic Cooperation (later called the Organization for Economic Cooperation and Development or OECD), which was headed by Frenchman Robert Marjolin.
Implementation.
The first substantial aid went to Greece and Turkey in January 1947, which were seen as the front line of the battle against communist expansion, and were already receiving aid under the Truman Doctrine. Initially, Britain had supported the anti-communist factions in those countries, but due to its dire economic condition it decided to pull out and in February 1947 requested the U.S. to continue its efforts. The ECA formally began operation in July 1948.
The ECA's official mission statement was to give a boost to the European economy: to promote European production, to bolster European currency, and to facilitate international trade, especially with the United States, whose economic interest required Europe to become wealthy enough to import U.S. goods. Another unofficial goal of ECA (and of the Marshall Plan) was the containment of growing Soviet influence in Europe, evident especially in the growing strength of communist parties in Czechoslovakia, France, and Italy.
The Marshall Plan money was transferred to the governments of the European nations. The funds were jointly administered by the local governments and the ECA. Each European capital had an ECA envoy, generally a prominent American businessman, who would advise on the process. The cooperative allocation of funds was encouraged, and panels of government, business, and labor leaders were convened to examine the economy and see where aid was needed.
The Marshall Plan aid was mostly used for the purchase of goods from the United States. The European nations had all but exhausted their foreign exchange reserves during the war, and the Marshall Plan aid represented almost their sole means of importing goods from abroad. At the start of the plan these imports were mainly much-needed staples such as food and fuel, but later the purchases turned towards reconstruction needs as was originally intended. In the latter years, under pressure from the United States Congress and with the outbreak of the Korean War, an increasing amount of the aid was spent on rebuilding the militaries of Western Europe. Of the some $13 billion allotted by mid-1951, $3.4 billion had been spent on imports of raw materials and semi-manufactured products; $3.2 billion on food, feed, and fertilizer; $1.9 billion on machines, vehicles, and equipment; and $1.6 billion on fuel.
Also established were counterpart funds, which used Marshall Plan aid to establish funds in the local currency. According to ECA rules 60% of these funds had to be invested in industry. This was prominent in Germany, where these government-administered funds played a crucial role in lending money to private enterprises which would spend the money rebuilding. These funds played a central role in the reindustrialization of Germany. In 1949–50, for instance, 40% of the investment in the German coal industry was by these funds.
The companies were obligated to repay the loans to the government, and the money would then be lent out to another group of businesses. This process has continued to this day in the guise of the state owned KfW bank, (Kreditanstalt für Wiederaufbau, meaning Reconstruction Credit Institute). The Special Fund, then supervised by the Federal Economics Ministry, was worth over DM 10 billion in 1971. In 1997 it was worth DM 23 billion. Through the revolving loan system, the Fund had by the end of 1995 made low-interest loans to German citizens amounting to around DM 140 billion. The other 40% of the counterpart funds were used to pay down the debt, stabilize the currency, or invest in non-industrial projects. France made the most extensive use of counterpart funds, using them to reduce the budget deficit. In France, and most other countries, the counterpart fund money was absorbed into general government revenues, and not recycled as in Germany.
Technical Assistance Program.
The US Bureau of Labor Statistics (BLS) contributed heavily to the success of the Technical Assistance Program. The United States Congress passed a law on June 7, 1940 that allowed the BLS to "make continuing studies of labor productivity" and appropriated funds for the creation of a Productivity and Technological Development Division. The BLS could then use its expertise in the field of productive efficiency to implement a productivity drive in each Western European country receiving Marshall Plan aid.
By implementing technological literature surveys and organized plant visits, American economists, statisticians, and engineers were able to educate European manufacturers in statistical measurement. The goal of the statistical and technical assistance from the Americans was to increase productive efficiency of European manufacturers in all industries.
In order to perform this analysis, the BLS performed two types of productivity calculations. First, they used existing data to calculate how much a worker produces per hour of work—the average output rate. Second, they compared the existing output rates in a particular country to output rates in other nations. By performing these calculations across all industries, the BLS was able to identify the strengths and weaknesses of each country's manufacturing and industrial production. From that, the BLS could recommend technologies (especially statistical) that each individual nation could implement. Often, these technologies came from the United States; by the time the Technical Assistance Program began, the United States used statistical technologies "more than a generation ahead of what [the Europeans] were using".
The BLS used these statistical technologies to create Factory Performance Reports for Western European nations. The American government sent hundreds of technical advisors to Europe in order to observe workers in the field; this on-site analysis made the Factory Performance Reports especially helpful to the manufacturers. In addition, the Technical Assistance Program funded 24,000 European engineers, leaders, and industrialists to visit America and tour America's factories, mines, and manufacturing plants. This way, the European visitors would be able to return to their home countries and implement the technologies used in the United States. The analyses in the Factory Performance Reports and the "hands-on" experience had by the European productivity teams effectively identified productivity deficiencies in European industries; from there, it became clearer how to make European production more effective.
Before the Technical Assistance Program even went into effect, Maurice Tobin (the United States Secretary of Labor) expressed his confidence in American productivity and technology to both American and European economic leaders. He urged that the United States play a large role in improving European productive efficiency by providing four recommendations for the program's administrators: 
The effects of the Technical Assistance Program were not limited to improvements in productive efficiency. While the thousands of European leaders took their work/study trips to the United States, they were able to observe a number of aspects of American society as well. The Europeans could watch local, state, and federal governments work together with citizens in a pluralist society. They observed a democratic society with open universities and civic societies in addition to more advanced factories and manufacturing plants. The Technical Assistance Program allowed Europeans to bring home many types of American ideas.
Another important aspect of the Technical Assistance Program was its low cost. While $19.4 billion was allocated for capital costs in the Marshall Plan, the Technical Assistance Program only required $300 million. Only one-third of that $300 million cost was paid by the United States.
German level of industry restrictions.
Even while the Marshall Plan was being implemented, the dismantling of German industry continued; and in 1949 Konrad Adenauer wrote to the Allies requesting the end of industrial dismantling, citing the inherent contradiction between encouraging industrial growth and removing factories, and also the unpopularity of the policy. Support for dismantling was by this time coming predominantly from the French, and the Petersberg Agreement of November 1949 greatly reduced the levels of deindustrialization, though dismantling of minor factories continued until 1951. The first "level of industry" plan, signed by the Allies on March 29, 1946, had stated that German heavy industry was to be lowered to 50% of its 1938 levels by the destruction of 1,500 listed manufacturing plants.
In January 1946 the Allied Control Council set the foundation of the future German economy by putting a cap on German steel production. The maximum allowed was set at about 5,800,000 tons of steel a year, equivalent to 25% of the pre-war production level. The UK, in whose occupation zone most of the steel production was located, had argued for a more limited capacity reduction by placing the production ceiling at 12 million tons of steel per year, but had to submit to the will of the U.S., France and the Soviet Union (which had argued for a 3 million ton limit). Steel plants thus made redundant were to be dismantled. Germany was to be reduced to the standard of life it had known at the height of the Great Depression (1932). Consequently, car production was set to 10% of pre-war levels, and the manufacture of other commodities was reduced as well.
The first "German level of industry" plan was subsequently followed by a number of new ones, the last signed in 1949. By 1950, after the virtual completion of the by then much watered-down "level of industry" plans, equipment had been removed from 706 manufacturing plants in western Germany and steel production capacity had been reduced by 6,700,000 tons. Vladimir Petrov concludes that the Allies "delayed by several years the economic reconstruction of the war-torn continent, a reconstruction which subsequently cost the United States billions of dollars." In 1951 West Germany agreed to join the European Coal and Steel Community (ECSC) the following year. This meant that some of the economic restrictions on production capacity and on actual production that were imposed by the International Authority for the Ruhr were lifted, and that its role was taken over by the ECSC.
Expenditures.
The Marshall Plan aid was divided amongst the participant states on a roughly per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The table below shows Marshall Plan aid by country and year (in millions of dollars) from "The Marshall Plan Fifty Years Later." There is no clear consensus on exact amounts, as different scholars differ on exactly what elements of American aid during this period were part of the Marshall Plan.
Loans and grants.
The Marshall plan, just as GARIOA, consisted of aid both in the form of grants and in the form of loans. Out of the total, 1.2 billion USD were loan-aid.
Ireland which received 146.2 million USD through the Marshall plan, received 128.2 million USD as loans, and the remaining 18 million USD as grants. By 1969 the Irish Marshall plan debt, which was still being repaid, amounted to 31 million pounds, out of a total Irish foreign debt of 50 million pounds.
The UK received 385 million USD of its Marshall plan aid in the form of loans. Unconnected to the Marshall plan the UK also received direct loans from the US amounting to 4.6 billion USD. The proportion of Marshall plan loans versus Marshall plan grants was roughly 15% to 85% for both the UK and France.
Germany, which up until the 1953 Debt agreement had to work on the assumption that all the Marshall plan aid was to be repaid, spent its funds very carefully. Payment for Marshall plan goods, "counterpart funds", were administered by the Reconstruction Credit Institute, which used the funds for loans inside Germany. In the 1953 Debt agreement the amount of Marshall plan aid that Germany was to repay was reduced to less than 1 billion USD. This made the proportion of loans versus grants to Germany similar to that of France and the UK.
The final German loan repayment was made in 1971. Since Germany chose to repay the aid debt out of the German Federal budget, leaving the German ERP fund intact, the fund was able to continue its reconstruction work. By 1996 it had accumulated a value of 23 billion Deutsche Mark.
Effects and legacy.
The Marshall Plan was originally scheduled to end in 1953. Any effort to extend it was halted by the growing cost of the Korean War and rearmament. American Republicans hostile to the plan had also gained seats in the 1950 Congressional elections, and conservative opposition to the plan was revived. Thus the plan ended in 1951, though various other forms of American aid to Europe continued afterwards.
The years 1948 to 1952 saw the fastest period of growth in European history. Industrial production increased by 35%. Agricultural production substantially surpassed pre-war levels. The poverty and starvation of the immediate postwar years disappeared, and Western Europe embarked upon an unprecedented two decades of growth that saw standards of living increase dramatically. There is some debate among historians over how much this should be credited to the Marshall Plan. Most reject the idea that it alone miraculously revived Europe, as evidence shows that a general recovery was already underway. Most believe that the Marshall Plan sped this recovery, but did not initiate it. Many argue that the structural adjustments that it forced were of great importance. Economic historians J. Bradford DeLong and Barry Eichengreen call it "history's most successful structural adjustment program." One effect of the plan was that it subtly "Americanized" countries, especially Austria, who embraced United States' assistance, through popular culture, such as Hollywood movies and rock n' roll.
The political effects of the Marshall Plan may have been just as important as the economic ones. Marshall Plan aid allowed the nations of Western Europe to relax austerity measures and rationing, reducing discontent and bringing political stability. The communist influence on Western Europe was greatly reduced, and throughout the region communist parties faded in popularity in the years after the Marshall Plan. The trade relations fostered by the Marshall Plan helped forge the North Atlantic alliance that would persist throughout the Cold War. At the same time, the nonparticipation of the states of Eastern Europe was one of the first clear signs that the continent was now divided.
The Marshall Plan also played an important role in European integration. Both the Americans and many of the European leaders felt that European integration was necessary to secure the peace and prosperity of Europe, and thus used Marshall Plan guidelines to foster integration. In some ways this effort failed, as the OEEC never grew to be more than an agent of economic cooperation. Rather it was the separate European Coal and Steel Community, which notably excluded Britain, that would eventually grow into the European Union. However, the OEEC served as both a testing and training ground for the structures that would later be used by the European Economic Community. The Marshall Plan, linked into the Bretton Woods system, also mandated free trade throughout the region.
While some historians today feel some of the praise for the Marshall Plan is exaggerated, it is still viewed favorably and many thus feel that a similar project would help other areas of the world. After the fall of communism several proposed a "Marshall Plan for Eastern Europe" that would help revive that region. Others have proposed a Marshall Plan for Africa to help that continent, and U.S. vice president Al Gore suggested a Global Marshall Plan. "Marshall Plan" has become a metaphor for any very large scale government program that is designed to solve a specific social problem. It is usually used when calling for federal spending to correct a perceived failure of the private sector.
Repayment.
The Organization for European Economic Cooperation took the leading role in allocating funds, and the OEEC arranged for the transfer of the goods. The American supplier was paid in dollars, which were credited against the appropriate European Recovery Program funds. The European recipient, however, was not given the goods as a gift, but had to pay for them (usually on credit) in local currency. These payments were kept by the European government involved in a special counterpart fund. This counterpart money, in turn, could be used by the government for further investment projects. 5% of the counterpart money was paid to the U.S. to cover the administrative costs of the ERP.
The Marshall Plan money was in the form of grants that did not have to be repaid. In addition to ERP grants, the Export-Import Bank (an agency of the U.S. government) at the same time made long-term loans at low interest rates to finance major purchases in the US, all of which were repaid.
In the case of Germany there also were 16 billion marks of debts from the 1920s which had defaulted in the 1930s, but which Germany decided to repay to restore its reputation. This money was owed to government and private banks in the U.S., France and Britain. Another 16 billion marks represented postwar loans by the U.S. Under the London Debts Agreement of 1953, the repayable amount was reduced by 50% to about 15 billion marks and stretched out over 30 years, and compared to the fast-growing German economy were of minor impact.
Areas without the Plan.
Large parts of the world devastated by World War II did not benefit from the Marshall Plan. The only major Western European nation excluded was Francisco Franco's Spain, which did not overtly participate in World War II. After the war, it pursued a policy of self-sufficiency, currency controls, and quotas, with little success. With the escalation of the Cold War, the United States reconsidered its position, and in 1951 embraced Spain as an ally, encouraged by Franco's aggressive anti-communist policies. Over the next decade, a considerable amount of American aid would go to Spain, but less than its neighbors had received under the Marshall Plan.
While the western portion of the Soviet Union had been as badly affected as any part of the world by the war, the eastern portion of the country was largely untouched and had seen a rapid industrialization during the war. The Soviets also imposed large reparations payments on the Axis allies that were in its sphere of influence. Austria, Finland, Hungary, Romania, and especially East Germany were forced to pay vast sums and ship large amounts of supplies to the USSR. These reparation payments meant the Soviet Union itself received about the same as 16 European countries received in total from Marshall Plan aid.
In accordance with the agreements with the USSR shipment of dismantled German industrial installations from the west began on March 31, 1946. Under the terms of the agreement the Soviet Union would in return ship raw materials such as food and timber to the western zones. In view of the Soviet failure to do so, the western zones halted the shipments east, ostensibly on a temporary basis, although they were never resumed. It was later shown that the main reason for halting shipments east was not the behavior of the USSR but rather the recalcitrant behavior of France. Examples of material received by the USSR were
equipment from the Kugel-Fischer ballbearing plant at Schweinfurt, the Daimler-Benz underground aircraft-engine plant at Obrigheim, the Deschimag shipyards at Bremen-Weser, and the Gendorf powerplant.
The USSR did establish COMECON as a riposte to the Marshall Plan to deliver aid for Eastern Bloc countries, but this was complicated by the Soviet efforts to manage their own recovery from the war. The members of Comecon looked to the Soviet Union for oil; in turn, they provided machinery, equipment, agricultural goods, industrial goods, and consumer goods to the Soviet Union. Economic recovery in the east was much slower than in the west, and the economies never fully recovered in the communist period, resulting in the formation of the shortage economies and a gap in wealth between East and West. Finland, which USSR forbade to join the Marshall Plan and which was required to give large reparations to the USSR, saw its economy recover to pre-war levels in 1947. France, which received billions of dollars through the Marshall Plan, similarly saw its average income per person return to almost pre-war level by 1949. By mid-1948 industrial production in Poland, Hungary, Bulgaria, and Czechoslovakia had recovered to a level somewhat above pre-war level.
Aid to Asia.
From the end of the war to the end of 1953, the U.S. provided grants and credits amounting to $5.9 billion to Asian countries, especially China/Taiwan ($1.051 billion), India ($255 million), Indonesia ($215 million), Japan ($2.44 billion), South Korea ($894 million), Pakistan ($98 million) and the Philippines ($803 million). In addition, another $282 million went to Israel and $196 million to the rest of the Middle East. All this aid was separate from the Marshall Plan.
Canada.
Canada, like the United States, was little damaged by the war and in 1945 was one of the world's largest economies. It operated its own aid program. In 1948, the U.S. allowed ERP aid to be used in purchasing goods from Canada. Canada made over a billion dollars in sales in the first two years of operation.
World total.
The total of American grants and loans to the world from 1945 to 1953 came to $44.3 billion.
Criticism.
Laissez-faire criticism.
Initial criticism of the Marshall Plan came from a number of economists. Wilhelm Röpke, who influenced German Minister for Economy Ludwig Erhard in his economic recovery program, believed recovery would be found in eliminating central planning and restoring a market economy in Europe, especially in those countries which had adopted more fascist and corporatist economic policies. Röpke criticized the Marshall Plan for forestalling the transition to the free market by subsidizing the current, failing systems. Erhard put Röpke's theory into practice and would later credit Röpke's influence for West Germany's preeminent success.
Henry Hazlitt criticized the Marshall Plan in his 1947 book "Will Dollars Save the World?", arguing that economic recovery comes through savings, capital accumulation and private enterprise, and not through large cash subsidies. Ludwig von Mises criticized the Marshall Plan in 1951, believing that "the American subsidies make it possible for [Europe's] governments to conceal partially the disastrous effects of the various socialist measures they have adopted".
Some critics and Congressmen at the time believed that America was giving too much aid to Europe. America had already given Europe $9 billion in other forms of help in previous years. The Marshall Plan gave another $13 billion, equivalent to about $100 billion in 2010 value. Critics did not think that it was necessary for Americans to be using so much money to help nations they had already assisted in many ways before.
Modern criticism.
Criticism of the Marshall Plan became prominent among historians of the revisionist school, such as Walter LaFeber, during the 1960s and 1970s. They argued that the plan was American economic imperialism, and that it was an attempt to gain control over Western Europe just as the Soviets controlled Eastern Europe. In a review of West Germany's economy from 1945 to 1951, German analyst Werner Abelshauser concluded that "foreign aid was not crucial in starting the recovery or in keeping it going". The economic recoveries of France, Italy, and Belgium, Cowen found, also predated the flow of U.S. aid. Belgium, the country that relied earliest and most heavily on free market economic policies after its liberation in 1944, experienced the fastest recovery and avoided the severe housing and food shortages seen in the rest of continental Europe.
Former US Chairman of the Federal Reserve Bank Alan Greenspan gives most credit to Ludwig Erhard for Europe's economic recovery. Greenspan writes in his memoir "The Age of Turbulence" that Erhard's economic policies were the most important aspect of postwar Western Europe recovery, far outweighing the contributions of the Marshall Plan. He states that it was Erhard's reductions in economic regulations that permitted Germany's miraculous recovery, and that these policies also contributed to the recoveries of many other European countries. Its recovery is attributed to traditional economic stimuli, such as increases in investment, fueled by a high savings rate and low taxes. Japan saw a large infusion of US investment during the Korean War. 
Criticism of the Marshall Plan also aims at showing that it began a legacy of disastrous foreign aid programs. Since the 1990s, economic scholarship has been more hostile to the idea of foreign aid. For example, Alberto Alesina and Beatrice Weder, summing up economic literature on foreign aid and corruption, find that aid is primarily used wastefully and self-servingly by government officials, and ends up increasing governmental corruption. This policy of promoting corrupt government is then attributed back to the initial impetus of the Marshall Plan.
Noam Chomsky wrote that the amount of American dollars given to France and the Netherlands equaled the funds these countries used to finance their military actions against their colonial subjects in East Asia, in French Indochina and the Netherlands East Indies respectively. The Marshall Plan was said to have "set the stage for large amounts of private U.S. investment in Europe, establishing the basis for modern transnational corporations". The Netherlands received U.S. aid for economic recovery in the Netherlands Indies. However, in January 1949, the American government suspended this aid in response to the Dutch efforts to restore colonial rule in Indonesia during the Indonesian National Revolution, and it implicitly threatened to suspend Marshall aid to the Netherlands if the Dutch government continued to oppose the independence of Indonesia.
In popular culture.
Alfred Friendly, press aide to the US Secretary of Commerce W. Averell Harriman, wrote a humorous operetta about the Marshall Plan during its first year; one of the lines in the operetta was: "Wines for Sale; will you swap / A little bit of steel for Chateau Neuf du Pape?"
The Spanish comedy film" Welcome Mr. Marshall! "tells the story of a small Spanish town, Villar del Río, which hears of the visit of American diplomats and begins preparations to impress the American visitors in the hopes of benefitting under the Marshall Plan.
External links.
Listen to this article ()
This audio file was created from a revision of the "Marshall Plan" article dated 2012-12-17, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="19769" url="http://en.wikipedia.org/wiki?curid=19769" title="Mariculture">
Mariculture

Mariculture is a specialized branch of aquaculture involving the cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater. An example of the latter is the farming of marine fish, including finfish and shellfish like prawns, or oysters and seaweed in saltwater ponds. Non-food products produced by mariculture include: fish meal, nutrient agar, jewellery (e.g. cultured pearls), and cosmetics.
Methods.
Shellfish.
Similar to algae cultivation, shellfish can be farmed in multiple ways: on ropes, in bags or cages, or directly on (or within) the intertidal substrate. Shellfish mariculture does not require feed or fertilizer inputs, nor insecticides or antibiotics, making shellfish aquaculture (or 'mariculture') a self-supporting system. Shellfish can also be used in multi-species cultivation techniques, where shellfish can utilize waste generated by higher trophic level organisms.
Open ocean.
Raising marine organisms under controlled conditions in exposed, high-energy ocean environments beyond significant coastal influence, is a relatively new approach to mariculture. Open ocean aquaculture (OOA) uses cages, nets, or long-line arrays that are moored, towed or float freely. Research and commercial open ocean aquaculture facilities are in operation or under development in Panama, Australia, Chile, China, France, Ireland, Italy, Japan, Mexico, and Norway. As of 2004, two commercial open ocean facilities were operating in U.S. waters, raising Threadfin near Hawaii and cobia near Puerto Rico. An operation targeting bigeye tuna recently received final approval. All U.S. commercial facilities are currently sited in waters under state or territorial jurisdiction. The largest deep water open ocean farm in the world is raising cobia 12km off the northern coast of Panama in highly exposed sites.
Enhanced stocking.
Enchanced Stocking (also known as sea ranching) is a Japanese principle based on operant conditioning and the migratory nature of certain species. The fishermen raise hatchlings in a closely knitted net in a harbor, sounding an underwater horn before each feeding. When the fish are old enough they are freed from the net to mature in the open sea. During spawning season, about 80% of these fish return to their birthplace. The fishermen sound the horn and then net those fish that respond.
Seawater ponds.
In seawater pond mariculture, fish are raised in ponds which receive water from the sea. This has the benefit that the nutrition (e.g. microorganisms) present in the seawater can be used. This is a great advantage over traditional fish farms (e.g. sweet water farms) for which the farmers buy feed (which is expensive). Other advantages are that water purification plants may be planted in the ponds to eliminate the buildup of nitrogen, from fecal and other contamination. Also, the ponds can be left unprotected from natural predators, providing another kind of filtering.
Environmental effects.
Mariculture has rapidly expanded over the last two decades due to new technology, improvements in formulated feeds, greater biological understanding of farmed species, increased water quality within closed farm systems, greater demand for seafood products, site expansion and government interest. As a consequence, mariculture has been subject to some controversy regarding its social and environmental impacts. Commonly identified environmental impacts from marine farms are:
As with most farming practices, the degree of environmental impact depends on the size of the farm, the cultured species, stock density, type of feed, hydrography of the site, and husbandry methods. The adjacent diagram connects these causes and effects.
Wastes from cage cultures.
Mariculture of finfish can require a significant amount of fishmeal or other high protein food sources. Originally, a lot of fishmeal went to waste due to inefficient feeding regimes and poor digestibility of formulated feeds which resulted in poor feed conversion ratios. 
In cage culture, several different methods are used for feeding farmed fish – from simple hand feeding to sophisticated computer-controlled systems with automated food dispensers coupled with "in situ" uptake sensors that detect consumption rates. In coastal fish farms, overfeeding primarily leads to increased disposition of detritus on the seafloor (potentially smothering seafloor dwelling invertebrates and altering the physical environment), while in hatcheries and land-based farms, excess food goes to waste and can potentially impact the surrounding catchment and local coastal environment. This impact is usually highly local, and depends significantly on the settling velocity of waste feed and the current velocity (which varies both spatially and temporally) and depth.
Farm escapees and invasives.
The impact of escapees from aquaculture operations depends on whether or not there are wild conspecifics or close relatives in the receiving environment, and whether or not the escapee is reproductively capable. Several different mitigation/prevention strategies are currently employed, from the development of infertile triploids to land-based farms which are completely isolated from any marine environment. Escapees can adversely impact local ecosystems through hybridization and loss of genetic diversity in native stocks, increase negative interactions within an ecosystem (such as predation and competition), disease transmission and habitat changes (from trophic cascades and ecosystem shifts to varying sediment regimes and thus turbidity).
The accidental introduction of invasive species is also of concern. Aquaculture is one of the main vectors for invasives following accidental releases of farmed stocks into the wild. One example is the Siberian sturgeon ("Acipenser baerii") which accidentally escaped from a fish farm into the Gironde Estuary (Southwest France) following a severe storm in December 1999 (5,000 individual fish escaped into the estuary which had never hosted this species before). Molluscan farming is another example whereby species can be introduced to new environments by ‘hitchhiking’ on farmed molluscs. Also, farmed molluscs themselves can become dominate predators and/or competitors, as well as potentially spread pathogens and parasites.
Genetic pollution and disease and parasite transfer.
One of the primary concerns with mariculture is the potential for disease and parasite transfer. Farmed stocks are often selectively bred to increase disease and parasite resistance, as well as improving growth rates and quality of products. As a consequence, the genetic diversity within reared stocks decreases with every generation – meaning they can potentially reduce the genetic diversity within wild populations if they escape into those wild populations. Such genetic pollution from escaped aquaculture stock can reduce the wild population’s ability to adjust to the changing natural environment. Also, maricultured species can harbour diseases and parasites (e.g., lice) which can be introduced to wild populations upon their escape. An example of this is the parasitic sea lice on wild and farmed Atlantic salmon in Canada. Also, non-indigenous species which are farmed may have resistance to, or carry, particular diseases (which they picked up in their native habitats) which could be spread through wild populations if they escape into those wild populations. Such ‘new’ diseases would be devastating for those wild populations because they would have no immunity to them.
Habitat modification.
With the exception of benthic habitats directly beneath marine farms, most mariculture causes minimal destruction to habitats. However, the destruction of mangrove forests from the farming of shrimps is of concern. Globally, shrimp farming activity is a small contributor to the destruction of mangrove forests; however, locally it can be devastating. Mangrove forests provide rich matrices which support a great deal of biodiversity – predominately juvenile fish and crustaceans. Furthermore, they act as buffering systems whereby they reduce coastal erosion, and improve water quality for in situ animals by processing material and ‘filtering’ sediments.
Others.
In addition, nitrogen and phosphorus compounds from food and waste may lead to blooms of phytoplankton, whose subsequent degradation can drastically reduce oxygen levels. If the algae are toxic, fish are killed and shellfish contaminated.
Sustainability.
Mariculture development must be sustained by basic and applied research and development in major fields such as nutrition, genetics, system management, product handling, and socioeconomics. One approach is closed systems that have no direct interaction with the local environment. However, investment and operational cost are currently significantly higher than open cages, limiting them to their current role as hatcheries.
Benefits.
Sustainable mariculture promises economic and environmental benefits. Economies of scale imply that ranching can produce fish at lower cost than industrial fishing, leading to better human diets and the gradual elimination of unsustainable fisheries. Maricultured fish are also perceived to be of higher quality than fish raised in ponds or tanks, and offer more diverse choice of species. Consistent supply and quality control has enabled integration in food market channels.
Scientific literature.
Scientific literature on mariculture can be found in the following journals:
</dl>

</doc>
<doc id="19770" url="http://en.wikipedia.org/wiki?curid=19770" title="Memetics">
Memetics

Memetics is a theory of mental content based on an analogy with Darwinian evolution, originating from the popularization of Richard Dawkins' 1976 book "The Selfish Gene." Proponents describe memetics as an approach to evolutionary models of cultural information transfer.
The meme, analogous to a gene, was conceived as a "unit of culture" (an idea, belief, pattern of behaviour, etc.) which is "hosted" in the minds of one or more individuals, and which can reproduce itself, thereby jumping from mind to mind. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.
Memetics is also notable for sidestepping the traditional concern with the "truth" of ideas and beliefs. Instead, it is interested in their success.
The Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The "Journal of Memetics" was published electronically from 1997 to 2005.
History.
In his book "The Selfish Gene" (1976), the evolutionary biologist Richard Dawkins used the term "meme" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Ted Cloak had briefly outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back to Darwin's era.
Dawkins (1976) contended that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This created great debate among sociologists, biologists, and scientists of other disciplines, because Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, since the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of "memetics" in "The Selfish Gene", but rather coined the term "meme" in a speculative spirit. Accordingly, the term "unit of information" came to be defined in different ways by many scientists.
The modern memetics movement dates from the mid-1980s. A January 1983 Metamagical Themas column by Douglas Hofstadter, in "Scientific American", was influential as was his 1985 book of the same name. "Memeticist" was coined as analogous to "geneticist" originally in "The Selfish Gene." Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as memetics by analogy with 'genetics.'" Dawkins' "The Selfish Gene" has been a factor in drawing in people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of "Consciousness Explained" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay "Viruses of the Mind", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's Snow Crash).
The idea of "language as a virus" had already been introduced by William S. Burroughs as early as 1962 in his book "The Ticket That Exploded", and later in "The Electronic Revolution", published in 1970 in "" and is also explored in by Douglas Rushkoff in 1995.
However, the foundation of memetics in full modern incarnation originates in the publication in 1996 of two books by authors outside the academic mainstream: "Virus of the Mind: The New Science of the Meme" by former Microsoft executive turned motivational speaker and professional poker player, Richard Brodie, and "Thought Contagion: How Belief Spreads Through Society" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' "The Selfish Gene" until his book was very close to publication.
Around the same time as the publication of the books by Lynch and Brodie the e-journal appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper memetics publication starting in 1990, the "Journal of Ideas" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published "The Meme Machine", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.
The term "meme".
The term "meme" derives from the Ancient Greek μιμητής ("mimētḗs"), meaning "imitator, pretender". The similar term "mneme" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work "Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen", translated into English in 1921 as "The Mneme". Until Daniel Schacter published "Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word "meme" independently of Semon, writing this:
“Mimeme” comes from a suitable Greek root, but I want a monosyllable that sounds a bit like “gene”. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to “memory”, or to the French word même.
Maturity.
In 2005, the "Journal of Memetics – Evolutionary Models of Information Transmission" ceased publication and published a set of articles on the future of memetics. The website states that although "there was to be a relaunch...after several years nothing has happened". Susan Blackmore has left the University of the West of England to become a freelance science writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words "meme" and "memetics" (without disowning the ideas in his book), adopting the self-description "thought contagionist". Lynch lost his previous funding from a private sponsor and after his book royalties declined, he was unable to support himself as a private memetics/thought-contagion consultant. He died in 2005.
Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins. That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In her definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every interaction of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.
Internalists and externalists.
The memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as "a unit of cultural transmission". Gibran Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as "a unit of cultural information that can be copied, located in the brain". This thinking is more in line with Dawkins' second definition of the meme in his book "The Extended Phenotype". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.
These two schools became known as the "internalists" and the "externalists." Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes (i-memes) in response to external e-memes.
An advanced statement of the internalist school came in 2002 with the publication of "The Electric Meme", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of "Darwinizing Culture: The Status of Memetics as a Science", edited by Aunger and with a foreword by Dennett, in 2000.
Criticism.
This evolutionary model of cultural information transfer is based on the concept that units of information, or "memes", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, it has since turned into a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.
Critics contend that some proponents' assertions are "untested, unsupported or incorrect." Luis Benitez-Bribiesca, a critic of memetics, calls it "a pseudoscientific dogma" and "a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution" among other things. As factual criticism, he refers to the lack of a "code script" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic.
Another criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.
Mary Midgley criticises memetics for at least two reasons: One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in "Darwin's Dangerous Idea", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of "meme" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple.
Like other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is "heuristically trivial", being a mere redescription of what is already known without offering any useful novelty.
New developments.
Dawkins responds in "A Devil's Chaplain" that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.
Another definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, "memeplexes", and the "deme", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see , he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word "meme". For example, in the sense of computer simulation the term "memetic algorithm" is used to define a particular computational viewpoint.
The possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.
In 2013 Australian academic JT Velikovsky proposed the holon as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.
Memetics can be simply understood as a method for scientific analysis of cultural evolution. However, proponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – "Evolutionary Models of Information Transmission" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. Keith Henson who wrote "Memetics and the Modular-Mind" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.
Recently, Christopher diCarlo has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In "Problem Solving and Neurotransmission in the Upper Paleolithic" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. But the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for memetic equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).
Jan E.M. Houben has argued on several occasions that the well-attested, exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia (at least from the mid-second millennium B.C.) can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.
This renders superfluous attempts to explain the phenomenon of the millennia old tradition of Vedic ritual in genetic terms.
The term ‘meme’ is here taken as a pointer to a rich analogy of biological-cultural parallelism (gene-meme) in a perspective of cultural evolution, which in itself does not suffice to turn ‘memetics’ into a ‘science’: 
with regard to Vedic ritual it is rather an extended metaphor useful to address both large-scale and micro-scale aspects of the phenomenon and their interrelation that remained hitherto out of view. The domain of Vedic ritual should nevertheless be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).
Applications.
Research methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.
The application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at . Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, , argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, , uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.
Another application of memetics in the sustainability space is the crowdfunded conducted by Joe Brewer and Balasz Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy.
Ben Cullen, in his book "Contagious Ideas", brought the idea of the meme into the discipline of archaeology. He coined the term "Cultural Virus Theory", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.
Francis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls "memetic selection criteria". These criteria opened the way to a specialized field of "applied memetics" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.
In "Selfish Sounds and Linguistic Evolution", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.
Australian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.
Swedish political scientist Mikael Sandberg argues against "Lamarckian" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active ("Lamarckian") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the "Lamarckian" IT strategy.

</doc>
<doc id="19773" url="http://en.wikipedia.org/wiki?curid=19773" title="March 25">
March 25

March 25 is the day of the year in the Gregorian calendar.

</doc>
<doc id="19780" url="http://en.wikipedia.org/wiki?curid=19780" title="List of islands of Michigan">
List of islands of Michigan

The following is a list of islands of Michigan. Michigan has the second longest coastline of any state after Alaska. Being bordered by four of the five Great Lakes—Erie, Huron, Michigan, and Superior—Michigan also has 64,980 inland lakes and ponds, as well as innumerable rivers, that may contain their own islands included in this list. The majority of the islands are within the Great Lakes. Other islands can also be found within other waterways of the Great Lake system, including Lake St. Clair, St. Clair River, Detroit River, and St. Marys River.
The largest of all the islands is Isle Royale in Lake Superior, which, in addition to its waters and other surrounding islands, is organized as Isle Royale National Park. Isle Royale itself is 206 sqmi. The most populated island is Grosse Ile with approximately 10,000 residents, located in the Detroit River about 10 miles (16 km) south of Detroit. The majority of Michigan's islands are uninhabited and very small. Some of these otherwise unusable islands have been used for the large number of Michigan's lighthouses to aid in shipping throughout the Great Lakes, while others have been set aside as nature reserves. Many islands in Michigan have the same name, even some that are in the same municipality and body of water, such as Gull, Long, or Round islands.
Lake Erie.
Only Monroe County has territory in the westernmost portion of Lake Erie, which has a surface elevation of 571 ft (174 m). The islands in the southern portion of the county are part of the North Maumee Bay Archeological District of the Detroit River International Wildlife Refuge, in which Turtle Island is the only island in the state of Michigan that is shared by another state. This remote and tiny island is cut in half and shared with Ohio. While not distinct named islands in their own right, Sterling State Park and Pointe Mouillee are situated on several islands.
Lake Huron.
Lake Huron is the second largest of the Great Lakes (after Lake Superior) with a surface area of 23,010 sqmi. Michigan is the only U.S. state to border Lake Huron, while the portion of the lake on the other side of the international border belongs to the Canadian province of Ontario. The vast majority of Michigan's islands in Lake Huron are centered around Drummond Island in the northernmost portion of the state's lake territory. Drummond Island is the largest of Michigan's islands in Lake Huron and is the second largest Michigan island after Lake Superior's Isle Royale. Another large group of islands is the Les Cheneaux Islands archipelago, which itself contains dozens of small islands. Many of the lake's islands are very small and uninhabited.
As the most popular tourist destination in the state, Mackinac Island is the most well known of Lake Huron's islands. Drummond Island is the most populous of Michigan's islands in Lake Huron, with a population of 992 at the 2000 census. While Mackinac Island had a population of only 553, there are thousands more seasonal workers and tourists during the summer months.
Lake Michigan.
Michigan only has islands in Lake Michigan in the northern portion of the lake. There are no islands in the southern half of Lake Michigan. The largest and most populated of Michigan's islands in Lake Michigan is Beaver Island at 55.8 sqmi and 551 residents. Some of the smaller islands surrounding Beaver Island are part of the larger Michigan Islands National Wildlife Refuge.
Lake Superior.
 
Lake Superior is the largest of the Great Lakes, and the coastline is sparsely populated. At 206 sqmi, Isle Royale is the largest Michigan island and is the center of Isle Royale National Park, which itself contains over 450 islands. The following is a list of islands in Lake Superior that are "not" part of Isle Royale National Park. For those islands, see the list of islands in Isle Royale National Park.
Lake St. Clair.
Lake St. Clair connects Lake Huron and Lake Erie through the St. Clair River in the north and the Detroit River in the south. At 430 sqmi, it is one of the largest non-Great Lakes in the United States, but it only contains a small number of islands near the mouth of the St. Clair River, where all of the following islands are located. The largest of these islands is Harsens Island, and all the islands are in Clay Township in St. Clair County.
Detroit River.
The Detroit River runs for 32 miles (51 km) and connects Lake St. Clair to Lake Erie. For its entire length, it carries the international border between the United States and Canada. Some islands belong to Ontario in Canada and are not included in the list below. All islands on the American side belong to Wayne County. Portions of the southern portion of the river serve as wildlife refuges as part of the Detroit River International Wildlife Refuge. The largest and most populous island is Grosse Ile at 9.0 sqmi and a population of around 10,000. Most of the islands are around and closely connected to Grosse Ile.
St. Marys River.
The St. Marys River connects Lake Superior and Lake Huron at the easternmost point of the Upper Peninsula. It carries the international border throughout its length, and some of the islands belong to neighboring Ontario. The largest of Michigan's islands in the river are Sugar Island and Neebish Island. Wider portions of the river are designated as Lake George, Lake Nicolet, and the Munuscong Lake. The whole length of the Michigan portion of the river is part of Chippewa County.
Inland islands.
Michigan has numerous inland lakes and rivers that also contain their own islands. The following also lists the body of water in which these islands are located. Five islands below (highlighted in green) are actually islands within an island; they are contained within inland lakes in Isle Royale.
Grand Lake.
Grand Lake is a large 8.8 sqmi lake in Presque Isle County. While it is not the largest inland lake in Michigan, it does contain the most inland islands. At its shortest distance, it is located less than one mile (0.6 km) from Lake Huron, but the two are not connected. Grand Lake contains 14 islands, of which Grand Island is by far the largest.

</doc>
<doc id="19808" url="http://en.wikipedia.org/wiki?curid=19808" title="List of Governors of Michigan">
List of Governors of Michigan

The governor of Michigan is the head of the executive branch of Michigan's state government and serves as the commander-in-chief of the state's military forces. The governor has a duty to enforce state laws; the power to either approve or veto appropriation bills passed by the Michigan Legislature; the power to convene the legislature; and the power to grant pardons, except in cases of impeachment. He or she is also empowered to reorganize the executive branch of the state government.
Michigan was originally part of French and British holdings, and administered by their colonial governors. After becoming part of the United States, various areas in what is now Michigan were part of the Northwest Territory, Indiana Territory and Illinois Territory, and administered by territorial governors. In 1805, the Michigan Territory was created, and five men served as territorial governors, until Michigan was granted statehood in 1837. Forty-eight individuals have held the position of state governor. The first female governor, Jennifer Granholm, was elected in 2003.
After Michigan gained statehood, governors held the office for a two year term, until the 1963 Michigan Constitution changed the term to four years. The number of times an individual could hold the office was unlimited until a 1992 constitutional amendment imposed a lifetime term limit of two four-year governorships. The longest serving governor in Michigan's history was William Milliken, who was promoted from lieutenant governor after Governor George W. Romney resigned, then was elected to three further successive terms.
Governors.
Michigan was part of colonial New France until the Treaty of 1763 transferred ownership to the Kingdom of Great Britain. During this time, it was governed by the Lieutenants General of New France until 1627, the Governors of New France from 1627 to 1663, and the Governors General of New France until the transfer to Great Britain. The 1783 Treaty of Paris ceded the territory that is now Michigan to the United States as part of the end of the Revolutionary War, but British troops were not removed from the area until 1796. During the British ownership, their governors administrated the area as part of the Canadian territorial holdings.
Prior to becoming its own territory, parts of Michigan were administered by the governors of the Northwest Territory, the governors of the Indiana Territory and the governors of the Illinois Territory. On June 30, 1805, the Territory of Michigan was created, with General William Hull as the first territorial governor.
Governors of the State of Michigan.
Michigan was admitted to the Union on January 26, 1837. The original 1835 Constitution of Michigan provided for the election of a governor and a lieutenant governor every 2 years. The fourth and current constitution of 1963 increased this term to four years. There was no term limit on governors until a constitutional amendment effective in 1993 limited governors to two terms.
Should the office of governor become vacant, the lieutenant governor becomes governor, followed in order of succession by the Secretary of State and the Attorney General. Prior to the current constitution, the duties of the office would devolve upon the lieutenant governor, without that person actually becoming governor. The term begins at noon on January 1 of the year following the election. Prior to the 1963 constitution, the governor and lieutenant governor were elected through separate votes, allowing them to be from different parties. In 1963, this was changed, so that votes are cast jointly for a governor and lieutenant governor of the same party.
Other high offices held.
Several governors also held other high positions within the state and federal governments. Eight governors served as U.S. House of Representatives members, while seven held positions in the U.S. Senate, all representing Michigan. Others have served as ambassadors, U.S. Cabinet members and state and federal Supreme Court justices.
Living former governors.
s of 2015[ [update]], four former governors are alive. The most recent governor to die, and also the most recently serving governor to have died, was George W. Romney, who left office on January 22, 1969 and died on July 26, 1995 eighteen days after his eighty-eighth birthday.

</doc>
<doc id="19809" url="http://en.wikipedia.org/wiki?curid=19809" title="Moses Amyraut">
Moses Amyraut

Moïse Amyraut, Latin Moyses Amyraldus (Bourgueil, September 1596 – January 8, 1664), in English texts often Moses Amyraut, was a French Protestant theologian and metaphysician. He is perhaps most noted for his modifications to Calvinist theology regarding the nature of Christ's atonement, which is referred to as Amyraldism or Amyraldianism.
Life.
Born at Bourgueil, in the valley of the Changeon in the province of Anjou, his father was a lawyer, and, preparing Moses for his own profession, sent him, on the completion of his study of the humanities at Orléans to the university of Poitiers.
At the university he took the degree of licentiate (BA) of laws. On his way home from the university he passed through Saumur, and, having visited the pastor of the Protestant church there, was introduced by him to Philippe de Mornay, governor of the city. Struck with young Amyraut's ability and culture, they both urged him to change from law to theology. His father advised him to revise his philological and philosophical studies, and read over Calvin's "Institutions," before finally determining a course. He did so, and decided for theology.
He moved to the Academy of Saumur and studied under John Cameron, who ultimately regarded him as his greatest scholar. He had a brilliant course, and was in due time licensed as a minister of the French Protestant Church. The contemporary civil wars and excitements hindered his advancement. His first church was in Saint-Aignan, in the province of Maine. There he remained two years. Jean Daillé, who moved to Paris, advised the church at Saumur to secure Amyraut as his successor, praising him "as above himself." The university of Saumur at the same time had fixed its eyes on him as professor of theology. The great churches of Paris and Rouen also contended for him, and to win him sent their deputies to the provincial synod of Anjou.
Amyraut had left the choice to the synod. He was appointed to Saumur in 1633, and to the professor's chair along with the pastorate. On the occasion of his inauguration he maintained for thesis "De Sacerdotio Christi". His co-professors were Louis Cappel and Josué de la Place, who also were Cameron's pupils and lifelong friends, who collaborated in the "Theses Salmurienses", a collection of theses propounded by candidates in theology prefaced by the inaugural addresses of the three professors. Amyraut soon gave to French Protestantism a new direction.
In 1631 he published his "Traité des religions"; and from this year onward he was a foremost man in the church. Chosen to represent the provincial synod of Anjou, Touraine and Maine at the national synod held in 1631 at Charenton, he was appointed as orator to present to the king "The Copy of their Complaints and Grievances for the Infractions and Violations of the Edict of Nantes".
Previous deputies had addressed the king on their bent knees, whereas the representatives of the Catholics had been permitted to stand. Amyraut consented to be orator only if the assembly authorized him to stand. There was intense resistance. Cardinal Richelieu himself, preceded by lesser dignitaries, condescended to visit Amyraut privately, to persuade him to kneel; but Amyraut held resolutely to his point and carried it. His "oration" on this occasion, which was immediately published in the French "Mercure", remains a striking landmark in the history of French Protestantism. During his absence on this matter the assembly debated "whether the Lutherans who desired it, might be admitted into communion with the Reformed Churches of France at the Lord's Table." It was decided in the affirmative previous to his return; but he approved with astonishing eloquence, and thereafter was ever in the front rank in maintaining intercommunion between all churches holding the main doctrines of the Reformation.
Pierre Bayle recounts the title-pages of no fewer than thirty-two books of which Amyraut was the author. These show that he took part in all the great controversies on predestination and Arminianism which then so agitated and harassed all Europe. Substantially he held fast the Calvinism of his preceptor Cameron; but, like Richard Baxter in England, by his breadth and charity he exposed himself to all manner of misconstruction. In 1634 he published his "Traité de la predestination", in which he tried to mitigate the harsh features of predestination by his "Universalismus hypotheticus". God, he taught, predestines all men to happiness on condition of their having faith. This gave rise to a charge of heresy, of which he was acquitted at the national synod held at Alençon in 1637, and presided over by Benjamin Basnage (1580–1652). The charge was brought up again at the national synod of Charenton in 1644, when he was again acquitted. A third attack at the synod of Loudun in 1659 met with no better success. The university of Saumur became the university of French Protestantism.
Amyraut had as many as a hundred students in attendance upon his lectures. One of these was William Penn, who would later go on to found the Pennsylvania Colony in America based in part on Amyraut's notions of religious freedom . Another historic part filled by Amyraut was in the negotiations originated by Pierre le Gouz de la Berchère (1600–1653), first president of the "parlement" of Grenoble, when exiled to Saumur, for a reconciliation and reunion of the Catholics of France with the French Protestants. Very large were the concessions made by Richelieu in his personal interviews with Amyraut; but, as with the Worcester House negotiations in England between the Church of England and nonconformists, they inevitably fell through. On all sides the statesmanship and eloquence of Amyraut were conceded. His "De l'elevation de la foy et de l'abaissement de la raison en la creance des mysteres de la religion" (1641) gave him early a high place as a metaphysician. Exclusive of his controversial writings, he left behind him a very voluminous series of practical evangelical books, which have long remained the "fireside" favourites of the peasantry of French Protestantism. Amongst these are "Estat des fideles apres la mort"; "Sur l'oraison dominicale"; "Du merite des oeuvres"; "Traité de la justification"; and paraphrases of books of the Old and New Testament. His closing years were weakened by a severe fall he met with in 1657. He died on 18 January 1664.
Seventeenth century opponents.
There were a number of theologians who defended Calvinistic orthodoxy against Amyraut and Saumur, including Friedrich Spanheim (1600–1649) and Francis Turretin (1623–1687). Ultimately, the Helvetic Consensus was drafted to counteract the theology of Saumur and Amyraldism.

</doc>
<doc id="19811" url="http://en.wikipedia.org/wiki?curid=19811" title="Murray River">
Murray River

The Murray River or River Murray is Australia's longest river at 2508 km in length, the Murray rises in the Australian Alps, draining the western side of Australia's highest mountains and, for most of its length, meanders across Australia's inland plains, forming the border between the states of New South Wales and Victoria as it flows to the northwest, before turning south for its final 500 km into South Australia, reaching the ocean at Lake Alexandrina.
The water of the Murray flows through several lakes that fluctuate in salinity (and were often fresh until recent decades) including Lake Alexandrina and The Coorong before emptying through the Murray Mouth into the southeastern portion of the Indian Ocean, often referenced on Australian maps as the Southern Ocean, near Goolwa. Despite discharging considerable volumes of water at times, particularly before the advent of largescale river regulation, the mouth has always been comparatively small and shallow.
As of 2010, the Murray River system receives 58 percent of its natural flow. It is perhaps Australia's most important irrigated region, and it is widely known as the food bowl of the nation.
Geography.
The Murray River forms part of the 3750 km long combined Murray-Darling river system which drains most of inland Victoria, New South Wales, and southern Queensland. Overall the catchment area is one seventh of Australia's total land mass. The Murray carries only a small fraction of the water of comparably-sized rivers in other parts of the world, and with a great annual variability of its flow. In its natural state it has even been known to dry up completely during extreme droughts, although that is extremely rare, with only two or three instances of this occurring since official record keeping began.
The Murray River makes up much of the border between the Australian states of Victoria and New South Wales. Where it does, the border is the top of the bank of the southern side of the river (i.e., none of the river itself is actually in Victoria). This boundary definition can be ambiguous, since the river changes course, and some of the river banks have been modified.
West of the line of longitude 141°E, the river continues as the border between Victoria and South Australia for 3.6 km, where this is the only stretch where a state border runs down the middle of the river. This was due to a miscalculation during the 1840s, when the border was originally surveyed. Past this point, the Murray River is entirely within the state of South Australia.
Major settlements along.
The following major settlements are located along the course of the river, with population figures from the 2011 Census:
River life.
The Murray River (and associated tributaries) support a variety of unique river life adapted to its vagaries. This includes a variety of native fish such as the famous Murray cod, trout cod, golden perch, Macquarie perch, silver perch, eel-tailed catfish, Australian smelt, and western carp gudgeon, and other aquatic species like the Murray short-necked turtle, Murray River crayfish, broad-clawed yabbies, and the large clawed "Macrobrachium" shrimp, as well as aquatic species more widely distributed through southeastern Australia such as common longnecked turtles, common yabbies, the small claw-less "paratya" shrimp, water rats, and platypus. The Murray River also supports fringing corridors and forests of the river red gum.
The health of the Murray River has declined significantly since European settlement, particularly due to river regulation, and much of its aquatic life including native fish are now declining, rare or endangered. Recent extreme droughts (2000–07) have put significant stress on river red gum forests, with mounting concern over their long-term survival. The Murray has also flooded on occasion, the most significant of which was the flood of 1956, which inundated many towns on the lower Murray and which lasted for up to six months.
Introduced fish species such as carp, "gambusia", weather loach, redfin perch, brown trout, and rainbow trout have also had serious negative effects on native fish, while carp have contributed to environmental degradation of the Murray River and tributaries by destroying aquatic plants and permanently raising turbidity. In some segments of the Murray River, carp have become the only species found.
Ancient history.
Lake Bungunnia.
Between 2.5 and 0.5 million years ago the Murray River terminated in a vast freshwater lake called Lake Bungunnia. Lake Bungunnia was formed by earth movements that blocked the Murray River near Swan Reach during this period of time. At its maximum extent Lake Bungunnia covered 33000 km2, extending to near the Menindee Lakes in the north and to near Boundary Bend on the Murray in the south. The draining of Lake Bungunnia occurred approximately 0.6 million years ago.
Deep clays deposited by the lake are evident in cliffs around Chowilla in South Australia. Considerably higher rainfall would have been required to keep such a lake full; the draining of Lake Bungunnia appears to mark the end of a wet phase in the history of the Murray-Darling Basin and the onset of widespread arid conditions similar to today. A species of "Neoceratodus" lungfish existed in Lake Bungunnia (McKay & Eastburn, 1990); today "Neoceratodus" lungfish are only found in several Queensland rivers.
Cadell Fault and formation of the Barmah Red Gum Forests.
The noted Barmah Red Gum Forests owe their existence to the Cadell Fault. About 25,000 years ago, displacement occurred along the Cadell fault, raising the eastern edge of the fault, which runs north-south, 8 to above the floodplain. This created a complex series of events. A section of the original Murray River channel immediately behind the fault was abandoned, and it exists today as an empty channel known as Green Gully. The Goulburn River was dammed by the southern end of the fault to create a natural lake.
The Murray River flowed to the north around the Cadell Fault, creating the channel of the Edward River which exists today and through which much of the Murray River's waters still flow. Then the natural dam on the Goulburn River failed, the lake drained, and the Murray River avulsed to the south and started to flow through the smaller Goulburn River channel, creating "The Barmah Choke" and "The Narrows" (where the river channel is unusually narrow), before entering into the proper Murray River channel again.
This complex series of events, however, diverts attention from the primary result of the Cadell Fault — that the west-flowing water of the Murray River strikes the north-south fault and diverts both north and south around the fault in the two main channels (Edward and ancestral Goulburn) as well as a fan of small streams, and regularly floods a large amount of low-lying country in the area. These conditions are perfect for River Red Gums, which rapidly formed forests in the area. Thus the displacement of the Cadell Fault 25,000 BP led directly to the formation of the famous Barmah River Red Gum Forests.
The Barmah Choke and The Narrows mean the amount of water that can travel down this part of the Murray River is restricted. In times of flood and high irrigation flows the majority of the water, in addition to flooding the Red Gum forests, actually travels through the Edward River channel. The Murray River has not had enough flow power to naturally enlarge The Barmah Choke and The Narrows to increase the amount of water they can carry.
The Cadell Fault is quite noticeable as a continuous, low, earthen embankment as one drives into Barmah from the west, although to the untrained eye it may appear man-made.
Burra Creek.
The Burra Creek (also known as the Worlds End Creek), which flows through Burra township, was once part of the ancient Murray River system. The system used to cross the Northern Mount Lofty Ranges through the Burra creek gorge, and then flow out to Spencer Gulf near Port Pirie. The Murray River was diverted south at Morgan during Eocene times, about 50 million years ago by fault movements and uplifting of the fold belt.
Murray mouth.
The Murray Mouth is the point at which the Murray River empties into the sea, and the interaction between its shallow, shifting and variable currents and the open sea can be complex and unpredictable. During the peak period of Murray River commerce (roughly 1855 to 1920), it presented a major impediment to the passage of goods and produce between Adelaide and the Murray settlements, and many vessels foundered or were wrecked there.
Since the early 2000s, dredging machines have operated at the Murray Mouth, moving sand from the channel to maintain a minimal flow from the sea and into the Coorong's lagoon system. Without the 24-hour dredging, the Mouth would silt up and close, cutting the supply of fresh sea-water into the Coorong, which would then warm up, stagnate and die.
Mythology.
Being one of the major river systems in one of the driest continents of Earth, the Murray has significant cultural relevance to Indigenous Australians. According to the peoples of Lake Alexandrina, the Murray was created by the tracks of the Great Ancestor, Ngurunderi, as he pursued Pondi, the Murray Cod. The chase originated in the interior of New South Wales. Ngurunderi pursued the fish (who, like many totem animals in Aboriginal myths, is often portrayed as a man) on rafts (or "lala") made from red gums and continually launched spears at his target. But Pondi was a wily prey and carved a weaving path, carving out the river's various tributaries. Ngurundi was forced to beach his rafts, and often create new ones as he changed from reach to reach of the river.
At Kobathatang, Ngurunderi finally got lucky, and struck Pondi in the tail with a spear. However, the shock to the fish was so great it launched him forward in a straight line to a place called Peindjalang, near Tailem Bend. Eager to rectify his failure to catch his prey, the hunter and his two wives (sometimes the escaped sibling wives of Waku and Kanu) hurried on, and took positions high on the cliff on which Tailem Bend now stands. They sprung an ambush on Pondi only to fail again. Ngurunderi set off in pursuit again, but lost his prey as Pondi dived into Lake Alexandrina. Ngurunderi and his women settled on the shore, only to suffer bad luck with fishing, being plagued by a water fiend known as Muldjewangk. They later moved to a more suitable spot at the site of present-day Ashville. The twin summits of Mount Misery are supposed to be the remnants of his rafts, they are known as "Lalangengall" or "the two watercraft".
Remarkably, this story of a hunter pursuing a Murray cod that carved out the Murray persists in numerous forms in various language groups that inhabit the enormous area spanned by the Murray system. The Wotojobaluk people of Victoria tell of Totyerguil from the area now known as Swan Hill who ran out of spears while chasing Otchtout the cod.
European exploration.
The first Europeans to encounter the river were Hamilton Hume and William Hovell, who crossed the river where Albury now stands in 1824: Hume named it the "Hume River" after his father. In 1830 Captain Charles Sturt reached the river after travelling down its tributary the Murrumbidgee River and named it the "Murray River" in honour of the then British Secretary of State for War and the Colonies, Sir George Murray, not realising it was the same river that Hume and Hovell had encountered further upstream.
Sturt continued down the remaining length of the Murray to finally reach Lake Alexandrina and the river's mouth. The area of the Murray Mouth was explored more thoroughly by Captain Collet Barker in 1831.
In 1852 Francis Cadell, in preparation for the launch of his steamer service, explored the river in a canvas boat, travelling 1300 miles downstream from Swan Hill.
In 1858 the Government Zoologist, William Blandowski, along with Gerard Krefft, explored the lower reaches of the Murray and Darling rivers, compiling a list of birds and mammals.
River transport.
The lack of an estuary means that shipping cannot enter the Murray from the sea. However, in the 19th century the river supported a substantial commercial trade using shallow-draft paddle steamers, the first trips being made by two boats from South Australia on the spring flood of 1853. The "Lady Augusta", captained by Francis Cadell, reached Swan Hill while another, "Mary Ann", captained by William Randell, made it as far as Moama (near Echuca). In 1855 a steamer carrying gold-mining supplies reached Albury but Echuca was the usual turn-around point though small boats continued to link with up-river ports such as Tocumwal, Wahgunya and Albury.
The arrival of steamboat transport was welcomed by pastoralists who had been suffering from a shortage of transport due to the demands of the gold fields. By 1860 a dozen steamers were operating in the high water season along the Murray and its tributaries. Once the railway reached Echuca in 1864, the bulk of the woolclip from the Riverina was transported via river to Echuca and then south to Melbourne.
The Murray was plagued by "snags", fallen trees submerged in the water, and considerable efforts were made to clear the river of these threats to shipping by using barges equipped with steam-driven winches. In recent times, efforts have been made to restore many of these "snags" by placing dead gum trees back into the river. The primary purpose of this is to provide habitat for fish species whose breeding grounds and shelter were eradicated by the removal of "snags".
The volume and value of river trade made Echuca Victoria's second port and in the decade from 1874 it underwent considerable expansion. By this time up to thirty steamers and a similar number of barges were working the river in season. River transport began to decline once the railways touched the Murray at numerous points. The unreliable levels made it impossible for boats to compete with the rail and later road transport. However, the river still carries pleasure boats along its entire length.
Today, most traffic on the river is recreational. Small private boats are used for water skiing and fishing. Houseboats are common, both commercial for hire and privately owned. There are a number of both historic paddle steamers and newer boats offering cruises ranging from half an hour to 5 days. In 2009, British Adventurer David Cornthwaite walked and kayaked 2476 km along the Murray River from source to sea.
River crossings.
The Murray River has been a significant barrier to land-based travel and trade. Many of the Ports for transport of goods along the Murray have also developed as places to cross the river, either by bridge or ferry. The first bridge to cross the Murray, which was built in 1869, is in the town of Murray Bridge, formerly called Edwards Crossing.
Water storage and irrigation.
Small-scale pumping plants began drawing water from the Murray in the 1850s and the first high-volume plant was constructed at Mildura in 1887. The introduction of pumping stations along the river promoted an expansion of farming and led ultimately to the development of irrigation areas (including the Murrumbidgee Irrigation Area).
In 1915, the three Murray states – New South Wales, Victoria, and South Australia – signed the River Murray Agreement which proposed the construction of storage reservoirs in the river's headwaters as well as at Lake Victoria near the South Australian border. Along the intervening stretch of the river a series of locks and weirs were built. These were originally proposed to support navigation even in times of low water, but riverborne transport was already declining due to improved highway and railway systems.
Four large reservoirs were built along the Murray. In addition to Lake Victoria (completed late 1920s). These are Lake Hume near Albury–Wodonga (completed 1936), Lake Mulwala at Yarrawonga (completed 1939), and Lake Dartmouth, which is actually on the Mitta Mitta River upstream of Lake Hume (completed 1979). The Murray also receives water from the complex dam and pipeline system of the Snowy Mountains Scheme.
From 1935 to 1940 a series of barrages was built near the Murray Mouth to stop seawater egress into the lower part of the river during low flow periods. They are the Goolwa Barrage, located at 632 m, Mundoo Channel Barrage at 800 m, Boundary Creek Barrage at 243 m, Ewe Island Barrage at 853 m, and Tauwitchere Barrage at 3.6 km.
These dams inverted the patterns of the river's natural flow from the original winter-spring flood and summer-autumn dry to the present low level through winter and higher during summer. These changes ensured the availability of water for irrigation and made the Murray Valley Australia's most productive agricultural region, but have seriously disrupted the life cycles of many ecosystems both inside and outside the river, and the irrigation has led to dryland salinity that now threatens the agricultural industries.
The disruption of the river's natural flow, runoff from agriculture, and the introduction of pest species like the European carp has led to serious environmental damage along the river's length and to concerns that the river will be unusably salty in the medium to long term – a serious problem given that the Murray supplies 40 percent of the water supply for Adelaide. Efforts to alleviate the problems proceed but disagreement between various groups stalls progress.
In 2006, the state government of South Australia revealed its plan to investigate the construction of the controversial Wellington Weir.
Locks.
Lock 1 was completed near Blanchetown in 1922. Torrumbarry weir downstream of Echuca began operating in December 1923. Of the numerous locks that were proposed, only thirteen were completed; Locks 1 to 11 on the stretch downstream of Mildura, Lock 15 at Euston and Lock 26 at Torrumbarry. Construction of the remaining weirs purely for navigation purposes was abandoned in 1934. The last lock to be completed was Lock 15, in 1937.
Lock 11, just downstream of Mildura, creates a 100 km long lock pool which aided irrigation pumping from Mildura and Red Cliffs.
Each lock has a navigable passage next to it through the weir, which is opened during periods of high river flow, when there is too much water for the lock. The weirs can be completely removed, and the locks completely covered by water during flood conditions. Lock 11 is unique in that the lock was built inside a bend of the river, with the weir in the bend itself. A channel was dug to the lock, creating an island between it and the weir. The weir is also of a different design, being dragged out of the river during high flow, rather than lifted out.

</doc>
<doc id="19812" url="http://en.wikipedia.org/wiki?curid=19812" title="Project Mercury">
Project Mercury

Project Mercury was the first human spaceflight program of the United States, running from 1959 through 1963. An early highlight of the Space Race, its goal was to put a solo human into Earth orbit and return the person safely, ideally before the Soviet Union. Taken over from the US Air Force by the newly created civilian space agency NASA, it spanned twenty unmanned developmental missions involving test animals, and successful missions completed by six of the seven Mercury astronauts.
The Space Race had begun with the 1957 launch of the Soviet satellite Sputnik 1. This came as a shock to the American public, and led to the creation of NASA to expedite existing U.S. space exploration efforts, and place most of them under civilian control. After the successful launch of the Explorer 1 satellite in 1958, manned spaceflight became the next goal.
The Soviet Union put the first human, cosmonaut Yuri Gagarin, into a single orbit aboard Vostok 1 in April 1961. Shortly after this, on May 5, the US launched its first astronaut, Alan Shepard, on a suborbital flight. Soviet Gherman Titov followed with a day-long orbital flight in August, 1961. The U.S. reached its orbital goal on February 20, 1962, when John Glenn made three orbits around the Earth. When Mercury ended in May 1963, both nations had sent six people into space, but the US was still behind the Soviets in terms of total time spent in space.
The cone-shaped Mercury capsule was produced by McDonnell Aircraft, and carried supplies of water, food and oxygen for about one day in a pressurized cabin. Mercury flights were launched from Cape Canaveral, Florida, on modified Redstone and Atlas D missiles. The capsule was fitted with an escape rocket to carry it safely away from the launch rocket in case of a failure of the latter. The flight was designed to be controlled from the ground via the Manned Space Flight Network, a system of tracking and communications stations; back-up controls were outfitted on board. Small retrorockets were used to bring the spacecraft out of its orbit, after which an ablative heat shield protected the spacecraft from the heat of atmospheric reentry. Finally, a parachute slowed the craft for a water landing. Both astronaut and capsule were recovered by helicopters deployed from the nearest suitable U.S. Navy ship.
The program took its name from the wing-footed, fleet god of travel in Roman mythology, and is estimated to have cost $ (current prices) and to have involved the work of 2 million people. The astronauts were collectively known as the "Mercury Seven", and each spacecraft was given a name ending with a "7" by its pilot.
After a slow start riddled with humiliating mistakes, the Mercury Project gained popularity, its missions followed by millions on radio and TV around the world. Its success laid the groundwork for Project Gemini, which carried two astronauts in each capsule and perfected space docking maneuvers essential for lunar travel, and the subsequent Apollo Moon-landing program announced a few weeks after the first manned Mercury flight.
Creation.
Project Mercury was officially approved on October 7, 1958 and publicly announced on December 17. Originally called Project Astronaut, President Dwight Eisenhower felt that gave too much attention to the pilot. Instead, the name "Mercury" was chosen from classical mythology, which had already lent names to rockets like the Greek "Atlas" and Roman "Jupiter" for the SM-65 and PGM-19 missiles. It absorbed military projects with the same aim, such as the Air Force Man In Space Soonest.
Background.
Following the end of World War II, a nuclear arms race evolved between the US and the Soviet Union to develop long-range missiles. At the same time both sides also developed satellites for gathering weather data, communications, and intelligence, mostly in secret. As a result, Americans were shocked when the Soviet Union placed the first satellite into orbit in October 1957, followed by a growing fear that the US was falling into a Missile gap. A month later, the Soviets launched Sputnik 2, carrying a dog into orbit. Though the animal was not recovered alive, it was obvious their goal was manned spaceflight. Unable to disclose details of military space projects, President Eisenhower ordered the creation of a civilian space agency in charge of civilian and scientific space exploration. Based on the federal research agency National Advisory Committee for Aeronautics (NACA), it was renamed the National Aeronautics and Space Administration. It achieved its first goal, an American satellite in space, in 1958. The next goal was to put a man there.
The limit of space was defined at the time as a minimum altitude of 62 mi, and the only way to reach it was by using rocket powered boosters. This created risks for the pilot, including explosion, high g-forces and vibrations during lift off through a dense atmosphere, and temperatures of more than 10,000 °F from air compression during reentry.
In space, pilots would require pressurized chambers or space suits to supply fresh air. While there, they would experience weightlessness, which could potentially cause disorientation. Further potential risks included radiation and micrometeoroid strikes, both of which would normally be absorbed in the atmosphere. All seemed possible to overcome: experience from satellites suggested micrometeoroid risk was negligible, and experiments in the early 1950s with simulated weightlessness, high g-forces on humans, and sending animals to the limit of space, all suggested potential problems could be overcome by known technologies. Finally, reentry was studied using the nuclear warheads of ballistic missiles, which demonstrated a blunt, forward-facing heat shield would create a shock wave that would force most of the heat generated to flow around the spacecraft while slowing it.
Organization and facilities.
Wallops
Island
Hampton
Johnsville
Cleveland
Greenbelt
C. Canaveral
Huntsville
St. Louis
Alamogordo
San Diego
Los Angeles
T. Keith Glennan had been appointed the first Administrator of NASA, with Hugh L. Dryden (last Director of NACA) as his Deputy, at the creation of the agency on October 1, 1958. Glennan would report to the president through the National Aeronautics and Space Council. The group responsible for Project Mercury was NASA's Space Task Group, and the goals of the program were to orbit a manned spacecraft around Earth, investigate the pilot's ability to function in space, and to recover both pilot and spacecraft safely. Existing technology and off-the-shelf equipment would be used wherever practical, the simplest and most reliable approach to system design would be followed, and an existing launch vehicle would be employed, together with a progressive test program. Spacecraft requirements included: a launch escape system to separate the spacecraft and its occupant from the launch vehicle in case of impending failure; attitude control for orientation of the spacecraft in orbit; a retrorocket system to bring the spacecraft out of orbit; drag braking blunt body for atmospheric reentry; and landing on water. To communicate with the spacecraft during an orbital mission, an extensive communications network had to be built. In keeping with his desire to keep from giving the US space program an overly military flavor, President Eisenhower at first hesitated to give the project top national priority (DX rating under the Defense Production Act), which meant that Mercury had to wait in line behind military projects for materials; however, this rating was granted in May 1959.
Twelve companies bid to build the Mercury spacecraft on a $20 million ($) contract. In January 1959, McDonnell Aircraft Corporation was chosen to be prime contractor for the spacecraft. Two weeks earlier, North American Aviation, based in Los Angeles, was awarded a contract for Little Joe, a small rocket to be used for development of the launch escape system. The World Wide Tracking Network for communication between the ground and spacecraft during a flight was awarded to the Western Electric Company. Redstone rockets for suborbital launches were manufactured in Huntsville, Alabama by the Chrysler Corporation and Atlas rockets by Convair in San Diego, California. For manned launches, the Atlantic Missile Range at Cape Canaveral Air Force Station in Florida was made available by the USAF. This was also the site of the Mercury Control Center while the computing center of the communication network was in Goddard Space Center, Maryland. Little Joe rockets were launched from Wallops Island, Virginia. Astronaut training took place at Langley Research Center in Virginia, Lewis Flight Propulsion Laboratory in Cleveland, Ohio, and Naval Air Development Center Johnsville in Warminster, PA. Langley wind tunnels together with a rocket sled track at Holloman Air Force Base at Alamogordo, New Mexico were used for aerodynamic studies. Both Navy and Air Force aircraft were made available for the development of the spacecraft's landing system, and Navy ships and Navy and Marine Corps helicopters were made available for recovery. South of Cape Canaveral the town of Cocoa Beach boomed. From here, 75,000 people watched the first American orbital flight being launched in 1962.
Spacecraft.
The Mercury spacecraft's principal designer was Maxime Faget, who started research for manned spaceflight during the time of the NACA. It was 10.8 ft long and 6.0 ft wide; with the launch escape system added overall length was 25.9 ft. With 100 ft3 of habitable volume, the capsule was just large enough for the single crew member. Inside were 120 controls: 55 electrical switches, 30 fuses and 35 mechanical levers. The heaviest spacecraft, Mercury-Atlas 9, weighed fully loaded 3000 lb. Its outer skin was made of René 41, a nickel alloy able to withstand high temperatures.
The spacecraft was-cone shaped, with a neck at the narrow end. It had a convex base, which carried a heat shield (Item 2 in the diagram below). consisting of an aluminum honeycomb covered with multiple layers of fiberglass. Strapped to it was a retropack (1) consisting of three rockets deployed to brake the spacecraft during reentry. Between these were three minor rockets for separating the spacecraft from the launch vehicle at orbital insertion. The straps that held the package could be severed when it was no longer needed. Next to the heat shield was the pressurized crew compartment (3). Inside an astronaut would be strapped to a form-fitting seat, with instruments in front and his back to the heat shield. Underneath the seat was the environmental control system supplying oxygen and heat., scrubbing the air of CO2, vapor and odors, and (on orbital flights) collecting urine. The recovery compartment (4) at the narrow end of the spacecraft contained three parachutes: a drogue to stabilize free fall and two main chutes, a primary and reserve. Between the heat shield and inner wall of the crew compartment was a landing skirt, deployed by letting down the heat shield before landing. On top of the recovery compartment was the antenna section (5) containing both antennas for communication and scanners for guiding spacecraft orientation. Attached was a flap used to ensure the spacecraft was faced heat shield first during reentry. A launch escape system (6) was mounted to the narrow end of the spacecraft containing three small solid-fueled rockets which could be fired briefly in a launch failure to separate the capsule safely from its booster. It would deploy the capsule's parachute for a landing nearby at sea.
Pilot accommodations.
The astronaut lay in a sitting position with his back to the heat shield, which was found to be the position which best enabled a human to withstand the high g-forces of launch and re-entry. A form-fitted fiberglass seat was custom-molded from each astronaut's space-suited body, for maximum support. Near his left hand was a manual abort handle to activate the launch escape system if necessary prior to or during liftoff, in case the automatic trigger failed.
To supplement the onboard environmental control system, he wore a pressure suit with its own oxygen supply, which would also cool him. A cabin atmosphere of pure oxygen at a low pressure of 5.5 psi (equivalent to an altitude of 24800 ft) was chosen, rather than one with the same composition as air (nitrogen/oxygen) at sea level. This was easier to control, avoided the risk of decompression sickness (known as "the bends"), and also saved on spacecraft weight. Fires (which never occurred) would have to be extinguished by emptying the cabin of oxygen. In such case, or failure of the cabin pressure for any reason, the astronaut could make an emergency return to Earth, relying on his suit for survival. The astronauts normally flew with their visor up, which meant that the suit was not inflated. With the visor down and the suit inflated, the astronaut could only reach the side and bottom panels, where vital buttons and handles were placed.
The astronaut also wore electrodes on his chest to record his heart rhythm, a cuff that could take his blood pressure, and a rectal thermometer to record his temperature (this was replaced by an oral thermometer on the last flight). Data from these was sent to the ground during the flight. The astronaut normally drank water and ate food pellets.
Once in orbit, the spacecraft could be rotated in three directions: along its longitudinal axis (roll), left to right from the astronaut's point of view (yaw), and up or down (pitch). Movement was created by rocket-propelled thrusters which used hydrogen peroxide as a fuel. For orientation, the pilot could look through the window in front of him or from a screen connected to a periscope which could be turned 360°.
The Mercury astronauts had taken part in the development of their spacecraft, and insisted that manual control, and a window, be elements of its design. As a result, spacecraft movement and other functions could be controlled three ways: remotely from the ground when passing over a ground station, automatically guided by onboard instruments, or manually by the astronaut, who could replace or override the two other methods. Experience validated the astronauts' insistence on manual controls. Without them, Gordon Cooper's manual reentry during the last flight would not have been possible.
Development and production.
The Mercury spacecraft design was modified three times by NASA between 1958 and 1959. After bidding by potential contractors had been completed, NASA selected the design submitted as "C" in November 1958. After it failed a test flight in July 1959, a final configuration, "D", emerged. The heat shield shape had been developed earlier in the 1950s through experiments with ballistic missiles, which had shown a blunt profile would create a shock wave that would lead most of the heat around the spacecraft. To further protect against heat, either a heat sink, or an ablative material, could be added to the shield. The heat sink would remove heat by the flow of the air inside the shock wave, whereas the ablative heat shield would remove heat by a controlled evaporation of the ablative material. After unmanned tests, the latter was chosen for manned flights. Apart from the capsule design, a rocket plane similar to the existing X-15 was considered. This approach was still too far from being able to make a spaceflight, and was consequently dropped. The heat shield and the stability of the spacecraft were tested in wind tunnels, and later in flight. The launch escape system was developed through unmanned flights. During a period of problems with development of the landing parachutes, alternative landing systems such as the Rogallo glider wing were considered, but ultimately scrapped.
The spacecraft were produced at McDonnell Aircraft, St. Louis, Missouri in clean rooms and tested in vacuum chambers at the McDonnell plant. The spacecraft had close to 600 subcontractors, such as Garrett AiResearch which built the spacecraft's environmental control system. Final quality control and preparations of the spacecraft were made at Hangar S at Cape Canaveral. NASA ordered 20 production spacecraft, numbered 1 through 20. Five of the 20, Nos. 10, 12, 15, 17, and 19, were not flown. Spacecraft No. 3 and No. 4 were destroyed during unmanned test flights. Spacecraft No. 11 sank and was recovered from the bottom of the Atlantic Ocean after 38 years. Some spacecraft were modified after initial production (refurbished after launch abort, modified for longer missions, etc.) A number of Mercury boilerplate spacecraft (made from non-flight materials or lacking production spacecraft systems) were also made by NASA and McDonnell. They were designed and used to test spacecraft recovery systems and the escape tower. McDonnell also built the spacecraft simulators used by the astronauts during training.
Launch vehicles.
The Mercury program used two launch vehicles for manned missions. The most important was the Atlas LV-3B or Atlas D, a two-stage liquid fueled rocket used for orbital flight. It was developed by Convair for the Air Force during the mid-1950s and was fueled with liquid oxygen (LOX) and kerosene. It was 67 ft by itself and with the spacecraft and the launch escape system (including the adapter that held both to the launch vehicle), its total height was 95 ft. The first stage was a booster skirt with two engines using liquid fuel from the sustainer stage. This together with the larger sustainer stage gave it sufficient power to launch a Mercury spacecraft into orbit. Both stages fired from lift-off until staging of the booster; the sustainer through an opening in the booster. After staging, the sustainer stage continued alone. The sustainer also steered the rocket by thrusters guided by gyroscopes. Smaller vernier rockets were added on its sides for precise control of maneuvers. The Atlas was made of paper-thin stainless steel and had to be kept under constant internal pressure by fuel or helium in order to prevent the rocket from collapsing. This meant that the airframe of the launch vehicle could be reduced to 2% of the weight of the fuel. The Atlas D rocket also required extra strengthening in order to handle the increased weight of the Mercury spacecraft beyond that of the nuclear warheads it was designed for. Its internal guidance system also had to be moved accordingly to its greater length. The Titan missile was also considered for later Mercury missions but was not ready in time. The Atlas was flown to Cape Canaveral and transported to the launch pad on a dolly. At the launch pad, the rocket and dolly were lifted to a vertical position by the service tower and the Atlas was then held by clamps to the launch pad.
The other manned launch vehicle was the Mercury-Redstone Launch Vehicle which was an 83 ft (spacecraft and escape system included) tall single-stage launch vehicle used for suborbital (ballistic) flights. It had a liquid-fueled engine that burned alcohol and liquid oxygen producing about 75,000 pounds of thrust, which was not enough for orbital missions. It was a descendant of the German V-2, and developed for the U.S. Army during the early 1950s. It was modified for Project Mercury by removing the warhead and adding a collar for supporting the spacecraft together with material for damping vibrations during launch. Its rocket motor was produced by North American Aviation and its direction could be altered during flight by its fins. They worked in two ways: by directing the air around them or by directing the thrust by their inner parts (or both at the same time). Both the Atlas-D and Redstone launch vehicles contained an automatic abort sensing system which allowed them to abort a launch by firing the launch escape system if something went wrong. The Jupiter rocket, a relative of the Redstone, was originally considered for the suborbital launch vehicle, but was replaced by the Redstone in July 1959 due to budget constraints.
A smaller launch vehicle (55 ft long) called Little Joe which carried a Mercury spacecraft with an escape tower mounted on it, was used for unmanned tests of the launch escape system. Its main purpose was to test the system at a point called max-q, at which air pressure against the spacecraft peaked, making separation of the launch vehicle and spacecraft most difficult. It was also the point at which the astronaut was subjected to the heaviest vibrations. The Little Joe rocket used solid-fuel propellant and was originally designed in 1958 by the NACA for suborbital manned flights, but was redesigned for Project Mercury to simulate an Atlas-D launch. It was produced by North American Aviation. It was not able to change direction, instead its flight depended on the angle from which it was launched. Its maximum altitude was 100 mi fully loaded. A Scout launch vehicle was used for a single flight intended to evaluate the tracking network; however, it failed and was destroyed from the ground shortly after launch.
Astronauts.
NASA announced the selected seven astronauts – known as the Mercury Seven – on April 9, 1959, they were:
Shepard became the first American in space by making a suborbital flight in May 1961. He went on to fly in the Apollo program and became the only Mercury astronaut to walk on the Moon. Gus Grissom, who became the second American in space, also participated in the Gemini and Apollo programs, but died in January 1967 during a pre-launch test for Apollo 1. Glenn became the first American to orbit the Earth in February 1962, then quit NASA and went into politics, but returned to space aboard STS-95. Deke Slayton was grounded in 1962, but remained with NASA and flew on the Apollo-Soyuz Test Project in 1975. Gordon Cooper became the last to fly in Mercury and made its longest flight, and also flew a Gemini mission. Carpenter's Mercury flight was his only trip into space. Schirra flew the third orbital Mercury mission, and then flew a Gemini mission. Three years later, he commanded the first manned Apollo mission, becoming the only person to fly in all three of those programs.
One of the astronauts' tasks was publicity; they gave interviews to the press and visited project manufacturing facilities to speak with those who worked on Project Mercury. To make their travels easier, they requested and got jet fighters for personal use. The press was especially fond of John Glenn, who was considered the best speaker of the seven. They sold their personal stories to Life magazine which portrayed them as patriotic, God-fearing family men. Life was also allowed to be at home with the families while the astronauts were in space. During the project, Grissom, Carpenter, Cooper, Schirra and Slayton stayed with their families at or near Langley Air Force Base; Glenn lived at the base and visited his family in Washington DC on weekends. Shepard lived with his family at Naval Air Station Oceana in Virginia.
Selection and training.
It was envisaged that the pilot could be any man or woman willing to take a personal risk. On President Eisenhower's insistence, however, the first Americans to venture into space were drawn from a group of 508 military test pilots. This excluded women, since there were no female military test pilots. It also excluded the NACA's X-15 pilot and later astronaut Neil Armstrong, since he was a civilian. It was further stipulated that candidates should be between 25 and 40 years old, no taller than 5 ft, and hold a college degree in science or engineering. The college degree requirement excluded the X-1 pilot Chuck Yeager, the first person to exceed the speed of sound. He later became a critic of the project, ridiculing especially the use of monkeys. Joseph Kittinger, a stratosphere balloonist, met all the requirements but preferred to stay in his contemporary project. Some other potential candidates declined because they did not believe that manned spaceflight had a future beyond Project Mercury. From the original 508, 110 candidates were selected for an interview, and from the interviews, 32 were selected for further physical and mental testing. Their health, vision, and hearing were examined, together with their tolerance to noise, vibrations, g-forces, personal isolation, and heat. In a special chamber, they were tested to see if they could perform their tasks under confusing conditions. The candidates had to answer more than 500 questions about themselves and describe what they saw in different images. Jim Lovell, later astronaut in Gemini and Apollo, did not pass the physical tests. After these tests it was intended to narrow the group down to six astronauts but in the end it was decided to keep seven.
The astronauts went through a training program covering some of the same exercises that were used in their selection. They simulated the g-force profiles of launch and reentry in a centrifuge at the Naval Air Development Center, and were taught special breathing techniques necessary when subjected to more than 6 g. Weightlessness training took place in aircraft, first on the rear seat of a two-seater fighter and later inside converted and padded cargo aircraft. They practiced gaining control of a spinning spacecraft in a machine at the Lewis Flight Propulsion Laboratory called the Multi-Axis Spin-Test Inertia Facility (MASTIF), by using an attitude controller handle simulating the one in the spacecraft. A further measure for finding the right attitude in orbit was star and Earth recognition training in planetaria and simulators. Communication and flight procedures were practiced in flight simulators, first together with a single person assisting them and later with the Mission Control Center. Recovery was practiced in pools at Langley, and later at sea with frogmen and helicopter crews.
Mission profile.
Project Mercury had two kinds of missions: suborbital, and orbital. In the suborbital mission, a Redstone rocket lifted the spacecraft for 2 minutes and 30 seconds to an altitude of 32 nmi, separated from it and let it continue on a ballistic curve. Though not necessary for re-entry because orbital speed had not been attained, the spacecraft's retrorockets were fired, and the spacecraft landed in the Atlantic Ocean. Orbital missions also landed in the Atlantic, and a flotation collar, not ready for suborbital missions, was fastened around the spacecraft after landing. The suborbital mission took 15 minutes, had an apogee altitude of 102 -, and a downrange distance of 262 nmi.
Preparations for a mission started a month in advance with the selection of the primary and back-up astronaut; they would practice together for the mission. For three days prior to launch, the astronaut went through a special diet to minimize his need for defecating during the flight. On the morning of the trip he typically ate a steak breakfast. After having sensors applied to his body and being dressed in the pressure suit, he started breathing pure oxygen to prepare him for the atmosphere of the spacecraft. He arrived at the launch pad, took the elevator up the launch tower and entered the spacecraft two hours before launch. Once the astronaut was secured inside, the hatch was bolted, the launch area evacuated and the mobile tower rolled back. After this, the launch vehicle was filled with liquid oxygen. The entire procedure of preparing for launch and launching the spacecraft followed a time table called the countdown. It started a day in advance with a pre-count, in which all systems of the launch vehicle and spacecraft were checked. After that followed a 15 hour hold, during which pyrotechnics were installed. Then came the main countdown which for orbital flights started 6½ hours before launch (T – 390 min), counted backwards to launch (T = 0) and then forward until orbital insertion (T + 5 min).
On an orbital mission, the Atlas' rocket engines were ignited 4 seconds before lift-off. The launch vehicle was held to the ground by clamps and then released when sufficient thrust was built up at lift-off (A). After 30 seconds of flight, the point of maximum dynamic pressure against the vehicle was reached, at which the astronaut felt heavy vibrations. The booster engines were cut off and released after 2 minutes and 10 seconds (B). At this point, the launch escape system was no longer needed, and was separated from the spacecraft by its jettison rocket (C). The launch vehicle moved gradually to a horizontal attitude during launch until, at an altitude of 87 nmi, the spacecraft was inserted into orbit (D). This happened after 5 minutes and 10 seconds in a direction pointing east, whereby the spacecraft would gain speed from the rotation of the Earth. Here the spacecraft fired the three posigrade rockets for a second to separate it from the launch vehicle. Just before orbital insertion and sustainer engine cutoff, g-loads peaked at 8 g (6 g for a suborbital flight). In orbit, the spacecraft automatically turned 180°, pointed the retropackage forward and its nose 14.5° downward and kept this attitude for the rest of the orbital phase of the mission, as it was necessary for communication with the ground. Once in orbit, it was not possible for the spacecraft to change its trajectory except by initiating reentry. Each orbit would typically take 88 minutes to complete. The lowest point of the orbit called perigee was at the point where the spacecraft entered orbit and was about 87 nmi, the highest called apogee was on the opposite side of Earth and was about 150 nmi. When leaving orbit (E) the angle downward was increased to 34°, which was the angle of retrofire. Retrorockets fired for 10 seconds each (F) in a sequence where one started 5 seconds after the other. During reentry (G), the astronaut would experience about 8 g (11–12 g on a suborbital mission). The temperature around the heat shield rose to 3,000 °F and at the same time, there was a two minute radio blackout due to ionization of the air around the spacecraft. After re-entry, a small, drogue parachute (H) was deployed at 21,000 ft for stabilizing the spacecraft's descent. The main parachute (I) was deployed at 10,000 ft starting with a narrow opening that opened fully in a few seconds to lessen the strain on the lines. Just before hitting the water, the landing bag inflated from behind the heat shield to reduce the force of impact (J). Upon landing the parachutes were released. An antenna (K) was raised and sent out signals that could be traced by ships and helicopters. Further, a green marker dye was spread around the spacecraft to make its location more visible from the air. Frogmen brought in by helicopters inflated a collar around the craft to keep it upright in the water. The recovery helicopter hooked onto the spacecraft and the astronaut blew the escape hatch to exit the capsule. He was then hoisted aboard the helicopter that finally brought both him and the spacecraft to the ship.
The number of personnel supporting a Mercury mission was typically around 18,000, with about 15,000 people associated with recovery. Most of the others followed the spacecraft from the World Wide Tracking Network, a chain of 18 stations placed around the equator, which was based on a network used for satellites and made ready in 1960. It collected data from the spacecraft and provided two-way communication between the astronaut and the ground. Each station had a range of 1300 km and a pass typically lasted 7 minutes. Mercury astronauts on the ground would take part of the Capsule Communicator or CAPCOM who communicated with the astronaut in orbit. Data from the spacecraft was sent to the ground, processed at the Goddard Space Center and relayed to the Mercury Control Center at Cape Canaveral. In the Control Center, the data was displayed on boards on each side of a world map, which showed the position of the spacecraft, its ground track and the place it could land in an emergency within the next 30 minutes.
Flights.
Cape Canaveral
Hawaii
Freedom 7
Liberty Bell 7
Friendship 7
Aurora 7
Sigma 7
Faith 7
On April 12, 1961 the Soviet cosmonaut Yuri Gagarin became the first person in space on an orbital flight. Alan Shepard became the first American in space on a suborbital flight three weeks later, on May 5, 1961. John Glenn, the third Mercury astronaut to fly, became the first American to reach orbit on February 20, 1962, but only after the Soviets had launched a second cosmonaut, Gherman Titov, into a day-long flight in August 1961. Three more Mercury orbital flights were made, ending on May 16, 1963 with a day-long, 22 orbit flight. However, the Soviet Union ended its Vostok program the next month, with the human spaceflight endurance record set by the 82-orbit, almost 5-day Vostok 5 flight.
Manned.
All of the manned Mercury flights were successful. The main medical problems encountered were simple personal hygiene, and post-flight symptoms of low blood pressure. The launch vehicles had been tested through unmanned flights, therefore the numbering of manned missions did not start with 1. Also, since two different launch vehicles were used, there were two separate numbered series: MR for "Mercury-Redstone" (suborbital flights), and MA for "Mercury-Atlas" (orbital flights). These names were not popularly used, since the astronauts followed a pilot tradition, each giving their spacecraft a name. They selected names ending with a "7" to commemorate the seven astronauts. Mercury-Redstone flights were launched from Launch Complex-5 while the Mercury-Atlas flights were launched from Launch Complex-14. Times given are Universal Coordinated Time, local time + 5 hours.
Unmanned.
The unmanned flights used Little Joe, Redstone, and Atlas launch vehicles. They were used to develop the launch vehicles, launch escape system, spacecraft and tracking network. One flight of a Scout rocket attempted to launch an unmanned satellite for testing the ground tracking network, but failed to reach orbit. The Little Joe program used seven airframes for eight flights, of which three were successful. The second Little Joe flight was named Little Joe 6, because it was inserted into the program after the first 5 airframes had been allocated.
Impact and legacy.
The project was delayed by 22 months, counting from the beginning until the first orbital mission. It had a dozen prime contractors, 75 major subcontractors, and about 7200 third-tier subcontractors, who together employed two million persons. An estimate of its cost made by NASA in 1969 gave $392.6 million ($ adjusted for inflation), broken down as follows: Spacecraft: $135.3 million, launch vehicles: $82.9 million, operations: $49.3 million, tracking operations and equipment: $71.9 million and facilities: $53.2 million.
Today the Mercury program is commemorated as the first manned American space program. It did not win the race against the Soviet Union, but gave back national prestige and was scientifically a successful precursor of later programs such as Gemini, Apollo and Skylab. During the 1950s, some experts doubted that manned spaceflight was possible. Still when John F. Kennedy was elected president, many including he had doubts about the project. As president he chose to support the programs a few months before the launch of "Freedom 7", which became a great public success. Afterwards, a majority of the American public supported manned spaceflight, and within a few weeks, Kennedy announced a plan for a manned mission to land on the Moon and return safely to Earth before the end of the 1960s. The six astronauts who flew were awarded medals, driven in parades and two of them were invited to address a joint session of the US Congress. As a response to the selection criteria, which ruled out women, a private project was founded in which 13 women pilots successfully underwent the same tests as the men in Project Mercury. It was named Mercury 13 by the media Despite this effort, NASA did not select female astronauts until 1978 for the Space Shuttle.
In 1964, a monument commemorating Project Mercury was unveiled near Launch Complex 14 at Cape Canaveral, featuring a metal logo combining the symbol of Mercury with the number 7. In 1962, the United States Postal Service honored the Mercury-Atlas 6 flight with a Project Mercury commemorative stamp, the first US postal issue to depict a manned spacecraft. The stamp first went on sale in Cape Canaveral, Florida on February 20, 1962, the same day as the first manned orbital flight. On May 4, 2011, the Postal Service released a stamp commemorating the 50th anniversary of "Freedom 7", the first manned flight of the project. On film, the program was portrayed in "The Right Stuff" a 1983 adaptation of Tom Wolfe's 1979 book of the same name. On February 25, 2011, the Institute of Electrical and Electronic Engineers, the world's largest technical professional society, awarded Boeing (the successor company to McDonnell Aircraft) a Milestone Award for important inventions which debuted on the Mercury spacecraft.
Patches.
Commemorative patches were designed by entrepreneurs after the Mercury program to satisfy collectors.

</doc>
