<doc id="13684" url="http://en.wikipedia.org/wiki?curid=13684" title="Hildegard of Bingen">
Hildegard of Bingen

Saint Hildegard of Bingen, O.S.B. (German: "Hildegard von Bingen"; Latin: "Hildegardis Bingensis") (1098 – 17 September 1179), also known as Saint Hildegard and Sibyl of the Rhine, was a German writer, composer, philosopher, Christian mystic, Benedictine abbess, visionary, and polymath. 
Hildegard was elected "magistra" by her fellow nuns in 1136; she founded the monasteries of Rupertsberg in 1150 and Eibingen in 1165. One of her works as a composer, the "Ordo Virtutum", is an early example of liturgical drama and arguably the oldest surviving morality play. She wrote theological, botanical, and medicinal texts, as well as letters, liturgical songs, and poems, while supervising miniature illuminations in the Rupertsberg manuscript of her first work, "Scivias".
Although the history of her formal consideration is complicated, she has been recognized as a saint by branches of the Roman Catholic Church for centuries. On 7 October 2012, Pope Benedict XVI named her a Doctor of the Church.
Biography.
Hildegard's exact date of birth is uncertain. She was born around the year 1098 to Mechtild of Merxheim-Nahet and Hildebert of Bermersheim, a family of the free lower nobility in the service of the Count Meginhard of Sponheim. Sickly from birth, Hildegard is traditionally considered their youngest and tenth child, although there are records of seven older siblings.In her "Vita", Hildegard states that from a very young age she had experienced visions.
Monastic life.
Perhaps due to Hildegard's visions, or as a method of political positioning, Hildegard's parents offered her as an oblate to the church. The date of Hildegard's enclosure in the church is the subject of a contentious debate. Her "Vita" says she was enclosed with an older nun, Jutta, at the age of eight. However, Jutta's enclosure date is known to be in 1112, when Hildegard would have been fourteen. Some scholars speculate that Hildegard was placed in the care of Jutta, the daughter of Count Stephan II of Sponheim, at the age of eight, and the two women were enclosed together six years later. The written record of the "Life of Jutta" indicates that Hildegard probably assisted her in reciting the Psalms, working in the garden, and tending to the sick.
In any case, Hildegard and Jutta were enclosed at Disibodenberg in the Palatinate Forest in what is now Germany. Jutta was also a visionary and thus attracted many followers who came to visit her at the enclosure. Hildegard tells us that Jutta taught her to read and write, but that she was unlearned and therefore incapable of teaching Hildegard Biblical interpretation. Hildegard and Jutta most likely prayed, meditated, read scriptures such as the psalter, and did handwork during the hours of the Divine Office. This might have been a time when Hildegard learned how to play the ten-stringed psaltery. Volmar, a frequent visitor, may have taught Hildegard simple psalm notation. The time she studied music could have been the beginning of the compositions she would later create.
Upon Jutta's death in 1136, Hildegard was unanimously elected as "magistra" of the community by her fellow nuns. Abbot Kuno of Disibodenberg asked Hildegard to be Prioress, which would be under his authority. Hildegard, however, wanted more independence for herself and her nuns, and asked Abbot Kuno to allow them to move to Rupertsberg. This was to be a move towards poverty, from a stone complex that was well established to a temporary dwelling place. When the abbot declined Hildegard's proposition, Hildegard went over his head and received the approval of Archbishop Henry I of Mainz. Abbot Kuno did not relent until Hildegard was stricken by an illness that kept her paralyzed and unable to move from her bed, an event that she attributed to God's unhappiness at her not following his orders to move her nuns to Rupertsberg. It was only when the Abbot himself could not move Hildegard that he decided to grant the nuns their own monastery. Hildegard and about twenty nuns thus moved to the St. Rupertsberg monastery in 1150, where Volmar served as provost, as well as Hildegard's confessor and scribe. In 1165 Hildegard founded a second monastery for her nuns at Eibingen.
Visions.
Hildegard says that she first saw "The Shade of the Living Light" at the age of three, and by the age of five she began to understand that she was experiencing visions. She used the term 'visio' to this feature of her experience, and recognized that it was a gift that she could not explain to others. Hildegard explained that she saw all things in the light of God through the five senses: sight, hearing, taste, smell, and touch. Hildegard was hesitant to share her visions, confiding only to Jutta, who in turn told Volmar, Hildegard's tutor and, later, secretary. Throughout her life, she continued to have many visions, and in 1141, at the age of 42, Hildegard received a vision she believed to be an instruction from God, to "write down that which you see and hear." Still hesitant to record her visions, Hildegard became physically ill. The illustrations recorded in the book of Scivias were visions that Hildegard experienced, causing her great suffering and tribulations. In her first theological text, "Scivias" ("Know the Ways"), Hildegard describes her struggle within:
But I, though I saw and heard these things, refused to write for a long time through doubt and bad opinion and the diversity of human words, not with stubbornness but in the exercise of humility, until, laid low by the scourge of God, I fell upon a bed of sickness; then, compelled at last by many illnesses, and by the witness of a certain noble maiden of good conduct [the nun Richardis von Stade] and of that man whom I had secretly sought and found, as mentioned above, I set my hand to the writing. While I was doing it, I sensed, as I mentioned before, the deep profundity of scriptural exposition; and, raising myself from illness by the strength I received, I brought this work to a close – though just barely – in ten years. (...) And I spoke and wrote these things not by the invention of my heart or that of any other person, but as by the secret mysteries of God I heard and received them in the heavenly places. And again I heard a voice from Heaven saying to me, 'Cry out therefore, and write thus!'
It was between November 1147 and February 1148 at the synod in Trier that Pope Eugenus heard about Hildegard’s writings. It was from this that she received Papal approval to document her visions as revelations from the Holy Spirit giving her instant credence.
Before Hildegard’s death, a problem arose with the clergy of Mainz. A man buried in Rupertsburg had died after excommunication from the Church. Therefore, the clergy wanted to remove his body from the sacred ground. Hildegard did not accept this idea, replying that it was a sin and that the man had been reconciled to the church at the time of his death.
On 17 September 1179, when Hildegard died, her sisters claimed they saw two streams of light appear in the skies and cross over the room where she was dying.
"Vita Sanctae Hildegardis".
Hildegard's hagiography, "Vita Sanctae Hildegardis", was compiled by the monk Theoderic of Echternach after Hildegard's death. He included the hagiographical work "Libellus" or "Little Book" begun by Godfrey of Disibodenberg. Godfrey had died before he was able to complete his work. Guibert of Gembloux was invited to finish the work; however, he had to return to his monastery with the project unfinished. Theoderic utilized sources Guibert had left behind to complete the "Vita".
Works.
Hildegard's works include three great volumes of visionary theology; a variety of musical compositions for use in liturgy, as well as the musical morality play "Ordo Virtutum"; one of the largest bodies of letters (nearly 400) to survive from the Middle Ages, addressed to correspondents ranging from Popes to Emperors to abbots and abbesses, and including records of many of the sermons she preached in the 1160s and 1170s; two volumes of material on natural medicine and cures; an invented language called the "Lingua ignota" ("unknown language"); and various minor works, including a gospel commentary and two works of hagiography.
Several manuscripts of her works were produced during her lifetime, including the illustrated Rupertsberg manuscript of her first major work, "Scivias" (lost since 1945); the Dendermonde manuscript, which contains one version of her musical works; and the Ghent manuscript, which was the first fair-copy made for editing of her final theological work, the "Liber Divinorum Operum". At the end of her life, and probably under her initial guidance, all of her works were edited and gathered into the single .
Visionary theology.
Hildegard's most significant works were her three volumes of visionary theology: "Scivias" ("Know the Ways", composed 1142-1151), "Liber Vitae Meritorum" ("Book of Life's Merits" or "Book of the Rewards of Life", composed 1158-1163); and "Liber Divinorum Operum" ("Book of Divine Works", also known as "De operatione Dei", "On God's Activity", composed 1163/4-1172 or 1174). In these volumes, the last of which was completed when she was well into her seventies, Hildegard first describes each vision, whose details are often strange and enigmatic; and then interprets their theological contents in the words of the "voice of the Living Light."
"Scivias".
The composition of the first work, "Scivias", was triggered by the insistence of her visionary experiences in about 1142, when she was already forty-three years old. Perceiving a divine command to "write down what you see and hear", Hildegard began to record her visionary experiences. "Scivias" is structured into three parts of unequal length. The first part (six visions) chronicles the order of God's creation: the Creation and Fall of Adam and Eve, the structure of the universe (famously described as an "egg"), the relationship between body and soul, God's relationship to his people through the Synagogue, and the choirs of angels. The second part (seven visions) describes the order of redemption: the coming of Christ the Redeemer, the Trinity, the Church as the Bride of Christ and the Mother of the Faithful in baptism and confirmation, the orders of the Church, Christ's sacrifice on the Cross and the Eucharist, and the fight against the devil. Finally, the third part (thirteen visions) recapitulates the history of salvation told in the first two parts, symbolized as a building adorned with various allegorical figures and virtues. It concludes with the Symphony of Heaven, an early version of Hildegard's musical compositions.
Portions of the uncompleted work were read aloud to Pope Eugenius III at the Synod of Trier in 1148, after which he sent Hildegard a letter with his blessing. This blessing was later construed as papal approval for all of Hildegard's wide-ranging theological activities. Towards the end of her life, Hildegard commissioned a richly decorated manuscript of "Scivias" (the Rupertsberg Codex); although the original has been lost since its evacuation to Dresden for safekeeping in 1945, its images are preserved in a hand-painted facsimile from the 1920s.
"Liber Vitae Meritorum".
In her second volume of visionary theology, composed between 1158 and 1163, after she had moved her community of nuns into independence at the Rupertsberg in Bingen, Hildegard tackled the moral life in the form of dramatic confrontations between the virtues and the vices. She had already explored this area in her musical morality play, "Ordo Virtutum", and the "Book of the Rewards of Life" takes up that play's characteristic themes. Each vice, although ultimately depicted as ugly and grotesque, nevertheless offers alluring, seductive speeches that attempt to entice the unwary soul into their clutches. Standing in our defense, however, are the sober voices of the Virtues, powerfully confronting every vicious deception.
Amongst the work's innovations is one of the earliest descriptions of purgatory as the place where each soul would have to work off its debts after death before entering heaven. Hildegard's descriptions of the possible punishments there are often gruesome and grotesque, which emphasize the work's moral and pastoral purpose as a practical guide to the life of true penance and proper virtue.
"Liber Divinorum Operum".
Hildegard's last and grandest visionary work had its genesis in one of the few times she experienced something like an ecstatic loss of consciousness. As she described it in an autobiographical passage included in her Vita, sometime in about 1163, she received "an extraordinary mystical vision" in which was revealed the "sprinkling drops of sweet rain" that John the Evangelist experienced when he wrote, "In the beginning was the Word..." (John 1:1). Hildegard perceived that this Word was the key to the "Work of God", of which humankind is the pinnacle. The "Book of Divine Works", therefore, became in many ways an extended explication of the Prologue to John's Gospel.
The ten visions of this work's three parts are cosmic in scale, often populated by the grand allegorical female figures representing Divine Love ("Caritas") or Wisdom ("Sapientia"). The first of these opens the work with a salvo of poetic and visionary images, swirling about to characterize the dynamic activity of God within the scope of his salvation-historical work. The remaining three visions of the first part introduce the famous image of a human being standing astride the spheres that make up the universe, and detail the intricate relationships between the human as microcosm and the universe as macrocosm. This culminates in the final chapters of Part One, Vision Four with Hildegard's direct rumination on the meaning of "In the beginning was the Word..." (John 1:1). The single vision that comprises the whole of Part Two stretches that rumination back to the opening of Genesis, and forms an extended meditation on the six days of the creation of the world. Finally, the five visions of the third part take up again the building imagery of "Scivias" to describe the course of salvation history.
Music.
Attention in recent decades to women of the medieval Church has led to a great deal of popular interest in Hildegard's music. In addition to the "Ordo Virtutum," sixty-nine musical compositions, each with its own original poetic text, survive, and at least four other texts are known, though their musical notation has been lost. This is one of the largest repertoires among medieval composers.
One of her better known works, "Ordo Virtutum" ("Play of the Virtues"), is a morality play. It is unsure when some of Hildegard’s compositions were composed, though the Ordo Virtutum is thought to have been composed as early as 1151. The morality play consists of monophonic melodies for the Anima (human soul) and 16 Virtues. There is also one speaking part for the Devil. Scholars assert that the role of the Devil would have been played by Volmar, while Hildegard's nuns would have played the parts of Anima and the Virtues.
In addition to the "Ordo Virtutum" Hildegard composed many liturgical songs that were collected into a cycle called the "Symphonia armoniae celestium revelationum." The songs from the Symphonia are set to Hildegard’s own text and range from antiphons, hymns, and sequences, to responsories. Her music is described as monophonic, that is, consisting of exactly one melodic line. Its style is characterized by soaring melodies that can push the boundaries of the more staid ranges of traditional Gregorian chant. Though Hildegard's music is often thought to stand outside the normal practices of monophonic monastic chant, current researchers are also exploring ways in which it may be viewed in comparison with her contemporaries, such as Hermannus Contractus. Another feature of Hildegard's music that both reflects twelfth-century evolutions of chant and pushes those evolutions further is that it is highly melismatic, often with recurrent melodic units. Scholars such as Margot Fassler, Marianne Richert Pfau, and Beverly Lomer also note the intimate relationship between music and text in Hildegard's compositions, whose rhetorical features are often more distinct than is common in twelfth-century chant. As with all medieval chant notation, Hildegard's music lacks any indication of tempo or rhythm; the surviving manuscripts employ late German style notation, which uses very ornamental neumes. The reverence for the Virgin Mary reflected in music shows how deeply influenced and inspired Hildegard of Bingen and her community were by the Virgin Mary and the saints.
The definition of viriditas or ‘greenness’ is an earthly expression of the heavenly in an integrity that overcomes dualisms. This ‘greenness’ or power of life appears frequently in Hildegard’s works.
One scholar has asserted that Hildegard made a close association between music and the female body in her musical compositions. If so, the poetry and music of Hildegard’s Symphonia would be concerned with the anatomy of female desire thus described as Sapphonic, or pertaining to Sappho, connecting her to a history of female rhetoricians.
Scientific and medicinal writings.
Hildegard’s medicinal and scientific writings, though thematically complementary to her ideas about nature expressed in her visionary works, are different in focus and scope. Neither claim to be rooted in her visionary experience and its divine authority. Rather, they spring from her experience helping in and then leading the monastery’s herbal garden and infirmary, as well as the theoretical information she likely gained through her wide-ranging reading in the monastery’s library. As she gained practical skills in diagnosis, prognosis, and treatment, she combined physical treatment of physical diseases with holistic methods centered on “spiritual healing.” She became well known for her healing powers involving practical application of tinctures, herbs, and precious stones. She combined these elements with a theological notion ultimately derived from Genesis: all things put on earth are for the use of humans.
Hildegard catalogued both her practical expertise and its theoretical basis in two works: "Physica," whose nine books focus on the scientific and medicinal properties of various plants, stones, fish, reptiles, and animals; and "Causae et Curae", an exploration of the human body, its connections to the rest of the natural world, and the causes and cures of various diseases. These works document a variety of medical practices, and Hildegard may have used them to teach another nun at the monastery to be her assistant. Moreover, they serve as a valuable witness to areas of medieval medicine that were often not as well documented because their practitioners (mainly women) did not often write in Latin. Among the practices that Hildegard discusses in "Causae et Curae" is the use of bleeding and home remedies for many common ailments. She also focuses many of her remedies on common agricultural injuries such as burns, fractures, dislocations, and cuts.
In addition to its wealth of practical evidence, "Causae et Curae" is also noteworthy for its organizational scheme. Its first part sets the work within the context of the creation of the cosmos and then humanity as its summit, and the constant interplay of the human person as microcosm both physically and spiritually with the macrocosm of the universe informs all of Hildegard’s approach. Her hallmark is to emphasize the vital connection between the “green” health of the natural world and the holistic health of the human person. Thus, when she approached medicine as a type of gardening, it was not just as an analogy. Rather, Hildegard understood the plants and elements of the garden as direct counterparts to the humors and elements within the human body, whose imbalance led to illness and disease.
Thus, the nearly three hundred chapters of the second book of "Causae et Curae" “explore the etiology, or causes, of disease as well as human sexuality, psychology, and physiology.” In this section, she give specific instructions for bleeding based on various factors, including gender, the phase of the moon (bleeding is best done when moon is waning), the place of disease (use veins near diseased organ of body part) or prevention (big veins in arms), and how much blood to take (described in imprecise measurements, like “the amount that a thirsty person can swallow in one gulp”). She even includes bleeding instructions for animals to keep them healthy. In the third and fourth sections, Hildegard turns her attention to treatments for malignant and minor problems and diseases according to the humoral theory, again including information on animal health. The fifth section is about diagnosis and prognosis, which includes instructions to check the patient’s blood, pulse, urine and stool. Finally, the sixth section documents a lunar horoscope to provide an additional means of prognosis for both disease and other medical conditions, such as conception and the outcome of pregnancy. For example, she indicates that a waxing moon is good for conception (for humans) and is also good for sowing seeds for plants (sowing seeds is the plant equivalent of conception). Elsewhere, Hildegard is even said to have stressed the value of boiling drinking water in an attempt to prevent infection.
As Hildegard elaborates the medical and scientific relationship between the human microcosm and the macrocosm of the universe, she often focuses on interrelated patterns of four: “the four elements (fire, air, water, and earth), the four seasons, the four humors, the four zones of the earth, and the four major winds.” Although she inherited the basic framework of humoral theory from ancient medicine, however, Hildegard’s conception of the hierarchical interbalance of the four humors (blood, phlegm, black bile, and yellow bile) was unique, based on their correspondence to “superior” and “inferior” elements—blood and phlegm corresponding to the “celestial” elements of fire and air, and the two biles corresponding to the “terrestrial” elements of water and earth. Hildegard understood the disease-causing imbalance of these humors to result from the improper dominance of the subordinate humors. This disharmony reflects that introduced by Adam and Eve in the Fall, which for Hildegard marked the indelible entrance of disease and humoral imbalance into humankind. As she writes in "Causae et Curae" c. 42:
It happens that certain men suffer diverse illnesses. This comes from the phlegm which is superabundant within them. For if man had remained in paradise, he would not have had the "flegmata" within his body, from which many evils proceed, but his flesh would been whole and without dark humor ["livor"]. However, because he consented to evil and relinquished good, he was made into a likeness of the earth, which produces good and useful herbs, as well as bad and useless ones, and which has in itself both good and evil moistures. From tasting evil, the blood of the sons of Adam was turned into the poison of semen, out of which the sons of man are begotten. And therefore their flesh is ulcerated and permeable [to disease]. These sores and openings create a certain storm and smoky moisture in men, from which the "flegmata" arise and coagulate, which then introduce diverse infirmities to the human body. All this arose from the first evil, which man began at the start, because if Adam had remained in paradise, he would have had the sweetest health, and the best dwelling-place, just as the strongest balsam emits the best odor; but on the contrary, man now has within himself poison and phlegm and diverse illnesses.
"Lingua Ignota" and invented alphabet.
Hildegard also invented an alternative alphabet. The text of her writing and compositions reveals Hildegard's use of this form of modified medieval Latin, encompassing many invented, conflated and abridged words. Due to her inventions of words for her lyrics and use of a constructed script, many conlangers look upon her as a medieval precursor. Scholars believe that Hildegard used her "Lingua Ignota" to increase solidarity among her nuns.
Significance.
During her lifetime.
Maddocks claims that it is likely Hildegard learned simple Latin and the tenets of the Christian faith but was not instructed in the Seven Liberal Arts, which formed the basis of all education for the learned classes in the Middle Ages: the "Trivium" of grammar, dialectic, and rhetoric plus the "Quadrivium" of arithmetic, geometry, astronomy, and music. The correspondence she kept with the outside world, both spiritual and social, transgressed the cloister as a space of female confinement and served to document Hildegard’s grand style and strict formatting of medieval letter writing.
Contributing to Christian European rhetorical traditions, Hildegard "authorized herself as a theologian" through alternative rhetorical arts. Hildegard was creative in her interpretation of theology. She believed that her monastery should exclude novices who were not from the nobility because she did not want her community to be divided on the basis of social status. She also stated that "woman may be made from man, but no man can be made without a woman."
Due to church limitation on public, discursive rhetoric, the medieval rhetorical arts included preaching, letter writing, poetry, and the encyclopedic tradition. Hildegard’s participation in these arts speaks to her significance as a female rhetorician, transcending bans on women's social participation and interpretation of scripture. The acceptance of public preaching by a woman, even a well-connected abbess and acknowledged prophet, does not fit the stereotype of this time. Her preaching was not limited to the monasteries; she preached publicly in 1160 in Germany. (New York: Routledge, 2001, 9). She conducted four preaching tours throughout Germany, speaking to both clergy and laity in chapter houses and in public, mainly denouncing clerical corruption and calling for reform.
Many abbots and abbesses asked her for prayers and opinions on various matters. She traveled widely during her four preaching tours. She had several fanatical followers, including Guibert of Gembloux, who wrote to her frequently and became her secretary after Volmar's death in 1173. Hildegard also influenced several monastic women, exchanging letters with Elisabeth of Schönau, a nearby visionary.
Hildegard corresponded with popes such as Eugene III and Anastasius IV, statesmen such as Abbot Suger, German emperors such as Frederick I Barbarossa, and other notable figures such as Saint Bernard of Clairvaux, who advanced her work, at the behest of her abbot, Kuno, at the Synod of Trier in 1147 and 1148. Hildegard of Bingen's correspondence is an important component of her literary output.
Beatification, canonization and recognition as a Doctor of the Church.
Hildegard was one of the first persons for whom the Roman canonization process was officially applied, but the process took so long that four attempts at canonization were not completed and she remained at the level of her beatification. Her name was nonetheless taken up in the Roman Martyrology at the end of the 16th century. Her feast day is 17 September. Numerous popes have referred to Hildegard as a saint, including Pope John Paul II and Pope Benedict XVI.
On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization," thus laying the groundwork for naming her a Doctor of the Church. On 7 October 2012, the feast of the Holy Rosary, the Pope named her a Doctor of the Church, the fourth woman of 35 saints given that title by the Roman Catholic Church. He called her "perennially relevant" and "an authentic teacher of theology and a profound scholar of natural science and music."
Hildegard of Bingen also appears in the calendar of saints of various Anglican churches, such as that of the Church of England, in which she is commemorated on 17 September.
Hildegard's parish and pilgrimage church in Eibingen near Rüdesheim houses her relics.
Modern interest.
Recordings and performances of Hildegard's music have gained critical praise and popularity since 1979. See Discography listed below.
In recent years, Hildegard has become of particular interest to feminist scholars. They note her reference to herself as a member of the "weaker sex" and her rather constant belittling of women. Hildegard frequently referred to herself as an unlearned woman, completely incapable of Biblical exegesis. Such a statement on her part, however, worked to her advantage because it made her statements that all of her writings and music came from visions of the Divine more believable, therefore giving Hildegard the authority to speak in a time and place where few women were permitted a voice. Hildegard used her voice to condemn church practices she disagreed with, in particular simony.
Hildegard has also become a figure of reverence within the contemporary New Age movement, mostly due to her holistic and natural view of healing, as well as her status as a mystic. Although her work has often been ignored or belittled in the history of medicince, she was the inspiration for Dr. Gottfried Hertzka's "Hildegard-Medicine", and is the namesake for June Boyce-Tillman's Hildegard Network, a healing center that focuses on a holistic approach to wellness and brings together people interested in exploring the links between spirituality, the arts, and healing. Her reputation as a medicinal writer and healer was also used by early feminists to argue for women's rights to attend medical schools. Hildegard's reincarnation has been debated since 1924 when Austrian mystic Rudolf Steiner lectured that a nun of her description was the past life of Russian poet philosopher Vladimir Soloviev, whose Sophianic visions are often compared to Hildegard. Sophiologist Robert Powell writes that hermetic astrology proves the match, while mystical communities in Hildegard's lineage include that of artist Carl Schroeder as studied by Columbia sociologist Courtney Bender and supported by reincarnation researchers Walter Semkiw and Kevin Ryerson.
In space, the minor planet 898 Hildegard is named for her.
In film, Hildegard has been portrayed by Patricia Routledge in a BBC documentary called "Hildegard of Bingen" (1994), by Ángela Molina in "Barbarossa" (2009) and by Barbara Sukowa in the film "Vision", directed by Margarethe von Trotta.
Hildegard was the subject of a 2012 fictionalized biographic novel "Illuminations" by Mary Sharratt.
The first single of the album "Mala" by folk singer Devendra Banhart is named after her.
The plant genus "Hildegardia" is named after her due to her contributions to herbal medicine.
A feature documentary film, "The Unruly Mystic: Saint Hildegard," was released by American director Michael M. Conti in 2014.
References.
Primary Sources (in translation):
Hildegard of Bingen. 
Secondary Sources:
</dl>
Further reading.
</dl>

</doc>
<doc id="13686" url="http://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum (]) is a municipality and a town in the Netherlands, in the province of North Holland. Located in the region called "Het Gooi", it is the largest town in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.
The town.
Hilversum lies 31 km south-east of Amsterdam and 19 km north of Utrecht.
The town is often called "media city" since it is the principal centre for radio and television broadcasting in The Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based here. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS (Nederlandse Omroep Stichting), as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies. As a result many old AM radio sets in Europe had a "Hilversum" dial position marked on their tuning scales (along with Athlone, Kalundborg and others).
Hilversum is also known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.
Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centers (such as Hilvertshof, De Gijsbrecht, Kerkelanden, Riebeeck Galerij and Seinhorst.) In the region, the city centre is called "het dorp", which means "the village".
History.
Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BCE material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424, on 21 March at 6:30 am (the hour at which people got up, as the farm was full of restless and loud animals), Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development. The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair. In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."
For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. The city was the headquarters of the German ground forces (Heer) in the Netherlands .
The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands, and in 1948 being taken over by Philips. By then the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. In the meantime, almost all Dutch radio broadcasting organizations (followed by television broadcasters in the 1950s) established their headquarters in Hilversum and provided a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.
In 1964, the population reached a record high – over 103,000 people called Hilversum home. The current population hovers around 85,000. Several factors figure into the decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat (""). The third reason for this decline of the population was due to the fact that the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.
Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.
Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from 'Leefbaar Hilversum' teamed up with 'Leefbaar Utrecht' leaders to found a national 'Leefbaar Nederland' party. By strange coincidence, in 2002 the most vocal 'Leefbaar Rotterdam' politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.
The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo (""), the largest man-made wildlife crossing in the world.
The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn, and of a January 29, 2015 fake gunman demanding airtime at Nederlandse Omroep Stichting's headquarters.
The population declined from 103,000 in 1964 to 84,000 in 2006.
Transport.
Hilversum is well connected to the Dutch railway network, and contains three stations: 
One can get the best connections from the station Hilversum, as this is an Intercity station.
Local government.
The municipal council of Hilversum in 2010 consists of 37 seats, which are divided as followed:
It was the first city with a "Leefbaar" party (which was intended as just a local party).
Notable residents.
Notable people born in Hilversum:

</doc>
<doc id="13688" url="http://en.wikipedia.org/wiki?curid=13688" title="The Hound of Heaven">
The Hound of Heaven

"The Hound of Heaven" is a 182-line poem written by English poet Francis Thompson (1859–1907). The poem became famous and was the source of much of Thompson's posthumous reputation. The poem was first published in Thompson's first volume of poems in 1893. It was included in the Oxford Book of English Mystical Verse (1917). It was also an influence on J. R. R. Tolkien, who read it a few years before it was published in 1917.
One of the most loved and possibly one of the more difficult Christian poems to read and appreciate, "The Hound of Heaven" has been loved for over a century. It is not, however, a poem that most people cannot read without some background. ... [D]o not be dissuaded from reading it.
"The name is strange. It startles one at first. It is so bold, so new, so fearless. It does not attract, rather the reverse. But when one reads the poem this strangeness disappears. The meaning is understood. As the hound follows the hare, never ceasing in its running, ever drawing nearer in the chase, with unhurrying and imperturbed pace, so does God follow the fleeing soul by His Divine grace. And though in sin or in human love, away from God it seeks to hide itself, Divine grace follows after, unwearyingly follows ever after, till the soul feels its pressure forcing it to turn to Him alone in that never ending pursuit." The Neumann Press "Book of Verse", 1988

</doc>
<doc id="13692" url="http://en.wikipedia.org/wiki?curid=13692" title="History of the Internet">
History of the Internet

The history of the Internet begins with the development of electronic computers in the 1950s. Initial concepts of packet networking originated in several computer science laboratories in the United States, Great Britain, and France. The US Department of Defense awarded contracts as early as the 1960s for packet network systems, including the development of the ARPANET (which would become the first network to use the Internet Protocol.) The first message was sent over the ARPANET from computer science Professor Leonard Kleinrock's laboratory at University of California, Los Angeles (UCLA) to the second network node at Stanford Research Institute (SRI).
Packet switching networks such as ARPANET, Mark I at NPL in the UK, CYCLADES, Merit Network, Tymnet, and Telenet, were developed in the late 1960s and early 1970s using a variety of communications protocols. The ARPANET in particular led to the development of protocols for internetworking, in which multiple separate networks could be joined into a network of networks.
Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet protocol suite (TCP/IP) was introduced as the standard networking protocol on the ARPANET. In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities, and provided interconnectivity in 1986 with the NSFNET project, which also created network access to the supercomputer sites in the United States from research and education organizations. Commercial Internet service providers (ISPs) began to emerge in the late 1980s. The ARPANET was decommissioned in 1990. Private connections to the Internet by commercial entities became widespread quickly, and the NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic.
In the 1980s, the work of Tim Berners-Lee in the United Kingdom, on the World Wide Web, theorised the fact that protocols link hypertext documents into a working system, marking the beginning the modern Internet. Since the mid-1990s, the Internet has had a revolutionary impact on culture and commerce, including the rise of near-instant communication by electronic mail, instant messaging, voice over Internet Protocol (VoIP) telephone calls, two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. The research and education community continues to develop and use advanced networks such as NSF's very high speed Backbone Network Service (vBNS), Internet2, and National LambdaRail. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more. The Internet's takeover of the global communication landscape was almost instant in historical terms: it only communicated 1% of the information flowing through two-way telecommunications networks in the year 1993, already 51% by 2000, and more than 97% of the telecommunicated information by 2007. Today the Internet continues to grow, driven by ever greater amounts of online information, commerce, entertainment, and social networking.
Precursors.
The telegraph system is the first fully digital communication system. Thus the Internet has precursors, such as the telegraph system, that date back to the 19th century, more than a century before the digital Internet became widely used in the second half of the 1990s. The concept of data communication – transmitting data between two different places, connected via some kind of electromagnetic medium, such as radio or an electrical wire – predates the introduction of the first computers. Such communication systems were typically limited to point to point communication between two end devices. Telegraph systems and telex machines can be considered early precursors of this kind of communication.
Fundamental theoretical work in data transmission and information theory was developed by Claude Shannon, Harry Nyquist, and Ralph Hartley, during the early 20th century.
Early computers used the technology available at the time to allow communication between the central processing unit and remote terminals. As the technology evolved, new systems were devised to allow communication over longer distances (for terminals) or with higher speed (for interconnection of local devices) that were necessary for the mainframe computer model. Using these technologies made it possible to exchange data (such as files) between remote computers. However, the point to point communication model was limited, as it did not allow for direct communication between any two arbitrary systems; a physical link was necessary. The technology was also deemed as inherently unsafe for strategic and military use, because there were no alternative paths for the communication in case of an enemy attack.
Inspiration.
A pioneer in the call for a global network, J. C. R. Licklider, a Vice President at Bolt Beranek and Newman, Inc., proposed in his January 1960 paper, "Man-Computer Symbiosis": "A network of such [computers], connected to one another by wide-band communication lines [which provided] the functions of present-day libraries together with anticipated advances in information storage and retrieval and [other] symbiotic functions."
In August 1962, Licklider and Welden Clark published the paper "On-Line Man-Computer Communication", which was one of the first descriptions of a networked future.
In October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within DARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos describing a distributed network to the IPTO staff, whom he called "Members and Affiliates of the Intergalactic Computer Network". As part of the information processing office's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at the University of California, Berkeley and one for the Compatible Time-Sharing System project at the Massachusetts Institute of Technology (MIT). Licklider's identified need for inter-networking would be made obvious by the apparent waste of resources this caused.
For each of these three terminals, I had three different sets of user commands. So if I was talking online with someone at S.D.C. and I wanted to talk to someone I knew at Berkeley or M.I.T. about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them... <BR><BR>I said, oh man, it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go where you have interactive computing. That idea is the ARPAnet.
Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus that led his successors such as Lawrence Roberts and Robert Taylor to further the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.
Packet switching.
At the tip of the problem lay the issue of connecting separate physical networks to form one logical network. In the 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information transmitted across Baran's network would be divided into what he called "message-blocks". Independently, Donald Davies (National Physical Laboratory, UK), proposed and developed a similar network based on what he called packet-switching, the term that would ultimately be adopted. Leonard Kleinrock (MIT) developed a mathematical theory behind this technology. Packet-switching provides better bandwidth utilization and response times than the traditional circuit-switching technology used for telephony, particularly on resource-limited interconnection links.
Packet switching is a rapid store and forward networking design that divides messages up into arbitrary packets, with routing decisions made per-packet. Early networks used message switched systems that required rigid routing structures prone to single point of failure. This led Tommy Krash and Paul Baran's U.S. military-funded research to focus on using message-blocks to include network redundancy.
Networks that led to the Internet.
ARPANET.
Promoted to the head of the information processing office at Defense Advanced Research Projects Agency (DARPA), Robert Taylor intended to realize Licklider's ideas of an interconnected networking system. Bringing in Larry Roberts from MIT, he initiated a project to build such a network. The first ARPANET link was established between the University of California, Los Angeles (UCLA) and the Stanford Research Institute at 22:30 hours on October 29, 1969.
 "We set up a telephone connection between us and the guys at SRI ...", Kleinrock ... said in an interview: "We typed the L and we asked on the phone,
Yet a revolution had begun" ...
By December 5, 1969, a 4-node network was connected by adding the University of Utah and the University of California, Santa Barbara. Building on ideas developed in ALOHAnet, the ARPANET grew rapidly. By 1981, the number of hosts had grown to 213, with a new host being added approximately every twenty days.
ARPANET development was centered around the Request for Comments (RFC) process, still used today for proposing and distributing Internet Protocols and Systems. RFC 1, entitled "Host Software", was written by Steve Crocker from the University of California, Los Angeles, and published on April 7, 1969. These early years were documented in the 1972 film .
ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used. The early ARPANET used the Network Control Program (NCP, sometimes Network Control Protocol) rather than TCP/IP. On January 1, 1983, known as flag day, NCP on the ARPANET was replaced by the more flexible and powerful family of TCP/IP protocols, marking the start of the modern Internet.
International collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks. Notable exceptions were the "Norwegian Seismic Array" (NORSAR) in 1972, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter Kirstein's research group in the UK, initially at the Institute of Computer Science, London University and later at University College London.
NPL.
In 1965, Donald Davies of the National Physical Laboratory (United Kingdom) proposed a national data network based on packet-switching. The proposal was not taken up nationally, but by 1970 he had designed and built the Mark I packet-switched network to meet the needs of the multidisciplinary laboratory and prove the technology under operational conditions. By 1976 12 computers and 75 terminal devices were attached and more were added until the network was replaced in 1986.
Merit Network.
The Merit Network was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.
CYCLADES.
The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the initial ARPANET design and to support network research generally. It was the first network to make the hosts responsible for the reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms.
X.25 and public data networks.
Based on ARPA's research, packet switching network standards were developed by the International Telecommunication Union (ITU) in the form of X.25 and related standards. While using packet switching, X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET. The initial ITU Standard on X.25 was approved in March 1976.
The British Post Office, Western Union International and Tymnet collaborated to create the first international packet switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.
Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.
The first public dial-in networks used asynchronous TTY terminal protocols to reach a concentrator operated in the public network. Some networks, such as CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.
UUCP and Usenet.
In 1979, two students at Duke University, Tom Truscott and Jim Ellis, originated the idea of using Bourne shell scripts to transfer news and messages on a serial line UUCP connection with nearby University of North Carolina at Chapel Hill. Following public release of the software, the mesh of UUCP hosts forwarding on the Usenet news rapidly expanded. UUCPnet, as it would later be named, also created gateways and links between FidoNet and dial-up BBS hosts. UUCP networks spread quickly due to the lower costs involved, ability to use existing leased lines, X.25 links or even ARPANET connections, and the lack of strict use policies compared to later networks like CSNET and Bitnet. All connects were local. By 1981 the number of UUCP hosts had grown to 550, nearly doubling to 940 in 1984. – Sublink Network, operating since 1987 and officially founded in Italy in 1989, based its interconnectivity upon UUCP to redistribute mail and news groups messages throughout its Italian nodes (about 100 at the time) owned both by private individuals and small companies. Sublink Network represented possibly one of the first examples of the internet technology becoming progress through popular diffusion.
Merging the networks and creating the Internet (1973–90).
TCP/IP.
With so many different network methods, something was needed to unify them. Robert E. Kahn of DARPA and ARPANET recruited Vinton Cerf of Stanford University to work with him on the problem. By 1973, they had worked out a fundamental reformulation, where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann, Gerard LeLann and Louis Pouzin (designer of the CYCLADES network) with important work on this design.
The specification of the resulting protocol, "RFC 675 – Specification of Internet Transmission Control Program", by Vinton Cerf, Yogen Dalal and Carl Sunshine, Network Working Group, December 1974, contains the first attested use of the term "internet", as a shorthand for "internetworking"; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.
With the role of the network reduced to the bare minimum, it became possible to join almost any networks together, no matter what their characteristics were, thereby solving Kahn's initial problem. DARPA agreed to fund development of prototype software, and after several years of work, the first demonstration of a gateway between the Packet Radio network in the SF Bay area and the ARPANET was conducted by the Stanford Research Institute. On November 22, 1977 a three network demonstration was conducted including the ARPANET, the SRI's Packet Radio Van on the Packet Radio Network and the Atlantic Packet Satellite network.
Stemming from the first specifications of TCP in 1974, TCP/IP emerged in mid-late 1978 in nearly final form. By 1981, the associated standards were published as RFCs 791, 792 and 793 and adopted for use. DARPA sponsored or encouraged the development of TCP/IP implementations for many operating systems and then scheduled a migration of all hosts on all of its packet networks to TCP/IP. On January 1, 1983, known as flag day, TCP/IP protocols became the only approved protocol on the ARPANET, replacing the earlier NCP protocol.
From ARPANET to NSFNET.
After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting edge research and development, not running a communications utility. Eventually, in July 1975, the network had been turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.
The networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, and even to a growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were.
Several other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.
NASA developed the TCP/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.
In 1981 NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP/IP, and ran TCP/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange.
Its experience with CSNET lead NSF to use TCP/IP when it created NSFNET, a 56 kbit/s backbone established in 1986, to supported the NSF sponsored supercomputing centers. The NSFNET Project also provided support for the creation of regional research and education networks in the United States and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990. NSFNET was expanded and upgraded to 45 Mbit/s in 1991, and was decommissioned in 1995 when it was replaced by backbones operated by several commercial Internet Service Providers.
Transition towards the Internet.
The term "internet" was adopted in the first RFC published on the TCP protocol (RFC 675: Internet Transmission Control Program, December 1974) as an abbreviation of the term "internetworking" and the two terms were used interchangeably. In general, an internet was any network using TCP/IP. It was around the time when ARPANET was interlinked with NSFNET in the late 1980s, that the term was used as the name of the network, Internet, being the large and global TCP/IP network.
As interest in networking grew and new applications for it were developed, the Internet's technologies spread throughout the rest of the world. The network-agnostic approach in TCP/IP meant that it was easy to use any existing network infrastructure, such as the IPSS X.25 network, to carry Internet traffic. In 1984, University College London replaced its transatlantic satellite links with TCP/IP over IPSS.
Many sites unable to link directly to the Internet created simple gateways for the transfer of electronic mail, the most important application of the time. Sites with only intermittent connections used UUCP or FidoNet and relied on the gateways between these networks and the Internet. Some gateway services went beyond simple mail peering, such as allowing access to File Transfer Protocol (FTP) sites via UUCP or mail.
Finally, routing technologies were developed for the Internet to remove the remaining centralized routing aspects. The Exterior Gateway Protocol (EGP) was replaced by a new protocol, the Border Gateway Protocol (BGP). This provided a meshed topology for the Internet and reduced the centric architecture which ARPANET had emphasized. In 1994, Classless Inter-Domain Routing (CIDR) was introduced to support better conservation of address space which allowed use of route aggregation to decrease the size of routing tables.
TCP/IP goes global (1989–2010).
CERN, the European Internet, the link to the Pacific and beyond.
Between 1984 and 1988 CERN began installation and operation of TCP/IP to interconnect its major internal computer systems, workstations, PCs and an accelerator control system. CERN continued to operate a limited self-developed system (CERNET) internally and several incompatible (typically proprietary) network protocols externally. There was considerable resistance in Europe towards more widespread use of TCP/IP, and the CERN TCP/IP intranets remained isolated from the Internet until 1989.
In 1988, Daniel Karrenberg, from Centrum Wiskunde & Informatica (CWI) in Amsterdam, visited Ben Segal, CERN's TCP/IP Coordinator, looking for advice about the transition of the European side of the UUCP Usenet network (much of which ran over X.25 links) over to TCP/IP. In 1987, Ben Segal had met with Len Bosack from the then still small company Cisco about purchasing some TCP/IP routers for CERN, and was able to give Karrenberg advice and forward him on to Cisco for the appropriate hardware. This expanded the European portion of the Internet across the existing UUCP networks, and in 1989 CERN opened its first external TCP/IP connections. This coincided with the creation of Réseaux IP Européens (RIPE), initially a group of IP network administrators who met regularly to carry out coordination work together. Later, in 1992, RIPE was formally registered as a cooperative in Amsterdam.
At the same time as the rise of internetworking in Europe, ad hoc networking to ARPA and in-between Australian universities formed, based on various technologies such as X.25 and UUCPNet. These were limited in their connection to the global networks, due to the cost of making individual international UUCP dial-up or X.25 connections. In 1989, Australian universities joined the push towards using IP protocols to unify their networking infrastructures. AARNet was formed in 1989 by the Australian Vice-Chancellors' Committee and provided a dedicated IP based network for Australia.
The Internet began to penetrate Asia in the late 1980s. Japan, which had built the UUCP-based network JUNET in 1984, connected to NSFNET in 1989. It hosted the annual meeting of the Internet Society, INET'92, in Kobe. Singapore developed TECHNET in 1990, and Thailand gained a global Internet connection between Chulalongkorn University and UUNET in 1992.
Global digital divide.
While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they are building organizations for Internet resource administration and sharing operational experience, as more and more transmission facilities go into place.
Africa.
At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.
In August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit/s, serving a Sun host computer and twelve US Robotics dial-up modems.
In 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Côte d'Ivoire and Benin in 1998.
Africa is building an Internet infrastructure. AfriNIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.
There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.
Asia and Oceania.
The Asia Pacific Network Information Centre (APNIC), headquartered in Australia, manages IP address allocation for the continent. APNIC sponsors an operational forum, the Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT).
In 1991, the People's Republic of China saw its first TCP/IP college network, Tsinghua University's TUNET. The PRC went on to make its first global Internet connection in 1994, between the Beijing Electro-Spectrometer Collaboration and Stanford University's Linear Accelerator Center. However, China went on to implement its own digital divide by implementing a country-wide content filter.
Latin America.
As with the other regions, the Latin American and Caribbean Internet Addresses Registry (LACNIC) manages the IP address space and other resources for its area. LACNIC, headquartered in Uruguay, operates DNS root, reverse DNS, and other key services.
Opening the network to commerce.
The interest in commercial use of the Internet became a hotly debated topic. Although commercial use was forbidden, the exact definition of commercial use could be unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.
During the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. The first commercial dialup ISP in the United States was The World, which opened in 1989.
In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, #redirect , which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.
By 1990, ARPANET had been overtaken and replaced by newer networking technologies and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point for Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995 when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service and the service ended. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.
Networking in outer space.
The first live Internet link into low earth orbit was established on January 22, 2010 when astronaut T. J. Creamer posted the first unassisted update to his Twitter account from the International Space Station, marking the extension of the Internet into space. (Astronauts at the ISS had used email and Twitter before, but these messages had been relayed to the ground through a NASA data link before being posted by a human proxy.) This personal Web access, which NASA calls the Crew Support LAN, uses the space station's high-speed Ku band microwave link. To surf the Web, astronauts can use a station laptop computer to control a desktop computer on Earth, and they can talk to their families and friends on Earth using Voice over IP equipment.
Communication with spacecraft beyond earth orbit has traditionally been over point-to-point links through the Deep Space Network. Each such data link must be manually scheduled and configured. In the late 1990s NASA and Google began working on a new network protocol, Delay-tolerant networking (DTN) which automates this process, allows networking of spaceborne transmission nodes, and takes the fact into account that spacecraft can temporarily lose contact because they move behind the Moon or planets, or because space weather disrupts the connection. Under such conditions, DTN retransmits data packages instead of dropping them, as the standard TCP/IP internet protocol does. NASA conducted the first field test of what it calls the "deep space internet" in November 2008. Testing of DTN-based communications between the International Space Station and Earth (now termed Disruption-Tolerant Networking) has been ongoing since March 2009, and is scheduled to continue until March 2014.
This network technology is supposed to ultimately enable missions that involve multiple spacecraft where reliable inter-vessel communication might take precedence over vessel-to-earth downlinks. According to a February 2011 statement by Google's Vint Cerf, the so-called "Bundle protocols" have been uploaded to NASA's EPOXI mission spacecraft (which is in orbit around the Sun) and communication with Earth has been tested at a distance of approximately 80 light seconds.
Internet governance.
As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. It has no centralized governance for either technology or policies, and each constituent network chooses what technologies and protocols it will deploy from the voluntary technical standards that are developed by the Internet Engineering Task Force (IETF). However, throughout its entire history, the Internet system has had an "Internet Assigned Numbers Authority" (IANA) for the allocation and assignment of various technical identifiers needed for the operation of the Internet. The Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.
NIC, InterNIC, IANA and ICANN.
The IANA function was originally performed by USC Information Sciences Institute, and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. In addition to his role as the RFC Editor, Jon Postel worked as the manager of IANA until his death in 1998.
As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by Paul Mockapetris. The Defense Data Network—Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.
The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the "growth of the Internet and its increasing globalization" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of "continental dimensions"). Registries would be "unbiased and widely recognized by network providers and subscribers" within their region. 
The RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.
Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.
Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.
In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry / registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.
Internet Engineering Task Force.
The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).
The IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the IETF's work is done in Working Groups. It does not "run the Internet", despite what some people might mistakenly say. The IETF does make voluntary standards that are often adopted by Internet users, but it does not control, or even patrol, the Internet.
The IETF started in January 1986 as a quarterly meeting of U.S. government-funded researchers. Non-government representatives were invited starting with the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth IETF meeting in February 1987. The seventh IETF meeting in July 1987 was the first meeting with more than 100 attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, The Netherlands, in July 1993. Today the IETF meets three times a year and attendnce is often about 1,300 people, but has been as high as 2,000 upon occasion. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is roughly 50%, even at meetings held in the United States.
The IETF is unusual in that it exists as a collection of happenings, but is not a corporation and has no board of directors, no members, and no dues. The closest thing there is to being an IETF member is being on the IETF or a Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer term research issues.
Request for Comments.
Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, "Host Software", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.
RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that "obsoletes" the original.
The Internet Society.
The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 "to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". With offices near Washington, DC, USA, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form "chapters" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.
ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision making.
Globalization and Internet governance in the 21st century.
Since the 1990s, the Internet's governance and organization has been of global importance to governments, commerce, civil society, and individuals. The organizations which held control of certain technical aspects of the Internet were the successors of the old ARPANET oversight and the current decision-makers in the day-to-day technical aspects of the network. While recognized as the administrators of certain aspects of the Internet, their roles and their decision making authority are limited and subject to increasing international scrutiny and increasing objections. These objections have led to the ICANN removing themselves from relationships with first the University of Southern California in 2000, and finally in September 2009, gaining autonomy from the US government by the ending of its longstanding agreements, although some contractual obligations with the U.S. Department of Commerce continued.
The IETF, with financial and organizational support from the Internet Society, continues to serve as the Internet's ad-hoc standards body and issues Request for Comments.
In November 2005, the World Summit on the Information Society, held in Tunis, called for an Internet Governance Forum (IGF) to be convened by United Nations Secretary General. The IGF opened an ongoing, non-binding conversation among stakeholders representing governments, the private sector, civil society, and the technical and academic communities about the future of Internet governance. The first IGF meeting was held in October/November 2006 with follow up meetings annually thereafter. Since WSIS, the term "Internet governance" has been broadened beyond narrow technical concerns to include a wider range of Internet-related policy issues.
Net neutrality.
On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.
On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."
On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
Use and culture.
Email and Usenet.
Email has often been called the killer application of the Internet. It predates the Internet and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.
The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.
A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.
In addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the mailing list).
During the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.
From Gopher to the WWW.
As the Internet grew through the 1980s and early 1990s, many people realized the increasing need to be able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the FTP Archive list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.
One of the most promising user interface paradigms during this period was hypertext. The technology had been inspired by Vannevar Bush's "Memex" and developed through Ted Nelson's research on Project Xanadu and Douglas Engelbart's research on NLS. Many small self-contained hypertext systems had been created before, such as Apple Computer's HyperCard (1987). Gopher became the first commonly used hypertext interface to the Internet. While Gopher menu items were examples of hypertext, they were not commonly perceived in that way.
In 1989, while working at CERN, Tim Berners-Lee invented a network-based implementation of the hypertext concept. By releasing his invention to public use, he ensured the technology would become widespread. For his work in developing the World Wide Web, Berners-Lee received the Millennium technology prize in 2004. One early popular web browser, modeled after HyperCard, was ViolaWWW.
A turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the "High-Performance Computing and Communications Initiative", a funding program initiated by the "High Performance Computing and Communication Act of 1991" also known as the "Gore Bill". Mosaic's graphical interface soon became more popular than Gopher, which at the time was primarily text-based, and the WWW became the preferred interface for accessing the Internet. (Gore's reference to his role in "creating the Internet", however, was ridiculed in his presidential election campaign. See the full article Al Gore and information technology).
Mosaic was superseded in 1994 by Andreessen's Netscape Navigator, which replaced Mosaic as the world's most popular browser. While it held this title for some time, eventually competition from Internet Explorer and a variety of other browsers almost completely displaced it. Another important event held on January 11, 1994, was "The Superhighway Summit" at UCLA's Royce Hall. This was the "first public conference bringing together all of the major industry, government and academic leaders in the field [and] also began the national dialogue about the "Information Superhighway" and its implications."
"24 Hours in Cyberspace", "the largest one-day online event" (February 8, 1996) up to that date, took place on the then-active website, "cyber24.com." It was headed by photographer Rick Smolan. A photographic exhibition was unveiled at the Smithsonian Institution's National Museum of American History on January 23, 1997, featuring 70 photos from the project.
Search engines.
Even before the World Wide Web, there were search engines that attempted to organize the Internet. The first of these was the Archie search engine from McGill University in 1990, followed in 1991 by WAIS and Gopher. All three of those systems predated the invention of the World Wide Web but all continued to index the Web and the rest of the Internet for several years after the Web appeared. There are still Gopher servers as of 2006, although there are a great many more web servers.
As the Web grew, search engines and Web directories were created to track pages on the Web and allow people to find things. The first full-text Web search engine was WebCrawler in 1994. Before WebCrawler, only Web page titles were searched. Another early search engine, Lycos, was created in 1993 as a university project, and was the first to achieve commercial success. During the late 1990s, both Web directories and Web search engines were popular—Yahoo! (founded 1994) and Altavista (founded 1995) were the respective industry leaders. By August 2001, the directory model had begun to give way to search engines, tracking the rise of Google (founded 1998), which had developed new approaches to relevancy ranking. Directory features, while still commonly available, became after-thoughts to search engines.
Database size, which had been a significant marketing feature through the early 2000s, was similarly displaced by emphasis on relevancy ranking, the methods by which search engines attempt to sort the best results first. Relevancy ranking first became a major issue circa 1996, when it became apparent that it was impractical to review full lists of results. Consequently, algorithms for relevancy ranking have continuously improved. Google's PageRank method for ordering the results has received the most press, but all major search engines continually refine their ranking methodologies with a view toward improving the ordering of results. As of 2006, search engine rankings are more important than ever, so much so that an industry has developed ("search engine optimizers", or "SEO") to help web-developers improve their search ranking, and an entire body of case law has developed around matters that affect search engine rankings, such as use of trademarks in metatags. The sale of search rankings by some search engines has also created controversy among librarians and consumer advocates.
On June 3, 2009, Microsoft launched its new search engine, Bing. The following month Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search.
File sharing.
Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.
In 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.
All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazza in 2006, and Limewire in 2010 to shutdown or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.
Dot-com bubble.
Suddenly the low price of reaching millions worldwide, and the possibility of selling to or hearing from those people at the same moment when they were reached, promised to overturn established business dogma in advertising, mail-order sales, customer relationship management, and many more areas. The web was a new killer app—it could bring together unrelated buyers and sellers in seamless and low-cost ways. Entrepreneurs around the world developed new business models, and ran to their nearest venture capitalist. While some of the new entrepreneurs had experience in business and economics, the majority were simply people with ideas, and did not manage the capital influx prudently. Additionally, many dot-com business plans were predicated on the assumption that by using the Internet, they would bypass the distribution channels of existing businesses and therefore not have to compete with them; when the established businesses with strong existing brands developed their own Internet presence, these hopes were shattered, and the newcomers were left attempting to break into markets dominated by larger, more established businesses. Many did not have the ability to do so.
The dot-com bubble burst in March 2000, with the technology heavy NASDAQ Composite index peaking at 5,048.62 on March 10 (5,132.52 intraday), more than double its value just a year before. By 2001, the bubble's deflation was running full speed. A majority of the dot-coms had ceased trading, after having burnt through their venture capital and IPO capital, often without ever making a profit. But despite this, the Internet continues to grow, driven by commerce, ever greater amounts of online information and knowledge and social networking.
Mobile phones and the Internet.
The first mobile phone with Internet connectivity was the Nokia 9000 Communicator, launched in Finland in 1996. The viability of Internet services access on mobile phones was limited until prices came down from that model and network providers started to develop systems and services conveniently accessible on phones. NTT DoCoMo in Japan launched the first mobile Internet service, i-mode, in 1999 and this is considered the birth of the mobile phone Internet services. In 2001, the mobile phone email system by Research in Motion for their BlackBerry product was launched in America. To make efficient use of the small screen and tiny keypad and one-handed operation typical of mobile phones, a specific document and networking model was created for mobile devices, the Wireless Application Protocol (WAP). Most mobile device Internet services operate using WAP. The growth of mobile phone services was initially a primarily Asian phenomenon with Japan, South Korea and Taiwan all soon finding the majority of their Internet users accessing resources by phone rather than by PC. Developing countries followed, with India, South Africa, Kenya, Philippines, and Pakistan all reporting that the majority of their domestic users accessed the Internet from a mobile phone rather than a PC. The European and North American use of the Internet was influenced by a large installed base of personal computers, and the growth of mobile phone Internet access was more gradual, but had reached national penetration levels of 20–30% in most Western countries. The cross-over occurred in 2008, when more Internet access devices were mobile phones than personal computers. In many parts of the developing world, the ratio is as much as 10 mobile phone users to one PC user.
Historiography.
Some concerns have been raised over the historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. Specifically that it is hard to find documentation of much of the Internet's development, for several reasons, including a lack of centralized documentation for much of the early developments that led to the Internet.
"The Arpanet period is somewhat well documented because the corporation in charge – BBN – left a physical record. Moving into the NSFNET era, it became an extraordinarily decentralized process. The record exists in people's basements, in closets. [...] So much of what happened was done verbally and on the basis of individual trust."—Doug Gale (2007)

</doc>
<doc id="13693" url="http://en.wikipedia.org/wiki?curid=13693" title="Horace">
Horace

Quintus Horatius Flaccus (December 8, 65 BC – November 27, 8 BC), known in the English-speaking world as Horace ( or ), was the leading Roman lyric poet during the time of Augustus (also known as Octavian). The rhetorician Quintillian regarded his "Odes" as just about the only Latin lyrics worth reading: "He can be lofty sometimes, yet he is also full of charm and grace, versatile in his figures, and felicitously daring in his choice of words."
Horace also crafted elegant hexameter verses ("Sermones" and "Epistles") and caustic iambic poetry ("Epodes"). The hexameters are amusing yet serious works, friendly in tone, leading the ancient satirist Persius to comment: "as his friend laughs, Horace slyly puts his finger on his every fault; once let in, he plays about the heartstrings".
His career coincided with Rome's momentous change from Republic to Empire. An officer in the republican army defeated at the Battle of Philippi in 42 BC, he was befriended by Octavian's right-hand man in civil affairs, Maecenas, and became a spokesman for the new regime. For some commentators, his association with the regime was a delicate balance in which he maintained a strong measure of independence (he was "a master of the graceful sidestep") but for others he was, in John Dryden's phrase, "a well-mannered court slave".
Life.
 - In his writings, he "tells us far more about himself, his character, his development, and his way of life than any other great poet in antiquity. Some of the biographical writings contained in his writings can be supplemented from the short but valuable "Life of Horace" by Suetonius (in his "Lives of the Poets").
Childhood.
He was born on 8 December 65 BC in the Samnite south of Italy. His home town, Venusia, lay on a trade route in the border region between Apulia and Lucania (Basilicata). Various Italic dialects were spoken in the area and this perhaps enriched his feeling for language. He could have been familiar with Greek words even as a young boy and later he poked fun at the jargon of mixed Greek and Oscan spoken in neighbouring Canusium. Literary Latin must have sounded to him like a semi-foreign language, heard only at school. One of the works he probably studied in school was the "Odyssia" of Livius Andronicus, crammed into Italian boys with threats and floggings by teachers like the 'Orbilius' mentioned in one of his poems. School was made particularly irksome by a number of his fellow pupils, the overgrown sons of beefy centurions. The army veterans could have been settled there at the expense of local families uprooted by Rome as punishment for their part in the Social War (91–88 BC). Such state-sponsored migration must have added still more linguistic variety to the area. According to a local tradition reported by Horace, a colony of Romans or Latins had been installed in Venusia after the Samnites had been driven out early in the third century. In that case, young Horace could have felt himself to be a Roman though there are also indications that he regarded himself as a Samnite or Sabellus by birth. Italians in modern and ancient times have always been devoted to their home towns, even after success in the wider world, and Horace was no different. Images of his childhood setting and references to it are found throughout his poems.
Horace's father was probably a Venusian taken captive by Romans in the Social War, or possibly he was descended from a Sabine captured in the Samnite Wars. Either way, he was a slave for at least part of his life. He was evidently a man of strong abilities however and managed to gain his freedom and improve his social position. Thus Horace claimed to be the free-born son of a prosperous 'coactor'. The term 'coactor' could denote various roles, such as tax collector, but its use by Horace was explained by scholia as a reference to 'coactor argentareus' i.e. an auctioneer with some of the functions of a banker, paying the seller out of his own funds and later recovering the sum with interest from the buyer.
The father spent a small fortune on his son's education, eventually accompanying him to Rome to oversee his schooling and moral development. The poet later paid tribute to him in a poem that one modern scholar considers the best memorial by any son to his father. The poem includes this passage:
If my character is flawed by a few minor faults, but is otherwise decent and moral, if you can point out only a few scattered blemishes on an otherwise immaculate surface, if no one can accuse me of greed, or of prurience, or of profligacy, if I live a virtuous life, free of defilement (pardon, for a moment, my self-praise), and if I am to my friends a good friend, my father deserves all the credit... As it is now, he deserves from me unstinting gratitude and praise. I could never be ashamed of such a father, nor do I feel any need, as many people do, to apologize for being a freedman's son. "Satires 1.6.65–92"
He never mentioned his mother in his verses and he might not have known much about her. Perhaps she also had been a slave.
Adulthood.
Horace left Rome, possibly after his father's death, and continued his formal education in Athens, a great centre of learning in the ancient world, where he arrived at nineteen years of age, enrolling in The Academy. Founded by Plato, The Academy was now dominated by Epicureans and Stoics, whose theories and practises made a deep impression on the young man from Venusia. Meanwhile he mixed and lounged about with the elite of Roman youth, such as Marcus, the idle son of Cicero, and the Pompeius to whom he later addressed a poem. It was in Athens too that he probably acquired deep familiarity with the ancient tradition of Greek lyric poetry, at that time largely the preserve of grammarians and academic specialists (access to such material was easier in Athens than in Rome, where the public libraries had yet to be built by Asinius Pollio and Augustus).
Rome's troubles following the assassination of Julius Caesar were soon to catch up with him. Marcus Junius Brutus came to Athens seeking support for a republican cause. Brutus was fêted around town in grand receptions and he made a point of attending academic lectures, all the while recruiting supporters among the impressionable young men studying there, including Horace. An educated young Roman could begin military service high in the ranks and Horace was made tribunus militum (one of six senior officers of a typical legion), a post usually reserved for men of senatorial or equestrian rank and which seems to have inspired jealousy among his well-born confederates. He learned the basics of military life while on the march, particularly in the wilds of northern Greece, whose rugged scenery became a backdrop to some of his later poems. It was there in 42 BC that Octavian (later Augustus) and his associate Mark Antony crushed the republican forces at the Battle of Philippi. Horace later recorded it as a day of embarrassment for himself, when he fled without his shield, but allowance should be made for his self-deprecating humour. Moreover, the incident allowed him to identify himself with some famous poets who had long ago abandoned their shields in battle, notably his heroes Alcaeus and Archilochus. The comparison with the latter poet is uncanny: Archilochus lost his shield in a part of Thrace near Philippi, and he was deeply involved in the Greek colonization of Thasos, where Horace's die-hard comrades finally surrendered.
Octavian offered an early amnesty to his opponents and Horace quickly accepted it. On returning to Italy, he was confronted with yet another loss: his father's estate in Venusia was one of many throughout Italy to be confiscated for the settlement of veterans (Virgil lost his estate in the north about the same time). Horace later claimed that he was reduced to poverty and this led him to try his hand at poetry. In reality, there was no money to be had from versifying. At best, it offered future prospects through contacts with other poets and their patrons among the rich. Meanwhile he obtained the sinecure of "scriba quaestorius", a civil service position at the "aerarium" or Treasury, profitable enough to be purchased even by members of the "ordo equester" and not very demanding in its work-load, since tasks could be delegated to "scribae" or permanent clerks. It was about this time that he began writing his "Satires" and "Epodes".
Poet.
The "Epodes" belong to the iambic genre of 'blame poetry', written to shame fellow citizens into a sense of their social obligations. Horace modelled these poems on the work of Archilochus. Social bonds in Rome had been decaying since the destruction of Carthage a little more than a hundred years earlier, due to the vast wealth that could be gained by plunder and corruption, and the troubles were magnified by rivalry between Julius Caesar, Mark Antony and confederates like Sextus Pompey, all jockeying for a bigger share of the spoils. One modern scholar has counted a dozen civil wars in the hundred years leading up to 31 BC, including the Spartacus rebellion, eight years before Horace's birth. As the heirs to Hellenistic culture, Horace and his fellow Romans were not well prepared to deal with these problems:
 "At bottom, all the problems that the times were stirring up were of a social nature, which the Hellenistic thinkers were ill qualified to grapple with. Some of them censured oppression of the poor by the rich, but they gave no practical lead, though they may have hoped to see well-meaning rulers doing so. Philosophy was drifting into absorption in self, a quest for private contentedness, to be achieved by self-control and restraint, without much regard for the fate of a disintegrating community."—V.G. Kiernan
Horace's Hellenistic background is clear in his Satires, even though the genre was unique to Latin literature. He brought to it a style and outlook suited to the social and ethical issues confronting Rome but he changed its role from public, social engagement to private meditation. Meanwhile, he was beginning to interest Octavian's supporters, a gradual process described by him in one of his satires. The way was opened for him by his friend, the poet Virgil, who had gained admission into the privileged circle around Maecenas, Octavian's lieutenant, following the success of his "Eclogues". An introduction soon followed and, after a discreet interval, Horace too was accepted. He depicted the process as an honourable one, based on merit and mutual respect, eventually leading to true friendship, and there is reason to believe that his relationship was genuinely friendly, not just with Maecenas but afterwards with Augustus as well. On the other hand, the poet has been unsympathetically described by one scholar as "a sharp and rising young man, with an eye to the main chance." There were advantages on both sides: Horace gained encouragement and material support, the politicians gained a hold on a potential dissident. His republican sympathies, and his role at Philippi, may have caused him some pangs of remorse over his new status. However most Romans considered the civil wars to be the result of "contentio dignitatis", or rivalry between the foremost families of the city, and he too seems to have accepted the principate as Rome's last hope for much needed peace.
In 37 BC, Horace accompanied Maecenas on a journey to Brundisium, described in one of his poems as a series of amusing incidents and charming encounters with other friends along the way, such as Virgil. In fact the journey was political in its motivation, with Maecenas en route to negotiatie the Treaty of Tarentum with Antony, a fact Horace artfully keeps from the reader (political issues are largely avoided in the first book of satires). Horace was probably also with Maecenas on one of Octavian's naval expeditions against the piratical Sextus Pompeius, which ended in a disastrous storm off Palinurus in 36 BC, briefly alluded to by Horace in terms of near-drowning. There are also some indications in his verses that he was with Maecenas at the Battle of Actium in 31 BC, where Octavian defeated his great rival, Antony. By then Horace had already received from Maecenas the famous gift of his Sabine farm, probably not long after the publication of the first book of "Satires". The gift, which included income from five tenants, may have ended his career at the Treasury, or at least allowed him to give it less time and energy. It signalled his identification with the Octavian regime yet, in the second book of "Satires" that soon followed, he continued the apolitical stance of the first book. By this time, he had attained the status of "eques Romanus", perhaps as a result of his work at the Treasury.
Knight.
"Odes" 1–3 were the next focus for his artistic creativity. He adapted their forms and themes from Greek lyric poetry of the seventh and sixth centuries BC. The fragmented nature of the Greek world had enabled his literary heroes to express themselves freely and his semi-retirement from the Treasury in Rome to his own estate in the Sabine hills perhaps empowered him to some extent also yet even when his lyrics touched on public affairs they reinforced the importance of private life. Nevertheless his work in the period 30–27 BC began to show his closeness to the regime and his sensitivity to its developing ideology. In "Odes" 1.2, for example, he eulogized Octavian in hyperboles that echo Hellenistic court poetry. The name "Augustus", which Octavian assumed in January 27 BC, is first attested in "Odes" 3.3 and 3.5. In the period 27–24 BC, political allusions in the "Odes" concentrated on foreign wars in Britain (1.35), Arabia (1.29) Spain (3.8) and Parthia (2.2). He greeted Augustus on his return to Rome in 24 BC as a beloved ruler upon whose good health he depended for his own happiness (3.14).
The public reception of "Odes" 1–3 disappointed him however. He attributed the lack of success to jealousy among imperial courtiers and to his isolation from literary cliques. Perhaps it was disappointment that led him to put aside the genre in favour of verse letters. He addressed his first book of "Epistles" to a variety of friends and acquaintances in an urbane style reflecting his new social status as a knight. In the opening poem, he professed a deeper interest in moral philosophy than poetry but, though the collection demonstrates a leaning towards stoic theory, it reveals no sustained thinking about ethics. Maecenas was still the dominant confidante but Horace had now begun to assert his own independence, suavely declining constant invitations to attend his patron. In the final poem of the first book of "Epistles", he revealed himself to be forty-four years old in the consulship of Lollius and Lepidus i.e. 21 BC, and "of small stature, fond of the sun, prematurely grey, quick-tempered but easily placated".
According to Suetonius, the second book of "Epistles" was prompted by Augustus, who desired a verse epistle to be addressed to himself. Augustus was in fact a prolific letter-writer and he once asked Horace to be his personal secretary. Horace refused the secretarial role but complied with the emperor's request for a verse letter. The letter to Augustus may have been slow in coming, being published possibly as late as 11 BC. It celebrated, among other things, the 15 BC military victories of his stepsons, Drusus and Tiberius, yet it and the following letter were largely devoted to literary theory and criticism. The literary theme was explored still further in "Ars Poetica", published separately but written in the form of an epistle and sometimes referred to as "Epistles" 2.3 (possibly the last poem he ever wrote). He was also commissioned to write odes commemorating the victories of Drusus and Tiberius and one to be sung in a temple of Apollo for the Secular Games, a long abandoned festival that Augustus revived in accordance with his policy of recreating ancient customs ("Carmen Saeculare").
Suetonius recorded some gossip about Horace's sexual activities late in life, involving mirrors. The poet died at 56 years of age, not long after his friend Maecenas, near whose tomb he was laid to rest. Both men bequeathed their property to Augustus, an honour that the emperor expected of his friends.
Works.
The dating of Horace's works isn't known precisely and scholars often debate the exact order in which they were first 'published'. There are good arguments for the following chronology:
Historical context.
Horace composed in traditional metres borrowed from Archaic Greece, employing hexameters in his "Satires" and "Epistles", and iambs in his "Epodes", all of which were relatively easy to adapt into Latin forms. His "Odes" featured more complex measures, including alcaics and sapphics, which were sometimes a difficult fit for Latin structure and syntax. Despite these traditional metres, he presented himself as a partisan in the development of a new and sophisticated style. He was influenced in particular by Hellenistic aesthetics of brevity, elegance and polish, as modeled in the work of Callimachus. 
 "As soon as Horace, stirred by his own genius and encouraged by the example of Virgil, Varius, and perhaps some other poets of the same generation, had determined to make his fame as a poet, being by temperament a fighter, he wanted to fight against all kinds of prejudice, amateurish slovenliness, philistinism, reactionary tendencies, in short to fight for the new and noble type of poetry which he and his friends were endeavouring to bring about."—Eduard Fraenkel
In modern literary theory, a distinction is often made between immediate personal experience ("Urerlebnis") and experience mediated by cultural vectors such as literature, philosophy and the visual arts ("Bildungserlebnis"). The distinction has little relevance for Horace however since his personal and literary experiences are implicated in each other. "Satires" 1.5, for example, recounts in detail a real trip Horace made with Virgil and some of his other literary friends, and which parallels a Satire by Lucilius, his predecessor. Unlike much Hellenistic-inspired literature, however, his poetry was not composed for a small coterie of admirers and fellow poets, nor does it rely on abstruse allusions for many of its effects. Though elitist in its literary standards, it was written for a wide audience, as a public form of art. Ambivalence also characterizes his literary persona, since his presentation of himself as part of a small community of philosophically aware people, seeking true peace of mind while shunning vices like greed, was well adapted to Augustus's plans to reform public morality, corrupted by greed – is personal plea for moderation was part of the emperor's grand message to the nation.
Horace generally followed the examples of poets established as classics in different genres, such as Archilochus in "Epodes", Lucilius in "Satires" and Alcaeus in the "Odes", later broadening his scope for the sake of variation and because his models weren't actually suited to the realities confronting him. Archilochus and Alcaeus were aristocratic Greeks whose poetry had a social and religious function that was immediately intelligible to their audiences but which became a mere artifice or literary motif when transposed to Rome. However, the artifice of the "Odes" is also integral to their success, since they could now accommodate a wide range of emotional effects, and the blend of Greek and Roman elements adds a sense of detachment and universality. Horace proudly claimed to introduce into Latin the spirit and iambic poetry of Archilochus but (unlike Archilochus) without persecuting anyone ("Epistles" 1.19.23–5). It was no idle boast. His "Epodes" were modeled on the verses of the Greek poet, as 'blame poetry', yet he avoided targeting real scapegoats. Whereas Archilochus presented himself as a serious and vigorous opponent of wrong-doers, Horace aimed for comic effects and adopted the persona of a weak and ineffectual critic of his times (as symbolized for example in his surrender to the witch Canidia in the final epode). He also claimed to be the first to introduce into Latin the lyrical methods of Alcaeus ("Epistles" 1.19.32–3) and he actually was the first Latin poet to make consistent use of Alcaic meters and themes: love, politics and the symposium. He imitated other Greek lyric poets as well, employing a 'motto' technique, beginning each ode with some reference to a Greek original and then diverging from it.
The satirical poet Lucilius was a senator's son who could castigate his peers with impunity. Horace was a mere freedman's son who had to tread carefully. Lucilius was a rugged patriot and a significant voice in Roman self-awareness, endearing himself to his countrymen by his blunt frankness and explicit politics. His work expressed genuine freedom or libertas. His style included 'metrical vandalism' and looseness of structure. Horace instead adopted an oblique and ironic style of satire, ridiculing stock characters and anonymous targets. His libertas was the private freedom of a philosophical outlook, not a political or social privilege. His "Satires" are relatively easy-going in their use of meter (relative to the tight lyric meters of the "Odes") but formal and highly controlled relative to the poems of Lucilius, whom Horace mocked for his sloppy standards ("Satires" 1.10.56–61)
The "Epistles" may be considered among Horace's most innovative works. There was nothing like it in Greek or Roman literature. Occasionally poems had had some resemblance to letters, including an elegiac poem from Solon to Mimnermus and some lyrical poems from Pindar to Hieron of Syracuse. Lucilius had composed a satire in the form of a letter, and some epistolary poems were composed by Catullus and Propertius. But nobody before Horace had ever composed an entire collection of verse letters, let alone letters with a focus on philosophical problems. The sophisticated and flexible style that he had developed in his "Satires" was adapted to the more serious needs of this new genre. Such refinement of style was not unusual for Horace. His craftsmanship as a wordsmith is apparent even in his earliest attempts at this or that kind of poetry, but his handling of each genre tended to improve over time as he adapted it to his own needs. Thus for example it is generally agreed that his second book of "Satires", where human folly is revealed through dialogue between characters, is superior to the first, where he propounds his ethics in monologues. Nevertheless, the first book includes some of his most popular poems.
Themes.
Horace developed a number of inter-related themes throughout his poetic career, including politics, love, philosophy and ethics, his own social role, as well as poetry itself. His "Epodes" and "Satires" are forms of 'blame poetry' and both have a natural affinity with the moralising and diatribes of Cynicism. This often takes the form of allusions to the work and philosophy of Bion of Borysthenes but it is as much a literary game as a philosophical alignment. By the time he composed his "Epistles", he was a critic of Cynicism along with all impractical and "high-falutin" philosophy in general. The "Satires" also include a strong element of Epicureanism, with frequent allusions to the Epicurean poet Lucretius. So for example the Epicurean sentiment "carpe diem" is the inspiration behind Horace's repeated punning on his own name ("Horatius ~ hora") in "Satires" 2.6. The "Satires" also feature some Stoic, Peripatetic and Platonic ("Dialogues") elements. In short, the "Satires" present a medley of philosophical programs, dished up in no particular order – a style of argument typical of the genre. The "Odes" display a wide range of topics. Over time, he becomes more confident about his political voice. Although he is often thought of as an overly intellectual lover, he is ingenuous in representing passion. The "Odes" weave various philosophical strands together, with allusions and statements of doctrine present in about a third of the "Odes" Books 1-3, ranging from the flippant (1.22, 3.28) to the solemn (2.10, 3.2, 3.3). Epicureanism is the dominant influence, characterizing about twice as many of these odes as Stoicism. A group of odes combines these two influences in tense relationships, such as "Odes" 1.7, praising Stoic virility and devotion to public duty while also advocating private pleasures among friends. While generally favouring the Epicurean lifestyle, the lyric poet is as eclectic as the satiric poet, and in "Odes" 2.10 even proposes Aristotle's golden mean as a remedy for Rome's political troubles. Many of Horace's poems also contain much reflection on genre, the lyric tradition, and the function of poetry. "Odes" 4, thought to be composed at the emperor's request, takes the themes of the first three books of "Odes" to a new level. This book shows greater poetic confidence after the public performance of his "Carmen saeculare" or "Century hymn" at a public festival orchestrated by Augustus. In it, Horace addresses the emperor Augustus directly with more confidence and proclaims his power to grant poetic immortality to those he praises. It is the least philosophical collection of his verses, excepting the twelfth ode, addressed to the dead Virgil as if he were living. In that ode, the epic poet and the lyric poet are aligned with Stoicism and Epicureanism respectively, in a mood of bitter-sweet pathos. The first poem of the "Epistles" sets the philosophical tone for the rest of the collection: "So now I put aside both verses and all those other games: What is true and what befits is my care, this my question, this my whole concern." His poetic renunciation of poetry in favour of philosophy is intended to be ambiguous. Ambiguity is the hallmark of the "Epistles". It is uncertain if those being addressed by the self-mocking poet-philosopher are being honoured or criticized. Though he emerges as an Epicurean, it is on the understanding that philosophical preferences, like political and social choices, are a matter of personal taste. Thus he depicts the ups and downs of the philosophical life more realistically than do most philosophers.
Reception.
The reception of Horace's work has varied from one epoch to another and varied markedly even in his own lifetime. "Odes" 1–3 were not well received when first 'published' in Rome, yet Augustus later commissioned a ceremonial ode for the Centennial Games in 17 BC and also encouraged the publication of "Odes" 4, after which Horace's reputation as Rome's premier lyricist was assured. His Odes were to become the best received of all his poems in ancient times, acquiring a classic status that discouraged imitation: no other poet produced a comparable body of lyrics in the four centuries that followed (though that might also be attributed to social causes, particularly the parasitism that Italy was sinking into). In the seventeenth and eighteenth centuries, ode-writing became highly fashionable in England and a large number of aspiring poets imitated Horace both in English and in Latin.
In a verse epistle to Augustus (Epistle 2.1), in 12 BC, Horace argued for classic status to be awarded to contemporary poets, including Virgil and apparently himself. In the final poem of his third book of Odes he claimed to have created for himself a monument more durable than bronze ("Exegi monumentum aere perennius", "Carmina" 3.30.1). For one modern scholar, however, Horace's personal qualities are more notable than the monumental quality of his achievement:
"...when we hear his name we don't really think of a monument. We think rather of a voice which varies in tone and resonance but is always recognizable, and which by its unsentimental humanity evokes a very special blend of liking and respect.—Niall Rudd
Yet for men like Wilfred Owen, scarred by experiences of World War I, his poetry stood for discredited values:
My friend, you would not tell with such high zest<br>To children ardent for some desperate glory,<br>The Old Lie: Dulce et decorum est<br>Pro patria mori.
The same motto, Dulce et decorum est pro patria mori, had been adapted to the ethos of martyrdom in the lyrics of early Christian poets like Prudentius.
These preliminary comments touch on a small sample of developments in the reception of Horace's work. More developments are covered epoch by epoch in the following sections.
Antiquity.
Horace's influence can be observed in the work of his near contemporaries, Ovid and Propertius. Ovid followed his example in creating a completely natural style of expression in hexameter verse, and Propertius cheekily mimicked him in his third book of elegies. His "Epistles" provided them both with a model for their own verse letters and it also shaped Ovid's exile poetry.
His influence had a perverse aspect. As mentioned before, the brilliance of his "Odes" may have discouraged imitation. Conversely, they may have created a vogue for the lyrics of the archaic Greek poet Pindar, due to the fact that Horace had neglected that style of lyric (see Pindar#Influence and legacy). The iambic genre seems almost to have disappeared after publication of Horace's "Epodes". Ovid's "Ibis" was a rare attempt at the form but it was inspired mainly by Callimachus, and there are some iambic elements in Martial but the main influence there was Catullus. A revival of popular interest in the satires of Lucillius may have been inspired by Horace's criticism of his unpolished style. Both Horace and Lucilius were considered good role-models by Persius, who critiqued his own satires as lacking both the acerbity of Lucillius and the gentler touch of Horace. Juvenal's caustic satire was influenced mainly by Lucilius but Horace by then was a school classic and Juvenal could refer to him respectfully and in a round-about way as "the Venusine lamp".
Statius paid homage to Horace by composing one poem in Sapphic and one in Alcaic meter (the verse forms most often associated with "Odes"), which he included in his collection of occasional poems, "Silvae". Ancient scholars wrote commentaries on the lyric meters of the "Odes", including the scholarly poet Caesius Bassus. By a process called "derivatio", he varied established meters through the addition or omission of syllables, a technique borrowed by Seneca the Younger when adapting Horatian meters to the stage.
Horace's poems continued to be school texts into late antiquity. Works attributed to Helenius Acro and Pomponius Porphyrio are the remnants of a much larger body of Horatian scholarship. Porphyrio arranged the poems in non-chronological order, beginning with the "Odes", because of their general popularity and their appeal to scholars (the "Odes" were to retain this privileged position in the medieval manuscript tradition and thus in modern editions also). Horace was often evoked by poets of the fourth century, such as Ausonius and Claudian. Prudentius presented himself as a Christian Horace, adapting Horatian meters to his own poetry and giving Horatian motifs a Christian tone. On the other hand, St Jerome, modelled an uncompromising response to the pagan Horace, observing: "What harmony can there be between Christ and the Devil? What has Horace to do with the Psalter?" By the early sixth century, Horace and Prudentius were both part of a classical heritage that was struggling to survive the disorder of the times. Boethius, the last major author of classical Latin literature, could still take inspiration from Horace, sometimes mediated by Senecan tragedy. It can be argued that Horace's influence extended beyond poetry to dignify core themes and values of the early Christian era, such as self-sufficiency, inner contentment and courage.
Middle Ages and Renaissance.
Classical texts almost ceased being copied in the period between the mid sixth century and the Middle Ages. Horace's work probably survived in just two or three books imported into northern Europe from Italy. These became the ancestors of six extant manuscripts dated to the ninth century. Two of those six manuscripts are French in origin, one was produced in Alsace, and the other three show Irish influence but were probably written in continental monasteries (Lombardy for example). By the last half of the ninth century, it was not uncommon for literate people to have direct experience of Horace's poetry. His influence on the Carolingian Renaissance can be found in the poems of Heiric of Auxerre and in some manuscripts marked with neumes, mysterious notations that may have been an aid to the memorization and discussion of his lyric meters. "Ode" is neumed with the melody of a hymn to John the Baptist, Ut queant laxis, composed in Sapphic stanzas. This hymn later became the basis of the solfege system ("Do, re, mi...")—an association with western music quite appropriate for a lyric poet like Horace, though the language of the hymn is mainly Prudentian. However, Lyons argues that the melody in question was linked with Horace's Ode well before Guido d'Arezzo fitted Ut queant laxis to it, and Ovid testifies to Horace's use of the lyre while performing his Odes.
The German scholar, Ludwig Traube, once dubbed the tenth and eleventh centuries "The age of Horace" ("aetas Horatiana"), and placed it between the "aetas Vergiliana" of the eighth and ninth centuries, and the "aetas Ovidiana" of the twelfth and thirteenth centuries, a distinction supposed to reflect the dominant classical Latin influences of those times. Such a distinction is over-schematized since Horace was a substantial influence in the ninth century as well. Traube had focused too much on Horace's "Satires". Almost all of Horace's work found favor in the Medieval period. In fact medieval scholars were also guilty of over-schematism, associating Horace's different genres with the different ages of man. A twelfth century scholar encapsulated the theory: "...Horace wrote four different kinds of poems on account of the four ages, the "Odes" for boys, the "Ars Poetica" for young men, the "Satires" for mature men, the "Epistles" for old and complete men." It was even thought that Horace had composed his works in the order in which they had been placed by ancient scholars. Despite its naivety, the schematism involved an appreciation of Horace's works as a collection, the "Ars Poetica", "Satires" and "Epistles" appearing to find favour as well as the "Odes". The later Middle Ages however gave special significance to "Satires" and "Epistles", being considered Horace's mature works. Dante referred to Horace as "Orazio satiro", and he awarded him a privileged position in the first circle of Hell, with Homer, Ovid and Lucan.
Horace's popularity is revealed in the large number of quotes from all his works found in almost every genre of medieval literature, and also in the number of poets imitating him in quantitative Latin meter . The most prolific imitator of his "Odes" was the Bavarian monk, Metellus of Tegernsee, who dedicated his work to the patron saint of Tegernsee Abbey, St Quirinus, around the year 1170. He imitated all Horace's lyrical meters then followed these up with imitations of other meters used by Prudentius and Boethius, indicating that variety, as first modelled by Horace, was considered a fundamental aspect of the lyric genre. The content of his poems however was restricted to simple piety. Among the most successful imitators of "Satires" and "Epistles" was another Germanic author, calling himself Sextus Amarcius, around 1100, who composed four books, the first two exemplifying vices, the second pair mainly virtues.
Petrarch is a key figure in the imitation of Horace in accentual meters. His verse letters in Latin were modelled on the "Epistles" and he wrote a letter to Horace in the form of an ode. However he also borrowed from Horace when composing his Italian sonnets. One modern scholar has speculated that authors who imitated Horace in accentual rhythms (including stressed Latin and vernacular languages) may have considered their work a natural sequel to Horace's metrical variety. In France, Horace and Pindar were the poetic models for a group of vernacular authors called the Pléiade, including for example Pierre de Ronsard and Joachim du Bellay. Montaigne made constant and inventive use of Horatian quotes. The vernacular languages were dominant in Spain and Portugal in the sixteenth century, where Horace's influence is notable in the works of such authors as Garcilaso de la Vega, Juan Boscán Sá de Miranda, Antonio Ferreira and Fray Luis de León, the latter for example writing odes on the Horatian theme "beatus ille" ("happy the man"). The sixteenth century in western Europe was also an age of translations (except in Germany, where Horace wasn't translated until well into the seventeenth century). The first English translator was Thomas Drant, who placed translations of Jeremiah and Horace side by side in "Medicinable Morall", 1566. That was also the year that the Scot George Buchanan paraphrased the Psalms in a Horatian setting. Ben Jonson put Horace on the stage in 1601 in "Poetaster", along with other classical Latin authors, giving them all their own verses to speak in translation. Horace's part evinces the independent spirit, moral earnestness and critical insight that many readers look for in his poems.
Age of Enlightenment.
During the seventeenth and eighteenth centuries, or the Age of Enlightenment, neo-classical culture was pervasive. English literature in the middle of that period has been dubbed Augustan. It is not always easy to distinguish Horace's influence during those centuries (the mixing of influences is shown for example in one poet's pseudonym, "Horace Juvenal"). However a measure of his influence can be found in the diversity of the people interested in his works, both among readers and authors.
New editions of his works were published almost yearly. There were three new editions in 1612 (two in Leiden, one in Frankfurt) and again in 1699 (Utrecht, Barcelona, Cambridge). Cheap editions were plentiful and fine editions were also produced, including one whose entire text was engraved by John Pine in copperplate. The poet James Thomson owned five editions of Horace's work and the physician James Douglas had five hundred books with Horace-related titles. Horace was often commended in periodicals such as The Spectator, as a hallmark of good judgement, moderation and manliness, a focus for moralising. His verses offered a fund of mottoes, such as "simplex munditiis", (elegance in simplicity) "splendide mendax" (nobly untruthful.), "sapere aude", "nunc est bibendum", "carpe diem" (the latter perhaps being the only one still in common use today), quoted even in works as prosaic as Edmund Quincy's "A treatise of hemp-husbandry" (1765). The fictional hero Tom Jones recited his verses with feeling. His works were also used to justify commonplace themes, such as patriotic obedience, as in James Parry's English lines from an Oxford University collection in 1736:
Horatian-style lyrics were increasingly typical of Oxford and Cambridge verse collections for this period, most of them in Latin but some like the previous ode in English. John Milton's Lycidas first appeared in such a collection. It has few Horatian echoes yet Milton's associations with Horace were lifelong. He composed a controversial version of "Odes" 1.5, and Paradise Lost includes references to Horace's 'Roman' "Odes" 3.1–6 (Book 7 for example begins with echoes of "Odes" 3.4). Yet Horace's lyrics could offer inspiration to libertines as well as moralists, and neo-Latin sometimes served as a kind of discrete veil for the risqué. Thus for example Benjamin Loveling authored a catalogue of Drury Lane and Covent Garden prostitutes, in Sapphic stanzas, and an encomium for a dying lady "of salacious memory". Some Latin imitations of Horace were politically subversive, such as a marriage ode by Anthony Alsop that included a rallying cry for the Jacobite cause. On the other hand, Andrew Marvell took inspiration from Horace's "Odes" 1.37 to compose his English masterpiece Horatian Ode upon Cromwell's Return from Ireland, in which subtly nuanced reflections on the execution of Charles I echo Horace's ambiguous response to the death of Cleopatra (Marvell's ode was suppressed in spite of its subtlety and only began to be widely published in 1776). Samuel Johnson took particular pleasure in reading "The Odes". Alexander Pope wrote direct "Imitations" of Horace (published with the original Latin alongside) and also echoed him in "Essays" and The Rape of the Lock. He even emerged as "a quite Horatian Homer" in his translation of the "Iliad". Horace appealed also to female poets, such as Anna Seward ("Original sonnets on various subjects, and odes paraphrased from Horace", 1799) and Elizabeth Tollet, who composed a Latin ode in Sapphic meter to celebrate her brother's return from overseas, with tea and coffee substituted for the wine of Horace's sympotic settings:
Horace's "Ars Poetica" is second only to Aristotle's "Poetics" in its influence on literary theory and criticism. Milton recommended both works in his treatise "of Education". Horace's "Satires" and "Epistles" however also had a huge impact, influencing theorists and critics such as John Dryden. There was considerable debate over the value of different lyrical forms for contemporary poets, as represented on one hand by the kind of four-line stanzas made familiar by Horace's Sapphic and Alcaic "Odes" and, on the other, the loosely structured Pindarics associated with the odes of Pindar. Translations occasionally involved scholars in the dilemmas of censorship. Thus Christopher Smart entirely omitted "Odes" and re-numbered the remaining odes. He also removed the ending of "Odes" . Thomas Creech printed "Epodes" and in the original Latin but left out their English translations. Philip Francis left out both the English and Latin for those same two epodes, a gap in the numbering the only indication that something was amiss. French editions of Horace were influential in England and these too were regularly bowdlerized.
Most European nations had their own 'Horaces': thus for example Friedrich von Hagedorn was called "The German Horace" and Maciej Kazimierz Sarbiewski "The Polish Horace" (the latter was much imitated by English poets such as Henry Vaughan and Abraham Cowley). Pope Urban VIII wrote voluminously in Horatian meters, including an ode on gout.
19th century, on.
Horace maintained a central role in the education of English-speaking elites right up until the 1960s. A pedantic emphasis on the formal aspects of language-learning at the expense of literary appreciation may have made him unpopular in some quarters yet it also confirmed his influence—a tension in his reception that underlies Byron's famous lines from "Childe Harold" (Canto iv, 77):
William Wordsworth's mature poetry, including the preface to Lyrical Ballads, reveals Horace's influence in its rejection of false ornament and he once expressed "a wish / to meet the shade of Horace...". John Keats echoed the opening of Horace's "Epodes" 14 in the opening lines of "Ode to a Nightingale".
The Roman poet was presented in the nineteenth century as an honorary English gentleman. William Thackery produced a version of "Odes" in which Horace's questionable 'boy' became 'Lucy', and Gerard Manley Hopkins translated the boy innocently as 'child'. Horace was translated by Sir Theodore Martin (biographer of Prince Albert) but minus some ungentlemanly verses, such as the erotic "Odes" and "Epodes" 8 and 12. Lord Lytton produced a popular translation and William Gladstone also wrote translations during his last days as Prime Minister.
Edward FitzGerald's "Rubaiyat of Omar Khayyam", though formally derived from the Persian "ruba'i", nevertheless shows a strong Horatian influence, since, as one modern scholar has observed,"...the quatrains inevitably recall the stanzas of the 'Odes', as does the narrating first person of the world-weary, ageing Epicurean Omar himself, mixing sympotic exhortation and 'carpe diem' with splendid moralising and 'memento mori' nihilism."" Matthew Arnold advised a friend in verse not to worry about politics, an echo of "Odes" , yet later became a critic of Horace's inadequacies relative to Greek poets, as role models of Victorian virtues, observing: "If human life were complete without faith, without enthusiasm, without energy, Horace...would be the perfect interpreter of human life." Christina Rossetti composed a sonnet depicting a woman willing her own death steadily, drawing on Horace's depiction of 'Glycera' in "Odes" and Cleopatra in "Odes" . A. E. Housman considered "Odes" , in Archilochian couplets, the most beautiful poem of antiquity and yet he generally shared Horace's penchant for quatrains, being readily adapted to his own elegiac and melancholy strain. The most famous poem of Ernest Dowson took its title and its heroine's name from a line of "Odes" , "Non sum qualis eram bonae sub regno Cynarae", as well as its motif of nostalgia for a former flame. Kipling wrote a famous parody of the "Odes", satirising their stylistic idiosyncrasies and especially the extraordinary syntax, but he also used Horace's Roman patriotism as a focus for British imperialism, as in the story "Regulus" in the school collection "Stalky & Co.", which he based on "Odes" . Wilfred Owen's famous poem, quoted above, incorporated Horatian text to question patriotism while ignoring the rules of Latin scansion. However there were few other echoes of Horace in the war period, possibly because war is not actually a major theme of Horace's work.
Both W.H.Auden and Louis MacNeice began their careers as teachers of classics and both responded as poets to Horace's influence. Auden for example evoked the fragile world of the 1930s in terms echoing "Odes" , where Horace advises a friend not to let worries about frontier wars interfere with current pleasures.
The American poet, Robert Frost, echoed Horace's "Satires" in the conversational and sententious idiom of some of his longer poems, such as "The Lesson for Today" (1941), and also in his gentle advocacy of life on the farm, as in "Hyla Brook" (1916), evoking Horace's "fons Bandusiae" in "Ode" . Now at the start of the third millennium, poets are still absorbing and re-configuring the Horatian influence, sometimes in translation (such as a 2002 English/American edition of the "Odes" by thirty-six poets) and sometimes as inspiration for their own work (such as a 2003 collection of odes by a New Zealand poet).
Horace's "Epodes" have largely been ignored in the modern era, excepting those with political associations of historical significance. The obscene qualities of some of the poems have repulsed even scholars yet more recently a better understanding of the nature of Iambic poetry has led to a re-evaluation of the "whole" collection. A re-appraisal of the "Epodes" also appears in creative adaptations by recent poets (such as a 2004 collection of poems that relocates the ancient context to a 1950s industrial town).

</doc>
<doc id="13694" url="http://en.wikipedia.org/wiki?curid=13694" title="History of Microsoft Windows">
History of Microsoft Windows

In 1983, Microsoft announced the development of Windows, a graphical user interface (GUI) for its own operating system (MS-DOS). The product line has changed from a GUI product to a modern operating system over two families of design, each with its own codebase and default file system.
The 3.x and 4.x family includes Windows 3.0, Windows 3.1x, Windows 95, Windows 98, and Windows ME. Windows for Workgroups 3.11 added 32-bit networking. Windows 95 added additional 32-bit capabilities (however, MS-DOS, some of the kernel, and supplementary utilities such as Disk Defragment remained 16-bit) and implemented a new object oriented user interface, elements of which are still used today.
The Windows NT family started with Windows NT 3.1 in 1993. Modern Windows operating system versions are based on the newer Windows NT kernel that was originally intended for OS/2. Windows runs on IA-32, x86-64, and on 32-bit ARM (ARMv7) processors. Earlier versions also ran on the i860, Alpha, MIPS, Fairchild Clipper, PowerPC, and Itanium architectures. Some work was done to port it to the SPARC architecture.
The familiar Windows Explorer desktop shell superseded Program Manager with the release of Windows 95, received major enhancements in 1997, and remained the default shell for all commercial Windows releases until Windows 8's Modern UI-derived Start screen debuted in 2012.
Windows 1.0x.
The first independent version of Microsoft Windows, version 1.0, released on 20 November 1985, achieved little popularity. It was originally going to be called "Interface Manager" but Rowland Hanson, the head of marketing at Microsoft, convinced the company that the name "Windows" would be more appealing to customers.
Windows 1.0 was not a complete operating system, but rather an "operating environment" that extended MS-DOS, and shared the latter's inherent flaws and problems.
The first version of Microsoft Windows included a simple graphics painting program called Windows Paint; Windows Write, a simple word processor; an appointment calendar; a card-filer; a notepad; a clock; a control panel; a computer terminal; Clipboard; and RAM driver. It also included the MS-DOS Executive and a game called Reversi.
Microsoft had worked with Apple Computer to develop applications for Apple's new Macintosh computer, which featured a graphical user interface. As part of the related business negotiations, Microsoft had licensed certain aspects of the Macintosh user interface from Apple; in later litigation, a district court summarized these aspects as "screen displays".
In the development of Windows 1.0, Microsoft intentionally limited its borrowing of certain GUI elements from the Macintosh user interface, to comply with its license.
For example, windows were only displayed "tiled" on the screen; that is, they could not overlap or overlie one another.
Windows 2.x.
Microsoft Windows version 2 came out on 9 December 1987, and proved slightly more popular than its predecessor.
Much of the popularity for Windows 2.0 came by way of its inclusion as a "run-time version" with Microsoft's new graphical applications, Excel and Word for Windows. They could be run from MS-DOS, executing Windows for the duration of their activity, and closing down Windows upon exit.
Microsoft Windows received a major boost around this time when Aldus PageMaker appeared in a Windows version, having previously run only on Macintosh. Some computer historians date this, the first appearance of a significant "and" non-Microsoft application for Windows, as the start of the success of Windows.
Versions 2.0x used the real-mode memory model, which confined it to a maximum of 1 megabyte of memory.
In such a configuration, it could run under another multitasker like DESQview, which used the 286 protected mode.
Later, two new versions were released: Windows/286 2.1 and Windows/386 2.1. Like prior versions of Windows, Windows/286 2.1 used the real-mode memory model, but was the first version to support the High Memory Area. Windows/386 2.1 had a protected mode kernel with LIM-standard EMS emulation. All Windows and DOS-based applications at the time were real mode, running over the protected mode kernel by using the virtual 8086 mode, which was new with the 80386 processor.
Version 2.03, and later 3.0, faced challenges from Apple over its overlapping windows and other features Apple charged mimicked the ostensibly copyrighted "look and feel" of its operating system and "embodie[d] and generated a copy of the Macintosh" in its OS. Judge William Schwarzer dropped all but 10 of Apple's 189 claims of copyright infringement, and ruled that most of the remaining 10 were over uncopyrightable ideas.
Success with Windows 3.0.
Microsoft Windows scored a significant success with Windows 3.0, released in May 1990. In addition to improved capabilities given to native applications, Windows also allowed users to better multitask older MS-DOS based software compared to Windows/386, thanks to the introduction of virtual memory.
Windows 3.0's user interface finally resembled a serious competitor to the user interface of the Macintosh computer. PCs had improved graphics by this time, due to VGA video cards, and the protected/enhanced mode allowed Windows applications to use more memory in a more painless manner than their DOS counterparts could. Windows 3.0 could run in real, standard, or 386 enhanced modes, and was compatible with any Intel processor from the 8086/8088 up to the 80286 and 80386. This was the first version to run Windows programs in protected mode, although the 386 enhanced mode kernel was an enhanced version of the protected mode kernel in Windows/386.
Windows 3.0 received two updates. A few months after introduction, Windows 3.0a was released as a maintenance release, resolving bugs and improving stability. A "multimedia" version, Windows 3.0 with Multimedia Extensions 1.0, was released in October 1991. This was bundled with "multimedia upgrade kits", comprising a CD-ROM drive and a sound card, such as the Creative Labs Sound Blaster Pro. This version was the precursor to the multimedia features available in Windows 3.1 and later, and was part of Microsoft's specification for the Multimedia PC.
The features listed above and growing market support from application software developers made Windows 3.0 wildly successful, selling around 10 million copies in the two years before the release of version 3.1. Windows 3.0 became a major source of income for Microsoft, and led the company to revise some of its earlier plans. Support was discontinued on 31 December 2001.
A step sideways: OS/2.
During the mid to late 1980s, Microsoft and IBM had cooperatively been developing OS/2 as a successor to DOS. OS/2 would take full advantage of the aforementioned protected mode of the Intel 80286 processor and up to 16 MB of memory. OS/2 1.0, released in 1987, supported swapping and multitasking and allowed running of DOS executables.
A GUI, called the Presentation Manager (PM), was not available with OS/2 until version 1.1, released in 1988. Its API was incompatible with Windows. Version 1.2, released in 1989, introduced a new file system, HPFS, to replace the FAT file system.
By the early 1990s, conflicts developed in the Microsoft/IBM relationship. They cooperated with each other in developing their PC operating systems, and had access to each other's code. Microsoft wanted to further develop Windows, while IBM desired for future work to be based on OS/2. In an attempt to resolve this tension, IBM and Microsoft agreed that IBM would develop OS/2 2.0, to replace OS/2 1.3 and Windows 3.0, while Microsoft would develop a new operating system, OS/2 3.0, to later succeed OS/2 2.0.
This agreement soon however fell apart, and the Microsoft/IBM relationship was terminated. IBM continued to develop OS/2, while Microsoft changed the name of its (as yet unreleased) OS/2 3.0 to Windows NT. Both retained the rights to use OS/2 and Windows technology developed up to the termination of the agreement; Windows NT, however, was to be written anew, mostly independently (see below).
After an interim 1.3 version to fix up many remaining problems with the 1.x series, IBM released OS/2 version 2.0 in 1992. This was a major improvement: it featured a new, object-oriented GUI, the Workplace Shell (WPS), that included a desktop and was considered by many to be OS/2's best feature. Microsoft would later imitate much of it in Windows 95. Version 2.0 also provided a full 32-bit API, offered smooth multitasking and could take advantage of the 4 gigabytes of address space provided by the Intel 80386. Still, much of the system had 16-bit code internally which required, among other things, device drivers to be 16-bit code also. This was one of the reasons for the chronic shortage of OS/2 drivers for the latest devices. Version 2.0 could also run DOS and Windows 3.0 programs, since IBM had retained the right to use the DOS and Windows code as a result of the breakup.
Windows 3.1x.
In response to the impending release of OS/2 2.0, Microsoft developed Windows 3.1, which included several improvements to Windows 3.0, such as display of TrueType scalable fonts (developed jointly with Apple), improved disk performance in 386 Enhanced Mode, multimedia support, and bugfixes. It also removed Real Mode, and only ran on an 80286 or better processor. Later Microsoft also released Windows 3.11, a touch-up to Windows 3.1 which included all of the patches and updates that followed the release of Windows 3.1 in 1992.
In 1992 and 1993, Microsoft released Windows for Workgroups (WfW), which was available both as an add-on for existing Windows 3.1 installations and in a version that included the base Windows environment and the networking extensions all in one package. Windows for Workgroups included improved network drivers and protocol stacks, and support for peer-to-peer networking. There were two versions of Windows for Workgroups, WfW 3.1 and WfW 3.11. Unlike prior versions, Windows for Workgroups 3.11 ran in 386 Enhanced Mode only, and needed at least an 80386SX processor. One optional download for WfW was the "Wolverine" TCP/IP protocol stack, which allowed for easy access to the Internet through corporate networks. 
All these versions continued version 3.0's impressive sales pace. Even though the 3.1x series still lacked most of the important features of OS/2, such as long file names, a desktop, or protection of the system against misbehaving applications, Microsoft quickly took over the OS and GUI markets for the IBM PC. The Windows API became the de facto standard for consumer software.
Windows NT 3.x.
Meanwhile, Microsoft continued to develop Windows NT. The main architect of the system was Dave Cutler, one of the chief architects of VMS at Digital Equipment Corporation (later acquired by Compaq, now part of Hewlett-Packard). Microsoft hired him in August 1988 to create a successor to OS/2, but Cutler created a completely new system instead. Cutler had been developing a follow-on to VMS at DEC called Mica, and when DEC dropped the project he brought the expertise and around 20 engineers with him to Microsoft. DEC also believed he brought Mica's code to Microsoft and sued. Microsoft eventually paid US$150 million and agreed to support DEC's Alpha CPU chip in NT.
Windows NT 3.1 (Microsoft marketing wanted Windows NT to appear to be a continuation of Windows 3.1) arrived in Beta form to developers at the July 1992 Professional Developers Conference in San Francisco. Microsoft announced at the conference its intentions to develop a successor to both Windows NT and Windows 3.1's replacement (Windows 95, codenamed Chicago), which would unify the two into one operating system. This successor was codenamed Cairo. In hindsight, Cairo was a much more difficult project than Microsoft had anticipated and, as a result, NT and Chicago would not be unified until Windows XP—albeit Windows 2000, oriented to business, had already unified most of the system’s bolts and gears, it was XP that was sold to home consumers like Windows 95 and came to be viewed as the final unified OS. Parts of Cairo have still not made it into Windows as of 2009 - specifically, the WinFS file system, which was the much touted Object File System of Cairo. Microsoft announced that they have discontinued the separate release of WinFS for Windows XP and Windows Vista and will gradually incorporate the technologies developed for WinFS in other products and technologies, notably Microsoft SQL Server.
Driver support was lacking due to the increased programming difficulty in dealing with NT's superior hardware abstraction model. This problem plagued the NT line all the way through Windows 2000. Programmers complained that it was too hard to write drivers for NT, and hardware developers were not going to go through the trouble of developing drivers for a small segment of the market. Additionally, although allowing for good performance and fuller exploitation of system resources, it was also resource-intensive on limited hardware, and thus was only suitable for larger, more expensive machines.
However, these same features made Windows NT perfect for the LAN server market (which in 1993 was experiencing a rapid boom, as office networking was becoming common). NT also had advanced network connectivity options and NTFS, an efficient file system. Windows NT version 3.51 was Microsoft's entry into this field, and took away market share from Novell (the dominant player) in the following years.
One of Microsoft's biggest advances initially developed for Windows NT was a new 32-bit API, to replace the legacy 16-bit Windows API. This API was called Win32, and from then on Microsoft referred to the older 16-bit API as Win16. The Win32 API had three main implementations: one for Windows NT, one for Win32s (which was a subset of Win32 which could be used on Windows 3.1 systems), and one for Chicago. Thus Microsoft sought to ensure some degree of compatibility between the Chicago design and Windows NT, even though the two systems had radically different internal architectures. Windows NT was the first Windows operating system based on a hybrid kernel.
Windows 95.
After Windows 3.11, Microsoft began to develop a new consumer oriented version of the operating system codenamed Chicago. Chicago was designed to have support for 32-bit preemptive multitasking like OS/2 and Windows NT, although a 16-bit kernel would remain for the sake of backward compatibility. The Win32 API first introduced with Windows NT was adopted as the standard 32-bit programming interface, with Win16 compatibility being preserved through a technique known as "thunking". A new object oriented GUI was not originally planned as part of the release, although elements of the Cairo user interface were borrowed and added as other aspects of the release (notably Plug and Play) slipped.
Microsoft did not change all of the Windows code to 32-bit; parts of it remained 16-bit (albeit not directly using real mode) for reasons of compatibility, performance, and development time. Additionally it was necessary to carry over design decisions from earlier versions of Windows for reasons of backwards compatibility, even if these design decisions no longer matched a more modern computing environment. These factors eventually began to impact the operating system's efficiency and stability.
Microsoft marketing adopted Windows 95 as the product name for Chicago when it was released on 24 August 1995. Microsoft had a double gain from its release: first, it made it impossible for consumers to run Windows 95 on a cheaper, non-Microsoft DOS; secondly, although traces of DOS were never completely removed from the system and MS DOS 7 would be loaded briefly as a part of the booting process, Windows 95 applications ran solely in 386 enhanced mode, with a flat 32-bit address space and virtual memory. These features make it possible for Win32 applications to address up to 2 gigabytes of virtual RAM (with another 2 GB reserved for the operating system), and in theory prevented them from inadvertently corrupting the memory space of other Win32 applications. In this respect the functionality of Windows 95 moved closer to Windows NT, although Windows 95/98/ME did not support more than 512 megabytes of physical RAM without obscure system tweaks.
IBM continued to market OS/2, producing later versions in OS/2 3.0 and 4.0 (also called Warp). Responding to complaints about OS/2 2.0's high demands on computer hardware, version 3.0 was significantly optimized both for speed and size. Before Windows 95 was released, OS/2 Warp 3.0 was even shipped preinstalled with several large German hardware vendor chains. However, with the release of Windows 95, OS/2 began to lose market share.
It is probably impossible to choose one specific reason why OS/2 failed to gain much market share. While OS/2 continued to run Windows 3.1 applications, it lacked support for anything but the Win32s subset of Win32 API (see above). Unlike with Windows 3.1, IBM did not have access to the source code for Windows 95 and was unwilling to commit the time and resources to emulate the moving target of the Win32 API. IBM later introduced OS/2 into the United States v. Microsoft case, blaming unfair marketing tactics on Microsoft's part.
Microsoft went on to release five different versions of Windows 95:
OSR2, OSR2.1, and OSR2.5 were not released to the general public; rather, they were available only to OEMs that would preload the OS onto computers. Some companies sold new hard drives with OSR2 preinstalled (officially justifying this as needed due to the hard drive's capacity).
The first Microsoft Plus! add-on pack was sold for Windows 95.
Windows NT 4.0.
Windows NT 4.0 was the successor of 3.51 (1995) and 3.5 (1994). Microsoft released Windows NT 4.0 to manufacturing in July 1996, one year after the release of Windows 95. Major new features included the new Explorer shell from Windows 95, scalability and feature improvements to the core architecture, kernel, USER32, COM and MSRPC.
Windows NT 4.0 came in four versions:
Windows 98.
On 25 June 1998, Microsoft released Windows 98 (codenamed Memphis). It included new hardware drivers and the FAT32 file system which supports disk partitions that are larger than 2 GB (first introduced in Windows 95 OSR2). USB support in Windows 98 is marketed as a vast improvement over Windows 95. The release continued the controversial inclusion of the Internet Explorer browser with the operating system that started with Windows 95 OEM Service Release 1. The action eventually led to the filing of the United States v. Microsoft case, dealing with the question of whether Microsoft was introducing unfair practices into the market in an effort to eliminate competition from other companies such as Netscape.
In 1999, Microsoft released Windows 98 Second Edition, an interim release. One of the more notable new features was the addition of Internet Connection Sharing, a form of network address translation, allowing several machines on a LAN (Local Area Network) to share a single Internet connection. Hardware support through device drivers was increased and this version shipped with Internet Explorer 5. Many minor problems that existed in the first edition were fixed making it, according to many, the most stable release of the Windows 9x family.
Windows 2000.
Microsoft released Windows 2000 in February 2000. It has the version number Windows NT 5.0. It was successfully deployed both on the server and the workstation markets. Amongst Windows 2000's most significant new features was Active Directory, a near-complete replacement of the NT 4.0 Windows Server domain model, which built on industry-standard technologies like DNS, LDAP, and Kerberos to connect machines to one another. Terminal Services, previously only available as a separate edition of NT 4, was expanded to all server versions. A number of features from Windows 98 were incorporated also, such as an improved Device Manager, Windows Media Player, and a revised DirectX that made it possible for the first time for many modern games to work on the NT kernel. Windows 2000 is also the last NT-kernel Windows operating system to lack product activation.
While Windows 2000 upgrades were available for Windows 95 and Windows 98, it was not intended for home users.
Windows 2000 was available in four editions:
Windows ME.
In September 2000, Microsoft released a successor to Windows 98 called Windows ME, short for "Millennium Edition". It was the last DOS-based operating system from Microsoft. Windows ME introduced a new multimedia-editing application called Windows Movie Maker, came standard with Internet Explorer 5.5 and Windows Media Player 7, and debuted the first version of System Restore – a recovery utility that enables the operating system to revert system files back to a prior date and time. System Restore was a notable feature that would continue to thrive in later versions of Windows, including XP, Vista, and Windows 7.
Windows ME was conceived as a quick one-year project that served as a stopgap release between Windows 98 and Windows XP. Many of the new features were available from the Windows Update site as updates for older Windows versions ("System Restore" and "Windows Movie Maker" were exceptions). Windows ME was criticized for stability issues, and for lacking real mode DOS support, to the point of being referred to as the "Mistake Edition" or "Many Errors." Windows ME was the last operating system to be based on the Windows 9x (monolithic) kernel and MS-DOS.
Windows XP.
On 25 August 2001, Microsoft released Windows XP (codenamed "Whistler"). The merging of the Windows NT/2000 and Windows 95/98/Me lines was finally achieved with Windows XP. Windows XP uses the Windows NT 5.1 kernel, marking the entrance of the Windows NT core to the consumer market, to replace the aging 16/32-bit branch. The initial release met with considerable criticism, particularly in the area of security, leading to the release of three major Service Packs. Windows XP SP1 was released in September 2002, SP2 came out in August 2004 and SP3 came out in April 2008. Service Pack 2 provided significant improvements and encouraged widespread adoption of XP among both home and business users. Windows XP lasted longer as Microsoft's flagship operating system than any other version of Windows, from 25 October 2001 to 30 January 2007 when it was succeeded by Windows Vista.
Windows XP is available in a number of versions:
Windows Server 2003.
On 25 April 2003 Microsoft launched Windows Server 2003, a notable update to Windows 2000 Server encompassing many new security features, a new "Manage Your Server" wizard that simplifies configuring a machine for specific roles, and improved performance. It has the version number NT 5.2. A few services not essential for server environments are disabled by default for stability reasons, most noticeable are the "Windows Audio" and "Themes" services; users have to enable them manually to get sound or the "Luna" look as per Windows XP. The hardware acceleration for display is also turned off by default, users have to turn the acceleration level up themselves if they trust the display card driver.
December 2005, Microsoft released Windows Server 2003 R2, which is actually Windows Server 2003 with SP1 (Service Pack 1) plus an add-on package.
Among the new features are a number of management features for branch offices, file serving, printing and company-wide identity integration.
Windows Server 2003 is available in six editions:
Windows XP x64 and Server 2003 x64 Editions.
On 25 April 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003, x64 Editions in Standard, Enterprise and Datacenter SKUs. Windows XP Professional x64 Edition is an edition of Windows XP for x86-64 personal computers. It is designed to use the expanded 64-bit memory address space provided by the x86-64 architecture.
Windows XP Professional x64 Edition is based on the Windows Server 2003 codebase; with the server features removed and client features added. Both "Windows Server 2003 x64" and Windows XP Professional x64 Edition use identical kernels.
Windows XP Professional "x64 Edition" is not to be confused with Windows XP "64-bit Edition", as the latter was designed for Intel Itanium processors. During the initial development phases, Windows XP Professional x64 Edition was named "Windows XP 64-Bit Edition for 64-Bit Extended Systems".
Windows Server 2003 R2.
Windows Server 2003 R2, an update of Windows Server 2003, was released to manufacturing on 6 December 2005. It is distributed on two CDs, with one CD being the Windows Server 2003 SP1 CD. The other CD adds many optionally installable features for Windows Server 2003. The R2 update was released for all x86 and x64 versions. Windows Server 2003 R2 Enterprise Edition was not released for Itanium.
Windows Fundamentals for Legacy PCs.
In July 2006, Microsoft released a thin-client version of Windows XP Service Pack 2, called Windows Fundamentals for Legacy PCs (WinFLP). It is only available to Software Assurance customers. The aim of WinFLP is to give companies a viable upgrade option for older PCs that are running Windows 95, 98, and Me that will be supported with patches and updates for the next several years. Most user applications will typically be run on a remote machine using Terminal Services or Citrix.
Windows Home Server.
Windows Home Server (codenamed Q, Quattro) is a server product based on Windows Server 2003, designed for consumer use. The system was announced on 7 January 2007 by Bill Gates. Windows Home Server can be configured and monitored using a console program that can be installed on a client PC. Such features as Media Sharing, local and remote drive backup and file duplication are all listed as features.
Windows Vista.
Windows Vista was released on 8 November 2006 to business customers, consumer versions following on 30 January 2007. Windows Vista intended to have enhanced security by introducing a new restricted user mode called User Account Control, replacing the "administrator-by-default" philosophy of Windows XP. One major difference between Vista and earlier versions of Windows, Windows 95 and later, is that the original start button was replaced with just the Windows icon. Vista also features new graphics features, the Windows Aero GUI, new applications (such as Windows Calendar, Windows DVD Maker and some new games including Chess, Mahjong, and Purble Place), Internet Explorer 7, Windows Media Player 11, and a large number of underlying architectural changes. Windows Vista has the version number NT 6.0.
Windows Vista ships in six editions:
All editions (except Starter edition) are currently available in both 32-bit and 64-bit versions. The biggest advantage of the 64-bit version is breaking the 4 gigabyte memory barrier, which 32-bit computers cannot fully access.
Windows Server 2008.
Windows Server 2008, released on 27 February 2008, was originally known as Windows Server Codename "Longhorn". Windows Server 2008 builds on the technological and security advances first introduced with Windows Vista, and is significantly more modular than its predecessor, Windows Server 2003.
Windows Server 2008 ships in ten editions:
Windows 7 and Windows Server 2008 R2.
Windows 7 was released to manufacturing on 22 July 2009, and reached general retail availability on 22 October 2009. It was previously known by the codenames Blackcomb and Vienna. Windows 7 has the version number NT 6.1.
Some features of Windows 7 are faster booting, Device Stage, Windows PowerShell, less obtrusive User Account Control, multi-touch, and improved window management. Features included with Windows Vista and not in Windows 7 include the sidebar (although gadgets remain) and several programs that were removed in favor of downloading their Windows Live counterparts.
Windows 7 ships in six editions:
In some countries (Austria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, United Kingdom, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and Switzerland), there are other editions that lack some features such as Windows Media Player, Windows Media Center and Internet Explorer called names such as "Windows 7 N."
Microsoft focuses on selling Windows 7 Home Premium and Professional. All editions, except the Starter edition, are available in both 32-bit and 64-bit versions.
Unlike the corresponding Vista editions, the Professional and Enterprise editions are supersets of the Home Premium edition.
At the Professional Developers Conference (PDC) 2008, Microsoft also announced Windows Server 2008 R2, as the server variant of Windows 7. Windows Server 2008 R2 ships in 64-bit versions (x64 and Itanium) only.
Windows Home Server 2011.
Windows Home Server 2011 code named 'Vail' was released on 6 April 2011. Windows Home Server 2011 is built on the Windows Server 2008 R2 code base. It follows the release of Windows Home Server Power Pack 3 which added support for Windows 7 to Windows Home Server. Windows Home Server 2011 is considered a "major release". (Its predecessor having been built on Windows Server 2003) and only supports x86-64 hardware.
Windows Thin PC.
In 2011, Microsoft introduced Windows Thin PC or WinTPC, which is a feature- and size-reduced locked-down version of Windows 7 expressly designed to turn older PCs into thin clients. WinTPC is available for software assurance customers and relies on cloud computing in a business network. Wireless operation is supported since WinTPC has full wireless stack integration, but wireless operation may not be as good as the operation on a wired connection.
Windows 8 and Windows Server 2012.
Windows 8 is the current version of Microsoft Windows. One edition, Windows RT, runs on some system-on-a-chip devices with mobile 32-bit ARM (ARMv7) processors. Windows 8 features a redesigned user interface, designed to make it easier for touchscreen users to use Windows. The interface introduced an updated Start menu known as the Start screen, and a new full-screen application platform. The desktop interface is also present for running windowed applications, although Windows RT will not run any desktop applications not included in the system. On the Building Windows 8 blog, it was announced that a computer running Windows 8 can boot up much faster than Windows 7. New features also include USB 3.0 support, the Windows Store, the ability to run from USB drives with Windows To Go, and others.
Windows 8 is available in the following editions:
The first public preview of Windows Server 2012 was also shown by Microsoft at the 2011 Microsoft Worldwide Partner Conference.
Windows 8 Release Preview and Windows Server 2012 Release Candidate were both released on 31 May 2012. Product development on Windows 8 was completed on 1 August 2012, and it was released to manufacturing the same day. Windows Server 2012 went on sale to the public on 4 September 2012. Windows 8 went on sale 26 October 2012.
Windows 8.1 and Windows Server 2012 R2 were released on 17 October 2013. Windows 8.1 is available as an update in the Windows store only for Windows 8 users and also available to download for clean installation. The update adds new options for resizing the live tiles on the Start screen.
Windows 10.
Windows 10 is an upcoming release of the Microsoft Windows operating system. Unveiled on September 30, 2014, it will be released in mid-2015.
Windows Server vNext.
Windows Server vNext (it may not be the final name) is an upcoming release of the Microsoft Windows Server operating system. Unveiled on September 30, 2014, it is planned to be released in 2016.
Further reading.
</dl>

</doc>
<doc id="13696" url="http://en.wikipedia.org/wiki?curid=13696" title="Helsinki">
Helsinki

Helsinki (; ]; Swedish: Helsingfors, ]) is the capital and largest city of Finland. It is in the region of Uusimaa, located in southern Finland, on the shore of the Gulf of Finland, an arm of the Baltic Sea. Helsinki has a population of 623,732, an urban population of 1.2 million (31 December 2013), and a metropolitan population of 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is located some 80 km north of Tallinn, Estonia, 400 km north east of Stockholm, Sweden, and 300 km west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities.
The Helsinki metropolitan area includes urban core of Helsinki, Espoo, Vantaa, Kauniainen and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the fourth largest Nordic metropolitan area after the metropolitan areas of Copenhagen, Stockholm and Oslo and Helsinki city is the third biggest Nordic city after Stockholm and Oslo.
Helsinki is Finland's major political, educational, financial, cultural and research centre as well as one of northern Europe's major cities. Approximately 70% of foreign companies operating in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia.
In 2009, Helsinki was chosen to be the World Design Capital for 2012 by the International Council of Societies of Industrial Design, narrowly beating Eindhoven for the title.
In the Economist Intelligence Unit's August 2012 Liveability survey, assessing the best and worst cities to live in, Helsinki placed eighth best overall. In 2011, the Monocle Magazine in turn ranked Helsinki the most liveable city in the world in its "Liveable Cities Index 2011".
Etymology of the name Helsinki.
"Helsinki" (Finnish pronunciation places stress on the first syllable: ]), is used to refer to the city in most languages, but not in Swedish.
The Swedish name "Helsingfors" (] or ]) is the original official name of the city of Helsinki (in the very beginning, spelled "Hellssingeforss"). The Finnish language form of the name probably originates from "Helsinga" and similar names used for the river that is currently known as the Vantaa River as documented already in the 14th century. "Helsingfors" comes from the name of the surrounding parish, "Helsinge" (source for Finnish "Helsinki") and the rapids (Swedish: "fors"), which flowed through the original village. As part of the Grand Duchy of Finland in the Russian Empire it was known as Gelsingfors – a Russian adaption of the name.
A suggestion for the origin of the name "Helsinge" is that it have originated with medieval Swedish settlers who came from Hälsingland in Sweden. Others have proposed that the name derives from the Swedish word "helsing", a former version of the word "hals" (neck), referring to the narrowest part of the river, i.e. the rapids. Other Scandinavian cities located at this kind of geographical locations were given similar names at the time, such as Helsingør and Helsingborg.
The name Helsinki has been used in Finnish official documents and in Finnish language newspapers since 1819, when the Senate of Finland moved to the town and the decrees issued there were dated with Helsinki as the place of issue. This is how the form Helsinki came to be used in the Finnish literary language.
In Helsinki slang the city is nicknamed as either "Stadi" (from the Swedish word "stad", meaning "city") or "Hesa" (short of Helsinki), with "Stadi" being used to assert that the speaker is native to the city. "Helsset" is the Northern Saami name of Helsinki.
History.
Early history.
Helsinki was established as a trading town by King Gustav I of Sweden in 1550 as the town of Helsingfors, which he intended to be a rival to the Hanseatic city of Reval (today known as Tallinn). Little came of the plans as Helsinki remained a tiny town plagued by poverty, wars, and diseases. The plague of 1710 killed the greater part of the inhabitants of Helsinki. The construction of the naval fortress Sveaborg (In Finnish "Viapori", today also "Suomenlinna") in the 18th century helped improve Helsinki's status, but it was not until Russia defeated Sweden in the Finnish War and annexed Finland as the autonomous Grand Duchy of Finland in 1809 that the town began to develop into a substantial city. During the war, Russians besieged the Sveaborg fortress and about one quarter of the town was destroyed in an 1808 fire.
Czar Alexander I of Russia moved the Finnish capital from Turku to Helsinki in 1812 to reduce Swedish influence in Finland and bring the capital closer to St. Petersburg. Following the Great Fire of Turku in 1827, The Royal Academy of Turku, at the time the country's only university, was also relocated to Helsinki, and eventually became the modern University of Helsinki. The move consolidated the city's new role and helped set it on the path of continuous growth. This transformation is highly apparent in the downtown core, which was rebuilt in neoclassical style to resemble St. Petersburg, mostly to a plan by the German-born architect C. L. Engel. As elsewhere, technological advancements such as railroads and industrialization were key factors behind the city's growth.
Twentieth century.
Despite the tumultuousness of Finnish history during the first half of the 20th century, Helsinki continued its steady development. A landmark event were the XV Olympic games (1952 Olympic Games) held in Helsinki. Finland's rapid urbanization in the 1970s, occurring late relative to the rest of Europe, tripled the population in the metropolitan area, and the Helsinki Metro subway system was built. The relatively sparse population density of Helsinki and its peculiar structure have often been attributed to the lateness of its growth. 
In July 1942, Nazi SS commander Heinrich Himmler came to Helsinki to try persuade their Finnish allies to hand over more than 200 foreign Jews in Finland. Only eight people were deported to Auschwitz on November 6, 1942. The other 2,000 Jews in Finland were not deported.
Geography.
Being called the "Daughter of the Baltic", Helsinki is located on the tip of a peninsula and 315 islands. The inner city area occupies a southern peninsula, which is rarely referred to by its actual name Vironniemi. Population density in certain parts of Helsinki's inner city area is very high, reaching 16494 PD/km2 in the district of Kallio, but as a whole Helsinki's population density of 3050 /km2 ranks it as quite sparsely populated in comparison to other European capital cities. Much of Helsinki outside the inner city area consists of postwar suburbs separated from each other by patches of forest. A narrow, ten-kilometre-long (6 mi) Helsinki Central Park that stretches from the inner city to the northern border of Helsinki is an important recreational area for residents. The City of Helsinki has about 11,000 boat berths and possesses over 14 000 hectares of marine fishing waters adjacent to the Metropolitan area. Some 60 fish species are found in this area. Recreational fishing is a popular hobby among kids and adults alike.
Major islands in Helsinki include Seurasaari, Vallisaari, Lauttasaari and Korkeasaari – the lattermost being the site of the country's biggest zoo. Other significant islands are the fortress island of Suomenlinna (Sveaborg), the military island of Santahamina and Isosaari. Pihlajasaari island is a favourite summer spot for gay men and naturists, very much comparable to Fire Island off New York City.
Metropolitan area.
Helsinki capital region consists of the four municipalities of Helsinki, Espoo, Vantaa and Kauniainen and is considered to be the only metropolis in Finland. It has a population of over 1,1 million, and is by far the biggest and most densely populated area of Finland, over four times bigger than Tampere. The capital region spreads over a land area of 770 km² and has a population density of 1,418 per square kilometre. With over 20 per cent of the country's population in just 0.2 per cent of its surface area, the housing density of the area is high by Finnish standards.
The Helsinki Metropolitan Area consists of the cities of Helsinki capital region and ten surrounding municipalities. The Metropolitan Area covers 3697 km2 and contains a total population of over 1.4 million, or about a fourth of the total population of Finland. The Metropolitan Area has a high concentration of employment: approximately 750,000 jobs. Despite the intensity of land use, the region also has large recreational areas and green spaces. The Helsinki metropolitan area is the world's northernmost urban area among those with a population of over one million people, and the city is the northernmost capital of an EU member state.
Climate.
Helsinki has a humid continental climate (Köppen: Dfb), less than 2 C-change above the threshold for subarctic classification. Owing to the mitigating influence of the Baltic Sea and Gulf Stream, temperatures in winter are higher than the northern location might suggest, with the average in January and February around -5 C. Winters in Helsinki are notably shorter and warmer than in the north, lasting around 3 months. Temperatures below −20 C occur a few times a year or less . However, because of the latitude, days last 5 hours and 48 minutes around the winter solstice with very low Sun rays (at noon Sun is little bit over 6 degrees in the sky), and the cloudy weather at this time of year accentuates the darkness. Conversely, Helsinki enjoys long days in summer, during the summer solstice days last 18 hours and 57 minutes . Summers are usually around 4 months long and the average daily temperatures are high compared to other places in Finland. The average maximum temperature from June to August is around 19 to. Due to the marine effect, especially during hot summer days, daily temperatures are a little cooler and night temperatures are higher than further away in the mainland. The highest temperature ever recorded in the city centre was 33.1 C on 18 July 1945 and the lowest was -34.3 C on 10 January 1987. Helsinki Airport recorded a temperature of 34.0 C on 29 July 2010 and a low of -35.9 C on 9 January 1987. Precipitation is received from frontal passages and thunderstorms. Thunderstorms are most common in summer.
Cityscape.
Carl Ludvig Engel (1778–1840) was appointed to design a new city centre all on his own. He designed several neoclassical buildings in Helsinki. The focal point of Engel's city plan is the Senate Square. It is surrounded by the Government Palace (to the east), the main building of Helsinki University (to the west), and (to the north) the enormous Cathedral, which was finished in 1852, twelve years after C. L. Engel's death. Subsequently, Engel's neoclassical plan stimulated the epithet, "The White City Of The North". Helsinki is, however, perhaps even more famous for its numerous Art Nouveau (Jugend in Finnish) influenced buildings of the romantic nationalism, designed in the early 1900s and strongly influenced by the Kalevala, which is a very popular theme in the national romantic art of that era. Helsinki's Art Nouveau style is also featured in large residential areas such as Katajanokka and Ullanlinna. The master of the Finnish Art Nouveau was Eliel Saarinen (1873–1950), whose architectural masterpiece was the Helsinki central railway station.
Helsinki also features several buildings by the world-renowned Finnish architect Alvar Aalto (1898–1976), recognized as one of the pioneers of architectural functionalism. However, some of his works, such as the headquarters of the paper company Stora Enso and the concert venue, Finlandia Hall, have been subject to divided opinions from the citizens.
Renowned functionalist buildings in Helsinki by other architects include the Olympic Stadium, the Tennis Palace, the Rowing Stadium, the Swimming Stadium, the Velodrome, the Glass Palace, the Exhibition Hall (now Töölö Sports Hall) and Helsinki-Malmi Airport. The sports venues were built to serve the 1940 Helsinki Olympic Games; the games were initially cancelled due to the Second World War, but the venues eventually got to fulfill their purpose in the 1952 Olympic Games. Many of them are listed by DoCoMoMo as significant examples of modern architecture. The Olympic Stadium and Helsinki-Malmi Airport are in addition catalogued by the National Board of Antiquities as cultural-historical environments of national significance. 
As a historical footnote, Helsinki's neoclassical buildings were often used as a backdrop for scenes set to take place in the Soviet Union in many Cold War era Hollywood movies, when filming in the USSR was not possible. Some of the more notable ones are "The Kremlin Letter" (1970), "Reds" (1981) and "Gorky Park" (1983). Because some streetscapes were reminiscent of Leningrad's and Moscow's old buildings, they too were used in movie productions—much to some residents' dismay. At the same time the government secretly instructed Finnish officials not to extend assistance to such film projects.
In the 21st century Helsinki has decided to allow the construction of skyscrapers. Several projects are already in progress, mainly in Pasila and Kalasatama. The tallest with 40 floors will rise at least 150 meters (500 feet). In Pasila, twenty new high rises will be erected within 10 years. In Kalasataman Keskus REDI, the first 35-story (130 meters) and 32-story (122 meters) residential towers are already under construction. Later they will be joined by a 37-story (140 meters), two 32-story (122 meters, 400 feet), 31-story (120 meters) and 27-story (100 meters) residential buildings. In Kalasatama area, there will be 30 high-rises within 10 years.
A panoramic view over the southernmost districts of Helsinki from Hotel Torni. The Helsinki Old Church and its surrounding park are seen in the foreground, while the towers of St. John's Church (near center) and Mikael Agricola Church (right) can be seen in the middle distance, backdropped by the Gulf of Finland.
A panoramic view of Helsinki Central Railway Station and its surroundings
A panoramic view of Kamppi Central and its surroundings
Suomenlinna at afternoon in winter
Government.
As in all Finnish municipalities, the City Council is the main decision-making organ in local politics, dealing with issues such as city planning, schools, health care, and public transport. The council is elected every four years.
The city council of Helsinki consists of eighty-five members. Following the most recent municipal elections, in 2012, the three largest parties are the National Coalition Party (23), the Greens (19), and the Social Democrats (15). The Mayor, Jussi Pajunen, is a member of the National Coalition Party.
Traditionally, the conservative National Coalition Party (Kokoomus) has been the biggest party on Helsinki City Council, with the Social Democrats being the second biggest. In 2000 the Greens, for which Helsinki is the strongest area of support nationally, gained the position of second most popular party in the city, in 2004 the Social Democrats regained that position, and since 2008 the Greens have again been the second biggest party.
The Left Alliance is the fourth largest party, while the True Finns have increased their support steadily to become the fifth largest party. Support for the Swedish People's Party has been steadily declining over the years, most likely because of the diminishing proportion of Swedish speakers in Helsinki. The Centre Party of Finland, despite being one of the major parties in national politics, has little support in Helsinki, as it does in most big cities.
Demographics.
Helsinki has a higher proportion of women (53.4%) than elsewhere in Finland (51.1%). Helsinki's current population density of 2,739.36 people per square kilometer is by far the highest in Finland. Life expectancy for both genders is slightly below the national averages: 75.1 years for men as compared to 75.7 years, 81.7 years for women as compared to 82.5 years.
Helsinki has experienced strong growth since the 1810s, when it replaced Turku as the capital of the Grand Duchy of Finland, which later became the sovereign Republic of Finland. The city continued to show strong growth from that time onwards, with the exception during the Finnish Civil War period. From the end of World War II up until the 1970s there was a massive exodus of people from the countryside to the cities of Finland, in particular Helsinki. Between 1944 and 1969 the population of the city nearly doubled from 275,000 to 525,600.
In the 1960s, the population growth of Helsinki proper began to ebb mainly due to lack of housing. Many residents began to move to neighbouring Espoo and Vantaa, where population growth has since soared. Espoo's population increased ninefold in sixty years, from 22,874 people in 1950 to 244,353 in 2009. Neighboring Vantaa has seen even more dramatic change in the same time span: from 14,976 in 1950 to 197,663 in 2009, a thirteenfold increase. These dramatic increases pushed the municipalities of greater Helsinki into more intense cooperation in such areas as public transportation and waste management. The increasing scarcity of housing and the higher costs of living in the Helsinki metropolitan area have pushed many daily commuters to find housing in formerly very rural areas, and even further, to such cities as Lohja (50 km northwest from the city centre), Hämeenlinna and Lahti (both 100 km from Helsinki), and Porvoo (50 km to the east).
Language.
Finnish and Swedish are the official languages of the municipality of Helsinki. The majority, or 81.9% of the population, speak Finnish as their native language. A minority, at 5.9%, speak Swedish. Around 12.2% of the population speak a native language other than Finnish or Swedish. Helsinki slang today combines influences mainly from Finnish and English, but has traditionally had strong Russian and Swedish influences. Finnish today is the common language of communication between Finnish speakers, Swedish speakers and speakers of other languages (New Finns) in day-to-day affairs in the public sphere between unknown persons. In case a speaker's knowledge of Finnish is not known, English is usually spoken. Swedish is commonly spoken in city or national agencies specifically aimed at Finland-Swedish speakers, such as the Social Services Department on Hämeentie or the Luckan Cultural centre in Kamppi. Knowledge of Finnish is also essential in business and is usually a basic requirement in the employment market.
Finnish speakers surpassed Swedish speakers in 1890 to become the majority of the city's population. At the time, the population of Helsinki was 61,530.
Immigration.
Helsinki is the global gateway of Finland. The city has Finland's largest immigrant population in both absolute and relative terms. There are over 140 nationalities represented in Helsinki. The largest groups (as of 2013) are from Sweden, Russia, Estonia, Somalia, China, Kurdistan, Spain, Germany, France, Vietnam, and Turkey. Helsinki was already an international city in the late 19th century with distinctive Swedish, Russian and German minorities.
Foreign citizens make up 8.0% of the population, while foreign born make up 11.1%.% In 2012, 68,375 residents spoke a native language other than Finnish, Swedish or one of the three Sami languages spoken in Finland. The largest groups of residents with a non-Finnish background come from Russia (14,532), Estonia (9,065) and Somalia (6,845). Half of the immigrant population in Finland lives in greater Helsinki, and one third in the city of Helsinki.
Economy.
The Helsinki metropolitan area generates approximately one third of Finland's GDP. GDP per capita is roughly 1.3 times the national average.
The metropolitan area's gross value added per capita is 200% of the mean of 27 European metropolitan areas, equalling those of Stockholm or Paris. The gross value added annual growth has been around 4%.
83 of the 100 largest Finnish companies are headquartered in Greater Helsinki. Two-thirds of the 200 highest-paid Finnish executives live in Greater Helsinki and 42% in Helsinki. The average income of the top 50 earners was 1.65 million euro.
The tap water is of excellent quality and it is supplied by 120 km long Päijänne Water Tunnel, one of the world's longest continuous rock tunnels. Bottled Helsinki tap water is even sold to countries such as Saudi Arabia.
Education.
Helsinki has 190 comprehensive schools, 41 upper secondary schools and 15 vocational institutes. Half of the 41 upper secondary schools are private or state-owned, the other half municipal. Higher level education is given in eight universities (see the section "Universities" below) and four polytechnics.
University of Applied Sciences.
Helsinki is one of the co-location centres of the Knowledge and Innovation Community (Future information and communication society) of The European Institute of Innovation and Technology (EIT).
The educational department takes part in Lifelong Learning Programme 2007–2013 in Finland.
Culture.
Museums.
The biggest historical museum in Helsinki is the National Museum of Finland, which displays a vast historical collection from prehistoric times to the 21st century. The museum building itself, a national romantic style neomedieval castle, is a tourist attraction. Other major historical museum is the Helsinki City Museum, which introduces visitors to Helsinki's 500-year history. The University of Helsinki also has many significant museums, including the University Museum and the Natural History Museum.
The Finnish National Gallery consists of three museums: Ateneum Art Museum for classical Finnish art, Sinebrychoff Art Museum for classical European art, and Kiasma Art Museum for modern art. The old Ateneum, a neo-Renaissance palace from the 19th century, is one of the city's major historical buildings. All three museum buildings are state-owned through Senate Properties.
The Design Museum is devoted to the exhibition of both Finnish and foreign design, including industrial design, fashion, and graphic design.
Theatres.
Helsinki has three major theatres: The Finnish National Theatre, the Helsinki City Theatre, and the Finland Swedish Svenska Teatern. The city's main musical venues are the Finnish National Opera, the Finlandia concert hall and the Helsinki Music Centre. The Music Centre also houses a part of the Sibelius Academy. Bigger concerts and events are usually held at one of the city's two big ice hockey arenas: the Hartwall Areena or the Helsinki Ice Hall. Helsinki has Finland's largest fairgrounds.
Music.
Helsinki is home to two full-size symphony orchestras, the Helsinki Philharmonic Orchestra and the Finnish Radio Symphony Orchestra, both of which perform at the Helsinki Music Centre concert hall. Acclaimed contemporary composers Kaija Saariaho, Magnus Lindberg, Esa-Pekka Salonen and Einojuhani Rautavaara, among others, were born and raised in Helsinki, and studied at the Sibelius Academy. The Finnish National Opera, the only full-time, professional opera company in Finland, is located in Helsinki. The opera singer Martti Wallén, one of the company's long-time soloists, was born and raised in Helsinki, as was mezzo-soprano Monica Groop.
Many widely renowned and acclaimed bands have originated in Helsinki, including Hanoi Rocks, HIM, Stratovarius, The 69 Eyes, Finntroll, Ensiferum, Wintersun, The Rasmus and Apocalyptica.
Art.
The Helsinki Festival is an annual arts and culture festival, which takes place every August (including the Night of the Arts).
Vappu is an annual carnival for students and workers.
Helsinki Arena hosted the Eurovision Song Contest 2007, the first Eurovision Song Contest arranged in Finland, following Lordi's win in 2006.
At the Senate Square in September / October 2010, the largest open-air art exhibition ever in Finland took place: About 1.4 million people saw the international exhibition of "United Buddy Bears".
Helsinki is the 2012 World Design Capital, in recognition of the use of design as an effective tool for social, cultural and economic development in the city. In choosing Helsinki, the World Design Capital selection jury highlighted Helsinki's use of 'Embedded Design', which has tied design in the city to innovation, "creating global brands, such as Nokia, Kone and Marimekko, popular events, like the annual Helsinki Design Week, outstanding education and research institutions, such as the University of Art and Design Helsinki, and exemplary architects and designers such as Eliel Saarinen and Alvar Aalto".
Helsinki also hosts many film festivals. Most of them are small venues, but some have gained renown even abroad. The most prolific would be the Love & Anarchy film festival (also known as Helsinki International Film Festival), which features films on a wide spectrum. Night Visions Film Festival on the other hand focuses on genre cinema, screening horror, fantasy and science fiction films in very popular movie marathons that take whole night. Another popular film festival is DocPoint, a festival that focuses solely on documentary cinema.
Media.
Today, there are around 200 newspapers, 320 popular magazines, 2,100 professional magazines, 67 commercial radio stations, three digital radio channels and one nationwide and five national public service radio channels.
Each year, around 12,000 book titles are published and 12 million records are sold.
Sanoma publishes the newspaper "Helsingin Sanomat" (its circulation of 412,000 making it the largest), the tabloid "Ilta-Sanomat", the commerce-oriented "Taloussanomat" and the television channel Nelonen. The other major publisher Alma Media publishes over thirty magazines, including the newspaper "Aamulehti", tabloid "Iltalehti" and commerce-oriented "Kauppalehti". Worldwide, Finns, along with other Nordic peoples and the Japanese, spend the most time reading newspapers.
YLE, Finland's public broadcasting station, operates five television channels and thirteen radio channels in both national languages. YLE is funded through a mandatory television license and fees for private broadcasters. All TV channels are broadcast digitally, both terrestrially and on cable. The commercial television channel MTV3 and commercial radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).
Around 79% of the population uses the Internet. Finland had around 1.52 million broadband Internet connections by the end of June 2007 or around 287 per 1,000 inhabitants. All Finnish schools and public libraries have Internet connections and computers and most residents have a mobile phone. Value-added services are rare. In October 2009, Finland's Ministry of Transport and Communications committed to ensuring that every person in Finland would be able to access the Internet at a minimum speed of one megabit-per-second beginning July 2010.
Sports.
Helsinki has a long tradition of sports: the city gained much of its initial international recognition during the 1952 Summer Olympics, and the city has arranged sporting events such as the first World Championships in Athletics 1983 and 2005, and the European Championships in Athletics 1971, 1994 and 2012. Helsinki hosts successful local teams in both of the most popular team sports in Finland, football and ice hockey. Helsinki houses HJK Helsinki, Finland's largest and most successful football club and IFK Helsingfors, their local rivals with 7 championship titles. The fixtures between the two are commonly known as Stadin derby. Helsinki's track and field club Helsingin Kisa-Veikot is also dominant within Finland. Ice hockey is popular among many Helsinki residents, who usually support either of the local clubs IFK Helsingfors (HIFK) or Jokerit. HIFK, with 14 Finnish championships titles, also plays in the highest bandy division, along with Botnia−69. The Olympic stadium hosted the first ever Bandy World Championship in 1957.
Helsinki was elected host-city of the 1940 Summer Olympics, but due to World War II they were canceled. Instead Helsinki was the host of the 1952 Summer Olympics. The Olympics were a landmark event symbolically and economically for Helsinki and Finland as a whole that was recovering from the winter war and the continuation war fought with the Soviet Union. Helsinki was also in 1983 the first ever city to host the World Championships in Athletics. Helsinki also hosted the event in 2005, thus also becoming the first city to ever host the Championships for a second time. The Helsinki City Marathon has been held in the city every year since 1980, usually in August. A Formula 3000 race through the city streets was held on 25 May 1997. In 2009 Helsinki was host of European Figure Skating Championships.
Transport.
Roads.
The backbone of Helsinki's motorway network consists of three semicircular ring roads, Ring I, Ring II, and Ring III, which connect expressways heading to other parts of Finland, and the western and eastern arteries of "Länsiväylä" and "Itäväylä" respectively. While variants of a "Keskustatunneli" tunnel under the city centre have been repeatedly proposed, as of 2011 the plan remains on the drawing board.
Helsinki has some 390 cars per 1000 inhabitants. This is less than in cities of similar density, such as Brussels' 483 per 1000, Stockholm's 401, and Oslo's 413.
Rail transport and buses.
Public transport is generally a hotly debated subject in the local politics of Helsinki. In Helsinki metropolitan area, public transportation is managed under Helsinki Region Transport, the metropolitan area transportation authority. The diverse public transport system consists of trams, commuter rail, the subway, bus lines, two ferry lines and on-demand minibuses.
Today, Helsinki is the only city in Finland to have trams and metro trains. There used to be two other cities in Finland with trams: Turku and Viipuri (Vyborg, now in Russia), but both have since abandoned trams. The Helsinki Metro, opened in 1982, is the only rapid transit system in Finland. In 2006, the construction of the long debated extension of the system west into Espoo was approved, and serious debate about an eastern extension into Sipoo has taken place.
The possibility of a Helsinki to Tallinn Tunnel is currently being researched. The rail tunnel would connect Helsinki to the Estonian capital Tallinn, further linking Helsinki to the rest of continental Europe by Rail Baltica.
Aviation.
Air traffic is handled primarily from the international Helsinki Airport, located approximately 19 km north of Helsinki's downtown area, in the neighbouring city of Vantaa. Helsinki's second airport, Malmi Airport, is mainly used for general and private aviation. Helicopter flights to Tallinn are available from Hernesaari Heliport.
Sea transport.
Like many other cities of the world, Helsinki had been deliberately founded next to the sea. The city therefore was able to benefit from good sea transportation links right from the start. The freezing of the sea imposed limitations on sea traffic up to the end of the 19th century. But for the last hundred years, the routes leading to Helsinki have been kept open even in winter with the aid of ice-breakers, many of them built in Helsinki Hietalahti shipyard. The arrival and departure of ships has also been a part of everyday life in Helsinki. Regular route traffic from Helsinki to Stockholm, Tallinn and St. Petersburg began as far back as 1837. 300 cruise ships and 360,000 cruise passengers visit Helsinki yearly, adding life and colour to the city life. There are international cruise ships dock in South Harbour, Katajanokka, West Harbour and Hernesaari. Helsinki is the second busiest passenger port in Europe with "circa" 11 million passengers in 2013. Ferry connections to Tallinn, Mariehamn and Stockholm are serviced by various companies. Finnlines passenger-freight ferries to Gdynia, Poland, Travemünde, Germany and Rostock, Germany are also available. St Peter Line offers passenger ferry service to Saint Petersburg several times a week.
International relations.
Special partnership cities.
Helsinki has a special partnership relation with:

</doc>
<doc id="13699" url="http://en.wikipedia.org/wiki?curid=13699" title="Hobart">
Hobart

Hobart ( ) 
is the capital and most populous city of the Australian island state of Tasmania. Founded in 1804 as a penal colony, Hobart is Australia's second oldest capital city after Sydney, New South Wales. The city is located in the state's south-east on the estuary of the Derwent River, making it the most southern of Australia's capital cities and its harbour forms the second-deepest natural port in the world.
In June 2013, the city had a greater area population of approximately 217,973. Its skyline is dominated by the 1271 m Mount Wellington, and much of the city's waterfront consists of reclaimed land. It is the financial and administrative heart of Tasmania, serving as the home port for both Australian and French Antarctic operations and acting as a major tourist hub, with over 1.192 million visitors in 2011/2012. The metropolitan area is often referred to as Greater Hobart to differentiate it from the City of Hobart, one of the five local government areas that cover the city.
History.
The first settlement began in 1803 as a penal colony at Risdon Cove on the eastern shores of the Derwent River, amid British concerns over the presence of French explorers. In 1804 it was moved to a better location at the present site of Hobart at Sullivans Cove. The city, initially known as Hobart Town or Hobarton, was named after Lord Hobart, the Colonial Secretary.
The area's indigenous inhabitants were members of the semi-nomadic "Mouheneener" tribe. Violent conflict with the European settlers, and the effects of diseases brought by them, dramatically reduced the aboriginal population, which was rapidly replaced by free settlers and the convict population. Charles Darwin visited Hobart Town in February 1836 as part of the Beagle expedition. He writes of Hobart and the Derwent estuary in his "Voyage of the Beagle":
...The lower parts of the hills which skirt the bay are cleared; and the bright yellow fields of corn, and dark green ones of potatoes, appear very luxuriant... I was chiefly struck with the comparative fewness of the large houses, either built or building. Hobart Town, from the census of 1835, contained 13,826 inhabitants, and the whole of Tasmania 36,505.
The Derwent River was one of Australia's finest deepwater ports and was the centre of the Southern Ocean whaling and the sealing trade. The settlement rapidly grew into a major port, with allied industries such as shipbuilding. 
Hobart Town became a city on 21 August 1842, and was renamed Hobart from the beginning of 1881.
Geography.
Topography.
Hobart is located on the estuary of the Derwent River in the state's south-east. Geologically Hobart is built predominantly on Jurassic dolerite around the foothills interspersed with smaller areas of Triassic siltstone and Permian mudstone.
Hobart extends along both sides of the Derwent River; on the western shore from the Derwent valley in the north through the flatter areas of Glenorchy which rests on older Triassic sediment and into the hilly areas of New Town, Lenah Valley. Both of these areas rest on the younger Jurassic dolerite deposits, before stretching into the lower areas such as the beaches of Sandy Bay in the south, in the Derwent estuary. South of the Derwent estuary lies Storm Bay and the Tasman Peninsula.
The Eastern Shore also extends from the Derwent valley area in a southerly direction hugging the Meehan Range in the east before sprawling into flatter land in suburbs such as Bellerive. These flatter areas of the eastern shore rest on far younger deposits from the Quaternary. From there the city extends in an easterly direction through the Meehan Range into the hilly areas of Rokeby and Oakdowns, before reaching into the tidal flatland area of Lauderdale.
Hobart has access to a number of beach areas including those in the Derwent estuary itself; Sandy Bay, Cornelian Bay, Nutgrove, Kingston, Bellerive, and Howrah Beaches as well as many more in Frederick Henry Bay such as; Seven Mile, Roaches, Cremorne, Clifton, and Goats Beaches.
Climate.
Hobart has a mild temperate oceanic climate (). The highest temperature recorded was 41.8 °C on 4 January 2013 and the lowest was −2.8 °C on 25 June 1972 and 11 July 1981. Annually, Hobart receives 40.8 clear days. Compared to other major Australian cities, Hobart has the second fewest daily average hours of sunshine, with 5.9 hours per day, Melbourne having the fewest. However, during the summer it has the most hours of daylight of any Australian city, with 15.2 hours on the summer solstice.
Although Hobart itself rarely receives snow during the winter, the adjacent Mount Wellington is often seen with a snowcap. Mountain snow covering has also been known to occur during the other seasons. During the 20th century, the city itself has received snowfalls at sea level on average only once every 15 years; however, outer suburbs lying higher on the slopes of Mount Wellington receive snow more often, owing to cold air masses arriving from Antarctica coupled with them resting at higher altitude. These snow-bearing winds often carry on through Tasmania and Victoria to the Snowy Mountains in northern Victoria and southern New South Wales.
Demographics.
At the 2011 census there were 211,656 people in the greater Hobart area and the City of Hobart local government area had a population of 48,703. According to the 2011 census, approximately 17.9% of greater Hobart's residents were born overseas, commonly the United Kingdom, New Zealand and China.
Most common occupations are Professionals 21.6%, Clerical and Administrative Workers 16.1%, Technicians and Trades Workers 13.8%, Managers 11.5% and Community and Personal Service Workers 10.6%. Median weekly household income was $869, compared with $1,027 nationally.
In the 2011 census, 58.6% of residents specified a Christian religion. Major religious affiliations are Anglican 26.2%, Catholic 20.3%, Uniting Church 3.4%, and Presbyterian and Reformed 1.9%. In addition, 29.3% specified "No Religion" and 8.6% did not answer.
Hobart has a small Mormon community of around 642 (2011), with meetinghouses in Glenorchy, Rosny, and Glen Huon. There is also a synagogue where the Jewish community, of around 111 (2001), or 0.1% of the Hobart population, worships. Hobart has a Bahá'í community, with a Bahá'í Centre of Learning, located within the city.
Economy.
Hobart is a busy seaport. Its economy is heavily reliant on the sea and it serves as the home port for the Antarctic activities of Australia and France. The port loads around 2,000 tonnes of Antarctic cargo a year for the Australian research vessel "Aurora Australis." The city is also a hub for cruise ships during the summer months, with up to 40 such ships docking during the course of the season.
The city also supports many other industries. Major local employers include catamaran builder Incat, zinc refinery Nyrstar, Cascade Brewery and Cadbury's Chocolate Factory, Norske Skog and Wrest Point Casino. The city also supports a host of light industry manufacturers.
Hobart also supports a huge tourist industry. Visitors come to the city to explore its historic inner suburbs and nationally acclaimed restaurants and cafes, as well as its vibrant music and nightlife culture. Tourists also come to visit the massive weekly market in Salamanca Place, as well as to use the city as a base from which to explore the rest of Tasmania.
The last 15–20 years has also seen Hobart's wine industry thrive as many vineyards have developed in countryside areas outside of the city in the Coal River Wine Region and D'Entrecasteaux Channel, including Moorilla Estate at Berriedale one of the most awarded vineyards in Australia.
Antarctic bases.
Hobart is an Antarctic gateway city, with geographical proximity to East Antarctica and the Southern Ocean. Infrastructure is provided by the port of Hobart for scientific research and cruise ships, and Hobart International Airport supports an Antarctic Airlink to Wilkins Runway at Casey Station.
Hobart is the home port for the Australian and French Antarctic programs, and provides port services for other visiting Antarctic nations and Antarctic cruise ships. Antarctic and Southern Ocean expeditions are supported by a specialist cluster offering cold climate products, services and scientific expertise. The majority of these businesses and organisations are members of the Tasmanian polar network, supported in part by the Tasmanian State Government.
Tasmania has a high concentration of Antarctic and Southern Ocean scientists. Hobart is home to the following Antarctic and Southern Ocean scientific institutions:
Distinctive features.
The Royal Tasmanian Botanical Gardens is a popular recreation area a short distance from the City centre. It is the second-oldest Botanic Gardens in Australia and holds extensive significant plant collections.
Mount Wellington, accessible by passing through Fern Tree, is the dominant feature of Hobart's skyline. Indeed, many descriptions of Hobart have used the phrase "nestled amidst the foothills", so undulating is the landscape. At 1,271 metres, the mountain has its own ecosystems, is rich in biodiversity and plays a large part in determining the local weather.
The Tasman Bridge is also a uniquely important feature of the city, connecting the two shores of Hobart and visible from many locations. The Hobart Synagogue is the oldest synagogue in Australia and a rare surviving example of an Egyptian Revival synagogue.
Architecture.
Hobart is known for its well-preserved historic architecture, much of it dating back to the Georgian and Victorian eras, giving the city a distinctly "Old World" feel. For locals, this became a source of discomfiture about the city's convict past, but is now a draw card for tourists. Regions within the city centre, such as Salamanca Place, contain many of the city's heritage-listed buildings. Historic homes and mansions also exist in the suburbs. 
Kelly's Steps were built in 1839 by shipwright and adventurer James Kelly to provide a short-cut from Kelly Street and Arthur Circus in Battery Point to the warehouse and dockyards district of Salamanca Place. In 1835, John Lee Archer designed and oversaw the construction of the sandstone Customs House, facing Sullivans Cove. Completed in 1840, it was used as Tasmania's parliament house, and is now commemorated by a pub bearing the same name (built in 1844) which is frequented by yachtsmen after they have completed the Sydney to Hobart yacht race. 
Hobart is also home to many historic churches. The Scots Church (formerly known as St Andrew's) was built in Bathurst Street from 1834–36, and a small sandstone building within the churchyard was used as the city's first Presbyterian Church. The Salamanca Place warehouses and the Theatre Royal were also constructed in this period. The Greek revival St George's Anglican Church in Battery Point was completed in 1838, and a classical tower, designed by James Blackburn, was added in 1847. St Joseph's was built in 1840. St David's Cathedral, Hobart's first cathedral, was consecrated in 1874.
Hobart has very few high rise buildings in comparison to other Australian cities. This is partly a result of height limits imposed due to Hobart's proximity to Derwent River and Mount Wellington.
Culture.
Arts and entertainment.
Hobart is home to the Tasmanian Symphony Orchestra, which is resident at the Federation Concert Hall on the city's waterfront. It offers a year-round program of concerts and is thought to be one of the finest small orchestras in the world. Hobart also plays host to the University of Tasmania's acclaimed Australian International Symphony Orchestra Institute (AISOI) which brings pre-professional advanced young musicians to town from all over Australia and internationally. The AISOI plays host to a public concert season during the first two weeks of December every year focusing on large symphonic music. Like the Tasmanian Symphony Orchestra, the AISOI uses the Federation Concert Hall as its performing base.
Hobart is home to Australia's oldest theatre, the Theatre Royal, as well as the Playhouse theatre, the Backspace theatre and many smaller stage theatres. It also has three Village Cinema complexes, one each in Hobart CBD, Glenorchy and Rosny, with the possibility of a fourth being developed in Kingston. The State Cinema in North Hobart specialises in arthouse and foreign films.
The city has also long been home to a thriving classical, jazz, folk, punk, hip-hop, electro, metal and rock music scene. Internationally recognised musicians such as metal acts Striborg and Psycroptic, indie-electro bands The Paradise Motel and The Scientists of Modern Music, singer/songwriters Sacha Lucashenko (of The Morning After Girls), Michael Noga (of The Drones), and Monique Brumby, two-thirds of indie rock band Love of Diagrams, post punk band Sea Scouts, blues guitarist Phil Manning (of blues-rock band Chain), power-pop group The Innocents are all successful expatriates. In addition, founding member of Violent Femmes, Brian Ritchie, now calls Hobart home, and has formed a local band, The Green Mist. Ritchie also curates the annual international arts festival MONA FOMA, held at Salamanca Place's waterfront venue, Princes Wharf, Shed No. 1. Hobart hosts many significant festivals including winter's landmark cultural event, the , Australia's premier festival celebration of voice, and Tasmania's biennial international arts festival Ten Days On The Island. Other festivals, including the Hobart Fringe Festival, Hobart Summer Festival, Southern Roots Festival, the Falls Festival in Marion Bay and the Soundscape Festival also capitalise on Hobart's artistic communities.
Hobart is home to the Tasmanian Museum and Art Gallery. The Meadowbank Estate winery and restaurant features a floor mural by Tom Samek, part funded by the Federal Government. The Museum of Old and New Art (MONA) opened in 2011 to coincide with the third annual MONA FOMA festival. The multi-storey MONA gallery was built directly underneath the historic Sir Roy Grounds courtyard house, overlooking the Derwent River. This building serves as the entrance to the MONA Gallery.
Designed by the prolific architect Sir Roy Grounds, the 17-storey Wrest Point Hotel Casino in Sandy Bay, opened as Australia's first legal casino in 1973.
The city's nightlife primarily revolves around Salamanca Place, the waterfront area, Elizabeth St in North Hobart and Sandy Bay, but popular pubs, bars and nightclubs exist around the city as well. Major national and international music events are usually held at the Derwent Entertainment Centre, or the Casino. Popular restaurant strips include Elizabeth Street in North Hobart, and Salamanca Place near the waterfront. These include numerous ethnic restaurants including Chinese, Thai, Greek, Pakistani, Italian, Indian and Mexican. The major shopping street in the CBD is Elizabeth Street, with the pedestrianised Elizabeth Mall and the General Post Office.
Events.
Hobart is internationally famous among the yachting community as the finish of the Sydney to Hobart Yacht Race which starts in Sydney on Boxing Day (the day after Christmas Day). The arrival of the yachts is celebrated as part of the Hobart Summer Festival, a food and wine festival beginning just after Christmas and ending in mid-January. The Taste of Tasmania is a major part of the festival, where locals and visitors can taste fine local and international food and wine.
The city is the finishing point of the Targa Tasmania rally car event, which has been held annually in April since 1991.
The annual Tulip Festival at the Royal Tasmanian Botanical Gardens is a popular Spring celebration in the City.
The Australian Wooden Boat Festival is a bi-annual event held in Hobart celebrating wooden boats. It is held concurrently with the Royal Hobart Regatta, which began in 1830 and is therefore Tasmania's oldest surviving sporting event.
Sport.
Most of Hobart's sporting teams in national competitions are statewide teams rather than exclusively city teams.
Cricket is the most popular game of the city. The Tasmanian Tigers cricket team plays its home games at the Bellerive Oval on the Eastern Shore. A new team, Hobart Hurricanes represent the city in the Big Bash League. Bellerive Oval has been the breeding ground of some world class cricket players including the former Australia captain Ricky Ponting.
Despite Australian rules football's huge popularity in the state of Tasmania, the state does not have a team in the Australian Football League. However, a bid for an Tasmanian AFL team is a popular topic among football fans. The State government is one of the potential sponsors of such a team.
Local domestic club football is still played. Tasmanian State League football features five clubs from Hobart, and other leagues such as Southern Football League and the Old Scholars Football Association are also played each Winter.
The city has two local rugby league football teams (Hobart Tigers and South Hobart Storm) who compete in the Tasmanian Rugby League.
Tasmania is not represented by teams in the NRL, Super Rugby, netball, soccer, or basketball leagues. However, the "Oasis Hobart Chargers" team does represent Hobart in the South East Australian Basketball League. Besides the bid for an AFL club which was passed over in favour of a second Queensland team, despite several major local businesses and the Premier pioneering for a club, there is also a Hobart bid for entry into the A-League.
Hockey Tasmania has a men's team (the Tasmanian Tigers) and a women's team (the Van Demons) competing in the Australian Hockey League.
Media.
Five free-to-air television stations service Hobart:
Each station broadcasts a primary channel and several multichannels.
The majority of pay television services are provided by Foxtel via satellite, although other smaller pay television providers do service Hobart.
Commercial radio stations licensed to cover the Hobart market include 100.9 Sea FM and 7HO FM. Local community radio stations include Christian radio station Ultra106five, Edge Radio and 92FM which targets the wider community with specialist programmes. The five ABC radio networks available on analogue radio broadcast to Hobart via 936 ABC Hobart, Radio National, Triple J, NewsRadio and ABC Classic FM.
Hobart's major newspaper is "The Mercury", which was founded by John Davies in 1854 and has been continually published ever since. The paper is currently owned and operated by Rupert Murdoch's News Limited.
Government.
The Greater Hobart metropolitan area consists of five local government areas of which three, City of Hobart, City of Glenorchy and City of Clarence are designated as cities. Hobart also includes the urbanised local governments of the Municipality of Kingborough and Municipality of Brighton. Each local government services all the suburbs that are within its geographical boundaries and are responsible for their own urban area, up to a certain scale, and residential planning as well as waste management and mains water storage.
Most city wide events such as the Taste of Tasmania and Hobart Summer Festival are funded by the Tasmanian State Government as a joint venture with the Hobart City Council. Urban planning of the Hobart CBD in particular the Heritage listed areas such as Sullivans Cove are also intensely scrutinised by State Government, which is operated out of Parliament House on the waterfront.
Education.
Hobart is home to the main campus of the University of Tasmania, situated in Sandy Bay. On-site accommodation colleges include Christ College, Jane Franklin Hall and St John Fisher College. Other campuses are in Launceston and Burnie.
The G.H.A (Greater Hobart Area) contains 122 Primary, Secondary and Pretertiary (College) schools distributed throughout Clarence, Glenorchy and Hobart City Councils and Kingborough and Brighton Municipalities. These schools are made up of a mix of public, catholic, private and independent run, with the heaviest distribution lying in the more densely populated West around the Hobart city core. The city also maintains a large Polytechnics College campus (formerly TAFE Tasmania) for post-secondary studies in Trades and other non-university qualifications.
Infrastructure.
The only public transportation within the city of Hobart is via a network of Metro Tasmania buses funded by
the Tasmanian Government and a small number of private bus services. Like many large Australian cities, Hobart once operated passenger tram services, a trolleybus network consisting of six routes which operated until 1968. However, the tramway closed in the early 1960s. The tracks are still visible in the older streets of Hobart.
Suburban passenger trains, run by the Tasmanian Government Railways, were closed in 1974 and the intrastate passenger service, the Tasman Limited, ceased running in 1978. Recently though there has been a push from the city, and increasingly from government, to establish a light rail network, intended to be fast, efficient, and eco-friendly, along existing tracks in a North South corridor; to help relieve the frequent jamming of traffic in Hobart CBD.
The main arterial routes within the urban area are the Brooker Highway to Glenorchy and the northern suburbs, the Tasman Bridge and Bowen Bridge across the river to Rosny and the Eastern Shore. The East Derwent Highway to Lindisfarne, Geilston Bay, and Northwards to Brighton, the South Arm Highway leading to Howrah, Rokeby, Lauderdale and Opossum Bay and the Southern Outlet south to Kingston and the D'Entrecasteaux Channel. Leaving the city, motorists can travel the Lyell Highway to the west coast, Midland Highway to Launceston and the north, Tasman Highway to the east coast, or the Huon Highway to the far south.
Ferry services from Hobart's Eastern Shore into the city were once a common form of public transportation, but with lack of government funding, as well as a lack of interest from the private sector, there has been the demise of a regular commuter ferry service – leaving Hobart's commuters relying solely on travel by automobiles and buses. There is however a water taxi service operating from the Eastern Shore into Hobart which provides an alternative to the Tasman Bridge.
Hobart is served by Hobart International Airport with flights to/from Melbourne (Qantas, Virgin Australia, Jetstar Airways and Tiger Airways Australia); Sydney (Qantas, Jetstar and Virgin); Brisbane (Virgin); Gold Coast (Jetstar); and Canberra (Virgin). The smaller Cambridge Aerodrome mainly serves small charter airlines offering local tourist flights. In the past decade, Hobart International Airport received a huge upgrade, with the airport now being a first class airport facility.
In 2009, it was announced that Hobart Airport would receive more upgrades, including a first floor, aerobridges (currently, passengers must walk on the tarmac) and shopping facilities. Possible new international flights to Asia and New Zealand, and possible new domestic flights to Darwin, Cairns and Perth have been proposed. A second runway, possibly to be constructed in the next 15 years, would assist with growing passenger numbers to Hobart. Hobart Control Tower may be renovated and fitted with new radar equipment, and the airport's carpark may be extended further. Also, new facilities will be built just outside the airport. A new service station, hotel and day care centre have already been built and the road leading to the airport has been maintained and re-sealed. In addition, Tony Abbott the Prime minister of Australia promised in the lead up to the 2013 federal election that his government would provide the funding needed for an extension of the one and only runway at Hobart international. This would allow larger planes to land which could boost the economy.

</doc>
<doc id="13700" url="http://en.wikipedia.org/wiki?curid=13700" title="Hesiod">
Hesiod

Hesiod ( or ; Greek: Ἡσίοδος "Hēsíodos") was a Greek poet generally thought by scholars to have been active between 750 and 650 BC, around the same time as Homer. His is the first European poetry in which the poet regards himself as a topic, an individual with a distinctive role to play. Ancient authors credited Hesiod and Homer with establishing Greek religious customs. Modern scholars refer to him as a major source on Greek mythology, farming techniques, early economic thought (he is sometimes identified as the first economist), archaic Greek astronomy and ancient time-keeping.
Life.
The dating of his life is a contested issue in scholarly circles and it is covered below in Dating.
Hesiod was the poet who told the story of Pandora's Jar in approximately 650 BC. Epic narrative allowed poets like Homer no opportunity for personal revelations. However, Hesiod's extant work comprises didactic poems in which he went out of his way to let his audience in on a few details of his life. There are three explicit references in "Works and Days", as well as some passages in his "Theogony" that support inferences made by scholars. We learn in the former poem that his father came from Cyme in Aeolis (on the coast of Asia Minor, a little south of the island Lesbos), and crossed the sea to settle at a hamlet, near Thespiae in Boeotia, named Ascra, "a cursed place, cruel in winter, hard in summer, never pleasant" ("Works", l. 640). Hesiod's patrimony there, a small piece of ground at the foot of Mount Helicon, occasioned lawsuits with his brother Perses, who seems at first to have cheated him of his rightful share thanks to corrupt authorities or "kings" but later became impoverished and ended up scrounging on the thrifty poet ("Works" l. 35, 396). 
Unlike their father, Hesiod was averse to sea travel, but he once crossed the narrow strait between the Greek mainland and Euboea to participate in funeral celebrations for one Athamas of Chalcis, and there he won a tripod in a singing competition. He also describes a meeting between himself and the Muses on Mount Helicon, where he had been pasturing sheep when the goddesses presented him with a laurel staff, a symbol of poetic authority ("Theogony", ll. 22–35) Fanciful though the story might seem, the account has led ancient and modern scholars to infer that he did not play the lyre, or that he was not professionally trained, otherwise he would have been presented with a lyre instead.
Some scholars have seen Perses as a literary creation, a foil for the moralizing that Hesiod develops in "Works and Days," but there are also arguments against this theory. For example, it is quite common for works of moral instruction to have an imaginative setting, as a means of getting the audience's attention, but it is difficult to see how Hesiod could have travelled the countryside entertaining people with a narrative about himself if the account was known to be fictitious. Gregory Nagy, on the other hand, sees both "Persēs" ("the destroyer": πέρθω "perthō") and "Hēsiodos" ("he who emits the voice:" ἵημι "hiēmi" and αὐδή "audē") as fictitious names for poetical personae. 
It might seem unusual that Hesiod's father migrated from Asia Minor westwards to mainland Greece, the opposite direction to most colonial movements at the time, and Hesiod himself gives no explanation for it. However around 750 BC, or a little later, there was a migration of seagoing merchants from his original home in Cyme in Asia Minor to Cumae in Campania (a colony they shared with Euboeans), and possibly his move west had something to do with that, since Euboea is not far from Boetia, where he eventually established himself and his family. The family association with Cyme might explain his familiarity with eastern myths, evident in his poems, though the Greek world might have already developed its own versions of them.
In spite of Hesiod's complaints about poverty, life on his father's farm could not have been too uncomfortable if "Works and Days" is anything to judge by, since he describes the routines of prosperous yeomanry rather than peasants. His farmer employs a friend (l. 370) as well as servants (ll. 502, 573, 597, 608, 766), an energetic and responsible ploughman of mature years (ll. 469–71), a slave boy to cover the seed (ll. 441–6), a female servant to keep house (ll. 405, 602) and working teams of oxen and mules (ll. 405, 607f.). One modern scholar surmises that Hesiod may have learned about world geography, especially the catalogue of rivers in "Theogony" (ll. 337–45), listening to his father's accounts of his own sea voyages as a merchant The father probably spoke in the Aeolian dialect of Cyme but Hesiod probably grew up speaking the local Boeotian dialect. However, while his poetry features some Aeolisms there are no words that are certainly Boeotian—he composed in the main literary dialect of the time (Homer's dialect): Ionian.
It is probable that Hesiod wrote his poems down, or dictated them, rather than passed them on orally, as rhapsodes did—otherwise the pronounced personality that now emerges from the poems would surely have been diluted through oral transmission from one rhapsode to another. Pausanias asserted that Boeotians showed him an old tablet made of lead on which the Works were engraved. If he did write or dictate, it was perhaps as an aid to memory or because he lacked confidence in his ability to produce poems extempore, as trained rhapsodes could do. It certainly wasn't in a quest for immortal fame since poets in his era had no such notions. However, some scholars suspect the presence of large-scale changes in the text and attribute this to oral transmission. Possibly he composed his verses during idle times on the farm, in the spring before the May harvest or the dead of winter.
The personality behind the poems is unsuited to the kind of "aristocratic withdrawal" typical of a rhapsode but is instead "argumentative, suspicious, ironically humorous, frugal, fond of proverbs, wary of women." He was in fact a misogynist of the same calibre as the later poet, Semonides. He resembles Solon in his preoccupation with issues of good versus evil and "how a just and all-powerful god can allow the unjust to flourish in this life". He resembles Aristophanes in his rejection of the idealised hero of epic literature in favour of an idealised view of the farmer. Yet the fact that he could eulogise kings in "Theogony" (ll. 80ff, 430, 434) and denounce them as corrupt in "Works and Days" suggests that he could resemble whichever audience he composed for.
Various legends accumulated about Hesiod and they are recorded in several sources: 
Two different—yet early—traditions record the site of Hesiod's grave. One, as early as Thucydides, reported in Plutarch, the "Suda" and John Tzetzes, states that the Delphic oracle warned Hesiod that he would die in Nemea, and so he fled to Locris, where he was killed at the local temple to Nemean Zeus, and buried there. This tradition follows a familiar ironic convention: the oracle that predicts accurately after all. The other tradition, first mentioned in an epigram by Chersias of Orchomenus written in the 7th century BC (within a century or so of Hesiod's death) claims that Hesiod lies buried at Orchomenus, a town in Boeotia. According to Aristotle's "Constitution of Orchomenus," when the Thespians ravaged Ascra, the villagers sought refuge at Orchomenus, where, following the advice of an oracle, they collected the ashes of Hesiod and set them in a place of honour in their "agora", next to the tomb of Minyas, their eponymous founder. Eventually they came to regard Hesiod too as their "hearth-founder" (οἰκιστής / "oikistēs"). Later writers attempted to harmonize these two accounts.
Dating.
Greeks in the late fifth and early 4th centuries BC considered their oldest poets to be Orpheus, Musaeus, Hesiod and Homer—in that order. Thereafter, Greek writers began to consider Homer earlier than Hesiod. Devotees of Orpheus and Musaeus were probably responsible for precedence being given to their two cult heroes and maybe the Homeridae were responsible in later antiquity for promoting Homer at Hesiod's expense.
The first known writers to locate Homer earlier than Hesiod were Xenophanes and Heraclides Ponticus, though Aristarchus of Samothrace was the first actually to argue the case. Ephorus made Homer a younger cousin of Hesiod, the 5th century BC historian Herodotus ("Histories", 2.53) evidently considered them near-contemporaries, and the 4th century BC sophist Alcidamas in his work "Mouseion" even brought them together for an imagined poetic "agon", which survives today as the "Contest of Homer and Hesiod". Most scholars today agree with Homer's priority but there are good arguments on either side.
Hesiod certainly predates the lyric and elegiac poets whose work has come down to the modern era. Imitations of his work have been observed in Alcaeus, Epimenides, Mimnermus, Semonides, Tyrtaeus and Archilochus, from which it has been inferred that the latest possible date for him is about 650 BC.
An upper limit of 750 BC is indicated by a number of considerations, such as the probability that his work was written down, the fact that he mentions a sanctuary at Delphi that was of little national significance before c. 750 BC ("Theogony" l. 499), and he lists rivers that flow into the Euxine, a region explored and developed by Greek colonists beginning in the 8th century BC. ("Theogony" 337–45).
Hesiod mentions a poetry contest at Chalcis in Euboea where the sons of one Amphidamas awarded him a tripod ("Works and Days" ll.654–662). Plutarch identified this Amphidamas with the hero of the Lelantine War between Chalcis and Eretria and he concluded that the passage must be an interpolation into Hesiod's original work, assuming that the Lelantine War was too late for Hesiod. Modern scholars have accepted his identification of Amphidamas but disagreed with his conclusion. The date of the war is not known precisely but estimates placing it around 730–705 BC, fit the estimated chronology for Hesiod. In that case, the tripod that Hesiod won might have been awarded for his rendition of "Theogony", a poem that seems to presuppose the kind of aristocratic audience he would have met at Chalcis.
Works.
Three works have survived which are attributed to Hesiod by ancient commentators: "Works and Days", "Theogony", and "Shield of Heracles". Other works attributed to him are only found now in fragments. The surviving works and fragments were all written in the conventional metre and language of epic. However, the "Shield of Heracles" is now known to be spurious and probably was written in the sixth century BC. Many ancient critics also rejected "Theogony" (e.g., Pausanias 9.31.3) but that seems rather perverse since Hesiod mentions himself by name in that poem (line 22). "Theogony" and "Works and Days" might be very different in subject matter, but they share a distinctive language, metre, and prosody that subtly distinguish them from Homer's work and from the "Shield of Heracles" (see Hesiod's Greek below). Moreover, they both refer to the same version of the Prometheus myth. Yet even these authentic poems may include interpolations. For example, the first ten verses of the "Works and Days" may have been borrowed from an Orphic hymn to Zeus (they were recognised as not the work of Hesiod by critics as ancient as Pausanias).
Some scholars have detected a proto-historical perspective in Hesiod, a view rejected by Paul Cartledge, for example, on the grounds that Hesiod advocates a not-forgetting without any attempt at verification. Hesiod has also been considered the father of gnomic verse. He had ""a passion for systematizing and explaining things". Ancient Greek poetry in general had strong philosophical tendencies and Hesiod, like Homer, demonstrates a deep interest in a wide range of 'philosophical' issues, from the nature of divine justice to the beginnings of human society. Aristotle ("Metaphysics" 983b–987a) believed that the question of first causes may even have started with Hesiod ("Theogony" 116–53) and Homer ("Iliad" 14.201, 246).
He viewed the world from outside the charmed circle of aristocratic rulers, protesting against their injustices in a tone of voice that has been described as having a "grumpy quality redeemed by a gaunt dignity"" but, as stated in the biography section, he could also change to suit the audience. This ambivalence appears to underlie his presentation of human history in "Works and Days", where he depicts a golden period when life was easy and good, followed by a steady decline in behaviour and happiness through the silver, bronze, and Iron Ages – except that he inserts a heroic age between the last two, representing its warlike men as better than their bronze predecessors. He seems in this case to be catering to two different world-views, one epic and aristocratic, the other unsympathetic to the heroic traditions of the aristocracy.
"Theogony".
The "Theogony" is commonly considered Hesiod's earliest work. Despite the different subject matter between this poem and the "Works and Days", most scholars, with some notable exceptions, believe that the two works were written by the same man. As M.L. West writes, "Both bear the marks of a distinct personality: a surly, conservative countryman, given to reflection, no lover of women or life, who felt the gods' presence heavy about him."
The "Theogony" concerns the origins of the world (cosmogony) and of the gods (theogony), beginning with Chaos, Gaia, Tartarus and Eros, and shows a special interest in genealogy. Embedded in Greek myth, there remain fragments of quite variant tales, hinting at the rich variety of myth that once existed, city by city; but Hesiod's retelling of the old stories became, according to Herodotus, the accepted version that linked all Hellenes.
The creation myth in Hesiod has long been held to have Eastern influences, such as the Hittite Song of Kumarbi and the Babylonian Enuma Elis. This cultural crossover would have occurred in the eighth and ninth century Greek trading colonies such as Al Mina in North Syria. (For more discussion, read Robin Lane Fox's "Travelling Heroes" and Walcot's "Hesiod and the Near East.")
"Works and Days".
The "Works and Days" is a poem of over 800 lines which revolves around two general truths: labour is the universal lot of Man, but he who is willing to work will get by. Scholars have interpreted this work against a background of agrarian crisis in mainland Greece, which inspired a wave of documented colonisations in search of new land. This poem is one of the earliest known musings on economic thought.
This work lays out the five Ages of Man, as well as containing advice and wisdom, prescribing a life of honest labour and attacking idleness and unjust judges (like those who decided in favour of Perses) as well as the practice of usury. It describes immortals who roam the earth watching over justice and injustice. The poem regards labor as the source of all good, in that both gods and men hate the idle, who resemble drones in a hive. In the horror of the triumph of violence over hard work and honor, verses describing the "Golden Age" present the social character and practice of nonviolent diet through agriculture and fruit-culture as a higher path of living sufficiently.
Other writings.
In addition to the "Theogony" and "Works and Days", numerous other poems were ascribed to Hesiod during antiquity. Modern scholarship has doubted their authenticity, and these works are generally referred to as forming part of the "Hesiodic Corpus" whether or not their authorship is accepted. The situation is summed up in this formulation by Glenn Most:
"Hesiod" is the name of a person; "Hesiodic" is a designation for a kind of poetry, including but not limited to the poems of which the authorship may reasonably be assigned to Hesiod himself.
Of these works forming the extended Hesiodic corpus, only the "Shield of Heracles" (Ἀσπὶς Ἡρακλέους, "Aspis Hērakleous") is transmitted intact via a medieval manuscript tradition.
Classical authors also attributed to Hesiod a lengthy genealogical poem known as "Catalogue of Women" or "Ehoiai" (because sections began with the Greek words "ē hoiē," "Or like the one who ..."). It was a mythological catalogue of the mortal women who had mated with gods, and of the offspring and descendants of these unions.
Several additional hexameter poems were ascribed to Hesiod:
In addition to these works, the "Suda" lists an otherwise unknown "dirge for Batrachus, [Hesiod's] beloved".
Reception.
Portrait bust.
The Roman bronze bust, the so-called "Pseudo-Seneca," of the late first century BC found at Herculaneum is now thought not to be of Seneca the Younger. It has been identified by Gisela Richter as an imagined portrait of Hesiod. In fact, it has been recognized since 1813 that the bust was not of Seneca, when an inscribed herma portrait of Seneca with quite different features was discovered. Most scholars now follow Richter's identification.
Hesiod's Greek.
Hesiod employed the conventional dialect of epic verse, which was Ionian. Comparisons with Homer, a native Ionian, can be unflattering. Hesiod's handling of the dactylic hexameter was not as masterful or fluent as Homer's and one modern scholar refers to his "hobnailed hexameters". His use of language and meter in "Works and Days" and "Theogony" distinguishes him also from the author of the "Shield of Heracles". All three poets, for example, employed digamma inconsistently, sometimes allowing it to affect syllable length and meter, sometimes not. The ratio of observance/neglect of digamma varies between them. The extent of variation depends on how the evidence is collected and interpreted but there is a clear trend, revealed for example in the following set of statistics.
Hesiod does not observe digamma as often as the others do. That result is a bit counter-intuitive since digamma was still a feature of the Boeotian dialect that Hesiod probably spoke, whereas it had already vanished from the Ionic vernacular of Homer. This anomaly can be explained by the fact that Hesiod made a conscious effort to compose like an Ionian epic poet at a time when digamma was not heard in Ionian speech, while Homer tried to compose like an older generation of Ionian bards, when it was heard in Ionian speech. There is also a significant difference in the results for "Theogony" and "Works and Days", but that is merely due to the fact that the former includes a catalog of divinities and therefore it makes frequent use of the definite article associated with digamma, oἱ.
Though typical of epic, his vocabulary features some significant differences from Homer's. One scholar has counted 278 un-Homeric words in "Works and Days", 151 in "Theogony" and 95 in "Shield of Heracles". The disproportionate number of un-Homeric words in "W & D" is due to its un-Homeric subject matter. Hesiod's vocabulary also includes quite a lot of formulaic phrases that are not found in Homer, which indicates that he may have been writing within a different tradition.

</doc>
<doc id="13702" url="http://en.wikipedia.org/wiki?curid=13702" title="Hebrew numerals">
Hebrew numerals

The system of Hebrew numerals is a quasi-decimal alphabetic numeral system using the letters of the Hebrew alphabet.
The system was adopted from that of the Greek numerals in the late 2nd century BC; it is also known as the "Hebrew alphabetic numerals" to contrast with earlier systems of writing numerals used in classical antiquity. 
These systems were inherited from usage in the Aramaic and Phoenician scripts, attested from c. 800 BC in the so-called Samaria ostraca and sometimes known as "Hebrew-Aramaic numerals", ultimately derived from the Egyptian Hieratic numerals.
The Greek system was adopted in Hellenistic Judaism and had been in use in Greece since about the 5th century BC.
In this system, there is no notation for zero, and the numeric values for individual letters are added together. Each unit (1, 2, ..., 9) is assigned a separate letter, each tens (10, 20, ..., 90) a separate letter, and the first four hundreds (100, 200, 300, 400) a separate letter. The later hundreds (500, 600, 700, 800 and 900) are represented by the sum of two or three letters representing the first four hundreds. To represent numbers from 1,000 to 999,999, the same letters are reused to serve as thousands, tens of thousands, and hundreds of thousands. Gematria (Jewish numerology) uses these transformations extensively.
In Israel today, the decimal system of Arabic numerals (ex. 0, 1, 2, 3, etc.) is used in almost all cases (money, age, date on the civil calendar). The Hebrew numerals are used only in special cases, such as when using the Hebrew calendar, or numbering a list (similar to a, b, c, d, etc.), much as Roman numerals are used in the West.
Main table.
Note: For ordinal numbers greater than 10, cardinal numbers are used instead.
Speaking and writing.
Cardinal and ordinal numbers must agree in gender (masculine or feminine; mixed groups are treated as masculine) with the noun they are describing. If there is no such noun (e.g. a telephone number or a house number in a street address), the feminine form is used. Ordinal numbers must also agree in number and definite status like other adjectives. The cardinal number precedes the noun (ex. shlosha yeladim), except for the number one which succeeds it (ex. yeled ehad). The number two is special - shnayim (m.) and shtayim (f.) become shney (m.) and shtey (f.) when followed by the noun they count. For ordinal numbers (numbers indicating position) greater than ten the cardinal is used.
Calculations.
The Hebrew numeric system operates on the additive principle in which the numeric values of the letters are added together to form the total. For example, 177 is represented as קעז which corresponds to 100 + 70 + 7 = 177.
Mathematically, this type of system requires 27 letters (1-9, 10-90, 100-900). In practice the last letter, "tav" (which has the value 400) is used in combination with itself and/or other letters from "kof" (100) onwards, to generate numbers from 500 and above. Alternatively, the 22-letter Hebrew numeral set is sometimes extended to 27 by using 5 "sofit" (final) forms of the Hebrew letters.
Key exceptions.
By convention, the numbers 15 and 16 are represented as ט״ו‎ (9 + 6) and ט״ז‎ (9 + 7), respectively, in order to refrain from using the two-letter combinations י-ה‎ (10 + 5) and י-ו‎ (10 + 6), which are alternate written forms for the Name of God in everyday writing. In the calendar, this manifests every full moon, since all Hebrew months start on a new moon.
Combinations which would spell out words with negative connotations are sometimes avoided by switching the order of the letters. For instance, 744 which should be written as תשמ״ד‎ (meaning "you/it will be destroyed") might instead be written as תשד״מ or תמש״ד (meaning "end to demon").
Gershayim.
Gershayim (U+05F4 in Unicode, and resembling a double quote mark) (sometimes erroneously referred to as "merkha'ot", which is Hebrew for double quote) are inserted before (to the right of) the last (leftmost) letter to indicate that the sequence of letters represents a number rather than a word. This is used in the case where a number is represented by two or more Hebrew numerals ("e.g.," 28 → כ״ח‎).
Similarly, a single Geresh (U+05F3 in Unicode, and resembling a single quote mark) is appended after (to the left of) a single letter to indicate that the letter represents a number rather than a (one-letter) word. This is used in the case where a number is represented by a single Hebrew numeral ("e.g.," 100 → ק׳‎).
Note that Geresh and Gershayim merely indicate ""not a (normal) word." Context usually determines whether they indicate a number or something else (such as "abbreviation"").
Decimals.
In print, Hindu-Arabic numerals are employed in Modern Hebrew for most purposes. Hebrew numerals are used nowadays primarily for writing the days and years of the Hebrew calendar; for references to traditional Jewish texts (particularly for Biblical chapter and verse and for Talmudic folios); for bulleted or numbered lists (similar to "A", "B", "C", "etc.", in English); and in numerology (gematria).
Thousands and date formats.
Thousands are counted separately, and the thousands count precedes the rest of the number (to the "right", since Hebrew is read from right to left). There are no special marks to signify that the “count” is starting over with thousands, which can theoretically lead to ambiguity, although a single quote mark is sometimes used after the letter. When specifying years of the Hebrew calendar in the present millennium, writers usually omit the thousands (which is presently 5 [ה]), but if they do not this is accepted to mean 5 * 1000, with no ambiguity. The current Israeli coinage includes the thousands.
Date examples.
“Monday, 15 Adar 5764” (where 5764 = 5(×1000) + 400 + 300 + 60 + 4, and 15 = 9 + 6):
“Thursday, 3 Nisan 5767” (where 5767 = 5(×1000) + 400 + 300 + 60 + 7):
To see how "today's" date in the Hebrew calendar is written, see, for example, .
Recent years.
5780 (2019–20) = תש״פ
5779 (2018–19) = תשע״ט
5772 (2011–12) = תשע״ב
5771 (2010–11) = תשע״א
5770 (2009–10) = תש״ע
5769 (2008–09) = ‫תשס״ט
5761 (2000–01) = ‫תשס״א
5760 (1999–00) = ‫תש״ס
Similar systems.
The Abjad numerals are equivalent to the Hebrew numerals up to 400. The Greek numerals differ from the Hebrew ones from 90 upwards because in the Greek alphabet there is no equivalent for "Tsadi" (צ).

</doc>
<doc id="13703" url="http://en.wikipedia.org/wiki?curid=13703" title="Hill system">
Hill system

The Hill system (or Hill notation) is a system of writing chemical formulas such that the number of carbon atoms in a molecule is indicated first, the number of hydrogen atoms next, and then the number of all other chemical elements subsequently, in alphabetical order of the chemical symbols. When the formula contains no carbon, all the elements, including hydrogen, are listed alphabetically.
By sorting formulas according to the number of atoms of each element present in the formula according to these rules, with differences in earlier elements or numbers being treated as more significant than differences in any later element or number — like sorting text strings into lexicographical order — it is possible to collate chemical formulas into what is known as Hill system order. 
The Hill system was first published by Edwin A. Hill of the United States Patent and Trademark Office in 1900. It is the most commonly used system in chemical databases and printed indexes to sort lists of compounds.
Example.
The following formulas are written using the Hill system, and listed in Hill order:
A list of formulas in Hill system order is arranged alphabetically, as above, with single-letter elements coming before two-letter symbols when the symbols begin with the same letter (so B comes before Be, which comes before Br).

</doc>
<doc id="13704" url="http://en.wikipedia.org/wiki?curid=13704" title="Hydroxy">
Hydroxy

Hydroxy can refer to:

</doc>
<doc id="13706" url="http://en.wikipedia.org/wiki?curid=13706" title="Hero">
Hero

A hero (masculine or gender-neutral) or heroine (feminine) (Ancient Greek: ἥρως, "hḗrōs") is a person or character who, in the face of danger and adversity or from a position of weakness, displays courage or self-sacrifice—that is, heroism—for some greater good. Historically, the first heroes displayed courage or excellence as warriors. The word's meaning was later extended to include moral excellence.
The word can be used as a gender-neutral term for both males and females because it has no gender-specific suffix in English.
Stories of heroism may serve as moral examples. In classical antiquity, cults that venerated deified heroes such as Heracles, Perseus, and Achilles played an important role in Ancient Greek religion. Politicians, ancient and modern, have employed hero worship for their own apotheosis (i.e., cult of personality). Stories of the antihero also play a major role in Greek mythology and much of literature. The antihero is a protagonist who lacks the typical characteristics of heroism, such as honor, nobility, bravery, compassion, and fortitude. The favorite type of antihero is an individual who lacks moral character.
Etymology.
The word "hero" comes from the Greek ἥρως ("hērōs"), "hero, warrior", literally "protector" or "defender". Before the decipherment of Linear B the original form of the word was assumed to be *ἥρωϝ-, "hērōw-"; R. S. P. Beekes has proposed a Pre-Greek origin.
According to the "American Heritage Dictionary of the English Language," the Indo-European root is "*ser" meaning "to protect". According to Eric Partridge in "Origins," the Greek word "Hērōs" "is akin to" the Latin "seruāre," meaning "to safeguard". Partridge concludes, "The basic sense of both Hera and hero would therefore be 'protector'."
Heroic monomyth.
The concept of a story archetype of the standard "hero's quest" or monomyth pervasive across all cultures is somewhat controversial. Expounded mainly by Joseph Campbell in "The Hero with a Thousand Faces" (published in 1949), it illustrates several uniting themes of hero stories that hold similar ideas of what a hero represents, despite vastly different cultures and beliefs. The monomyth or Hero's Journey consists of three separate stages including the Departure, Initiation, and Return. Within these stages there are several archetypes that the hero or heroine may follow including the call to adventure (which they may initially refuse), supernatural aid, proceeding down a road of trials, achieving a realization about themselves (or an apotheosis), and attaining the freedom to live through their quest or journey. Campbell offered examples of stories with similar themes such as Krishna, Buddha, Apollonius of Tyana, and Jesus. In his 1968 book, "The Masks of God: Occidental Mythology", Campbell writes "It is clear that, whether accurate or not as to biographical detail, the moving legend of the Crucified and Risen Christ was fit to bring a new warmth, immediacy, and humanity, to the old motifs of the beloved Tammuz, Adonis, and Osiris cycles."
Examples of Campbell's formula can be found in modern stories such as "The Lord of the Rings", "The Matrix", and "Star Wars".
Mythic hero archetype.
The "Mythic Hero Archetype" is a set of 22 common traits shared by many heroes in various cultures, myths and religions throughout history and around the world. The concept was first developed by FitzRoy Somerset, 4th Baron Raglan (Lord Raglan) in his 1936 book, "The Hero, A Study in Tradition, Myth and Drama". Raglan argued that the higher the score, the more likely the figure is mythical. Otto Rank and Alan Dundes later elaborated on the list:
Dundes offered the following list of top ten figures who best matched the archetype along with their scores of 22 when he appeared in the documentary "The God Who Wasn't There".
Lord Raglan did not score Jesus as agreed with his publisher, but contemporary author Robert M. Price argues that the high score among otherwise mythical figures supports the Christ myth theory.
Classical hero cults.
When Cleisthenes divided the ancient Athenians into new "demes" for voting, he consulted the Oracle of Delphi about what heroes he should name each division after. According to Herodotus, the Spartans attributed their conquest of Arcadia to their theft of the bones of Orestes from the Arcadian town of Tegea.
Heroes in myth often had close but conflicted relationships with the gods. Thus Heracles's name means "the glory of Hera", even though he was tormented all his life by Hera, the Queen of the Gods. Perhaps the most striking example is the Athenian king Erechtheus, whom Poseidon killed for choosing Athena over him as the city's patron god. When the Athenians worshiped Erechtheus on the Acropolis, they invoked him as "Poseidon Erechtheus".
In the Hellenistic Greek East, dynastic leaders such as the Ptolemies or Seleucids were also proclaimed heroes.
Validity of the hero in historical studies.
The philosopher Hegel gave a central role to the "hero", personalized by Napoleon, as the incarnation of a particular culture's "Volksgeist", and thus of the general "Zeitgeist". Thomas Carlyle's 1841 "On Heroes, Hero Worship and the Heroic in History" also accorded a key function to heroes and great men in history. Carlyle centered history on the biography of a few central individuals such as Oliver Cromwell or Frederick the Great. His heroes were political and military figures, the founders or topplers of states. His history of great men, of geniuses good and evil, sought to organize change in the advent of greatness.
Explicit defenses of Carlyle's position were rare in the second part of the 20th century. Most philosophers of history contend that the motive forces in history can best be described only with a wider lens than the one he used for his portraits. For example, Karl Marx argued that history was determined by the massive social forces at play in "class struggles", not by the individuals by whom these forces are played out. After Marx, Herbert Spencer wrote at the end of the 19th century: "You must admit that the genesis of the great man depends on the long series of complex influences which has produced the race in which he appears, and the social state into which that race has slowly grown...Before he can remake his society, his society must make him."
As Michel Foucault pointed out in his analysis of societal communication and debate, history was mainly the "science of the sovereign", until its inversion by the "historical and political popular discourse".
The Annales School, led by Lucien Febvre, Marc Bloch and Fernand Braudel, would contest the exaggeration of the role of individual subjects in history. Indeed, Braudel distinguished various time scales, one accorded to the life of an individual, another accorded to the life of a few human generations, and the last one to civilizations, in which geography, economics and demography play a role considerably more decisive than that of individual subjects. Foucault's conception of an "archeology" "(not to be confused with the anthropological discipline of archaeology)" or Louis Althusser's work were attempts at linking together these various heterogeneous layers composing history.
Among noticeable events in the studies of the role of the hero and Great man in history one should mention Sydney Hook's book "The Hero in History".
In the epoch of globalization an individual can still change the development of the country and of the whole world so this gives reasons to some scholars to suggest returning to the problem of the role of the hero in history from the viewpoint of modern historical knowledge and using up-to-date methods of historical analysis.
Within the frameworks of developing counterfactual history, attempts are made to examine some hypothetical scenarios of historical development. And the hero attracts much attention because most of those scenarios are based on the suppositions: what would have happened if this or that historical individual had or had not been alive.
Folk and fairy tales.
Vladimir Propp, in his analysis of the Russian fairy tale, concluded that a fairy tale had only eight "dramatis personæ", of which one was the hero,:p. 80 and his analysis has been widely applied to non-Russian folklore. The actions that fall into such a hero's sphere include:
He distinguished between "seekers" and "victim-heroes". A villain could initiate the issue by kidnapping the hero or driving him out; these were victim-heroes. On the other hand, an antagonist could rob the hero, or kidnap someone close to him, or, without the villain's intervention, the hero could realize that he lacked something and set out to find it; these heroes are seekers. Victims may appear in tales with seeker heroes, but the tale does not follow them both.:36 
The modern fictional hero.
The word "hero" or "heroine" is sometimes used simply to describe the protagonist of a story, or the love interest, a usage which can conflict with the superhuman expectations of heroism. William Makepeace Thackeray gave "Vanity Fair" the subtitle "A Novel without a Hero". The larger-than-life hero is a more common feature of fantasy (particularly sword and sorcery and epic fantasy) than more realist works.
Hero as self.
Roma Chatterji has suggested that the hero or more generally protagonist is first and foremost a symbolic representation of the person who is experiencing the story while reading, listening or watching; thus the relevance of the hero to the individual relies a great deal on how much similarity there is between the two. One reason for the hero-as-self interpretation of stories and myths is the human inability to view the world from any perspective but a personal one.
Psychology of heroism.
Social psychology has begun paying attention to heroes and heroism. Zeno Franco and Philip Zimbardo point out differences between heroism and altruism, and they offer evidence that observers' perceptions of unjustified risk plays a role, above and beyond risk type, in determining the ascription of heroic status.
An evolutionary psychology explanation for heroic risk-taking is that it is a costly signal demonstrating the ability of the hero. It can be seen as one form of altruism for which there are also several other evolutionary explanations.
Further reading.
</dl>

</doc>
<doc id="13711" url="http://en.wikipedia.org/wiki?curid=13711" title="Hydroxide">
Hydroxide

Hydroxide is a diatomic anion with chemical formula OH−. It consists of an oxygen and a hydrogen atom held together by a covalent bond, and carries a negative electric charge. It is an important but usually minor constituent of water. It functions as a base, a ligand, a nucleophile and a catalyst. The hydroxide ion forms salts, some of which dissociate in aqueous solution, liberating solvated hydroxide ions. Sodium hydroxide is a multi-million-ton per annum commodity chemical. A hydroxide attached to a strongly electropositive center may itself ionize, liberating a hydrogen cation (H+), making the parent compound an acid.
The corresponding electrically neutral compound •HO is the hydroxyl radical. The corresponding covalently-bound group -OH of atoms is the hydroxyl group.
Hydroxide ion and hydroxyl group are nucleophiles and can act as a catalyst in organic chemistry.
Many inorganic substances which bear the word "hydroxide" in their names are not ionic compounds of the hydroxide ion, but covalent compounds which contain hydroxyl groups.
Hydroxide ion.
The hydroxide ion is a natural part of water, because of the self-ionization reaction:
The equilibrium constant for this reaction, defined as 
has a value close to 10−14 at 25 °C, so the concentration of hydroxide ions in pure water is close to 10−7 mol∙dm−3, in order to satisfy the equal charge constraint. The pH of a solution is equal to the decimal cologarithm of the hydrogen cation concentration; the pH of pure water is close to 7 at ambient temperatures. The concentration of hydroxide ions can be expressed in terms of pOH, which is close to 14 − pH, so pOH of pure water is also close to 7. Addition of a base to water will reduce the hydrogen cation concentration and therefore increase the hydroxide ion concentration (increase pH, decrease pOH) even if the base does not itself contain hydroxide. For example, ammonia solutions have a pH greater than 7 due to the reaction NH3 + H+ NH4+, which results in a decrease in hydrogen cation concentration and an increase in hydroxide ion concentration. pOH can be kept at a nearly constant value with various buffer solutions.
In aqueous solution the hydroxide ion is a base in the Brønsted–Lowry sense as it can accept a proton from a Brønsted–Lowry acid to form a water molecule. It can also act as a Lewis base by donating a pair of electrons to a Lewis acid. In aqueous solution both hydrogen and hydroxide ions are strongly solvated, with hydrogen bonds between oxygen and hydrogen atoms. Indeed, the bihydroxide ion H3O2− has been characterized in the solid state. This compound is centrosymmetric and has a very short hydrogen bond (114.5 pm) that is similar to the length in the bifluoride ion HF2− (114 pm). In aqueous solution the hydroxide ion forms strong hydrogen bonds with water molecules. A consequence of this is that concentrated solutions of sodium hydroxide have high viscosity due to the formation of an extended network of hydrogen bonds as in hydrogen fluoride solutions.
In solution, exposed to air, the hydroxide ion reacts rapidly with atmospheric carbon dioxide, acting as an acid, to form, initially, the bicarbonate ion. 
The equilibrium constant for this reaction can be specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see carbonic acid for values and details). At neutral or acid pH, the reaction is slow, but is catalyzed by the enzyme carbonic anhydrase, which effectively creates hydroxide ions at the active site.
Solutions containing the hydroxide ion attack glass. In this case, the silicates in glass are acting as acids. Basic hydroxides, whether solids or in solution, are stored in air-tight plastic containers.
The hydroxide ion can function as a typical electron-pair donor ligand, forming such complexes as [Al(OH)4]−. It is also often found in mixed-ligand complexes of the type [MLx(OH)y]z+, where L is a ligand. The hydroxide ion often serves as a bridging ligand, donating one pair of electrons to each of the atoms being bridged. As illustrated by [Pb2(OH)]3+, metal hydroxides are often written in a simplified format. It can even act as a 3 electron-pair donor, as in the tetramer [PtMe3OH]4).
When bound to a strongly electron-withdrawing metal centre, hydroxide ligands tend to ionises into oxide ligands. For example, the bichromate ion [HCrO4]− dissociates according to
with a pKa of about 5.9.
Vibrational spectra.
The infrared spectra of compounds containing the OH functional group have strong absorption bands in the region centered around 3500 cm−1. The high frequency of molecular vibration is a consequence of the small mass of the hydrogen atom as compared to the mass of the oxygen atom and this makes detection of hydroxyl groups by infrared spectroscopy relatively easy. A band due to an OH group tends to be sharp. However, the band width increases when the OH group is involved in hydrogen bonding. A water molecule has an HOH bending mode at about 1600 cm−1, so the absence of this band can be used to distinguish an OH group from a water molecule.
When the OH group is bound to a metal ion in a coordination complex, an M−OH bending mode can be observed. For example, in [Sn(OH)6]2− it occurs at 1065 cm−1. The bending mode for a bridging hydroxide tends to be at a lower frequency as in [(bipyridine)Cu(OH)2Cu(bipyridine)]2+ (955 cm−1). M−OH stretching vibrations occur below about 600 cm−1. For example, the tetrahedral ion [Zn(OH)4]2− has bands at 470 cm−1 (Raman-active, polarized) and 420 cm−1 (infrared). The same ion has an (OH)Zn(OH) bending vibration at 300 cm−1.
Applications.
Sodium hydroxide solutions, also known as lye and caustic soda, are used in the manufacture of pulp and paper, textiles, drinking water, soaps and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes. The principal method of manufacture is the chlor-alkali process.
Solutions containing the hydroxide ion are generated when a salt of a weak acid is dissolved in water. Sodium carbonate is used as an alkali, for example, by virtue of the hydrolysis reaction
Although the base strength of sodium carbonate solutions is lower than a concentrated sodium hydroxide solution, it has the advantage of being a solid. It is also manufactured on a vast scale (42 million tonnes in 2005) by the Solvay process. An example of the use of sodium carbonate as an alkali is when washing soda (another name for sodium carbonate) acts on insoluble esters, such as triglycerides, commonly known as fats, to hydrolyze them and make them soluble.
Bauxite, a basic hydroxide of aluminium, is the principal ore from which the metal is manufactured. Similarly, goethite (α-FeO(OH)) and lepidocrocite (γ-FeO(OH)), basic hydroxides of iron, are among the principal ores used for the manufacture of metallic iron. Numerous other uses can be found in the articles on individual hydroxides. 
Inorganic hydroxides.
Alkali metals.
Aside from NaOH and KOH, which enjoy very large scale applications, the hydroxides of the other alkali metals also are useful. Lithium hydroxide is a strong base, with a pKb of -0.36. Lithium hydroxide is used in breathing gas purification systems for spacecraft, submarines, and rebreathers to remove carbon dioxide from exhaled gas.
The hydroxide of lithium is preferred to that of sodium because of its lower mass. Sodium hydroxide, potassium hydroxide and the hydroxides of the other alkali metals are also strong bases.
Alkaline earth metals.
Beryllium hydroxide Be(OH)2 is amphoteric. The hydroxide itself is insoluble in water, with a solubility product log K*sp of −11.7. Addition of acid gives soluble hydrolysis products, including the trimeric ion [Be3(OH)3(H2O)6]3+, which has OH groups bridging between pairs of beryllium ions making a 6-membered ring. At very low pH the aqua ion [Be(H2O)4]2+ is formed. Addition of hydroxide to Be(OH)2 gives the soluble tetrahydroxo anion [Be(OH)4]2−.
The solubility in water of the other hydroxides in this group increases with increasing atomic number. Magnesium hydroxide Mg(OH)2 is a strong base as are the hydroxides of the heavier alkaline earths, calcium hydroxide, strontium hydroxide and barium hydroxide. A solution/suspension of calcium hydroxide is known as limewater and can be used to test for the weak acid carbon dioxide. The reaction Ca(OH)2 + CO2 Ca2+ + [HCO3]− + OH− illustrates the strong basicity of calcium hydroxide. Soda lime, which is a mixture of NaOH and Ca(OH)2, is used as a CO2 absorbent.
Boron group elements.
The simplest hydroxide of boron B(OH)3, known as boric acid, is an acid. Unlike the hydroxides of the alkali and alkaline earth hydroxides, it does not dissociate in aqueous solution. Instead, it reacts with water molecules acting as a Lewis acid, releasing protons.
A variety of oxyanions of boron are known, which, in the protonated form, contain hydroxide groups.
Aluminium hydroxide Al(OH)3 is amphoteric and dissolves in alkaline solution.
In the Bayer process for the production of pure aluminium oxide from bauxite minerals this equilibrium is manipulated by careful control of temperature and alkali concentration. In the first phase, aluminium dissolves in hot alkaline solution as [Al(OH)4]− but other hydroxides usually present in the mineral, such as iron hydroxides, do not dissolve because they are not amphoteric. After removal of the insolubles, the so-called red mud, pure aluminium hydroxide is made to precipitate by reducing the temperature and adding water to the extract, which, by diluting the alkali, lowers the pH of the solution. Basic aluminium hydroxide AlO(OH), which may be present in bauxite, is also amphoteric.
In mildly acidic solutions the hydroxo complexes formed by aluminium are somewhat different from those of boron, reflecting the greater size of Al(III) vs. B(III). The concentration of the species [Al13(OH)32]7+ is very dependent on the total aluminium concentration. Various other hydroxo complexes are found in crystalline compounds. Perhaps the most important is the basic hydroxide AlO(OH), a polymeric material known by the names of the mineral forms boehmite or diaspore, depending on crystal structure. Gallium hydroxide, indium hydroxide and thallium(III) hydroxides are also amphoteric. Thallium(I) hydroxide is a strong base.
Carbon group elements.
Carbon forms no simple hydroxides. The hypothetical compound C(OH)4 is unstable in aqueous solution:
Carbon dioxide is also known as carbonic anhydride, meaning that it forms by dehydration of carbonic acid H2CO3 (OC(OH)2).
Silicic acid is the name given to a variety of compounds with a generic formula [SiOx(OH)4−2x]n. "Orthosilicic acid" has been identified in very dilute aqueous solution. It is a weak acid with pKa1 = 9.84, pKa2 = 13.2 at 25 °C. It is usually written as H4SiO4 but the formula SiO2(OH)2 is generally accepted . Other silicic acids such as "metasilicic acid" (H2SiO3), "disilicic acid" (H2Si2O5), and "pyrosilicic acid" (H6Si2O7) have been characterized. These acids also have hydroxide groups attached to the silicon; the formulas suggest that these acids are protonated forms of polyoxyanions.
Few hydroxo complexes of germanium have been characterized. Tin(II) hydroxide Sn(OH)2 was prepared in anhydrous media. When tin(II) oxide is treated with alkali the pyramidal hydroxo complex Sn(OH)3− is formed. When solutions containing this ion are acidified the ion [Sn3(OH)4]2+ is formed together with some basic hydroxo complexes. The structure of [Sn3(OH)4]2+ has a triangle of tin atoms connected by bridging hydroxide groups. Tin(IV) hydroxide is unknown but can be regarded as the hypothetical acid from which stannates, with a formula [Sn(OH)6]2−, are derived by reaction with the (Lewis) basic hydroxide ion.
Hydrolysis of Pb2+ in aqueous solution is accompanied by the formation of various hydroxo-containing complexes, some of which are insoluble. The basic hydroxo complex [Pb6O(OH)6]4+ is a cluster of six lead centres with metal-metal bonds surrounding a central oxide ion. The six hydroxide groups lie on the faces of the two external Pb4 tetrahedra. In strongly alkaline solutions soluble plumbate ions are formed, including [Pb(OH)6]2−.
Other main-group elements.
In the higher oxidation states of the elements in groups 5, 6 and 7 there are oxoacids in which the central atom is attached to oxide ions and hydroxide ions. Examples include phosphoric acid H3PO4, and sulfuric acid H2SO4. In these compounds one or more hydroxide groups can dissociate with the liberation of hydrogen cations as in a standard Brønsted–Lowry acid. Many oxoacids of sulfur are known and all feature OH groups that can dissociate.
Telluric acid is often written with the formula H2TeO4·2H2O but is better described structurally as Te(OH)6.
"Ortho"-periodic acid can lose all its protons, eventually forming the periodate ion [IO4]−. It can also be protonated in strongly acidic conditions to give the octahedral ion [I(OH)6]+, completing the isoelectronic series, [E(OH)6]z, E = Sn, Sb, Te, I; z = −2, −1, 0, +1. Other acids of iodine(VII) that contain hydroxide groups are known, in particular in salts such as the "meso"periodate ion that occurs in K4[I2O8(OH)2]·8H2O.
As is common outside of the alkali metals, hydroxides of the elements in lower oxidation states are complicated. For example, phosphorous acid H3PO3 predominantly has the structure OP(H)(OH)2, in equilibrium with a small amount of P(OH)3.
The oxoacids of chlorine, bromine and iodine have the formula O("n"−1)/2A(OH) where "n" is the oxidation number: +1, +3 or +5, and A = Cl, Br or I. The only oxoacid of fluorine is F(OH). When these acids are neutralized the hydrogen atom is removed from the hydroxide group.
Transition and post-transition metals.
The hydroxides of the transition metals and post-transition metals usually have the metal in the +2 (M = Mn, Fe, Co, Ni, Cu, Zn) or +3 (M = Fe, Ru, Rh, Ir) oxidation state. None are soluble in water, and many are poorly defined. One complicating feature of the hydroxides is their tendency to undergo further condensation to the oxides, a process called olation. Hydroxides of metals in the +1 oxidation state are also poorly defined or unstable. For example, silver hydroxide Ag(OH) decomposes spontaneously to the oxide (Ag2O). Copper(I) and gold(I) hydroxides are also unstable, although stable adducts of CuOH and AuOH are known. The polymeric compounds M(OH)2 and M(OH)3 are in general prepared by increasing the pH of an aqueous solutions of the corresponding metal cations until the hydroxide precipitates out of solution. On the converse, the hydroxides dissolve in acidic solution. Zinc hydroxide Zn(OH)2 is amphoteric, forming the zincate ion Zn(OH)42− in strongly alkaline solution.
Numerous mixed ligand complexes of these metals with the hydroxide ion exist. In fact these are in general better defined than the simpler derivatives. Many can be made by deprotonation of the corresponding metal aquo complex.
Vanadic acid H3VO4 shows similarities with phosphoric acid H3PO4 though it has a much more complex vanadate oxoanion chemistry. Chromic acid H2CrO4, has similarities with sulfuric acid H2SO4; for example, both form acid salts A+[HMO4]−. Some metals, e.g. V, Cr, Nb, Ta, Mo, W, tend to exist in high oxidation states. Rather than forming hydroxides in aqueous solution, they convert to oxo clusters by the process of olation, forming polyoxometalates.
Basic salts containing hydroxide.
In some cases the products of partial hydrolysis of metal ion, described above, can be found in crystalline compounds. A striking example is found with zirconium(IV). Because of the high oxidation state, salts of Zr4+ are extensively hydrolyzed in water even at low pH. The compound originally formulated as ZrOCl2·8H2O was found to be the chloride salt of a tetrameric cation [Zr4(OH)8(H2O)16]8+ in which there is a square of Zr4+ ions with two hydroxide groups bridging between Zr atoms on each side of the square and with four water molecules attached to each Zr atom.
The mineral malachite is a typical example of a basic carbonate. The formula, Cu2CO3(OH)2 shows that it is half-way between copper carbonate and copper hydroxide. Indeed, in the past the formula was written as CuCO3·Cu(OH)2. The crystal structure is made up of copper, carbonate and hydroxide ions. The mineral atacamite is an example of a basic chloride. It has the formula, Cu2Cl(OH)3. In this case the composition is nearer to that of the hydroxide than that of the chloride CuCl2·3Cu(OH)2. Copper forms hydroxy phosphate (libethenite), arsenate (olivenite), sulfate (brochantite) and nitrate compounds. White lead is a basic lead carbonate, (PbCO3)2·Pb(OH)2, which has been used as a white pigment because of its opaque quality, though its use is now restricted because it can be a source for lead poisoning.
Structural chemistry.
The hydroxide ion appears to rotate freely in crystals of the heavier alkali metal hydroxides at higher temperatures so as to present itself as a spherical ion, with an effective ionic radius of about 153 pm. Thus, the high-temperature forms of KOH and NaOH have the sodium chloride structure, which gradually freezes in a monocinically distorted sodium chloride structure at temperatures below about 300 °C. The OH groups still rotate even at room temperature around their symmetry axes and, therefore, cannot be detected by X-ray diffraction. The room-temperature form of NaOH has the thallium iodide structure. LiOH, however, has a layered structure, made up of tetrahedral Li(OH)4 and (OH)Li4 units. This is consistent with the weakly basic character of LiOH in solution, indicating that the Li-OH bond has much covalent character.
The hydroxide ion displays cylindrical symmetry in hydroxides of divalent metals Ca, Cd, Mn, Fe, and Co. For example, magnesium hydroxide Mg(OH)2 (brucite) crystallizes with the cadmium iodide layer structure, with a kind of close-packing of magnesium and hydroxide ions.
The amphoteric hydroxide Al(OH)3 has four major crystalline forms: gibbsite (most stable), bayerite, nordstrandite and doyleite. 
All these polymorphs are built up of double layers of hydroxide ions – the aluminium atoms on two-thirds of the octahedral holes between the two layers – and differ only in the stacking sequence of the layers. The structures are similar to the brucite structure. However, whereas the brucite structure can be described as a close-packed structure in gibbsite the OH groups on the underside of one layer rest on the groups of the layer below. This arrangement led to the suggestion that there are directional bonds between OH groups in adjacent layers. This is an unusual form of hydrogen bonding since the two hydroxide ion involved would be expected to point away from each other. The hydrogen atoms have been located by neutron diffraction experiments on αAlO(OH) (diaspore). The O-H-O distance is very short, at 265 pm; the hydrogen is not equidistant between the oxygen atoms and the short OH bond makes an angle of 12° with the O-O line. A similar type of hydrogen bond has been proposed for other amphoteric hydroxides, including Be(OH)2, Zn(OH)2 and Fe(OH)3
A number of mixed hydroxides are known with stoichiometry A3MIII(OH)6, A2MIV(OH)6 and AMV(OH)6. As the formula suggests these substances contain M(OH)6 octahedral structural units. Layered double hydroxides may be represented by the formula [Mz+1−xM3+x(OH)2]q+(Xn−)q/n·"y"H2O. Most commonly, z = 2, and M2+ = Ca2+, Mg2+, Mn2+, Fe2+, Co2+, Ni2+, Cu2+ or Zn2+; hence q = x.
In organic reactions.
Potassium hydroxide and sodium hydroxide are two well-known reagents in organic chemistry.
Base catalysis.
The hydroxide ion may act as a base catalyst. The base abstracts a proton from a weak acid to give an intermediate that goes on to react with another reagent. Common substrates for proton abstraction are alcohols, phenols, amines and carbon acids. The pKa value for dissociation of a C–H bond is extremely high, but the pKa alpha hydrogens of a carbonyl compound are about 3 log units lower. Typical pKa values are 16.7 for acetaldehyde and 19 for acetone. Dissociation can occur in the presence of a suitable base.
The base should have a pKa value not less than about 4 log units smaller or the equilibrium will lie almost completely to the left.
The hydroxide ion by itself is not a strong enough base, but it can be converted in one by adding sodium hydroxide to ethanol
to produce the ethoxide ion. The pKa for self-dissociation of ethanol is about 16 so the alkoxide ion is a strong enough base The addition of an alcohol to an aldehyde to form a hemiacetal is an example of a reaction that can be catalyzed by the presence of hydroxide. Hydroxide can also act as a Lewis-base catalyst.
As a nucleophilic reagent.
The hydroxide ion is intermediate in nucleophilicity between the fluoride ion F−, and the amide ion NH2−. The hydrolysis of an ester
also known as saponification is an example of a nucleophilic acyl substitution with the hydroxide ion acting as a nucleophile. In this case the leaving group is an alkoxide ion, which immediately removes a proton from a water molecule to form an alcohol. In the manufacture of soap, sodium chloride is added to salt out the sodium salt of the carboxylic acid; this is an example of the application of the common-ion effect.
Other cases where hydroxide can act as a nucleophilic reagent are amide hydrolysis, the Cannizzaro reaction, nucleophilic aliphatic substitution, nucleophilic aromatic substitution and in elimination reactions. The reaction medium for KOH and NaOH is usually water but with a phase-transfer catalyst the hydroxide anion can be shuttled into an organic solvent as well, for example in the generation of dichlorocarbene.

</doc>
<doc id="13713" url="http://en.wikipedia.org/wiki?curid=13713" title="H. R. Giger">
H. R. Giger

Hans Rudolf "Ruedi" Giger ( ; ]; 5 February 1940 – 12 May 2014) was a Swiss surrealist painter, sculptor and set designer. He was part of the special effects team that won an Academy Award for Best Achievement in Visual Effects for their design work on the film "Alien". He was named to the Science Fiction and Fantasy Hall of Fame in 2013.
Early life.
Giger was born in 1940 in Chur, capital city of Graubünden, the largest and easternmost Swiss canton. His father, a chemist, viewed art as a "breadless profession" and strongly encouraged him to enter pharmaceutics, Giger recalls. Yet he moved to Zürich in 1962, where he studied Architecture and industrial design at the School of Applied Arts until 1970.
Career.
Giger's style and thematic execution were influential. His design for the Alien was inspired by his painting "Necronom IV" and earned him an Oscar in 1980. His books of paintings, particularly "Necronomicon" and "Necronomicon II" (1985) and the frequent appearance of his art in "Omni" magazine continued his rise to international prominence. Giger is also well known for artwork on several music recording albums.
In 1998 Giger acquired the Château St. Germain in Gruyères, Switzerland, and it now houses the H.R. Giger Museum, a permanent repository of his work.
Personal life.
The artist lived and worked in Zürich with his second wife, Carmen Maria Scheifele Giger, who is the Director of the H.R. Giger Museum.
Giger had a relationship with Swiss actress and muse Li Tobler until she committed suicide in 1975. He married Mia Bonzanigo in 1979; they separated a year and a half later.
Death.
On 12 May 2014, Giger died in a hospital in Zürich after having suffered injuries in a fall.
Style.
Giger started with small ink drawings before progressing to oil paintings. For most of his career, Giger had worked predominantly in airbrush, creating monochromatic canvasses depicting surreal, nightmarish dreamscapes. However, he then largely abandoned large airbrush works in favor of works with pastels, markers or ink.
Giger's most distinctive stylistic innovation was that of a representation of human bodies and machines in a cold, interconnected relationship, he described as "biomechanical". His main influences were painters Dado, Ernst Fuchs and Salvador Dalí. He met Salvador Dalí, to whom he was introduced by painter Robert Venosa. He was also a personal friend of Timothy Leary. Giger studied interior and industrial design at the School of Commercial Art in Zurich (from 1962 to 1965) and made his first paintings as a means of art therapy.
Other works.
In the 1960s and 1970s, Giger directed a number of films, including "Swiss Made" (1968), "Tagtraum" (1973), "Giger's Necronomicon" (1975) and "Giger's Alien" (1979).
Giger created furniture designs, particularly the Harkonnen Capo Chair for a movie of the novel "Dune" that was to be directed by Alejandro Jodorowsky. Many years later, David Lynch directed the film, using only rough concepts by Giger. Giger had wished to work with Lynch, as he stated in one of his books that Lynch's film "Eraserhead" was closer than even Giger's own films to realizing his vision.
Giger applied his biomechanical style to interior design. One "Giger Bar" sprang up in Tokyo, but the realization of his designs were a great disappointment to the artist, since the Japanese organization behind the venture did not wait for his final designs, but decided to move ahead with nothing more than Giger's rough preliminary sketches. For that reason, Giger disowned the Tokyo Giger Bar and never set foot inside. Within a few years, the establishment was out of business. The two Giger Bars in his native Switzerland (in Gruyères and Chur), however, were built under Giger's close personal supervision and reflect his original concepts for them accurately. At The Limelight in Manhattan, Giger's artwork was licensed to decorate the VIP room, the uppermost chapel of the landmarked church, but it was never intended to be a permanent installation and bore no similarity to the real Giger Bars in Switzerland. The arrangement was terminated after two years when the Limelight closed its doors. As of 2009 only the two authentic Swiss Giger Bars remain.
Giger's art has greatly influenced tattooists and fetishists worldwide. Under a licensing deal Ibanez guitars released an H. R. Giger signature series: the Ibanez ICHRG2, an Ibanez Iceman, features "NY City VI", the Ibanez RGTHRG1 has "NY City XI" printed on it, the S Series SHRG1Z has a metal-coated engraving of "Biomechanical Matrix" on it, and a 4-string SRX bass, SRXHRG1, has "N.Y. City X" on it.
Giger is often referred to in popular culture, especially in science fiction and cyberpunk. William Gibson (who wrote an early script for "Alien 3") seems particularly fascinated: A minor character in "Virtual Light", Lowell, is described as having "New York XXIV" tattooed across his back, and in "Idoru" a secondary character, Yamazaki, describes the buildings of nanotech Japan as Giger-esque.

</doc>
<doc id="13714" url="http://en.wikipedia.org/wiki?curid=13714" title="Hispaniola">
Hispaniola

Hispaniola (Spanish: "Española"; French: "Hispaniola"; Taíno: "Ayiti") is the 22nd-largest island in the world, located in the Caribbean island group, the Greater Antilles. It is the second largest island in the Caribbean after Cuba, the tenth most populous island in the world, and the most populous in the Americas.
Two sovereign nations share the 76480 km2 island. The Dominican Republic with 48445 km2 is nearly twice as large as its neighbor, Haiti, which contains 27750 km2.
It is the site of the first European settlement in the Americas founded by Christopher Columbus on his voyages in 1492 and 1493.
History.
Etymology.
The island was called by various names by its native people, the Taíno Amerindians. When Columbus took possession of the island in 1492, he named it "Insula Hispana", meaning "the Spanish Island" in Latin and "La Isla Española", meaning "the Spanish Island", in Spanish. Bartolomé de las Casas shortened the name to "Española", and when Pietro Martyr d‘Anghiera detailed his account of the island in Latin, he translated the name as "Hispaniola".
Gonzalo Fernández de Oviedo and de las Casas documented that the island was called "Haiti" ("Mountainous Land") by the Taíno. D'Anghiera added another name, "Quizqueia" (supposedly "Mother of all Lands"), but later research shows that the word does not seem to derive from the original Arawak Taíno language. Although the Taínos use of "Haiti" is verified and the name was used by all three historians, evidence suggests that it probably was not the Taíno name of the whole island. However, Haiti was the Taíno name of a region (now known as "Los Haitises") in the northeastern section of the present-day Dominican Republic. In the oldest documented map of the island, created by Andrés de Morales, that region is named "Montes de Haití" ("Haiti Mountains"). Las Casas apparently named the whole island Haiti on the basis of that particular region; d'Anghiera said that the name of one part was given to the whole island.
Due to French and Spanish influences on the island, historically the whole island was often referred to as "Haiti," "Hayti," "St. Domingue", "San Domingo" or "Santo Domngo," which didn't favor either side of the island as the locality of these names could have been anywhere on Hispaniola. Only recently, has the term Hispaniola has come into use as a geographic unit. The colonial terms "Saint-Domingue" and "Santo Domingo" are sometimes still applied to the whole island, though these names refer, respectively, to the colonies that became Haiti and the Dominican Republic.
Since Anghiera's literary work was translated into English and French in a short period of time, the name "Hispaniola" became the most frequently used term in English-speaking countries for the island in scientific and cartographic works.
The name "Haïti" was adopted by Haitian revolutionary Jean-Jacques Dessalines as the official name of independent Saint-Domingue, as a tribute to the Amerindian predecessors. "Quisqueya" (from "Quizqueia") although used on both sides of the island is mostly adopted in the Dominican Republic.
Post-Columbian.
Christopher Columbus inadvertently found the island during his first voyage across the Atlantic in 1492, where his flagship, the "Santa Maria", sank after running aground on December 25. A contingent of men were left at an outpost christened La Navidad on the north coast of present-day Haiti. On his return the subsequent year, following the destruction of La Navidad by the local population, Columbus quickly established a second compound farther east in present-day Dominican Republic, La Isabela.
The island was inhabited by the Taíno, one of the indigenous Arawak peoples. The Taino were at first tolerant of Columbus and his crew, and helped him to construct La Navidad on what is now Môle Saint-Nicolas, Haiti, in December 1492. European colonization of the island began in earnest the following year, when 1,300 men arrived from Spain under the watch of Bartolomeo Columbus. In 1496 the town of "Nueva Isabela" was founded. After being destroyed by a hurricane, it was rebuilt on the opposite side of the Ozama River and called Santo Domingo. It is the oldest permanent European settlement in the Americas.
The Taíno population of the island rapidly died, 90% from new infectious diseases, to which they had no immunity.
Harsh enslavement by Spanish colonists resulted in the death of most of the remainder. In 1503 the colony began to import African slaves, believing them more capable of performing physical labor. The natives had no immunity to European diseases, including smallpox, and entire tribes were destroyed. From an estimated initial population of 250,000 in 1492, 14,000 Arawaks survived in 1517.
In 1574, a census taken of the Greater Antilles reported 1,000 Spaniards and 12,000 African slaves on Hispaniola.
As Spain conquered new regions on the mainland of the Americas, its interest in Hispaniola waned, and the colony’s population grew slowly. By the early 17th century, the island and its smaller neighbors (notably Tortuga) became regular stopping points for Caribbean pirates. In 1606, the government of Philip III ordered all inhabitants of Hispaniola to move close to Santo Domingo, to avoid interaction with pirates. Rather than secure the island, his action meant that French, English and Dutch pirates established their own bases on the abandoned north and west coasts of the island.
In 1665, French colonization of the island was officially recognized by King Louis XIV. The French colony was given the name Saint-Domingue. In the 1697 Treaty of Ryswick, Spain formally ceded the western third of the island to France. Saint-Domingue quickly came to overshadow the east in both wealth and population. Nicknamed the "Pearl of the Antilles," it became the richest and most prosperous colony in the West Indies, with a system of human enslavement used to grow and harvest sugar cane, during a time when demand for sugar was high in Europe. Slavery kept prices low and profit was maximized at the expense of human lives. It was an important port in the Americas for goods and products flowing to and from France and Europe.
With the treaty of Peace of Basel, revolutionary France emerged as a major European power. In the second 1795 Treaty of Basel (July 22), Spain ceded the eastern two-thirds of the island of Hispaniola, later to become the Dominican Republic. French settlers had begun to colonize some areas in the Spanish side of the territory.
European colonists often died young due to tropical fevers, as well as from slave resistance in the late eighteenth century. When the French Revolution abolished slavery in the colonies on February 4, 1794, it was a European first, and when Napoleon reimposed slavery in 1802 it led to a major upheaval by the emancipated black slaves.
Thousands succumbed to a yellow fever during the summer months and more than half of the French army died because of disease. After the French removed the surviving 7,000 troops in late 1803, the leaders of the revolution declared the new nation of independent Haiti in early 1804.
Fearing the influence of a society that had successfully fought and won against their enslavers, the United States and European powers refused to recognize Haiti, the second republic in the western hemisphere. In addition, the US maintained an arms and goods embargo against the country during the years of its own conflict with Great Britain. France demanded a high payment for compensation to slaveholders who lost their property, and Haiti was saddled with unmanageable debt for decades. It became one of the poorest countries in the Americas, while the Dominican Republic, whose independence was won via a very different route gradually has developed into the largest economy of Central America and the Caribbean.
Geography.
Hispaniola is the second-largest island in the Caribbean (after Cuba), with an area of 76480 km2, 18704 mi2 of which is under the sovereignty of the Dominican Republic occupying the eastern portion and 10714 mi2 under the sovereignty of Haiti occupying the western portion.
The island of Cuba lies 80 km to the northwest across the Windward Passage; to the southwest lies Jamaica, separated by the Jamaica Channel. Puerto Rico lies east of Hispaniola across the Mona Passage. The Bahamas and Turks and Caicos Islands lie to the north. Its westernmost point is known as Cap Carcasse.
Cuba, Hispaniola, Jamaica, and Puerto Rico are collectively known as the Greater Antilles. The Greater Antilles are made up of continental rock.
The island has five major mountain ranges: The Central Range, known in the Dominican Republic as the "Cordillera Central", spans the central part of the island, extending from the south coast of the Dominican Republic into northwestern Haiti, where it is known as the "Massif du Nord". This mountain range boasts the highest peak in the Antilles, Pico Duarte at 3087 m above sea level. The "Cordillera Septentrional" runs parallel to the Central Range across the northern end of the Dominican Republic, extending into the Atlantic Ocean as the Samaná Peninsula. The "Cordillera Central" and "Cordillera Septentrional" are separated by the lowlands of the Cibao Valley and the Atlantic coastal plains, which extend westward into Haiti as the "Plaine du Nord" (Northern Plain). The lowest of the ranges is the "Cordillera Oriental", in the eastern part of the country.
The "Sierra de Neiba" rises in the southwest of the Dominican Republic, and continues northwest into Haiti, parallel to the "Cordillera Central", as the "Montagnes Noires", "Chaîne des Matheux" and the "Montagnes du Trou d'Eau". "The Plateau Central" lies between the "Massif du Nord" and the "Montagnes Noires", and the "Plaine de l‘Artibonite" lies between the "Montagnes Noires" and the "Chaîne des Matheux", opening westward toward the Gulf of Gonâve, the largest gulf of the Antilles.
The southern range begins in the southwestern most Dominican Republic as the Sierra de Bahoruco, and extends west into Haiti as the Massif de la Selle and the Massif de la Hotte, which form the mountainous spine of Haiti’s southern peninsula. Pic de la Selle is the highest peak in the southern range, the third highest peak in the Antilles and consequently the highest point in Haiti, at 2680 m above sea level. A depression runs parallel to the southern range, between the southern range and the "Chaîne des Matheux"-"Sierra de Neiba". It is known as the "Plaine du Cul-de-Sac" in Haiti, and Haiti’s capital Port-au-Prince lies at its western end. The depression is home to a chain of salt lakes, including Lake Azuei in Haiti and Lake Enriquillo in the Dominican Republic.
The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to 2100 m elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above 850 m elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.
Fauna.
There are many bird species in Hispaniola, and the island's amphibian species are also diverse.
Flora.
The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to 2100 m elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above 850 m elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.
In Haiti, deforestation has long been cited by scientists as a source of ecological crisis; the timber industry dates back to French colonial rule.
Haiti has seen a dramatic reduction of forests due to the excessive and increasing use of charcoal as fuel for cooking, to the point that today less than 2% of Haitian territory is covered by forest. The consequence has been that most Haitians rivers have dried up, its fertile agricultural land has decreased and the Haitian territory is becoming desertified. In the extreme northwest, on the peninsula of San Nicolas, there has emerged the largest desert in the Antilles and it continues to expand due to the widespread deforestation. Most Haitian mountain ranges look totally deforested and in many cases the rains have removed sediment, exposing the rock foundation of the mountains and doing catastrophic irreversible ecological damage.
In the Dominican Republic the forest cover has increased. In 2003 the Dominican forest cover had been reduced to 32% of the territory, but in 2011 the trend towards reducing reverts to increase forest cover by eight percentage points to stand at nearly 40% of territory.. The success of the Dominican forest growth is due to several Dominican government policies and private organizations for the purpose, and a strong educational campaign that has resulted in increased awareness on the Dominican people of the importance of forests for their welfare and in other forms of life on the island.
Climate.
Owing to its mountainous topography, Hispaniola’s climate shows considerable variation over short distance, and is the most varied of all the Antilles.
Except in the Northern Hemisphere summer season, the predominant winds over Hispaniola are the northeast trade winds. As in Jamaica and Cuba, these winds deposit their moisture on the northern mountains and create a distinct rain shadow on the southern coast, where some areas receive as little as 400 mm of rainfall and have semi-arid climates. Annual rainfalls under 600 mm also occur on the southern coast of Haiti’s northwest peninsula and in the central Azúa region of the "Plaine du Cul-de-Sac". In these regions, moreover, there is generally little rainfall outside hurricane season from August to October and droughts are by no means uncommon when hurricanes do not come.
On the northern coast, in contrast, rainfall may peak between December and February, though some rain falls in all months of the year. Annual amounts typically range from 1700 to on the northern coastal lowlands; there is probably much more in the Cordillera Septentrional, though no data exist.
The interior of Hispaniola, along with the southeastern coast centred around Santo Domingo, typically receive around 1400 mm per year with a distinct wet season from May to October. Usually this wet season has two peaks: one around May, the other around the hurricane season. In the interior highlands rainfall is much greater, around 3100 mm per year, but with a similar pattern to that observed in the central lowlands.
As is usual for tropical islands, variations of temperature are much less marked than with rainfall and depend only on altitude. Lowland Hispaniola is generally oppressively hot and humid, with temperatures averaging 28 C with high humidity during the daytime and around 20 C at night. At higher altitudes, temperatures fall steadily, so that frosts occur during the dry season on the highest peaks, where maxima are no higher than 18 C.
Demographics.
The island is divided into two sovereign states: the Dominican Republic, which occupies most of the island and is the heir to the Spanish province of Santo Domingo; and the Republic of Haiti which occupies the western third of the island.
The Dominican Republic is a Hispanophone nation of approximately 10 million people. Spanish is spoken by all Dominicans as a primary language. Catholicism is the dominant religion. 
Haiti is a Francophone nation of roughly 10 million people. Although French is spoken as a primary language by the educated and wealthy minority, virtually the entire population speaks Haitian Creole, one of several French-derived creole languages. Catholicism is the dominant religion. Haiti is the first Black republic in the world and the poorest in Latin America.
Ethnic composition.
The ethnic composition of the Dominican population is 73% Mixed, 16% white and 11% black.
The ethnic composition of Haiti is estimated to be 95% black, 5% white and mixed.
Economics.
The island has the largest economy in the Greater Antilles, however most of the economic development is found in the Dominican Republic, the Dominican economy being nearly 800% larger than the Haitian economy.
The estimated annual per capita income is US$1,300 in Haiti and US$8,200 in Dominican Republic.
The divergence between the level of economic development between Haiti and Dominican Republic makes its border the higher contrast of all western land borders and is evident that the Dominican Republic has one of the highest migration issues in the Americas.

</doc>
<doc id="13717" url="http://en.wikipedia.org/wiki?curid=13717" title="Halle Berry">
Halle Berry

Halle Maria Berry (born Maria Halle Berry; August 14, 1966) is an American actress and former fashion model. She won an Academy Award for Best Actress in 2002 for her performance in the romantic drama "Monster's Ball" (2001), becoming the first and, as of 2015, the only woman of color to win an Oscar for a leading role. She was one of the highest paid actresses in Hollywood during the 2000s and has been involved in the production of several of the films in which she performed. Berry is also a Revlon spokesmodel.
Before becoming an actress, Berry started modeling and entered several beauty contests, finishing as the 1st runner-up in the Miss USA Pageant and coming in 6th place in the Miss World Pageant in 1986. Her breakthrough film role was in the romantic comedy "Boomerang" (1992), alongside Eddie Murphy, which led to roles in films such as the comedy "The Flintstones" (1994), the political comedy-drama "Bulworth" (1998) and the television film "Introducing Dorothy Dandridge" (1999), for which she won the Emmy Award and Golden Globe Award for Best Actress, among many other awards.
In addition to her Academy Award win, Berry garnered high-profile roles in the 2000s such as Storm in the "X-Men" film series (beginning in 2000), the action crime thriller "Swordfish" (2001), and the spy film "Die Another Day" (2002), where she played Bond Girl Jinx. She then appeared in the "X-Men" sequels, "X2: X-Men United" (2003) and ' (2006). In the 2010s, she appeared in movies such as the science fiction film "Cloud Atlas" (2012), the crime thriller "The Call" (2013) and ' (2014).
Divorced from baseball player David Justice and singer-songwriter Eric Benét, Berry has a daughter by model Gabriel Aubry, and a son by actor Olivier Martinez.
Early life.
Berry was born Maria Halle Berry, though her name was legally changed to Halle Maria Berry at the age of five. Berry's parents selected her middle name from Halle's Department Store, which was then a local landmark in her birthplace of Cleveland, Ohio. Her mother, Judith Ann (née Hawkins), who is Caucasian, and has English and German ancestry, was a psychiatric nurse. Her father, Jerome Jesse Berry, was an African-American hospital attendant in the same psychiatric ward where her mother worked; he later became a bus driver. Berry's maternal grandmother, Nellie Dicken, was born in the United Kingdom (Sawley, Derbyshire, England), while her maternal grandfather, Earl Ellsworth Hawkins, was born in Ohio. Berry's parents divorced when she was four years old; she and her older sister, Heidi Berry-Henderson, were raised exclusively by her mother. Berry has said in published reports that she has been estranged from her father since her childhood, noting in 1992, "I haven't heard from him since [he left]. Maybe he's not alive." Her father was very abusive of her mother. Berry has recalled witnessing her mother being beaten daily, kicked down stairs and hit in the head with a wine bottle.
Berry graduated from Bedford High School where she was a cheerleader, honor student, editor of the school newspaper and prom queen. She worked in the children's department at Higbee's Department store. She then studied at Cuyahoga Community College. In the 1980s, she entered several beauty contests, winning Miss Teen All American in 1985 and Miss Ohio USA in 1986. She was the 1986 Miss USA first runner-up to Christy Fichtner of Texas. In the Miss USA 1986 pageant interview competition, she said she hoped to become an entertainer or to have something to do with the media. Her interview was awarded the highest score by the judges. She was the first African-American Miss World entrant in 1986, where she finished sixth and Trinidad and Tobago's Giselle Laronde was crowned Miss World. Berry then traveled to Chicago to pursue a career in modeling.
Career.
Early career.
In 1989, Berry moved to New York City to pursue her acting ambitions. During her early time there, she ran out of money and had to live briefly in a homeless shelter. Later in 1989, her situation improved and she was cast in the role of model Emily Franklin in the short-lived ABC television series "Living Dolls", which was shot in New York and was a spin-off of the hit series "Who's the Boss?". During the taping of "Living Dolls", she lapsed into a coma and was diagnosed with Type 2 diabetes. After the cancellation of "Living Dolls", she moved to Los Angeles. She went on to have a recurring role on the long-running primetime serial "Knots Landing".
Her film debut was in a small role for Spike Lee's "Jungle Fever" (1991), in which she played Vivian, a drug addict. That same year, Berry had her first co-starring role in "Strictly Business". In 1992, Berry portrayed a career woman who falls for the lead character played by Eddie Murphy in the romantic comedy "Boomerang". The following year, she caught the public's attention as a headstrong biracial slave in the TV adaptation of "", based on the book by Alex Haley. Berry was in the live-action "Flintstones" movie playing the part of "Sharon Stone", a sultry secretary who seduced Fred Flintstone.
Berry tackled a more serious role, playing a former drug addict struggling to regain custody of her son in "Losing Isaiah" (1995), starring opposite Jessica Lange. She portrayed Sandra Beecher in "Race the Sun" (1996), which was based on a true story, shot in Australia, and co-starred alongside Kurt Russell in "Executive Decision". Beginning in 1996, she was a Revlon spokeswoman for seven years and renewed her contract in 2004.
Late 1990s–2000s.
She starred alongside Natalie Deselle Reid in the 1997 comedy film "B*A*P*S". In 1998, Berry received praise for her role in "Bulworth" as an intelligent woman raised by activists who gives a politician (Warren Beatty) a new lease on life. The same year, she played the singer Zola Taylor, one of the three wives of pop singer Frankie Lymon, in the biopic "Why Do Fools Fall in Love". In the 1999 HBO biopic "Introducing Dorothy Dandridge", she portrayed the first black woman to be nominated for a Best Actress Academy Award, and it was to Berry a heart-felt project that she introduced, co-produced and fought intensely for it to come through. Berry's performance was recognized with several awards, including an Emmy and a Golden Globe.
Berry portrayed the mutant superhero Storm in the film adaptation of the comic book series "X-Men" (2000) and its sequels, "X2: X-Men United" (2003), ' (2006) and ' (2014). In 2001, Berry appeared in the film "Swordfish", which featured her first topless scene. At first, she refused to be filmed topless in a sunbathing scene, but she changed her mind when Warner Brothers raised her fee substantially. The brief flash of her breasts added $500,000 to her fee. Berry considered these stories to be rumors and was quick to deny them. After turning down numerous roles that required nudity, she said she decided to make "Swordfish" because her then-husband, Eric Benét, supported her and encouraged her to take risks.
She appeared as Leticia Musgrove, the troubled wife of an executed murderer (Sean Combs), in the 2001 feature film "Monster's Ball". Her performance was awarded the National Board of Review and the Screen Actors Guild best-actress prizes; in an interesting coincidence she became the first African American to win the Academy Award for Best Actress (earlier in her career, she portrayed Dorothy Dandridge, the first African American to be nominated for Best Actress, and who was born at the same hospital as Berry, in Cleveland, Ohio). The NAACP issued the statement: "Congratulations to Halle Berry and Denzel Washington for giving us hope and making us proud. If this is a sign that Hollywood is finally ready to give opportunity and judge performance based on skill and not on skin color then it is a good thing." Her role also generated controversy. Berry's graphic nude love scene with a racist character played by co-star Billy Bob Thornton was the subject of much media chatter and discussion among African Americans. Many in the African-American community were critical of Berry for taking the part. Berry responded: "I don't really see a reason to ever go that far again. That was a unique movie. That scene was special and pivotal and needed to be there, and it would be a really special script that would require something like that again."
Berry asked for a higher fee for Revlon advertisements after winning the Academy Award. Ron Perelman, the cosmetics firm's chief, congratulated her, saying how happy he was that she modeled for his company. She replied, "Of course, you'll have to pay me more." Perelman stalked off in a rage. Her win at the Academy Awards led to two famous "Oscar moments." In accepting her award, she gave an acceptance speech honoring previous black actresses who had never had the opportunity. She said, "This moment is so much bigger than me. This is for every nameless, faceless woman of colour who now has a chance tonight because this door has been opened." One year later, as she presented the Best Actor award, winner Adrien Brody ran on stage and, instead of giving her the standard peck on the cheek, planted a long kiss on Berry.
As Bond girl Giacinta 'Jinx' Johnson in the 2002 blockbuster "Die Another Day", Berry recreated a scene from "Dr. No", emerging from the surf to be greeted by James Bond as Ursula Andress had 40 years earlier. Lindy Hemming, costume designer on "Die Another Day", had insisted that Berry wear a bikini and knife as a homage. Berry has said of the scene: "It's splashy", "exciting", "sexy", "provocative" and "it will keep me still out there after winning an Oscar." The bikini scene was shot in Cadiz; the location was reportedly cold and windy, and footage has been released of Berry wrapped in thick towels in between takes to try to stay warm. According to an ITV news poll, Jinx was voted the fourth toughest girl on screen of all time. Berry was hurt during filming when debris from a smoke grenade flew into her eye. It was removed in a 30-minute operation. After Berry won the Academy Award, rewrites were commissioned to give her more screentime for "X2".
She starred in the psychological thriller "Gothika" opposite Robert Downey, Jr. in November 2003, during which she broke her arm in a scene with Downey, who twisted her arm too hard. Production was halted for eight weeks. It was a moderate hit at the United States box office, taking in $60 million; it earned another $80 million abroad. Berry appeared in the nu metal band Limp Bizkit's music video for "Behind Blue Eyes" for the motion picture soundtrack for the film. The same year, she was named #1 in "FHM"'s 100 Sexiest Women in the World poll.
Recent work.
Berry received $12.5 million for the title role in the film "Catwoman", a $100 million movie; it grossed $17 million on its first weekend. She was awarded a "worst actress" Razzie award in 2005 for this role. She appeared at the ceremony to accept the award in person (making her the third person, and second actor, ever to do so) with a sense of humor, considering it an experience of the "rock bottom" in order to be "at the top". Holding the Academy Award in one hand and the Razzie in the other she said, "I never in my life thought that I would be here, winning a Razzie. It's not like I ever aspired to be here, but thank you. When I was a kid, my mother told me that if you could not be a good loser, then there's no way you could be a good winner." The Fund for Animals praised Berry's compassion towards cats and for squelching rumors that she was keeping a Bengal tiger from the sets of Catwoman as a "pet."
Her next film appearance was in the Oprah Winfrey-produced ABC TV movie "Their Eyes Were Watching God" (2005), an adaptation of Zora Neale Hurston's novel, in which Berry portrayed Janie Crawford, a free-spirited woman whose unconventional sexual mores upset her 1920s contemporaries in a small community. She was nominated for an Emmy for this TV film. Meanwhile, she voiced the character of Cappy, one of the many mechanical beings in the animated feature "Robots" (2005).
Berry is involved in production of films and television. She served as executive producer on "Introducing Dorothy Dandridge" in 1999, and "Lackawanna Blues" in 2005. Berry both produced and starred in the thriller "Perfect Stranger" with Bruce Willis and in "Things We Lost in the Fire" with Benicio del Toro, the first film in which she worked with a female director, Danish Susanne Bier, a new feeling of "thinking the same way", which she appreciated. Berry then starred in the film "Frankie and Alice", in which she plays Frankie Murdoch, a young multiracial American women with dissociative identity disorder struggling against her alter personality to retain her true self. She was awarded the African-American Film Critics Association Award for Best Actress and was nominated for the Golden Globe Award for Best Actress – Motion Picture Drama.
Berry was one of the highest-paid actresses in Hollywood during the 2000s, earning an estimated $10 million per film. In July 2007, she topped "In Touch" magazine's list of the world's most fabulous 40-something celebrities. On April 3, 2007, she was awarded a star on the Hollywood Walk of Fame in front of the Kodak Theatre at 6801 Hollywood Boulevard for her contributions to the film industry. As of 2013, Berry's worldwide box office gross has been more than 3.3 billion US$. In 2011, she appeared in "New Year's Eve". She played one of the leads in the film "Cloud Atlas", which was released in October 2012. 
On October 4, 2013 Berry signed on to star in the CBS drama series "Extant". Berry will play Molly Watts, an astronaut who struggles to reconnect with her husband and android son after spending 13 months in space. The show will premier on July 9, 2014. She will also serve as a co-executive producer on the series.
Berry has served for many years as the face of Revlon cosmetics and as the face of Versace. The Coty Inc. fragrance company signed Berry to market her debut fragrance in March 2008. Berry was delighted, saying that she had created her own fragrances at home by mixing scents.
In March 2014 Berry launched new production company, 606 Films, with producing partner Elaine Goldsmith-Thomas. Named after the Anti-Paparazzi Bill, SB 606, that the actress pushed for and was signed into law by Californian Governor Jerry Brown in the fall of 2013. The new company emerges as part of a deal for Berry to star in the CBS sci-fi drama series "Extant." 606 Films will be housed within CBS.
Personal life.
In February 2000, Berry was involved in a traffic collision in which she left the scene of the accident. Some in the media complained that her misdemeanor hit and run charge was preferential treatment. She had also been the driver in an alleged hit-and-run incident three years earlier in which no charges were filed. Berry pleaded no contest, did community service, paid a fine and was placed on three years' probation. A civil lawsuit was settled out of court.
Relationships and marriages.
Berry dated Chicago dentist John Ronan from March 1989 to October 1991. In November 1993, Ronan sued Berry for $80,000 in what he claimed were unpaid loans to help launch her career. Berry contended that the money was a gift, and a judge dismissed the case because Ronan did not list Berry as a debtor when he filed for bankruptcy in 1992.
According to Berry, a beating from a former abusive boyfriend during the filming of "The Last Boy Scout" punctured her eardrum and caused her to lose eighty percent of her hearing in her left ear. Berry has never named the abuser but has said that he is someone well known in Hollywood.
Berry first saw baseball player David Justice on TV playing in an MTV celebrity baseball game in February 1992. When a reporter from Justice's hometown of Cincinnati told her that Justice was a fan, Berry gave her phone number to the reporter to give to Justice. Berry married Justice shortly after midnight on January 1, 1993. Following their separation in February 1996, Berry stated publicly that she was so depressed that she considered taking her own life. Berry and Justice were officially divorced on June 24, 1997.
Berry married her second husband, singer-songwriter Eric Benét, on January 24, 2001, following a two-year courtship. but by early October 2003 they had separated, with the divorce finalized on January 3, 2005. Benét underwent treatment for sex addiction in 2002.
Berry began dating French Canadian model Gabriel Aubry in November 2005. The couple met at a Versace photoshoot. Berry gave birth to their daughter, Nahla, in 2008. On April 30, 2010, Berry and Aubry announced their separation.
Berry began dating French actor Olivier Martinez in 2010 after they met while filming "Dark Tide" in South Africa. They confirmed their engagement in March 2012, and married in France on July 13, 2013. They have a son, Maceo, born in 2013.
After their 2010 separation, Berry and Aubry became involved in a highly publicized custody battle, centered primarily on Berry's desire to move with their daughter Nahla from Los Angeles, where Berry and Aubry currently reside, to Martinez's native France. Aubry objected to the move, on the grounds that it would interfere with their joint custody arrangement. In November 2012, a judge denied Berry's request to move Nahla to France in light of Aubry's objections. Less than two weeks later, on November 22, 2012, Aubry and Martinez were both treated at a hospital for injuries after engaging in a physical altercation at Berry's residence. Martinez performed a citizen's arrest on Aubry, and because it was considered a domestic violence incident, was granted a temporary emergency protective order preventing Aubry from coming within 100 yards of Berry, Martinez, and Nahla until November 29, 2012. In turn, Aubry obtained a temporary restraining order against Martinez on November 26, 2012, asserting that the fight began when Martinez threatened to kill Aubry if he did not allow the couple to move to France. Leaked court documents included photos showing significant injuries to Aubry's face, which were widely displayed in the media. On November 29, 2012, Berry's lawyer announced that Berry and Aubry had reached an amicable custody agreement in court. In June 2014, a Superior Court ruling called for Berry to pay Aubry $16,000 a month in child support (around 200k/year) as well as a retroactive payment of $115,000 and a sum of $300,000 for Aubry's attorney fees.
Activism.
Along with Pierce Brosnan, Cindy Crawford, Jane Seymour, Dick Van Dyke, Téa Leoni, and Daryl Hannah, Berry successfully fought in 2006 against the Cabrillo Port Liquefied Natural Gas facility that was proposed off the coast of Malibu. Berry said, "I care about the air we breathe, I care about the marine life and the ecosystem of the ocean." In May 2007, Governor Arnold Schwarzenegger vetoed the facility. Hasty Pudding Theatricals gave her its 2006 "Woman of The Year" award.
Berry took part in a nearly 2000-house cell-phone bank campaign for Barack Obama in February 2008. In April 2013, she appeared in a video clip for Gucci's "Chime for Change" campaign that aims to raise funds and awareness of women's issues in terms of education, health, and justice. In August 2013, Berry testified alongside Jennifer Garner before the California State Assembly's Judiciary Committee in support of a bill that would protect celebrities' children from harassment by photographers. The bill passed in September.
Public image.
Berry was ranked No. 1 on "People"‍‍ '​‍s "50 Most Beautiful People in the World" list in 2003 after making the top ten seven times and appeared No. 1 on "FHM"‍‍ '​‍s "100 Sexiest Women in the World" the same year. She was named "Esquire" magazine's "Sexiest Woman Alive" in October 2008, about which she stated: "I don't know exactly what it means, but being 42 and having just had a baby, I think I'll take it." "Men's Health" ranked her at No. 35 on their "100 Hottest Women of All-Time" list. In 2009, she was voted #23 on "Empire"'s 100 Sexiest Film Stars. The same year, rapper Hurricane Chris released a song entitled "Halle Berry (She's Fine)", extolling Berry's beauty and sex appeal. At the age of 42 (in 2008), she was named the “Sexiest Black Woman” by Access Hollywood’s TV One Access survey.
Born to an African-American father and Caucasian mother, Berry stated that her biracial background was "painful and confusing" when she was a young woman, and she made the decision early on to identify as a black woman because she knew that was how she would be perceived.

</doc>
<doc id="13722" url="http://en.wikipedia.org/wiki?curid=13722" title="Robert Koch">
Robert Koch

Robert Heinrich Herman Koch (; ]; 11 December 1843 – 27 May 1910) was a celebrated German physician and pioneering microbiologist. The founder of modern bacteriology, he is known for his role in identifying the specific causative agents of tuberculosis, cholera, and anthrax and for giving experimental support for the concept of infectious disease. In addition to his trail-blazing studies on these diseases, Koch created and improved laboratory technologies and techniques in the field of microbiology, and made key discoveries in public health. His research led to the creation of Koch’s postulates, a series of four generalized principles linking specific microorganisms to specific diseases that remain today the "gold standard" in medical microbiology. As a result of his groundbreaking research on tuberculosis, Koch received the Nobel Prize in Physiology or Medicine in 1905.
Personal life.
Robert Koch was born in Clausthal, Hanover, Germany, on 11 December 1843, to Hermann Koch and Mathilde Julie Henriette Biewand. Koch excelled in academics from an early age. Before entering school in 1848, he had taught himself how to read and write. He graduated from high school in 1862, having excelled in science and maths. At the age of 19, Koch entered the University of Göttingen, studying natural science. However, after two semesters, Koch decided to change his area of study to medicine, as he aspired to be a physician. During his fifth semester of medical school, Jacob Henle, an anatomist who had published a theory of contagion in 1840, asked him to participate in his research project on uterine nerve structure. In his sixth semester, Koch began to conduct research at the Physiological Institute, where he studied succinic acid secretion. This would eventually form the basis of his dissertation. In January 1866, Koch graduated from medical school, earning honors of the highest distinction. In July 1867, following his graduation from medical school, Koch married Emma Adolfine Josephine Fraatz, and the two had a daughter, Gertrude, in 1868. After his graduation in 1866, he worked as a surgeon in the Franco-Prussian War, and following his service, worked as a physician in Wollstein, Posen. Koch’s marriage with Emma Fraatz ended in 1893, and later that same year, he married actress Hedwig Freiberg. From 1885 to 1890, he served as an administrator and professor at Berlin University. Koch suffered a heart attack on 9 April 1910, and never made a complete recovery. On 27 May, only three days after giving a lecture on his tuberculosis research at the Prussian Academy of Sciences, Robert Koch died in Baden-Baden at the age of 66. Following his death, the Institute named its establishment after him in his honour.
Research contributions.
Anthrax.
Robert Koch is widely known for his work with anthrax, discovering the causative agent of the fatal disease to be "Bacillus anthracis". Koch discovered the formation in anthrax bacteria of spores that could remain dormant under specific conditions. However, under optimal conditions, the spores were activated and caused disease. To determine this causative agent, he dry-fixed bacterial cultures onto glass slides, used dyes to stain the cultures, and observed them through a microscope. Koch’s work with anthrax is notable in that he was the first to link a specific microorganism with a specific disease, rejecting the idea of spontaneous generation and supporting the germ theory of disease.
Koch's four postulates.
Koch accepted a position as government advisor with the Imperial Department of Health in 1880. During his time as government advisor, he published a report in which he stated the importance of pure cultures in isolating disease-causing organisms and explained the necessary steps to obtain these cultures, methods which are summarized in Koch’s four postulates. Koch’s discovery of the causative agent of anthrax led to the formation of a generic set of postulates which can be used in the determination of the cause of any infectious disease. These postulates, which not only outlined a method for linking cause and effect of an infectious disease but also established the significance of laboratory culture of infectious agents, are listed here:
1. The organism must always be present, in every case of the disease.
2. The organism must be isolated from a host containing the disease and grown in pure culture.
3. Samples of the organism taken from pure culture must cause the same disease when inoculated into a healthy, susceptible animal in the laboratory.
4. The organism must be isolated from the inoculated animal and must be identified as the same original organism first isolated from the originally diseased host.
Isolating pure culture on solid media.
Koch began conducting research on microorganisms in a laboratory connected to his patient examination room. Koch’s early research in this laboratory proved to yield one of his major contributions to the field of microbiology, as it was there that he developed the technique of growing bacteria. Koch's second postulate calls for the isolation and growth of a selected pathogen in pure laboratory culture. In an attempt to grow bacteria, Koch began to use solid nutrients such as potato slices. Through these initial experiments, Koch observed individual colonies of identical, pure cells. Coming to the conclusion that potato slices were not suitable media for all organisms, Koch later began to use nutrient solutions with gelatin. However, he soon realized that gelatin, like potato slices, was not the optimal medium for bacterial growth, as it did not remain solid at 37˚C, the ideal temperature for growth of most human pathogens. As suggested to him by Walther and Angelina Hesse, Koch began to utilize agar to grow and isolate pure cultures, as this polysaccharide remains solid at 37˚C, is not degraded by most bacteria, and results in a transparent medium.
Cholera.
Koch next turned his attention to cholera, and began to conduct research in Egypt in the hopes of isolating the causative agent of the disease. However, he was not able to complete the task before the epidemic in Egypt ended, and subsequently traveled to India to continue with the study. In India, Koch was indeed able to determine the causative agent of cholera, isolating "Vibrio cholera". The bacterium had originally been isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.
Tuberculosis.
During his time as the government advisor with the Imperial Department of Health in Berlin in the 1880s, Robert Koch became interested in tuberculosis research. At the time, it was widely believed that tuberculosis was an inherited disease. However, Koch was convinced that the disease was caused by a bacterium and was infectious, and tested his four postulates using guinea pigs. Through these experiments, he found that his experiments with tuberculosis satisfied all four of his postulates. In 1882, he published his findings on tuberculosis, in which he reported the causative agent of the disease to be the slow-growing "Mycobacterium tuberculosis". His work with this disease won Koch the Nobel Prize in Physiology and Medicine in 1905. Additionally, Koch's research on tuberculosis, along with his studies on tropical diseases, won him the Prussian Order Pour le Merite in 1906 and the Robert Koch medal, established to honour the greatest living physicians, in 1908.

</doc>
<doc id="13726" url="http://en.wikipedia.org/wiki?curid=13726" title="Hogshead">
Hogshead

A hogshead is a large cask of liquid (or, less often, of a food commodity). More specifically, it refers to a specified volume, measured in either imperial or US customary measures, primarily applied to alcoholic beverages, such as wine, ale, or cider. 
A tobacco hogshead was used in British and American colonial times to transport and store tobacco. It was a very large wooden barrel. A standardized hogshead measured 48 in long and 30 in in diameter at the head (at least 550 L, depending on the width in the middle). Fully packed with tobacco, it weighed about 1000 lb.
A wine hogshead contains about 300 L. 
The "Oxford English Dictionary" (OED) notes that the hogshead was first standardized by an act of Parliament in 1423, though the standards continued to vary by locality and content. For example, the OED cites an 1897 edition of "Whitaker's Almanack", which specified the number of gallons of wine in a hogshead varying by type of wine: claret (presumably) 46 impgal, port 57 impgal, sherry 54 impgal; and Madeira 46 impgal. The "American Heritage Dictionary" claims that a hogshead can consist of anything from (presumably) 62.5 to. 
Eventually, a hogshead of wine came to be 63 USgal, while a hogshead of beer or ale is 54 gallons (250 L if old beer/ale gallons, 245 L if imperial).
A hogshead was also used as unit of measurement for sugar in Louisiana for most of the 19th century. Plantations were listed in sugar schedules as having produced "x" number of hogsheads of sugar or molasses. A hogshead was also used for the measurement of herring fished for sardines in Blacks Harbour, New Brunswick.
The etymology of hogshead is uncertain. According to English philologist Walter William Skeat (1835-1912), the origin is to be found in the name for a cask or liquid measure appearing in various forms in several Teutonic languages, in Dutch oxhooft (modern okshoofd), Danish oxehoved, Old Swedish oxhufvod, etc. The word should therefore be "oxhead", "hogshead" being a mere corruption. It has been suggested that the name arose from the branding of such a measure with the head of an ox.
Charts.
A hogshead of Madeira wine was approximately equal to 45-48 gallons (0.205-0.218) m3.<br>
A hogshead of brandy was approximately equal to 56-61 gallons (0.255-0.277) m3.

</doc>
<doc id="13727" url="http://en.wikipedia.org/wiki?curid=13727" title="Huallaga">
Huallaga

Huallaga may refer to:

</doc>
<doc id="13729" url="http://en.wikipedia.org/wiki?curid=13729" title="Honda">
Honda

Honda Motor Co., Ltd. (本田技研工業株式会社, Honda Giken Kōgyō KK, ]; ) is a Japanese public multinational corporation primarily known as a manufacturer of automobiles, motorcycles and power equipment.
Honda has been the world's largest motorcycle manufacturer since 1959, as well as the world's largest manufacturer of internal combustion engines measured by volume, producing more than 14 million internal combustion engines each year. Honda became the second-largest Japanese automobile manufacturer in 2001. Honda was the eighth largest automobile manufacturer in the world behind General Motors, Volkswagen Group, Toyota, Hyundai Motor Group, Ford, Nissan, and PSA in 2011.
Honda was the first Japanese automobile manufacturer to release a dedicated luxury brand, Acura, in 1986. Aside from their core automobile and motorcycle businesses, Honda also manufactures garden equipment, marine engines, personal watercraft and power generators, amongst others. Since 1986, Honda has been involved with artificial intelligence/robotics research and released their ASIMO robot in 2000. They have also ventured into aerospace with the establishment of GE Honda Aero Engines in 2004 and the Honda HA-420 HondaJet, which began production in 2012. Honda has three joint-ventures in China (Honda China, Dongfeng Honda, and Guangqi Honda).
In 2013, Honda invested about 5.7% (US$6.8 billion) of its revenues in research and development. Also in 2013, Honda became the first Japanese automaker to be a net exporter from the United States, exporting 108,705 Honda and Acura models while importing only 88,357.
History.
Throughout his life, Honda's founder, Soichiro Honda had an interest in automobiles. He worked as a mechanic at the Art Shokai garage, where he tuned cars and entered them in races. In 1937, with financing from his acquaintance Kato Shichirō, Honda founded Tōkai Seiki (Eastern Sea Precision Machine Company) to make piston rings working out of the Art Shokai garage. After initial failures, Tōkai Seiki won a contract to supply piston rings to Toyota, but lost the contract due to the poor quality of their products. After attending engineering school without graduating, and visiting factories around Japan to better understand Toyota's quality control processes, by 1941 Honda was able to mass-produce piston rings acceptable to Toyota, using an automated process that could employ even unskilled wartime laborers.:16–19
Tōkai Seiki was placed under control of the Ministry of Commerce and Industry (called the Ministry of Munitions after 1943) at the start of World War II, and Soichiro Honda was demoted from president to senior managing director after Toyota took a 40% stake in the company. Honda also aided the war effort by assisting other companies in automating the production of military aircraft propellers. The relationships Honda cultivated with personnel at Toyota, Nakajima Aircraft Company and the Imperial Japanese Navy would be instrumental in the postwar period. A US B-29 bomber attack destroyed Tōkai Seiki's Yamashita plant in 1944, and the Itawa plant collapsed in the 1945 Mikawa earthquake, and Soichiro Honda sold the salvageable remains of the company to Toyota after the war for ¥450,000, and used the proceeds to found the Honda Technical Research Institute in October 1946. With a staff of 12 men working in a 16 m2 shack, they built and sold improvised motorized bicycles, using a supply of 500 two-stroke "50 cc" Tohatsu war surplus radio generator engines.:19 When the engines ran out, Honda began building their own copy of the Tohatsu engine, and supplying these to customers to attach their bicycles. This was the Honda Model A, nicknamed the Bata Bata for the sound the engine made. In 1949, the Honda Technical Research Institute was liquidated for ¥1,000,000, or about US$5,000 today; these funds were used to incorporate Honda Motor Co., Ltd.:21 At about the same time Honda hired engineer Kihachiro Kawashima, and Takeo Fujisawa who provided indispensable business and marketing expertise to complement Soichiro Honda's technical bent.:21 The close partnership between Soichiro Honda and Fujisawa lasted until they stepped down together in October 1973.:21
The first complete motorcycle, with both the frame and engine made by Honda, was the 1949 Model D, the first Honda to go by the name Dream. Honda Motor Company grew in a short time to become the world's largest manufacturer of motorcycles by 1964.
The first production automobile from Honda was the T360 mini pick-up truck, which went on sale in August 1963. Powered by a small 356-cc straight-4 gasoline engine, it was classified under the cheaper Kei car tax bracket. The first production car from Honda was the S500 sports car, which followed the T360 into production in October 1963. Its chain-driven rear wheels pointed to Honda's motorcycle origins.
Over the next few decades, Honda worked to expand its product line and expanded operations and exports to numerous countries around the world. In 1986, Honda introduced the successful Acura brand to the American market in an attempt to gain ground in the luxury vehicle market. The year 1991 saw the introduction of the Honda NSX supercar, the first all-aluminum monocoque vehicle that incorporated a mid-engine V6 with variable-valve timing.
CEO Tadashi Kume was succeeded by Nobuhiko Kawamoto in 1990. Kawamoto was selected over Shoichiro Irimajiri, who oversaw the successful establishment of Honda of America Manufacturing, Inc. in Marysville, Ohio. Both Kawamoto and Irimajiri shared a friendly rivalry within Honda, and Irimajiri would resign in 1992 due to health issues.
Following the death of Soichiro Honda and the departure of Irimajiri, Honda found itself quickly being outpaced in product development by other Japanese automakers and was caught off-guard by the truck and sport utility vehicle boom of the 1990s, all which took a toll on the profitability of the company. Japanese media reported in 1992 and 1993 that Honda was at serious risk of an unwanted and hostile takeover by Mitsubishi Motors, who at the time was a larger automaker by volume and flush with profits from their successful Pajero and Diamante.
Kawamoto acted quickly to change Honda's corporate culture, rushing through market-driven product development that resulted in recreational vehicles such as the Odyssey and the CR-V, and a refocusing away from some of the numerous sedans and coupes that were popular with Honda's engineers but not with the buying public. The most shocking change to Honda came when Kawamoto ended Honda's successful participation in Formula One after the 1992 season, citing costs in light of the takeover threat from Mitsubishi as well as the desire to create a more environmentally-friendly company image.
Later, 1995 gave rise to the Honda Aircraft Company with the goal of producing jet aircraft under Honda's name.
On February 23, 2015, Honda announced that CEO and President Takanobu Ito would step down and be replaced by Takahiro Hachigo by June; additional retirements by senior managers and directors were expected.
Corporate profile and divisions.
Honda is headquartered in Minato, Tokyo, Japan. Their shares trade on the Tokyo Stock Exchange and the New York Stock Exchange, as well as exchanges in Osaka, Nagoya, Sapporo, Kyoto, Fukuoka, London, Paris and Switzerland.
The company has assembly plants around the globe. These plants are located in China, the United States, Pakistan, Canada, England, Japan, Belgium, Brazil, México, New Zealand, Malaysia, Indonesia, India, Philippines, Thailand, Vietnam, Turkey, Taiwan, Perú and Argentina. As of July 2010, 89 percent of Honda and Acura vehicles sold in the United States were built in North American plants, up from 82.2 percent a year earlier. This shields profits from the yen's advance to a 15-year high against the dollar.
Honda's Net Sales and Other Operating Revenue by Geographical Regions in 2007
American Honda Motor Company is based in Torrance, California. Honda Racing Corporation (HRC) is Honda's motorcycle racing division. Honda Canada Inc. is headquartered in Markham, Ontario, their manufacturing division, Honda of Canada Manufacturing, is based in Alliston, Ontario. Honda has also created joint ventures around the world, such as Honda Siel Cars and Hero Honda Motorcycles in India, Guangzhou Honda and Dongfeng Honda in China, Boon Siew Honda in Malaysia and Honda Atlas in Pakistan.
Following the Japanese earthquake and tsunami in March 2011 Honda announced plans to halve production at its UK plants. The decision was made to put staff at the Swindon plant on a 2-day week until the end of May as the manufacturer struggled to source supplies from Japan. It's thought around 22,500 cars were produced during this period.
Products.
Automobiles.
Honda's global lineup consists of the Fit, Civic, Accord, Insight, CR-V, CR-Z, Legend and two versions of the Odyssey, one for North America, and a smaller vehicle sold internationally. An early proponent of developing vehicles to cater to different needs and markets worldwide, Honda's lineup varies by country and may have vehicles exclusive to that region. A few examples are the latest Honda Odyssey minivan and the Ridgeline, Honda's first light-duty uni-body pickup truck. Both were designed and engineered primarily in North America and are produced there. Other example of exclusive models includes the Honda Civic five-door hatchback sold in Europe.
Honda's automotive manufacturing ambitions can be traced back to 1963, with the Honda T360, a kei car truck built for the Japanese market. This was followed by the two-door roadster, the Honda S500 also introduced in 1963. In 1965, Honda built a two-door commercial delivery van, called the Honda L700. Honda's first four-door sedan was not the Accord, but the air-cooled, four-cylinder, gasoline-powered Honda 1300 in 1969. The Civic was a hatchback that gained wide popularity internationally, but it wasn't the first two-door hatchback built. That was the Honda N360, another "Kei car" that was adapted for international sale as the N600. The Civic, which appeared in 1972 and replaced the N600 also had a smaller sibling that replaced the air-cooled N360, called the Honda Life that was water-cooled.
The Honda Life represented Honda's efforts in competing in the "kei" car segment, offering sedan, delivery van and small pick-up platforms on a shared chassis. The Life StepVan had a novel approach that, while not initially a commercial success, appears to be an influence in vehicles with the front passengers sitting behind the engine, a large cargo area with a flat roof and a liftgate installed in back, and utilizing a transversely installed engine with a front-wheel-drive powertrain.
As Honda entered into automobile manufacturing in the late 1960s, where Japanese manufacturers such as Toyota and Nissan had been making cars since before WWII, it appears that Honda instilled a sense of doing things a little differently than its Japanese competitors. Its mainstay products, like the Accord and Civic (with the exception of its USA-market 1993–97 Passport which was part of a vehicle exchange program with Isuzu (part of the Subaru-Isuzu joint venture)), have always employed front-wheel-drive powertrain implementation, which is currently a long held Honda tradition. Honda also installed new technologies into their products, first as optional equipment, then later standard, like anti lock brakes, speed sensitive power steering, and multi-port fuel injection in the early 1980s. This desire to be the first to try new approaches is evident with the creation of the first Japanese luxury chain Acura, and was also evident with the all aluminum, mid-engined sports car, the Honda NSX, which also introduced variable valve timing technology, Honda calls VTEC.
The Civic is a line of compact cars developed and manufactured by Honda. In North America, the Civic is the second-longest continuously running nameplate from a Japanese manufacturer; only its perennial rival, the Toyota Corolla, introduced in 1968, has been in production longer. The Civic, along with the Accord and Prelude, comprised Honda's vehicles sold in North America until the 1990s, when the model lineup was expanded. Having gone through several generational changes, the Civic has become larger and more upmarket, and it currently slots between the Fit and Accord.
Honda produces Civic hybrid, a hybrid electric vehicle that competes with the Toyota Prius, and also produces the Insight and CR-Z.
In 2008, Honda increased global production to meet demand for small cars and hybrids in the U.S. and emerging markets. The company shuffled U.S. production to keep factories busy and boost car output, while building fewer minivans and sport utility vehicles as light truck sales fell.
Its first entrance into the pickup segment, the light duty Ridgeline, won Truck of the Year from "Motor Trend" magazine in 2006. Also in 2006, the redesigned Civic won Car of the Year from the magazine, giving Honda a rare double win of Motor Trend honors. Honda's 9th generation Civic also won the award based on a public survey held by 
It is reported that Honda plans to increase hybrid sales in Japan to more than 20% of its total sales in fiscal year 2011, from 14.8% in previous year.
Five of United States Environmental Protection Agency's top ten most fuel-efficient cars from 1984 to 2010 comes from Honda, more than any other automakers. The five models are: 2000–2006 Honda Insight (53 mpgus combined), 1986–1987 Honda Civic Coupe HF (46 mpgus combined), 1994–1995 Honda Civic hatchback VX (43 mpgus combined), 2006– Honda Civic Hybrid (42 mpgus combined), and 2010– Honda Insight (41 mpgus combined). The ACEEE has also rated the Civic GX as the greenest car in America for seven consecutive years.
Motorcycles.
Honda is the largest motorcycle manufacturer in Japan and has been since it started production in 1955.
At its peak in 1982, Honda manufactured almost three million motorcycles annually. By 2006 this figure had reduced to around 550,000 but was still higher than its three domestic competitors.
During the 1960s, when it was a small manufacturer, Honda broke out of the Japanese motorcycle market and began exporting to the U.S. Working with the advertising agency Grey Advertising, Honda created an innovative marketing campaign, using the slogan "You meet the nicest people on a Honda." In contrast to the prevailing negative stereotypes of motorcyclists in America as tough, antisocial rebels, this campaign suggested that Honda motorcycles were made for the everyman. The campaign was hugely successful; the ads ran for three years, and by the end of 1963 alone, Honda had sold 90,000 motorcycles.:
Taking Honda's story as an archetype of the smaller manufacturer entering a new market already occupied by highly dominant competitors, the story of their market entry, and their subsequent huge success in the U.S. and around the world, has been the subject of some academic controversy. Competing explanations have been advanced to explain Honda's strategy and the reasons for their success.
The first of these explanations was put forward when, in 1975, Boston Consulting Group (BCG) was commissioned by the UK government to write a report explaining why and how the British motorcycle industry had been out-competed by its Japanese competitors. The report concluded that the Japanese firms, including Honda, had sought a very high scale of production (they had made a large number of motorbikes) in order to benefit from economies of scale and learning curve effects. It blamed the decline of the British motorcycle industry on the failure of British managers to invest enough in their businesses to profit from economies of scale and scope.
The second explanation was offered in 1984 by Richard Pascale, who had interviewed the Honda executives responsible for the firm's entry into the U.S. market. As opposed to the tightly focused strategy of low cost and high scale that BCG accredited to Honda, Pascale found that their entry into the U.S. market was a story of "miscalculation, serendipity, and organizational learning" – in other words, Honda's success was due to the adaptability and hard work of its staff, rather than any long term strategy. For example, Honda's initial plan on entering the US was to compete in large motorcycles, around 300 cc. Honda's motorcycles in this class suffered performance and reliability problems when ridden the relatively long distances of the US highways.:41–43 When the team found that the scooters they were using to get themselves around their U.S. base of San Francisco attracted positive interest from consumers that they fell back on selling the Super Cub instead.:41–43
The most recent school of thought on Honda's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989. Creating the concept of core competencies with Honda as an example, they argued that Honda's success was due to its focus on leadership in the technology of internal combustion engines. For example, the high power-to-weight ratio engines Honda produced for its racing bikes provided technology and expertise which was transferable into mopeds. Honda's entry into the U.S. motorcycle market during the 1960s is used as a case study for teaching introductory strategy at business schools worldwide.
Power equipment.
Production started in 1953 with H-type engine (prior to motorcycle).<br>
Honda power equipment reached record sales in 2007 with 6.4 million units. By 2010 this figure had decreased to 4,7 million units. Cumulative production of power products has exceeded 85 million units (as of September 2008).
Honda power equipment includes:
Engines.
Honda engines powered the entire 33-car starting field of the 2010 Indianapolis 500 and for the fifth consecutive race, there were no engine-related retirements during the running of the Memorial Day Classic.
In the 1980s Honda developed the GY6 engine for use in motor scooters. Although no longer manufactured by Honda it is still commonly used in many Chinese, Korean and Taiwanese light vehicles.
Honda, despite being known as an engine company, has never built a V8 for passenger vehicles. In the late 1990s, the company resisted considerable pressure from its American dealers for a V8 engine (which would have seen use in top-of-the-line Honda SUVs and Acuras), with American Honda reportedly sending one dealer a shipment of V8 beverages to silence them. Honda considered starting V8 production in the mid-2000s for larger Acura sedans, a new version of the high end NSX sports car (which previously used DOHC V6 engines with VTEC to achieve its high power output) and possible future ventures into the American full-size truck and SUV segment for both the Acura and Honda brands, but this was cancelled in late 2008, with Honda citing environmental and worldwide economic conditions as reasons for the termination of this project.
Robots.
ASIMO is the part of Honda's . It is the eleventh in a line of successive builds starting in 1986 with Honda E0 moving through the ensuing Honda E series and the Honda P series. Weighing 54 kilograms and standing 130 centimeters tall, ASIMO resembles a small astronaut wearing a backpack, and can walk on two feet in a manner resembling human locomotion, at up to 6 km/h. ASIMO is the world's only humanoid robot able to ascend and descend stairs independently. However, human motions such as climbing stairs are difficult to mimic with a machine, which ASIMO has demonstrated by taking two plunges off a staircase.
Honda's robot ASIMO (see below) as an R&D project brings together expertise to create a robot that walks, dances and navigates steps.
2010 marks the year Honda has developed a machine capable of reading a user's brainwaves to move ASIMO. The system uses a helmet covered with electroencephalography and near-infrared spectroscopy sensors that monitor electrical brainwaves and cerebral blood flow—signals that alter slightly during the human thought process. The user thinks of one of a limited number of gestures it wants from the robot, which has been fitted with a Brain Machine Interface.
Aircraft.
Honda has also pioneered new technology in its HA-420 HondaJet, manufactured by its subsidiary Honda Aircraft Company, which allows new levels of reduced drag, increased aerodynamics and fuel efficiency thus reducing operating costs.
Solar cells.
Honda's solar cell subsidiary company Honda Soltec (Headquarters: Kikuchi-gun, Kumamoto; President and CEO: Akio Kazusa) started sales throughout Japan of thin-film solar cells for public and industrial use on 24 October 2008, after selling solar cells for residential use since October 2007. Honda announced in the end of October 2013 that Honda Soltec would cease the business operation except for support for existing customers in Spring 2014 and the subsidiary would be dissolved.
Mountain bikes.
Honda has also built a downhill racing bicycle known as the Honda RN-01. It is not available for sale to the public. The bike has a gearbox, which replaces the standard derailleur found on most bikes.
Honda has hired several people to pilot the bike, among them Greg Minnaar. The team is known as Team G Cross Honda.
ATV.
Honda also builds all-terrain vehicles (ATV).
450r
400ex
300ex
250r
Motorsports.
Honda has been active in motorsports, like Motorcycle Grand Prix, Superbike racing and others.
Automobile.
Honda entered Formula One as a constructor for the first time in the 1964 season at the German Grand Prix with Ronnie Bucknum at the wheel. 1965 saw the addition of Richie Ginther to the team, who scored Honda's first point at the Belgian Grand Prix, and Honda's first win at the Mexican Grand Prix. 1967 saw their next win at the Italian Grand Prix with John Surtees as their driver. In 1968, Jo Schlesser was killed in a Honda RA302 at the French Grand Prix. This racing tragedy, coupled with their commercial difficulties selling automobiles in the United States, prompted Honda to withdraw from all international motorsport that year.
After a learning year in 1965, Honda-powered Brabhams dominated the 1966 French Formula Two championship in the hands of Jack Brabham and Denny Hulme. As there was no European Championship that season, this was the top F2 championship that year. In the early 1980s Honda returned to F2, supplying engines to Ron Tauranac's Ralt team. Tauranac had designed the Brabham cars for their earlier involvement. They were again extremely successful. In a related exercise, John Judd's Engine Developments company produced a turbo "Brabham-Honda" engine for use in IndyCar racing. It won only one race, in 1988 for Bobby Rahal at Pocono.
Honda returned to Formula One in 1983, initially with another Formula Two partner, the Spirit team, before switching abruptly to Williams in 1984. In the late 1980s and early 1990s, Honda powered cars won six consecutive Formula One Constructors Championships. WilliamsF1 won the crown in 1986 and 1987. Honda switched allegiance again in 1988. New partners Team McLaren won the title in 1988, 1989, 1990 and 1991. Honda withdrew from Formula One at the end of 1992, although the related Mugen-Honda company maintained a presence up to the end of 1999, winning four races with Ligier and Jordan Grand Prix.
Honda debuted in the CART IndyCar World Series as a works supplier in 1994. The engines were far from competitive at first, but after development, the company powered six consecutive drivers championships. In 2003, Honda transferred its effort to the rival IRL IndyCar Series with Ilmor as joint development until 2006. In 2004, Honda-powered cars overwhelmingly dominated the IndyCar Series, winning 14 of 16 IndyCar races, including the Indianapolis 500, and claimed the IndyCar Series Manufacturers' Championship, Drivers' Championship and Rookie of the Year titles. From 2006 to 2011, Honda was the lone engine supplier for the IndyCar Series, including the Indianapolis 500. In the 2006 Indianapolis 500, for the first time in Indianapolis 500 history, the race was run without a single engine problem. Since 2012, HPD has constructed turbocharged V-6 engines for its IndyCar effort.
During 1998, Honda considered returning to Formula One with their own team. The project was aborted after the death of its technical director, Harvey Postlethwaite. Honda instead came back as an official engine supplier to British American Racing (BAR) and Jordan Grand Prix. Honda bought a stake in the BAR team in 2004 before buying the team outright at the end of 2005, becoming a constructor for the first time since the 1960s. Honda won the 2006 Hungarian Grand Prix with driver Jenson Button.
It was announced on 5 December 2008, that Honda would be exiting Formula One with immediate effect due to the 2008 global economic crisis. The team was sold to former team principal Ross Brawn, renamed Brawn GP and subsequently Mercedes.
Honda became an official works team in the British Touring Car Championship in 2010.
Honda made an official announcement on 16 May 2013 that it will re-enter Formula One racing in 2015 as an engine supplier to the McLaren team.
Motorcycles.
Honda Racing Corporation (HRC) was formed in 1982. The company combines participation in motorcycle races throughout the world with the development of high potential racing machines. Its racing activities are an important source for the creation of leading edge technologies used in the development of Honda motorcycles. HRC also contributes to the advancement of motorcycle sports through a range of activities that include sales of production racing motorcycles, support for satellite teams, and rider education programs.
Soichiro Honda, being a race driver himself, could not stay out of international motorsport. In 1959, Honda entered five motorcycles into the Isle of Man TT race, the most prestigious motorcycle race in the world. While always having powerful engines, it took until 1961 for Honda to tune their chassis well enough to allow Mike Hailwood to claim their first Grand Prix victories in the 125 and 250 cc classes. Hailwood would later pick up their first Senior TT wins in 1966 and 1967. Honda's race bikes were known for their "sleek & stylish design" and exotic engine configurations, such as the 5-cylinder, 22,000 rpm, 125 cc bike and their 6-cylinder 250 cc and 297 cc bikes.
In 1979, Honda returned to Grand Prix motorcycle racing with the monocoque-framed, four-stroke NR500. The FIM rules limited engines to four cylinders, so the NR500 had non-circular, 'race-track', cylinders, each with 8 valves and two connecting rods, in order to provide sufficient valve area to compete with the dominant two-stroke racers. Unfortunately, it seemed Honda tried to accomplish too much at one time and the experiment failed. For the 1982 season, Honda debuted their first two-stroke race bike, the NS500 and in 1983, Honda won their first 500 cc Grand Prix World Championship with Freddie Spencer. Since then, Honda has become a dominant marque in motorcycle Grand Prix racing, winning a plethora of top level titles with riders such as Mick Doohan and Valentino Rossi.
In the Motocross World Championship, Honda has claimed six world championships. In the World Enduro Championship, Honda has captured eight titles, most recently with Stefan Merriman in 2003 and with Mika Ahola from 2007 to 2010. In observed trials, Honda has claimed three world championships with Belgian rider Eddy Lejeune.
Electric and alternative fuel vehicles.
Compressed natural gas.
The Honda Civic GX is the only purpose-built natural gas vehicle (NGV) commercially available in some parts of the U.S. The Honda Civic GX first appeared in 1998 as a factory-modified Civic LX that had been designed to run exclusively on compressed natural gas. The car looks and drives just like a contemporary Honda Civic LX, but does not run on gasoline. In 2001, the Civic GX was rated the cleanest-burning internal combustion engine in the world by the U.S. Environmental Protection Agency (EPA).
First leased to the City of Los Angeles, in 2005, Honda started offering the GX directly to the public through factory trained dealers certified to service the GX. Before that, only fleets were eligible to purchase a new Civic GX. In 2006, the Civic GX was released in New York, making it the second state where the consumer is able to buy the car. Home refueling is available for the GX with the addition of the Phill Home Refueling Appliance.
Flexible-fuel.
Honda's Brazilian subsidiary launched flexible-fuel versions for the Honda Civic and Honda Fit in late 2006. As other Brazilian flex-fuel vehicles, these models run on any blend of hydrous ethanol (E100) and E20-E25 gasoline. Initially, and in order to test the market preferences, the carmaker decided to produce a limited share of the vehicles with flex-fuel engines, 33 percent of the Civic production and 28 percent of the Fit models. Also, the sale price for the flex-fuel version was higher than the respective gasoline versions, around US$1,000 premium for the Civic, and US$650 for the Fit, despite the fact that all other flex-fuel vehicles sold in Brazil had the same tag price as their gasoline versions. In July 2009, Honda launched in the Brazilian market its third flexible-fuel car, the Honda City.
During the last two months of 2006, both flex-fuel models sold 2,427 cars against 8,546 gasoline-powered automobiles, jumping to 41,990 flex-fuel cars in 2007, and reaching 93,361 in 2008. Due to the success of the flex versions, by early 2009 a hundred percent of Honda's automobile production for the Brazilian market is now flexible-fuel, and only a small percentage of gasoline version is produced in Brazil for exports.
In March 2009, Honda launched in the Brazilian market the first flex-fuel motorcycle in the world. Produced by its Brazilian subsidiary Moto Honda da Amazônia, the CG 150 Titan Mix is sold for around US$2,700.
Hybrid electric.
In late 1999, Honda launched the first commercial hybrid electric car sold in the U.S. market, the Honda Insight, just one month before the introduction of the Toyota Prius, and initially sold for US$20,000. The first-generation Insight was produced from 2000 to 2006 and had a fuel economy of 70 mpgus for the EPA's highway rating, the most fuel-efficient mass-produced car at the time. Total global sales for the Insight amounted to only around 18,000 vehicles. Cumulative global sales reached 100,000 hybrids in 2005 and 200,000 in 2007.
Honda introduced the second-generation Insight in Japan in February 2009, and released it in other markets through 2009 and in the U.S. market in April 2009. At $19,800 as a five-door hatchback it will be the least expensive hybrid available in the U.S.
Since 2002, Honda has also been selling the Honda Civic Hybrid (2003 model) in the U.S. market. It was followed by the Honda Accord Hybrid, offered in model years 2005 through 2007. Sales of the Honda CR-Z began in Japan in February 2010, becoming Honda's third hybrid electric car in the market. s of 2011[ [update]], Honda was producing around 200,000 hybrids a year in Japan.
Sales of the Fit Hybrid began in Japan in October 2010, at the time, the lowest price for a gasoline-hybrid electric vehicle sold in the country. The European version, called Honda Jazz Hybrid, was released in early 2011. During 2011 Honda launched three hybrid models available only in Japan, the Fit Shuttle Hybrid, Freed Hybrid and Freed Spike Hybrid.
Honda's cumulative global hybrid sales passed the 1 million unit milestone at the end of September 2012, 12 years and 11 months after sales of the first generation Insight began in Japan November 1999. A total of 187,851 hybrids were sold worldwide in 2013, and 158,696 hybrids during the first six months of 2014. s of 2014[ [update]], Honda has sold more than 1.35 million hybrids worldwide.
Hydrogen fuel cell.
In Takanezawa, Japan, on 16 June 2008, Honda Motors produced the first assembly-line FCX Clarity, a hybrid hydrogen fuel cell vehicle. More efficient than a gas-electric hybrid vehicle, the FCX Clarity combines hydrogen and oxygen from ordinary air to generate electricity for an electric motor. In July 2014 Honda announced the end of production of the Honda FCX Clarity for the 2015 model.
The vehicle itself does not emit any pollutants and its only by products are heat and water. The FCX Clarity also has an advantage over gas-electric hybrids in that it does not use an internal combustion engine to propel itself. Like a gas-electric hybrid, it uses a lithium ion battery to assist the fuel cell during acceleration and capture energy through regenerative braking, thus improving fuel efficiency. The lack of hydrogen filling stations throughout developed countries will keep production volumes low. Honda will release the vehicle in groups of 150. California is the only U.S. market with infrastructure for fueling such a vehicle, though the number of stations is still limited. Building more stations is expensive, as the California Air Resources Board (CARB) granted $6.8 million for four H2 fueling stations, costing $1.7 million USD each.
Plug-in electric vehicles.
The all-electric Honda EV Plus was introduced in 1997 as a result of CARB's zero-emissions vehicle mandate and was available only for leasing in California. The EV plus was the first battery electric vehicle from a major automaker with non-lead–acid batteries The EV Plus had an all-electric range of 100 mi. Around 276 units were sold in the U.S. and production ended in 1999.
The all-electric Honda Fit EV was introduced in 2012 and has an range of 82 mi. The all-electric car was launched in the U.S. to retail customers in July 2012 with initial availability limited to California and Oregon. Production is limited to only 1,100 units over the first three years. A total of 1,007 units have been leased in the U.S. through September 2014. The Fit EV was released in Japan through leasing to local government and corporate customers in August 2012. Availability in the Japanese market is limited to 200 units during its first two years. In July 2014 Honda announced the end of production of the Fit EV for the 2015 model.
The Honda Accord Plug-in Hybrid was introduced in 2013 and has an all-electric range of 13 mi Sales began in the U.S. in January 2013 and the plug-in hybrid is available only in California and New York. A total of 835 units have been sold in the U.S. through September 2014. The Accord PHEV was introduced in Japan in June 2013 and is available only for leasing, primarily to corporations and government agencies.
Marketing.
Japan.
Starting in 1978, Honda in Japan decided to diversify their sales distribution channels, and created Honda Verno, which sold established products with a higher content of standard equipment and a more sporting nature. The establishment of "Honda Verno" coincided with its new sports compact, called the Honda Prelude. Later, the Honda Vigor, the Honda Ballade, and the Honda Quint were added to "Honda Verno" stores. This approach was implemented due to efforts in place by rival Japanese automakers Toyota and Nissan.
As sales progressed, Honda created two more sales channels, called Honda Clio in 1984, and Honda Primo in 1985. The "Honda Clio" chain sold products that were traditionally associated with Honda dealerships before 1978, like the Honda Accord, and "Honda Primo" sold the Honda Civic, kei cars, such as the Honda Today, superminis like the Honda Capa, along with other Honda products, such as farm equipment, lawn mowers, portable generators, marine equipment, plus motorcycles and scooters like the Honda Super Cub. A styling tradition was established when "Honda Primo" and "Clio" began operations, in that all "Verno" products had the rear license plate installed in the rear bumper, while "Primo" and "Clio" products had the rear license plate installed on the trunk lid or rear door for minivans.
As time progressed and sales began to diminish partly due to the collapse of the Japanese "bubble economy", "supermini" and "kei" vehicles that were specific to "Honda Primo" were "badge engineered" and sold at the other two sales channels, thereby providing smaller vehicles that sold better at both "Honda Verno" and "Honda Clio" locations. As of March 2006, the three sales chains were discontinued, with the establishment of "Honda Cars" dealerships.
Honda sells genuine accessories through a separate retail chain called "" for both their motorcycle, scooter and automobile products. In cooperation with corporate "keiretsu" partner Pioneer, Honda sells an aftermarket line of audio and in-car navigation equipment that can be installed in any vehicle under the brand name , which is available at Honda Access locations as well as Japanese auto parts retailers, such as Autobacs. Buyers of used vehicles are directed to a specific Honda retail chain that sells only used vehicles called "."
In the spring of 2012, Honda in Japan introduced "" (Japanese) which is devoted to compact cars like the Honda Fit, and "kei" vehicles like the Honda Today.
Prelude, Integra, CR-X, Vigor, Saber, Ballade, Quint, Crossroad, Element, NSX, HR-V, Mobilio Spike, S2000, CR-V, That's, MDX, Rafaga, Capa, and the Torneo
Accord, Legend, Inspire, Avancier, S-MX, Lagreat, Stepwgn, Elysion, Stream, Odyssey (int'l), Domani, Concerto, Accord Tourer, Logo, Fit, Insight, That's, Mobilio, and the City
Civic, Life, Acty, Vamos, Hobio, Ascot, Ascot Innova, Torneo, Civic Ferio, Freed, Mobilio, Orthia, Capa, Today, Z, and the Beat
International efforts.
In 2003, Honda released its "Cog" advertisement in the UK and on the Internet. To make the ad, the engineers at Honda constructed a Rube Goldberg Machine made entirely out of car parts from a Honda Accord Touring. To the chagrin of the engineers at Honda, all the parts were taken from two of only six hand-assembled pre-production models of the Accord. The advertisement depicted a single cog which sets off a chain of events that ends with the Honda Accord moving and Garrison Keillor speaking the tagline, "Isn't it nice when things just... work?" It took 606 takes to get it perfect.
In 2004, they produced the "Grrr" advert, usually immediately followed by a shortened version of the 2005 "Impossible Dream" advert.
In December 2005, Honda released "The Impossible Dream" a two-minute panoramic advertisement filmed in New Zealand, Japan and Argentina which illustrates the founder's dream to build performance vehicles. While singing the song "Impossible Dream", a man reaches for his racing helmet, leaves his trailer on a minibike, then rides a succession of vintage Honda vehicles: a motorcycle, then a car, then a powerboat, then goes over a waterfall only to reappear piloting a hot air balloon, with Garrison Keillor saying "I couldn't have put it better myself" as the song ends. The song is from the 1960s musical "Man Of La Mancha", sung by Andy Williams.
In 2006, Honda released its "Choir" advertisement, for the UK and the internet. This had a 60-person choir who sang the car noises as film of the Honda Civic are shown.
In the mid to late 2000s in the United States, during model close-out sales for the current year before the start of the new model year, Honda's advertising has had an animated character known simply as Mr. Opportunity, voiced by Rob Paulsen. The casual looking man talked about various deals offered by Honda and ended with the phrase "I'm Mr. Opportunity, and I'm knockin'", followed by him "knocking" on the television screen or "thumping" the speaker at the end of radio ads. In addition, commercials for Honda's international hatchback, the Jazz, are parodies of well-known pop culture images such as Tetris and Thomas The Tank Engine.
In late 2006, Honda released an ad with ASIMO exploring a museum, looking at the exhibits with almost childlike wonderment (spreading out its arms in the aerospace exhibit, waving hello to an astronaut suit that resembles him, etc.), while Garrison Keillor ruminates on progress. It concludes with the tagline: "More forwards please".
Honda also sponsored ITV's coverage of Formula One in the UK for 2007. However they had announced that they would not continue in 2008 due to the sponsorship price requested by ITV being too high.
In May 2007, focuses on their strengths in racing and the use of the Red H badge – a symbol of what is termed as "Hondamentalism". The campaign highlights the lengths that Honda engineers go to in order to get the most out of an engine, whether it is for bikes, cars, powerboats – even lawnmowers. Honda released its campaign. In the TV spot, Garrison Keillor says, "An engineer once said to build something great is like swimming in honey", while Honda engineers in white suits walk and run towards a great light, battling strong winds and flying debris, holding on to anything that will keep them from being blown away. Finally one of the engineers walks towards a red light, his hand outstretched. A web address is shown for the Hondamentalism website. The digital campaign aims to show how visitors to the site share many of the Hondamentalist characteristics.
At the beginning of 2008, Honda released – the "Problem Playground". The advert outlines Honda's environmental responsibility, demonstrating a hybrid engine, more efficient solar panels and the FCX Clarity, a hydrogen powered car. The 90 second advert has large scale puzzles, involving Rubik's Cubes, large shapes and a 3-dimensional puzzle.
On 29 May 2008, Honda, in partnership with Channel 4, broadcast a live advertisement. It showed skydivers jumping from an aeroplane over Spain and forming the letters H, O, N, D and A in mid-air. This live advertisement is generally agreed to be the first of its kind on British television. The advert lasted three minutes.
In 2009, American Honda released the "Dream the Impossible" documentary series, a collection of 5–8 minute web vignettes that focus on the core philosophies of Honda. Current short films include "Failure: The Secret to Success", "Kick Out the Ladder" and "Mobility 2088". They have Honda employees as well as Danica Patrick, Christopher Guest, Ben Bova, Chee Pearlman, Joe Johnston and Orson Scott Card. The film series plays at dreams.honda.com.
Sports.
In Australia, Honda advertised heavily during most motor racing telecasts, and was the official sponsor of the 2006 FIA Formula 1 telecast on broadcaster channel "Ten". In fact, it was the only manufacturer involved in the 2006 Indy Racing League season. In a series of adverts promoting the history of Honda's racing heritage, Honda claimed it "built" cars that won 72 Formula 1 Grand Prix. Skeptics have accused Honda of interpreting its racing history rather liberally, saying that virtually all of the 72 victories were achieved by Honda "powered" (engined) machines, whereas the cars themselves were designed and built by Lotus F1, Williams F1, and McLaren F1 teams, respectively. However, former and current staff of the McLaren F1 team have reiterated that Honda contributed more than just engines and provided various chassis, tooling, and aerodynamic parts as well as funding.
The late F1 driver Ayrton Senna stated that Honda probably played the most significant role in his three world championships. He had immense respect for founder, Soichiro Honda, and had a good relationship with Nobuhiko Kawamoto, the chairman of Honda at that time. Senna once called Honda "the greatest company in the world".
As part of its marketing campaign, Honda is an official partner and sponsor of the National Hockey League, the Anaheim Ducks of the NHL, and the arena named after it: Honda Center. Honda also sponsors The Honda Classic golf tournament and is a sponsor of Major League Soccer. The "Honda Player of the Year" award is presented in United States soccer. The "Honda Sports Award" is given to the best female athlete in each of twelve college sports in the United States. One of the twelve Honda Sports Award winners is chosen to receive the Honda-Broderick Cup, as "Collegiate Woman Athlete of the Year."
Honda will be sponsoring La Liga club Valencia CF starting from 2014–15 season. Valencia CF will carry "Honda Cars Valencia" insignia on their football kits. 
Honda has been a presenting sponsor of the Los Angeles Marathon since 2010 in a three-year sponsorship deal, with winners of the LA Marathon receiving a free Honda Accord. Since 1989, the Honda Campus All-Star Challenge has been a quizbowl tournament for Historically black colleges and universities.
External links.
class="navbox collapsible autocollapse"
!colspan="39" class="navbox-title"

</doc>
<doc id="13730" url="http://en.wikipedia.org/wiki?curid=13730" title="Handball">
Handball

Handball (also known as team handball, Olympic handball, European team handball, European handball, or Borden ball) is a team sport in which two teams of seven players each (six outfield players and a goalkeeper) pass a ball using their hands with the aim of throwing it into the goal of the other team. A standard match consists of two periods of 30 minutes, and the team that scores more goals wins.
Modern handball is played on a court 40 by, with a goal in the center of each end. The goals are surrounded by a 6-meter zone where only the defending goalkeeper is allowed; the goals must be scored by throwing the ball from outside the zone or while "jumping" into it. The sport is usually played indoors, but outdoor variants exist in the forms of field handball and Czech handball (which were more common in the past) and beach handball (also called sandball). The game is quite fast and includes body contact, as the defenders try to stop the attackers from approaching the goal. Goals are scored quite frequently; teams typically score between 20 and 35 goals each.
The game was codified at the end of the 19th century in northern Europe, chiefly in Scandinavia and Germany. The modern set of rules was published in 1917 in Germany, and had several revisions since. The first international games were played under these rules for men in 1925 and for women in 1930. Men's handball was first played at the 1936 Summer Olympics in Berlin as outdoors, and the next time at the 1972 Summer Olympics in Munich as indoors, and has been an Olympics sport since. Women's team handball was added at the 1976 Summer Olympics.
The International Handball Federation was formed in 1946, and as of 2013[ [update]] has 174 member federations. The sport is most popular in continental Europe, whose countries have won all medals but one in men's world championships since 1938, and all women's titles until 2013, when Brazil broke the series. The game also enjoys popularity in the Far East, North Africa and Brazil.
Origins and development.
There is evidence of ancient Roman women playing a version of handball called "expulsim ludere". There are records of handball-like games in medieval France, and among the Inuit in Greenland, in the Middle Ages. By the 19th century, there existed similar games of "håndbold" from Denmark, "házená" in the Czech Republic, "hádzaná" in Slovakia, "gandbol" in Ukraine, and "torball" in Germany.
The team handball game of today was codified at the end of the 19th century in northern Europe—primarily in Denmark, Germany, Norway and Sweden. The first written set of team handball rules was published in 1906 by the Danish gym teacher, lieutenant and Olympic medalist Holger Nielsen from Ordrup grammar school north of Copenhagen. The modern set of rules was published on 29 October 1917 by Max Heiser, Karl Schelenz, and Erich Konigh from Germany. After 1919 these rules were improved by Karl Schelenz. The first international games were played under these rules, between Germany and Belgium for men in 1925 and between Germany and Austria for women in 1930.
In 1926, the Congress of the International Amateur Athletics Federation nominated a committee to draw up international rules for field handball. The International Amateur Handball Federation was formed in 1928, and the International Handball Federation was formed in 1946.
Men's field handball was played at the 1936 Summer Olympics in Berlin. During the next several decades, indoor handball flourished and evolved in the Scandinavian countries. The sport re-emerged onto the world stage as team handball for the 1972 Summer Olympics in Munich. Women's team handball was added at the 1976 Summer Olympics. Due to its popularity in the region, the Eastern European countries that refined the event became the dominant force in the sport when it was reintroduced.
The International Handball Federation organised the men's world championship in 1938 and every 4 (sometimes 3) years from World War II to 1995. Since the 1995 world championship in Iceland, the competition has been every two years. The women's world championship has been played since 1957. The IHF also organizes women's and men's junior world championships. By July 2009, the IHF listed 166 member federations - approximately 795,000 teams and 19 million players.
Rules.
The rules are laid out in the IHF's set of rules.
Summary.
Two teams of seven players (six field players plus one goalkeeper) take the field and attempt to score points by putting the game ball into the opposing team's goal. In handling the ball, players are subject to the following restrictions:
Notable scoring opportunities can occur when attacking players jump into the goal area. For example, an attacking player may catch a pass while launching inside the goal area, and then shoot or pass before touching the floor. "Doubling" occurs when a diving attacking player passes to another diving team-mate.
Playing field.
Handball is played on a court 40 x, with a goal in the centre of each end. The goals are surrounded by a near-semicircular area, called the zone or the crease, defined by a line six meters from the goal. A dashed near-semicircular line nine metres from the goal marks the free-throw line. Each line on the court is part of the area it encompasses. This implies that the middle line belongs to both halves at the same time.
Goals.
Each goal has a circle clearance area of three meters in width and two meters in height. It must be securely bolted either to the floor or the wall behind.
The goal posts and the crossbar must be made out of the same material (e.g., wood or aluminium) and feature a quadratic cross section with sides of 8 cm. The three sides of the beams visible from the playing field must be painted alternatingly in two contrasting colors which both have to contrast against the background. The colors on both goals must be the same.
Each goal must feature a net. This must be fastened in a such a way that a ball thrown into does not leave or pass the goal under normal circumstances. If necessary, a second net may be clasped to the back of the net on the inside.
D-Zone.
The goals are surrounded by the crease. This area is delineated by two quarter circles with a radius of six metres around the far corners of each goal post and a connecting line parallel to the goal line. Only the defending goalkeeper is allowed inside this zone. However, the court players may catch and touch the ball in the air within it as long as the player starts his jump outside the zone and releases the ball before he lands (landing inside the perimeter is allowed in this case as long as the ball has been released).
If a player without the ball contacts the ground inside the goal perimeter, or the line surrounding the perimeter, he must take the most direct path out of it. However, should a player cross the zone in an attempt to gain an advantage (e.g., better position) their team cedes the ball. Similarly, violation of the zone by a defending player is penalized only if they do so in order to gain an advantage in defending.
Substitution area.
Outside of one long edge of the playing field to both sides of the middle line are the substitution areas for each team. The areas usually contain the benches as seating opportunities. Team officials, substitutes, and suspended players must wait within this area. The area always lies to the same side as the team's own goal. During half-time, substitution areas are swapped. Any player entering or leaving the play must cross the substitution line which is part of the side line and extends 4.5 meters from the middle line to the team's side.
Duration.
 A standard match for all teams of 16 and older has two periods of 30 minutes with an interval of 10–15 minutes. At half-time, teams switch sides of the court as well as benches. For youths the length of the halves is reduced—25 minutes at ages 12 to 16, and 20 minutes at ages 8 to 12; though national federations of some countries may differ in their implementation from the official guidelines.
If a decision must be reached in a particular match (e.g., in a tournament) and it ends in a draw after regular time, there are at maximum two overtimes, each consisting of two straight 5-minute periods with a one-minute break in between. Should these not decide the game either, the winning team is determined in a penalty shootout (best-of-five rounds; if still tied, extra rounds afterwards until won by one team).
The referees may call "timeout" according to their sole discretion; typical reasons are injuries, suspensions, or court cleaning. Penalty throws should trigger a timeout only for lengthy delays, such as a change of the goalkeeper.
Each team may call one "team timeout" (TTO) per period which lasts one minute. This right may only be invoked by team in ball possession. To do so, the representative of the team lays a green card marked with a black "T" on the desk of the timekeeper. The timekeeper then immediately interrupts the game by sounding an acoustic signal and stops the time. 
As of 2012, rule changes allow three TTOs, and two of them can be used in either period of the game or overtime.
Referees.
A handball match is led by two equal referees, namely the goal line referee and the court referee. Some national bodies allow games with only a single referee in special cases like illness on short notice. Should the referees disagree on any occasion, a decision is made on mutual agreement during a short timeout; or, in case of punishments, the more severe of the two comes into effect. The referees are obliged to make their decisions "on the basis of their observations of facts". Their judgements are final and can be appealed against only if not in compliance with the rules.
 The referees position themselves in such a way that the team players are confined between them. They stand diagonally aligned so that each can observe one side line. Depending on their positions, one is called "field referee" and the other "goal referee". These positions automatically switch on ball turnover. They physically exchange their positions approximately every 10 minutes (long exchange), and change sides every five minutes (short exchange).
The IHF defines 18 hand signals for quick visual communication with players and officials. The signal for warning or disqualification is accompanied by a yellow or red card, respectively. The referees also use whistle blows to indicate infractions or to restart the play.
The referees are supported by a "scorekeeper" and a "timekeeper" who attend to formal things such as keeping track of goals and suspensions, or starting and stopping the clock, respectively. They also keep an eye on the benches and notify the referees on substitution errors. Their desk is located in between the two substitutions areas.
Team players, substitutes, and officials.
Each team consists of seven players on court and seven substitute players on the bench. One player on the court must be the designated goalkeeper, differing in his clothing from the rest of the field players. Substitution of players can be done in any number and at any time during game play. An exchange takes place over the substitution line. A prior notification of the referees is not necessary.
Some national bodies, such as the Deutsche Handball Bund (DHB, "German Handball Federation"), allow substitution in junior teams only when in ball possession or during timeouts. This restriction is intended to prevent early specialization of players to offence or defence.
Field players.
Field players are allowed to touch the ball with any part of their bodies above and including the knee. As in several other team sports, a distinction is made between catching and dribbling. A player who is in possession of the ball may stand stationary for only three seconds, and may take only three steps. They must then either shoot, pass, or dribble the ball. Taking more than three steps at any time is considered travelling, and results in a turnover. A player may dribble as many times as they want (though, since passing is faster, it is the preferred method of attack), as long as during each dribble the hand contacts only the top of the ball. Therefore, carrying is completely prohibited, and results in a turnover. After the dribble is picked up, the player has the right to another three seconds or three steps. The ball must then be passed or shot, as further holding or dribbling will result in a "double dribble" turnover and a free throw for the other team. Other offensive infractions that result in a turnover include charging, setting an illegal screen, or carrying the ball into the six-meter zone.
Goalkeeper.
Only the goalkeepers are allowed to move freely within the goal perimeter, although they may not cross the goal perimeter line while carrying or dribbling the ball. Within the zone, they are allowed to touch the ball with all parts of their bodies including their feet. The goalkeepers may participate in the normal play of their teammates. They may be substituted by a regular field player if their team elects to use this scheme in order to outnumber the defending players. This field player becomes the designated goalkeeper on the court; and must wear some vest or bib to be identified as such.
If either goalkeeper deflects the ball over the outer goal line, their team stays in possession of the ball, in contrast to other sports like soccer. The goalkeeper resumes the play with a throw from within the zone ("goalkeeper throw"). Passing to one's own goalkeeper results in a turnover. Throwing the ball against the head of the goalkeeper when he is not moving is to be punished by disqualification ("red card").
Team officials.
Each team is allowed to have a maximum of four team officials seated on the benches. An official is anybody who is neither player nor substitute. One official must be the designated representative who is usually the team manager. The representative may call a team timeout once every period and may address the scorekeeper, timekeeper, and referees. As of 2012, the representative may call a total of three team timeouts, with a maximum of two per period or overtime. Other officials typically include physicians or managers. Neither official is allowed to enter the playing court without the permission of the referees.
Ball.
 The ball is spherical and must be made either of leather or a synthetic material. It is not allowed to have a shiny or slippery surface. As the ball is intended to be operated by a single hand, its official sizes vary depending on age and gender of the participating teams.
Though this is not officially regulated, the ball is usually resinated. The resin improves the ability of the players to manipulate the ball with a single hand, as in spinning trick shots. Some indoor arenas prohibit the usage of resin, since many products leave sticky stains on the floor.
Awarded throws.
The referees may award a special throw to a team. This usually happens after certain events such as scored goals, off-court balls, turnovers and timeouts. All of these special throws require the thrower to obtain a certain position, and pose restrictions on the positions of all other players. Sometimes the execution must wait for a whistle blow by the referee.
Penalties.
Penalties are given to players, in progressive format, for fouls that require more punishment than just a free-throw. Actions directed mainly at the opponent and not the ball (such as reaching around, holding, pushing, hitting, tripping, and jumping into opponent) as well as contact from the side, from behind a player or impeding the opponent's counterattack are all considered illegal and are subject to penalty. Any infraction that prevents a clear scoring opportunity will result in a seven-meter penalty shot.
Typically the referee will give a warning yellow card for an illegal action; but, if the contact was particularly dangerous, like striking the opponent in the head, neck or throat, the referee can forego the warning for an immediate two-minute suspension. A player can get only one warning before receiving a two-minute suspension. One player is only permitted two two-minute suspensions; after the third time, they will be shown the red card.
A red card results in an ejection from the game and a two-minute penalty for the team. A player may receive a red card directly for particularly rough penalties. For instance, any contact from behind during a fast break is now being treated with a red card. A red-carded player has to leave the playing area completely. A player who is disqualified may be substituted with another player after the two-minute penalty is served. A coach or official can also be penalized progressively. Any coach or official who receives a two-minute suspension will have to pull out one of their players for two minutes—note: the player is not the one punished, and can be substituted in again, as the penalty consists of the team playing with a one player less than the opposing team.
After having lost the ball during an attack, the ball has to be laid down quickly or else the player not following this rule will face a two-minute suspension. Also, gesticulating or verbally questioning the referee's order, as well as arguing with the officials' decisions, will normally result in a two-minute suspension. If this is done in a very provocative way, the player can be given a double two-minute suspension if they do not walk straight off the field to the bench after being given a suspension, or if the referee deems the tempo deliberately slow. Illegal substitution (that is, any substitution that does not take place in the specified substitution area, or where the entering player enters before the exiting player exits) is also punishable by a two-minute suspension.
Gameplay.
Formations.
Players are typically referred to by the position they are playing. The positions are always denoted from the view of the respective goalkeeper, so that a defender on the right opposes an attacker on the left. However, not all of the following positions may be occupied depending on the formation or potential suspensions.
Offensive play.
Attacks are played with all field players on the side of the defenders. Depending on the speed of the attack, one distinguishes between three attack "waves" with a decreasing chance of success:
The third wave evolves into the normal offensive play when all defenders not only reach the zone, but gain their accustomed positions. Some teams then substitute specialized offence players. However, this implies that these players must play in the defence should the opposing team be able to switch quickly to offence. The latter is another benefit for fast playing teams.
If the attacking team does not make sufficient progress (eventually releasing a shot on goal), the referees can call passive play (since about 1995, the referee gives a passive warning some time before the actual call by holding one hand up in the air, signalling that the attacking team should release a shot soon), turning control over to the other team. A shot on goal or an infringement leading to a yellow card or two-minute penalty will mark the start of a new attack, causing the hand to be taken down; but a shot blocked by the defense or a normal free throw will not. If it were not for this rule, it would be easy for an attacking team to stall the game indefinitely, as it is difficult to intercept a pass without at the same time conceding dangerous openings towards the goal.
Defensive play.
The usual formations of the defense are 6–0, when all the defense players line up between the 6-meter and 9-meter lines to form a wall; the 5–1, when one of the players cruises outside the 9-meter perimeter, usually targeting the center forwards while the other 5 line up on the 6-meter line; and the less common 4–2 when there are two such defenders out front. Very fast teams will also try a 3–3 formation which is close to a switching man-to-man style. The formations vary greatly from country to country, and reflect each country's style of play. 6–0 is sometimes known as "flat defense", and all other formations are usually called "offensive defense".
Organisation.
Handball teams are usually organised as clubs. On a national level, the clubs are associated in federations which organize matches in leagues and tournaments.
International body.
The International Handball Federation (IHF) is the administrative and controlling body for international handball. The federation organizes world championships, held in uneven years, with separate competitions for men and women.
The IHF World Men's Handball Championship 2009 and 2011 title holders were France; the 2013 title holders are Spain.
The IHF World Women's Handball Championship 2009 title holders were Russia; the IHF 2011 Women’s World Championship title holders were Norway. The IHF 2013 Women’s World Championship title holders are Brazil, representing the first team from the American continents to hold the title.
The IHF is composed of five continental federations which organize continental championships held every other second year. In addition to these competitions between national teams, the federations arrange international tournaments between club teams. The federations and their corresponding tournaments and members are summarize in the following table:
Handball is an Olympic sport played during the Summer Olympics. It is also played during the Pan American Games, All-Africa Games, and Asian Games. It is also played on Mediterranean games.
Attendance records.
The current worldwide attendance record for seven-a-side handball was set on 21 May 2011, during the Danish Cup final between AG København (Copenhagen) and Bjerringbro-Silkeborg Elitehåndbold. The game drew 36,651 spectators to Parken Stadium.
On 20 April 2012, the EHF Champions League attendance record was also set at Parken, albeit in a different seating configuration. 21,293 spectators saw AG København host defending champions FC Barcelona Handbol in a quarterfinal game. 
Commemorative coins.
Handball events have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Handball commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the coin, the modern athlete directs the ball in his hands towards his target, while in the background the ancient athlete is just about to throw a ball, in a game known as cheirosphaira, in a representation taken from a black-figure pottery vase of the Archaic period.
The most recent commemorative coin featuring handball is the British 50 pence coin, part of the series of coins commemorating the London 2012 Olympic Games 
References.
Notes
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="13733" url="http://en.wikipedia.org/wiki?curid=13733" title="Hilbert's basis theorem">
Hilbert's basis theorem

In mathematics, specifically commutative algebra, Hilbert's basis theorem says that a polynomial ring over a Noetherian ring is Noetherian.
Statement.
If formula_1 a ring, let formula_2 denote the ring of polynomials in the indeterminate formula_3 over formula_1. Hilbert proved that if formula_1 is "not too large", in the sense that if formula_1 is Noetherian, the same must be true for formula_2. Formally,
Hilbert's Basis Theorem. If formula_1 is a Noetherian ring, then formula_2 is a Noetherian ring.
Corollary. If formula_1 is a Noetherian ring, then formula_11 is a Noetherian ring.
This can be translated into algebraic geometry as follows: every algebraic set over a field can be described as the set of common roots of finitely many polynomial equations. Hilbert (1890) proved the theorem (for the special case of polynomial rings over a field) in the course of his proof of finite generation of rings of invariants. 
Hilbert produced an innovative proof by contradiction using mathematical induction; his method does not give an algorithm to produce the finitely many basis polynomials for a given ideal: it only shows that they must exist. One can determine basis polynomials using the method of Gröbner bases.
Proof.
Remark. We will give two proofs, in both only the "left" case is considered, the proof for the right case is similar.
First Proof.
Suppose formula_14 were a non-finitely generated left-ideal. Then by recursion (using the axiom of dependent choice) there is a sequence formula_15 of polynomials such that if formula_16 is the left ideal generated by formula_17 then formula_18 in formula_19 is of minimal degree. It is clear that formula_20 is a non-decreasing sequence of naturals. Let formula_21 be the leading coefficient of formula_18 and let formula_23 be the left ideal in formula_1 generated by formula_25. Since formula_1 is Noetherian the chain of ideals formula_27 must terminate. Thus formula_28 for some integer formula_29. So in particular, 
Now consider 
whose leading term is equal to that of formula_32; moreover, formula_33. However, formula_34, which means that formula_35 has degree less than formula_32, contradicting the minimality.
Second Proof.
Let formula_14 be a left-ideal. Let formula_23 be the set of leading coefficients of members of formula_39. This is obviously a left-ideal over formula_1, and so is finitely generated by the leading coefficients of finitely many members of formula_39; say formula_42. Let formula_43 be the maximum of the set formula_44, and let formula_45 be the set of leading coefficients of members of formula_39, whose degree is formula_47. As before, the formula_45 are left-ideals over formula_1, and so are finitely generated by the leading coefficients of finitely many members of formula_39, say 
with degrees formula_47. Now let formula_53 be the left-ideal generated by 
We have formula_55 and claim also formula_56. Suppose for the sake of contradiction this is not so. Then let formula_57 be of minimal degree, and denote its leading coefficient by formula_58.
Thus our claim holds, and formula_74 which is finitely generated.
Note that the only reason we had to split into two cases was to ensure that the powers of formula_3 multiplying the factors, were non-negative in the constructions.
Applications.
Let formula_1 be a Noetherian commutative ring. Hilbert's basis theorem has some immediate corollaries. 
Mizar System.
The Mizar project has completely formalized and automatically checked a proof of Hilbert's basis theorem in the .

</doc>
<doc id="13734" url="http://en.wikipedia.org/wiki?curid=13734" title="Heterocyclic compound">
Heterocyclic compound

A heterocyclic compound or ring structure is a cyclic compound that has atoms of at least two different elements as members of its ring(s). Heterocyclic chemistry is the branch of chemistry dealing with the synthesis, properties and applications of these heterocycles. In contrast, the rings of homocyclic compounds consist entirely of atoms of the same element.
Although heterocyclic compounds may be inorganic, most contain at least one carbon. While atoms that are neither carbon nor hydrogen are normally referred to in organic chemistry as heteroatoms, this is usually in comparison to the all-carbon backbone. But this does not prevent a compound such as borazine (which has no carbon atoms) from being labelled "heterocyclic". IUPAC recommends the Hantzsch-Widman nomenclature for naming heterocyclic compounds.
Classification based on electronic structure.
Heterocyclic compounds can be usefully classified based on their electronic structure. The saturated heterocycles behave like the acyclic derivatives. Thus, piperidine and tetrahydrofuran are conventional amines and ethers, with modified steric profiles. Therefore, the study of heterocyclic chemistry focuses especially on unsaturated derivatives, and the preponderance of work and applications involves unstrained 5- and 6-membered rings. Included are pyridine, thiophene, pyrrole, and furan. Another large class of heterocycles are fused to benzene rings, which for pyridine, thiophene, pyrrole, and furan are quinoline, benzothiophene, indole, and benzofuran, respectively. Fusion of two benzene rings gives rise to a third large family of compounds, respectively the acridine, dibenzothiophene, carbazole, and dibenzofuran. The unsaturated rings can be classified according to the participation of the heteroatom in the pi system.
3-membered rings.
Heterocycles with three atoms in the ring are more reactive because of ring strain. Those containing one heteroatom are, in general, stable. Those with two heteroatoms are more likely to occur as reactive intermediates.<br>
Common 3-membered heterocycles with "one" heteroatom are:
Those with "two" heteroatoms include:
4-membered rings.
Compounds with one heteroatom:
Compounds with two heteroatoms:
5-membered rings.
With heterocycles containing five atoms, the unsaturated compounds are frequently more stable because of aromaticity.
Five-membered rings with "one" heteroatom:
The 5-membered ring compounds containing "two" heteroatoms, at least one of which is nitrogen, are collectively called the azoles. Thiazoles and isothiazoles contain a sulfur and a nitrogen atom in the ring. Dithiolanes have two sulfur atoms.
A large group of 5-membered ring compounds with "three" heteroatoms also exists. One example is dithiazoles that contain two sulfur and a nitrogen atom.
Five-member ring compounds with "four" heteroatoms:
With 5-heteroatoms, the compound may be considered inorganic rather than heterocyclic. Pentazole is the all nitrogen heteroatom unsaturated compound.
6-membered rings.
Six-membered rings with a "single" heteroatom:
With "two" heteroatoms:
With three heteroatoms:
With four heteroatoms:
The hypothetical compound with six nitrogen heteroatoms would be hexazine.
7-membered rings.
With 7-membered rings, the heteroatom must be able to provide an empty pi orbital (e.g., boron) for "normal" aromatic stabilization to be available; otherwise, homoaromaticity may be possible. Compounds with one heteroatom include:
Those with two heteroatoms include:
Fused rings.
Heterocyclic rings systems that are formally derived by fusion with other rings, either carbocyclic or heterocyclic, have a variety of common and systematic names. For example, with the benzo-fused unsaturated nitrogen heterocycles, pyrrole provides indole or isoindole depending on the orientation. The pyridine analog is quinoline or isoquinoline. For azepine, benzazepine is the preferred name. Likewise, the compounds with two benzene rings fused to the central heterocycle are carbazole, acridine, and dibenzoazepine.
History of heterocyclic chemistry.
The history of heterocyclic chemistry began in the 1800s, in step with the development of organic chemistry. Some noteworthy developments:
1818: Brugnatelli isolates alloxan from uric acid
1832: Dobereiner produces furfural (a furan) by treating starch with sulfuric acid
1834: Runge obtains pyrrole ("fiery oil") by dry distillation of bones
1906: Friedlander synthesizes indigo dye, allowing synthetic chemistry to displace a large agricultural industry
1936: Treibs isolates chlorophyl derivatives from crude oil, explaining the biological origin of petroleum.
1951: Chargaff's rules are described, highlighting the role of heterocyclic compounds (purines and pyrimidines) in the genetic code.
Commercial exploitation.
Leading companies with a vast number of patents related to heterocyclic compounds are Bayer, Merck, Ciba-Geigy, Pfizer, Eli Lily, BASF, Hoffmann La Roche, ER Sqibb, Warner Lambert and Hoechst.

</doc>
<doc id="13742" url="http://en.wikipedia.org/wiki?curid=13742" title="Hero Wars">
Hero Wars

Hero Wars is the name for the fantasy role-playing game published by Issaries, Inc.
Description.
The first-edition rulebook, "Hero Wars", was published in 2000.
Like RuneQuest, Hero Wars is set in the world of Glorantha but the rules system is designed for more epic games; there is no real relationship between the RuneQuest game and Hero Wars except for the setting of Glorantha.
Hero Wars used an innovative resolution system capable of fulfilling either simulationist or narrativist play with no modification. Some players found the abstraction of the system problematic, while others have found it to have been a better match for the mythic vision of Glorantha.
The first-edition core books had serious quality issues as the publisher did not have sufficient funding to complete production.
As of 2002 the line has expanded to
The game's extensively revised second edition was published in 2003 as HeroQuest; the Hero Wars products are highly compatible, and conversion guidelines are available at http://www.heroquest-rpg.com

</doc>
<doc id="13743" url="http://en.wikipedia.org/wiki?curid=13743" title="Harry Connick, Jr.">
Harry Connick, Jr.

Joseph Harry Fowler Connick, Jr. (born September 11, 1967) is an American singer, musician and actor. He has sold over 28 million albums worldwide. Connick is ranked among the top 60 best-selling male artists in the United States by the Recording Industry Association of America, with 16 million in certified sales. He has had seven top 20 US albums, and ten number-one US jazz albums, earning more number-one albums than any other artist in US jazz chart history.
Connick's best-selling album in the United States is his 1993 Christmas album "When My Heart Finds Christmas", which is also one of the best selling Christmas albums in the United States. His highest-charting album is his 2004 release "Only You", which reached No. 5 in the U.S. and No. 6 in Britain. He has won three Grammy Awards and two Emmy Awards. He played Grace's husband, Dr. Leo Markus, on the TV sitcom "Will & Grace" from 2002 to 2006.
Connick began his acting career as a tail gunner in the World War II film "Memphis Belle" in 1990. He played a serial killer in "Copycat" in 1995, before being cast as jet fighter pilot in the 1996 blockbuster "Independence Day". Connick's first role as a leading man was in 1998's "Hope Floats" with Sandra Bullock. His first thriller film since "Copycat" came in 2003 in the film "Basic" with John Travolta. Additionally, he played the violent ex-husband in "Bug", before two romantic comedies, 2007's "P.S. I Love You", and the leading man in "New in Town" with Renée Zellweger in 2009. In 2011, he appeared in the family film "Dolphin Tale" as Dr. Clay Haskett and in the 2014 sequel, "Dolphin Tale 2".
Early life.
Harry Connick, Jr. was born and raised in New Orleans, Louisiana. His mother, Anita Frances (née Levy; later Livingston; May 22, 1926 – July 1981), was a lawyer and judge in New Orleans and, later, a Louisiana Supreme Court justice. His father, Joseph Harry Fowler Connick, Sr., was the district attorney of Orleans Parish from 1973–2003. His parents also owned a record store. Connick's father is a Catholic of Irish, English, and German ancestry. Connick's mother, who died from ovarian cancer, was Jewish (her parents had immigrated from Minsk and Vienna, respectively). Connick has a sister, Suzanna; the siblings were raised in the Lakeview neighborhood of New Orleans. Connick is a first cousin of both Jefferson Parish District Attorney, Paul Connick, and State Representative Patrick Connick (of Harvey, Jefferson Parish).
Connick's musical talents soon came to the fore when he started learning the keyboards at age three, playing publicly at age five, and recording with a local jazz band at ten. When Connick was nine years old, he performed the Piano Concerto No. 3 Opus 37 of Beethoven with the New Orleans Symphony Orchestra (now the Louisiana Philharmonic), and later played a duet with Eubie Blake at the Royal Orleans Esplanade Lounge in New Orleans. The song was "I'm Just Wild About Harry". This was recorded for a Japanese documentary called "Jazz Around the World". The clip was also shown in a Bravo special, called "Worlds of Harry Connick, Junior." in 1999. His musical talents were developed at the New Orleans Center for Creative Arts and under the tutelage of Ellis Marsalis, Jr. and James Booker.
Connick attended Jesuit High School, Isidore Newman School, Lakeview School, and the New Orleans Center for Creative Arts, all in New Orleans. Following an unsuccessful attempt to study jazz academically, and having given recitals in the classical and jazz piano programs at Loyola University, Connick moved to the 92nd Street YMHA in New York City to study at Hunter College and the Manhattan School of Music, where a Columbia Records executive Sr. V.P. of A&R, Dr. George Butler, persuaded him to sign with that label. His first record for the label, "Harry Connick Junior.", was a mainly instrumental album of standards. He soon acquired a reputation in jazz because of extended stays at high-profile New York venues. His next album, "20", featured his vocals and added to this reputation.
Career.
"When Harry Met Sally...", chart and movie success.
With Connick's reputation growing, director Rob Reiner asked him to provide a soundtrack for his 1989 romantic comedy, "When Harry Met Sally...", starring Meg Ryan and Billy Crystal. The soundtrack consisted of several standards, including "It Had to Be You", "Let's Call the Whole Thing Off" and "Don't Get Around Much Anymore", and achieved double-platinum status in the United States. He won his first Grammy Award for Best Jazz Male Vocal Performance for his work on the soundtrack.
Connick made his screen debut in "Memphis Belle" (1990), about a B-17 Flying Fortress bomber crew in World War II. In that year he began a two-year world tour. In addition he released two albums in July 1990: the instrumental jazz trio album "Lofty's Roach Souffle" and a big-band album of mostly original songs titled "We Are in Love", which also went double platinum. "We Are in Love" earned him his second consecutive Grammy for Best Jazz Male Vocal.
"Promise Me You'll Remember", his contribution to the "Godfather III" soundtrack, was nominated for both an Academy Award and a Golden Globe in 1991. In a year of recognition, he was also nominated for an Emmy Award for Best Performance in a Variety Special for his PBS special "Swingin' Out Live", which was also released as a video. In October 1991 he released his third consecutive multi-platinum album, "Blue Light, Red Light", on which he wrote and arranged the songs. Also in October 1991 he starred in "Little Man Tate", directed by Jodie Foster, playing the friend of a child prodigy who goes to college.
In November 1992, Connick released "25", a solo piano collection of standards that again went platinum. He also re-released the album "Eleven". Connick contributed "A Wink and a Smile" to the "Sleepless in Seattle" soundtrack, released in 1993. His multi-platinum album of holiday songs, "When My Heart Finds Christmas", was the best-selling Christmas album in 1993.
Mid–1990s: funk.
In 1994, Connick decided to branch out. He released "She", an album of New Orleans funk that also went platinum. In addition, he released a song called "(I Could Only) Whisper Your Name" for the soundtrack of "The Mask", starring Jim Carrey, which is his most successful single in the United States to date.
Connick took his funk music on a tour of the United Kingdom in 1994, an effort that did not please some of his fans, who were expecting a jazz crooner. Connick also took his funk music to the People's Republic of China in 1995, playing at the "Shanghai Center Theatre". The performance was televised live in China for what became known as the Shanghai Gumbo special. In his third film "Copycat", Connick played a serial killer. Released in 1995, "Copycat" also starred Holly Hunter and Sigourney Weaver. The following year, he released his second funk album, "Star Turtle", which did not sell as well as previous albums, although it did reach No. 38 on the charts. However, he appeared in the most successful movie of 1996, "Independence Day", with Will Smith and Jeff Goldblum.
Late 1990s: jazz and "Hope Floats".
For his 1997 release "To See You", Connick recorded original love songs, touring the United States and Europe with a full symphony orchestra backing him and his piano in each city. As part of his tour, he played at the Nobel Peace Prize Concert in Oslo, Norway, with his final concert of that tour in Paris being recorded for a Valentine's Day special on PBS in 1998. He also continued his film career, starring in "Excess Baggage" opposite Alicia Silverstone and Benicio del Toro in 1997.
In May 1998, he had his first leading role in director Forest Whitaker's "Hope Floats", with Sandra Bullock as his female lead. He released "Come By Me", his first album of big band music in eight years in 1999, and embarked on a world tour visiting the United States, Europe, Japan and Australia. In addition, he provided the voice of Dean McCoppin in the animated film "The Iron Giant".
2000–02: Broadway debut, musicals, "Will & Grace".
Connick wrote the score for Susan Stroman's Broadway musical "Thou Shalt Not", based on Émile Zola's novel "Thérèse Raquin", in 2000; it premiered in 2001. His music and lyrics earned a Tony Award nomination. He was also the narrator of the film "My Dog Skip", released in that year.
In March 2001, Connick starred in a television production of "South Pacific" with Glenn Close, televised on the ABC network. He also starred in his twelfth movie, "Mickey", featuring a screenplay by John Grisham that same year. In October 2001, he again released two albums: "Songs I Heard", featuring big band re-workings of children's show themes, and "30", featuring Connick on piano with guest appearances by several other musical artists. "Songs I Heard" won Connick another Grammy for Best Traditional Pop Album and he toured performing songs from the album, holding matinees at which each parent had to be accompanied by a child.
In 2002, he received a U.S. Patent for a "system and method for coordinating music display among players in an orchestra." Connick appeared as Grace Adler's boyfriend (and later husband) Leo Markus on the NBC sitcom "Will & Grace" from 2002 to 2006.
2003–05: "Connick on Piano" and "Only You".
In July 2003, Connick released his first instrumental album in fifteen years, "Other Hours Connick on Piano Volume 1". It was released on Branford Marsalis' new label Marsalis Music and led to a short tour of nightclubs and small theaters. Connick appeared in the film "Basic". In October 2003, he released his second Christmas album, "Harry for the Holidays", which went gold and reached No. 12 on the "Billboard" 200 albums chart. He also had a television special on NBC featuring Whoopi Goldberg, Nathan Lane, Marc Anthony and Kim Burrell. "Only You", his seventeenth album for Columbia Records, was released in February 2004. A collection of 1950s and 1960s ballads, "Only You", went top ten on both sides of the Atlantic and was certified gold in the United States in March 2004. The "Only You" tour with big band went on in America, Australia and a short trip to Asia. "Harry for the Holidays" was certified platinum in November 2004. A music DVD "Harry Connick Jr. — "Only You" in Concert" was released in March 2004, after it had first aired as a "Great Performances" special on PBS. The special won him an Emmy for Outstanding Music Direction. The DVD received a Gold & Platinum Music Video — Long Form awards from the RIAA in November 2005.
An animated holiday special, "The Happy Elf", aired on NBC in December 2005, with Connick as the composer, the narrator, and one of the executive producers. Shortly after, it was released on DVD. The holiday special was based on his original song "The Happy Elf", from his 2003 album "Harry for the Holidays". Another album from Marsalis Music was recorded in 2005, "", a duo album with Harry Connick, Jr. on piano together with Branford Marsalis on saxophone. A music DVD, "A Duo Occasion", was filmed at the Ottawa International Jazz Festival 2005 in Canada, and released in November 2005.
He appeared in another episode of NBC sitcom "Will & Grace" in November 2005, and appeared in an additional three episodes in 2006.
2006–08: "The Pajama Game", "Bug" and "P.S. I Love You".
"Bug", a film directed by William Friedkin, is a psychological thriller filmed in 2005, starring Connick, Ashley Judd, and Michael Shannon. The film was released in 2007. He starred in the Broadway revival of "The Pajama Game", produced by the Roundabout Theater Company, along with Michael McKean and Kelli O'Hara, at the "American Airlines Theatre" in 2006. It ran from February 23 to June 17, 2006, including five benefit performances running from June 13 to June 17. The "Pajama Game" cast recording was nominated for a Grammy, after being released as part of Connick's double disc album Harry on Broadway, Act I.
He hosted The Weather Channel's mini series "100 Biggest Weather Moments" which aired in 2007. He was part of the documentary , released in November 2007. He sat in on piano on Bob French's 2007 album "Marsalis Music Honors Series: Bob French". He appeared in the film "P.S. I Love You", released in December 2007. A third album in the "Connick on Piano" series, "Chanson du Vieux Carré" was released in 2007, and Connick received two Grammy nominations for the track "Ash Wednesday", for the Grammy awards in 2008. "Chanson du Vieux Carré" was released simultaneously with the album "Oh, My NOLA". Connick toured North America and Europe in 2007, and toured Asia and Australia in 2008, as part of his My New Orleans Tour. Connick did the arrangements for, wrote a couple of songs, and sang a duet on Kelli O'Hara's album that was released in May 2008. He was also the featured singer at the Concert of Hope immediately preceding Pope Benedict XVI's Mass at Yankee Stadium in April 2008. He had the starring role of Dr. Dennis Slamon in the 2008 Lifetime TV film "Living Proof". His third Christmas album, "What a Night!", was released in November 2008.
2009–11: "New in Town", "Your Songs".
The film "New in Town" starring Connick and Renée Zellweger, began filming in January 2008, and was released in January 2009. Connick's album "Your Songs" was released on CD, September 22, 2009. In contrast to Connick's previous albums, this album is a collaboration with a record company producer, the multiple Grammy Award winning music executive Clive Davis.
Connick starred in the Broadway revival of "On a Clear Day You Can See Forever", which opened at the St. James Theatre in November 2011 in previews.
"American Idol" (Season 9).
Connick appeared on the May 4, 2010 episode of "American Idol" season 9, where he acted as a mentor for the top 5 finalists. He appeared again the next night on May 5 to perform "And I Love Her".
2012–present: "Law & Order: Special Victims Unit", "Every Man Should Know".
On January 6, 2012, NBC president Robert Greenblatt announced at the Television Critics Association winter press tour that Harry Connick Junior had been cast in a four-episode arc of NBC's long-running legal drama, "" as new Executive ADA, David Haden, a dedicated, straight-shooting prosecutor who is assigned a case with Detective Benson (Mariska Hargitay).
"Every Man Should Know".
On June 11, 2013, Connick released a new album of all original music titled "Every Man Should Know". Connick debuted the title track live on the May 2, 2013 episode of "American Idol" and appeared on "The Ellen DeGeneres Show" the following week to discuss his new project. A 2013 US summer tour was announced in support of the album.
"American Idol" (Season 12).
Connick returned to "American Idol" to mentor the top four of season 12. He performed "Every Man Should Know" on the results show the following night.
American Idol (Season 13).
On September 3, 2013, the officials of "American Idol" officially announced that Connick would be a part of the judging panel for season 13 alongside former judge Jennifer Lopez and returning judge Keith Urban.
Touring Big Band members.
The following musicians have toured as the Harry Connick, Jr., Big Band since its inception in 1990:
Connick and New Orleans, Hurricane Katrina.
Connick, a New Orleans native, is a founder of the Krewe of Orpheus, a music-based New Orleans krewe, taking its name from Orpheus of classical mythology. The Krewe of Orpheus parades on St. Charles Avenue and Canal Street in New Orleans on Lundi Gras (Fat Monday) — the day before Mardi Gras (Fat Tuesday).
On September 2, 2005, Harry Connick, Jr., helped to organize, and appeared in, the NBC-sponsored live telethon concert, "A Concert for Hurricane Relief", for relief in the wake of Hurricane Katrina. He spent several days touring the city to draw attention to the plight of citizens stranded at the Ernest N. Morial Convention Center and other places. At the concert he paired with host Matt Lauer, and entertainers including Tim McGraw, Faith Hill, Kanye West, Mike Myers, and John Goodman.
On September 6, 2005, Connick was made honorary chair of Habitat for Humanity's Operation Home Delivery, a long-term rebuilding plan for families victimized by Hurricane Katrina in New Orleans and along the Gulf Coast.
Connick's album "Oh, My NOLA", and "" were released in 2007, with a following tour called the My New Orleans Tour.
Musicians' Village.
Connick and Branford Marsalis devised an initiative to help restore New Orleans' musical heritage. Habitat for Humanity and New Orleans Area Habitat for Humanity, working with Connick and Branford Marsalis announced December 6, 2005, plans for a Musicians' Village in New Orleans. The Musicians' Village includes Habitat-constructed homes, with an "Ellis Marsalis Center for Music", as the area's centerpiece. The Habitat-built homes provide musicians, and anyone else who qualifies, the opportunity to buy decent, affordable housing.
In 2012, Connick and Marsalis received the S. Roger Horchow Award for Greatest Public Service by a Private Citizen, an award given out annually by Jefferson Awards.
Personal life.
On April 16, 1994, Connick Jr. married former "Victoria's Secret" model Jill Goodacre, originally from Texas, at the St. Louis Cathedral, New Orleans. Jill is the daughter of sculptor Glenna Goodacre, originally from Lubbock and now Santa Fe, New Mexico. The song "Jill", on the album "Blue Light, Red Light" (1991) is about her. They have three daughters: Georgia Tatum (born April 17, 1996), Sarah Kate (September 12, 1997), and Charlotte (born June 26, 2002). The family currently resides in New Canaan, Connecticut and New York City. Connick is a practicing Roman Catholic.
Connick Jr. is a supporter of hometown NFL franchise New Orleans Saints. He was caught on camera at the Super Bowl XLIV, which the Saints won, in Miami by the filming crew of "The Ellen DeGeneres Show" during the post-game celebrations. Ellen's mother Betty was on the sidelines watching the festivities when she spotted Connick Jr. in the stands sporting a Drew Brees jersey.
Arrest.
In December 1992, he was charged with bringing a gun to the security checkpoint in an airport. Connick was arrested by the Port Authority Police in 1992 and charged with having a 9mm pistol in his possession at JFK International Airport. After spending a day in jail, he agreed to make a public-service television commercial warning against breaking gun laws. The court agreed to drop all charges if Connick stayed out of trouble for six months.

</doc>
<doc id="13744" url="http://en.wikipedia.org/wiki?curid=13744" title="List of humorists">
List of humorists

 
A humorist or humourist (see spelling differences) is a person who writes or performs humorous material.
A humorist is usually distinct from a stand-up comedian. For people who are primarily stand-ups, see list of stand-up comedians.
Notable humorists include:

</doc>
<doc id="13746" url="http://en.wikipedia.org/wiki?curid=13746" title="Hydrostatic shock">
Hydrostatic shock

Hydrostatic shock or hydraulic shock is a term which describes the observation that a penetrating projectile can produce remote wounding and incapacitating effects in living targets through a hydraulic effect in their liquid-filled tissues, in addition to local effects in tissue caused by direct impact. Just as force applied by a pump in a hydraulic circuit is transmitted to elsewhere in the circuit because of the near incompressibility of the liquid, so the kinetic energy of a bullet can sometimes send a shock wave through the body, transferring shock (in the mechanical sense) to tissues whose physiologic function may be disrupted by it (especially in the circulatory or nervous systems). (Other kinds of shock, namely circulatory and psychological, may follow, but mechanical shock is the immediate disruptor.) There is scientific evidence that hydrostatic shock can produce remote neural damage and produce incapacitation more quickly than blood loss effects. In arguments about the differences in stopping power between calibers and between cartridge models, proponents of cartridges that are "light and fast" (such as the 9x19mm Parabellum) versus cartridges that are "slow and heavy" (such as the .45 ACP) often refer to this phenomenon.
Human autopsy results have demonstrated brain hemorrhaging from fatal hits to the chest, including cases with handgun bullets. Thirty-three cases of fatal penetrating chest wounds by a single bullet were selected from a much larger set by excluding all other traumatic factors, including past history.
 In such meticulously selected cases brain tissue was examined histologically; samples were taken from brain hemispheres, basal ganglia, the pons, the oblongate and from the cerebellum. Cufflike pattern haemorrhages around small brain vessels were found in all specimens. These haemorrhages are caused by sudden changes of the intravascular blood pressure as a result of a compression of intrathoracic great vessels by a shock wave caused by a penetrating bullet.
 — J. Krajsa
It has often been asserted that hydrostatic shock and other descriptions of remote wounding effects are nothing but myths. Correspondence in the journal, "Neurosurgery", reviews the published evidence and concludes that the phenomenon is well-established.
 A myth is an assertion which has either been disproven by careful experiment or for which there is no historical or scientific evidence in cases where it is reasonably expected. Belief in remote effects of penetrating projectiles may have originated with hunters and soldiers, but their reality is now well established in a broad body of scientific literature...
 — Neurosurgery
Origin of the theory.
In the scientific literature, the first discussion of pressure waves created when a bullet hits a living target is presented by E. Harvey Newton and his research group at Princeton University in 1947:
 It is not generally recognized that when a high velocity missile strikes the body and moves through soft tissues, pressures develop which are measured in thousands of atmospheres. Actually, three different types of pressure change appear: (1) shock wave pressures or sharp, high pressure pulses, formed when the missile hits the body surface; (2) very high pressure regions immediately in front and to each side of the moving missile; (3) relatively slow, low pressure changes connected with the behavior of the large explosive temporary cavity, formed behind the missile. Such pressure changes appear to be responsible for what is known to hunters as hydraulic shock—a hydraulic transmission of energy which is believed to cause instant death of animals hit by high velocity bullets (Powell (1))."
 — An Experimental Study of shock waves resulting from the impact of high velocity missiles on animal tissues
Frank Chamberlin, a World War II trauma surgeon and ballistics researcher, noted remote pressure wave effects. Col. Chamberlin described what he called “explosive effects” and “hydraulic reaction” of bullets in tissue. "...liquids are put in motion by ‘shock waves’ or hydraulic effects... with liquid filled tissues, the effects and destruction of tissues extend in all directions far beyond the wound axis". He avoided the ambiguous use of the term “shock” because it can refer to either a specific kind of pressure wave associated with explosions and supersonic projectiles or to a medical condition in the body.
Col. Chamberlin recognized that many theories have been advanced in wound ballistics. During World War II he commanded an 8,500-bed hospital center that treated over 67,000 patients during the fourteen months that he operated it. P.O. Ackley estimates that 85% of the patients were suffering from gunshot wounds. Col. Chamberlin spent many hours interviewing patients as to their reactions to bullet wounds. He conducted many live animal experiments after his tour of duty. On the subject of wound ballistics theories, he wrote:
 If I had to pick one of these theories as gospel, I’d still go along with the Hydraulic Reaction of the Body Fluids plus the reactions on the Central Nervous System.
 — Col. Frank Chamberlin, M.D.
Other World War II era scientists noted remote pressure wave effects in the peripheral nerves. There was support for the idea of remote neural effects of ballistic pressure waves in the medical and scientific communities, but the phrase "’hydrostatic shock’" and similar phrases including “shock” were used mainly by gunwriters (such as Jack O'Conner) and the small arms industry (such as Roy Weatherby, and Federal “Hydrashock.”)
Fackler's contra-claim.
Dr. Martin Fackler, a Vietnam-era trauma surgeon, wound ballistics researcher, a Colonel in the U.S. Army and the head of the Wound Ballistics Laboratory for the U.S. Army’s Medical Training Center, Letterman Institute, claimed that hydrostatic shock had been disproved and that the assertion that a pressure wave plays a role in injury or incapacitation is a myth. Others expressed similar views.
Dr. Fackler based his argument on the lithotriptor, a tool commonly used to break up kidney stones. The lithotriptor uses sonic pressure waves which are stronger than those caused by most handgun bullets, yet it produces no damage to soft tissues whatsoever. Hence, Fackler argued, ballistic pressure waves cannot damage tissue either.
Dr. Fackler claimed that a study of rifle bullet wounds in Vietnam (Wound Data and Munitions Effectiveness Team) found “no cases of bones being broken, or major vessels torn, that were not hit by the penetrating bullet. In only two cases, an organ that was not hit (but was within a few cm of the projectile path), suffered some disruption.” Dr. Fackler cited a personal communication with R. F. Bellamy. However, Bellamy’s published findings the following year estimated that 10% of fractures in the data set might be due to indirect injuries, and one specific case is described in detail (pp. 153–154). In addition, the published analysis documents five instances of abdominal wounding in cases where the bullet did not penetrate the abdominal cavity (pp. 149–152), a case of lung contusion resulting from a hit to the shoulder (pp. 146–149), and a case of indirect effects on the central nervous system (p. 155). Fackler's critics argue that Fackler's evidence does not contradict distant injuries, as Fackler claimed, but the WDMET data from Vietnam actually provides supporting evidence for it.
A summary of the debate was published in 2009 as part of a "Historical Overview of Wound Ballistics Research."
 Fackler [10, 13] however, disputed the shock wave theory, claiming there is no physical evidence to support it, although some support for this theory had already been provided by Harvey [20, 21], Kolsky [31], Suneson et. al. [42, 43], and Crucq [5]. Since that time, other authors suggest there is increasing evidence to support the theory that shock waves from high velocity bullets can cause tissue related damage and damage to the nervous system. This has been shown in various experiments using simulant models [24, 48]. One of the most interesting is a study by Courtney and Courtney [4] who showed a link between traumatic brain injury and pressure waves originating in the thoracic cavity and extremities.
 — Historical Overview of Wound Ballistics Research
Distant injuries in the WDMET data.
The Wound Data and Munitions Effectiveness Team (WDMET) gathered data on wounds sustained during the Vietnam War. In their analysis of this data published in the Textbook of Military Medicine, Ronald Bellamy and Russ Zajtchuck point out a number of cases which seem to be examples of distant injuries. Bellamy and Zajtchuck describe three mechanisms of distant wounding due to pressure transients: 1) stress waves 2) shear waves and 3) a vascular pressure impulse.
After citing Harvey's conclusion that “stress waves probably do not cause any tissue damage” (p. 136), Bellamy and Zajtchuck express their view that Harvey's interpretation might not be definitive because they write “the possibility that stress waves from a penetrating projectile might also cause tissue damage cannot be ruled out.” (p. 136) The WDMET data includes a case of a lung contusion resulting from a hit to the shoulder. The caption to Figure 4-40 (p. 149) says, “The pulmonary injury may be the result of a stress wave.” They describe the possibility that a hit to a soldier's trapezius muscle caused temporary paralysis due to “the stress wave passing through the soldier's neck indirectly [causing] cervical cord dysfunction.” (p. 155)
In addition to stress waves, Bellamy and Zajtchuck describe shear waves as a possible mechanism of indirect injuries in the WDMET data. They estimate that 10% of bone fractures in the data may be the result of indirect injuries, that is, bones fractured by the bullet passing close to the bone without a direct impact. A Chinese experiment is cited which provides a formula estimating how pressure magnitude decreases with distance. Together with the difference between strength of human bones and strength of the animal bones in the Chinese experiment, Bellamy and Zajtchuck use this formula to estimate that assault rifle rounds “passing within a centimeter of a long bone might very well be capable of causing an indirect fracture.” (p. 153) Bellamy and Zajtchuck suggest the fracture in Figures 4-46 and 4-47 is likely an indirect fracture of this type. Damage due to shear waves extends to even greater distances in abdominal injuries in the WDMET data. Bellamy and Zajtchuck write, “The abdomen is one body region in which damage from indirect effects may be common.” (p. 150) Injuries to the liver and bowel shown in Figures 4-42 and 4-43 are described, “The damage shown in these examples extends far beyond the tissue that is likely to direct contact with the projectile.” (p. 150)
In addition to providing examples from the WDMET data for indirect injury due to propagating shear and stress waves, Bellamy and Zajtchuck expresses an openness to the idea of pressure transients propagating via blood vessels can cause indirect injuries. “For example, pressure transients arising from an abdominal gunshot wound might propagate through the vena cavae and jugular venous system into the cranial cavity and cause a precipitous rise in intracranial pressure there, with attendant transient neurological dysfunction.” (p. 154) However, no examples of this injury mechanism are presented from the WDMET data. However, the authors suggest the need for additional studies writing, “Clinical and experimental data need to be gathered before such indirect injuries can be confirmed.” Distant injuries of this nature were later confirmed in the experimental data of Swedish and Chinese researchers, in the clinical findings of Krajsa and in autopsy findings from Iraq.
Autopsy Findings in Iraq.
An 8 month study in Iraq performed in 2010 and published in 2011 reports on autopsies of 30 gunshot victims struck with high-velocity (greater than 2500 fps) rifle bullets. In all 30 cases, autopsies revealed injuries distant from the main wound channel due to hydrostatic shock. The authors determined that the lungs and chest are the most susceptible to distant wounding, followed by the abdomen. The authors conclude:
 Distant injuries away from the main track in high velocity missile injuries are very important and almost always present in all cases especially in the chest and abdomen and this should be put in the consideration on the part of the forensic pathologist
and probably the general surgeon.
 — R. S. Selman et al.
Inferences from blast pressure wave observations.
A shock wave can be created when fluid is rapidly displaced by an explosive or projectile. Tissue behaves similarly enough to water that a sonic pressure wave can be created by a bullet impact, generating pressures in excess of 1500 psi.
Duncan McPherson, a former member of the International Wound Ballistics Association and author of the book, Bullet Penetration, claimed that shock waves cannot result from bullet impacts with tissue. In contrast, Brad Sturtevant, a leading researcher in shock wave physics at Caltech for many decades, found that shock waves can result from handgun bullet impacts in tissue. Other sources indicate that ballistic impacts can create shock waves in tissue.
Blast and ballistic pressure waves have physical similarities. Prior to wave reflection, they both are characterized by a steep wave front followed by a nearly exponential decay at close distances. They have similarities in how they cause neural effects in the brain. In tissue, both types of pressure waves have similar magnitudes, duration, and frequency characteristics. Both have been shown to cause damage in the hippocampus. It has been hypothesized that both reach the brain from the thoracic cavity via major blood vessels.
For example, Ibolja Cernak, a leading researcher in blast wave injury at the Applied Physics Laboratory at Johns Hopkins University, hypothesized, "alterations in brain function following blast exposure are induced by kinetic energy transfer of blast overpressure via great blood vessels in abdomen and thorax to the central nervous system." This hypothesis is supported by observations of neural effects in the brain from localized blast exposure focused on the lungs in experiments in animals.
“Hydrostatic shock” expresses the idea that organs can be damaged by the pressure wave in addition to damage from direct contact with the penetrating projectile. If one interprets the "shock" in the term "hydrostatic shock" to refer to the physiological effects rather than the physical wave characteristics, the question of whether the pressure waves satisfy the definition of “shock wave” is unimportant, and one can consider the weight of scientific evidence and various claims regarding the possibility of a ballistic pressure wave to create tissue damage and incapacitation in living targets.
Physics of ballistic pressure waves.
A number of papers describe the physics of ballistic pressure waves created when a high-speed projectile enters a viscous medium. These results show that ballistic impacts produce pressure waves that propagate at close to the speed of sound.
Lee et al. present an analytical model showing that unreflected ballistic pressure waves are well approximated by an exponential decay, which is similar to blast pressure waves. Lee et al. note the importance of the energy transfer:
 As would be expected, an accurate estimation of the kinetic energy loss by a projectile is always important in determining the ballistic waves.
 — Lee, Longoria, and Wilson
The rigorous calculations of Lee et al. require knowing the drag coefficient and frontal area of the penetrating projectile at every instant of the penetration. Since this is not generally possible with expanding handgun bullets, Courtney and Courtney developed a model for estimating the peak pressure waves of handgun bullets from the impact energy and penetration depth in ballistic gelatin. This model agrees with the more rigorous approach of Lee et al. for projectiles where they can both be applied. For expanding handgun bullets, the peak pressure wave magnitude is proportional to the bullet’s kinetic energy divided by the penetration depth.
Remote cerebral effects of ballistic pressure waves.
Goransson et al. were the first contemporary researchers to present compelling evidence for remote cerebral effects of extremity bullet impact. They observed changes in EEG readings from pigs shot in the thigh. A follow-up experiment by Suneson et al. implanted high-speed pressure transducers into the brain of pigs and demonstrated that a significant pressure wave reaches the brain of pigs shot in the thigh. These scientists observed apnea, depressed EEG readings, and neural damage in the brain caused by the distant effects of the ballistic pressure wave originating in the thigh.
The results of Suneson et al. were confirmed and expanded upon by a later experiment in dogs
which "confirmed that distant effect exists in the central nervous system after a high-energy missile impact to an extremity. A high-frequency oscillating pressure wave with large amplitude and short duration was found in the brain after the extremity impact of a high-energy missile . . ." Wang et al. observed significant damage in both the hypothalamus and hippocampus regions of the brain due to remote effects of the ballistic pressure wave.
Remote pressure wave effects in the spine and internal organs.
In a study of a handgun injury, Sturtevant found that pressure waves from a bullet impact in the torso can reach the spine and that a focusing effect from concave surfaces can concentrate the pressure wave on the spinal cord producing significant injury. This is consistent with other work showing remote spinal cord injuries from ballistic impacts.
Roberts et al. present both experimental work and finite element modeling showing that there can be considerable pressure wave magnitudes in the thoracic cavity for handgun projectiles stopped by a Kevlar vest. For example, an 8 gram projectile at 360 m/s impacting a NIJ level II vest over the sternum can produce an estimated pressure wave level of nearly 2.0 MPa (280 psi) in the heart and a pressure wave level of nearly 1.5 MPa (210 psi) in the lungs. Impacting over the liver can produce an estimated pressure wave level of 2.0 MPa (280 psi) in the liver.
Energy transfer required for remote neural effects.
The work of Courtney et al. supports the role of a ballistic pressure wave in incapacitation and injury. The work of Suneson et al. and Courtney et al. suggest that remote neural effects can occur with levels of energy transfer possible with handguns, about 500 ft.lbf. Using sensitive biochemical techniques, the work of Wang et al. suggests even lower impact energy thresholds for remote neural injury to the brain. In analysis of experiments of dogs shot in the thigh they report highly significant (p < 0.01), easily detectable neural effects in the hypothalamus and hippocampus with energy transfer levels close to 550 ft.lbf. Wang et al. reports less significant (p < 0.05) remote effects in the hypothalamus with energy transfer just under 100 ft.lbf.
Even though Wang et al. document remote neural damage for low levels of energy transfer, roughly 100 ft.lbf, these levels of neural damage are probably too small to contribute to rapid incapacitation. Courtney and Courtney believe that remote neural effects only begin to make significant contributions to rapid incapacitation for ballistic pressure wave levels above 500 psi (corresponds to transferring roughly 300 ft.lbf in 12 in of penetration) and become easily observable above 1000 psi (corresponds to transferring roughly 600 ft.lbf in 12 in of penetration). Incapacitating effects in this range of energy transfer are consistent with observations of remote spinal injuries, observations of suppressed EEGs and apnea in pigs and with observations of incapacitating effects of ballistic pressure waves without a wound channel.
Other scientific findings.
The scientific literature contains significant other findings regarding injury mechanisms of ballistic pressure waves. Ming et al. found that ballistic pressure waves can break bones. Tikka et al. reports abdominal pressure changes produced in pigs hit in one thigh. Akimov et al. report on injuries to the nerve trunk from gunshot wounds to the extremities.
Recommendations.
The FBI recommends that loads intended for self-defense and law enforcement applications meet a minimum penetration requirement of 12 in in ballistic gelatin and explicitly advises against selecting rounds based on hydrostatic shock effects.
Hydrostatic shock as a factor in selection of ammunition.
Ammunition selection for self-defense, military, and law enforcement.
In self-defense, military, and law enforcement communities, opinions vary regarding the importance of remote wounding effects in ammunition design and selection. In his book on hostage rescuers, Leroy Thompson discusses the importance of hydrostatic shock in choosing a specific design of .357 Magnum and 9x19mm Parabellum bullets. In "Armed and Female", Paxton Quigley explains that hydrostatic shock is the real source of “stopping power.” Jim Carmichael, who served as shooting editor for Outdoor Life magazine for 25 years, believes that hydrostatic shock is important to “a more immediate disabling effect” and is a key difference in the performance of .38 Special and .357 Magnum hollow point bullets. In “The search for an effective police handgun,” Allen Bristow describes that police departments recognize the importance of hydrostatic shock when choosing ammunition. A research group at West Point suggests handgun loads with at least 500 ft.lbf of energy and 12 in of penetration and recommends:
 One should not be overly impressed by the propensity for shallow penetrating loads to produce larger pressure waves. Selection criteria should first determine the required penetration depth for the given risk assessment and application, and only use pressure wave magnitude as a selection criterion for loads meeting minimum penetration requirements. Reliable expansion, penetration, feeding, and functioning are all important aspects of load testing and selection. We do not advocate abandoning long-held aspects of the load testing and selection process, but it seems prudent to consider the pressure wave magnitude along with other factors.
 — Courtney and Courtney
A number of law enforcement and military agencies have adopted the 5.7x28mm cartridge. These agencies include the Navy SEALs and the Federal Protective Service branch of the ICE. In contrast, some defense contractors, law enforcement analysts, and military analysts say that hydrostatic shock is an unimportant factor when selecting cartridges for a particular use because any incapacitating effect it may have on a target is difficult to measure and inconsistent from one individual to the next. This is in contrast to factors such as proper shot placement and massive blood loss which are almost always eventually incapacitating for nearly every individual.
Ammunition selection for hunting.
Hydrostatic shock is commonly considered as a factor in the selection of hunting ammunition. Peter Capstick explains that hydrostatic shock may have value for animals up to the size of white-tailed deer, but the ratio of energy transfer to animal weight is an important consideration for larger animals. If the animal’s weight exceeds the bullet’s energy transfer, penetration in an undeviating line to a vital organ is a much more important consideration than energy transfer and hydrostatic shock. Jim Carmichael, in contrast, describes evidence that hydrostatic shock can affect animals as large as Cape Buffalo in the results of a carefully controlled study carried out by veterinarians in a buffalo culling operation.
 Whereas virtually all of our opinions about knockdown power are based on isolated examples, the data gathered during the culling operation was taken from a number of animals. Even more important, the animals were then examined and dissected in a scientific manner by professionals.
Predictably, some of the buffalo dropped where they were shot and some didn't, even though all received near-identical hits in the vital heart-lung area. When the brains of all the buffalo were removed, the researchers discovered that those that had been knocked down instantly had suffered massive rupturing of blood vessels in the brain. The brains of animals that hadn't fallen instantly showed no such damage.
 — Jim Carmichael
Dr. Randall Gilbert describes hydrostatic shock as an important factor in bullet performance on whitetail deer, “When it [a bullet] enters a whitetail’s body, huge accompanying shock waves send vast amounts of energy through nearby organs, sending them into arrest or shut down.” Dave Ehrig expresses the view that hydrostatic shock depends on impact velocities above 1100 ft per second. Sid Evans explains the performance of the Nosler Partition bullet and Federal Cartridge Company’s decision to load this bullet in terms of the large tissue cavitation and hydrostatic shock produced from the frontal diameter of the expanded bullet. The North American Hunting Club suggests big game cartridges that create enough hydrostatic shock to quickly bring animals down.
External links.
Terminal Ballistics Research http://www.ballisticstudies.com/Knowledgebase.html

</doc>
<doc id="13749" url="http://en.wikipedia.org/wiki?curid=13749" title="Hadith">
Hadith

Hadith ( or ;) are the collections of the reports of the teachings, deeds and sayings of the Islamic prophet Muhammad. The term comes from the Arabic: حديث‎, plural:أحاديث, meaning "report" "account" or "narrative".
The hadith literature is based on spoken reports that were in circulation in society after the death of Muhammad. Islamic scholars then compiled these hadith together in collections.
Different branches of Islam refer to different collections of hadith, though the same incident may be found in hadith in different collections:
Some minor heterodox groups, collectively known as Quranists, reject the authority of all Hadith. These groups, which are outside the main denominations, are present in Pakistan, Nigeria and the United States, and are essentially a twentieth century development.
The "hadith" also had a profound and controversial influence on moulding the commentaries ("tafsir") on the Quran. The earliest commentary of the Quran by Muhammad ibn Jarir al-Tabari is mostly sourced from the hadith. The "hadith" was used in forming the basis of 'Shariah' law. Much of early Islamic history available today is also based on the hadith and is challenged for lack of basis in primary source material and contradictions based on secondary material available.
Each hadith is based on two parts, a chain of narrators reporting the hadith ("isnad"), and the text itself ("matn"). Hadiths are still regarded by traditional Islamic schools of jurisprudence as important tools for understanding the Quran and in matters of jurisprudence. Hadith were evaluated and gathered into large collections during the 8th and 9th centuries. These works are referred to in matters of Islamic law and history to this day.
Muslim clerics and jurists classify individual hadith as "sahih" ("authentic"), "hasan" ("good") or "da'if" ("weak"). However there is no overall agreement: different groups and different individual scholars may classify a hadith differently.
Etymology.
In Arabic, the word "ḥadīth" (Arabic: حديث‎ "ḥadīth"  ]) means a "report, account, narrative". The Arabic plural is ʾaḥādīth (أحاديث) (]). "Hadith" also refers to the speech of a person. It is a noun.
Definition.
In Islamic terminology, the term "hadith" refers to reports of statements or actions of Muhammad, or of his tacit approval or criticism of something said or done in his presence. Classical hadith specialist Ibn Hajar al-Asqalani says that the intended meaning of "hadith" in religious tradition is something attributed to Muhammad but that is not found in the Quran. Other associated words possess similar meanings including: "khabar" (news, information) often refers to reports about Muhammad, but sometimes refers to traditions about his companions and their successors from the following generation; conversely, "athar" (trace, vestige) usually refers to traditions about the companions and successors, though sometimes connotes traditions about Muhammad. The word "sunnah" (custom) is also used in reference to a normative custom of Muhammad or the early Muslim community.
Components.
The two major aspects of a hadith are the text of the report (the "matn"), which contains the actual narrative, and the chain of narrators (the "isnad"), which documents the route by which the report has been transmitted. The "sanad", literally 'support', is so named due to the reliance of the hadith specialists upon it in determining the authenticity or weakness of a hadith. The "isnad" consists of a chronological list of the narrators, each mentioning the one from whom they heard the hadith, until mentioning the originator of the "matn" along with the "matn" itself.
The first people to hear hadith were the companions who preserved it and then conveyed it to those after them. Then the generation following them received it, thus conveying it to those after them and so on. So a companion would say, "I heard the Prophet say such and such." The Follower would then say, "I heard a companion say, 'I heard the Prophet.'" The one after him would then say, "I heard someone say, 'I heard a Companion say, 'I heard the Prophet..."" and so on.
History, tradition and usage.
History.
Traditions of the life of Muhammad and the early history of Islam were passed down mostly orally for more than a hundred years after Muhammad's death in AD 632. Muslim historians say that Caliph Uthman ibn Affan (the third khalifa (caliph) of the Rashidun Empire, or third successor of Muhammad, who had formerly been Muhammad's secretary), is generally believed to urge Muslims to record the hadith just as Muhammad suggested to some of his followers to write down his words and actions.
Uthman's labours were cut short by his assassination, at the hands of aggrieved soldiers, in 656. No sources survive directly from this period so we are dependent on what later writers tell us about this period.
By the 9th century the number of hadiths had grown exponentially. Islamic scholars of the Abbasid period were faced with a huge corpus of miscellaneous traditions, some of them flatly contradicting each other. Many of these traditions supported differing views on a variety of controversial matters. Scholars had to decide which hadith were to be trusted as authentic and which had been invented for political or theological purposes. To do this, they used a number of techniques which Muslims now call the science of hadith.
Shia and Sunni textual traditions.
Sunni and Shia hadith collections differ because scholars from the two traditions differ as to the reliability of the narrators and transmitters. Narrators who took the side of Abu Bakr and Umar rather than Ali, in the disputes over leadership that followed the death of Muhammad, are seen as unreliable by the Shia; narrations sourced to Ali and the family of Muhammad, and to their supporters, are preferred. Sunni scholars put trust in narrators, such as Aisha, whom Shia reject. Differences in hadith collections have contributed to differences in worship practices and shari'a law and have hardened the dividing line between the two traditions.
Extent and nature in the Sunni tradition.
In the Sunni tradition, the number of such texts is ten thousand plus or minus a few thousand. But if, say, ten companions record a text reporting a single incident in the life of the prophet, hadith scholars can count this as ten hadiths. So Musnad Ahmad, for example, has over 30,000 hadiths—but this count includes texts that are repeated in order to record slight variations within the text or within the chains of narrations. Identifying the narrators of the various texts, comparing their narrations of the same texts to identify both the soundest reporting of a text and the reporters who are most sound in their reporting occupied experts of hadith throughout the 2nd century. In the 3rd century of Islam (from 225/840 to about 275/889), hadith experts composed brief works recording a selection of about two- to five-thousand such texts which they felt to have been most soundly documented or most widely referred to in the Muslim scholarly community. The 4th and 5th century saw these six works being commented on quite widely. This auxiliary literature has contributed to making their study the place of departure for any serious study of hadith. In addition, Bukhari and Muslim in particular, claimed that they were collecting only the soundest of sound hadiths. These later scholars tested their claims and agreed to them, so that today, they are considered the most reliable collections of hadith. Toward the end of the 5th century, Ibn al-Qaisarani formally standardized the Sunni canon into six pivotal works, a delineation which remains to this day.
Over the centuries, several different categories of collections came into existence. Some are more general, like the "muṣannaf", the "muʿjam", and the "jāmiʿ", and some more specific, either characterized by the topics treated, like the "sunan" (restricted to legal-liturgical traditions), or by its composition, like the "arbaʿīniyyāt" (collections of forty hadiths).
Extent and nature in the Shia tradition.
Shi'a Muslims do not use the six major hadith collections followed by the Sunni, as they do not trust many of the Sunni narrators and transmitters. They have their own extensive hadith literature. The best-known hadith collections are The Four Books, which were compiled by three authors who are known as the 'Three Muhammads'. The Four Books are: "Kitab al-Kafi" by Muhammad ibn Ya'qub al-Kulayni al-Razi (329 AH), "Man la yahduruhu al-Faqih" by Muhammad ibn Babuya and "Al-Tahdhib" and "Al-Istibsar" both by Shaykh Muhammad Tusi. Shi'a clerics also make use of extensive collections and commentaries by later authors.
Unlike Sunnis, Shia do not consider any of their hadith collections to be sahih (authentic) in their entirety. Therefore, every individual hadith in a specific collection must be investigated separately to determine its authenticity.
Today usage.
The mainstream sects consider hadith to be essential supplements to, and clarifications of, the Quran, Islam's holy book, as well as for clarifying issues pertaining to Islamic jurisprudence. Ibn al-Salah, a hadith specialist, described the relationship between hadith and other aspect of the religion by saying: "It is the science most pervasive in respect to the other sciences in their various branches, in particular to jurisprudence being the most important of them." "The intended meaning of 'other sciences' here are those pertaining to religion," explains Ibn Hajar al-Asqalani, "Quranic exegesis, hadith, and jurisprudence. The science of hadith became the most pervasive due to the need displayed by each of these three sciences. The need hadith has of its science is apparent. As for Quranic exegesis, then the preferred manner of explaining the speech of God is by means of what has been accepted as a statement of Muhammad. The one looking to this is in need of distinguishing the acceptable from the unacceptable. Regarding jurisprudence, then the jurist is in need of citing as an evidence the acceptable to the exception of the later, something only possible utilizing the science of hadith."
Studies.
Hadith studies use a number of methods of evaluation developed by early Muslim scholars in determining the veracity of reports attributed to Muhammad. This is achieved by analyzing the text of the report, the scale of the report's transmission, the routes through which the report was transmitted, and the individual narrators involved in its transmission. On the basis of these criteria, various classifications were devised for hadith. The earliest comprehensive work in hadith studies was Abu Muhammad al-Ramahurmuzi's "al-Muhaddith al-Fasil", while another significant work was al-Hakim al-Naysaburi's "Ma‘rifat ‘ulum al-hadith". Ibn al-Salah's "ʻUlum al-hadith" is considered the standard classical reference on hadith studies.
Terminology: admissible and inadmissible hadiths.
By means of hadith terminology, hadith are categorized as "ṣaḥīḥ" (sound, authentic), "ḍaʿīf" (weak), or "mawḍūʿ" (fabricated). Other classifications used also include: "ḥasan" (good), which refers to an otherwise "ṣaḥīḥ" report suffering from minor deficiency, or a weak report strengthened due to numerous other corroborating reports; and "munkar" (denounced) which is a report that is rejected due to the presence of an unreliable transmitter contradicting another more reliable narrator. Both "sahīh" and "hasan" reports are considered acceptable for usage in Islamic legal discourse. Classifications of hadith may also be based upon the scale of transmission. Reports that pass through many reliable transmitters at each point in the "isnad" up until their collection and transcription are known as "mutawātir". These reports are considered the most authoritative as they pass through so many different routes that collusion between all of the transmitters becomes an impossibility. Reports not meeting this standard are known as "aahad", and are of several different types.
Some hadith are also called "Hadith Qudsi" (or Sacred Hadith), Like Ziyarat Ashura. It is a sub-category of hadith which some Muslims regard as the words of God (Arabic: Allah). According to as-Sayyid ash-Sharif al-Jurjani, the Hadith Qudsi differ from the Quran in that the former are "expressed in Muhammad's words", whereas the latter are the "direct words of God". However, note that a Hadith Qudsi is not necessarily "Sahih", it can also be considered as "Daif" (weak Hadith) and even "Mawdou".
An example of a Hadith Qudsi is the hadith of Abu Hurairah who said that Muhammad said:
When God decreed the Creation He pledged Himself by writing in His book which is laid down with Him: My mercy prevails over My wrath.
Biographical evaluation.
Another area of focus in the study of hadith is biographical analysis ("‘ilm al-rijāl", lit. "science of people"), in which details about the transmitter are scrutinized. This includes analyzing their date and place of birth; familial connections; teachers and students; religiosity; moral behaviour; literary output; their travels; as well as their date of death. Based upon these criteria, the reliability ("thiqāt") of the transmitter is assessed. Also determined is whether the individual was actually able to transmit the report, which is deduced from their contemporaneity and geographical proximity with the other transmitters in the chain. Examples of biographical dictionaries include: Abd al-Ghani al-Maqdisi's "Al-Kamal fi Asma' al-Rijal", Ibn Hajar al-Asqalani's "Tahdhīb al-Tahdhīb" and al-Dhahabi's "Tadhkirat al-huffaz".
Criticism and debates.
Islamic scholars through history.
Early criticism of the Hadith predates the time of Al-Shafii (d. 204 AH/820 CE) and is found in a text that Muslim tradition holds to be a letter from the Kharijite Abd Allah Ibn Ibad to the Caliph Abd al-Malik in 76/695. Though the authorship and dating of this letter are in some dispute, it still predates al-Shafii and its importance as a challenge to the authority of the Hadith remains undented. A key passage of this letter criticizes the Kufans for taking "Hadiths" for their religion abandoning the Quran. "They believed in a book which was not from God, written by the hands of men; they then attributed it to the Messenger of God." A group referred to as Ahl al-Kalam, who lived during the time of Al-Shafii and mentioned in his Kitab Jima al-Ilm rejected the Hadith on theological grounds. Their basic argument was that the Quran was an explanation of everything (16:89). They contended that obedience to the Prophet was contained in obeying only the Qur'an that God has sent down to him, and that when the Qur'an mentioned the Book together with Wisdom, the Wisdom was the specific rulings of the Book." Daniel Brown notes that the principal argument of Ahl al-Kalam was that the Hadith does not accurately reflect the Prophetic example, as the transmission of Hadith reports was not reliable. The Prophetic example, they argued, "has to be found elsewhere – first and foremost in following the Qur’an." And according to them, "the corpus of Hadith is filled with contradictory, blasphemous, and absurd traditions."
Mutazilites, who represented one of the earliest rationalist Muslim theological schools, and are the later Ahl al-Kalam, also viewed the transmission of the Prophetic sunnah as not sufficiently reliable. The Hadith, according to them, was mere "guesswork and conjecture" and "the Quran was complete and perfect, and did not require the Hadith or any other book to supplement or complement it."
Syed Ahmed Khan (1817–1898) is often considered the founder of the modernist movement within Islam, noted for his application of "rational science" to the Quran and Hadith and his conclusion that the Hadith were not legally binding on Muslims. He "questioned the historicity and authenticity of many, if not most, traditions, much as the noted scholars Ignaz Goldziher and Joseph Schacht would later do." He doubted Hadith compilers’ capacity to judge the character of Hadith transmitters of several past generations involved in oral Hadith transmission, and notes, "it is difficult enough to judge the character of living people, let alone long dead. The muhaddithun [Hadith scholars/transmitters] did the best they could, but their task was almost impossible." His student, Chiragh ‘Ali, went further, suggesting nearly all the Hadith were fabrications.
Ghulam Ahmed Pervez (1903–1985), a friend of Muhammad Ali Jinnah the founder of Pakistan and a student of the renowned Islamic poet and philosopher Allama Iqbal, was a noted critic of the Hadith and believed that the Quran was sufficient for Muslims to understand and practice Islam, but with the important caveat that the Quran had to be studied using the appropriate rules and conventions of the classical language in which it was revealed. He also rejected the arbitrary authority of the clerical establishment and deemed them counter-productive. He argued that translations and commentaries of the Quran do not accurately reflect the meanings of the original Classical Arabic language and accused the clerical establishment of depriving Muslims of the real message of the Quran intentionally to serve their own self-serving purposes. A fatwa, ruling, signed by more than a thousand orthodox clerics, denounced him as a 'kafir', a non-believer. However, he continued his research and work in Pakistan, having gathered an appreciative audience. The organization which he founded Tolu-e-Islam continues to expand the base of his ideas. His seminal work, "Maqam-e Hadith" argued that the Hadith were composed of "the garbled words of previous centuries", but suggests that he is not against the "idea" of collected sayings of the Prophet, only that he would consider any hadith that goes against the teachings of Quran to have been falsely attributed to the Prophet. He was also against mystical interpretations of Islam which relegated Islam to the private sphere, as he believed Islam was not actually a "religion" to be practiced individually and based in a dogmatic blind faith. Pervez argued that since God requires certainty from believers and certainty can only be achieved by reason, therefore true Islam is actually inherently opposed to Religion, an argument he elaborated in his scholarly work "Islam: A Challenge to Religion".
The 1986 Malaysian book "Hadith: A Re-evaluation" by Kassim Ahmad was met with controversy and some scholars declared him an apostate from Islam for suggesting that "the hadith are sectarian, anti-science, anti-reason and anti-women".
Western academic scholarship.
Early Western exploration of Islam consisted primarily of translation of the Qur'an and a few histories. In the 19th century, scholars translated and commented upon a great variety of Muslim religious texts; by the beginning of the 20th century, Western scholars of Islam started to critically engage with these Islamic texts. Ignaz Goldziher is the best known of these turn-of-the-century critics, who also included D. S. Margoliuth, Henri Lammens, and Leone Caetani. Goldziher writes, in his "Mohammedan Studies": "it is not surprising that, among the hotly debated controversial issues of Islam, whether political or doctrinal, there is not one in which the champions of the various views are unable to cite a number of traditions, all equipped with imposing "isnads"". John Esposito notes that "Modern Western scholarship has seriously questioned the historicity and authenticity of the "hadith"", maintaining that "the bulk of traditions attributed to the Prophet Muhammad were actually written much later." He mentions Joseph Schacht as one scholar who argues this, claiming that Schacht "found no evidence of legal traditions before 722," from which Schacht concluded that "the Sunna of the Prophet is not the words and deeds of the Prophet, but apocryphal material" dating from later.
Contemporary Western scholars of hadith include: Herbert Berg, Fred M. Donner and Wilfred Madelung. Madelung has immersed himself in the hadith literature and has made his own selection and evaluation of tradition. Having done this, he is much more willing to trust hadith than many of his contemporaries. Madelung said of hadith: "Work with the narrative sources, both those that have been available to historians for a long time and others which have been published recently, made it plain that their wholesale rejection as late fiction is unjustified and that with a judicious use of them, a much more reliable and accurate portrait of the period can be drawn than has been realized so far."
Harald Motzki said: "The mere fact that ahadith and asanid were forged must not lead us to conclude that all of them are fictitious or that the genuine and the spurious cannot be distinguished with some degree of certainty."
Unreliable famous hadiths.
The authenticity of a number of famous hadiths is contested. They do not appear in any of the authoritative collections: Muhammad al-Bukhari, Muslim ibn al-Hajjaj, and Abu Dawood, among others.
Some of them are famous.
- Hadith about greater jihad : Al-Suyuti said: al-Khatib al-Baghdadi relates in his "History" on the authority of Jabir: The Prophet came back from one of his campaigns saying: "You have come forth in the best way of coming forth: you have come from the smaller jihad to the greater jihad." They said: "And what is the greater jihad?" He replied: "The striving (mujahadat) of Allah's servants against their idle desires." 
Reasons of the contestation : According to the Muslim Jurist Ibn Hajar al-Asqalani, in Tasdid al-qaws : "This saying is widespread and it is a saying by Ibrahim ibn Ablah according to Nisa'i in al-Kuna. 
Al-Bayhaqi narrated it in al-Zuhd al-Kabir (Haydar ed. p. 165 §373 = p. 198 §374) and said: "This is a chain that contains weakness" (hadha isnadun fihi da`f). 
Al-Khatib narrated it in Tarikh Baghdad (13:493=13:523). Both their chains contain Yahya ibn al-`Ala' al-Bajali al-Razi who is accused of forgery as per Ibn Hajar in the Taqrib, in addition to Layth ibn Abi Sulaym - Ibn Hajar said he was abandoned as a hadith narrator due to the excessiveness of his mistakes in addition to being a concealer of his sources (mudallis). 
- Hadith about ink scholars : “The ink of the scholar is more sacred than the blood of the martyr” or "The ink of scholars (used in writing) is weighed on the Day of Judgment with the blood of martyrs and the ink of scholars out-weighs the blood of martyrs (Shahadah)". It is also mentioned by Ibn Abd al-Bar in his book: "Jamie Bayan al-'Ilm wa Fadlu". As it was also mentioned by Ibn al-Jawzi in his book: "Al-Ilal". 
Reasons of the contestation : Theses hadeeths were narrated from a number of the Sahabah, but they have weak and flimsy, or fabricated isnaads, which we will mention here in brief:
From Abu’d-Darda’ (may Allah be pleased with him): It was narrated by Ibn ‘Abd al-Barr in Jaami‘ Bayaan al-‘Ilm (1/150). His isnaad includes Ismaa‘eel ibn Abi Ziyaad, of whom Ibn Hibbaan said: He is a charlatan. Hence al-‘Iraqi classed it as da‘eef in Takhreej al-Ihya’, p. 5 
From ‘Abdullah ibn ‘Amr ibn al-‘Aas (may Allah be pleased with him). It was narrated by Abu Na‘eem in Akhbaar Asbahaan (1718) and ad-Daylami in Musnad al-Firdaws. Its isnaad also includes Ismaa‘eel ibn Abi Ziyaad, who is mentioned above.
It was also narrated by Ibn al-Jawzi in al-‘Ilal al-Mutanaahiyah (1/81) via another isnaad. He said: This is not saheeh. Ahmad ibn Hanbal said: Muhammad ibn Yazeed al-Waasiti did not narrate anything from ‘Abd ar-Rahmaan ibn Ziyaad. Ibn Hibbaan said: He narrates fabricated reports from trustworthy narrators.
However, for this particular Hadith, it should be noted that it is not fully agreed upon (Mutaffakun Alayhee) by all scholars as authentic due to gaps in its chain of narrators and Al-Suyuti himself grades it as 'weak'. 
- Hadith about knowledge : "Seek knowledge from the cradle to the grave." 
Reasons of the contestation : The Fatwa Department Research Committee - chaired by Sheikh `Abd al-Wahhâb al-Turayrî said : "We could not find any trance of this phrase in the hadîth literature. We could not even find it in any of the compilations the preserve the saying of the Companions and Successors."
We believe this is just an old wise saying. The meaning of this statement is sound. The Qur’ân and Sunnah come with numerous encouragements for seeking knowledge at all times and in all beneficial fields, whatever the age of the person. 
- Other Hadith about knowledge :
“Seek knowledge even in China” 
Reasons of the contestation : Shaykh al-Albaani said in Da’eef al-Jaami’: “(It is) fabricated.”
Narrated from Anas by al-Bayhaqi in Shu`ab al-Imaan and al-Madkhal, Ibn `Abd al-Barr in Jami` Bayaan al-`Ilm, and al-Khatib through three chains at the opening of his al-Rihla fi Talab al-Hadith (p. 71-76 #1-3) where Shaykh Nur al-Din `Itr declares it weak (da`îf). Also narrated from Ibn `Umar, Ibn `Abbas, Ibn Mas`ud, Jabir, and Abu Sa`id al-Khudri, all through very weak chains.
The hadith master al-Mizzi said it has so many chains that it deserves a grade of fair (hasan), as quoted by al-Sakhawi in al-Maqaasid al-Hasana. Al-`Iraqi in his Mughni `an Haml al-Asfar similarly stated that some scholars declared it sound (sahîh) for that reason, even if al-Hakim and al-Dhahabi correctly said no sound chain is known for it. Ibn `Abd al-Barr's "Salafi" editor Abu al-Ashbal al-Zuhayri declares the hadith hasan in Jami` Bayaan al-`Ilm (1:23ff.) but all the above fair gradings actually apply to the wording: "Seeking knowledge is an obligation upon every Muslim." 
The first to declare the "China" hadith forged seems to be Ibn al-Qaysarani (d. 507) in his Ma`rifa al-Tadhkira (p. 101 #118). This grading was kept by Ibn al-Jawzi in his Mawdu`at but rejected, among others, by al-Suyuti in al-La'ali' (1:193), al-Mizzi, al-Dhahabi in Talkhis al-Wahiyat, al-Bajuri's student Shams al-Din al-Qawuqji (d. 1305) in his book al-Lu'lu' al-Marsu` (p. 40 #49), and notably by the Indian muhaddith Muhammad Taahir al-Fattani (d. 986) in his Tadhkira al-Mawdu`at (p. 17) in which he declares it hasan.
Al-Munawi, like Ibn `Abd al-Barr before him, gave an excellent explanation of the hadith in his Fayd al-Qadir (1:542). See also its discussion in al-`Ajluni's Kashf al-Khafa' under the hadith: "Seeking knowledge is an obligation upon every Muslim," itself a fair (hasan) narration in Ibn Maajah because of its many chains as stated by al-Mizzi, although al-Nawawi in his Fatawa (p. 258) declared it weak while Dr. Muhammad `Ajaj al-Khaatib in his notes on al-Khatib's al-Jami` (2:462-463) declared it "sound due to its witness-chains" (sahîh li ghayrih). Cf. al-Sindi's Hashya Sunan Ibn Maajah (1:99), al-Munawi's Fayd al-Qadir (4:267) and al-Sakhaawi's al-Maqaasid al-Hasana (p. 275-277). 

</doc>
<doc id="13755" url="http://en.wikipedia.org/wiki?curid=13755" title="Hull (watercraft)">
Hull (watercraft)

A hull is the watertight body of a ship or boat. Above the hull is the superstructure and/or deckhouse, where present. The line where the hull meets the water surface is called the waterline.
The structure of the hull varies depending on the vessel type. In a typical modern steel ship, the structure consists of watertight and non-tight decks, major transverse and watertight (and also sometimes non-tight or longitudinal) members called bulkheads, intermediate members such as girders, stringers and webs, and minor members called ordinary transverse frames, frames, or longitudinals, depending on the structural arrangement. The uppermost continuous deck may be called the "upper deck", "weather deck", "spar deck", "main deck", or simply "deck". The particular name given depends on the context—the type of ship or boat, the arrangement, or even where it sails. Not all hulls are decked (for instance a dinghy).
In a typical wooden sailboat, the hull is constructed of wooden planking, supported by transverse frames (often referred to as ribs) and bulkheads, which are further tied together by longitudinal stringers or ceiling. Often but not always there is a centerline longitudinal member called a keel.
In fiberglass or composite hulls, the structure may resemble wooden or steel vessels to some extent, or be of a monocoque arrangement. In many cases, composite hulls are built by sandwiching thin fiber-reinforced skins over a lightweight but reasonably rigid core of foam, balsa wood, impregnated paper honeycomb or other material.
General features.
The shape of the hull is entirely dependent upon the needs of the design. Shapes range from a nearly perfect box in the case of scow barges, to a needle-sharp surface of revolution in the case of a racing multihull sailboat. The shape is chosen to strike a balance between cost, hydrostatic considerations (accommodation, load carrying and stability), hydrodynamics (speed, power requirements, and motion and behavior in a seaway) and special considerations for the ship's role, such as the rounded bow of an icebreaker or the flat bottom of a landing craft.
Hull shapes.
Hulls come in many varieties and can have composite shape, (e.g., a fine entry forward and inverted bell shape aft), but are grouped primarily as follows:
Categorization.
After this they can be categorized as:
Most used hull forms.
At present, the most widely used form is the round bilge hull.
The inverted bell shape of the hull, with smaller payload the waterline cross-section is less, hence the resistance is less and the speed is higher. With higher payload the outward bend provides smoother performance in waves. As such, the inverted bell shape is a popular form used with planing hulls.
Hull forms.
Chined and hard-chined hulls.
A chined hull consists of straight, smooth, tall, long, or short plates,timbers or sheets of ply, which are set at an angle to each other when viewed in transverse section . The traditional chined hull is a simple hull shape because it works with only straight planks bent into a curve. These boards are often bent lengthwise. Plywood chined boats made of 8' x 4' sheets have most bend along the long axis of the sheet. Only thin ply 3–6 mm can easily be shaped into a compound bend. Most home-made constructed boats are chined hull boats. Mass-produced chine powerboats are usually made of sprayed chop strand fibreglass over a wooden mold. The Cajun "pirogue" is an example of a craft with hard chines. Benefits of this type of boating activity is the low production cost and the (usually) fairly flat bottom, making the boat faster at planing. Sail boats with chined hull make use of a dagger board or keel.
Chined hulls can be divided up into 3 shapes: 
Each of these chine hulls has its own unique characteristics and use. The flat bottom hull has high initial stability but high drag. To counter the high drag hull forms are narrow and sometimes severely tapered at bow and stern. This leads to poor stability when heeled in a sail boat. This is often countered by using heavy interior ballast on sailing versions. They are best suited to sheltered inshore waters. Early racing power boats were fine forward and flat aft. This produced maximum lift and a smooth,fast ride in flat water but this hull form is easily unsettled in waves. The multi chine hull approximates a curved hull form. It has less drag than a flat bottom boat. Multi chines are more complex to build but produce a more seaworthy hull form. They are usually displacement hulls. V or arc bottom chine boats have a v shape between 6 and 23 degrees. This is called the deadrise angle. The flatter shape of a 6 degrees hull will plane with less wind or a lower horse power engine but will pound more in waves. The deep V form (between 18 and 23 degrees) is only suited to high power planing boats. They require more powerful engines to lift the boat onto the plane but give a faster smoother ride in waves.
Displacement chined hulls have more wetted surface area, hence more drag, than an equivalent round hull form, for any given displacement.
Smooth curve hulls.
Smooth curve hulls are hulls which use, just like the curved hulls, a sword or an attached keel.
Semi round bilge hulls are somewhat less round. The advantage of the semi-round is that it is a nice middle between the S-bottom and chined hull. Typical examples of a semi-round bilge hull can be found in the Centaur and Laser cruising dinghies.
S-bottom hulls are hulls shaped like an "s" . In the s-bottom, the hull runs smooth to the keel. As there are no sharp corners in the fuselage. Boats with this hull have a fixed keel, or a "kielmidzwaard" (literally "keel with sword"). This is a short fixed keel, with a swing keel inside. Examples of cruising dinghies that use this s-shape are the Yngling and Randmeer.
Metrics.
Hull forms are defined as follows:
History.
Rafts have a hull of sorts, however, hulls of the earliest design are thought to have each consisted of a hollowed out tree bole: in effect the first canoes. Hull form then proceeded to the coracle shape and on to more sophisticated forms as the science of naval architecture advanced.

</doc>
<doc id="13756" url="http://en.wikipedia.org/wiki?curid=13756" title="Hymn">
Hymn

A hymn is a type of song, usually religious, specifically written for the purpose of praise, adoration or prayer, and typically addressed to a deity or deities, or to a prominent figure or personification. Although most familiar to speakers of English in the context of Christian churches, hymns are also a fixture of other world religions, especially on the Indian subcontinent. Hymns also survive from antiquity, especially from Egyptian and Greek cultures. Some of the oldest surviving examples of notated music are hymns with Greek texts. The word "hymn" derives from Greek ὕμνος ("hymnos"), which means "a song of praise". Collections of hymns are known as hymnals or hymn books. Hymns may or may not include instrumental accompaniment.
Origins.
Ancient hymns include the Egyptian "Great Hymn to the Aten", composed by Pharaoh Akhenaten; the "Vedas", a collection of hymns in the tradition of Hinduism; and the Psalms, a collection of songs from Judaism. The Western tradition of hymnody begins with the Homeric Hymns, a collection of ancient Greek hymns, the oldest of which were written in the 7th century BC, praising deities of the ancient Greek religions. Surviving from the 3rd century BC is a collection of six literary hymns ("Ὕμνοι") by the Alexandrian poet Callimachus.
Patristic writers began applying the term ὕμνος, or "hymnus" in Latin, to Christian songs of praise, and frequently used the word as a synonym for "psalm".
The word "hymn" came from the Greek word "hymnos".<http://www.sharefaith.com/guide/Christian-Music/hymns-the-songs-and-the-stories/history-of-hymns.html>
Christian hymnody.
Originally modeled on the Psalms and other poetic passages (commonly referred to as "canticles") in the Scriptures, Christian hymns are generally directed as praise to the Christian God. Many refer to Jesus Christ either directly or indirectly.
Since the earliest times, Christians have sung "psalms and hymns and spiritual songs", both in private devotions and in corporate worship (; ; ; ; ; ; ; cf. ; ).
One definition of a hymn is "...a lyric poem, reverently and devotionally conceived, which is designed to be sung and which expresses the worshipper's attitude toward God or God's purposes in human life. It should be simple and metrical in form, genuinely emotional, poetic and literary in style, spiritual in quality, and in its ideas so direct and so immediately apparent as to unify a congregation while singing it."
Christian hymns are often written with special or seasonal themes and these are used on holy days such as Christmas, Easter and the Feast of All Saints, or during particular seasons such as Advent and Lent. Others are used to encourage reverence for the Holy Bible or to celebrate Christian practices such as the eucharist or baptism. Some hymns praise or address individual saints, particularly the Blessed Virgin Mary; such hymns are particularly prevalent in Catholicism, Eastern Orthodoxy and to some extent High Church Anglicanism.
A writer of hymns is known as a "hymnist" or "hymnodist", and the practice of singing hymns is called "hymnody"; the same word is used for the collectivity of hymns belonging to a particular denomination or period (e.g. "nineteenth century Methodist hymnody" would mean the body of hymns written and/or used by Methodists in the 19th century). A collection of hymns is called a "hymnal" or "hymnary". These may or may not include music. A student of hymnody is called a "hymnologist", and the scholarly study of hymns, hymnists and hymnody is hymnology. The music to which a hymn may be sung is a hymn tune. 
In many Evangelical churches, traditional songs are classified as hymns while more contemporary worship songs are not considered hymns. The reason for this distinction is unclear, but according to some it is due to the radical shift of style and devotional thinking that began with the Jesus movement and Jesus music.
Music and accompaniment.
In ancient and medieval times, stringed instruments such as the harp, lyre and lute were used with psalms and hymns.
Since there is a lack of musical notation in early writings, the actual musical forms in the early church can only be surmised. During the Middle Ages a rich hymnody developed in the form of Gregorian chant or plainsong. This type was sung in unison, in one of eight church modes, and most often by monastic choirs. While they were written originally in Latin, many have been translated; a familiar example is the 4th century "Of the Father's Heart Begotten" sung to the 11th century plainsong "Divinum Mysterium".
Western church.
Later hymnody in the Western church introduced four-part vocal harmony as the norm, adopting major and minor keys, and came to be led by organ and choir. It shares many elements with classical music.
Today, except for choirs, more musically inclined congregations and "a cappella" congregations, hymns are typically sung in unison. In some cases complementary full settings for organ are also published, in others organists and other accompanists are expected to transcribe the four-part vocal score for their instrument of choice.
To illustrate Protestant usage, in the traditional services and liturgies of the Methodist churches, which are based upon Anglican practice, hymns are sung (often accompanied by an organ) during the processional to the altar, during the receiving of the Eucharist, during the recessional, and sometimes at other points during the service. These hymns can be found in a common book such as the United Methodist Hymnal. The Doxology is also sung after the tithes and offerings are brought up to the altar.
Contemporary Christian worship, as often found in Evangelicalism and Pentecostalism, may include the use of contemporary worship music played with electric guitars and the drum kit, sharing many elements with rock music.
Other groups of Christians have historically excluded instrumental accompaniment, citing the absence of instruments in worship by the church in the first several centuries of its existence, and adhere to an unaccompanied "a cappella" congregational singing of hymns. These groups include the 'Brethren' (often both 'Open' and 'Exclusive'), the Churches of Christ, Mennonites, Primitive Baptists, and certain Reformed churches, although during the last century or so, several of these, such as the Free Church of Scotland have abandoned this stance.
Eastern church.
Eastern Christianity (the Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches) have a very rich and ancient hymnographical tradition. 
Eastern chant is almost always a cappella, and instrumental accompaniment is rare. The central form of chant in the Eastern Orthodoxy is Byzantine Chant, which is used to chant all forms of liturgical worship. Exceptions include the Coptic Orthodox tradition which makes use of the sistrum, the Indian Orthodox (Malankara Orthodox Syrian Church) which makes use of the organ and the Ethiopian Orthodox Tewahedo Church, which also uses drums, cymbals and other instruments on certain occasions.
The development of Christian hymnody.
Thomas Aquinas, in the introduction to his commentary on the Psalms, defined the Christian hymn thus: ""Hymnus est laus Dei cum cantico; canticum autem exultatio mentis de aeternis habita, prorumpens in vocem"." ("A hymn is the praise of God with song; a song is the exultation of the mind dwelling on eternal things, bursting forth in the voice.")
The Protestant Reformation resulted in two conflicting attitudes to hymns. One approach, the regulative principle of worship, favoured by many Zwinglians, Calvinists and some radical reformers, considered anything that was not directly authorised by the Bible to be a novel and Catholic introduction to worship, which was to be rejected. All hymns that were not direct quotations from the Bible fell into this category. Such hymns were banned, along with any form of instrumental musical accompaniment, and organs were removed from churches. Instead of hymns, biblical psalms were chanted, most often without accompaniment, to very basic melodies. This was known as exclusive psalmody. Examples of this may still be found in various places, including in some of the Presbyterian churches of western Scotland. 
The other Reformation approach, the normative principle of worship, produced a burst of hymn writing and congregational singing. Martin Luther is notable not only as a reformer, but as the author of many hymns including "Ein feste Burg ist unser Gott" ("A Mighty Fortress Is Our God"), which is sung today even by Catholics, and "Gelobet seist du, Jesu Christ" ("Praise be to You, Jesus Christ") for Christmas. Luther and his followers often used their hymns, or chorales, to teach tenets of the faith to worshipers. The first Protestant hymnal was published in Bohemia in 1532 by the Unitas Fratrum. Count Zinzendorf, the Lutheran leader of the Moravian Church in the 18th century wrote some 2,000 hymns. The earlier English writers tended to paraphrase biblical texts, particularly Psalms; Isaac Watts followed this tradition, but is also credited as having written the first English hymn which was not a direct paraphrase of Scripture.
Watts (1674–1748), whose father was an Elder of a dissenter congregation, complained at age 16, that when allowed only psalms to sing, the faithful could not even sing about their Lord, Christ Jesus. His father invited him to see what he could do about it; the result was Watts' first hymn, "Behold the glories of the Lamb".
Found in few hymnals today, the hymn has eight stanzas in common meter and is based on Revelation 5:6, 8, 9, 10, 12.
Relying heavily on Scripture, Watts wrote metered texts based on New Testament passages that brought the Christian faith into the songs of the church. Isaac Watts has been called "the father of English hymnody", but Erik Routley sees him more as "the liberator of English hymnody", because his hymns, and hymns like them, moved worshipers beyond singing only Old Testament psalms, inspiring congregations and revitalizing worship.
Later writers took even more freedom, some even including allegory and metaphor in their texts.
Charles Wesley's hymns spread Methodist theology, not only within Methodism, but in most Protestant churches. He developed a new focus: expressing one's personal feelings in the relationship with God as well as the simple worship seen in older hymns. Wesley wrote:
<poem>
Where shall my wondering soul begin?
How shall I all to heaven aspire?
A slave redeemed from death and sin,
A brand plucked from eternal fire,
How shall I equal triumphs raise,
Or sing my great deliverer's praise.
</poem>
Wesley's contribution, along with the Second Great Awakening in America led to a new style called gospel, and a new explosion of sacred music writing with Fanny Crosby, Lina Sandell, Philip Bliss, Ira D. Sankey, and others who produced testimonial music for revivals, camp meetings, and evangelistic crusades. The tune style or form is technically designated "gospel songs" as distinct from hymns. Gospel songs generally include a refrain (or chorus) and usually (though not always) a faster tempo than the hymns. As examples of the distinction, "Amazing Grace" is a hymn (no refrain), but "How Great Thou Art" is a gospel song. During the 19th century the gospel-song genre spread rapidly in Protestantism and, to a lesser but still definite extent, in Roman Catholicism; the gospel-song genre is unknown in the worship "per se" by Eastern Orthodox churches, which rely exclusively on traditional chants (a type of hymn).
The Methodist Revival of the 18th century created an explosion of hymn-writing in Welsh, which continued into the first half of the 19th century. The most prominent names among Welsh hymn-writers are William Williams Pantycelyn and Ann Griffiths. The second half of the 19th century witnessed an explosion of hymn tune composition and choir singing in Wales. 
Along with the more classical sacred music of composers ranging from Mozart to Monteverdi, the Catholic Church continued to produce many popular hymns such as Lead, Kindly Light, Silent Night, O Sacrament Divine and Faith of our Fathers. 
Many churches today use contemporary worship music which includes a range of styles often influenced by popular music. This often leads to some conflict between older and younger congregants (see contemporary worship). This is not new; the Christian pop music style began in the late 1960s and became very popular during the 1970s, as young hymnists sought ways in which to make the music of their religion relevant for their generation.
This long tradition has resulted in a wide variety of hymns. Some modern churches include within hymnody the traditional hymn (usually describing God), contemporary worship music (often directed to God) and gospel music (expressions of one's personal experience of God). This distinction is not perfectly clear; and purists remove the second two types from the classification as hymns. It is a matter of debate, even sometimes within a single congregation, often between revivalist and traditionalist movements.
American developments.
African-Americans developed a rich hymnody from spirituals during times of slavery to the modern, lively black gospel style. The first influences of African American Culture into hymns came from Slave Songs of the United States a collection of slave hymns complied by William Francis Allen who had difficulty pinning them down from the oral tradition, and though he succeeded, he points out the awe inspiring effect of the hymns when sung in by their originators.
Thomas Symmes spread throughout churches a new idea of how to sing hymns, in which anyone could sing a hymn any way they felt led to; this idea was opposed by the views of Symmes' colleagues who felt it was "like Five Hundred different Tunes roared out at the same time". William Billings, a singing school teacher, created the first tune book with only American born compositions. Within his books, Billings did not put as much emphasis on "common measure" which was the typical way hymns were sung, but he attempted "to have a Sufficiency in each measure". Boston's Handel and Haydn Society aimed at raising the level of church music in America, publishing their "Collection of Church Music". In the late 19th century Ira D. Sankey and Dwight L. Moody developed the relatively new subcategory of Gospel hymns.
Hymn meters.
The meter indicates the number of syllables for the lines in each stanza of a hymn. This provides a means of marrying the hymn's text with an appropriate hymn tune for singing. In practice many hymns conform to one of a relatively small number of meters (syllable count and stress patterns). Care must be taken, however, to ensure that not only the metre of words and tune match, but also the stresses on the words in each line. Technically speaking an iambic tune, for instance, cannot be used with words of, say, trochaic metre.
The meter is often denoted by a row of figures besides the name of the tune, such as "87.87.87", which would inform the reader that each verse has six lines, and that the first line has eight syllables, the second has seven, the third line eight, etc. The meter can also be described by initials; L.M. indicates long meter, which is 88.88 (four lines, each eight syllables long); S.M. is short meter (66.86); C.M. is common metre (86.86), while D.L.M., D.S.M. and D.C.M. (the "D" stands for double) are similar to their respective single meters except that they have eight lines in a verse instead of four.
Also, if the number of syllables in one verse differ from another verse in the same hymn (e.g., the hymn "I Sing a Song of the Saints of God"), the meter is called Irregular.
Sikh hymnody.
The Sikh holy book, the Guru Granth Sahib Ji (Punjabi: ਗੁਰੂ ਗ੍ਰੰਥ ਸਾਹਿਬ ]), is a collection of hymns (Shabad) or "Gurbani" describing the qualities of God and why one should meditate on God's name. The "Guru Granth Sahib" is divided by their musical setting in different ragas into fourteen hundred and thirty pages known as "Angs" (limbs) in Sikh tradition. Guru Gobind Singh (1666–1708), the tenth guru, after adding Guru Tegh Bahadur's bani to the Adi Granth affirmed the sacred text as his successor, elevating it to "Guru Granth Sahib". The text remains the holy scripture of the Sikhs, regarded as the teachings of the Ten Gurus. The role of Guru Granth Sahib, as a source or guide of prayer, is pivotal in Sikh worship.
External links.
The links below are restricted to either material that is historical or resources that are non-denominational or inter-denominational. Denomination-specific resources are mentioned from the relevant denomination-specific articles. 

</doc>
<doc id="13758" url="http://en.wikipedia.org/wiki?curid=13758" title="History of physics">
History of physics

Physics (from the Ancient Greek φύσις "physis" meaning "nature") is the fundamental branch of science that developed out of the study of nature and philosophy known, until around the end of the 19th century, as "natural philosophy". Today, physics is ultimately defined as the study of matter, energy and the relationships between them. Physics is, in some senses, the oldest and most basic pure science; its discoveries find applications throughout the natural sciences, since matter and energy are the basic constituents of the natural world. The other sciences are generally more limited in their scope and may be considered branches that have split off from physics to become sciences in their own right. Physics today may be divided loosely into classical physics and modern physics.
Ancient history.
Elements of what became physics were drawn primarily from the fields of astronomy, optics, and mechanics, which were methodologically united through the study of geometry. These mathematical disciplines began in antiquity with the Babylonians and with Hellenistic writers such as Archimedes and Ptolemy. Ancient philosophy, meanwhile – including what was called "physics" – focused on explaining nature through ideas such as Aristotle's four types of "cause".
Ancient Greece.
The move towards a rational understanding of nature began at least since the Archaic period in Greece (650–480 BCE) with the Pre-Socratic philosophers. The philosopher Thales of Miletus (7th and 6th centuries BCE), dubbed "the Father of Science" for refusing to accept various supernatural, religious or mythological explanations for natural phenomena, proclaimed that every event had a natural cause. Thales also made advancements in 580 BCE by suggesting that water is the basic element, experimenting with the attraction between magnets and rubbed amber and formulating the first recorded cosmologies. Anaximander, famous for his proto-evolutionary theory, disputed the Thales' ideas and proposed that rather than water, a substance called "apeiron" was the building block of all matter. Around 500 BCE, Heraclitus proposed that the only basic law governing the Universe was the principle of change and that nothing remains in the same state indefinitely. This observation made him one of the first scholars in ancient physics to address the role of time in the universe, a key and sometimes contentious concept in modern and present-day physics. The early physicist Leucippus (fl. first half of the 5th century BCE) adamantly opposed the idea of direct divine intervention in the universe, proposing instead that natural phenomena had a natural cause. Leucippus and his student Democritus were the first to develop the theory of atomism, the idea that everything is composed entirely of various imperishable, indivisible elements called atoms.
During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy slowly developed into an exciting and contentious field of study. Aristotle (Greek: Ἀριστοτέλης, "Aristotélēs") (384 – 322 BCE), a student of Plato, promoted the concept that observation of physical phenomena could ultimately lead to the discovery of the natural laws governing them. Aristotle's writings cover physics, metaphysics, poetry, theater, music, logic, rhetoric, linguistics, politics, government, ethics, biology and zoology. He wrote the first work which refers to that line of study as "Physics" – in the 4th century BCE, Aristotle founded the system known as Aristotelian physics. He attempted to explain ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that all matter was made up of aether, or some combination of four elements: earth, water, air, and fire. According to Aristotle, these four terrestrial elements are capable of inter-transformation and move toward their natural place, so a stone falls downward toward the center of the cosmos, but flames rise upward toward the circumference. Eventually, Aristotelian physics became enormously popular for many centuries in Europe, informing the scientific and scholastic developments of the Middle Ages. It remained the mainstream scientific paradigm in Europe until the time of Galileo Galilei and Isaac Newton.
Early in Classical Greece, knowledge that the Earth is spherical ("round") was common. Around 240 BCE, as the result a seminal experiment, Eratosthenes (276–194 BCE) accurately estimated its circumference. In contrast to Aristotle's geocentric views, Aristarchus of Samos (Greek: Ἀρίσταρχος; c.310 – c.230 BCE) presented an explicit argument for a heliocentric model of the Solar system, i.e. for placing the Sun, not the Earth, at its centre. Seleucus of Seleucia, a follower of Aristarchus' heliocentric theory, stated that the Earth rotated around its own axis, which, in turn, revolved around the Sun. Though the arguments he used were lost, Plutarch stated that Seleucus was the first to prove the heliocentric system through reasoning.
In the 3rd century BCE, the Greek mathematician Archimedes of Syracuse (Greek: Ἀρχιμήδης (287–212 BCE) – generally considered to be the greatest mathematician of antiquity and one of the greatest of all time – laid the foundations of hydrostatics, statics and calculated the underlying mathematics of the lever. A leading scientist of classical antiquity, Archimedes also developed elaborate systems of pulleys to move large objects with a minimum of effort. The Archimedes' screw underpins modern hydroengineering, and his machines of war helped to hold back the armies of Rome in the First Punic War. Archimedes even tore apart the arguments of Aristotle and his metaphysics, pointing out that it was impossible to separate mathematics and nature and proved it by converting mathematical theories into practical inventions. Furthermore, in his work "On Floating Bodies", around 250 BCE, Archimedes developed the law of buoyancy, also known as Archimedes' Principle. In mathematics, Archimedes used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers. He also developed the principles of equilibrium states and centers of gravity, ideas that would influence the Islamic scholars, Galileo, and Newton.
Hipparchus (190–120 BCE), focusing on astronomy and mathematics, used sophisticated geometrical techniques to map the motion of the stars and planets, even predicting the times that Solar eclipses would happen. In addition, he added calculations of the distance of the Sun and Moon from the Earth, based upon his improvements to the observational instruments used at that time. Another of the most famous of the early physicists was Ptolemy (90–168 CE), one of the leading minds during the time of the Roman Empire. Ptolemy was the author of several scientific treatises, at least three of which were of continuing importance to later Islamic and European science. The first is the astronomical treatise now known as the "Almagest" (in Greek, Ἡ Μεγάλη Σύνταξις, "The Great Treatise", originally Μαθηματικὴ Σύνταξις, "Mathematical Treatise"). The second is the "Geography", which is a thorough discussion of the geographic knowledge of the Greco-Roman world.
Much of the accumulated knowledge of the ancient world was lost. Even of the works of the better known thinkers, few fragments survived. Although he wrote at least fourteen books, almost nothing of Hipparchus' direct work survived. Of the 150 reputed Aristotelian works, only 30 exist, and some of those are "little more than lecture notes".
India and China.
Important physical and mathematical traditions also existed in ancient Chinese and Indian sciences.
In Indian philosophy, Maharishi Kanada was the first to systematically develop a theory of atomism around 200 BCE though some authors have allotted him an earlier era in the 6th century BCE. It was further elaborated by the Buddhist atomists Dharmakirti and Dignāga during the 1st millennium CE. Pakudha Kaccayana, a 6th-century BCE Indian philosopher and contemporary of Gautama Buddha, had also propounded ideas about the atomic constitution of the material world. These philosophers believed that other elements (except ether) were physically palpable and hence comprised minuscule particles of matter. The last minuscule particle of matter that could not be subdivided further was termed Parmanu. These philosophers considered the atom to be indestructible and hence eternal. The Buddhists thought atoms to be minute objects unable to be seen to the naked eye that come into being and vanish in an instant. The Vaisheshika school of philosophers believed that an atom was a mere point in space. Indian theories about the atom are greatly abstract and enmeshed in philosophy as they were based on logic and not on personal experience or experimentation. In Indian astronomy, Aryabhata's "Aryabhatiya" (499 CE) proposed the Earth's rotation, while Nilakantha Somayaji (1444–1544) of the Kerala school of astronomy and mathematics proposed a semi-heliocentric model resembling the Tychonic system.
The study of magnetism in Ancient China dates back to the 4th century BCE. (in the "Book of the Devil Valley Master"), A main contributor to this field was Shen Kuo (1031–1095), a polymath and statesman who was the first to describe the magnetic-needle compass used for navigation, as well as establishing the concept of true north. In optics, Shen Kuo independently developed a camera obscura.
Muslim scientists.
In the 5th to 15th centuries, scientific progress occurred in the Muslim world. Many classic works in Latin and Greek were translated into Arabic. Ibn Sīnā (980–1037), known as "Avicenna", was a polymath from Bukhara (now in present-day Uzbekistan) responsible for important contributions to physics, optics, philosophy and medicine. He is most famous for writing "The Canon of Medicine", a text that was used to teach student doctors in Europe until the 1600s.
Important contributions were made by Ibn al-Haytham (965–1040), a mathematician from Basra (in present-day Iraq) considered one of the founders of modern optics. Ptolemy and Aristotle theorised that light either shone from the eye to illuminate objects or that light emanated from objects themselves, whereas al-Haytham (known by the Latin name Alhazen) suggested that light travels to the eye in rays from different points on an object. The works of Ibn al-Haytham and Abū Rayhān Bīrūnī eventually passed on to Western Europe where they were studied by scholars such as Roger Bacon and Witelo. Omar Khayyám (1048–1131), a Persian scientist, calculated the length of a solar year and was only out by a fraction of a second when compared to our modern day calculations. He used this to compose a calendar considered more accurate than the Gregorian calendar that came along 500 years later. He is classified as one of the world's first great science communicators, said, for example to have convinced a Sufi theologian that the world turns on an axis. 
Nasir al-Din al-Tusi (1201–1274), an astronomer and mathematician from Baghdad, authored the "Treasury of Astronomy", a remarkably accurate table of planetary movements that reformed the existing planetary model of Roman astronomer Ptolemy by describing a uniform circular motion of all planets in their orbits. This work led to the later discovery, by one of his students, that planets actually have an elliptical orbit. Copernicus later drew heavily on the work of al-Din al-Tusi and his students, but without acknowledgment. The gradual chipping away of the Ptolemaic system paved the way for the revolutionary idea that the Earth actually orbited the Sun (heliocentrism).
Medieval Europe.
Awareness of ancient works re-entered the West through translations from Arabic to Latin. Their re-introduction, combined with Judeo-Islamic theological commentaries, had a great influence on Medieval philosophers such as Thomas Aquinas. Scholastic European scholars, who sought to reconcile the philosophy of the ancient classical philosophers with Christian theology, proclaimed Aristotle the greatest thinker of the ancient world. In cases where they didn't directly contradict the Bible, Aristotelian physics became the foundation for the physical explanations of the European Churches. Quantification became a core element of medieval physics. 
Based on Aristotelian physics, Scholastic physics described things as moving according to their essential nature. Celestial objects were described as moving in circles, because perfect circular motion was considered an innate property of objects that existed in the uncorrupted realm of the celestial spheres. The theory of impetus, the ancestor to the concepts of inertia and momentum, was developed along similar lines by medieval philosophers such as John Philoponus and Jean Buridan. Motions below the lunar sphere were seen as imperfect, and thus could not be expected to exhibit consistent motion. More idealized motion in the "sublunary" realm could only be achieved through artifice, and prior to the 17th century, many did not view artificial experiments as a valid means of learning about the natural world. Physical explanations in the sublunary realm revolved around tendencies. Stones contained the element earth, and earthly objects tended to move in a straight line toward the centre of the earth (and the universe in the Aristotelian geocentric view) unless otherwise prevented from doing so.
Scientific revolution.
During the 16th and 17th centuries, a large advancement of scientific progress known as the Scientific revolution took place in Europe. Dissatisfaction with older philosophical approaches had begun earlier and had produced other changes in society, such as the Protestant Reformation, but the revolution in science began when natural philosophers began to mount a sustained attack on the Scholastic philosophical program and supposed that mathematical descriptive schemes adopted from such fields as mechanics and astronomy could actually yield universally valid characterizations of motion and other concepts.
Nicolaus Copernicus.
A breakthrough in astronomy was made by Polish astronomer Nicolaus Copernicus (1473–1543) when, in 1543, he proposed a heliocentric model of the Solar system, ostensibly as a means to render tables charting planetary motion more accurate and to simplify their production. In heliocentric models of the Solar system, the Earth orbits the Sun along with other bodies in Earth's galaxy, a contradiction according to the Greek-Egyptian astronomer Ptolemy (2nd century CE; see above), whose system placed the Earth at the center of the Universe and had been accepted for over 1,400 years. The Greek astronomer Aristarchus of Samos (c.310 – c.230 BCE) had suggested that the Earth revolves around the Sun, but Copernicus' theory was the first to be accepted as a valid scientific possibility. Copernicus' book presenting the theory ("De revolutionibus orbium coelestium", "On the Revolutions of the Celestial Spheres") was published just before his death in 1543 and, as it is now generally considered to mark the beginning of modern astronomy, is also considered to mark the beginning of the Scientific revolution. Copernicus' new perspective, along with the accurate observations made by Tycho Brahe, enabled German astronomer Johannes Kepler (1571–1630) to formulate his laws regarding planetary motion that remain in use today.
Galileo Galilei.
The Italian mathematician, astronomer, and physicist Galileo Galilei (1564–1642) was the central figure in the Scientific revolution and famous for his support for Copernicanism, his astronomical discoveries, empirical experiments and his improvement of the telescope. As a mathematician, Galileo's role in the university culture of his era was subordinated to the three major topics of study: law, medicine, and theology (which was closely allied to philosophy). Galileo, however, felt that the descriptive content of the technical disciplines warranted philosophical interest, particularly because mathematical analysis of astronomical observations – notably, Copernicus' radical analysis of the relative motions of the Sun, Earth, Moon, and planets – indicated that philosophers' statements about the nature of the universe could be shown to be in error. Galileo also performed mechanical experiments, insisting that motion itself – regardless of whether it was produced "naturally" or "artificially" (i.e. deliberately) – had universally consistent characteristics that could be described mathematically.
Galileo's early studies at the University of Pisa were in medicine, but he was soon drawn to mathematics and physics. At 19, he discovered (and, subsequently, verified) the isochronal nature of the pendulum when, using his pulse, he timed the oscillations of a swinging lamp in Pisa's cathedral and found that it remained the same for each swing regardless of the swing's amplitude. He soon became known through his invention of a hydrostatic balance and for his treatise on the center of gravity of solid bodies. While teaching at the University of Pisa (1589–92), he initiated his experiments concerning the laws of bodies in motion that brought results so contradictory to the accepted teachings of Aristotle that strong antagonism was aroused. He found that bodies do not fall with velocities proportional to their weights. The famous story in which Galileo is said to have dropped weights from the Leaning Tower of Pisa is apocryphal, but he did find that the path of a projectile is a parabola and is credited with conclusions that anticipated Newton's laws of motion (e.g. the notion of inertia). Among these is what is now called Galilean relativity, the first precisely formulated statement about properties of space and time outside three-dimensional geometry.
Galileo has been called the "father of modern observational astronomy", the "father of modern physics", the "father of science", and "the father of modern science". According to Stephen Hawking, "Galileo, perhaps more than any other single person, was responsible for the birth of modern science." As religious orthodoxy decreed a geocentric or Tychonic understanding of the Solar system, Galileo's support for heliocentrism provoked controversy and he was tried by the Inquisition. Found "vehemently suspect of heresy", he was forced to recant and spent the rest of his life under house arrest.
The contributions that Galileo made to observational astronomy include the telescopic confirmation of the phases of Venus; his discovery, in 1609, of Jupiter's four largest moons (subsequently given the collective name of the "Galilean moons"); and the observation and analysis of sunspots. Galileo also pursued applied science and technology, inventing, among other instruments, a military compass. His discovery of the Jovian moons was published in 1610 and enabled him to obtain the position of mathematician and philosopher to the Medici court. As such, he was expected to engage in debates with philosophers in the Aristotelian tradition and received a large audience for his own publications such as the "Discourses and Mathematical Demonstrations Concerning Two New Sciences" (published abroad following his arrest for the publication of "Dialogue Concerning the Two Chief World Systems") and "The Assayer". Galileo's interest in experimenting with and formulating mathematical descriptions of motion established experimentation as an integral part of natural philosophy. This tradition, combining with the non-mathematical emphasis on the collection of "experimental histories" by philosophical reformists such as William Gilbert and Francis Bacon, drew a significant following in the years leading up to and following Galileo's death, including Evangelista Torricelli and the participants in the Accademia del Cimento in Italy; Marin Mersenne and Blaise Pascal in France; Christiaan Huygens in the Netherlands; and Robert Hooke and Robert Boyle in England.
René Descartes.
The French philosopher René Descartes (1596–1650) was well-connected to, and influential within, the experimental philosophy networks of the day. Descartes had a more ambitious agenda, however, which was geared toward replacing the Scholastic philosophical tradition altogether. Questioning the reality interpreted through the senses, Descartes sought to re-establish philosophical explanatory schemes by reducing all perceived phenomena to being attributable to the motion of an invisible sea of "corpuscles". (Notably, he reserved human thought and God from his scheme, holding these to be separate from the physical universe). In proposing this philosophical framework, Descartes supposed that different kinds of motion, such as that of planets versus that of terrestrial objects, were not fundamentally different, but were merely different manifestations of an endless chain of corpuscular motions obeying universal principles. Particularly influential were his explanations for circular astronomical motions in terms of the vortex motion of corpuscles in space (Descartes argued, in accord with the beliefs, if not the methods, of the Scholastics, that a vacuum could not exist), and his explanation of gravity in terms of corpuscles pushing objects downward.
Descartes, like Galileo, was convinced of the importance of mathematical explanation, and he and his followers were key figures in the development of mathematics and geometry in the 17th century. Cartesian mathematical descriptions of motion held that all mathematical formulations had to be justifiable in terms of direct physical action, a position held by Huygens and the German philosopher Gottfried Leibniz, who, while following in the Cartesian tradition, developed his own philosophical alternative to Scholasticism, which he outlined in his 1714 work, "The Monadology". Descartes has been dubbed the 'Father of Modern Philosophy', and much subsequent Western philosophy is a response to his writings, which are studied closely to this day. In particular, his "Meditations on First Philosophy" continues to be a standard text at most university philosophy departments. Descartes' influence in mathematics is equally apparent; the Cartesian coordinate system — allowing algebraic equations to be expressed as geometric shapes in a two-dimensional coordinate system — was named after him. He is credited as the father of analytical geometry, the bridge between algebra and geometry, important to the discovery of calculus and analysis.
Isaac Newton.
The late 17th and early 18th centuries saw the achievements of the greatest figure of the Scientific revolution: Cambridge University physicist and mathematician Sir Isaac Newton (1642-1727), considered by many to be the greatest and most influential scientist who ever lived. Newton, a fellow of the Royal Society of England, combined his own discoveries in mechanics and astronomy to earlier ones to create a single system for describing the workings of the universe. Newton formulated three laws of motion and the law of universal gravitation, the latter of which could be used to explain the behavior not only of falling bodies on the earth but also planets and other celestial bodies in the heavens. To arrive at his results, Newton invented one form of an entirely new branch of mathematics: calculus (also invented independently by Gottfried Leibniz), which was to become an essential tool in much of the later development in most branches of physics. Newton's findings were set forth in his "Philosophiæ Naturalis Principia Mathematica" ("Mathematical Principles of Natural Philosophy"), the publication of which in 1687 marked the beginning of the modern period of mechanics and astronomy.
Newton was able to refute the Cartesian mechanical tradition that all motions should be explained with respect to the immediate force exerted by corpuscles. Using his three laws of motion and law of universal gravitation, Newton removed the idea that objects followed paths determined by natural shapes and instead demonstrated that not only regularly observed paths, but all the future motions of any body could be deduced mathematically based on knowledge of their existing motion, their mass, and the forces acting upon them. However, observed celestial motions did not precisely conform to a Newtonian treatment, and Newton, who was also deeply interested in theology, imagined that God intervened to ensure the continued stability of the solar system.
Newton's principles (but not his mathematical treatments) proved controversial with Continental philosophers, who found his lack of metaphysical explanation for movement and gravitation philosophically unacceptable. Beginning around 1700, a bitter rift opened between the Continental and British philosophical traditions, which were stoked by heated, ongoing, and viciously personal disputes between the followers of Newton and Leibniz concerning priority over the analytical techniques of calculus, which each had developed independently. Initially, the Cartesian and Leibnizian traditions prevailed on the Continent (leading to the dominance of the Leibnizian calculus notation everywhere except Britain). Newton himself remained privately disturbed at the lack of a philosophical understanding of gravitation, while insisting in his writings that none was necessary to infer its reality. As the 18th century progressed, Continental natural philosophers increasingly accepted the Newtonians' willingness to forgo ontological metaphysical explanations for mathematically described motions.
Newton built the first functioning reflecting telescope and developed a theory of color, published in "Opticks", based on the observation that a prism decomposes white light into the many colours forming the visible spectrum. While Newton explained light as being composed of tiny particles, a rival theory of light which explained its behavior in terms of waves was presented in 1690 by Christiaan Huygens. However, the belief in the mechanistic philosophy coupled with Newton's reputation meant that the wave theory saw relatively little support until the 19th century. Newton also formulated an empirical law of cooling, studied the speed of sound, investigated power series, demonstrated the generalised binomial theorem and developed a method for approximating the roots of a function. His work on infinite series was inspired by Simon Stevin's decimals. Most importantly, Newton showed that the motions of objects on Earth and of celestial bodies are governed by the same set of natural laws, which were neither capricious nor malevolent. By demonstrating the consistency between Kepler's laws of planetary motion and his own theory of gravitation, Newton also removed the last doubts about heliocentrism. By bringing together all the ideas set forth during the Scientific revolution, Newton effectively established the foundation for modern society in mathematics and science.
Other achievements.
Other branches of physics also received attention during the period of the Scientific revolution. Wilbert Gilbert, court physician to Queen Elizabeth I, published an important work on magnetism in 1600, describing how the earth itself behaves like a giant magnet. Robert Boyle (1627–91) studied the behavior of gases enclosed in a chamber and formulated the gas law named for him; he also contributed to physiology and to the founding of modern chemistry. Another important factor in the scientific revolution was the rise of learned societies and academies in various countries. The earliest of these were in Italy and Germany and were short-lived. More influential were the Royal Society of England (1660) and the Academy of Sciences in France (1666). The former was a private institution in London and included such scientists as John Wallis, William Brouncker, Thomas Sydenham, John Mayow, and Christopher Wren (who contributed not only to architecture but also to astronomy and anatomy); the latter, in Paris, was a government institution and included as a foreign member the Dutchman Huygens. In the 18th century, important royal academies were established at Berlin (1700) and at St. Petersburg (1724). The societies and academies provided the principal opportunities for the publication and discussion of scientific results during and after the scientific revolution. In 1690, James Bernoulli showed that the cycloid is the solution to the tautochrone problem; and the following year, in 1691, Johann Bernoulli showed that a chain freely suspended from two points will form a catenary, the curve with the lowest possible center of gravity available to any chain hung between two fixed points. He then showed, in 1696, that the cycloid is the solution to the brachistochrone problem.
Early thermodynamics.
A precursor of the engine was designed by the German scientist Otto von Guericke who, in 1650, designed and built the world's first vacuum pump and created the world's first ever vacuum known as the Magdeburg hemispheres experiment. He was driven to make a vacuum to disprove Aristotle's long-held supposition that 'Nature abhors a vacuum'. Shortly thereafter, Irish physicist and chemist Boyle had learned of Guericke's designs and in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed the pressure-volume correlation for a gas: "PV" = "k", where "P" is pressure, "V" is volume and "k" is a constant: this relationship is known as Boyle's Law. In that time, air was assumed to be a system of motionless particles, and not interpreted as a system of moving molecules. The concept of thermal motion came two centuries later. Therefore Boyle's publication in 1660 speaks about a mechanical concept: the air spring. Later, after the invention of the thermometer, the property temperature could be quantified. This tool gave Gay-Lussac the opportunity to derive his law, which led shortly later to the ideal gas law. But, already before the establishment of the ideal gas law, an associate of Boyle's named Denis Papin built in 1679 a bone digester, which is a closed vessel with a tightly fitting lid that confines steam until a high pressure is generated.
Later designs implemented a steam release valve to keep the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and cylinder engine. He did not however follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. Hence, prior to 1698 and the invention of the Savery Engine, horses were used to power pulleys, attached to buckets, which lifted water out of flooded salt mines in England. In the years to follow, more variations of steam engines were built, such as the Newcomen Engine, and later the Watt Engine. In time, these early engines would eventually be utilized in place of horses. Thus, each engine began to be associated with a certain amount of "horse power" depending upon how many horses it had replaced. The main problem with these first engines was that they were slow and clumsy, converting less than 2% of the input fuel into useful work. In other words, large quantities of coal (or wood) had to be burned to yield only a small fraction of work output. Hence the need for a new science of engine dynamics was born.
18th-century developments.
During the 18th century, the mechanics founded by Newton was developed by several scientists as more mathematicians learned calculus and elaborated upon its initial formulation. The application of mathematical analysis to problems of motion was known as rational mechanics, or mixed mathematics (and was later termed classical mechanics).
Mechanics.
In 1714, Brook Taylor derived the fundamental frequency of a stretched vibrating string in terms of its tension and mass per unit length by solving a differential equation. The Swiss mathematician Daniel Bernoulli (1700–1782) made important mathematical studies of the behavior of gases, anticipating the kinetic theory of gases developed more than a century later, and has been referred to as the first mathematical physicist. In 1733, Daniel Bernoulli derived the fundamental frequency and harmonics of a hanging chain by solving a differential equation. In 1734, Bernoulli solved the differential equation for the vibrations of an elastic bar clamped at one end. Bernoulli's treatment of fluid dynamics and his examination of fluid flow was introduced in his 1738 work "Hydrodynamica".
Rational mechanics dealt primarily with the development of elaborate mathematical treatments of observed motions, using Newtonian principles as a basis, and emphasized improving the tractability of complex calculations and developing of legitimate means of analytical approximation. A representative contemporary textbook was published by Johann Baptiste Horvath. By the end of the century analytical treatments were rigorous enough to verify the stability of the solar system solely on the basis of Newton's laws without reference to divine intervention—even as deterministic treatments of systems as simple as the three body problem in gravitation remained intractable. In 1705, Edmond Halley predicted the periodicity of Halley's Comet, William Herschel discovered Uranus in 1781, and Henry Cavendish measured the gravitational constant and determined the mass of the Earth in 1798. In 1783, John Michell suggested that some objects might be so massive that not even light could escape from them.
In 1739, Leonhard Euler solved the ordinary differential equation for a forced harmonic oscillator and noticed the resonance phenomenon. In 1742, Colin Maclaurin discovered his uniformly rotating self-gravitating spheroids. British work, carried on by mathematicians such as Taylor and Maclaurin, fell behind Continental developments as the century progressed. Meanwhile, work flourished at scientific academies on the Continent, led by such mathematicians as Bernoulli, Euler, Lagrange, Laplace, and Legendre. In 1743, Jean le Rond d'Alembert published his "Traite de Dynamique", in which he introduces the concept of generalized forces for accelerating systems and systems with constraints. In 1747, Pierre Louis Maupertuis applied minimum principles to mechanics. In 1759, Euler solved the partial differential equation for the vibration of a rectangular drum. In 1764, Euler examined the partial differential equation for the vibration of a circular drum and found one of the Bessel function solutions. In 1776, John Smeaton published a paper on experiments relating power, work, momentum and kinetic energy, and supporting the conservation of energy. In 1788, Joseph Louis Lagrange presented Lagrange's equations of motion in "Mécanique Analytique". In 1789, Antoine Lavoisier states the law of conservation of mass. Newton's mechanics received brilliant exposition in both Lagrange's 1788 work and the "Celestial Mechanics" (1799–1825) of Pierre-Simon Laplace.
Thermodynamics.
During the 18th century, thermodynamics was developed through the theories of weightless "imponderable fluids", such as heat ("caloric"), electricity, and phlogiston (which was rapidly overthrown as a concept following Lavoisier's identification of oxygen gas late in the century). Assuming that these concepts were real fluids, their flow could be traced through a mechanical apparatus or chemical reactions. This tradition of experimentation led to the development of new kinds of experimental apparatus, such as the Leyden Jar; and new kinds of measuring instruments, such as the calorimeter, and improved versions of old ones, such as the thermometer. Experiments also produced new concepts, such as the University of Glasgow experimenter Joseph Black's notion of latent heat and Philadelphia intellectual Benjamin Franklin's characterization of electrical fluid as flowing between places of excess and deficit (a concept later reinterpreted in terms of positive and negative charges). Franklin also showed that lightning is electricity in 1752.
The accepted theory of heat in the 18th century viewed it as a kind of fluid, called caloric; although this theory was later shown to be erroneous, a number of scientists adhering to it nevertheless made important discoveries useful in developing the modern theory, including Joseph Black (1728–99) and Henry Cavendish (1731–1810). Opposed to this caloric theory, which had been developed mainly by the chemists, was the less accepted theory dating from Newton's time that heat is due to the motions of the particles of a substance. This mechanical theory gained support in 1798 from the cannon-boring experiments of Count Rumford (Benjamin Thompson), who found a direct relationship between heat and mechanical energy.
While it was recognized early in the 18th century that finding absolute theories of electrostatic and magnetic force akin to Newton's principles of motion would be an important achievement, none were forthcoming. This impossibility only slowly disappeared as experimental practice became more widespread and more refined in the early years of the 19th century in places such as the newly established Royal Institution in London. Meanwhile, the analytical methods of rational mechanics began to be applied to experimental phenomena, most influentially with the French mathematician Joseph Fourier's analytical treatment of the flow of heat, as published in 1822. Joseph Priestley proposed an electrical inverse-square law in 1767, and Charles-Augustin de Coulomb introduced the inverse-square law of electrostatics in 1798.
At the end of the century, the members of the French Academy of Sciences had attained clear dominance in the field. At the same time, the experimental tradition established by Galileo and his followers persisted. The Royal Society and the French Academy of Sciences were major centers for the performance and reporting of experimental work. Experiments in mechanics, optics, magnetism, static electricity, chemistry, and physiology were not clearly distinguished from each other during the 18th century, but significant differences in explanatory schemes and, thus, experiment design were emerging. Chemical experimenters, for instance, defied attempts to enforce a scheme of abstract Newtonian forces onto chemical affiliations, and instead focused on the isolation and classification of chemical substances and reactions.
19th century.
In 1800, Alessandro Volta invented the electric battery (known of the voltaic pile) and thus improved the way electric currents could also be studied. A year later, Thomas Young demonstrated the wave nature of light—which received strong experimental support from the work of Augustin-Jean Fresnel—and the principle of interference. In 1813, Peter Ewart supported the idea of the conservation of energy in his paper "On the measure of moving force". In 1820, Hans Christian Ørsted found that a current-carrying conductor gives rise to a magnetic force surrounding it, and within a week after Ørsted's discovery reached France, André-Marie Ampère discovered that two parallel electric currents will exert forces on each other. In 1821, William Hamilton began his analysis of Hamilton's characteristic function. In 1821, Michael Faraday built an electricity-powered motor, while Georg Ohm stated his law of electrical resistance in 1826, expressing the relationship between voltage, current, and resistance in an electric circuit. A year later, botanist Robert Brown discovered Brownian motion: pollen grains in water undergoing movement resulting from their bombardment by the fast-moving atoms or molecules in the liquid. In 1829, Gaspard Coriolis introduced the terms of work (force times distance) and kinetic energy with the meanings they have today.
In 1831, Faraday (and independently Joseph Henry) discovered the reverse effect, the production of an electric potential or current through magnetism – known as electromagnetic induction; these two discoveries are the basis of the electric motor and the electric generator, respectively. In 1834, Carl Jacobi discovered his uniformly rotating self-gravitating ellipsoids. In 1834, John Russell observed a nondecaying solitary water wave (soliton) in the Union Canal near Edinburgh and used a water tank to study the dependence of solitary water wave velocities on wave amplitude and water depth. In 1835, William Hamilton stated Hamilton's canonical equations of motion. In the same year, Gaspard Coriolis examined theoretically the mechanical efficiency of waterwheels, and deduced the Coriolis effect. In 1841, Julius Robert von Mayer, an amateur scientist, wrote a paper on the conservation of energy but his lack of academic training led to its rejection. In 1842, Christian Doppler proposed the Doppler effect. In 1847, Hermann von Helmholtz formally stated the law of conservation of energy. In 1851, Léon Foucault showed the Earth's rotation with a huge pendulum (Foucault pendulum).
There were important advances in continuum mechanics in the first half of the century, namely formulation of laws of elasticity for solids and discovery of Navier–Stokes equations for fluids.
Laws of thermodynamics.
In the 19th century, the connection between heat and mechanical energy was established quantitatively by Julius Robert von Mayer and James Prescott Joule, who measured the mechanical equivalent of heat in the 1840s. In 1849, Joule published results from his series of experiments (including the paddlewheel experiment) which show that heat is a form of energy, a fact that was accepted in the 1850s. The relation between heat and energy was important for the development of steam engines, and in 1824 the experimental and theoretical work of Sadi Carnot was published. Carnot captured some of the ideas of thermodynamics in his discussion of the efficiency of an idealized engine. Sadi Carnot's work provided a basis for the formulation of the first law of thermodynamics—a restatement of the law of conservation of energy—which was stated around 1850 by William Thomson, later known as Lord Kelvin, and Rudolf Clausius. Lord Kelvin, who had extended the concept of absolute zero from gases to all substances in 1848, drew upon the engineering theory of Lazare Carnot, Sadi Carnot, and Émile Clapeyron–as well as the experimentation of James Prescott Joule on the interchangeability of mechanical, chemical, thermal, and electrical forms of work—to formulate the first law.
Kelvin and Clausius also stated the second law of thermodynamics, which was originally formulated in terms of the fact that heat does not spontaneously flow from a colder body to a hotter. Other formulations followed quickly (for example, the second law was expounded in Thomson and Peter Guthrie Tait's influential work "Treatise on Natural Philosophy") and Kelvin in particular understood some of the law's general implications. The second Law was the idea that gases consist of molecules in motion had been discussed in some detail by Daniel Bernoulli in 1738, but had fallen out of favor, and was revived by Clausius in 1857. In 1850, Hippolyte Fizeau and Léon Foucault measured the speed of light in water and find that it is slower than in air, in support of the wave model of light. In 1852, Joule and Thomson demonstrated that a rapidly expanding gas cools, later named the Joule–Thomson effect or Joule–Kelvin effect. Hermann von Helmholtz puts forward the idea of the heat death of the universe in 1854, the same year that Clausius established the importance of "dQ/T" (Clausius's theorem) (though he did not yet name the quantity).
James Clerk Maxwell.
In 1859, James Clerk Maxwell discovered the distribution law of molecular velocities. Maxwell showed that electric and magnetic fields are propagated outward from their source at a speed equal to that of light and that light is one of several kinds of electromagnetic radiation, differing only in frequency and wavelength from the others. In 1859, Maxwell worked out the mathematics of the distribution of velocities of the molecules of a gas. The wave theory of light was widely accepted by the time of Maxwell's work on the electromagnetic field, and afterward the study of light and that of electricity and magnetism were closely related. In 1864 James Maxwell published his papers on a dynamical theory of the electromagnetic field, and stated that light is an electromagnetic phenomenon in the 1873 publication of Maxwell's "Treatise on Electricity and Magnetism". This work drew upon theoretical work by German theoreticians such as Carl Friedrich Gauss and Wilhelm Weber. The encapsulation of heat in particulate motion, and the addition of electromagnetic forces to Newtonian dynamics established an enormously robust theoretical underpinning to physical observations.
The prediction that light represented a transmission of energy in wave form through a "luminiferous ether", and the seeming confirmation of that prediction with Helmholtz student Heinrich Hertz's 1888 detection of electromagnetic radiation, was a major triumph for physical theory and raised the possibility that even more fundamental theories based on the field could soon be developed. Experimental confirmation of Maxwell's theory was provided by Hertz, who generated and detected electric waves in 1886 and verified their properties, at the same time foreshadowing their application in radio, television, and other devices. In 1887, Heinrich Hertz discovered the photoelectric effect. Research on the electromagnetic waves began soon after, with many scientists and inventors conducting experiments on their properties. In the mid to late 1890s Guglielmo Marconi developed a radio wave based wireless telegraphy system (see invention of radio).
The atomic theory of matter had been proposed again in the early 19th century by the chemist John Dalton and became one of the hypotheses of the kinetic-molecular theory of gases developed by Clausius and James Clerk Maxwell to explain the laws of thermodynamics. The kinetic theory in turn led to the statistical mechanics of Ludwig Boltzmann (1844–1906) and Josiah Willard Gibbs (1839–1903), which held that energy (including heat) was a measure of the speed of particles. Interrelating the statistical likelihood of certain states of organization of these particles with the energy of those states, Clausius reinterpreted the dissipation of energy to be the statistical tendency of molecular configurations to pass toward increasingly likely, increasingly disorganized states (coining the term "entropy" to describe the disorganization of a state). The statistical versus absolute interpretations of the second law of thermodynamics set up a dispute that would last for several decades (producing arguments such as "Maxwell's demon"), and that would not be held to be definitively resolved until the behavior of atoms was firmly established in the early 20th century. In 1902, James Jeans found the length scale required for gravitational perturbations to grow in a static nearly homogeneous medium.
20th century: birth of modern physics.
At the end of the 19th century, physics had evolved to the point at which classical mechanics could cope with highly complex problems involving macroscopic situations; thermodynamics and kinetic theory were well established; geometrical and physical optics could be understood in terms of electromagnetic waves; and the conservation laws for energy and momentum (and mass) were widely accepted. So profound were these and other developments that it was generally accepted that all the important laws of physics had been discovered and that, henceforth, research would be concerned with clearing up minor problems and particularly with improvements of method and measurement. However, around 1900 serious doubts arose about the completeness of the classical theories—the triumph of Maxwell's theories, for example, was undermined by inadequacies that had already begun to appear—and their inability to explain certain physical phenomena, such as the energy distribution in blackbody radiation and the photoelectric effect, while some of the theoretical formulations led to paradoxes when pushed to the limit. Prominent physicists such as Hendrik Lorentz, Emil Cohn, Ernst Wiechert and Wilhelm Wien believed that some modification of Maxwell's equations might provide the basis for all physical laws. These shortcomings of classical physics were never to be resolved and new ideas were required. At the beginning of the 20th century a major revolution shook the world of physics, which led to a new era, generally referred to as modern physics.
Radiation experiments.
In the 19th century, experimenters began to detect unexpected forms of radiation: Wilhelm Röntgen caused a sensation with his discovery of X-rays in 1895; in 1896 Henri Becquerel discovered that certain kinds of matter emit radiation on their own accord. In 1897, J. J. Thomson discovered the electron, and new radioactive elements found by Marie and Pierre Curie raised questions about the supposedly indestructible atom and the nature of matter. Marie and Pierre coined the term "radioactivity" to describe this property of matter, and isolated the radioactive elements radium and polonium. Ernest Rutherford and Frederick Soddy identified two of Becquerel's forms of radiation with electrons and the element helium. Rutherford identified and named two types of radioactivity and in 1911 interpreted experimental evidence as showing that the atom consists of a dense, positively charged nucleus surrounded by negatively charged electrons. Classical theory, however, predicted that this structure should be unstable. Classical theory had also failed to explain successfully two other experimental results that appeared in the late 19th century. One of these was the demonstration by Albert A. Michelson and Edward W. Morley—known as the Michelson–Morley experiment—which showed there did not seem to be a preferred frame of reference, at rest with respect to the hypothetical luminiferous ether, for describing electromagnetic phenomena. Studies of radiation and radioactive decay continued to be a preeminent focus for physical and chemical research through the 1930s, when the discovery of nuclear fission opened the way to the practical exploitation of what came to be called "atomic" energy.
Albert Einstein's theory of relativity.
In 1905 a young, 26-year-old German physicist (then a Bern patent clerk) named Albert Einstein (1879–1955), showed how measurements of time and space are affected by motion between an observer and what is being observed. To say that Einstein's radical theory of relativity revolutionized science is no exaggeration. Although Einstein made many other important contributions to science, the theory of relativity alone represents one of the greatest intellectual achievements of all time. Although the concept of relativity was not introduced by Einstein, his major contribution was the recognition that the speed of light in a vacuum is constant, i.e. the same for all observers, and an absolute physical boundary for motion. This does not impact a person's day-to-day life since most objects travel at speeds much slower than light speed. For objects traveling near light speed, however, the theory of relativity shows that clocks associated with those objects will run more slowly and that the objects shorten in length according to measurements of an observer on Earth. Einstein also derived the famous equation, "E" = "mc"2, which expresses the equivalence of mass and energy.
Special relativity.
Einstein argued that the speed of light was a constant in all inertial reference frames and that electromagnetic laws should remain valid independent of reference frame—assertions which rendered the ether "superfluous" to physical theory, and that held that observations of time and length varied relative to how the observer was moving with respect to the object being measured (what came to be called the "special theory of relativity"). It also followed that mass and energy were interchangeable quantities according to the equation "E"="mc"2. In another paper published the same year, Einstein asserted that electromagnetic radiation was transmitted in discrete quantities ("quanta"), according to a constant that the theoretical physicist Max Planck had posited in 1900 to arrive at an accurate theory for the distribution of blackbody radiation—an assumption that explained the strange properties of the photoelectric effect.
The special theory of relativity is a formulation of the relationship between physical observations and the concepts of space and time. The theory arose out of contradictions between electromagnetism and Newtonian mechanics and had great impact on both those areas. The original historical issue was whether it was meaningful to discuss the electromagnetic wave-carrying "ether" and motion relative to it and also whether one could detect such motion, as was unsuccessfully attempted in the Michelson–Morley experiment. Einstein demolished these questions and the ether concept in his special theory of relativity. However, his basic formulation does not involve detailed electromagnetic theory. It arises out of the question: "What is time?" Newton, in the "Principia" (1686), had given an unambiguous answer: "Absolute, true, and mathematical time, of itself, and from its own nature, flows equably without relation to anything external, and by another name is called duration." This definition is basic to all classical physics.
Einstein had the genius to question it, and found that it was incomplete. Instead, each "observer" necessarily makes use of his or her own scale of time, and for two observers in relative motion, their time-scales will differ. This induces a related effect on position measurements. Space and time become intertwined concepts, fundamentally dependent on the observer. Each observer presides over his or her own space-time framework or coordinate system. There being no absolute frame of reference, all observers of given events make different but equally valid (and reconcilable) measurements. What remains absolute is stated in Einstein's relativity postulate: "The basic laws of physics are identical for two observers who have a constant relative velocity with respect to each other."
Special Relativity had a profound effect on physics: started as a rethinking of the theory of electromagnetism, it found a new symmetry law of nature, now called "Poincaré symmetry", that replaced the old Galilean (see above) symmetry.
Special Relativity exerted another long-lasting effect on dynamics. Although initially it was credited with the "unification of mass and energy", it became evident that relativistic dynamics established a firm "distinction" between rest mass, which is an invariant (observer independent) property of a particle or system of particles, and the energy and momentum of a system. The latter two are separately conserved in all situations but not invariant with respect to different observers. The term "mass" in particle physics underwent a semantic change, and since the late 20th century it almost exclusively denotes the rest (or "invariant") mass. See mass in special relativity for additional discussion.
General relativity.
By 1916, Einstein was able to generalize this further, to deal with all states of motion including non-uniform acceleration, which became the general theory of relativity. In this theory Einstein also specified a new concept, the curvature of space-time, which described the gravitational effect at every point in space. In fact, the curvature of space-time completely replaced Newton's universal law of gravitation. According to Einstein, gravitational force in the normal sense is a kind of illusion caused by the geometry of space. The presence of a mass causes a curvature of space-time in the vicinity of the mass, and this curvature dictates the space-time path that all freely-moving objects must follow. It was also predicted from this theory that light should be subject to gravity - all of which was verified experimentally. This aspect of relativity explained the phenomena of light bending around the sun, predicted black holes as well as properties of the Cosmic microwave background radiation — a discovery rendering fundamental anomalies in the classic Steady-State hypothesis. For his work on relativity, the photoelectric effect and blackbody radiation, Einstein received the Nobel Prize in 1921.
The gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the "general theory of relativity") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.
Quantum mechanics.
Although relativity resolved the electromagnetic phenomena conflict demonstrated by Michelson and Morley, a second theoretical problem was the explanation of the distribution of electromagnetic radiation emitted by a black body; experiment showed that at shorter wavelengths, toward the ultraviolet end of the spectrum, the energy approached zero, but classical theory predicted it should become infinite. This glaring discrepancy, known as the ultraviolet catastrophe, was solved by the new theory of quantum mechanics. Quantum mechanics is the theory of atoms and subatomic systems. Approximately the first 30 years of the 20th century represent the time of the conception and evolution of the theory. The basic ideas of quantum theory were introduced in 1900 by Max Planck (1858–1947), who was awarded the Nobel Prize for Physics in 1918 for his discovery of the quantified nature of energy. The quantum theory (which previously relied in the "correspondence" at large scales between the quantized world of the atom and the continuities of the "classical" world) was accepted when the Compton Effect established that light carries momentum and can scatter off particles, and when Louis de Broglie asserted that matter can be seen as behaving as a wave in much the same way as electromagnetic waves behave like particles (wave–particle duality).
In 1905, Einstein used the quantum theory to explain the photoelectric effect, and in 1913 the Danish physicist Niels Bohr used the same constant to explain the stability of Rutherford's atom as well as the frequencies of light emitted by hydrogen gas. The quantized theory of the atom gave way to a full-scale quantum mechanics in the 1920s. New principles of a "quantum" rather than a "classical" mechanics, formulated in matrix-form by Werner Heisenberg, Max Born, and Pascual Jordan in 1925, were based on the probabilistic relationship between discrete "states" and denied the possibility of causality. Quantum mechanics was extensively developed by Heisenberg, Wolfgang Pauli, Paul Dirac, and Erwin Schrödinger, who established an equivalent theory based on waves in 1926; but Heisenberg's 1927 "uncertainty principle" (indicating the impossibility of precisely and simultaneously measuring position and momentum) and the "Copenhagen interpretation" of quantum mechanics (named after Bohr's home city) continued to deny the possibility of fundamental causality, though opponents such as Einstein would metaphorically assert that "God does not play dice with the universe". The new quantum mechanics became an indispensable tool in the investigation and explanation of phenomena at the atomic level. Also in the 1920s, the Indian scientist Satyendra Nath Bose's work on photons and quantum mechanics provided the foundation for Bose–Einstein statistics, the theory of the Bose–Einstein condensate.
 The spin–statistics theorem established that any particle in quantum mechanics may be either a boson (statistically Bose–Einstein) or a fermion (statistically Fermi–Dirac). It was later found that all fundamental bosons transmit forces, such as the photon that transmits light.
Fermions are particles "like electrons and nucleons" and are the usual constituents of matter. Fermi–Dirac statistics later found numerous other uses, from astrophysics (see Degenerate matter) to semiconductor design.
Contemporary and particle physics.
Quantum field theory.
As the philosophically inclined continued to debate the fundamental nature of the universe, quantum theories continued to be produced, beginning with Paul Dirac's formulation of a relativistic quantum theory in 1928. However, attempts to quantize electromagnetic theory entirely were stymied throughout the 1930s by theoretical formulations yielding infinite energies. This situation was not considered adequately resolved until after World War II ended, when Julian Schwinger, Richard Feynman and Sin-Itiro Tomonaga independently posited the technique of renormalization, which allowed for an establishment of a robust quantum electrodynamics (QED).
Meanwhile, new theories of fundamental particles proliferated with the rise of the idea of the quantization of fields through "exchange forces" regulated by an exchange of short-lived "virtual" particles, which were allowed to exist according to the laws governing the uncertainties inherent in the quantum world. Notably, Hideki Yukawa proposed that the positive charges of the nucleus were kept together courtesy of a powerful but short-range force mediated by a particle with a mass between that of the electron and proton. This particle, the "pion", was identified in 1947 as part of what became a slew of particles discovered after World War II. Initially, such particles were found as ionizing radiation left by cosmic rays, but increasingly came to be produced in newer and more powerful particle accelerators.
Outside particle physics, significant advances of the time were:
Unified field theories.
Einstein deemed that all fundamental interactions in nature can be explained in a single theory. Unified field theories were numerous attempts to "merge" several interactions. One of formulations of such theories (as well as field theories in general) is a "gauge theory", a generalization of the idea of symmetry. Eventually the Standard Model (see below) succeeded in unification of strong, weak, and electromagnetic interactions. All attempts to unify gravitation with something else failed.
Standard Model.
The interaction of these particles by scattering and decay provided a key to new fundamental quantum theories. Murray Gell-Mann and Yuval Ne'eman brought some order to these new particles by classifying them according to certain qualities, beginning with what Gell-Mann referred to as the "Eightfold Way". While its further development, the quark model, at first seemed inadequate to describe strong nuclear forces, allowing the temporary rise of competing theories such as the S-Matrix, the establishment of quantum chromodynamics in the 1970s finalized a set of fundamental and exchange particles, which allowed for the establishment of a "standard model" based on the mathematics of gauge invariance, which successfully described all forces except for gravitation, and which remains generally accepted within its domain of application.
The Standard Model groups the electroweak interaction theory and quantum chromodynamics into a structure denoted by the gauge group SU(3)×SU(2)×U(1). The formulation of the unification of the electromagnetic and weak interactions in the standard model is due to Abdus Salam, Steven Weinberg and, subsequently, Sheldon Glashow. Electroweak theory was later confirmed experimentally (by observation of neutral weak currents), and distinguished by the 1979 Nobel Prize in Physics.
Since the 1970s, fundamental particle physics has provided insights into early universe cosmology, particularly the Big Bang theory proposed as a consequence of Einstein's general theory of relativity. However, starting in the 1990s, astronomical observations have also provided new challenges, such as the need for new explanations of galactic stability ("dark matter") and the apparent acceleration in the expansion of the universe ("dark energy").
While accelerators have confirmed most aspects of the Standard Model by detecting expected particle interactions at various collision energies, no theory reconciling general relativity with the Standard Model has yet been found, although supersymmetry and string theory were believed by many theorists to be a promising avenue forward. The Large Hadron Collider, however, which began operating in 2008, has failed to find any evidence whatsoever that is supportive of supersymmetry and string theory.
Cosmology.
Cosmology may be said to have become a serious research question with the publication of Einstein's General Theory of Relativity in 1916 [1915?] although it did not enter the scientific mainstream until the period known as the "Golden age of general relativity".
About a decade later, in the midst of what was dubbed the "Great Debate", Hubble and Slipher discovered the expansion of universe in the 1920s measuring the redshifts of Doppler spectra from galactic nebulae. Using Einstein's general relativity, Lemaître and Gamow formulated what would become known as the big bang theory. A rival, called the steady state theory was devised by Hoyle, Gold, Narlikar and Bondi.
Cosmic background radiation was verified in the 1960s by Penzias and Wilson, and this discovery favoured the big bang at the expense of the steady state scenario. Later work was by Smoot et al. (1989), among other contributors, using data from the Cosmic Background explorer (CoBE) and the Wilkinson Microwave Anistropy Probe (WMAP) satellites that refined these observations. The 1980s (the same decade of the COBE measurements) also saw the proposal of inflation theory by Guth.
Recently the problems of dark matter and dark energy have risen to the top of the cosmology agenda.
Higgs boson.
On July 4, 2012, physicists working at CERN's Large Hadron Collider announced that they had discovered a new subatomic particle greatly resembling the Higgs boson, a potential key to an understanding of why elementary particles have mass and indeed to the existence of diversity and life in the universe. For now, some physicists are calling it a "Higgslike" particle. Joe Incandela, of the University of California, Santa Barbara, said, "It's something that may, in the end, be one of the biggest observations of any new phenomena in our field in the last 30 or 40 years, going way back to the discovery of quarks, for example." Michael Turner, a cosmologist at the University of Chicago and the chairman of the physics center board, said:
"This is a big moment for particle physics and a crossroads — will this be the high water mark or will it be the first of many discoveries that point us toward solving the really big questions that we have posed?"—Michael Turner, University of Chicago
Peter Higgs was one of six physicists, working in three independent groups, who, in 1964, invented the notion of the Higgs field ("cosmic molasses"). The others were Tom Kibble of Imperial College, London; Carl Hagen of the University of Rochester; Gerald Guralnik of Brown University; and François Englert and Robert Brout, both of Université libre de Bruxelles.
Although they have never been seen, Higgslike fields play an important role in theories of the universe and in string theory. Under certain conditions, according to the strange accounting of Einsteinian physics, they can become suffused with energy that exerts an antigravitational force. Such fields have been proposed as the source of an enormous burst of expansion, known as inflation, early in the universe and, possibly, as the secret of the dark energy that now seems to be speeding up the expansion of the universe.
Physical sciences.
With increased accessibility to and elaboration upon advanced analytical techniques in the 19th century, physics was defined as much, if not more, by those techniques than by the search for universal principles of motion and energy, and the fundamental nature of matter. Fields such as acoustics, geophysics, astrophysics, aerodynamics, plasma physics, low-temperature physics, and solid-state physics joined optics, fluid dynamics, electromagnetism, and mechanics as areas of physical research. In the 20th century, physics also became closely allied with such fields as electrical, aerospace and materials engineering, and physicists began to work in government and industrial laboratories as much as in academic settings. Following World War II, the population of physicists increased dramatically, and came to be centered on the United States, while, in more recent decades, physics has become a more international pursuit than at any time in its previous history.
Influential physicists.
The following is a gallery of highly influential and important figures in the history of physics. For a list that includes even more people, see list of physicists. 
Sources.
</dl>

</doc>
<doc id="13761" url="http://en.wikipedia.org/wiki?curid=13761" title="Hydrofoil">
Hydrofoil

A hydrofoil is a lifting surface, or foil, which operates in water. They are similar in appearance and purpose to airfoils used by airplanes. Boats using hydrofoil technology are also simply termed hydrofoils. As speed is gained, hydrofoils lift the boat's hull out of the water, decreasing drag and thus allowing greater speeds.
Description.
The hydrofoil usually consists of a wing-like structure mounted on struts below the hull, or across the keels of a catamaran in a variety of boats (see illustration). As a hydrofoil-equipped watercraft increases in speed, the hydrofoil elements below the hull(s) develop enough lift to raise the hull out of the water in order to greatly reduce hull drag. This gives a further corresponding increase in speed and efficiency of operation in terms of fuel consumption.
A wider adoption of the technical innovations of hydrofoils is prevented by the increased complexity of building and maintaining them. Hydrofoils are generally prohibitively more expensive than conventional watercraft. However, the design is simple enough that there are many human-powered hydrofoil designs. Amateur experimentation and development of the concept is popular.
Hydrodynamic mechanics.
Since air and water are governed by similar fluid equations, albeit with vastly different levels of viscosity, density, and compressibility, the hydrofoil and airfoil (both types of foil) create lift in identical ways. The foil is shaped to move smoothly through the water causing the flow to be deflected downward which according to Newton's Third Law of Motion exerts an upward force on the foil. This turning of the water causes higher pressure on the bottom and reduced pressure on the top of the foil. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the foil has a higher average velocity on one side than the other.
When used as a lifting element on a hydrofoil boat, this upward force lifts the body of the vessel, decreasing drag and increasing speed. The lifting force eventually balances with the weight of the craft, reaching a point where the hydrofoil no longer lifts out of the water, but remains in equilibrium. Since wave resistance and other impeding forces such as various types of drag (physics) on the hull are eliminated as the hull is lifted clear, turbulence and drag act increasingly on the much smaller surface area of the hydrofoil, and decreasingly on the hull, creating a marked increase in speed.
Foil configurations.
Early hydrofoils used V-shaped foils. Hydrofoils of this type are known as "surface-piercing" since portions of the V-shape hydrofoils will rise above the water surface when foilborne. Some modern hydrofoils use inverted T-shape foils which are fully submerged. Fully submerged hydrofoils are less subject to the effects of wave action, and are therefore more stable at sea and are more comfortable for the crew and passengers. This type of configuration, however, is not self-stabilizing. The angle of attack on the hydrofoils needs to be adjusted continuously in accordance to the changing conditions, a control process that is performed by sensors, computer and active surfaces.
History.
Prototypes.
Italian inventor Enrico Forlanini began working on hydrofoils in 1898 and used a "ladder" foils system. Forlanini obtained patents in Britain and the United States for his ideas and designs.
Between 1899 and 1901, British boat designer John Thornycroft worked on a series of models with a stepped hull and single bow foil. In 1909 his company built the full scale 22 ft long boat, "Miranda III". Driven by a 60 hp engine, it rode on a bowfoil and flat stern. The subsequent "Miranda IV" was credited with a speed of 35 kn.
A March 1906 Scientific American article by American hydrofoil pioneer William E. Meacham explained the basic principle of hydrofoils. Alexander Graham Bell considered the invention of the hydroplane a very significant achievement, and after reading the article began to sketch concepts of what is now called a hydrofoil boat. With his chief engineer Casey Baldwin, Bell began hydrofoil experiments in the summer of 1908. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models based on his designs, which led them to the development of hydrofoil watercraft. During Bell's world tour of 1910–1911, Bell and Baldwin met with Forlanini in Italy, where they rode in his hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying. On returning to Bell's large laboratory at his Beinn Bhreagh estate near Baddeck, Nova Scotia, they experimented with a number of designs, culminating in Bell's "HD-4". Using Renault engines, a top speed of 87 km/h (54 mph) was achieved, accelerating rapidly, taking waves without difficulty, steering well and showing good stability. Bell's report to the United States Navy permitted him to obtain two 260 kW (350 horsepower) engines. On 9 September 1919 the "HD-4" set a world marine speed record of 114 km/h (70.86 mph), a record which stood for two decades. A full-scale replica of the "HD-4" is viewable at the Alexander Graham Bell National Historic Site museum in Baddeck, Nova Scotia.
In the early 1950s an English couple built the "White Hawk", a jet-powered hydrofoil water craft, in an attempt to beat the absolute water speed record. However, in tests, "White Hawk" could barely top the record breaking speed of the 1919 "HD-4". The designers had faced an engineering phenomenon that limits the top speed of even modern hydrofoils: cavitation disturbs the lift created by the foils as they move through the water at speed above 70 mph, bending the lifting foil.
First passenger boats.
German engineer Hanns von Schertel worked on hydrofoils prior to and during World War II in Germany. After the war Schertel's team was captured by the Russians. As Germany was not authorized to build fast boats, Schertel himself went to Switzerland, where he established the Supramar company. In 1952, Supramar launched the first commercial hydrofoil, PT10 "Freccia d'Oro" (Golden Arrow), in Lake Maggiore, between Switzerland and Italy. The PT10 is of surface-piercing type, it can carry 32 passengers and travel at 35 kn. In 1968, Hussain Najadi the Bahraini born banker, acquired the Supramar AG and expanded its operations into Japan, Hong Kong, Singapore, the UK, Norway and the US. General Dynamics of the United States became its licensee, and the Pentagon awarded its first R&D naval research project in the field of supercavitation. Hitachi Shipbuilding of Osaka, Japan, was another licensee of Supramar, as well as many leading ship owners and shipyards in the OECD countries.
From 1952 to 1971, Supramar designed many models of hydrofoils: PT20, PT50, PT75, PT100 and PT150. All are of surface-piercing type, except the PT150 combining a surface-piercing foil forward with a fully submerged foil in the aft location. Over 200 of Supramar's design were built, most of them by Rodriquez in Italy.
During the same period the Soviet Union experimented extensively with hydrofoils, constructing hydrofoil river boats and ferries with streamlined designs during the cold war period and into the 1980s. Such vessels include the Raketa (1957) type, followed by the larger Meteor type and the smaller Voskhod type. One of the most successful Soviet designer/inventor in this area was Rostislav Alexeyev who some consider the 'father' of the modern hydrofoil due to his 1950's era high speed hydrofoil designs. Later, circa 1970's, Alexeyev combined his hydrofoil experience with the surface effect principle to create the Ekranoplan.
In 1961, SRI International issued a study on "The Economic Feasibility of Passenger Hydrofoil Craft in U.S. Domestic and Foreign Commerce". Commercial use of hydrofoils in the U.S. first appeared in 1961 when two commuter vessels were commissioned by Harry Gale Nye, Jr.'s North American Hydrofoils to service the route from Atlantic Highlands, New Jersey to the financial district of Lower Manhattan.
Military usage.
In Canada during World War II, Baldwin worked on an experimental smoke laying hydrofoil (later called the Comox Torpedo) that was later superseded by other smoke-laying technology and an experimental target-towing hydrofoil. The forward two foil assemblies of what is believed to be the latter hydrofoil were salvaged in the mid-1960s from a derelict hulk in Baddeck, Nova Scotia by Colin MacGregor Stevens. These were donated to the Maritime Museum in Halifax, Nova Scotia. The Canadian Armed Forces built and tested a number of hydrofoils (e.g. Baddeck and two vessels named Bras d'Or), which culminated in the high-speed anti-submarine hydrofoil HMCS Bras d'Or in the late 1960s. However, the program was cancelled in the early 1970s due to a shift away from anti-submarine warfare by the Canadian military. The Bras d'Or was a surface-piercing type that performed well during her trials, reaching a maximum speed of 63 kn.
The USSR introduced several hydrofoil-based fast attack craft into their navy, principally:
The U.S. Navy began experiments with hydrofoils in the mid-1950s by funding a sailing vessel that used hydrofoils to reach speeds in the 30 mph range. The "XCH-4" (officially, "Experimental Craft, Hydrofoil No. 4"), designed by William P. Carl, exceeded speeds of 65 mph and was mistaken for a seaplane due to its shape. The US Navy implemented a small number of combat hydrofoils, such as the "Pegasus" class, from 1977 through 1993. These hydrofoils were fast and well armed, and were capable of sinking all but the largest surface vessels.
The Italian Navy has used six hydrofoils of the "Sparviero" class since the late 1970s. These were armed with a 76 mm gun and two missiles, and were capable of speeds up to 50 kn. Three similar boats were built for the Japan Maritime Self-Defense Force.
Sailing and sports.
The French experimental sail powered hydrofoil "Hydroptère" is the result of a research project that involves advanced engineering skills and technologies. In September 2009, the "Hydroptère" set new sailcraft world speed records in the 500 m category, with a speed of 51.36 knot and in the one nautical mile (1.9 km) category with a speed of 50.17 knot.
Another trimaran sailboat is the Windrider Rave. The Rave is a commercially available 17 ft, two person, hydrofoil trimaran, capable of reaching speeds of 40 kn. The boat was designed by Jim Brown.
The Moth dinghy has evolved into some radical foil configurations.
Hobie Sailboats produced a production foiling trimaran, the Hobie Trifoiler, the fastest production sailboat. Trifoilers have clocked speeds upward of thirty knots.
A new kayak design, called Flyak, has hydrofoils that lift the kayak enough to significantly reduce drag, allowing speeds of up to 27 km/h. Some surfers have developed surfboards with hydrofoils called foilboards, specifically aimed at surfing big waves further out to sea.
Modern passenger boats.
Soviet-built Voskhods are one of the most successful passenger hydrofoil designs. Manufactured in Russia and Ukraine, they are in service in more than 20 countries. The most recent model, Voskhod-2M FFF, also known as Eurofoil, was built in Feodosiya, Ukraine for the Dutch public transport operator Connexxion.
The Boeing 929 is widely used in Asia for passenger services between the many islands of Japan, between Hong Kong and Macau and on the Korean peninsula.
Current operation.
Current operators of hydrofoils include:
Disadvantages.
Hydrofoils had their peak in popularity in the 1960s and 70s. Since then there has been a steady decline in their use and popularity for leisure, military and commercial passenger transport use. There are a number of reasons for this:

</doc>
<doc id="13763" url="http://en.wikipedia.org/wiki?curid=13763" title="Henri Chopin">
Henri Chopin

Henri Chopin (18 June 1922 – 3 January 2008) was an avant-garde poet and musician.
Life.
Henri Chopin was born in Paris,18 June 1922, one of three brothers, and the son of an accountant. Both his siblings died during the war. One was shot by a German soldier the day after an armistice was declared in Paris, the other while sabotaging a train (Acquaviva 2008).
Chopin was a French practitioner of concrete and sound poetry, well known throughout the second half of the 20th century. His work, though iconoclastic, remained well within the historical spectrum of poetry as it moved from a spoken tradition to the printed word and now back to the spoken word again (Wendt 1996, 112). He created a large body of pioneering recordings using early tape recorders, studio technologies and the sounds of the manipulated human voice. His emphasis on sound is a reminder that language stems as much from oral traditions as from classic literature, of the relationship of balance between order and chaos.
Chopin is significant above all for his diverse spread of creative achievement, as well as for his position as a focal point of contact for the international arts. As poet, painter, graphic artist and designer, typographer, independent publisher, filmmaker, broadcaster and arts promoter, Chopin's work is a barometer of the shifts in European media between the 1950s and the 1970s. 
In 1964 he created "OU", one of the most notable reviews of the second half of the 20th century, and he ran it until 1974. "OU"'s contributors included William S. Burroughs, Brion Gysin, Gil J Wolman, François Dufrêne, Bernard Heidsieck, John Furnival, Tom Phillips, and the Austrian sculptor, writer and Dada pioneer Raoul Hausmann.
His books included "Le Dernier Roman du Monde" (1971), "Portrait des 9" (1975), "The Cosmographical Lobster" (1976), "Poésie Sonore Internationale" (1979), "Les Riches Heures de l'Alphabet" (1992) and "Graphpoemesmachine" (2006). Henri also created many graphic works on his typewriter: the typewriter poems (also known as dactylopoèmes) feature in international art collections such as those of Francesco Conz in Verona, the Morra Foundation in Naples and Ruth and Marvin Sackner in Miami, and have been the subject of Australian, British and French retrospectives (Aquaviva 2008).
His publication and design of the classic audio-visual magazines "Cinquième Saison" and "OU" between 1958 and 1974, each issue containing recordings as well as texts, images, screenprints and multiples, brought together international contemporary writers and artists such as members of Lettrisme and Fluxus, Jiri Kolar, Ian Hamilton Finlay, Tom Phillips, Brion Gysin, William S. Burroughs and many others, as well as bringing the work of survivors from earlier generations such as Raoul Hausmann and Marcel Janco to a fresh audience.
From 1968 to 1986 Henri Chopin lived in Ingatestone, Essex, but with the death of his wife Jean in 1985, he moved back to France.
In 2001 with his health failing, he returned to England, living with his daughter and family at Dereham, Norfolk (Acquaviva 2008).
Aesthetics.
Chopin's "poesie sonore" aesthetics included a deliberate cultivation of a "barbarian" approach in production, using raw or crude sound manipulations to explore the area between
distortion and intelligibility. He avoided high-quality, professional recording machines, preferring to use very basic equipment and "bricolage" methods, such as sticking matchsticks in the erase heads of a second-hand tape recorder, or manually interfering with the tape path (Wendt 1985, 16–17).

</doc>
<doc id="13764" url="http://en.wikipedia.org/wiki?curid=13764" title="Hassium">
Hassium

Hassium is a chemical element with symbol Hs and atomic number 108, named after the German state of Hesse. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, 269Hs, has a half-life of approximately 9.7 seconds, although an unconfirmed metastable state, 277mHs, may have a longer half-life of about 11 minutes. More than 100 atoms of hassium have been synthesized to date.
In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 8 elements. Chemistry experiments have confirmed that hassium behaves as the heavier homologue to osmium in group 8. The chemical properties of hassium are characterized only partly, but they compare well with the chemistry of the other group 8 elements. In bulk quantities, hassium is expected to be a silvery metal that reacts readily with oxygen in the air, forming a volatile tetroxide.
History.
The synthesis of element 108 was first attempted in 1978 by a Russian research team led by Yuri Oganessian and Vladimir Utyonkov at the Joint Institute for Nuclear Research (JINR) in Dubna, using reactions that would generate the isotopes hassium-270 and hassium-264. The data was uncertain and they carried out new experiments on hassium five years later, where these two isotopes as well as hassium-263 were produced; the hassium-264 experiment was repeated again and confirmed in 1984.
Hassium was officially discovered in 1984 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of lead-208 with accelerated nuclei of iron-58 to produce 3 atoms of the isotope hassium-265.
Due to this issue, a controversy arose over who should be recognized as the official discoverer of the element. The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report. They stated that the GSI collaboration was "more detailed and, of itself, carries conviction", and that while the combined data from Dubna and Darmstadt confirmed that hassium had been synthesized, the major credit was awarded to the GSI. This statement came in spite of the combined data also supporting the Russian 1983 discovery claim and the TWG also acknowledging that "very probably element 108 played a role in the Dubna experiment."
The name "hassium" was proposed by Peter Armbruster and his colleagues, the officially recognised German discoverers, in 1992, derived from the Latin name ("Hassia") for the German state of Hesse where the institute is located. Using Mendeleev's nomenclature for unnamed and undiscovered elements, hassium should be known as "eka-osmium" or "dvi-ruthenium". In 1979, during the Transfermium Wars (but before the synthesis of hassium), IUPAC published recommendations according to which the element was to be called "unniloctium" (with the corresponding symbol of "Uno"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. The recommendations were mostly ignored among scientists, who either called it "element 108", with the symbol of "(108)" or even simply "108", or used the proposed name "hassium".
In 1994 a committee of IUPAC recommended that element 108 be named "hahnium" (Hn) after the German physicist Otto Hahn, after an older suggestion of "ottohahnium" (Oh) in spite of the long-standing convention to give the discoverer the right to suggest a name, so that elements named after Hahn and Lise Meitner (meitnerium) would be next to each other, honoring their joint discovery of nuclear fission. This was because they felt that Hesse did not merit an element being named after it. After protests from the German discoverers and the American Chemical Society, IUPAC relented and the name "hassium" (Hs) was adopted internationally in 1997.
Natural occurrence.
Hassium is not known to occur naturally on Earth; the half-lives of all its known isotopes are short enough that no primordial hassium would have survived to the present day. However, this does not rule out the possibility of as yet unknown longer-lived isotopes or nuclear isomers existing, some of which could, if long-lived enough, still exist in trace quantities today. In the early 1960s, it was predicted that long-lived deformed isomers of hassium might occur naturally on Earth in trace quantities. This was theorized in order to explain the extreme radiation damage in some minerals that could not have been caused by any known natural radioisotopes, but could have been caused by superheavy elements.
In 1963, Soviet scientist Victor Cherdyntsev, who had previously claimed the existence of primordial curium-247, claimed to have discovered element 108 (specifically, the 267Hs isotope, which supposedly had a half life of 400 to 500 million years) in natural molybdenite and suggested the name "sergenium" (symbol Sg; at the time, this symbol had not yet been taken by seaborgium) for it, after the ancient city of Serik along the Silk Road in Kazakhstan where his molybdenite samples came from. His rationale for claiming that sergenium was the heavier homologue to osmium was that minerals supposedly containing sergenium formed volatile oxides when boiled in nitric acid, similarly to osmium. His findings were however criticized by V. M. Kulakov on the grounds that some of the properties Cherdyntsev claimed sergenium had were inconsistent with the then-current nuclear physics.
The chief questions raised by Kulakov were that the claimed alpha decay energy of sergenium was many orders of magnitude lower than expected and the half-life given was eight orders of magnitude shorter than what would be predicted for a nuclide alpha decaying with the claimed decay energy, but at the same time a corrected half-life in the region of 1016 years would be impossible as it would imply that the samples contained about 100 milligrams of sergenium. However, more recently, it has been suggested that the observed alpha decay with energy 4.5 MeV could be due to a low-energy and strongly enhanced transition between different hyperdeformed states of a hassium isotope around 271Hs, thus suggesting that the existence of superheavy elements in nature was at least possible, although unlikely.
In 2004, the Joint Institute for Nuclear Research conducted a search for natural hassium. This was done underground to avoid interference and false positives from cosmic rays, but no results have been released, strongly implying that no natural hassium was found. The possible extent of primordial hassium on Earth is uncertain; it might now only exist in traces, or could even have completely decayed by now after having caused the radiation damage long ago.
In 2006, it was hypothesized that an isomer of 271Hs might have a half-life of around (2.5±0.5)×108 y, which would explain the observation of alpha particles with energies of around 4.4 MeV in some samples of molybdenite and osmiride. This isomer of 271Hs could be produced from the beta decay of 271Bh and 271Sg, which, being homologous to rhenium and molybdenum respectively, should occur in molybdenite along with rhenium and molybdenum if they occurred in nature. Since hassium is homologous to osmium, it should also occur along with osmium in osmiride if it occurred in nature. However, the decay chains of these isotopes are very hypothetical and the predicted half-life of this hypothetical hassium isomer is not long enough for any sufficient quantity to remain on Earth. It is possible that more 271Hs may be deposited on the Earth as the Solar System travels through the spiral arms of the Milky Way, which would also explain excesses of plutonium-239 found on the floors of the Pacific Ocean and the Gulf of Finland, but minerals enriched with 271Hs are predicted to also have excesses of uranium-235 and lead-207, and would have different proportions of elements that are formed during spontaneous fission, such as krypton, zirconium, and xenon. Thus, the occurrence of hassium in nature in minerals such as molybdenite and osmiride is theoretically possible, but highly unlikely.
Isotopes.
Hassium has no stable or naturally-occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes have been reported with atomic masses from 263 to 277 (with the exceptions of 272, 274, and 276), four of which, hassium-265, hassium-267, hassium-269, and hassium-277, have known metastable states (although that of hassium-277 is unconfirmed). Most of these decay predominantly through alpha decay, but some also undergo spontaneous fission.
The lighter isotopes usually have shorter half-lives; half-lives of under 1 ms for 263Hs, 264Hs, and 265mHs were observed. 265Hs and 266Hs are more stable at around 2 ms, 267Hs has a half-life of about 50 ms, 267mHs, 268Hs, 273Hs, and 275Hs live between 0.1 and 1 second, and 269Hs, 269mHs, 270Hs, 271Hs, and 277Hs are more stable, at between 1 and 30 seconds. The unconfirmed 277mHs may have a half-life of about 11 minutes. The unknown isotopes 274Hs and 276Hs are predicted to have longer half-lives of around 1 minute and 1 hour respectively. Before its discovery, 271Hs was also predicted to have a long half-life of 40 seconds, but it was found to have a shorter half-life of only about 4 seconds.
The lightest isotopes were synthesized by direct fusion between two lighter nuclei and as decay products. The heaviest isotope produced by direct fusion is 271Hs; heavier isotopes have only been observed as decay products of elements with larger atomic numbers. In 1999, American scientists at the University of California, Berkeley, announced that they had succeeded in synthesizing three atoms of 293118. These parent nuclei were reported to have successively emitted three alpha particles to form hassium-273 nuclei, which were claimed to have undergone an alpha decay, emitting alpha particles with decay energies of 9.78 and 9.47 MeV and half-life 1.2 s, but their claim was retracted in 2001. The isotope, however, was produced in 2010 by the same team. The new data matched the previous (fabricated) data.
270Hs: prospects for a deformed doubly magic nucleus.
According to calculations, 108 is a proton magic number for deformed nuclei (nuclei that are far from spherical), and 162 is a neutron magic number for deformed nuclei. This means that such nuclei are permanently deformed in their ground state but have high, narrow fission barriers to further deformation and hence relatively long life-times to spontaneous fission. The spontaneous fission half-lives in this region are typically reduced by a factor of 109 in comparison with those in the vicinity of the spherical doubly magic nucleus 298Fl, caused by the narrower fission barrier for such deformed nuclei. Hence, the nucleus 270Hs has promise as a deformed doubly magic nucleus. Experimental data from the decay of the darmstadtium (Z=110) isotopes 271Ds and 273Ds provides strong evidence for the magic nature of the N=162 sub-shell. The recent synthesis of 269Hs, 270Hs, and 271Hs also fully support the assignment of N=162 as a magic number. In particular, the low decay energy for 270Hs is in complete agreement with calculations.
Evidence for the magicity of the Z=108 proton shell can be obtained from two sources: the variation in the partial spontaneous fission half-lives for isotones and the large gap in the alpha Q value for isotonic nuclei of hassium and darmstadtium. For spontaneous fission, it is necessary to measure the half-lives for the isotonic nuclei 268Sg, 270Hs and 272Ds. Since the isotopes 268Sg and 272Ds are not currently known, and fission of 270Hs has not been measured, this method cannot be used to date to confirm the stabilizing nature of the Z=108 shell. However, good evidence for the magicity of the Z=108 shell can be deemed from the large differences in the alpha decay energies measured for 270Hs, 271Ds and 273Ds. More conclusive evidence would come from the determination of the decay energy for the unknown nucleus 272Ds.
Predicted properties.
Various calculations show that hassium should be the heaviest known group 8 element, consistent with the periodic law. Its properties should generally match those expected for a heavier homologue of osmium, with a few deviations arising from relativistic effects.
Physical and atomic.
The previous members of group 8 have relatively high melting points (Fe, 1538 °C; Ru, 2334 °C; Os, 3033 °C). Much like them, hassium is predicted to be a solid at room temperature, although the melting point of hassium has not been precisely calculated. Hassium should crystallize in the hexagonal close-packed structure ("c"/"a" = 1.59), similarly to its lighter congener osmium. Pure metallic hassium is calculated to have a bulk modulus (resistance to uniform compression) comparable to that of diamond (442 GPa). Hassium is expected to have a bulk density of 40.7 g/cm3, the highest of any of the 118 known elements and nearly twice the density of osmium, the most dense measured element, at 22.61 g/cm3. This results from hassium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough hassium to measure this quantity would be impractical, and the sample would quickly decay. Osmium is the densest element of the first 6 periods, and its heavier congener hassium is expected to be the densest element of the first 7 periods.
The atomic radius of hassium is expected to be around 126 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Hs+ ion is predicted to have an electron configuration of [Rn] 5f14 6d5 7s2, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues. On the other hand, the Hs2+ ion is expected to have an electron configuration of [Rn] 5f14 6d5 7s1, analogous to that calculated for the Os2+ ion.
Chemical.
Hassium is the sixth member of the 6d series of transition metals and is expected to be much like the platinum group metals. Calculations on its ionization potentials, atomic radius, as well as radii, orbital energies, and ground levels of its ionized states are similar to that of osmium, implying that hassium's properties would resemble those of the other group 8 elements, iron, ruthenium, and osmium. Some of these properties were confirmed by gas-phase chemistry experiments. The group 8 elements portray a wide variety of oxidation states, but ruthenium and osmium readily portray their group oxidation state of +8 (the highest known oxidation state for any element, which is very rare for other elements) and this state becomes more stable as the group is descended. Thus hassium is expected to form a stable +8 state. Analogously to its lighter congeners, hassium is expected to also show other stable lower oxidation states, such as +6, +5, +4, +3, and +2.
The group 8 elements show a very distinctive oxide chemistry which allows extrapolations to be made easily for hassium. All the lighter members have known or hypothetical tetroxides, MO4. Their oxidising power decreases as one descends the group. FeO4 is not known due to its extraordinarily large electron affinity (the amount of energy released when an electron is added to a neutral atom or molecule to form a negative ion) which results in the formation of the well-known oxoanion ferrate(VI), FeO42-. Ruthenium tetroxide, RuO4, formed by oxidation of ruthenium(VI) in acid, readily undergoes reduction to ruthenate(VI), RuO42-. Oxidation of ruthenium metal in air forms the dioxide, RuO2. In contrast, osmium burns to form the stable tetroxide, OsO4, which complexes with the hydroxide ion to form an osmium(VIII) -"ate" complex, [OsO4(OH)2]2−. Therefore, eka-osmium properties for hassium should be demonstrated by the formation of a stable, very volatile tetroxide HsO4, which undergoes complexation with hydroxide to form a hassate(VIII), [HsO4(OH)2]2−. Ruthenium tetroxide and osmium tetroxide are both volatile, due to their symmetrical tetrahedral molecular geometry and their being charge-neutral; hassium tetroxide should similarly be a very volatile solid. The trend of the volatilities of the group 8 tetroxides is known to be RuO4 < OsO4 > HsO4, which completely confirms the calculated results. In particular, the calculated enthalpies of adsorption (the energy required for the adhesion of atoms, molecules, or ions from a gas, liquid, or dissolved solid to a surface) of HsO4, −(45.4 ± 1) kJ·mol−1 on quartz, agrees very well with the experimental value of −(46 ± 2) kJ·mol−1.
Experimental atomic gas phase chemistry.
Despite the fact that the selection of a volatile hassium compound (hassium tetroxide) for gas-phase chemical studies was clear from the beginning, the chemical characterization of hassium was considered a difficult task for a long time. Although hassium isotopes were first synthesized in 1984, it was not until 1996 that a hassium isotope long-lived enough to allow chemical studies to be performed was synthesized. Unfortunately, this hassium isotope, 269Hs, was then synthesized indirectly from the decay of 277Cn; not only are indirect synthesis methods not favourable for chemical studies, but also the reaction that produced the isotope 277Cn had a low yield (its cross-section was only 1 pb), and thus did not provide enough hassium atoms for a chemical investigation. The direct synthesis of 269Hs and 270Hs in the reaction 248Cm(26Mg,xn)274−xHs (x = 4 or 5) appeared more promising, as the cross-section for this reaction was somewhat larger, at 7 pm. However, this yield was still around ten times lower than that for the reaction used for the chemical characterization of bohrium. New techniques for irradiation, separation, and detection had to be introduced before hassium could be successfully characterized chemically as a typical member of group 8 in early 2001.
Ruthenium and osmium have very similar chemistry due to the lanthanide contraction, but iron shows some differences from them: for example, although ruthenium and osmium form stable tetroxides in which the metal is in the +8 oxidation state, iron does not. Consequently, in preparation for the chemical characterization of hassium, researches focused on ruthenium and osmium rather than iron, as hassium was expected to also be similar to ruthenium and osmium due to the actinide contraction. However, in the planned experiment to study hassocene (Hs(C5H5)2), ferrocene may also be used for comparison along with ruthenocene and osmocene.
The first chemistry experiments were performed using gas thermochromatography in 2001, using 172Os and 173Os as a reference. During the experiment, 5 hassium atoms were synthesized using the reaction 248Cm(26Mg,5n)269Hs. They were then thermalized and oxidized in a mixture of helium and oxygen gas to form the tetroxide.
The measured deposition temperature indicated that hassium(VIII) oxide is less volatile than osmium tetroxide, OsO4, and places hassium firmly in group 8. However, the enthalpy of adsorption for HsO4 measured, (−46 ± 2) kJ/mol, was significantly lower than what was predicted, (−36.7 ± 1.5) kJ/mol, indicating that OsO4 was more volatile than HsO4, contradicting earlier calculations, which implied that they should have very similar volatilities. For comparison, the value for OsO4 is (−39 ± 1) kJ/mol. It is possible that hassium tetroxide interacts differently with the different chemicals (silicon nitride and silicon dioxide) used for the detector; further research is required, including more accurate measurements of the nuclear properties of 269Hs and comparisons with RuO4 in addition to OsO4.
In order to further probe the chemistry of hassium, scientists decided to assess the reaction between hassium tetroxide and sodium hydroxide to form sodium hassate(VIII), a reaction well-known with osmium. In 2004, scientists announced that they had succeeded in carrying out the first acid-base reaction with a hassium compound, forming sodium hassate(VIII):
The team from the University of Mainz are planning to study the electrodeposition of hassium atoms using the new TASCA facility at the GSI. The current aim is to use the reaction 226Ra(48Ca,4n)270Hs. In addition, scientists at the GSI are hoping to utilize TASCA to study the synthesis and properties of the hassium(II) compound hassocene, Hs(C5H5)2, using the reaction 226Ra(48Ca,xn). This compound is analogous to the lighter ferrocene, ruthenocene, and osmocene, and is expected to have the two cyclopentadienyl rings in an eclipsed conformation like ruthenocene and osmocene and not in a staggered conformation like ferrocene. Hassocene was chosen because it has hassium in the low formal oxidation state of +2 (although the bonding between the metal and the rings is mostly covalent in metallocenes) rather than the high +8 state which has been investigated, and relativistic effects were expected to be stronger in the lower oxidation state. Many metals in the periodic table form metallocenes, so that trends could be more easily determined, and the highly symmetric structure of hassocene and its low number of atoms also make relativistic calculations easier. Hassocene should be a stable and highly volatile compound.

</doc>
<doc id="13765" url="http://en.wikipedia.org/wiki?curid=13765" title="Henry Kissinger">
Henry Kissinger

Henry Alfred Kissinger (; born Heinz Alfred Kissinger ]; May 27, 1923) is an American diplomat and political scientist. He served as National Security Advisor and later concurrently as Secretary of State in the administrations of Presidents Richard Nixon and Gerald Ford. For his actions negotiating the never actualised ceasefire in Vietnam, Kissinger received the 1973 Nobel Peace Prize. Le Duc Tho, with whom Kissinger shared the prize, refused it, and two members of the Nobel judging committee resigned in protest. After his term, his opinion has still been sought by many subsequent U.S. presidents and other world leaders.
A proponent of "Realpolitik", Kissinger played a prominent role in United States foreign policy between 1969 and 1977. During this period, he pioneered the policy of "détente" with the Soviet Union, orchestrated the opening of relations with the People's Republic of China, and negotiated the Paris Peace Accords, ending American involvement in the Vietnam War. Kissinger's "Realpolitik" resulted in controversial policies such as CIA involvement in Chile and the US's support for Pakistan, despite its genocidal actions during the Bangladesh War. He is the founder and chairman of Kissinger Associates, an international consulting firm. Kissinger has been a prolific author of books in politics and international relations with over one dozen books authored.
Early life and education.
Kissinger was born Heinz Alfred Kissinger in Fürth, Bavaria, Germany, in 1923 during the Weimar Republic, to a family of German Jews. His father, Louis Kissinger (1887–1982), was a schoolteacher. His mother, Paula (Stern) Kissinger (1901–1998), was a homemaker. Kissinger has a younger brother, Walter Kissinger. The surname Kissinger was adopted in 1817 by his great-great-grandfather Meyer Löb, after the Bavarian spa town of Bad Kissingen. As a youth, Heinz enjoyed playing football, and even played for the youth side of his favorite club and one of the nation's best clubs at the time, SpVgg Fürth. In 1938, fleeing Nazi persecution, his family moved to London, England, before arriving in New York on September 5.
Kissinger spent his high school years in the Washington Heights section of upper Manhattan as part of the German Jewish immigrant community there. Although Kissinger assimilated quickly into American culture, he never lost his pronounced Frankish accent, due to childhood shyness that made him hesitant to speak. Following his first year at George Washington High School, he began attending school at night and worked in a shaving brush factory during the day.
Following high school, Kissinger enrolled in the City College of New York, studying accounting. He excelled academically as a part-time student, continuing to work while enrolled. His studies were interrupted in early 1943, when he was drafted into the U.S. Army.
Army experience.
Kissinger underwent basic training at Camp Croft in Spartanburg, South Carolina. On June 19, 1943, while stationed in South Carolina, at the age of 20 years, he became a naturalized U.S. citizen. The army sent him to study engineering at Lafayette College, Pennsylvania, but the program was cancelled, and Kissinger was reassigned to the 84th Infantry Division. There, he made the acquaintance of Fritz Kraemer, a fellow immigrant from Germany who noted Kissinger's fluency in German and his intellect, and arranged for him to be assigned to the military intelligence section of the division. Kissinger saw combat with the division, and volunteered for hazardous intelligence duties during the Battle of the Bulge.
During the American advance into Germany, Kissinger, only a private, was put in charge of the administration of the city of Krefeld, owing to a lack of German speakers on the division's intelligence staff. Within eight days he had established a civilian administration. Kissinger was then reassigned to the Counter Intelligence Corps, with the rank of sergeant. He was given charge of a team in Hanover assigned to tracking down Gestapo officers and other saboteurs, for which he was awarded the Bronze Star. In June 1945, Kissinger was made commandant of the Bensheim metro CIC detachment, Bergstrasse district of Hesse, with responsibility for de-Nazification of the district. Although he possessed absolute authority and powers of arrest, Kissinger took care to avoid abuses against the local population by his command.
In 1946, Kissinger was reassigned to teach at the European Command Intelligence School at Camp King, continuing to serve in this role as a civilian employee following his separation from the army.
Academic career.
Henry Kissinger received his AB degree "summa cum laude" in political science at Harvard College in 1950, where he lived in Adams House and studied under William Yandell Elliott. He received his MA and PhD degrees at Harvard University in 1951 and 1954, respectively. In 1952, while still at Harvard, he served as a consultant to the director of the Psychological Strategy Board. His doctoral dissertation was titled "Peace, Legitimacy, and the Equilibrium (A Study of the Statesmanship of Castlereagh and Metternich)".
Kissinger remained at Harvard as a member of the faculty in the Department of Government and, with Robert R. Bowie, co-founded the Center for International Affairs in 1958. In 1955, he was a consultant to the National Security Council's Operations Coordinating Board. During 1955 and 1956, he was also study director in nuclear weapons and foreign policy at the Council on Foreign Relations. He released his book "Nuclear Weapons and Foreign Policy" the following year. From 1956 to 1958 he worked for the Rockefeller Brothers Fund as director of its Special Studies Project. He was director of the Harvard Defense Studies Program between 1958 and 1971. He was also director of the Harvard International Seminar between 1951 and 1971. Outside of academia, he served as a consultant to several government agencies and think tanks, including the Operations Research Office, the Arms Control and Disarmament Agency, and the Department of State, and the Rand Corporation.
Keen to have a greater influence on U.S. foreign policy, Kissinger became a supporter of, and advisor to, Nelson Rockefeller, Governor of New York, who sought the Republican nomination for president in 1960, 1964 and 1968. After Richard Nixon won the presidency in 1968, he made Kissinger National Security Advisor.
Foreign policy.
Kissinger served as National Security Advisor and Secretary of State under President Richard Nixon, and continued as Secretary of State under Nixon's successor Gerald Ford.
A proponent of "Realpolitik", Kissinger played a dominant role in United States foreign policy between 1969 and 1977. In that period, he extended the policy of "détente". This policy led to a significant relaxation in U.S.-Soviet tensions and played a crucial role in 1971 talks with Chinese Premier Zhou Enlai. The talks concluded with a rapprochement between the United States and the People's Republic of China, and the formation of a new strategic anti-Soviet Sino-American alignment. He was jointly awarded the 1973 Nobel Peace Prize with Le Duc Tho for helping to establish a ceasefire and U.S. withdrawal from Vietnam. The ceasefire, however, was not durable, and Tho declined to accept the award. As National Security Advisor, in 1974 Kissinger directed the much-debated National Security Study Memorandum 200.
"Détente" and the opening to China.
As National Security Advisor under Nixon, Kissinger pioneered the policy of "détente" with the Soviet Union, seeking a relaxation in tensions between the two superpowers. As a part of this strategy, he negotiated the Strategic Arms Limitation Talks (culminating in the SALT I treaty) and the Anti-Ballistic Missile Treaty with Leonid Brezhnev, General Secretary of the Soviet Communist Party. Negotiations about strategic disarmament were originally supposed to start under the Johnson Administration but were postponed in protest to the invasion by Warsaw Pact troops of Czechoslovakia in August 1968.
Kissinger sought to place diplomatic pressure on the Soviet Union. He made two trips to the People's Republic of China in July and October 1971 (the first of which was made in secret) to confer with Premier Zhou Enlai, then in charge of Chinese foreign policy. According to Kissinger's book, "The White House Years", the first secret China trip was arranged through Pakistan's diplomatic and Presidential involvement that paved the way to initial vital contact with China since the Americans were unable to communicate directly with the Chinese leaders because of earlier cold relations.
Kissinger would show his support for the regime in Beijing by supporting their actions during the unrest which included the Tiananmen Square Massacre.
This paved the way for the groundbreaking 1972 summit between Nixon, Zhou, and Communist Party of China Chairman Mao Zedong, as well as the formalization of relations between the two countries, ending 23 years of diplomatic isolation and mutual hostility. The result was the formation of a tacit strategic anti-Soviet alliance between China and the United States.
While Kissinger's diplomacy led to economic and cultural exchanges between the two sides and the establishment of Liaison Offices in the Chinese and American capitals, with serious implications for Indochinese matters, full normalization of relations with the People's Republic of China would not occur until 1979, because the Watergate scandal overshadowed the latter years of the Nixon presidency and because the United States continued to recognize the government of Taiwan.
Vietnam War.
Kissinger's involvement in Indochina started prior to his appointment as National Security Adviser to Nixon. While still at Harvard, he had worked as a consultant on foreign policy to both the White House and State Department. Kissinger says that "In August 1965... [Henry Cabot Lodge Jr.], an old friend serving as Ambassador to Saigon, had asked me to visit Vietnam as his consultant. I toured Vietnam first for two weeks in October and November 1965, again for about ten days in July 1966, and a third time for a few days in October 1966... Lodge gave me a free hand to look into any subject of my choice". He became convinced of the meaninglessness of military victories in Vietnam, "... unless they brought about a political reality that could survive our ultimate withdrawal". In a 1967 peace initiative, he would mediate between Washington and Hanoi.
Nixon had been elected in 1968 on the promise of achieving "peace with honor" and ending the Vietnam War. In office, and assisted by Kissinger, Nixon implemented a policy of Vietnamization that aimed to gradually withdraw U.S. troops while expanding the combat role of the South Vietnamese Army so that it would be capable of independently defending its government against the National Front for the Liberation of South Vietnam, a Communist guerrilla organization, and North Vietnamese army (Vietnam People's Army or PAVN). Kissinger played a key role in secretly bombing Cambodia to disrupt PAVN and Viet Cong units launching raids into South Vietnam from within Cambodia's borders and resupplying their forces by using the Ho Chi Minh trail and other routes, as well as the 1970 Cambodian Incursion and subsequent widespread bombing of suspected Khmer Rouge targets in Cambodia. The bombing campaign contributed to the chaos of the Cambodian Civil War, which saw the forces of U.S.-backed leader Lon Nol unable to retain foreign support to combat the growing Khmer Rouge insurgency that would overthrow him in 1975. Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese invasion of Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea. The American bombing of Cambodia killed an estimated 40,000 Cambodian combatants and civilians. Pol Pot biographer David P. Chandler argues that the bombing "had the effect the Americans wanted – it broke the Communist encirclement of Phnom Penh."
Along with North Vietnamese Politburo Member Le Duc Tho, Kissinger was awarded the Nobel Peace Prize on December 10, 1973, for their work in negotiating the ceasefires contained in the Paris Peace Accords on "Ending the War and Restoring Peace in Vietnam", signed the previous January. According to Irwin Abrams, this prize was the most controversial to date. For the first time in the history of the Peace Prize two members left the Nobel Committee in protest. Tho rejected the award, telling Kissinger that peace had not been restored in South Vietnam. Kissinger wrote to the Nobel Committee that he accepted the award "with humility". The conflict continued until an invasion of the South by the North Vietnamese Army resulted in a North Vietnamese victory in 1975 and the subsequent progression of the Pathet Lao in Laos towards figurehead status.
Bangladesh War.
Under Kissinger's guidance, the United States government supported Pakistan in the Bangladesh Liberation War in 1971. Kissinger was particularly concerned about the expansion of Soviet influence in South Asia as a result of a treaty of friendship recently signed by India and the USSR, and sought to demonstrate to the People's Republic of China (Pakistan's ally and an enemy of both India and the USSR) the value of a tacit alliance with the United States.
Kissinger sneered at people who “bleed” for “the dying Bengalis” and ignored the first telegram from the United States consul general in East Pakistan, Archer K. Blood, and 20 members of his staff, which informed the US that their allies West Pakistan were undertaking, in Blood's words, "a selective genocide". In the second, more famous, Blood Telegram the word genocide was again used to describe the events, and further that with its continuing support for West Pakistan the US government had "evidenced [...] moral bankruptcy".
As a direct response to the dissent against US policy Kissinger and Nixon ended Archer Blood's tenure as United States consul general in East Pakistan and put him to work in the State Department's Personnel Office.
Henry Kissinger had also come under fire for private comments he made to Nixon during the Bangladesh–Pakistan War in which he described Indian Prime Minister Indira Gandhi as a "bitch" and a "witch". He also said "The Indians are bastards," shortly before the war. Kissinger has since expressed his regret over the comments.
Israeli policy and Soviet Jewry.
According to notes taken by H. R. Haldeman, Nixon "ordered his aides to exclude all Jewish-Americans from policy-making on Israel", including Kissinger. One note quotes Nixon as saying "get K. [Kissinger] out of the play—Haig handle it".
In 1973, Kissinger did not feel that pressing the Soviet Union concerning the plight of Jews being persecuted there was in the interest of U.S. foreign policy. In conversation with Nixon shortly after a meeting with Golda Meir on March 1, 1973, Kissinger stated, "The emigration of Jews from the Soviet Union is not an objective of American foreign policy, and if they put Jews into gas chambers in the Soviet Union, it is not an American concern. Maybe a humanitarian concern." Kissinger argued, however:
That emigration existed at all was due to the actions of "realists" in the White House. Jewish emigration rose from 700 a year in 1969 to near 40,000 in 1972. The total in Nixon's first term was more than 100,000. To maintain this flow by quiet diplomacy, we never used these figures for political purposes. ... The issue became public because of the success of our Middle East policy when Egypt evicted Soviet advisers. To restore its relations with Cairo, the Soviet Union put a tax on Jewish emigration. There was no Jackson–Vanik Amendment until there was a successful emigration effort. Sen. Henry Jackson, for whom I had, and continue to have, high regard, sought to remove the tax with his amendment. We thought the continuation of our previous approach of quiet diplomacy was the wiser course. ... Events proved our judgment correct. Jewish emigration fell to about a third of its previous high.
1973 Yom Kippur War.
Documents show that Kissinger delayed telling President Richard Nixon about the start of the Yom Kippur War in 1973 to keep him from interfering. On October 6, 1973, the Israelis informed Kissinger about the attack at 6 am; Kissinger waited nearly 3 and a half hours before he informed Nixon.
According to Kissinger, in an interview in November 2013, he was notified at 6:30 a.m. (12:30 p.m. Israel time) that war was imminent, and his urgent calls to the Soviets and Egyptians were ineffective. He says Golda Meir's decision not to preempt was wise and reasonable, balancing the risk of Israel looking like the aggressor and Israel's actual ability to strike within such a brief span of time.
The war began on October 6, 1973, when Egypt and Syria attacked Israel. Kissinger published lengthy telephone transcripts from this period in the 2002 book "Crisis". On October 12, under Nixon's direction, and against Kissinger's initial advice, while Kissinger was on his way to Moscow to discuss conditions for a cease-fire, Nixon sent a message to Brezhnev giving Kissinger full negotiating authority.
Israel regained the territory it lost in the early fighting and gained new territories from Syria and Egypt, including land in Syria east of the previously captured Golan Heights, and additionally on the western bank of the Suez Canal, although they did lose some territory on the eastern side of the Suez Canal that had been in Israeli hands since the end of the Six Day War. Kissinger pressured the Israelis to cede some of the newly captured land back to its Arab neighbors, contributing to the first phases of Israeli-Egyptian non-aggression. The move saw a warming in U.S.–Egyptian relations, bitter since the 1950s, as the country moved away from its former independent stance and into a close partnership with the United States. The peace was finalized in 1978 when U.S. President Jimmy Carter mediated the Camp David Accords, during which Israel returned the Sinai Peninsula in exchange for an Egyptian peace agreement that included the recognition of the state of Israel.
Latin American policy.
The United States continued to recognize and maintain relationships with non-left-wing governments, democratic and authoritarian alike. John F. Kennedy's Alliance for Progress was ended in 1973. In 1974, negotiations about a new settlement over the Panama Canal started. They eventually led to the Torrijos-Carter Treaties and the handing over of the Canal to Panamanian control.
Kissinger initially supported the normalization of United States-Cuba relations, broken since 1961 (all U.S.–Cuban trade was blocked in February 1962, a few weeks after the exclusion of Cuba from the Organization of American States because of U.S. pressure). However, he quickly changed his mind and followed Kennedy's policy. After the involvement of the Cuban Revolutionary Armed Forces in the independence struggles in Angola and Mozambique, Kissinger said that unless Cuba withdrew its forces relations would not be normalized. Cuba refused.
Intervention in Chile.
Chilean Socialist Party presidential candidate Salvador Allende was elected by a plurality in 1970, causing serious concern in Washington, D.C. due to his openly socialist and pro-Cuban politics. The Nixon administration, with Kissinger's input, authorized the Central Intelligence Agency (CIA) to encourage a military coup that would prevent Allende's inauguration, but the plan was not successful.:115:495:177
United States-Chile relations remained frosty during Salvador Allende's tenure, following the complete nationalization of the partially U.S.-owned copper mines and the Chilean subsidiary of the U.S.-based ITT Corporation, as well as other Chilean businesses. The U.S. claimed that the Chilean government had greatly undervalued fair compensation for the nationalization by subtracting what it deemed "excess profits". Therefore, the U.S. implemented economic sanctions against Chile. The CIA also provided funding for the mass anti-government strikes in 1972 and 1973, and extensive black propaganda in the newspaper "El Mercurio".:93
The most expeditious way to prevent Allende from assuming office was somehow to convince the Chilean congress to confirm Jorge Alessandri as the winner of the election. Once elected by the congress, Alessandri—a party to the plot through intermediaries—was prepared to resign his presidency within a matter of days so that new elections could be held. This first, nonmilitary, approach to stopping Allende was called the Track I approach. The CIA's second approach, the Track II approach, was designed to encourage a military overthrow.
On September 11, 1973, Allende died during a military coup launched by Army Commander-in-Chief Augusto Pinochet, who became President.
A document released by the CIA in 2000 titled "CIA Activities in Chile" revealed that the United States, acting through the CIA, actively supported the military junta after the overthrow of Allende and that it made many of Pinochet's officers into paid contacts of the CIA or U.S. military.
In 1976, Orlando Letelier, a Chilean opponent of the Pinochet regime, was assassinated in Washington, D.C. with a car bomb. Previously, Kissinger had helped secure his release from prison, and had chosen to cancel a letter to Chile warning them against carrying out any political assassinations. The U.S. ambassador to Chile, David H. Popper, said that Pinochet might take as an insult any inference that he was connected with assassination plots.
Argentina.
Kissinger took a similar line as he had toward Chile when the Argentinian military, led by Jorge Videla, toppled the elected government of Isabel Perón in 1976 with a process called the National Reorganization Process by the military, with which they consolidated power, launching brutal reprisals and "disappearances" against political opponents. During a meeting with Argentinian foreign minister César Augusto Guzzetti, Kissinger assured him that the United States was an ally, but urged him to "get back to normal procedures" quickly before the U.S. Congress reconvened and had a chance to consider sanctions.
Africa.
In September 1976 Kissinger was actively involved in negotiations regarding the Rhodesian Bush War. Kissinger, along with South Africa's Prime Minister John Vorster, pressured Rhodesian Prime Minister Ian Smith to hasten the transition to black majority rule in Rhodesia. With FRELIMO in control of Mozambique and even South Africa withdrawing its support, Rhodesia's isolation was nearly complete. According to Smith's autobiography, Kissinger told Smith of Mrs. Kissinger's admiration for him, but Smith stated that he thought Kissinger was asking him to sign Rhodesia's "death certificate". Kissinger, bringing the weight of the United States, and corralling other relevant parties to put pressure on Rhodesia, hastened the end of minority-rule.
East Timor.
The Portuguese decolonization process brought U.S. attention to the former Portuguese colony of East Timor, which lies within the Indonesian archipelago and declared its independence in 1975. Indonesian president Suharto was a strong U.S. ally in Southeast Asia and began to mobilize the Indonesian army, preparing to annex the nascent state, which had become increasingly dominated by the popular leftist FRETILIN party. In December 1975, Suharto discussed the invasion plans during a meeting with Kissinger and President Ford in the Indonesian capital of Jakarta. Both Ford and Kissinger made clear that U.S. relations with Indonesia would remain strong and that it would not object to the proposed annexation. They only wanted it done "fast" and proposed to delay the invasion until they had returned to Washington. Accordingly Suharto delayed the operation for one day. Finally on December 7 Indonesian forces invaded the former Portuguese colony. U.S. arms sales to Indonesia continued, and Suharto went ahead with the annexation plan.
Later roles.
Kissinger left office when a Democrat, former Governor of Georgia Jimmy Carter, defeated Republican Gerald Ford in the 1976 presidential elections. Kissinger continued to participate in policy groups, such as the Trilateral Commission, and to maintain political consulting, speaking, and writing engagements.
Shortly after Kissinger left office in 1977, he was offered an endowed chair at Columbia University. There was significant student opposition to the appointment, which eventually became a subject of wide media commentary. Columbia cancelled the appointment as a result.
Kissinger was then appointed to Georgetown University's Center for Strategic and International Studies. He taught at Georgetown's Edmund Walsh School of Foreign Service for several years in the late 1970s. In 1982, with the help of a loan from the international banking firm of E.M. Warburg, Pincus and Company, Kissinger founded a consulting firm, Kissinger Associates, and is a partner in affiliate Kissinger McLarty Associates with Mack McLarty, former chief of staff to President Bill Clinton. He also serves on board of directors of Hollinger International, a Chicago-based newspaper group, and as of March 1999, he also serves on the board of directors of Gulfstream Aerospace.
In 1978, Kissinger was named chairman of the North American Soccer League board of directors. From 1995 to 2001, he served on the board of directors for Freeport-McMoRan, a multinational copper and gold producer with significant mining and milling operations in Papua, Indonesia. In February 2000, then-president of Indonesia Abdurrahman Wahid appointed Kissinger as a political advisor. He also serves as an honorary advisor to the United States-Azerbaijan Chamber of Commerce.
From 2000–2006, Kissinger served as chairman of the board of trustees of Eisenhower Fellowships. In 2006, upon his departure from Eisenhower Fellowships, he received the Dwight D. Eisenhower Medal for Leadership and Service.
In November 2002, he was appointed by President George W. Bush to chair the newly established National Commission on Terrorist Attacks Upon the United States to investigate the September 11 attacks. Kissinger stepped down as chairman on December 13, 2002 rather than reveal his business client list, when queried about potential conflicts of interest.
Kissinger — along with William Perry, Sam Nunn, and George Shultz — has called upon governments to embrace the vision of a world free of nuclear weapons, and in three "Wall Street Journal" op-eds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Nunn reinforced that agenda during a speech at the Harvard Kennedy School on October 21, 2008, saying, "I’m much more concerned about a terrorist without a return address that cannot be deterred than I am about deliberate war between nuclear powers. You can’t deter a group who is willing to commit suicide. We are in a different era. You have to understand the world has changed." In 2010, the four were featured in a documentary film entitled "Nuclear Tipping Point". The film is a visual and historical depiction of the ideas laid forth in the "Wall Street Journal" op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.
Role in U.S. foreign policy.
Yugoslav wars.
In several articles of his and interviews that he gave during the Yugoslav wars, he criticized the United States' policies in Southeast Europe, among other things for the recognition of Bosnia and Herzegovina as a sovereign state, which he described as a foolish act. Most importantly he dismissed the notion of Serbs, and Croats for that part, being aggressors or separatist, saying that "they can't be separating from something that has never existed".
In addition, he repeatedly warned the West of inserting itself into a conflict that has its roots at least hundreds of years back in time, and said that the West would do better if it allowed the Serbs and Croats to join their respective countries.
Kissinger shared similarly critical views on Western involvement in Kosovo. In particular, he held a disparaging view of the Rambouillet Agreement:
The Rambouillet text, which called on Serbia to admit NATO troops throughout Yugoslavia, was a provocation, an excuse to start bombing. Rambouillet is not a document that any Serb could have accepted. It was a terrible diplomatic document that should never have been presented in that form.—Henry Kissinger, Daily Telegraph, June 28, 1999
However, as the Serbs did not accept the Rambouillet text and NATO bombings started, he opted for a continuation of the bombing as NATO's credibility was now at stake, but dismissed the usage of ground forces, claiming that it was not worth it.
Iraq.
In 2006, it was reported in the book "" by Bob Woodward that Kissinger met regularly with President George W. Bush and Vice President Dick Cheney to offer advice on the Iraq War. Kissinger confirmed in recorded interviews with Woodward that the advice was the same as he had given in an August 12, 2005 column in "The Washington Post": "Victory over the insurgency is the only meaningful exit strategy."
In a November 19, 2006, interview on BBC "Sunday AM", Kissinger said, when asked whether there is any hope left for a clear military victory in Iraq, "If you mean by 'military victory' an Iraqi government that can be established and whose writ runs across the whole country, that gets the civil war under control and sectarian violence under control in a time period that the political processes of the democracies will support, I don't believe that is possible. ... I think we have to redefine the course. But I don't believe that the alternative is between military victory as it had been defined previously, or total withdrawal."
In an April 3, 2008, interview with Peter Robinson of the Hoover Institution, Kissinger reiterated that even though he supported the 2003 invasion of Iraq he thought that the George W. Bush administration rested too much of its case for war on Saddam's supposed weapons of mass destruction. Robinson noted that Kissinger had criticized the administration for invading with too few troops, for disbanding the Iraqi Army, and for mishandling relations with certain allies.
India.
Kissinger said in April 2008 that "India has parallel objectives to the United States," and he called it an ally of the U.S.
China.
Kissinger was present at the opening ceremony of the Beijing Summer Olympics.
In 2011, Kissinger published "On China", chronicling the evolution of Sino-American relations and laying out the challenges to a partnership of 'genuine strategic trust' between the U.S. and China.
Iran.
Kissinger's position on this issue of U.S.–Iran talks was reported by the "Tehran Times" to be that "Any direct talks between the U.S. and Iran on issues such as the nuclear dispute would be most likely to succeed if they first involved only diplomatic staff and progressed to the level of secretary of state before the heads of state meet."
2014 Ukrainian crisis.
On 5 March 2014, before the 16 March referendum in Crimea, "The Washington Post" published an op-ed piece by Kissinger. In it, he attempted to balance the Ukrainian, Russian and Western desires for a functional state. He made four propositions:
Kissinger also wrote: "The west speaks Ukrainian; the east speaks mostly Russian. Any attempt by one wing of Ukraine to dominate the other — as has been the pattern — would lead eventually to civil war or break up."
Following the publication of his new book titled "World Order", Kissinger participated in an interview with Charlie Rose and updated his position on Ukraine which he sees as a possible geographical mediator between Russia and the West. In a question he posed to himself for illustration regarding re-conceiving policy regarding Ukraine, Kissinger stated: "If Ukraine is considered an outpost, then the situation is that its eastern border is the NATO strategic line, and NATO will be within 200 miles of [Volgograd]. That will never be accepted by Russia. On the other hand, if the Russian western line is at the border of Poland, Europe will be permanently disquieted. The Strategic objective should have been to see whether one can build Ukraine as a bridge between East and West, and whether one can do it as a kind of a joint effort."
Public perception.
At the height of Kissinger's prominence, many commented on his wit. In one instance, at the Washington Press Club annual congressional dinner, "Kissinger mocked his reputation as a secret swinger." He was quoted as saying "Power is the ultimate aphrodisiac."
Kissinger has shied away from mainstream media and cable talk shows. He granted a rare interview to the producers of a documentary examining the underpinnings of the 1979 peace treaty between Israel and Egypt entitled "". In the film, Kissinger revealed how close he felt the world was to nuclear war during the 1973 Yom Kippur War launched by Egypt and Syria against Israel.
Since he left office, some efforts have been made to hold Kissinger responsible for perceived injustices of American foreign policy during his tenure in government. These attempts have at times followed him in his international travels. Christopher Hitchens, the British-American journalist and author, was highly critical of Kissinger, authoring "The Trial of Henry Kissinger", in which Hitchens called for the prosecution of Kissinger "for war crimes, for crimes against humanity, and for offenses against common or customary or international law, including conspiracy to commit murder, kidnap, and torture". In 2011 an interview-based documentary, titled "Kissinger", was released, in which Kissinger "reflects on some of his most important and controversial decisions" during his tenure as Secretary of State.
Family and personal life.
Kissinger married Ann Fleischer, with whom he had two children, Elizabeth and David. They divorced in 1964. Ten years later, he married Nancy Maginnes. They now live in Kent, Connecticut and New York City. His son David Kissinger was an executive with NBC Universal before becoming head of Conaco, Conan O'Brien's production company.
Since his childhood, Kissinger has been a fan of his hometown's soccer club, SpVgg Greuther Fürth. Even during his time in office he was informed about the team's results by the German Embassy every Monday morning. He is an honorary member with lifetime season tickets. In September 2012, Kissinger attended a home game in which SpVgg Greuther Fürth lost, 0–2, against Schalke after promising years ago he would attend a Greuther Fürth home game if they were promoted to the Bundesliga, the top football league in Germany, from the 2. Bundesliga.
Kissinger described "Diplomacy" as his favorite game in a 1973 interview.

</doc>
<doc id="13767" url="http://en.wikipedia.org/wiki?curid=13767" title="Hydra (genus)">
Hydra (genus)

Hydra is a genus of small, simple, fresh-water animals that possess radial symmetry. "Hydra" are predatory animals belonging to the phylum Cnidaria and the class Hydrozoa. They can be found in most unpolluted fresh-water ponds, lakes, and streams in the temperate and tropical regions and can be found by gently sweeping a collecting net through weedy areas. They are multicellular organisms which are usually a few millimetres long and are best studied with a microscope. Biologists are especially interested in "Hydra" because of their regenerative ability; they appear not to age or die of old age.
Morphology.
"Hydra" has a tubular body up to 10 mm long when extended, secured by a simple adhesive foot called the basal disc. Gland cells in the basal disc secrete a sticky fluid that accounts for its adhesive properties.
At the free end of the body is a mouth opening surrounded by one to twelve thin, mobile tentacles. Each tentacle, or cnida (plural: cnidae), is clothed with highly specialised stinging cells called cnidocytes. Cnidocytes contain specialized structures called nematocysts, which look like miniature light bulbs with a coiled thread inside. At the narrow outer edge of the cnidocyte is a short trigger hair called a cnidocil. Upon contact with prey, the contents of the nematocyst are explosively discharged, firing a dart-like thread containing neurotoxins into whatever triggered the release which can paralyse the prey, especially if many hundreds of nematocysts are fired.
"Hydra" has two main body layers, which makes it "diploblastic". The layers are separated by mesoglea, a gel-like substance. The outer layer is the epidermis, and the inner layer is called the gastrodermis, because it lines the stomach. The cells making up these two body layers are relatively simple. Hydramacin is a bactericide recently discovered in "Hydra"; it protects the outer layer against infection.
The nervous system of "Hydra" is a nerve net, which is structurally simple compared to mammalian nervous systems. "Hydra" does not have a recognizable brain or true muscles. Nerve nets connect sensory photoreceptors and touch-sensitive nerve cells located in the body wall and tentacles.
Respiration and excretion occur by diffusion everywhere through the epidermis.
Motion and locomotion.
If "Hydra" are alarmed or attacked, the tentacles can be retracted to small buds, and the body column itself can be retracted to a small gelatinous sphere. "Hydra" generally react in the same way regardless of the direction of the stimulus, and this may be due to the simplicity of the nerve net.
"Hydra" are generally or sessile, but do occasionally move quite readily, especially when hunting. They do this by bending over and attaching themselves to the with the mouth and tentacles and then release the foot, which provides the usual attachment, this process is called looping. The body then bends over and makes a new place of attachment with the foot. By this process of "looping" or "somersaulting", a "Hydra" can move several inches (c. 100 mm) in a day. "Hydra" may also move by amoeboid motion of their bases or by simply detaching from the substrate and floating away in the current.
Reproduction and life cycle.
When food is plentiful, many "Hydra" reproduce asexually by producing buds in the body wall, which grow to be miniature adults and simply break away when they are mature. When a hydra is well fed, a new bud can form every two days. When conditions are harsh, often before winter or in poor feeding conditions, sexual reproduction occurs in some "Hydra". Swellings in the body wall develop into either a simple ovary or testes. The testes release free-swimming gametes into the water, and these can fertilize the egg in the ovary of another individual. The fertilized eggs secrete a tough outer coating, and, as the adult dies, these resting eggs fall to the bottom of the lake or pond to await better conditions, whereupon they hatch into nymph "Hydra". Some, like "Hydra circumcincta" and "Hydra viridissima", are hermaphrodites and may produce both testes and an ovary at the same time.
Many members of the Hydrozoa go through a body change from a polyp to an adult form called a medusa.
However, all "Hydra", despite being hydrozoans, remain as polyps throughout their lives.
Feeding.
"Hydra" mainly feed on small aquatic invertebrates such as "Daphnia" and "Cyclops".
When feeding, "Hydra" extend their body to maximum length and then slowly extend their tentacles. Despite their simple construction, the tentacles of "Hydra" are extraordinarily extensible and can be four to five times the length of the body. Once fully extended, the tentacles are slowly manoeuvred around waiting for contact with a suitable prey animal. Upon contact, nematocysts on the tentacle fire into the prey, and the tentacle itself coils around the prey. Within 30 seconds, most of the remaining tentacles will have already joined in the attack to subdue the struggling prey. Within two minutes, the tentacles will have surrounded the prey and moved it into the opened mouth aperture. Within ten minutes, the prey will have been enclosed within the body cavity, and digestion will have started. "Hydra" is able to stretch its body wall considerably in order to digest prey more than twice its size. After two or three days, the indigestible remains of the prey will be discharged by contractions through the mouth aperture.
The feeding behaviour of "Hydra" demonstrates the sophistication of what appears to be a simple nervous system.
Some species of "Hydra" exist in a mutual relationship with various types of unicellular algae. The algae are protected from predators by "Hydra" and, in return, photosynthetic products from the algae are beneficial as a food source to "Hydra".
Measuring the feeding response.
The feeding response in hydra is known to be induced by reduced glutathione released from the injured prey. There are several methods which are conventionally used for quantification of the feeding response. In some of such methods, the duration for which mouth of hydra remains open is measured. Whereas, few other methods rely on courting the number of hydra out of a small population showing the feeding response after addition of glutathione. Recently, an assay for measuring the feeding response in hydra has been developed. In this method, the linear two-dimensional distance between the tip of the tentacle and the mouth of hydra was shown to give the direct measure of the extent of feeding response. This method has been validated using a starvation model, as starvation is known to cause enhancement in the feeding response in hydra.
Morphallaxis.
"Hydra" undergoes morphallaxis (tissue regeneration) when injured or severed.
Non-senescence.
Daniel Martinez claimed in a 1998 article in "Experimental Gerontology" that "Hydra" are biologically immortal. This publication has been widely cited as evidence that "Hydra" do not senesce (do not age), and that they are proof of the existence of non-senescing organisms generally. In 2010 Preston Estep published (also in "Experimental Gerontology") a letter to the editor arguing that the Martinez data support rather than refute the hypothesis that "Hydra" senesce.
The controversial unlimited life span of "Hydra" has attracted the attention of natural scientists for a long time. Research today appears to confirm Martinez' study. "Hydra" stem cells have a capacity for indefinite self-renewal. The transcription factor, "forkhead box O" (FoxO) has been identified as a critical driver of the continuous self-renewal of "Hydra". A drastically reduced population growth resulted from FoxO down-regulation, so research findings do contribute to both a confirmation and an understanding of "Hydra" immortality.
While "Hydra" immortality is well-supported today, the implications for human aging are still controversial. There is much optimism; however, it appears that researchers still have a long way to go before they are able to understand how the results of their work might apply to the reduction or elimination of human senescence.
Genomics.
A draft of the genome of "Hydra magnipapillata" was reported in 2010.

</doc>
<doc id="13768" url="http://en.wikipedia.org/wiki?curid=13768" title="Hydrus">
Hydrus

Hydrus is a small constellation in the deep southern sky. It was first depicted on a celestial atlas by Johann Bayer in his 1603 "Uranometria". The French explorer and astronomer Nicolas Louis de Lacaille charted the brighter stars and gave their Bayer designations in 1756. Its name means "male water snake", as opposed to Hydra, a much larger constellation that represents a female water snake. It remains below the horizon for most Northern Hemisphere observers.
The brightest star is the 2.8-magnitude Beta Hydri, also the closest reasonably bright star to the south celestial pole. Pulsating between magnitude 3.26 and 3.33, Gamma Hydri is a variable red giant some 60 times the diameter of our Sun. Lying near it is VW Hydri, one of the brightest dwarf novae in the heavens. Four star systems have been found to have exoplanets to date, most notably HD 10180, which could bear up to nine planetary companions.
History.
Hydrus was one of the twelve constellations established by the Dutch astronomer Petrus Plancius from the observations of the southern sky by the Dutch explorers Pieter Dirkszoon Keyser and Frederick de Houtman, who had sailed on the first Dutch trading expedition, known as the "Eerste Schipvaart", to the East Indies. It first appeared on a 35-cm (14 in) diameter celestial globe published in 1598 in Amsterdam by Plancius with Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in the German cartographer Johann Bayer's "Uranometria" of 1603. De Houtman included it in his southern star catalogue the same year under the Dutch name "De Waterslang", "The Water Snake", it representing a type of snake encountered on the expedition rather than a mythical creature. The French explorer and astronomer Nicolas Louis de Lacaille called it "l’Hydre Mâle" on the 1756 version of his planisphere of the southern skies, distinguishing it from the feminine Hydra. The French name was retained by Jean Fortin in 1776 for his "Atlas Céleste", while Lacaille Latinised the name to Hydrus for his revised "Coelum Australe Stelliferum" in 1763.
Characteristics.
Irregular in shape, Hydrus is bordered by Mensa to the southeast, Eridanus to the east, Horologium and Reticulum to the northeast, Phoenix to the north, Tucana to the northwest and west, and Octans to the south; Lacaille had shortened Hydrus' tail to make space for this last constellation he had drawn up. Covering 243 square degrees and 0.589% of the night sky, it ranks 61st of the 88 constellations in size. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Hyi'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 12 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between 00h 06.1m and 04h 35.1m, while the declination coordinates are between −57.85° and −82.06°. As one of the deep southern constellations, it remains below the horizon at latitudes north of the 30th parallel in the Northern Hemisphere, and is circumpolar at latitudes south of the 50th parallel in the Southern Hemisphere. Indeed, Herman Melville mentions it and Argo Navis in Moby Dick "beneath effulgent Antarctic Skies", highlighting his knowledge of the southern constellations from whaling voyages. A line drawn between the long axis of the Southern Cross to Beta Hydri and then extended 4.5 times will mark a point due south. Hydrus culminates at midnight around the 26th of October.
Notable features.
Stars.
Keyzer and de Houtman assigned 15 stars to the constellation in their Malay and Madagascan vocabulary, with a star that would be later designated as Alpha Hydri marking the head, Gamma the chest and a number of stars that were later allocated to Tucana, Reticulum, Mensa and Horologium marking the body and tail. Lacaille charted and designated 20 stars with the Bayer designations Alpha through to Tau in 1756. Of these, he used the designations Eta, Pi and Tau twice each, for three sets of two stars close together, and omitted Omicron and Xi. He assigned Rho to a star that subsequent astronomers were unable to find.
Beta Hydri, the brightest star in Hydrus, is a yellow star of apparent magnitude 2.8, lying 24 light-years from Earth. It has about 104% of the mass of the Sun and 181% of the Sun's radius, with more than three times the Sun's luminosity. The spectrum of this star matches a stellar classification of G2 IV, with the luminosity class of 'IV' indicating this is a subgiant star. As such, it is a slightly more evolved star than the Sun, with the supply of hydrogen fuel at its core becoming exhausted. It is the nearest subgiant star to the Sun and one of the oldest stars in the solar neighbourhood. Thought to be between 6.4 and 7.1 billion years old, this star bears some resemblance to what the Sun may look like in the far distant future, making it an object of interest to astronomers. It is also the closest bright star to the south celestial pole.
Located at the northern edge of the constellation and just southwest of Achernar is Alpha Hydri, a white sub-giant star of magnitude 2.9, situated 72 light-years from Earth. Of spectral type F0IV, it is beginning to cool and enlarge as it uses up its supply of hydrogen. It is twice as massive and 3.3 times as wide as our sun and 26 times more luminous. A line drawn between Alpha Hydri and Beta Centauri is bisected by the south celestial pole.
In the southeastern corner of the constellation is Gamma Hydri, a red giant of spectral type M2III located 214 light-years from Earth. It is a semi-regular variable star, pulsating between magnitudes 3.26 and 3.33. Observations over five years were not able to establish its periodicity. An ageing star, it is around 1.5 to 2 times as massive as our Sun, yet has expanded to have about 60 times the Sun's diameter. It shines with about 655 times the luminosity of our Sun. Located 3° northeast of Gamma is the VW Hydri, a dwarf nova of the SU Ursae Majoris type. It is a close binary system that consists of a white dwarf and other star, the former drawing off matter from the latter into a bright accretion disk. These systems are characterised by frequent eruptions and less frequent supereruptions. The former are smooth, while the latter exhibit short "superhumps" of heightened activity. One of the brightest dwarf novae in the sky, it has a baseline magnitude of 14.4 and can brighten to magnitude 8.4 during peak activity. BL Hydri is another close binary system composed of a low mass star and a strongly magnetic white dwarf. Known as a polar or AM Herculis variable, these produce polarized optical and infrared emissions and intense soft and hard X-ray emissions to the frequency of the white dwarf's rotation period—in this case 113.6 minutes.
There are two notable optical double stars in Hydrus. Pi Hydri, composed of Pi1 Hydri and Pi2 Hydri, is divisible in binoculars. Around 476 light-years distant, Pi1 is a red giant of spectral type M1III that varies between magnitudes 5.52 and 5.58. Pi2 is an orange giant of spectral type K2III and shining with a magnitude of 5.7, around 488 light-years from Earth.
Eta Hydri is the other optical double, composed of Eta1 and Eta2. Eta1 is a blue-white main sequence star of spectral type B9V that was suspected of being variable, and is located just over 700 light-years away. Eta2 has a magnitude of 4.7 and is a yellow giant star of spectral type G8.5III around 218 light-years distant, which has evolved off the main sequence and is expanding and cooling on its way to becoming a red giant. Calculations of its mass indicate it was most likely a white A-type main sequence star for most of its existence, around twice the mass of our Sun. A planet, Eta2 Hydri b, greater than 6.5 times the mass of Jupiter was discovered in 2005, orbiting around Eta2 every 711 days at a distance of 1.93 astronomical units (AU).
Three other systems have been found to have planets, most notably the Sun-like star HD 10180, which has seven planets, plus possibly an additional two for a total of nine—as of 2012 more than any other system to date, including our own solar system. Lying around 127 ly from the Earth, it has an apparent magnitude of 7.33.
GJ 3021 is a solar twin—a star very like our own Sun—around 57 light-years distant with a spectral type G8V and magnitude of 6.7. It has a Jovian planet companion (GJ 3021 b). Orbiting about 0.5 AU from its sun, it has a minimum mass 3.37 times that of Jupiter and a period of around 133 days. The system is a complex one as the faint star GJ 3021B orbits at a distance of 68 AU; it is a red dwarf of spectral type M4V.
HD 20003 is a star of magnitude 8.37. It is a yellow main sequence star of spectral type G8V a little cooler and smaller than our Sun around 143 light-years away. It has two planets that are around 12 and 13.5 times as massive as the Earth with periods of just under 12 and 34 days respectively.
Deep-sky objects.
Hydrus contains only faint deep-sky objects. IC 1717 was a deep-sky object discovered by the Danish astronomer John Louis Emil Dreyer in the late 19th century. However, the object at the coordinate Dreyer observed is no longer there, and is now a mystery. It was very likely to have been a faint comet. Known as the white rose galaxy, PGC 6240 is a giant spiral galaxy surrounded by shells resembling rose petals, located around 345 million light years from our Solar System. Unusually, it has cohorts of globular clusters of three distinct ages suggesting bouts of post-starburst formation following a merger with another galaxy. The constellation also contains a spiral galaxy NGC 1511 that lies edge on to observers on Earth and is readily viewed in amateur telescopes.
Located mostly in Dorado, the Large Magellanic Cloud extends into Hydrus. The globular cluster NGC 1466 is an outlying component of the galaxy, and contains many RR Lyrae-type variable stars. It has a magnitude of 11.59 and is thought to be over 12 billion years old. Two stars, HD 24188 of magnitude 6.3 and HD 24115 of magnitude 9.0, lie nearby in its foreground. NGC 602 is composed of an emission nebula and a young, bright open cluster of stars that is an outlying component on the eastern edge of the Small Magellanic Cloud, a satellite galaxy to the Milky Way. Most of the cloud is located in the neighbouring constellation Tucana.
External links.
Coordinates: 

</doc>
<doc id="13770" url="http://en.wikipedia.org/wiki?curid=13770" title="Hercules">
Hercules

Hercules is the Roman name for the Greek divine hero Heracles, who was the son of Zeus (Roman equivalent Jupiter) and the mortal Alcmene. In classical mythology, Hercules is famous for his strength and for his numerous far-ranging adventures.
The Romans adapted the Greek hero's iconography and myths for their literature and art under the name "Hercules". In later Western art and literature and in popular culture, "Hercules" is more commonly used than "Heracles" as the name of the hero. Hercules was a multifaceted figure with contradictory characteristics, which enabled later artists and writers to pick and choose how to represent him. This article provides an introduction to representations of Hercules in the later tradition.
Labours.
Hercules is known for his many adventures, which took him to the far reaches of the Greco-Roman world. One cycle of these adventures became canonical as the "Twelve Labours," but the list has variations. One traditional order of the labours is found in the "Bibliotheca" as follows:
Side adventures.
Hercules had a greater number of "deeds on the side" "(parerga)" that have been popular subjects for art, including:
Roman era.
The Latin name "Hercules" was borrowed through Etruscan, where it is represented variously as Heracle, Hercle, and other forms. Hercules was a favorite subject for Etruscan art, and appears often on bronze mirrors. The Etruscan form "Herceler" derives from the Greek "Heracles" via syncope. A mild oath invoking Hercules ("Hercule!" or "Mehercle!") was a common interjection in Classical Latin.
Hercules had a number of myths that were distinctly Roman. One of these is Hercules' defeat of Cacus, who was terrorizing the countryside of Rome. The hero was associated with the Aventine Hill through his son Aventinus. Mark Antony considered him a personal patron god, as did the emperor Commodus. Hercules received various forms of religious veneration, including as a deity concerned with children and childbirth, in part because of myths about his precocious infancy, and in part because he fathered countless children. Roman brides wore a special belt tied with the "knot of Hercules", which was supposed to be hard to untie. The comic playwright Plautus presents the myth of Hercules' conception as a sex comedy in his play "Amphitryon"; Seneca wrote the tragedy "Hercules Furens" about his bout with madness. During the Roman Imperial era, Hercules was worshipped locally from Hispania through Gaul.
Germanic association.
Tacitus records a special affinity of the Germanic peoples for Hercules. In chapter 3 of his "Germania", Tacitus states:
"... they say that Hercules, too, once visited them; and when going into battle, they sang of him first of all heroes. They have also those songs of theirs, by the recital of this "barditus" as they call it, they rouse their courage, while from the note they augur the result of the approaching conflict. For, as their line shouts, they inspire or feel alarm."
Some have taken this as Tacitus equating the Germanic "Þunraz" with Hercules by way of "interpretatio romana".
In the Roman era Hercules' Club amulets appear from the 2nd to 3rd century, distributed over the empire (including Roman Britain, c.f. Cool 1986), mostly made of gold, shaped like wooden clubs. A specimen found in Köln-Nippes bears the inscription [culi]", confirming the association with Hercules.
In the 5th to 7th centuries, during the Migration Period, the amulet is theorized to have rapidly spread from the Elbe Germanic area across Europe. These Germanic "Donar's Clubs" were made from deer antler, bone or wood, more rarely also from bronze or precious metals.They are found exclusively in female graves, apparently worn either as a belt pendant, or as an ear pendant. The amulet type is replaced by the Viking Age Thor's hammer pendants in the course of the Christianization of Scandinavia from the 8th to 9th century.
Medieval mythography.
After the Roman Empire became Christianized, mythological narratives were often reinterpreted as allegory, influenced by the philosophy of late antiquity. In the 4th century, Servius had described Hercules' return from the underworld as representing his ability to overcome earthly desires and vices, or the earth itself as a consumer of bodies. In medieval mythography, Hercules was one of the heroes seen as a strong role model who demonstrated both valor and wisdom, with the monsters he battles as moral obstacles. One glossator noted that when Hercules became a constellation, he showed that strength was necessary to gain entrance to Heaven.
Medieval mythography was written almost entirely in Latin, and original Greek texts were little used as sources for Hercules' myths.
Renaissance mythography.
The Renaissance and the invention of the printing press brought a renewed interest in and publication of Greek literature. Renaissance mythography drew more extensively on the Greek tradition of Heracles, typically under the Romanized name Hercules, or the alternate name Alcides. In a chapter of his book "Mythologiae" (1567), the influential mythographer Natale Conti collected and summarized an extensive range of myths concerning the birth, adventures, and death of the hero under his Roman name Hercules. Conti begins his lengthy chapter on Hercules with an overview description that continues the moralizing impulse of the Middle Ages:
Hercules, who subdued and destroyed monsters, bandits, and criminals, was justly famous and renowned for his great courage. His great and glorious reputation was worldwide, and so firmly entrenched that he'll always be remembered. In fact the ancients honored him with his own temples, altars, ceremonies, and priests. But it was his wisdom and great soul that earned those honors; noble blood, physical strength, and political power just aren't good enough.
In art.
In Roman works of art and in Renaissance and post-Renaissance art, Hercules can be identified by his attributes, the lion skin and the gnarled club (his favorite weapon); in mosaic he is shown tanned bronze, a virile aspect.
In numismatics.
Hercules was among the earliest figures on ancient Roman coinage, and has been the main motif of many collector coins and medals since. One example is the 20 euro Baroque Silver coin issued on September 11, 2002. The obverse side of the coin shows the Grand Staircase in the town palace of Prince Eugene of Savoy in Vienna, currently the Austrian Ministry of Finance. Gods and demi-gods hold its flights, while Hercules stands at the turn of the stairs.
In films.
A series of nineteen Italian Hercules movies were made in the late 1950s and early 1960s. The actors who played Hercules in these films were Steve Reeves, Gordon Scott, Kirk Morris, Mickey Hargitay, Mark Forest, Alan Steel, Dan Vadis, Brad Harris, Reg Park, Peter Lupus (billed as Rock Stevens) and Michael Lane. A number of English-dubbed Italian films that featured the name of Hercules in their title were not intended to be movies about Hercules.

</doc>
<doc id="13772" url="http://en.wikipedia.org/wiki?curid=13772" title="History of Poland">
History of Poland

The history of Poland is rooted in the migrations of Slavs who gave rise to permanent settlement and historic development on Polish lands during the Dark Ages. In 966 AD, under the Piast dynasty, the Poles adopted Christianity and a medieval monarchy was established. The period of the Jagiellonian dynasty in the 14th-16th centuries brought close ties with the Grand Duchy of Lithuania, a cultural Renaissance in Poland and territorial expansion that culminated in the establishment of the Polish–Lithuanian Commonwealth in 1569.
The Commonwealth in its early phase constituted a continuation of Jagiellonian prosperity, with its remarkable development of a sophisticated noble democracy. From the mid-17th century, the huge state entered a period of decline caused by devastating wars and the deterioration of the country's political system. Significant internal reforms were introduced during the later part of the 18th century, especially in the Constitution of May 3, 1791, but the reform process was not allowed to run its course, as the Russian Empire, the Kingdom of Prussia, and the Austrian Habsburg Monarchy terminated the Commonwealth's independent existence in 1795 after a series of invasions and partitions of Polish territory .
From 1795 until 1918 there was no independent Polish state, although there was a strong Polish resistance movement until 1864. After the failure of the last uprising against the Russian Empire, the January Uprising of 1863-65, the nation preserved its identity through educational initiatives and the program of "organic work" intended to modernize the economy and society. The opportunity to regain freedom appeared only after World War I, when the partitioning imperial powers were dissolved by war and revolution.
The Second Polish Republic was established and existed from 1918 to 1939. It was destroyed by Nazi Germany and the Soviet Union in their Invasion of Poland at the beginning of World War II. Millions of Polish citizens perished in the course of the Nazi occupation of Poland (1939–45) as Germany classified ethnic Poles and other Slavs, Jews and Romani (Gypsies) as subhuman and targeted the latter two groups for extermination in the short term (whereas the extermination and/or enslavement of the Slavs was to be completed later). A Polish government in exile nonetheless functioned throughout the war and the Poles were able to contribute to the Allied victory through participation in military formations on both the western and eastern fronts. Nazi Germany's forces were compelled to retreat from Poland as the Soviet Red Army advanced, which led to the creation of the communist Polish People's Republic, a satellite state of the Soviet Union.
The country's geographic location was shifted to the west, and it largely lost its traditional multi-ethnic character through the extermination, expulsion and migration of the various nationalities during and after World War II. By the late 1980s, the Polish reform movement Solidarity became crucial in bringing about a peaceful transition from a communist state to the capitalist economic system and liberal parliamentary democracy. This process resulted in the creation of the modern Polish state.
Prehistory and protohistory.
Members of the "Homo" genus have lived in north Central Europe for thousands of years since its environment was altered by prehistoric glaciation. In prehistoric and protohistoric times, over a period of at least 500,000 years, the area of present-day Poland went through the Stone Age, Bronze Age and Iron Age stages of development, along with the nearby regions. The Neolithic period ushered in the Linear Pottery culture, whose founders migrated from the Danube River area beginning about 5,500 BC. This culture was distinguished by the establishment of the first settled agricultural communities in modern Polish territory. Later, between about 4,400 and 2,000 BC, the native post-Mesolithic populations would also adopt and further develop the agricultural way of life.
Poland's Early Bronze Age began around 2300–2400 BC, whereas its Iron Age commenced ca. 700–750 BC. One of the many cultures that have been uncovered, the Lusatian culture, spanned the Bronze and Iron Ages and left notable settlement sites. Around 400 BC, Poland was settled by La Tène culture Celtic arrivals. They were soon followed by emerging cultures with a strong Germanic component, influenced first by the Celts and then by the Roman Empire. The Germanic peoples migrated out of the area by about 500 AD during the great Migration Period of the European Dark Ages. Wooded regions to the north and east were settled by Balts.
The Slavs have resided in modern Polish territories for over 1500 years. They organized into tribal units, of which the larger ones were later known as the Polish tribes; the names of many tribes are found on the list compiled by the anonymous Bavarian Geographer in the 9th century. In the 9th and 10th centuries, these tribes gave rise to developed regions along the upper Vistula, the coast of the Baltic Sea and in Greater Poland. This latest tribal undertaking resulted in the formation of a lasting political structure in the 10th century that became the state of Poland, one of the West Slavic nations.[x]
Piast period (10th century–1385).
Mieszko I.
Poland was established as a nation state under the Piast dynasty, which ruled the country between the 10th and 14th centuries. Historical records of an official Polish state begin with Duke Mieszko I in the second half of the 10th century. Mieszko, who began his rule sometime before 963 and continued as the Polish monarch until his death in 992, chose to be baptized in the Western Latin Rite, probably on 14 April 966, following his marriage to Princess Dobrawa of Bohemia. This event became known as the baptism of Poland, and its date is often used to mark the beginning of Polish statehood symbolically. Mieszko completed a unification of the West Slavic tribal lands that was fundamental to the new country's existence. The "Dagome iudex", a document from the year 991 AD, places Mieszko's country under the protection of the pope. Following its emergence, the Polish nation was led by a series of rulers who converted the population to Christianity, created a strong Kingdom of Poland and fostered a distinctive Polish culture that was integrated into broader European culture.
Bolesław I Chrobry.
Mieszko's son, Duke Bolesław I Chrobry (r. 992–1025), established a Polish Church structure, pursued territorial conquests and was officially crowned the first King of Poland in 1025, near the end of his life. Bolesław also sought to spread Christianity to parts of eastern Europe that remained pagan, but suffered a setback when his greatest missionary, Adalbert of Prague, was killed in Prussia in 997. During the Congress of Gniezno in the year 1000, Holy Roman Emperor Otto III recognized the Archbishopric of Gniezno, an institution crucial for the continuing existence of the sovereign Polish state. During the reign of Otto's successor, Holy Roman Emperor Henry II, Bolesław fought prolonged wars with the Kingdom of Germany between 1002 and 1018.
Piast monarchy under Casimir I, Bolesław II and Bolesław III.
Bolesław's expansive rule overstretched the military resources of the early Polish state, and it was followed by a collapse of the monarchy. Restoration took place under Casimir I (r. 1039–58). Casimir's son Bolesław II the Bold (r. 1058–79) became involved in a conflict with Bishop Stanislaus of Szczepanów that seriously marred his reign. Bolesław had the bishop murdered in 1079 after being excommunicated by the Polish church on charges of adultery. This act sparked a revolt of Polish nobles that led to Bolesław's deposition and expulsion from the country. Around 1116, Gallus Anonymous wrote a seminal chronicle, the "Gesta principum Polonorum", intended as a glorification of his patron Bolesław III Wrymouth (r. 1107–38), a ruler who revived the tradition of military prowess of Bolesław I's time. Gallus' work became important as a key source for the early history of Poland.
Fragmentation.
After Bolesław III divided Poland among his sons in his Testament of 1138, internal fragmentation eroded the Piast monarchical structures in the 12th and 13th centuries. In 1180, Casimir II, who sought papal confirmation of his status as a senior duke, granted immunities and additional privileges to the Polish Church at the Congress of Łęczyca. Around 1220, Wincenty Kadłubek wrote his "Chronica seu originale regum et principum Poloniae", another major source for early Polish history. In 1226, one of the regional Piast dukes, Konrad I of Masovia, invited the Teutonic Knights to help him fight the Baltic Prussian pagans. Konrad's move caused centuries of warfare between Poland and the Teutonic Knights, and later between Poland and the German Prussian state. The first Mongol invasion of Poland began in 1240; it culminated in the defeat of Polish and allied Christian forces and the death of the Silesian Piast Duke Henry II at the Battle of Legnica in 1241. In 1242, Wrocław became the first Polish municipality to be incorporated, as the period of fragmentation brought economic development and growth of towns. In 1264, Bolesław the Pious granted Jewish liberties in the Statute of Kalisz.
Late Piast monarchy under Władysław I and Casimir III.
Attempts to reunite the Polish lands gained momentum in the 13th century, and in 1295, Duke Przemysł II of Greater Poland managed to become the first ruler since Bolesław II to be crowned king of Poland. He ruled over a limited territory and was soon killed. In 1300–05 the Czech ruler Václav II also reigned as king of Poland. The Piast Kingdom was effectively restored under Władysław I the Elbow-high (r. 1306–33), who was crowned king in 1320. In 1308, the Teutonic Knights seized Gdańsk and the surrounding region (Pomerelia).
King Casimir III the Great (r. 1333–70), Władysław's son and the last of the Piast rulers, strengthened and expanded the restored Kingdom of Poland, but the western provinces of Silesia (formally ceded by Casimir in 1339) and most of Pomerania were lost to the Polish state for centuries to come. Progress was made in the recovery of the central province of Mazovia, however, and in 1340, the conquest of Red Ruthenia began, marking Poland's expansion to the east. The Congress of Kraków, a vast convocation of central, eastern, and northern European rulers probably assembled to plan an anti-Turkish crusade, took place in 1364, the same year that the future Jagiellonian University, one of the oldest European universities, was founded.
Angevin transition.
After the Polish royal line and Piast junior branch died out in 1370, Poland came under the rule of Louis I of Hungary of the Angevin dynasty, who presided over a union of Hungary and Poland that lasted until 1382. In 1374, Louis granted the Polish nobility the Privilege of Koszyce to assure the succession of one of his daughters in Poland. His youngest daughter Jadwiga (d. 1399) assumed the Polish throne in 1384.
Jagiellonian dynasty (1385–1572).
Dynastic union with Lithuania, Władysław II Jagiełło.
In 1386, Grand Duke Jogaila of Lithuania became also a king of Poland, to rule as Władysław II Jagiełło until 1434. The act established a Polish–Lithuanian union ruled by the Jagiellonian dynasty. The first in a series of formal "unions" was the Union of Krewo of 1385, whereby arrangements were made for the marriage of Jogaila and Queen Jadwiga. The Polish-Lithuanian partnership brought vast areas of Ruthenia controlled by the Grand Duchy of Lithuania into Poland's sphere of influence and proved beneficial for the nationals of both countries, who coexisted and cooperated in one of the largest political entities in Europe for the next four centuries (also after the extinction of the Jagellonian dynasty in 1572). When Queen Jadwiga died in 1399, the Kingdom of Poland fell to her husband's sole possession; her gifts helped to renew the activities of the University in 1400.
In the Baltic Sea region, Poland's struggle with the Teutonic Knights continued and culminated in the Battle of Grunwald (1410), a great victory that the Poles and Lithuanians were unable to follow up with a decisive strike against the main seat of the Order at Malbork Castle. The Union of Horodło of 1413 further defined the evolving relationship between the Crown of Poland and the Grand Duchy of Lithuania and their elites.
The privileges of the "szlachta" (nobility) kept growing and in 1425 the rule of "Neminem captivabimus", which protected the noblemen from arbitrary royal arrests, was formulated.
Władysław III and Casimir IV Jagiellon.
The reign of the young Władysław III (1434–44), a king of Poland and Hungary, was cut short by his death at the Battle of Varna fought against the forces of the Ottoman Empire.
Critical developments of the Jagiellonian period were concentrated in the long reign of Casimir IV Jagiellon (1447–92). In 1454, Royal Prussia was incorporated by Poland and the Thirteen Years' War of 1454–66 with the Teutonic state ensued. In 1466, the milestone Peace of Thorn was concluded. This treaty divided Prussia to create East Prussia, the future Duchy of Prussia, a separate entity that functioned as a fief of Poland under the administration of the Teutonic Knights. Poland also confronted the Ottoman Empire and the Crimean Tatars in the south, and in the east helped Lithuania fight the Grand Duchy of Moscow. The country was developing as a feudal state, with a predominantly agricultural economy and an increasingly dominant landed nobility. Kraków, the royal capital, was turning into a major academic and cultural center, and in 1473 the first printing press began operating there. With the growing importance of the "szlachta", the king's council evolved to become by 1493 a bicameral general sejm (parliament) that no longer represented only the top dignitaries of the realm.
The "Nihil novi" act, adopted in 1505 by the Sejm, transferred most of the legislative power from the monarch to the Sejm. This event marked the beginning of the period known as "Golden Liberty", when the state was ruled in principle by the "free and equal" Polish nobility. In the 16th century, the massive development of folwark agribusinesses operated by the nobility led to increasingly abusive conditions for the peasant serfs who worked them. The political monopoly of the nobles also stifled the development of cities, some of which were thriving during the late Jagiellonian era, and limited the rights of townspeople, effectively holding back the emergence of a middle class.
Early modern Poland under Sigismund I and Sigismund II.
Protestant Reformation movements made deep inroads into Polish Christianity and the resulting Reformation in Poland phenomenon involved a number of different denominations. The policies of religious tolerance that developed were nearly unique in Europe at that time and many who fled regions torn by religious strife found refuge in Poland. The reigns of King Sigismund I and King Sigismund II Augustus witnessed an intense cultivation of culture and science (a Golden Age of the Renaissance in Poland), of which the astronomer Nicolaus Copernicus (d. 1543) is the best known representative. In 1525, during the reign of Sigismud I (1506–48), the Teutonic Order was secularized and Duke Albrecht von Hohenzollern performed an act of homage before the Polish king (the Prussian Homage) for his fief, the Duchy of Prussia. Mazovia was finally fully incorporated into the Polish Crown in 1529.
The reign of Sigismund II (1548–72) ended the Jagiellonian period, but gave rise to the Union of Lublin (1569), the ultimate fulfillment of the union with Lithuania. This agreement transferred Ukraine from the Grand Duchy of Lithuania to Poland and transformed the Polish-Lithuanian polity into a real union, preserving it beyond the death of the childless Sigismund II, whose active involvement made the completion of this process possible.
Livonia in the far northeast was incorporated by Poland in 1561 and Poland entered the Livonian War against Russia. The executionist movement (an attempt to prevent domination by the magnate families of Poland and Lithuania) peaked at the sejm in Piotrków in 1562–63. On the religious front, the Polish Brethren split from the Calvinists, and the Protestant Brest Bible was published in 1563. The Jesuits, who arrived in 1564, were destined to make a major impact on Poland's history.
Polish–Lithuanian Commonwealth.
Establishment (1569–1648).
Union of Lublin.
The Union of Lublin of 1569 established the Polish–Lithuanian Commonwealth, a more closely unified federal state than the earlier political arrangement between Poland and Lithuania. The Union was largely run by the nobility through the system of a central parliament and local assemblies, but was led by elected kings. The formal rule of the nobility, who were proportionally more numerous than in other European countries, constituted an early democratic system ("a sophisticated noble democracy"), in contrast to the absolute monarchies prevalent at that time in the rest of Europe. The beginning of the Commonwealth coincided with a period in Polish history of great political power, advancements in civilization and prosperity. The Polish–Lithuanian Union became an influential player in Europe and a vital cultural entity that spread Western culture (with Polish characteristics) eastward. In the second half of the 16th century and the first half of the 17th century, the Commonwealth was one of the largest and most populous states in contemporary Europe, with an area approaching one million square kilometres and a population of about ten million. Its economy was dominated by export-focused agriculture. Nationwide religious toleration was guaranteed at the Warsaw Confederation in 1573.
First elective kings.
After the rule of the Jagiellonian dynasty had ended, Henry of Valois (later King Henry III of France) was the winner of the first "free election" by the Polish nobility in 1573. He had to agree to the restrictive "pacta conventa" obligations, but soon fled Poland when news arrived of the vacancy of the French throne, to which he was the heir presumptive. From the start, the royal elections increased foreign influence in the Commonwealth as foreign powers sought to manipulate the Polish nobility to place candidates amicable to their interests.
The reign of Stephen Báthory of Hungary (1576–86) followed; he was militarily and domestically assertive. The establishment of the legal Crown Tribunal in 1578 meant a transfer of many appellate cases from the royal to noble jurisdiction. Jan Kochanowski, a poet and the premier artistic personality of the Polish Renaissance, died in 1584.
Vasa dynasty kings.
The Commonwealth suffered from dynastic distractions (the Vasa kings unsuccessfully attempted to obtain the Swedish crown and prioritized this activity) during the reigns of the Swedish House of Vasa kings Sigismund III (1587–1632) and Władysław IV (1632–48). The Catholic Church embarked on an ideological counter-offensive and the Counter-Reformation claimed many converts from Polish and Lithuanian Protestant circles. In 1596, the Union of Brest split the Eastern Christians of the Commonwealth to create the Uniate Church of the Eastern Rite, but subject to the authority of the pope. The Zebrzydowski Rebellion against Sigismund III unfolded in 1606–8.
The Commonwealth fought wars between 1605 and 1618 with Russia for supremacy in Eastern Europe in the wake of Russia's Time of Troubles, a period referred to as the Polish–Muscovite War (or the "Dymitriads"). The efforts resulted in expansion of the eastern territories of the Polish–Lithuanian Commonwealth, but the goal of taking over the Russian throne for the Polish ruling dynasty was not achieved. Sweden sought supremacy in the Baltic during the Polish–Swedish wars of 1617–29, and the Ottoman Empire pressed from the south in the Battles at Cecora in 1620 and Khotyn in 1621. The agricultural expansion and serfdom policies in Polish Ukraine resulted in a series of Cossack uprisings. Allied with the Habsburg Monarchy, the Commonwealth did not directly participate in the Thirty Years' War.[s] Władysław's IV reign was mostly peaceful, with a Russian invasion in the form of the Smolensk War of 1632–34 successfully repelled. The Orthodox Church hierarchy, banned in Poland after the Union of Brest, was re-established in 1635.
Decline (1648–1764).
Deluge of wars.
During the reign of John II Casimir Vasa (1648–68), the nobles' democracy fell into decline as a result of foreign invasions and domestic disorder. These calamities multiplied rather suddenly and marked the end of the Polish Golden Age. Their effect was to render the once powerful Commonwealth increasingly vulnerable to foreign intervention.
The Cossack Khmelnytsky Uprising of 1648–57, a nationwide attempt to liberate Ukraine, engulfed the south-eastern regions of the Polish crown; its long-term effects were disastrous for the Commonwealth. The first "liberum veto" (a parliamentary device that allowed any member of the Sejm to dissolve a current session immediately) was exercised by a deputy in 1652. This practice would eventually weaken Poland's central government critically. In the Treaty of Pereyaslav (1654), the Ukrainian rebels declared themselves subjects of the Tsar of Russia. The Second Northern War raged through the core Polish lands in 1655–60, including an invasion of Poland so brutal and devastating that it is referred to as the Swedish Deluge. The war ended in 1660 with the Treaty of Oliva, which resulted in the loss of some of Poland's northern possessions. In 1657 the Treaty of Wehlau-Bromberg established the independence of the Duchy of Prussia. The Commonwealth forces did well in the Russo-Polish War of 1654–67, but the end result was the permanent division of Ukraine between Poland and Russia, as agreed to in the Truce of Andrusovo (1667). Towards the end of the war, the Rokosz of Lubomirski, a major magnate rebellion against the king, destabilized and weakened the country. The large-scale slave raids of the Crimean Tatars also had highly deleterious effects on the Polish economy. "Merkuriusz Polski", the first Polish newspaper, was published in 1661.
John III Sobieski and last military victories.
The Second Polish–Ottoman War (1672–76) broke out during the reign of King Michał Korybut Wiśniowiecki (1669–73) and continued under his successor, John III Sobieski (1674–96). Sobieski intended to pursue Baltic area expansion (and to this end he signed the secret Treaty of Jaworów with France in 1675), but was forced instead to fight protracted wars with the Ottoman Empire. By doing so the hetman who became king briefly revived the Commonwealth's military might. He defeated the expanding Muslims at the Battle of Khotyn in 1673 and decisively helped deliver Vienna from a Turkish onslaught at the Battle of Vienna in 1683. Sobieski's reign marked the last high point in the history of the Commonwealth: in the first half of the 18th century Poland ceased to be an active player in international politics. The Eternal Peace Treaty with Russia of 1686 was the final border settlement between the two countries before the First Partition of Poland in 1772.
The Commonwealth, subjected to almost constant warfare until 1720, suffered enormous population losses and massive damage to its economy and social structure. The government became ineffective in the wake of large-scale internal conflicts, corrupted legislative processes and manipulation by foreign interests. The nobility fell under the control of a handful of feuding magnate families with established territorial domains. The urban population and infrastructure fell into ruin, together with most peasant farms, whose inhabitants were subjected to increasingly extreme forms of serfdom. The development of science, culture and education came to a halt or regressed.
Saxon kings.
The royal election of 1697 brought a ruler of the Saxon House of Wettin to the Polish throne: Augustus II, "the Strong" (r. 1697–1733), who was able to assume the throne only by agreeing to convert to Roman Catholicism. He was succeeded eventually by his son Augustus III (r. 1734–63). The reigns of the Saxon kings (who were both simultaneously prince-electors of Saxony) were disrupted by competing candidates for the throne and witnessed further disintegration of the Commonwealth. The Great Northern War (1700–21), a period seen by the contemporaries as a temporary eclipse, may have been the fatal blow that brought down the Polish political system. Stanisław Leszczyński was installed as king in 1704 under Swedish protection, but lasted only a few years. The Silent Sejm of 1717 marked the beginning of the Commonwealth's existence as a Russian protectorate: the Tsardom would guarantee the reform-impeding Golden Liberty of the nobility from that time on in order to cement the Commonwealth's weak central authority and a state of perpetual political impotence. In a resounding break with traditions of religious tolerance, Protestants were executed during the Tumult of Thorn in 1724. In 1732 Russia, Austria and Prussia, Poland's three increasingly powerful and scheming neighbors, entered into the secret Treaty of the Three Black Eagles with the intention of controlling the future royal succession in the Commonwealth. The War of the Polish Succession was fought in 1733–35 to assist Leszczyński in assuming the throne of Poland for a second time. Amidst considerable foreign involvement, his efforts were unsuccessful. The Kingdom of Prussia became a strong regional power and succeeded in wresting the historically Polish province of Silesia from the Habsburg Monarchy in the Silesian Wars; it thus became an ever greater threat to Poland's security. The personal union between the Commonwealth and the Electorate of Saxony did give rise to the emergence of a reform movement in the Commonwealth and the beginnings of the Polish Enlightenment culture, the major positive developments of this era. The first Polish public library was the Załuski Library in Warsaw, opened to the public in 1747.
Reforms and loss of statehood (1764–95).
Czartoryski reforms and Stanisław August Poniatowski.
During the later part of the 18th century, fundamental internal reforms were attempted in the Polish–Lithuanian Commonwealth as it slid into extinction. The reform activity, initially promoted by the magnate Czartoryski family faction known as the "Familia", provoked a hostile reaction and eventually a military response on the part of neighboring powers – yet it created conditions that fostered economic improvement. The most populous urban center, the capital city of Warsaw, replaced Danzig (Gdańsk) as the leading trade center, and the importance of the more prosperous urban strata increased. The last decades of the independent Commonwealth's existence were characterized by intense reform movements and far-reaching progress in the areas of education, intellectual life, art, and, especially toward the end of the period, in the evolution of the social and political system.
The royal election of 1764 resulted in the elevation of Stanisław August Poniatowski, a refined and worldly aristocrat connected to the Czartoryski family, but hand-picked and imposed by Empress Catherine the Great of Russia, who expected him to be her obedient follower. Stanisław August ruled the Polish–Lithuanian state until its dissolution in 1795. The king spent his reign torn between his desire to implement reforms necessary to save the failing state and the perceived necessity of remaining in a subordinate relationship to his Russian sponsors.
The Bar Confederation (1768–72) was a noble rebellion directed against Russia's influence in general and Stanisław August, who was seen as its representative, in particular. It was fought to preserve Poland's independence and the nobility's traditional interests. After several years, it was brought under control by forces loyal to the king and those of the Russian Empire.
Following the suppression of the Bar Confederation, at the instigation of Frederick the Great of Prussia, Poland was divided up among Prussia, Austria and Russia in 1772, with only a rump state remaining. In what became known as the First Partition of Poland, the outer provinces of the Commonwealth were seized by agreement among the country's three powerful neighbors. In 1773, the Partition Sejm "ratified" under duress the partition as a "fait accompli". However, it also established the Commission of National Education, a pioneering in Europe education authority often called the world's first ministry of education.
Great Sejm and May 3 Constitution.
The long-lasting Sejm convened by Stanisław August in 1788 is known as the Great Sejm, or "Four-Year" Sejm. Its landmark achievement was the passing of the Constitution of May 3, 1791, the first singular pronouncement of a supreme law of the state in modern Europe, also characterized as the world's third oldest constitution. A reformist but moderate document condemned by detractors as being of French revolutionary sympathies, it soon generated strong opposition from the conservative circles of the Commonwealth's upper nobility and the Russian Empress Catherine, who was determined to prevent the rebirth of a strong Commonwealth. The nobility's Targowica Confederation, formed in Russian imperial capital of Saint Petersburg, appealed to Catherine for help, and in May 1792, the Russian army entered the Commonwealth's territory. The Polish–Russian War of 1792, a defensive war fought by the forces of the Commonwealth against Russian invaders, ended when the Polish king, convinced of the futility of resistance, capitulated by joining the Targowica Confederation. The Confederation took over the government, but Russia and Prussia in 1793 arranged for the Second Partition of Poland, which left the country with a critically reduced territory that rendered it essentially incapable of an independent existence. The Commonwealth's Grodno Sejm of 1793, the last Sejm of its existence, was compelled to confirm the new partition.
Kościuszko Uprising and loss of independence.
Radicalized by recent events, Polish reformers (whether in exile or still resident in the reduced area remaining to the Commonwealth) were soon working on preparations for a national insurrection. Tadeusz Kościuszko, a popular general and a veteran of the American Revolution, was chosen as its leader. He returned from abroad and issued Kościuszko's proclamation in Kraków on March 24, 1794. It called for a national uprising under his supreme command as "naczelnik". Kościuszko emancipated many peasants in order to enroll them as "kosynierzy" in his army, but the hard-fought insurrection, despite widespread national support, proved incapable of generating the foreign assistance necessary for its success. In the end it was suppressed by the combined forces of Russia and Prussia, with Warsaw captured in November 1794 at the Battle of Praga. In 1795, a Third Partition of Poland was undertaken by all three of the partitioning powers (Russia, Prussia and Austria) as a final division of territory that resulted in the effective dissolution of the Polish–Lithuanian Commonwealth. The Polish king was escorted to Grodno, forced to abdicate, and retired to Saint Petersburg. Kościuszko, initially imprisoned, was allowed to emigrate to the United States in 1796.
The response of the Polish leadership to the last partition is a matter of historical debate. Literary scholars found that the dominant emotion of the first decade was despair that produced a moral desert ruled by violence and treason. On the other hand, historians have looked for signs of resistance to foreign rule. Apart from those who went into exile, the nobility took oaths of loyalty to their new rulers and served as officers in their armies.
Partitioned Poland.
Armed resistance (1795–1864).
Napoleonic wars.
Although no sovereign Polish state existed between 1795 and 1918, the idea of Polish independence was kept alive throughout the 19th century. There were a number of uprisings and other military conflicts against the partitioning powers. Military efforts after the partitions were first based on the alliances of Polish émigrés with post-revolutionary France. Jan Henryk Dąbrowski's Polish Legions fought in French campaigns outside of Poland between 1797 and 1802 in hopes that their involvement and contribution would be rewarded with the liberation of their Polish homeland. The Polish national anthem, "Poland Is Not Yet Lost," or "Dąbrowski's Mazurka," was written in praise of his actions by Józef Wybicki in 1797.
The Duchy of Warsaw, a small, semi-independent Polish state, was created in 1807 by Napoleon Bonaparte in the wake of his defeat of Prussia and the signing of the Peace of Tilsit with Emperor Alexander I of Russia. The Army of the Duchy of Warsaw, led by Józef Poniatowski, participated in numerous campaigns in alliance with France, including the successful Polish–Austrian War of 1809, which, combined with the outcomes of other theaters of the War of the Fifth Coalition, resulted in an enlargement of the Duchy's territory. The French invasion of Russia in 1812 and the German campaign of 1813 saw the Duchy's last military engagements. The Constitution of the Duchy of Warsaw abolished serfdom as a reflection of the ideals of the French Revolution, but it did not promote land reform.
Congress of Vienna.
After Napoleon's defeat, a new European order was established at the Congress of Vienna. Adam Czartoryski, a former close associate of Alexander I, became the leading advocate for the Polish national cause. The Congress implemented a new partition scheme, which took into account some of the gains realized by the Poles during the Napoleonic period. The Duchy of Warsaw was replaced in 1815 with a new Kingdom of Poland, unofficially known as Congress Poland. The residual Polish kingdom was joined to the Russian Empire in a personal union under the Russian tsar, and it was allowed its own constitution and military. East of the Kingdom, large areas of the former Polish–Lithuanian Commonwealth remained directly incorporated into the Russian Empire as the Western Krai; both these territories are generally referred to as the Russian Partition. The Russian, Prussian, and Austrian "partitions" were the lands of the former Commonwealth, not actual units of its administrative division. The Prussian Partition was formed from territoriies acquired from Poland and included a portion separated as the Grand Duchy of Posen. Peasants under the Prussian administration were gradually enfranchised under the reforms of 1811 and 1823. The limited legal reforms in the Austrian Partition were overshadowed by its rural poverty. The Free City of Kraków was a tiny republic newly created by the Congress of Vienna in 1815 under the joint supervision of the three partitioning powers. As bleak as the new political divisions of the former Commonwealth were to Polish patriots, economic progress was made because the period after the Congress of Vienna witnessed a significant development in the building of early industry in the lands taken over by foreign powers.
Uprising of November 1830.
The increasingly repressive policies of the partitioning powers led to resistance movements in partitioned Poland, and in 1830 Polish patriots staged the November Uprising. This revolt developed into a full-scale war with Russia, but the leadership was taken over by Polish conservatives who were reluctant to challenge the Empire and hostile to broadening the independence movement's social base through measures such as land reform. Despite the significant resources mobilized, a series of mistakes by several successive chief commanders appointed by the Polish government who were either reluctant to serve or performed incompetently in battle led to the defeat of the insurgents by the Russian army in 1831. Congress Poland lost its constitution and military, but formally remained a separate administrative unit within the Russian Empire.
After the defeat of the November Uprising, thousands of former Polish combatants and other activists emigrated to Western Europe. This phenomenon, known as the Great Emigration, soon dominated Polish political and intellectual life. Together with the leaders of the independence movement, the Polish community abroad included the greatest Polish literary and artistic minds, including the Romantic poets Adam Mickiewicz (traditionally considered Poland's greatest poet, who died as an émigré in 1855), Juliusz Słowacki, Cyprian Norwid, and the composer Frédéric Chopin. In occupied and repressed Poland, some sought progress through nonviolent activism focused on education and economy, known as organic work; others, in cooperation with emigrant circles, organized conspiracies and prepared for the next armed insurrection.
Spring of Nations era revolts.
After the authorities in the partitions had found out about secret preparations, the planned national uprising failed to materialize. The Greater Poland Uprising ended in a fiasco in early 1846. In the Kraków Uprising of February 1846, patriotic action was combined with revolutionary demands, but the result was the incorporation of the Republic of Kraków into the Austrian Partition. The Austrian officials took advantage of peasant discontent and incited villagers against the noble-dominated insurgent units. This resulted in the Galician slaughter (1846), a large scale rebellion of serfs seeking relief from their post-feudal "folwark" condition of slavery. The uprising freed many from bondage and hastened decisions that led to peasant enfranchisement in the Austrian Empire (1848). A new wave of Polish military and other involvement, in the partitions and in other parts of Europe (e.g. Józef Bem in Austria and Hungary), soon took place in the context of the 1848 Spring of Nations revolutions. In particular, the events in Berlin precipitated the Greater Poland Uprising, where peasants in the Prussian Partition, who were by then largely enfranchised, played a prominent role.
Uprising of January 1863.
Despite the limited liberalization measures allowed in the Kingdom of Poland under the rule of Tsar Alexander II of Russia, a renewal of popular liberation activities took place in 1860–61. During large-scale demonstrations in Warsaw, Russian forces inflicted numerous casualties on the civilian participants. The "Red," or left-wing faction, which promoted peasant enfranchisement and cooperated with the Russian revolutionaries, became involved in immediate preparations for a national uprising. The "White," or right-wing faction, was inclined to cooperate with the Russian authorities and countered with partial reform proposals. In order to cripple the manpower potential of the Reds, Aleksander Wielopolski, the conservative leader of the Kingdom's government, arranged for a partial selective conscription of young Poles for the Russian army in the years 1862 and 1863. This action hastened the outbreak of hostilities. The January Uprising, joined and led after the initial period by the Whites, was fought by partisan units against an overwhelmingly advantaged enemy. The war lasted from January 1863 to the spring of 1864, when Romuald Traugutt, the last supreme commander of the insurgency, was captured by the tsarist police.
On 2 March 1864, the Russian authority, compelled by the Uprising to compete for the loyalty of Polish peasants, officially published an enfranchisement decree in Congress Poland along the lines of an earlier land reform proclamation of the insurgents. The act created the conditions necessary for the development of the capitalist system on central Polish lands. At the time when the futility of armed resistance without external support was realized by most Poles, the various sections of Polish society were undergoing deep and far-reaching social, economic and cultural changes.
Formation of modern Polish society under foreign rule (1864–1914).
Repression and organic work.
The failure of the January Uprising in Poland caused a major psychological trauma and became a historic watershed; indeed, it sparked the development of modern Polish nationalism. The Poles, subjected within the territories under the Russian and Prussian administrations to still stricter controls and increased persecution, preserved their identity in non-violent ways. After the Uprising, Congress Poland was downgraded in official usage from the "Kingdom of Poland" to the "Vistula Land" and was more fully integrated into Russia proper, but not entirely obliterated. The Russian and German languages were imposed in all public communication, and the Catholic Church was not spared from severe repression; public education was increasingly subjected to Russification and Germanization measures. Illiteracy was reduced, most effectively in the Prussian partition, but education in Polish was preserved mostly through unofficial efforts. The Prussian government pursued German colonization, including the purchase of Polish-owned land. On the other hand, the region of Galicia in western Ukraine and southern Poland, experienced a gradual relaxation of authoritarian policies and even a Polish cultural revival. Economically and socially backward, it was under the milder rule of the Austro-Hungarian Monarchy and from 1867 allowed increasingly limited autonomy. "Stańczycy", a conservative Polish pro-Austrian faction led by great land owners, dominated the Galician government. The Polish Academy of Arts and Sciences was founded in Kraków in 1872. Positivism replaced Romanticism as the leading intellectual, social and literary trend.
Social activities termed "organic work" consisted of self-help organizations that promoted economic advancement and worked on improving the competitiveness of Polish-owned businesses: industrial, agricultural or other. New commercial methods and ways of generating higher productivity were discussed and implemented through trade associations and special interest groups, while Polish banking and cooperative financial institutions made the necessary business loans available. The other major area of effort in organic work was the educational and intellectual development of the common people. Many libraries and reading rooms were established in small towns and villages, and numerous printed periodicals reflected the growing interest in popular education. Scientific and educational societies were active in a number of cities. Such activities were most pronounced in the Prussian Partition.
Economic development and social change.
Under the partitioning powers, large-scale industrialization, economic diversification and progress were introduced in the traditionally agrarian Polish lands, but this development turned out to be very uneven. In the Prussian Partition, advanced agriculture was practiced, except for Upper Silesia, where the coal-mining industry created a large labor force. The densest network of railroads was built in German-ruled western Poland. In Russian Congress Poland, a striking growth of industry, railways and towns was taking place, all against the background of an extensive, but less productive agriculture. Warsaw (a metallurgical center) and Łódź (a textiles center) grew rapidly, as did the total proportion of the urban population, making the region the most advanced in the Russian Empire (industrial production exceeded agricultural production by 1909). The coming of the railways spurred some industrial growth even in the vast Russian Partition territories outside Congress Poland. The Austrian Partition was rural and poor, except for the industrialized Cieszyn Silesia area. Galician economic expansion after 1890 included oil extraction and resulted in the growth of Lemberg (Lwów, Lviv) and Kraków.
Economic and social changes involving land reform and industrialization, combined with the effects of foreign domination, altered the centuries-old social structure of Polish society. Among the newly-emergent strata were wealthy industrialists and financiers, distinct from the traditional, but still critically important landed aristocracy. The intelligentsia, an educated, professional or business middle class, often originated from lower gentry, landless or alienated from their rural possessions, and from urban people. Many smaller agricultural enterprises based on serfdom did not survive the land reforms. The industrial proletariat, a new underprivileged class, was composed mainly of poor peasants or townspeople forced by deteriorating conditions to migrate and search for work in urban centers in their countries of origin or abroad. Millions of residents of the former Commonwealth of various ethnic groups worked or settled in Europe and in North and South America.
Social and economic changes were partial and gradual, and the degree of (fast-paced in some areas) industrialization generally lagged behind the advanced regions of western Europe. The three partitions developed different economies and were more economically integrated with their mother states than with each other (for example the Prussian Partition's agricultural production depended heavily on the German market, whereas the industrial sector of Congress Poland relied more on the Russian market).
Nationalism, socialism and other movements.
In the 1870s–90s, large-scale socialist, nationalist, agrarian and other political movements of great ideological fervor became established in partitioned Poland and Lithuania, along with corresponding political parties to promote them. Of the major parties, the socialist First Proletariat was founded in 1882, the Polish League (precursor of National Democracy) in 1887, the Polish Socialist Party in 1892, the Marxist SDKPiL in 1893, the agrarian People's Party of Galicia in 1895 and the Jewish socialist Bund in 1897. Christian democracy regional associations allied with the Catholic Church were also active; they united into the Polish Christian Democratic Party in 1919. The main minority ethnic groups of the former Commonwealth, including Ukrainians, Lithuanians, Belarusians and Jews, were getting involved in their own national movements and plans, which met with disapproval on the part of those Polish independence activists who counted on an eventual rebirth of the Commonwealth or the rise of a Commonwealth-inspired federal structure (a political movement referred to as Prometheism).
Around the start of the 20th century, the Young Poland cultural movement, centered in Galicia, took advantage of a milieu conducive to liberal expression in that region and was the source of Poland's finest artistic and literary productions. In this same era, Marie Skłodowska-Curie, a pioneer radiation scientist, performed her groundbreaking research in Paris.
Revolution of 1905.
The Revolution of 1905–07 in Russian Poland, the result of many years of pent-up political frustrations and stifled national ambitions, was marked by political maneuvering, strikes and rebellion. The revolt was part of much broader disturbances throughout the Russian Empire associated with the general Revolution of 1905. In Poland, the principal revolutionary figures were Roman Dmowski and Józef Piłsudski. Dmowski was associated with the right-wing nationalist movement National Democracy, whereas Piłsudski was associated with the Polish Socialist Party. As the authorities re-established control within the Russian Empire, the revolt in Congress Poland, placed under martial law, withered as well, partially as a result of tsarist concessions in the areas of national and workers' rights, including Polish representation in the newly created Russian Duma. Some of the acquired gains were however rolled back, which coupled with intensified Germanization in the Prussian Partition, left Austrian Galicia as the territory most amenable to patriotic action.
In the Austrian Partition, Polish culture was openly cultivated, and in the Prussian Partition, higher standards of developed civilization were achieved, but the Russian Partition remained of primary importance for the Polish nation and its aspirations. About 15.5 million Polish-speakers lived in core central and western Poland, over a relatively small and compact territory. Much fewer were spread in the east: 1.3 million in Austrian Eastern Galicia and about 2 million along Russia's western districts, with the heaviest concentration in the Vilnius Region.
Polish paramilitary organizations oriented toward independence, such as the Union of Active Struggle, were being formed in 1908–14, mainly in Galicia. The Poles were divided, and their political parties fragmented on the eve of World War I, with Dmowski's National Democracy (pro-Entente) and Piłsudski's faction assuming opposing positions.
World War I and Poland's independence issue.
The outbreak of World War I in the Polish lands offered Poles unexpected hopes for achieving independence as a result of the turbulence that engulfed the empires of the partitioning powers. All three of the monarchies that had benefited from the partition of Polish territories (Germany, Austria and Russia) were dissolved by the end of the war, and many of their territories were dispersed into new political units. At the start of the war, the Poles found themselves conscripted into the armies of the partitioning powers in a war that was not theirs. Furthermore, they were frequently forced to fight each other, since the armies of Germany and Austria were allied against Russia. Piłsudski's paramilitary units stationed in Galicia were turned into the Polish Legions in 1914, and as a part of the Austro-Hungarian Army, they fought on the Russian front until 1917, when the formation was disbanded. Piłsudski, who refused the demands that his men fight under German command, was arrested and imprisoned by the Germans and became a heroic symbol of Polish nationalism.
Due to a series of German victories on the Eastern Front, the area of Congress Poland became occupied by the Central Powers of Germany and Austria; Warsaw was captured by the Germans on 5 August 1915. In the Act of 5th November 1916, a fresh incarnation of the Kingdom of Poland ("Królestwo Regencyjne") was created by Germany and Austria on formerly Russian-controlled territories within the German Mitteleuropa scheme. The sponsor states were never able to agree on a candidate to assume the throne, however; rather, it was governed in turn by German and Austrian Governor-Generals, a Provisional Council of State, and a Regency Council. This puppet, but increasingly autonomous state existed until November 1918, when it was replaced by the newly established Republic of Poland. The existence of this "kingdom" and its planned Polish army had a positive effect on the Polish national efforts on the Allied side. But the Treaty of Brest-Litovsk (March 1918) between Germany and defeated Russia ignored Polish interests.
The independence of Poland had been campaigned for in Russia and in the West by Dmowski and in the West by Ignacy Paderewski. Tsar Nicholas II of Russia, and then the leaders of the February Revolution and the October Revolution of 1917, installed governments who declared in turn their support for Polish independence. In 1917, France formed the Blue Army (placed under Józef Haller) that comprised about 70,000 Poles by the end of the war, including men captured from German and Austrian units and 20,000 volunteers from the U.S. There was also a 30,000-men strong Polish anti-German army in Russia. Dmowski, operating from Paris as head of the Polish National Committee (KNP), became the spokesman for Polish nationalism in the Allied camp. On the initiative of Woodrow Wilson's Fourteen Points, Polish independence was officially endorsed by the Allies in June 1918.
In all, about two million Poles served in the war, counting both sides, and about 400–450,000 died. Much of the fighting on the Eastern Front took place in Poland, and civilian casualties and devastation were high. Total World War I casualties from 1914 to 1918, military and civilian, within the 1919–39 borders of Poland, were estimated at 1,128,000.
The final upsurge of the push for independence of Poland took place on the ground in October–November 1918. With the end of the war, Austro-Hungarian and German units were being disarmed, and the Austrian army's collapse freed Cieszyn and Kraków at the end of October. Lviv was then contested in the Polish–Ukrainian War of 1918–19. Ignacy Daszyński headed the first short-lived independent Polish government in Lublin from November 7, the leftist Provisional People's Government of the Republic of Poland, which was proclaimed as a democracy. Germany, now defeated, was forced by the Allies to stand down its large military forces in Poland. Overtaken by the German Revolution of 1918–19 at home, the Germans released Piłsudski from prison. He arrived in Warsaw on November 10 and was granted extensive authority by the Kingdom's Regency Council, which was also recognized by the Lublin government. On November 22 Piłsudski became the Temporary Head of State. He was held by many in high regard, but was resented by the right-wing National Democrats. The emerging Polish state was internally divided, heavily war-damaged and economically dysfunctional.
Second Polish Republic (1918–39).
Securing national borders, war with Soviet Russia.
After more than a century of foreign rule, Poland regained its independence at the end of World War I as one of the outcomes of the negotiations that took place at the Paris Peace Conference of 1919. The Treaty of Versailles that emerged from the conference set up an independent nation with an outlet to the sea, but left some of its boundaries to be decided by plebiscites. The largely German Free City of Danzig was granted a separate status that guaranteed its use as a port by Poland. In the end, the settlement of the German-Polish border turned out to be a prolonged and convoluted process. It helped engender the Greater Poland Uprising of 1918–19, the three Silesian Uprisings of 1919–21, the East Prussian plebiscite of 1920, the Upper Silesia plebiscite of 1921 and the 1922 Silesian Convention in Geneva.
Other boundaries were settled by war and subsequent treaties. A total of six border wars were fought in 1918–21, including the Polish–Czechoslovak border conflicts over Cieszyn Silesia in January 1919.
As distressing as these border conflicts were, the Polish–Soviet War of 1919–21 was the most important conflict of the era. Piłsudski had entertained far-reaching anti-Russian cooperative designs in Eastern Europe, and in 1919 the Polish forces pushed eastward into Lithuania, Belarus and Ukraine by taking advantage of the Russian preoccupation with the civil war, but his forces were soon confronted with the Soviet westward offensive of 1918–19. Western Ukraine was already a theater of the Polish–Ukrainian War, which eliminated the proclaimed West Ukrainian People's Republic in July 1919. In the autumn of 1919, Piłsudski rejected urgent pleas from the former Entente powers to support Anton Denikin's White movement in its advance on Moscow. The Polish–Soviet War proper began with the Polish Kiev Offensive in April 1920. By June, the Polish armies had advanced past Vilnius, Minsk and Kiev (in alliance with the Directorate of Ukraine of the Ukrainian People's Republic). At that time, a massive Soviet counter-offensive pushed the Poles out of most of Ukraine. On the northern front, the Soviet army reached the outskirts of Warsaw in early August. A Soviet triumph and the quick end of Poland seemed inevitable. However, the Poles scored a stunning victory at the Battle of Warsaw. Afterwards, more Polish military successes followed, the Soviets had to pull back and left swathes of territory occupied largely by Belarusians or Ukrainians to Polish rule. The new eastern boundary was finalized by the Peace of Riga in 1921.
The defeat of the Russian armies forced Vladimir Lenin and the Soviet leadership to postpone their strategic objective of linking up with the German and other European revolutionary-minded comrades and spread communist revolution. Lenin's hope of generating support for the Red Army in Poland had already failed to materialize. Piłsudski's seizure of Vilnius in October 1920 (known as Żeligowski's Mutiny) was a nail in the coffin of the already poor Polish–Lithuanian relations that had been strained by the Polish–Lithuanian War of 1919–20; both states would remain hostile to one another for the remainder of the interwar period. Piłsudski's planned Intermarium (an East European federation of states inspired by the tradition of the multiethnic Polish–Lithuanian Commonwealth that would include a hypothetical multinational successor state to the Grand Duchy of Lithuania) thus became incompatible with his assumption of Polish domination and encroachment on neighboring peoples' lands and aspirations, at a time of rising national movements in countries outside of Poland. It soon ceased to be a feature of Poland's politics.[a] A larger federated structure was also opposed by Dmowski's National Democrats. Their representative at the Peace of Riga talks, Stanisław Grabski, opted for leaving Minsk, Berdychiv, Kamianets-Podilskyi and the surrounding areas on the Soviet side of the border, since the National Democrats did not want to permit population shifts that they considered politically undesirable, especially if the transfers would result in a reduced proportion of citizens who were ethnically Polish.
The Peace of Riga settled the eastern border by preserving for Poland a substantial portion of the old Commonwealth's eastern territories, at the cost of partitioning the lands of the former Grand Duchy of Lithuania (Lithuania and Belarus) and Ukraine. The Ukrainians ended up with no state of their own and felt betrayed by the Riga arrangements; their resentment gave rise to extreme nationalism and anti-Polish hostility. The Kresy (or borderland) territories in the east won by 1921 would form the basis for a swap arranged and carried out by the Soviets in 1943–45, who at that time compensated the re-emerging Polish state for the eastern lands lost to the Soviet Union with conquered areas of eastern Germany.
The successful outcome of the Polish–Soviet War gave Poland a false sense of its prowess as a self-sufficient military power and encouraged the government to try to resolve international problems through imposed unilateral solutions. The territorial and ethnic policies of the interwar period contributed to bad relations with most of Poland's neighbors and to uneasy cooperation with the more distant centers of power, including France, Britain and the League of Nations.
Democratic politics.
Among the chief difficulties faced by the government of the new Polish republic was the lack of an integrated infrastructure among the formerly separate partitions, a deficiency that disrupted industry, transportation, trade and other areas.
The first Polish legislative election for the re-established Sejm of the Republic of Poland took place in January 1919. A temporary Small Constitution was passed by the body the following month.
The rapidly growing population of Poland within its new boundaries was ¾ agricultural and ¼ urban; Polish was the primary language of only ⅔ of the inhabitants of the new country. The minorities had very little voice in the government. The permanent March Constitution of Poland was adopted in March 1921. At the insistence of the National Democrats, who were concerned how aggressively Józef Piłsudski might exercise presidential powers if he were elected to office, the Constitution mandated limited prerogatives for the presidency.
The proclamation of the March Constitution was followed by a short and turbulent period of constitutional order and parliamentary democracy that lasted until 1926. The legislature remained fragmented, without stable majorities, and governments changed frequently. The open-minded Gabriel Narutowicz was elected president constitutionally (without a popular vote) by the National Assembly in 1922, however members of the nationalist right wing faction did not regard his elevation as legitimate. They viewed Narutowicz rather as a traitor whose election was pushed through by the votes of alien minorities. Narutowicz and his supporters were subjected to an intense harassment campaign, and the president was assassinated on December 16, 1922, after serving only five days in office.
Corruption was held to be commonplace in the political culture of the early Polish Republic, however the investigations conducted by the new regime after the 1926 May Coup failed to uncover any major affair or corruption scheme within the state apparatus of its predecessors.
Land reform measures were passed in 1919 and 1925 under pressure from an impoverished peasantry. They were partially implemented, but resulted in the parcellation of only 20% of the great agricultural estates. Poland endured numerous economic calamities and disruptions in the early 1920s, including waves of workers' strikes such as the 1923 Kraków riot. The German–Polish customs war, initiated by Germany in 1925, was one of the most damaging external factors that put a strain on Poland's economy. On the other hand, there were also signs of progress and stabilization, for example a critical reform of finances carried out by the competent government of Władysław Grabski, which lasted almost two years. Certain other achievements of the democratic period having to do with the management of governmental and civic institutions necessary to the functioning of the reunited state and nation, were too easily overlooked. Lurking on the sidelines was a disgusted army officer corps, unwilling to subject itself to civilian control but ready to follow the retired Piłsudski, who was highly popular with Poles and just as dissatisfied with the Polish system of government as his former colleagues in the military.
Piłsudski's coup and the Sanation Era.
On May 12, 1926, Piłsudski staged the May Coup, a military overthrow of the civilian government mounted against President Stanisław Wojciechowski and the troops loyal to the legitimate government. Hundreds died in fratricidal fighting. Piłsudski was supported by several leftist factions, who ensured the success of his coup by blocking the railway transportation of government forces. He also had the support of the conservative great landowners, which left the right-wing National Democrats as the only major social force opposed to the takeover.[l]
Following the coup, the new regime initially respected many parliamentary formalities, but gradually tightened its control and abandoned pretenses. Centrolew, a coalition of center-left parties, was formed in 1928, and in 1930 called for the "abolition of dictatorship." In 1930 the Sejm was dissolved, and a number of opposition deputies were imprisoned at the Brest Fortress. The Polish legislative election of 1930 was rigged to award a majority of seats to the pro-regime Nonpartisan Bloc for Cooperation with the Government (BBWR).
The authoritarian "Sanation" regime (meant to denote a "healing" regime) that Piłsudski lead until his death in 1935 (and would remain in place until 1939) reflected the dictator's evolution from his center-left past to conservative alliances. Political institutions and parties were allowed to function, but the electoral process was manipulated and those not willing to cooperate submissively were subjected to repression. From 1930, persistent opponents of the regime, many of the leftist persuasion, were imprisoned and subjected to staged legal processes, such as the Brest trials, with harsh sentences, or else detained in the Bereza Kartuska prison and similar camps for political prisoners. About three thousand were detained without trial at different times at the Bereza concentration camp between 1934 and 1939. In 1936 for example, 369 activists were taken there, including 342 Polish communists. Rebellious peasants staged the 1937 peasant strike in Poland, and other civil disturbances were caused by striking industrial workers, nationalist Ukrainians[p] and the activists of the incipient Belarusian movement. All became targets of ruthless military pacification. Besides sponsoring political repression, the regime also fostered the Piłsudski cult of personality that had already existed long before he assumed dictatorial powers.
Piłsudski signed the Soviet–Polish Non-Aggression Pact in 1932 and German–Polish Non-Aggression Pact in 1934, but in 1933 insisted that there was no threat from the East or West and said that Poland's politics were focused on becoming fully independent without serving foreign interests. He initiated the policy of maintaining an equal distance and an adjustable middle course regarding the two great neighbors, later continued by Józef Beck. Piłsudski kept personal control of the army, but it was poorly equipped, poorly trained and had poor preparations in place for possible future conflicts. His only war plan was a defensive war against a Soviet invasion.[r] The slow modernization after Piłsudski's death fell far behind the progress made by Poland's neighbors and measures to protect the western border, discontinued by Piłsudski from 1926, were not undertaken until March 1939.
Sanation deputies in the Sejm used a parliamentary maneuver to abolish the democratic March Constitution and push through a more authoritarian April Constitution in 1935; it reduced the powers of the Sejm (which Piłsudski despised). The process and the resulting document were seen as illegitimate by the anti-Sanation opposition, but during World War II, the Polish government-in-exile recognized the April Constitution in order to uphold the legal continuity of the Polish state.
When Marshal Piłsudski died in 1935, he retained the support of the main sections of Polish society even though he never risked testing his popularity in an honest election. His regime was dictatorial, but at that time only Czechoslovakia remained democratic in all of the regions neighboring Poland. Historians have taken widely divergent views of the meaning and consequences of the coup he perpetrated and his personal rule that followed.
Social and economic trends.
Independence stimulated the development of Polish culture in the Interbellum and intellectual achievement was high. Warsaw, whose population had almost doubled between World War I and World War II, was a restless, burgeoning metropolis. It outpaced Kraków, Lwów and Wilno, the other major population centers of the country.
Mainstream Polish society was not affected by the repressions of the Sanation authorities overall; many Poles enjoyed relative stability, and the economy improved markedly between 1926 and 1929, only to become caught up in the global Great Depression. After 1929, the country's industrial production and gross national income slumped by about 50%.
The Great Depression brought low prices for farmers and unemployment for workers. Social tensions increased, including rising antisemitism; the reconstituted Polish state had only 20 years of relative stability and uneasy peace between the two wars. A major economic transformation and multi-year state plan to achieve national industrial development, as embodied in the Central Industrial Region initiative launched in 1936, was led by Minister Eugeniusz Kwiatkowski. Motivated primarily by the need for a native arms industry, it was in progress at the time of the outbreak of World War II. Kwiatkowski was also the main architect of the earlier Gdynia seaport project.
The prevalent nationalism in political circles was fueled by the large size of Poland's minority populations and their separate agendas. According to the language criterion of the Polish census of 1931, the Poles constituted 69% of the population, Ukrainians 15%, Jews (defined as speakers of the Yiddish language) 8.5%, Belarusians 4.7%, Germans 2.2%, Lithuanians 0.25%, Russians 0.25% and Czechs 0.09%, with some geographical areas dominated by a particular minority. In time, the ethnic conflicts intensified, and the Polish state grew less tolerant of the interests of its national minorities. In interwar Poland, compulsory free general education substantially reduced illiteracy rates, but discrimination was practiced in a way that resulted in a dramatic decrease in the number of Ukrainian language schools and official restrictions on Jewish attendance at selected schools in the late 1930s.
The population grew steadily, reaching 35 million in 1939. However, the overall economic situation in the interwar period was one of stagnation. There was little money for investment inside Poland, and few foreigners were interested in investing there. Total industrial production barely increased between 1913 and 1939 (within the area delimited by the 1939 borders), but because of population growth (from 26.3 millions in 1919 to 34.8 millions in 1939), the "per capita" output actually decreased by 18%.
Conditions in the predominant agricultural sector kept deteriorating between 1929 and 1939, which resulted in rural unrest and a progressive radicalization of the Polish peasant movement that became increasingly inclined toward militant anti-state activities. It was firmly repressed by the authorities. According to Norman Davies, the failures of the Sanation regime (combined with the objective economic realities) caused a radicalization of the Polish masses by the end of the 1930s, but he warns against drawing parallels with the incomparably more destructive precedents of Nazi Germany or the Soviet Union under Stalin.
Final years.
After Piłsudski's death in 1935, Poland was governed until the German invasion of 1939 by old allies and subordinates known as "Piłsudski's colonels." They had neither the vision nor the resources to cope with the perilous situation facing Poland in the late 1930s. The colonels had gradually assumed greater powers during Piłsudski's life by manipulating the ailing marshal behind the scenes. Eventually they achieved an overt politicization of the army that did nothing to help prepare the country for war.
Foreign policy was the responsibility of Józef Beck, under whom Polish diplomacy attempted balanced approaches toward Germany and the Soviet Union, unfortunately without success, on the basis of a flawed understanding of the European geopolitics of his day. Beck had numerous foreign policy schemes and harbored illusions of Poland's status as a great power. He alienated most of Poland's neighbors, but is not blamed by historians for the ultimate failure of relations with Germany. The principal events of his tenure were concentrated in its last two years. In 1938, the Polish government opportunistically undertook a hostile action against the Czechoslovak state as weakened by the Munich Agreement and annexed a small piece of territory on its borders. In the case of the 1938 Polish ultimatum to Lithuania, the Polish action nearly resulted in a German takeover of southwest Lithuania. In the case of Czechoslovakia, Beck's understanding of the consequences of the Polish military move turned out to be completely mistaken. In the end, the German occupation of Czechoslovakia ushered in by the Munich Agreement markedly weakened Poland's own position. Furthermore, Beck mistakenly believed that Nazi-Soviet ideological contradictions would preclude their cooperation.
At home, increasingly alienated minorities threatened unrest and violence and were suppressed. Extreme nationalist circles such as the National Radical Camp grew more outspoken. One of the groups, the Camp of National Unity, combined many nationalists with Sanation supporters and was connected to a new strongman, Marshal Edward Rydz-Śmigły, who in many ways replaced Dmowski as a leader of the nationalist political movement in Poland.
In the late 1930s, the exile bloc Front Morges united several major Polish anti-Sanation figures, including Ignacy Paderewski, Władysław Sikorski, Wincenty Witos, Wojciech Korfanty and Józef Haller. It gained little influence inside Poland, but its spirit soon reappeared during World War II, within the Polish government-in-exile.
In October 1938, Joachim von Ribbentrop first proposed German-Polish territorial adjustments and Poland's participation in the Anti-Comintern Pact against the Soviet Union. The status of the Free City of Danzig was one of the key bones of contention. Approached by Ribbentrop again in March 1939, the Polish government expressed willingness to address issues causing German concern, but effectively rejected Germany's stated demands and thus refused to allow Poland to be turned by Adolf Hitler into a German puppet state. Hitler, incensed by the British and French declarations of support for Poland, abrogated the German–Polish Non-Aggression Pact in late April 1939.
To protect itself from an increasingly aggressive Nazi Germany, already responsible for the annexations of Austria (in the Anschluss of 1938), Czechoslovakia (in 1939) and a part of Lithuania after the 1939 German ultimatum to Lithuania, Poland entered into a military alliance with Britain and France (the 1939 Anglo-Polish military alliance and the earlier Franco-Polish military alliance of 1921, as updated in 1939). However, the two western powers were defense-oriented and not in a strong position, either geographically or in terms of resources, to assist Poland. Attempts were therefore made to induce Soviet-Polish cooperation, which was viewed as the only militarily viable possibility. Diplomatic manoeuvers continued in the spring and summer of 1939, but in their final attempts, the Franco-British talks with the Soviets in Moscow on forming an anti-Nazi defensive military alliance failed. Warsaw's refusal to allow the Red Army to operate on Polish territory doomed the Western efforts. The final contentious exchanges took place on August 21 and 23, 1939.[b] Stalin's regime was the target of an intense German counter-initiative and was concurrently involved in increasingly effective negotiations with Hitler's agents. On August 23, an outcome contrary to the exertions of the Allies became a reality: in Moscow, Germany and the Soviet Union hurriedly signed the Molotov–Ribbentrop non-aggression pact, which secretly provided for the dismemberment of Poland into Nazi and Soviet-controlled zones.
World War II and its violence.
Invasions and resistance.
On September 1, 1939, Hitler ordered the invasion of Poland, the opening event of World War II. Poland had signed an Anglo-Polish military alliance as recently as August 25, and had long been in alliance with France, thus the two Western powers soon declared war on Germany, but they remained largely inactive (as part of a period early in the conflict known as the Phoney War) and extended no aid to the attacked country. The numerically and technically superior "Wehrmacht" formations rapidly advanced eastwards and engaged massively in the murder of Polish civilians over the entire occupied territory. On September 17, a Soviet invasion of Poland began. The Soviet Union quickly occupied most of the areas of eastern Poland that contained large populations of Ukrainians and Belarusians.[h] The two invading powers divided up the country as they had agreed in the secret provisions of the Molotov–Ribbentrop Pact. Poland's top government officials and military high command fled the war zone and arrived at the Romanian Bridgehead in mid-September. After the Soviet entry they sought refuge in Romania.
Among the military operations in which Poles held out the longest (until late September or early October) were the Siege of Warsaw, the Battle of Hel and the resistance of the Independent Operational Group Polesie. Warsaw fell on 27 September after a heavy German bombardment that killed about 40,000 civilians. Poland was ultimately partitioned between Germany and the Soviet Union according to the terms of the German–Soviet Treaty of Friendship, Cooperation and Demarcation signed by the two powers in Moscow on September 29.
Gerhard Weinberg has argued that the most significant Polish contribution to World War II was sharing its code-breaking results. This allowed the British to perform the cryptanalysis of the Enigma and decipher the main German military code, which gave the Allies a major advantage in the conflict. As regards actual military campaigns, some Polish historians have argued that simply resisting the initial invasion of Poland was the country's greatest contribution to the victory over Nazi Germany, despite its defeat. The Polish Army of nearly one million men significantly delayed the start of the Battle of France, planned for 1939. When the Nazi offensive in the West did happen, the delay caused it to be less effective, a possibly crucial factor in the victory of the Battle of Britain.
After Germany invaded the Soviet Union as part of its Operation Barbarossa in June 1941, the whole of pre-war Poland was overrun and occupied by German troops.
German-occupied Poland was divided from 1939 into two regions: Polish areas annexed by Nazi Germany directly into the German Reich and areas ruled under a so-called General Government of occupation. The Poles formed an underground resistance movement and a Polish government-in-exile that operated first in Paris, then, from July 1940, in London, which was recognized by the Soviet Union. Polish-Soviet diplomatic relations, broken since September 1939, were resumed in July 1941 under the Sikorski–Mayski agreement, which facilitated the formation of a Polish army (the Anders' Army) in the Soviet Union.<ref name="Department of State 10/03">.</ref> In November 1941, Prime Minister Sikorski flew to the Soviet Union to negotiate with Stalin on its role on the Soviet-German front, but the British wanted the Polish soldiers in the Middle East. Stalin agreed, and the army was evacuated there.[w]
The members of the Polish Underground State that functioned in Poland throughout the war were loyal to and formally under the Polish government-in-exile, acting through its Government Delegation for Poland. During World War II, about 400,000 Poles joined the underground Polish Home Army (Armia Krajowa),[t] a part of the Polish Armed Forces of the government-in-exile. About 200,000 fought in the Western Front in Polish armed forces loyal to the government-in-exile, and about 300,000 in the Eastern Front. The pro-Soviet resistance movement, led by the Polish Workers' Party, was active from 1941. It was opposed by the gradually forming extreme nationalistic National Armed Forces.
Beginning in late 1939, hundreds of thousands of Poles from the Soviet-occupied areas were deported and taken east. Of the upper-ranking military personnel and others deemed uncooperative or potentially harmful by the Soviets, about 22,000 were secretly executed. 
In April 1943, the Soviet Union broke off deteriorating relations with the Polish government-in-exile after the German military announced the discovery of mass graves containing Polish army officers murdered by the Soviets at the Katyn massacre. The Soviets claimed that the Poles committed a hostile act by requesting that the Red Cross investigate these reports.
From 1941, the implementation of the Final Solution began, and the Holocaust in Poland proceeded with force. As the Jewish ghetto in occupied Warsaw was being liquidated by Nazi SS units, the city was the scene of the Warsaw Ghetto Uprising in April–May 1943. The elimination of Jewish ghettos in German-occupied Poland took place in a number of cities besides Warsaw and other uprisings took place against impossible odds by desperate Jewish insurgents, whose people were being removed and exterminated.
Soviet advance 1944–45, Warsaw Uprising.
At a time of increasing cooperation between the Western Allies and the Soviet Union in the wake of the Nazi invasion of 1941, the influence of the Polish government-in-exile was seriously diminished by the death of Prime Minister Władysław Sikorski, its most capable leader, in a plane crash on July 4, 1943. His successors lacked the ability or willingness to negotiate effectively with the Soviets and proved equally ineffective in pressing for the interests of the Polish people with the Western Allies.
In July 1944, the Soviet Red Army and Soviet-controlled People's Army of Poland entered the territory of future postwar Poland. In protracted fighting in 1944 and 1945, the Soviets and their Polish allies defeated and expelled the German army from Poland at a cost of over 600,000 Soviet and over 60,000 Polish soldiers lost.
The greatest single action of the Polish resistance movement in World War II (and a major political event of World War II) was the Warsaw Uprising that began on August 1, 1944. The uprising, in which most of the city's population participated, was instigated by the underground Armia Krajowa (Home Army) and approved by the Polish government-in-exile in an attempt to establish a non-communist Polish administration ahead of the arrival of the Red Army. The uprising was originally planned as a short-lived armed demonstration in expectation that the Soviet forces approaching Warsaw would assist in any battle to take the city. The Soviets had never agreed to an intervention, however, and they halted their advance at the Vistula River. The Germans used the opportunity to carry out a brutal suppression of the forces of the pro-Western Polish underground.[m]
The bitterly fought uprising lasted for two months and resulted in the death or expulsion from the city of hundreds of thousands of civilians. After the Poles realised the hopelessness of the situation and surrendered on 2 October, the Germans carried out a planned destruction of Warsaw on Hitler's orders that obliterated the remaining infrastructure of the city. The Polish First Army, fighting alongside the Soviet Red Army, entered a devastated Warsaw on 17 January 1945.[n]
Allied conferences, Polish governments.
From the time of the Tehran Conference in late 1943, there was broad agreement among the United States, Great Britain, and the Soviet Union that the locations of the borders between Germany and Poland and between Poland and the Soviet Union would be fundamentally changed after the conclusion of World War II.[c] Stalin's proposal that Poland should be moved far to the west was readily accepted by the Polish communists, who were at that time in the early stages of forming a post-war government (the State National Council, a quasi-parliamentary body, was created). In July 1944, a communist-controlled "Polish Committee of National Liberation" was established in Lublin nominally to govern the areas liberated from German control, a move that prompted protests from Prime Minister Stanisław Mikołajczyk and his government-in-exile.
By the time of the Yalta Conference in February 1945, the communists had already established a Provisional Government of the Republic of Poland. The Soviet position at the conference was strong because of their decisive contribution to the war effort and as a result of their occupation of immense amounts of land in central and eastern Europe. The three Great Powers (the United States, Great Britain, and the Soviet Union) gave assurances that the communist provisional government would be converted into an entity that would include democratic forces from within the country and active abroad, but the London-based government-in-exile was not mentioned. A Provisional Government of National Unity and subsequent democratic elections were the agreed stated goals. The disappointing results of these plans and the failure of the Western powers to ensure the strong participation of non-communists in the immediate post-war Polish government were seen by many Poles as a manifestation of Western betrayal.
War losses, extermination of Jews.
A lack of accurate data makes it difficult to document numerically the extent of the human losses suffered by Polish citizens during World War II. Additionally, many assertions made in the past must be considered suspect due to flawed methodology and a desire to promote certain political agendas. One of the most serious impediments to precise estimates of population loss in Poland during World War II is the lack of an accurate enumeration of the total population of the country in 1939, both of the ethnic Poles who lived there and the large ethnic minorities. The last available enumeration is the Polish census of 1931.
Modern research indicates that about 5 million Polish citizens were killed during the war, including 3 million Polish Jews. According to the United States Holocaust Memorial Museum, at least 1.9 to 2 million ethnic Poles and 3 million Polish Jews were killed. Millions of Polish citizens were deported to Germany for forced labor or to German death camps such as Treblinka, Auschwitz and Sobibor. According to another estimate, between 2.35 and 2.9 million Polish Jews and about 2 million ethnic Poles were killed. Nazi Germany intended to exterminate the Jews completely, in actions that have come to be described collectively as the Holocaust. The Poles were to be expelled from areas controlled by Nazi Germany through a process of resettlement that started in 1939 and was expected to be completed within 15 years.
In an attempt to incapacitate Polish society, the Nazis and the Soviets executed tens of thousands of members of the intelligentsia and community leadership during events such as the German AB-Aktion in Poland, Operation Tannenberg and the Katyn massacre.[j] Over 95% of the Jewish losses and 90% of the ethnic Polish losses were caused directly by Nazi Germany,[d] whereas 5% of the ethnic Polish losses were caused by the Soviets and 5% by Ukrainian nationalists. The large-scale Jewish presence in Poland that had endured for centuries was rather quickly put to an end by the policies of extermination implemented by the Nazis during the war. Waves of displacement and emigration that took place both during and after the war removed from Poland a majority of the Jews who survived. Further significant Jewish emigration followed events such as the Polish October political thaw of 1956 and the 1968 Polish political crisis. The magnitudes of the losses of Polish citizens of German, Ukrainian, Belarusian and other nationalities, which were also great, are not known.
In 1940–41, some 325,000 Polish citizens were deported by the Soviet regime. The number of Polish citizens who died at the hands of the Soviets is estimated at less than 100,000. In 1943–44, Ukrainian nationalists associated with the Organization of Ukrainian Nationalists (OUN) and the Ukrainian Insurgent Army perpetrated the Massacres of Poles in Volhynia and Eastern Galicia.
Approximately 90% of Poland's war casualties were the victims of prisons, death camps, raids, executions, the annihilation of ghettos, epidemics, starvation, excessive work and ill treatment. The war left one million children orphaned and 590,000 persons disabled. The country lost 38% of its national assets (whereas Britain lost only 0.8%, and France only 1.5%). Nearly half of pre-war Poland was expropriated by the Soviet Union, including the two great cultural centers of Lwów and Wilno.
Changing boundaries and population transfers.
By the terms of the 1945 Potsdam Agreement signed by the United States, the Soviet Union, and Great Britain, the Soviet Union retained most of the territories captured as a result of the Molotov–Ribbentrop Pact of 1939, including western Ukraine and western Belarus, and gained others. Lithuania and the Königsberg area of East Prussia were officially incorporated into the Soviet Union, in the case of the former without the recognition of Western powers. Poland was compensated with the bulk of Silesia, including Breslau (Wrocław) and Grünberg (Zielona Góra), the bulk of Pomerania, including Stettin (Szczecin), and large portions of the former East Prussia, along with Danzig (Gdańsk). Collectively referred to as the "Recovered Territories," they were included in the reconstituted Polish state. With Germany's defeat, the re-established Polish state was thus shifted west to the area between the Oder–Neisse and Curzon lines. The Poles lost 70% of their pre-war oil capacity to the Soviets, but gained from the Germans a highly developed industrial base and infrastructure that made a diversified industrial economy possible for the first time in Polish history.
The flight and expulsion of Germans from the "recovered" territories began before and during the Soviet conquest of those regions from the Nazis, and the process continued in the years immediately after the war. Of those who remained, many chose to emigrate to post-war Germany. On the other hand, 1.5–2 million Poles moved or were expelled from Polish areas annexed by the Soviet Union. The vast majority were resettled in the former German territories.
Many exiled Poles could not return to the country for which they had fought because they belonged to political groups incompatible with the new communist regimes, or because they originated from areas of pre-war eastern Poland that were incorporated into the Soviet Union (see Polish population transfers of the period 1944-46). Some were deterred from returning simply on the strength of warnings that anyone who had served in Western military units would be endangered under the new communist regimes. Many Poles were pursued, arrested, tortured and imprisoned by the Soviet authorities for belonging to the Home Army or other formations (see Anti-communist resistance in Poland during the period 1944-46), or were persecuted because they had fought on the Western front.
Territories on both sides of the new Polish-Ukrainian border were also "ethnically cleansed." Of the Ukrainians and Lemkos living in Poland within the new borders (about 700,000), close to 95% were forcibly moved to the Soviet Ukraine, or (in 1947) to the new territories in northern and western Poland under Operation Vistula. In Volhynia, 98% of the Polish pre-war population was either killed or expelled; in Eastern Galicia, the Polish population was reduced by 92%. In all, about 70,000 Poles and about 20,000 Ukrainians were killed in the ethnic violence that occurred in the 1940s, both during and after the war.
According to an estimate by Polish researchers, 40–60,000 of the 200–250,000 Polish Jews who escaped the Nazis survived without leaving Poland (the remainder perished). More were repatriated from the Soviet Union and elsewhere, and the February 1946 population census showed about 300,000 Jews within the new borders.[e] Of the surviving Jews, many chose to emigrate or felt compelled to because of anti-Jewish violence in Poland.
Because of changing borders and the mass movements of people of various nationalities, the emerging communist Poland ended up with a mainly homogeneous, ethnically Polish population (97.6% according to the December 1950 census). Minority members were not encouraged by the authorities or their neighbors to emphasize their ethnic identities.[i]
Polish People's Republic (1945–89).
Post-war struggle for power.
In response to the February 1945 Yalta Conference directives, a Polish Provisional Government of National Unity was formed in June 1945 under Soviet auspices; it was soon recognized by the United States and many other countries. Communist rule and Soviet domination were apparent from the beginning: sixteen prominent leaders of the Polish anti-Nazi underground were brought to trial in Moscow ("the Trial of the Sixteen") already in June 1945. In the immediate post-war years, emerging communist rule was challenged by opposition groups ("cursed soldiers"), and many thousands perished in the fight or were pursued by the Ministry of Public Security and executed. Such insurgents often pinned their hopes on expectations of the imminent outbreak of a World War III and the defeat of the Soviet Union. The Polish right-wing insurgency faded after the amnesty of February 1947.
The Polish people's referendum of June 1946 was arranged by the communist Polish Workers' Party to legitimize its dominance over Polish politics and claim widespread support for the party's policies. Although the Yalta agreement called for free elections, the Polish legislative election of January 1947 was controlled by the communists. Some democratic and pro-Western elements, led by Stanisław Mikołajczyk, the former prime minister-in-exile, participated in the Provisional Government and the 1947 elections, but were ultimately eliminated through electoral fraud, intimidation and violence. In times of radical political and economic change, members of Mikołajczyk's agrarian movement attempted to preserve some degree of market economy protections in the interest of limited property ownership. After the 1947 elections, the communist-dominated Front of National Unity was officially the only source of governmental authority. The Polish government-in-exile remained in continuous existence until 1990, although its influence declined.
Under Stalinism.
After a brief period of a coalition government of "National Unity," a Polish People's Republic ("Polska Rzeczpospolita Ludowa") was established under the rule of the communist Polish United Workers' Party. The name was not officially adopted, however, until the proclamation of the Constitution of the Polish People's Republic in 1952.
The ruling party itself was formed by the forced amalgamation in December 1948 of the communist Polish Workers' Party and the historically non-communist Polish Socialist Party. The latter, re-established in 1944 by its left wing, had since been allied with the communists. The ruling communists, who in post-war Poland preferred to use the term "socialism" instead of "communism" to identify their ideological basis,[f] needed to include the socialist junior partner to broaden their appeal, claim greater legitimacy and eliminate competition on the political Left. The socialists, who were losing their organization, were subjected to political pressure, ideological cleansing and purges in order to become suitable for unification on the terms of the "Workers' Party." The leading pro-communist leaders of the socialists were the prime ministers Edward Osóbka-Morawski and Józef Cyrankiewicz.
During the most oppressive phase of the Stalinist period (1948–53), terror was justified in Poland as necessary to eliminate reactionary subversion. Many thousands of perceived opponents of the communist regime were arbitrarily tried, and large numbers were executed.[u] The People's Republic was led by discredited Soviet operatives such as Bolesław Bierut, Jakub Berman and Konstantin Rokossovsky. The independent Catholic Church in Poland was subjected to property confiscations and other curtailments from 1949, and in 1950 was pressured into signing an accord with the government. In 1953 and later, despite a partial thaw after the death of Joseph Stalin that year, the persecution of the Church intensified and its head, Cardinal Stefan Wyszyński, was detained. A key event in the persecution of the Polish church was the Stalinist show trial of the Kraków Curia in January 1953.
In the Warsaw Pact, formed in 1955, the army of the Polish People's Republic was the second largest, after the Soviet Army.
Economic and social developments.
Beginning in 1944, large agricultural holdings and former German property in Poland started to be redistributed through land reform and industry started to be nationalized. Communist restructuring and the imposition of work-space rules encountered active worker opposition already in the years 1945–47. The Three-Year Plan of 1947–49 continued with the rebuilding, socialization and socialist restructuring of the economy. It was followed by the Six-Year Plan of 1950–55 for heavy industry. The rejection of the Marshall Plan in 1947 made aspirations for catching up with West European standards of living unrealistic.
The government's highest economic priority was the development of heavy industry useful to the military. State-run or controlled institutions common in all the socialist countries of eastern Europe were imposed on Poland, including collective farms and worker cooperatives. The latter were dismantled in the late 1940s as not socialist enough, although they were later re-established; even small-scale private enterprises were eradicated. Stalinism introduced heavy political and ideological propaganda in Poland and indoctrination in social life, culture and education.
Great strides were made, however, in the areas of employment (which became nearly full), universal public education (which nearly eradicated adult illiteracy), health care and recreational amenities. Many historic sites, including the central districts of Warsaw and Gdańsk, both devastated during the war, were rebuilt at great cost.
The communist industrialization program led to increased urbanization and educational and career opportunities for the intended beneficiaries of the social transformation along the lines of the peasants-workers-working intelligentsia paradigm. The most significant improvement was accomplished in the lives of Polish peasants, many of whom were able to leave their impoverished and overcrowded village communities for better conditions in urban centers. Those who stayed behind took advantage of the implementation of the 1944 land reform decree of the Polish Committee of National Liberation, which terminated the antiquated, but widespread parafeudal socioeconomic relations in Poland. Under Stalinism, attempts were made at establishing collective farms; they generally failed. Due to urbanization, the national percentage of the rural population decreased in communist Poland by about 50%. A majority of Poland's residents of cities and towns still live in apartment blocks built during the communist era in part to accommodate migrants from rural areas.
Thaw.
In March 1956, after the 20th Soviet Party Congress in Moscow ushered in de-Stalinization, Edward Ochab was chosen to replace the deceased Bolesław Bierut as First Secretary of the Polish Communist Party. As a result, Poland was rapidly overtaken by social restlessness and reformist undertakings; thousands of political prisoners were released and many people previously persecuted were officially rehabilitated. Worker's riots in Poznań in June 1956 were violently suppressed, but they gave rise to the formation of a reformist current within the communist party.
Amidst continuing social and national upheaval, a further shakeup took place in the party leadership as part of what is known as the Polish October of 1956.[k] While retaining most traditional communist economic and social aims, the regime led by the new Polish Party's First Secretary Władysław Gomułka liberalized internal life in Poland. The dependence on the Soviet Union was somewhat mollified, and the state's relationships with the Church and Catholic lay activists were put on a new footing. A repatriation agreement with the Soviet Union allowed the repatriation of hundreds of thousands of Poles who were still in Soviet hands, including many former political prisoners. Collectivization efforts were abandoned - agricultural land, unlike in other Comecon countries, mostly remained in the private ownership of farming families. State-mandated provisions of agricultural products at fixed, artificially low prices were reduced and, from 1972, eliminated.
Culture in the Polish People's Republic, to varying degrees linked to the intelligentsia's opposition to the totalitarian system, developed to a sophisticated level under Gomułka and his successors. The creative process was often compromised by state censorship, but significant works were created in fields such as literature, theater, cinema and music, among others. Journalism of veiled understanding and varieties of native and western popular culture were well represented. Uncensored information and works generated by émigré circles were conveyed through a variety of channels. The Paris-based Kultura magazine developed a conceptual framework for dealing with the issues of borders and the neighbors of a future free Poland, but Radio Free Europe was of foremost importance.
Stagnation and crackdown.
The legislative election of 1957 was followed by several years of political stability that was accompanied by economic stagnation and curtailment of reforms and reformists. One of the last initiatives of the brief reform era was a nuclear weapons–free zone in Central Europe proposed in 1957 by Adam Rapacki, Poland's foreign minister. One of the confirmations of the end of an era of greater tolerance was the expulsion from the communist party of several prominent "Marxist revisionists" in the 1960s.
In 1965, the Conference of Polish Bishops issued the Letter of Reconciliation of the Polish Bishops to the German Bishops, a gesture intended to heal bad mutual feelings left over from World War II. In 1966, the celebrations of the 1,000th anniversary of the Baptism of Poland led by Cardinal Stefan Wyszyński and other bishops turned into a huge demonstration of the power and popularity of the Catholic Church in Poland.
The post-1956 liberalizing trend, in decline for a number of years, was reversed in March 1968, when student demonstrations were suppressed during the 1968 Polish political crisis. Motivated in part by the Prague Spring movement, the Polish opposition leaders, intellectuals, academics and students used a historical-patriotic Dziady theater spectacle series in Warsaw (and its termination forced by the authorities) as a springboard for protests, which soon spread to other centers of higher education and turned nationwide. The authorities responded with a major crackdown on opposition activity, including the firing of faculty and the dismissal of students at universities and other institutions of learning. At the center of the controversy was also the small number of Catholic deputies in the Sejm (the Znak Association members) who attempted to defend the students.
In an official speech, Gomułka drew attention to the role of Jewish activists in the events taking place. This provided ammunition to a nationalistic and antisemitic communist party faction headed by Mieczysław Moczar that was opposed to Gomułka's leadership. Using the context of the military victory of Israel in the Six-Day War of 1967, some in the Polish communist leadership waged an antisemitic campaign against the remnants of the Jewish community in Poland. The targets of this campaign were accused of disloyalty and active sympathy with Israeli aggression. Branded "Zionists," they were scapegoated and blamed for the unrest in March, which eventually led to the emigration of much of Poland's remaining Jewish population (about 15,000 Polish citizens left the country).
With the active support of the Gomułka regime, the People's Army of Poland took part in the infamous Warsaw Pact invasion of Czechoslovakia in August 1968 after the informal announcement of the Brezhnev Doctrine.
In December 1970, the governments of Poland and West Germany signed the Treaty of Warsaw, which normalized their relations and made possible meaningful cooperation in a number of areas of bilateral interest. West Germany recognized the post-war "de facto" border between Poland and East Germany.
Worker revolts and Solidarity.
Price increases for essential consumer goods triggered the Polish protests of 1970. In December, there were disturbances and strikes in the port cities of Gdańsk, Gdynia, and Szczecin that reflected deep dissatisfaction with living and working conditions in the country. The activity was centered in the industrial shipyard areas of the three coastal cities. Dozens of protesting workers and bystanders were killed in police and military actions, generally under the authority of Gomułka and Minister of Defense Wojciech Jaruzelski. In the aftermath, Edward Gierek replaced Gomułka as First Secretary of the Communist Party. The new regime was seen as more modern, friendly and pragmatic, and at first it enjoyed a degree of popular and foreign support.[g][o]
Gierek's regime between 1970 and 1980 introduced wide-ranging (but ultimately unsuccessful) government reforms to revitalize the economy. Another attempt to raise food prices resulted in the June 1976 protests. During this period the opposition circles were emboldened by the Helsinki Conference processes. Jacek Kuroń was among the activists who defended accused rioters from Radom and other towns. The Workers' Defence Committee (KOR), established in response to the crackdown, consisted of dissident intellectuals willing to support industrial workers, farmers and students who were struggling with and persecuted by the authorities throughout the late 1970s.
In October 1978, the Archbishop of Kraków, Cardinal Karol Józef Wojtyła, became Pope John Paul II, head of the Roman Catholic Church. Catholics and others rejoiced at the elevation of a Pole to the papacy and greeted his June 1979 visit to Poland with an outpouring of emotion.
Fueled by large infusions of Western credit, Poland's economic growth rate was one of the world's highest during the first half of the 1970s, but much of the borrowed capital was misspent, and the centrally planned economy was unable to use the new resources effectively. The 1973 oil crisis caused recession and high interest rates in the West, to which the Polish government had to respond with sharp domestic consumer price increases. The growing debt burden became insupportable in the late 1970s, and negative economic growth set in by 1979.
Around July 1, 1980, with the Polish foreign debt standing at more than $20 billion, the government made another attempt to increase meat prices. Workers responded with escalating work stoppages that culminated in the 1980 general strikes in Lublin. In mid-August, labor protests at the Gdańsk Shipyard gave rise to a chain reaction of strikes that virtually paralyzed the Baltic coast by the end of the month and, for the first time, closed most coal mines in Silesia. The Inter-Enterprise Strike Committee coordinated the strike action across hundreds of workplaces and formulated the 21 demands as the basis for negotiations with the authorities. The Strike Committee was sovereign in its decision-making, but was aided by a team of "expert" advisers that included Bronisław Geremek and Tadeusz Mazowiecki, well-known intellectuals and dissidents.
On August 31, 1980, representatives of workers at the Gdańsk Shipyard, led by an electrician and activist Lech Wałęsa, signed the Gdańsk Agreement with the government that ended their strike. Similar agreements were concluded in Szczecin (the Szczecin Agreement) and in Silesia. The key provision of these agreements was the guarantee of the workers' right to form independent trade unions and the right to strike. Following the successful resolution of the largest labor confrontation in communist Poland's history, nationwide union organizing movements swept the country.
Edward Gierek was blamed by the Soviets for not following their "fraternal" advice, not shoring up the Party and the official trade unions and allowing "anti-socialist" forces to emerge. On September 5, 1980, Gierek was replaced by Stanisław Kania as First Secretary.
Delegates of the emergent worker committees from all over Poland gathered in Gdańsk on September 17 and decided to form a single national union organization named "Solidarity" (the name was adopted following a suggestion by Karol Modzelewski).
While party–controlled courts took up the contentious issues of Solidarity's legal registration as a trade union (finalized by November 10), planning had already begun for the imposition of martial law. A parallel farmers' union was organized and strongly opposed by the regime, but Rural Solidarity was finally registered on May 12, 1981. In the meantime, a rapid deterioration of the authority of the communist party, the disintegration of state power and an escalation of demands and threats by the various Solidarity–affiliated groups were occurring. According to Kuroń, a "tremendous social democratization movement in all spheres" was taking place and could not be contained. Wałęsa had meetings with Kania, which brought no resolution to the impasse. Following the Warsaw Pact summit in Moscow, the Soviet Union proceeded with a massive military build-up along Poland's border in December 1980, but during the summit, Kania forcefully argued with Leonid Brezhnev and other allied communists leaders against the feasibility of an external military intervention, and no action was taken. The United States, under presidents Jimmy Carter and Ronald Reagan, repeatedly warned the Soviets about the consequences of a direct intervention, while discouraging an open insurrection in Poland and signaling to the Polish opposition that there would be no rescue by the NATO forces.
In February 1981, Defense Minister General Wojciech Jaruzelski assumed the position of Prime Minister. A World War II veteran with a generally positive image, Jaruzelski engaged in preparations for calming the Polish unrest by the use of force, utilizing ZOMO troops and other security forces backed up by the Polish and Soviet bloc military. The 1980–81 Solidarity social revolt had thus far been free of any major use of force, but in March 1981 in Bydgoszcz, three activists were beaten up by the secret police. A nationwide "warning strike" took place, in which the 9.5 million strong Solidarity union was supported by the population at large. A general strike was called off by Wałęsa after the March 30 settlement with the government. Both Solidarity and the Party were badly split and the Soviets were losing patience. Kania was re-elected at the Party Congress in July, but the collapse of the economy continued and so did the general disorder.
At the first Solidarity national congress in September–October 1981 in Gdańsk, Lech Wałęsa was elected national chairman of the union with 55% of the vote. An appeal was issued to the workers of the other East European countries, urging them to follow in the footsteps of Solidarity. To the Soviets, the gathering was an "anti-socialist and anti-Soviet orgy" and the Polish communist leaders, increasingly led by Jaruzelski and General Czesław Kiszczak, were ready to apply force.
In October 1981, Jaruzelski was named the party's First Secretary, an unusual advancement for a military figure in the communist world. The Plenum's vote was 180 to 4, and he kept his government posts. Jaruzelski asked parliament to ban strikes and grant him extraordinary powers, but when neither was accomplished, he decided to proceed with his plans anyway.
Martial law and end of communism.
On December 12–13, 1981, the regime declared martial law in Poland, under which the army and ZOMO riot police were used to crush Solidarity. In the Soviet reaction to the Polish crisis of 1980–81, the Soviet leaders insisted that Jaruzelski pacify the opposition with the forces at his disposal, without direct Soviet involvement or backup. Virtually all Solidarity leaders and many affiliated intellectuals were arrested or detained. Nine workers were killed in the Pacification of Wujek. The United States and other Western countries responded by imposing economic sanctions against Poland and the Soviet Union. Unrest in the country was subdued but continued.
During martial law, Poland was ruled by the "Crow" (the Military Council of National Salvation). The open or semi-open opposition communications, as recently practiced, were replaced by underground publishing (known in the eastern bloc as Samizdat), and Solidarity was reduced to a few thousand underground activists.
Having achieved some semblance of stability, the Polish regime relaxed and then rescinded martial law over several stages. By December 1982, martial law was suspended, and a small number of political prisoners, including Wałęsa, were released. Although martial law formally ended in July 1983 and a partial amnesty was enacted, several hundred political prisoners remained in jail. Jerzy Popiełuszko, a popular pro-Solidarity priest, was abducted and murdered by security functionaries in October 1984.
Further developments in Poland occurred concurrently with and were influenced by the reformist leadership of Mikhail Gorbachev in the Soviet Union (known as Glasnost). In September 1986, a general amnesty was declared, and the government released nearly all political prisoners, but the authorities continued to harass dissidents and Solidarity activists. The regime's efforts to organize society from the top down had failed, while the opposition's attempts at creating an "alternate society" were also unsuccessful. With the economic crisis unresolved and societal institutions dysfunctional, both the ruling establishment and the opposition led by Solidarity leading figures began looking for ways out of the stalemate. Facilitated by the indispensable mediation of the Catholic Church, exploratory contacts were established.
Student protests resumed from February 1988. The government's inability to forestall Poland's economic decline led to the 1988 Polish strikes across the country in April, May and August. The Soviet Union was becoming increasingly destabilized and unwilling to apply military and other pressure to prop up allied regimes in trouble. The Polish government felt compelled to negotiate with the opposition and in September 1988 preliminary talks with Solidarity ensued in Magdalenka. Numerous meetings took place involving Wałęsa and General Kiszczak, among others, and the regime made a major public relations mistake by allowing a televised debate in November between Wałęsa and Alfred Miodowicz, chief of the All-Poland Alliance of Trade Unions, the official trade union organization. The fitful bargaining and intra-party squabbling led to the official Round Table Negotiations in the following year, followed by the Polish legislative election of 1989, a watershed event marking the fall of communism in Poland.
Third Polish Republic (1989–today).
Transition from communism.
The Polish Round Table Agreement of April 1989 called for local self-government, policies of job guarantees, legalization of independent trade unions and many wide-ranging reforms. The current Sejm promptly implemented the deal and agreed to partly open National Assembly elections that were set for June 4 and June 18. Only 35% of the seats in the Sejm (the national legislature's lower house) and all of the Senate seats were freely contested; the remaining Sejm seats (65%) were guaranteed for the communists and their allies.
The failure of the communists at the polls (almost all of the contested seats were won by the opposition) resulted in a political crisis. The new April constitutional agreement called for the re-establishment of the Polish presidency and on July 19 the National Assembly elected the communist leader General Wojciech Jaruzelski to that office. His election, seen at the time as politically necessary, was barely accomplished with tacit support from some Solidarity deputies, and the new president's position was not strong. Moreover, the unexpected definitiveness of the parliamentary election results created new dynamics and attempts by the communists to form a government failed.
On August 19, President Jaruzelski asked journalist and Solidarity activist Tadeusz Mazowiecki to form a government; on September 12, the Sejm voted approval of Prime Minister Mazowiecki and his cabinet. Mazowiecki decided to leave the economic reform entirely in the hands of economic liberals led by the new Deputy Prime Minister Leszek Balcerowicz, who proceeded with the design and implementation of his "shock therapy" policy. For the first time in post-war history, Poland had a government led by non-communists, setting a precedent soon to be followed by many other communist-ruled nations in a phenomenon known as the Revolutions of 1989 . Mazowiecki's acceptance of the "Thick line" formula meant no "witch-hunt," an absence of revenge seeking or exclusion from politics in regard to former communist officials.
In part because of the attempted indexation of wages, inflation reached 900% by the end of 1989, but was soon dealt with by the shock therapy. In December 1989, the Sejm approved the Balcerowicz Plan to transform the Polish economy rapidly from a centrally-planned one to a free market economy.[v] The Constitution of the People's Republic of Poland was amended to eliminate references to the "leading role" of the communist party and the country was renamed the "Republic of Poland." The communist Polish United Workers' Party dissolved itself in January 1990, creating in its place a new party, Social Democracy of the Republic of Poland. "Territorial self-government," abolished in 1950, was legislated back in March 1990, to be led by locally elected officials; its fundamental unit was the administratively independent gmina.
In October 1990, the constitution was amended to curtail the term of President Jaruzelski. In November 1990, the German–Polish Border Treaty was signed.
In November 1990, Lech Wałęsa was elected president for a five-year term; in December, he became the first popularly elected President of Poland. Poland's first free parliamentary election was held in October 1991. 18 parties entered the new Sejm, but the largest representation received only 12% of the total vote.
Democratic constitution, NATO and European Union memberships.
Several post-Solidarity governments were in existence between the 1989 election and the 1993 election, after which the "post-communist" left-wing parties took over. In 1993, the formerly Soviet Northern Group of Forces, a vestige of past domination, left Poland.
In 1995, Aleksander Kwaśniewski of the Social Democracy of the Republic of Poland was elected president and remained in that capacity for the next ten years (two terms).
In 1997, the new Constitution of Poland was finalized and approved in a referendum; it replaced the Small Constitution of 1992, an amended version of the communist constitution.
Poland joined NATO in 1999. Elements of the Polish Armed Forces have since participated in the Iraq War and the Afghanistan War. Poland joined the European Union as part of its enlargement in 2004. The two memberships were indicative of the Third Polish Republic's integration with the West. Poland has not adopted the euro currency, however.
Notes.
"a."^ Piłsudski's family roots in the Polonized gentry of the Grand Duchy of Lithuania and the resulting perspective of seeing himself and people like him as legitimate Lithuanians put him in conflict with modern Lithuanian nationalists (who in Piłsudski's lifetime redefined the scope of the meaning of "Lithuanian"), and by extension with other nationalists and also with the Polish modern nationalist movement.
"b."^ In 1938 Poland and Romania refused to agree to a Franco-British proposal that in the event of war with Germany Soviet forces would be allowed to cross their territories to aid Czechoslovakia. The Polish ruling elites considered the Soviets in some ways more threatening than the Nazis.
The Soviet Union repeatedly declared its intention to fulfill its obligations under the 1935 treaty with Czechoslovakia and defend Czechoslovakia militarily. A transfer of land and air forces through Poland and/or Romania was required and the Soviets approached the French about it, who also had a treaty with Czechoslovakia (and with Poland). Edward Rydz-Śmigły rebuked the French suggestion on that matter in 1936, and in 1938 Józef Beck pressured Romania not to allow even Soviet warplanes to fly over its territory. Like Hungary, Poland was looking into using the German-Czechoslovak conflict to settle its own territorial grievances, namely disputes over parts of Zaolzie, Spiš and Orava.
"c."^ An establishment of Poland restricted to "minimal size", according to ethnographic boundaries (such as those shown on this 1920 map, or the lands common to both prewar Poland and postwar Poland), was planned by the Soviet People's Commissariat for Foreign Affairs in 1943–44, and recommended by Ivan Maisky to Vyacheslav Molotov in early 1944 because of what Maisky saw as Poland's historically unfriendly disposition toward Russia and the Soviet Union. Joseph Stalin opted for a larger version, allowing a "swap" (territorial compensation for Poland), which involved the eastern lands gained by Poland at the Peace of Riga of 1921 and now lost, and eastern Germany conquered from the Nazis in 1944–45. In regard to the several disputed areas, including Stettin, "Zakerzonia" and Białystok (Białystok was claimed by the communists of the Byelorussian SSR), the Soviet leader made decisions that favored Poland.
Other territorial and ethnic scenarios were also possible, generally with outcomes less advantageous to Poland than its present form.
"d."^ Timothy Snyder spoke of about 100,000 Jews killed by Poles during the Nazi occupation, the majority probably by members of the collaborationist Blue Police. This number would have likely been many times higher had Poland entered into an alliance with Germany in 1939, as advocated by some Polish historians and others.
"e."^ Some may have falsely claimed Jewish identity hoping for permission to emigrate. The communist authorities, pursuing the concept of a Poland of single ethnicity (in accordance with the recent border changes and expulsions), were allowing the Jews to leave the country. For a discussion of early communist Poland's ethnic politics, see Timothy Snyder, "The Reconstruction of Nations", chapters on modern "Ukrainian Borderland".
"f."^ A Communist Party of Poland had existed in the past, but was eliminated in Stalin's purges in 1938.
"g."^ The Soviet leadership, which had previously ordered the crushing of the Uprising in East Germany, the Hungarian Revolution and the Prague Spring, now became worried about the demoralization of the Polish army, a crucial Warsaw Pact component, because of its deployment against Polish workers. The Soviets withdrew their support for Gomułka, who insisted on the use of force; he and his close associates were subsequently ousted from the Polish Politburo by the Polish Central Committee.
"h."^ East of the Molotov-Ribbentrop line, the population was 43% Polish, 33% Ukrainian, 8% Belarusian and 8% Jewish. The Soviet Union did not want to appear as an aggressor, and moved its troops to Eastern Poland under the pretext of offering protection to "the kindred Ukrainian and Belorussian people".
"i."^ Joseph Stalin at the 1943 Tehran Conference discussed with Winston Churchill and Franklin Roosevelt new post-war borders in central-eastern Europe, including the shape of a future Poland. He endorsed the Piast Concept, which justified a massive shift of Poland's frontiers to the west. Stalin resolved to secure and stabilize the western reaches of the Soviet Union and disable the future military potential of Germany by constructing a compact and ethnically-defined Poland (along with the Soviet ethnic Ukraine, Belarus and Lithuania) and by radically altering the region's system of national borders. After 1945, the Polish communist regime wholeheartedly adopted and promoted the Piast Concept, making it the centerpiece of their claim to be the true inheritors of Polish nationalism. After all the killings and population transfers during and after the war the country was 99% "Polish."
"j."^ "All the currently available documents of Nazi administration show that, together with the Jews, the stratum of the Polish intelligentsia was marked for total extermination. In fact, Nazi Germany achieved this goal almost by half, since Poland lost 50 percent of her citizens with university diplomas and 35 percent of those with a gimnazium diploma."
"k."^ Decisive political events took place in Poland shortly before the Soviet intervention in Hungary. Władysław Gomułka, a reformist leader at that time, was reinstated to the Polish Politburo and the Eighth Plenum of the party's Central Committee was announced to convene on October 19, 1956, all without seeking Soviet approval. The Soviet Union responded with military moves and intimidation and its "military-political delegation", led by Nikita Khrushchev, quickly arrived in Warsaw. Gomułka tried to convince them of his loyalty but insisted on the reforms which he considered essential, including a replacement of Poland's Soviet-trusted minister of defense, Konstantin Rokossovsky. The disconcerted Soviets returned to Moscow, the Polish Plenum elected Gomułka First Secretary and removed Rokossovsky from the Politburo. On October 21, the Soviet Presidium followed Khrushchev's lead and decided unanimously to "refrain from military intervention" in Poland, a decision likely influenced also by the ongoing preparations for the invasion of Hungary. The Soviet gamble paid off because Gomułka in the coming years turned out to be a very dependable Soviet ally and an orthodox communist.
Unlike the other Warsaw Pact countries, Poland did not endorse the Soviet armed intervention in Hungary. The Hungarian Uprising was intensely supported by the Polish public.
"l."^ The delayed reinforcements were coming and the government military commanders General Tadeusz Rozwadowski and Władysław Anders wanted to keep on fighting the coup perpetrators, but President Stanisław Wojciechowski and the government decided to surrender to prevent the imminent spread of civil war. The coup brought to power the "Sanation" regime under Józef Piłsudski and Edward Rydz-Śmigły after Piłsudski's death. The Sanation regime persecuted the opposition within the military and in general. Rozwadowski died after abusive imprisonment, according to some accounts murdered. According to Aleksandra Piłsudska, the Marshal's wife, following the coup and for the rest of his life Piłsudski lost his composure and appeared over-burdened.
At the time of Rydz-Śmigły's command, the Sanation camp embraced the ideology of Roman Dmowski, Piłsudski's nemesis. Rydz-Śmigły did not allow General Władysław Sikorski, an anti-Sanation enemy, to participate as a soldier in the defense of the country in September 1939. During World War II in France and Britain the Polish government in exile became dominated by anti-Sanation politicians. The perceived Sanation followers were in turn persecuted (in exile) under prime ministers Sikorski and Stanisław Mikołajczyk.
"m."^ General Zygmunt Berling of the Soviet-allied First Polish Army attempted in mid-September a crossing of the Vistula and landing at Czerniaków to aid the insurgents, but the operation was defeated by the Germans and the Poles suffered heavy losses.
"n."^ The decision to launch the Warsaw Uprising resulted in the destruction of the city, its population and its elites and has been a source of lasting controversy. According to the historians Czesław Brzoza and Andrzej Leon Sowa, orders of further military offensives, issued at the end of August 1944 as a part of Operation Tempest, show the loss of a sense of responsibility for the country's fate on the part of the Polish leadership.
"o."^ One of the party leaders Mieczysław Rakowski, who abandoned his mentor Gomułka following the 1970 crisis, saw the demands of the demonstrating workers as "exclusively socialist" in character, because of the way they were phrased. Most people in communist Poland, including opposition activists, did not question the supremacy of "socialism" or the socialist idea; misconduct by party officials, such as not following the provisions of the constitution, was blamed. This assumed standard of political correctness was increasingly challenged in the years that followed, when pluralism became a frequently used concept.
"p."^ The Polish Sanation authorities were provoked by the independence-seeking Organization of Ukrainian Nationalists (OUN). OUN engaged in political assassinations, terror and sabotage, to which the Polish state responded with a repressive campaign in the 1930s, as Józef Piłsudski and his successors imposed collective responsibility on the villagers in the affected areas. After the disturbances of 1933 and 1934, a prison camp in Bereza Kartuska was established, which became notorious for its brutal regime. The government brought Polish settlers and administrators to Volhynian areas with a centuries-old tradition of Ukrainian peasant rising against Polish land owners (and to Eastern Galicia). In the late 1930s, after Piłsudski's death, military persecution intensified and a policy of "national assimilation" was aggressively pursued. Military raids, public beatings, property confiscations and the closing and destruction of Orthodox churches aroused lasting enmity in Galicia and antagonized Ukrainian society in Volhynia at, according to Timothy Snyder, the worst possible moment. However, he also notes that "Ukrainian terrorism and Polish reprisals touched only part of the population, leaving vast regions unaffected" and "the OUN's nationalist prescription, a Ukrainian state for ethnic Ukrainians alone was far from popular." Halik Kochanski wrote of the legacy of bitterness between the Ukrainians and Poles that soon exploded in the context of the World War II. See also: History of the Ukrainian minority in Poland.
"r."^ Foreign policy was one of the few governmental areas in which Piłsudski took an active interest. He saw Poland's role and opportunity as lying in Eastern Europe and advocated passive relations with the West. He felt that a German attack should not be feared because, even if this unlikely event were to take place, the Western powers would be bound to restrain Germany and come to Poland's rescue.
"s."^ According to the researcher Jan Sowa, the Commonwealth failed as a state because it was not able to conform to the emerging new European order established at the Peace of Westphalia of 1648. Poland's elective kings, restricted by the self-serving but short-sighted nobility, could not impose a strong and efficient central government, with its characteristic post-Westphalian internal and external sovereignty. The inability of Polish kings to levy and collect taxes (and therefore sustain a standing army) and conduct independent foreign policy were among the chief obstacles to Poland competing effectively on the changed European scene, where absolutist power was a prerequisite for survival and became the foundation for the abolition of serfdom and gradual formation of parliamentarism.
"t."^ Besides the Home Army (Armia Krajowa) there were other major underground fighting formations: Bataliony Chłopskie, Narodowe Siły Zbrojne and Gwardia Ludowa (later Armia Ludowa). In 1943, the leaders of the nationalistic Narodowe Siły Zbrojne collaborated with Nazi Germany in a case unique in occupied Poland. The NSZ conducted an anti-communist civil war. According to the historians Czesław Brzoza and Andrzej Leon Sowa, participation figures given for the underground resistance are often inflated. In the spring of 1944, at the time of the most extensive involvement of the underground organizations, there were most likely considerably fewer than a total 500,000 military and civilian personnel participating, over the entire spectrum, from the right wing to the communists.
"u."^ According to Jerzy Eisler, about 1.1 million people may have been imprisoned or detained in 1944–56 and about 50,000 may have died because of the struggle and persecution, including about 7,000 soldiers of the right-wing underground killed in the 1940s. According to Adam Leszczyński, up to 30,000 people were killed by the communist regime during the first several years after the war.
"v."^ According to Andrzej Stelmachowski, one of the key participants of the Polish systemic transformation, Minister Leszek Balcerowicz pursued extremely liberal economic policies, often unusually painful for society. The December 1989 Sejm statute of credit relations reform introduced an "incredible" system of privileges for banks. Banks were allowed to alter unilaterally interest rates on already existing contracts. The extremely high rates they instantly introduced ruined many previously profitable enterprises and caused a complete breakdown of the apartment block construction industry, which had long-term deleterious effects on the state budget as well. Balcerowicz's policies also caused permanent damage to Polish agriculture, which Balcerowicz "did not understand", and to the often successful and useful Polish cooperative movement.
"w."^ Led by Władysław Anders, the Polish II Corps fought at the famous Battle of Monte Cassino in 1944, as part of the Allied Italian Campaign.
"x."^ The concept which had become known as the Piast Idea, the chief proponent of which was Jan Ludwik Popławski, was based on the statement that the Piast homeland was inhabited by so-called "native" aboriginal Slavs and Slavonic Poles since time immemorial and only later was "infiltrated" by "alien" Celts, Germans and others. After 1945, the so-called "autochthonous" or "aboriginal" school of Polish prehistory received official backing in Poland and a considerable degree of popular support. According to this view, the Lusatian Culture which archaeologists have identified between the Oder and the Vistula in the early Iron Age, was said to be Slavonic; all non-Slavonic tribes and peoples recorded in the area at various points in ancient times were dismissed as "migrants" and "visitors". In contrast, the critics of this theory, such as Marija Gimbutas, regarded it as an unproved hypotheses and for them the date and origin of the westward migration of the Slavs were largely uncharted; the Slavonic connections of the Lusatian Culture were entirely imaginary; and the presence of an ethnically mixed and constantly changing collection of peoples on the North European Plain was taken for granted.
</dl>
Further reading.
More recent general history of Poland books in English
</dl>
Published in Poland
</dl>

</doc>
<doc id="13773" url="http://en.wikipedia.org/wiki?curid=13773" title="Hradčany">
Hradčany

Hradčany (common ]; German: '), the Castle District, is the district of the city of Prague, Czech Republic, surrounding the Prague Castle.
The castle is said to be the biggest castle in the world at about 570 meters in length and an average of about 130 meters wide. Its history stretches back to the 9th century. St Vitus Cathedral is located in the castle area.
Most of the district consists of noble historical . There are many other attractions for visitors: romantic nooks, peaceful places and beautiful lookouts.
Hradčany was an independent borough until 1784, when the four independent boroughs that had formerly constituted Prague were proclaimed a single city. The other three were: Malá Strana , , and .

</doc>
<doc id="13774" url="http://en.wikipedia.org/wiki?curid=13774" title="Houston">
Houston

Houston ( ) is the most populous city in Texas, and the fourth most populous city in the United States. With a census-estimated 2013 population of 2.2 million people within a land area of 599.6 sqmi, Houston is the largest city in the Southern United States, the seat of Harris County, and fifth-most populated metropolitan area in the United States.
Houston was founded in 1836 on land near the banks of Buffalo Bayou (now known as Allen's Landing) and incorporated as a city on June 5, 1837. The city was named after former General Sam Houston, who was president of the Republic of Texas and had commanded and won at the Battle of San Jacinto 25 mi east of where the city was established. The burgeoning port and railroad industry, combined with oil discovery in 1901, has induced continual surges in the city's population. In the mid-twentieth century, Houston became the home of the Texas Medical Center—the world's largest concentration of healthcare and research institutions—and NASA's Johnson Space Center, where the Mission Control Center is located.
Houston's economy has a broad industrial base in energy, manufacturing, aeronautics, and transportation. It is also leading in health care sectors and building oilfield equipment; only New York City is home to more Fortune 500 headquarters. The Port of Houston ranks first in the United States in international waterborne tonnage handled and second in total cargo tonnage handled. Nicknamed the "Space City", Houston is a global city, with strengths in business, international trade, entertainment, culture, media, fashion, science, sports, technology, education, medicine and research. The city has a population from various ethnic and religious backgrounds and a large and growing international community. Houston is considered to be the most diverse city in Texas and the United States. It is home to many cultural institutions and exhibits, which attract more than 7 million visitors a year to the Museum District. Houston has an active visual and performing arts scene in the Theater District and offers year-round resident companies in all major performing arts.
History.
In August 1836, two real estate entrepreneurs—Augustus Chapman Allen and John Kirby Allen—from New York, purchased 6642 acre of land along Buffalo Bayou with the intent of founding a city. The Allen brothers decided to name the city after Sam Houston, the popular general at the Battle of San Jacinto, who was elected President of Texas in September 1836.
Houston was granted incorporation on June 5, 1837, with James S. Holman becoming its first mayor. In the same year, Houston became the county seat of Harrisburg County (now Harris County) and the temporary capital of the Republic of Texas. In 1840, the community established a chamber of commerce in part to promote shipping and waterborne business at the newly created port on Buffalo Bayou.
By 1860, Houston had emerged as a commercial and railroad hub for the export of cotton. Railroad spurs from the Texas inland converged in Houston, where they met rail lines to the ports of Galveston and Beaumont. During the American Civil War, Houston served as a headquarters for General John Bankhead Magruder, who used the city as an organization point for the Battle of Galveston. After the Civil War, Houston businessmen initiated efforts to widen the city's extensive system of bayous so the city could accept more commerce between downtown and the nearby port of Galveston. By 1890, Houston was the railroad center of Texas.
In 1900, after Galveston was struck by a devastating hurricane, efforts to make Houston into a viable deep-water port were accelerated. The following year, oil discovered at the Spindletop oil field near Beaumont prompted the development of the Texas petroleum industry. In 1902, President Theodore Roosevelt approved a $1 million improvement project for the Houston Ship Channel. By 1910 the city's population had reached 78,800, almost doubling from a decade before. African-Americans formed a large part of the city's population, numbering 23,929 people, or nearly one-third of the residents.
President Woodrow Wilson opened the deep-water Port of Houston in 1914, seven years after digging began. By 1930, Houston had become Texas' most populous city and Harris the most populous county. In 1940, the Census Bureau reported Houston's population as 77.5% white and 22.4% black.
When World War II started, tonnage levels at the port decreased and shipping activities were suspended; however, the war did provide economic benefits for the city. Petrochemical refineries and manufacturing plants were constructed along the ship channel because of the demand for petroleum and synthetic rubber products during the war. Ellington Field, initially built during World War I, was revitalized as an advanced training center for bombardiers and navigators. The Brown Shipbuilding Company was founded in 1942 to build ships for the U.S. Navy during World War II. The M.D. Anderson Foundation formed the Texas Medical Center in 1945. After the war, Houston's economy reverted to being primarily port-driven. In 1948, several unincorporated areas were annexed into the city limits, which more than doubled the city's size, and Houston proper began to spread across the region.
In 1950, the availability of air conditioning provided impetus for many companies to relocate to Houston resulting in an economic boom and producing a key shift in the city's economy toward the energy sector. 
The increased production of the local shipbuilding industry during World War II spurred Houston's growth, as did the establishment in 1961 of NASA's "Manned Spacecraft Center" (renamed the Lyndon B. Johnson Space Center in 1973), which created the city's aerospace industry. The Astrodome, nicknamed the "Eighth Wonder of the World", opened in 1965 as the world's first indoor domed sports stadium.
During the late 1970s, Houston experienced a population boom as people from the Rust Belt states moved to Texas in large numbers. The new residents came for numerous employment opportunities in the petroleum industry, created as a result of the Arab Oil Embargo.
The population boom ended abruptly in the mid-1980s, as oil prices fell precipitously. The space industry also suffered in 1986 after the Space Shuttle Challenger disintegrated shortly after launch. The late 1980s saw a recession adversely affecting the city's economy. After the early 1990s recession, Houston made efforts to diversify its economy by focusing on aerospace and health care/biotechnology and reduced its dependence on the petroleum industry, but as oil prices increased again in the 2000s, the petroleum industry has again increased its share of the local economy.
In 1997, Houstonians elected Lee P. Brown as the city's first African American mayor.
In June 2001, Tropical Storm Allison dumped up to 40 in of rain on parts of Houston, causing the worst flooding in the city's history. The storm cost billions of dollars in damage and killed 20 people in Texas. By December of that same year, Houston-based energy company Enron collapsed into the third-largest ever U.S. bankruptcy during an investigation surrounding fabricated partnerships that were allegedly used to hide debt and inflate profits.
In August 2005, Houston became a shelter to more than 150,000 people from New Orleans who evacuated from Hurricane Katrina. One month later, approximately 2.5 million Houston area residents evacuated when Hurricane Rita approached the Gulf Coast, leaving little damage to the Houston area. This was the largest urban evacuation in the history of the United States.
Geography.
According to the United States Census Bureau, the city has a total area of 656.3 sqmi; this comprises 634.0 sqmi of land and 22.3 sqmi of water.
Most of Houston is located on the gulf coastal plain, and its vegetation is classified as temperate grassland and forest. Much of the city was built on forested land, marshes, swamp, or prairie which resembles the Deep South, and are all still visible in surrounding areas. Flatness of the local terrain, when combined with urban sprawl, has made flooding a recurring problem for the city. Downtown stands about 50 ft above sea level, and the highest point in far northwest Houston is about 125 ft in elevation. The city once relied on groundwater for its needs, but land subsidence forced the city to turn to ground-level water sources such as Lake Houston, Lake Conroe and Lake Livingston. The city owns surface water rights for 1.20 billion gallons of water a day in addition to 150 million gallons a day worth of groundwater.
Houston has four major bayous passing through the city. Buffalo Bayou runs through downtown and the Houston Ship Channel, and has three tributaries: White Oak Bayou, which runs through the Houston Heights community northwest of Downtown and then towards Downtown; Brays Bayou, which runs along the Texas Medical Center; and Sims Bayou, which runs through the south of Houston and downtown Houston. The ship channel continues past Galveston and then into the Gulf of Mexico.
Geology.
Underpinning Houston's land surface are unconsolidated clays, clay shales, and poorly cemented sands up to several miles deep. The region's geology developed from river deposits formed from the erosion of the Rocky Mountains. These sediments consist of a series of sands and clays deposited on decaying organic marine matter, that over time, transformed into oil and natural gas. Beneath the layers of sediment is a water-deposited layer of halite, a rock salt. The porous layers were compressed over time and forced upward. As it pushed upward, the salt dragged surrounding sediments into salt dome formations, often trapping oil and gas that seeped from the surrounding porous sands. The thick, rich, sometimes black, surface soil is suitable for rice farming in suburban outskirts where the city continues to grow.
The Houston area has over 150 active faults (estimated to be 300 active faults) with an aggregate length of up to 310 mi, including the Long Point–Eureka Heights fault system which runs through the center of the city. There have been no significant historically recorded earthquakes in Houston, but researchers do not discount the possibility of such quakes having occurred in the deeper past, nor occurring in the future. Land in some areas southeast of Houston is sinking because water has been pumped out of the ground for many years. It may be associated with slip along the faults; however, the slippage is slow and not considered an earthquake, where stationary faults must slip suddenly enough to create seismic waves. These faults also tend to move at a smooth rate in what is termed "fault creep", which further reduces the risk of an earthquake.
Climate.
Houston's climate is classified as humid subtropical ("Cfa" in Köppen climate classification system), typical of the lower South. While not located in "Tornado Alley", like much of the rest of Texas, spring supercell thunderstorms sometimes bring tornadoes to the area. Prevailing winds are from the south and southeast during most of the year, which bring heat and moisture from the nearby Gulf of Mexico.
During the summer months, it is common for temperatures to reach over 90 °F, with an average of 106.5 days per year, including a majority from June to September, with a high of 90 °F or above and 4.6 days at or over 100 °F. However, humidity usually yields a higher heat index. Summer mornings average over 90 percent relative humidity. Winds are often light in the summer and offer little relief, except in the far southeastern outskirts near the Gulf coast and Galveston. To cope with the strong humidity and heat, people use air conditioning in nearly every vehicle and building. In 1980, Houston was described as the "most air-conditioned place on earth". Officially, the hottest temperature ever recorded in Houston is 109 °F, which was reached both on September 4, 2000 and August 28, 2011. Dewpoints in the summer hover around 70 to.
Houston has mild winters in contrast to most areas of the United States. In January, the normal mean temperature at Intercontinental Airport is 53.1 °F, while that station has an average of 13 days with a low at or below freezing. Snowfall is rare. Recent snow events in Houston include a storm on December 24, 2004 when one inch (2.5 cm) of snow accumulated in parts of the metro area. Falls of at least one inch on both December 10, 2008 and December 4, 2009 marked the first time measurable snowfall had occurred in two consecutive years in the city's recorded history. The coldest temperature officially recorded in Houston was 5 °F on January 23, 1940. Houston has historically received an ample amount of rainfall, averaging about 49.8 in annually per 1981–2010 normals. Localized flooding often occurs, owing to the extremely flat topography and widespread typical clay-silt prairie soils, which do not drain quickly.
Houston has excessive ozone levels and is routinely ranked among the most ozone-polluted cities in the United States. Ground-level ozone, or smog, is Houston's predominant air pollution problem, with the American Lung Association rating the metropolitan area's ozone level 6th on the "Top 10 Most Ozone-Polluted Cities" in 2014. The industries located along the ship channel are a major cause of the city's air pollution. In 2006, Houston's air quality was comparable to that of Los Angeles.
Cityscape.
Houston was incorporated in 1837 under the ward system of representation. The ward designation is the progenitor of the eleven current-day geographically oriented Houston City Council districts. Locations in Houston are generally classified as either being inside or outside the Interstate 610 Loop. The inside encompasses the central business district and many residential neighborhoods that predate World War II. More recently, high-density residential areas have been developed within the loop. The city's outlying areas, suburbs and enclaves are located outside of the loop. Beltway 8 encircles the city another 5 mi farther out.
Though Houston is the largest city in the United States without formal zoning regulations, it has developed similarly to other Sun Belt cities because the city's land use regulations and legal covenants have played a similar role. Regulations include mandatory lot size for single-family houses and requirements that parking be available to tenants and customers. Such restrictions have had mixed results. Though some have blamed the city's low density, urban sprawl, and lack of pedestrian-friendliness on these policies, the city's land use has also been credited with having significant affordable housing, sparing Houston the worst effects of the 2008 real estate crisis. The city issued 42,697 building permits in 2008 and was ranked first in the list of healthiest housing markets for 2009.
Voters rejected efforts to have separate residential and commercial land-use districts in 1948, 1962, and 1993. Consequently, rather than a single central business district as the center of the city's employment, multiple districts have grown throughout the city in addition to downtown which include Uptown, Texas Medical Center, Midtown, Greenway Plaza, Memorial City, Energy Corridor, Westchase, and Greenspoint.
The western view of Downtown Houston skyline
The eastern view of Downtown Houston skyline
North Western View of the Texas Medical Center Skyline
The Uptown Houston skyline
Architecture.
Houston has the third tallest skyline in North America and twelfth tallest in the world, as of 2011. A seven-mile (11 km) system of tunnels and skywalks link downtown buildings containing shops and restaurants, enabling pedestrians to avoid summer heat and rain while walking between buildings.
In the 1960s, Downtown Houston consisted of a collection of mid-rise office structures. Downtown was on the threshold of an energy industry–led boom in 1970. A succession of skyscrapers were built throughout the 1970s—many by real estate developer Gerald D. Hines—culminating with Houston's tallest skyscraper, the 75-floor, 1002 ft-tall JPMorgan Chase Tower (formerly the Texas Commerce Tower), completed in 1982. It is the tallest structure in Texas, 15th tallest building in the United States, and the 85th tallest skyscraper in the world, based on highest architectural feature. In 1983, the 71-floor, 992 ft-tall Wells Fargo Plaza (formerly Allied Bank Plaza) was completed, becoming the second-tallest building in Houston and Texas. Based on highest architectural feature, it is the 17th tallest in the United States and the 95th tallest in the world. In 2007, downtown Houston had over 43 million square feet (4,000,000 m²) of office space.
Centered on Post Oak Boulevard and Westheimer Road, the Uptown District boomed during the 1970s and early 1980s when a collection of mid-rise office buildings, hotels, and retail developments appeared along Interstate 610 west. Uptown became one of the most prominent instances of an edge city. The tallest building in Uptown is the 64-floor, 901 ft-tall, Philip Johnson and John Burgee designed landmark Williams Tower (known as the Transco Tower until 1999). At the time of construction, it was believed to be the world's tallest skyscraper outside of a central business district. The new 20-story Skanska building and BBVA Compass Plaza are the newest office buildings built in the Galleria area after thirty years. The Uptown District is also home to buildings designed by noted architects I. M. Pei, César Pelli, and Philip Johnson. In the late 1990s and early 2000s decade, there was a mini-boom of mid-rise and high-rise residential tower construction, with several over 30 stories tall. Since 2000 more than 30 high-rise buildings have gone up in Houston; all told, 72 high-rises tower over the city, which adds up to about 8,300 units. In 2002, Uptown had more than 23 million square feet (2,100,000 m²) of office space with 16 million square feet (1,500,000 m²) of Class A office space.
Demographics.
Houston is multicultural, in part because of its many academic institutions and strong industries as well as being a major port city. Over 90 languages are spoken in the city. It has among the youngest populations in the nation, partly due to an influx of immigrants into Texas. An estimated 400,000 illegal immigrants reside in the Houston area.
According to the 2010 Census, whites made up 51% of Houston's population; 26% of the total population were non-Hispanic whites. Blacks or African Americans made up 25% of Houston's population. American Indians made up 0.7% of the population. Asians made up 6% (1.7% Vietnamese, 1.3% Chinese, 1.3% Indian, 0.9% Pakistani, 0.4% Filipino, 0.3% Korean, 0.1% Japanese), while Pacific Islanders made up 0.1%. Individuals from some other race made up 15.2% of the city's population, of which 0.2% were non-Hispanic. Individuals from two or more races made up 3.3% of the city. People of Hispanic origin, regardless of race, made up 44% of Houston's population.
At the 2000 Census, there were 1,953,631 people and the population density was 3,371.7 people per square mile (1,301.8/km²). The racial makeup of the city was 49.3% White, 25.3% African American, 5.3% Asian, 0.4% American Indian, 0.1% Pacific Islander, 16.5% from some other race, and 3.1% from two or more races. In addition, Hispanics made up 37.4% of Houston's population while non-Hispanic whites made up 30.8%, down from 62.4% in 1970.
The median income for a household in the city was $37,000, and the median income for a family was $40,000. Males had a median income of $32,000 versus $27,000 for females. The per capita income was $20,000. Nineteen percent of the population and 16% of families were below the poverty line. Out of the total population, 26% of those under the age of 18 and 14% of those 65 and older were living below the poverty line.
Economy.
Houston is recognized worldwide for its energy industry—particularly for oil and natural gas—as well as for biomedical research and aeronautics. Renewable energy sources—wind and solar—are also growing economic bases in Houston. The ship channel is also a large part of Houston's economic base. Because of these strengths, Houston is designated as a global city by the Globalization and World Cities Study Group and Network and by global management consulting firm A.T. Kearney. The Houston area is the top U.S. market for exports, surpassing New York City in 2013, according to data released by the U.S. Department of Commerce's International Trade Administration. In 2012, the Houston–The Woodlands–Sugar Land area recorded $110.3 billion in merchandise exports. Petroleum products, chemicals, and oil and gas extraction equipment accounted for approximately two-thirds of the metropolitan area's exports last year. The top three destinations for exports were Mexico, Canada, and Brazil.
The Houston area is a leading center for building oilfield equipment. Much of Houston's success as a petrochemical complex is due to its busy ship channel, the Port of Houston. The port ranks first in the United States in international commerce, and is the tenth-largest port in the world. Unlike most places, high oil and gasoline prices are beneficial for Houston's economy as many of its residents are employed in the energy industry.
The Houston–The Woodlands–Sugar Land MSA's gross domestic product (GDP) in 2012 was $489 billion, the fourth-largest of any metropolitan area in the United States and larger than Austria's, Venezuela's or South Africa's GDP. Only 26 countries other than the United States have a gross domestic product exceeding Houston's regional gross area product. In 2010, mining, which in Houston consists almost entirely of exploration and production of oil and gas, accounted for 26.3% of Houston's GAP, up sharply in response to high energy prices and a decreased worldwide surplus of oil production capacity; followed by engineering services, health services, and manufacturing.
The University of Houston System's annual impact on the Houston-area's economy equates to that of a major corporation: $1.1 billion in new funds attracted annually to the Houston area, $3.13 billion in total economic benefit, and 24,000 local jobs generated. This is in addition to the 12,500 new graduates the UH System produces every year who enter the workforce in Houston and throughout Texas. These degree-holders tend to stay in Houston. After five years, 80.5 percent of graduates are still living and working in the region.
In 2006, the Houston metropolitan area ranked first in Texas and third in the U.S. within the Category of "Best Places for Business and Careers" by "Forbes" magazine. Foreign governments have established 92 consular offices in metropolitan Houston, the third highest in the nation. Forty foreign governments maintain trade and commercial offices here and 23 active foreign chambers of commerce and trade associations. Twenty-five foreign banks representing 13 nations operate in Houston, providing financial assistance to the international community.
In 2008, Houston received top ranking on Kiplinger's Personal Finance "Best Cities of 2008" list which ranks cities on their local economy, employment opportunities, reasonable living costs and quality of life. The city ranked fourth for highest increase in the local technological innovation over the preceding 15 years, according to "Forbes" magazine. In the same year, the city ranked second on the annual Fortune 500 list of company headquarters, ranked first for "Forbes" "Best Cities for College Graduates", and ranked first on Forbes list of "Best Cities to Buy a Home". In 2010, the city was rated the best city for shopping, according to Forbes.
In 2012, the city was ranked #1 for paycheck worth by Forbes; and in late May 2013, Houston was identified as America's top city for employment creation.
In 2013, Houston was identified as the #1 U.S. city for job creation by the U.S. Bureau of Statistics after it was not only the first major city to regain all the jobs lost in the preceding economic downturn, but after the crash, more than two jobs were added for every one lost. Economist and vice president of research at the Greater Houston Partnership Patrick Jankowski attributed Houston's success to the ability of the region's real estate and energy industries to learn from historical mistakes. Furthermore, Jankowski stated that "more than 100 foreign-owned companies relocated, expanded or started new businesses in Houston" between 2008 and 2010, and this openness to external business boosted job creation during a period when domestic demand was problematically low. Also in 2013, Houston again appeared on "Forbes"' list of Best Places for Business and Careers.
Culture.
Located in the American South, Houston is a diverse city with a large and growing international community. The metropolitan area is home to an estimated 1.1 million (21.4 percent) residents who were born outside the United States, with nearly two-thirds of the area's foreign-born population from south of the United States–Mexico border. Additionally, more than one in five foreign-born residents are from Asia. The city is home to the nation's third-largest concentration of consular offices, representing 86 countries.
Many annual events celebrate the diverse cultures of Houston. The largest and longest running is the annual Houston Livestock Show and Rodeo, held over 20 days from early to late March, is the largest annual livestock show and rodeo in the world. Another large celebration is the annual night-time Houston Pride Parade, held at the end of June. Other annual events include the Houston Greek Festival, Art Car Parade, the Houston Auto Show, the Houston International Festival, and the Bayou City Art Festival, which is considered to be one of the top five art festivals in the United States.
Houston received the official nickname of "Space City" in 1967 because it is the location of NASA's Lyndon B. Johnson Space Center. Other nicknames often used by locals include "Bayou City", "Clutch City", "Magnolia City", "New Houston" (a tribute to the cultural contributions of New Orleans natives who left their city during the 2005 Hurricane Katrina catastrophe), and "H-Town".
The annual Houston Livestock Show and Rodeo held inside the NRG Stadium.
The George R. Brown Convention Center regularly holds various kinds of conventions.
Arts and theater.
The Houston Theater District, located downtown, is home to nine major performing arts organizations and six performance halls. It is the second-largest concentration of theater seats in a downtown area in the United States. Houston is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Houston Grand Opera), ballet (Houston Ballet), music (Houston Symphony Orchestra), and theater (The Alley Theatre). Houston is also home to folk artists, art groups and various small progressive arts organizations. Houston attracts many touring Broadway acts, concerts, shows, and exhibitions for a variety of interests. Facilities in the Theater District include the Jones Hall—home of the Houston Symphony Orchestra and Society for the Performing Arts—and the Hobby Center for the Performing Arts.
The Museum District's cultural institutions and exhibits attract more than 7 million visitors a year. Notable facilities include The Museum of Fine Arts, Houston Museum of Natural Science, the Contemporary Arts Museum Houston, the Station Museum of Contemporary Art, Holocaust Museum Houston, and the Houston Zoo. Located near the Museum District are The Menil Collection, Rothko Chapel, and the Byzantine Fresco Chapel Museum.
Bayou Bend is a 14 acre facility of the Museum of Fine Arts that houses one of America's best collections of decorative art, paintings and furniture. Bayou Bend is the former home of Houston philanthropist Ima Hogg.
The National Museum of Funeral History is located in Houston near the George Bush Intercontinental Airport. The museum houses the original Popemobile used by Pope John Paul II in the 1980s along with numerous hearses, embalming displays and information on famous funerals.
Venues across Houston regularly host local and touring rock, blues, country, dubstep, and Tejano musical acts. While Houston has never been widely known for its music scene, Houston hip-hop has become a significant, independent music scene that is influential nationwide.
Tourism and recreation.
The Theater District is a 17-block area in the center of downtown Houston that is home to the Bayou Place entertainment complex, restaurants, movies, plazas, and parks. Bayou Place is a large multilevel building containing full-service restaurants, bars, live music, billiards, and Sundance Cinema. The Bayou Music Center stages live concerts, stage plays, and stand-up comedy.
Space Center Houston is the official visitors' center of NASA's Lyndon B. Johnson Space Center. The Space Center has many interactive exhibits including moon rocks, a shuttle simulator, and presentations about the history of NASA's manned space flight program. Other tourist attractions include the Galleria (Texas's largest shopping mall located in the Uptown District), Old Market Square, the Downtown Aquarium, and Sam Houston Race Park.
Of worthy mention are Houston's current Chinatown and the Mahatma Gandhi District. Both areas offer a picturesque view of Houston's multicultural makeup. Restaurants, bakeries, traditional-clothing boutiques and specialty shops can be found in both areas.
Houston is home to 337 parks including Hermann Park, Terry Hershey Park, Lake Houston Park, Memorial Park, Tranquility Park, Sesquicentennial Park, Discovery Green, and Sam Houston Park. Within Hermann Park are the Houston Zoo and the Houston Museum of Natural Science. Sam Houston Park contains restored and reconstructed homes which were originally built between 1823 and 1905. There is a proposal to open the city's first botanic garden at Herman Brown Park.
Of the 10 most populous U.S. cities, Houston has the most total area of parks and green space, 56405 acre. The city also has over 200 additional green spaces—totaling over 19600 acre that are managed by the city—including the Houston Arboretum and Nature Center. The Lee and Joe Jamail Skatepark is a public skatepark owned and operated by the city of Houston, and is one of the largest skateparks in Texas consisting of 30,000 (2,800 m2) square foot in-ground facility. The Gerald D. Hines Waterwall Park—located in the Uptown District of the city—serves as a popular tourist attraction, weddings, and various celebrations. A 2011 study by Walk Score ranked Houston the 23rd most walkable of the 50 largest cities in the United States. Wet'n'Wild SplashTown is a water park located north of Houston. A 640-acre theme park, called the Grand Texas Theme Park, will open in 2015 and is located near Houston in New Caney, Texas.
The Bayport Cruise Terminal on the Houston Ship Channel will become port of call for both Princess Cruises and Norwegian Cruise Line in 2013-2014.
Sports.
Houston has sports teams for every major professional league except the National Hockey League (NHL). The Houston Astros are a Major League Baseball (MLB) expansion team formed in 1962 (known as the "Colt .45s" until 1965) that made one World Series appearance in 2005. The Houston Rockets are a National Basketball Association (NBA) franchise based in Houston since 1971. The Rockets have won two NBA Championships: in 1994 and 1995. The Houston Texans are a National Football League (NFL) expansion team formed in 2002. The Houston Dynamo are a Major League Soccer (MLS) franchise that has been based in Houston since 2006. The Houston Dash play in the National Women's Soccer League.
Minute Maid Park (home of the Astros) and Toyota Center (home of the Rockets), are located in downtown Houston. Houston has the NFL's first retractable-roof stadium with natural grass, NRG Stadium (home of the Texans). Minute Maid Park is also a retractable-roof stadium. Toyota Center also has the largest screen for an indoor arena in the United States built to coincide with the arena's hosting of the 2013 NBA All-Star Game. BBVA Compass Stadium is a soccer-specific stadium for the Dynamo and Dash, located in East Downtown. In addition, NRG Astrodome was the first indoor stadium in the world, built in 1965. Other sports facilities include Hofheinz Pavilion (Houston Cougars basketball), Rice Stadium (Rice Owls football), and Reliant Arena. TDECU Stadium is where the University of Houston Houston Cougars football team plays.
Houston has hosted several major sports events: the 1968, 1986 and 2004 Major League Baseball All-Star Games; the 1989, 2006 and 2013 NBA All-Star Games; Super Bowl VIII and Super Bowl XXXVIII, as well as hosting the 2005 World Series and 1981, 1986, 1994 and 1995 NBA Finals, winning the latter two. Super Bowl LI is currently slated to be hosted in NRG Stadium in 2017.
The city has hosted several major professional and college sporting events, including the annual Houston Open golf tournament. Houston hosts the annual NCAA College Baseball Classic every February and NCAA football's Texas Bowl in December.
The Grand Prix of Houston, an annual auto race on the IndyCar Series circuit is held on a 1.7-mile temporary street circuit in Reliant Park. The October 2013 event was held using a tweaked version of the 2006-2007 course. The event has a 5-year race contract through 2017 with IndyCar.
Government and politics.
The city of Houston has a strong mayoral form of municipal government. Houston is a home rule city and all municipal elections in the state of Texas are nonpartisan. The City's elected officials are the mayor, city controller and 16 members of the Houston City Council. The current mayor of Houston is Annise Parker, a Democrat elected on a nonpartisan ballot whose third (and final) term in office will expire at the end of 2015. Houston's mayor serves as the city's chief administrator, executive officer, and official representative, and is responsible for the general management of the city and for seeing that all laws and ordinances are enforced.
The original city council line-up of 14 members (nine district-based and five at-large positions) was based on a U.S. Justice Department mandate which took effect in 1979. At-large council members represent the entire city. Under the city charter, once the population in the city limits exceeded 2.1 million residents, two additional districts were to be added. The city of Houston's official 2010 census count was 600 shy of the required number; however, as the city was expected to grow beyond 2.1 million shortly thereafter, the two additional districts were added for, and the positions filled during, the August 2011 elections.
The city controller is elected independently of the mayor and council. The controller's duties are to certify available funds prior to committing such funds and processing disbursements. The city's fiscal year begins on July 1 and ends on June 30. Ronald Green is the city controller, serving his first term as of January 2010.
As the result of a 1991 referendum in Houston, a mayor is elected for a two-year term, and can be elected to as many as three consecutive terms. The term limits were spearheaded by conservative political activist Clymer Wright. The city controller and city council members are also subject to the same two-year, three-term limitations.
Houston is considered to be a politically divided city whose balance of power often sways between Republicans and Democrats. Much of the city's wealthier areas vote Republican while the city's working class and minority areas vote Democratic. According to the 2005 Houston Area Survey, 68 percent of non-Hispanic whites in Harris County are declared or favor Republicans while 89 percent of non-Hispanic blacks in the area are declared or favor Democrats. About 62 percent Hispanics (of any race) in the area are declared or favor Democrats. The city has often been known to be the most politically diverse city in Texas, a state known for being generally conservative. As a result the city is often a contested area in statewide elections. In 2009, Houston became the first US city with a population over 1 million citizens to elect a gay mayor, by electing Annise Parker.
Crime.
Houston's murder rate ranked 46th of U.S. cities with a population over 250,000 in 2005 (per capita rate of 16.3 murders per 100,000 population). In 2010, the city's murder rate (per capita rate of 11.8 murders per 100,000 population) was ranked fifth among U.S. cities with a population of over 750,000 (behind New York City, Chicago, Detroit, Dallas, and Philadelphia) according to the FBI.
Murders fell by 37 percent from January to June 2011, compared with the same period in 2010. Houston's total crime rate including violent and nonviolent crimes decreased by 11 percent.
Houston is a significant hub for trafficking of cocaine, cannabis, heroin, MDMA, and methamphetamine due to its size and proximity to major illegal drug exporting nations. Houston is one of the country's largest hubs for human trafficking.
In the early 1970s, Houston, Pasadena and several coastal towns were the site of the Houston Mass Murders, which at the time were the deadliest case of serial killing in American history.
Education.
Seventeen school districts exist within the city of Houston. The Houston Independent School District (HISD) is the seventh-largest school district in the United States. HISD has 112 campuses that serve as magnet or vanguard schools—specializing in such disciplines as health professions, visual and performing arts, and the sciences. There are also many charter schools that are run separately from school districts. In addition, some public school districts also have their own charter schools.
The Houston area encompasses more than 300 private schools, many of which are accredited by Texas Private School Accreditation Commission recognized agencies. The Houston Area Independent Schools offer education from a variety of different religious as well as secular viewpoints. The Houston area Catholic schools are operated by the Archdiocese of Galveston-Houston.
Colleges and universities.
 Four separate and distinct state universities are located in Houston. The University of Houston is a nationally recognized Tier One research university, and is the flagship institution of the University of Houston System. The third-largest university in Texas, the University of Houston has nearly 40,000 students on its 667-acre campus in southeast Houston. The University of Houston–Clear Lake and the University of Houston–Downtown are stand-alone universities; they are not branch campuses of the University of Houston. Located in the historic community of Third Ward is Texas Southern University, one of the largest historically black colleges and universities in the United States.
Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic University to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes such as the James A. Baker Institute for Public Policy.
Three community college districts exist with campuses in and around Houston. The Houston Community College System serves most of Houston. The northwestern through northeastern parts of the city are served by various campuses of the Lone Star College System, while the southeastern portion of Houston is served by San Jacinto College, and a northeastern portion is served by Lee College. The Houston Community College and Lone Star College systems are within the 10 largest institutions of higher learning in the United States.
Media.
The primary network-affiliated television stations are KPRC-TV (NBC), KHOU-TV (CBS), KTRK-TV (ABC), and KRIV-TV (Fox).
The Houston–The Woodlands–Sugar Land metropolitan area is served by one public television station and two public radio stations. KUHT ("HoustonPBS") is a PBS member station and is the first public television station in the United States. Houston Public Radio is listener-funded and comprises two NPR member stations: KUHF ("KUHF News") and KUHA ("Classical 91.7"). KUHF is news/talk radio and KUHA is a classical music station. The University of Houston System owns and holds broadcasting licenses to KUHT, KUHF, and KUHA. The stations broadcast from the Melcher Center for Public Broadcasting, located on the campus of the University of Houston.
Houston is served by the "Houston Chronicle", its only major daily newspaper with wide distribution. The Hearst Corporation, which owns and operates the "Houston Chronicle", bought the assets of the "Houston Post"—its long-time rival and main competition—when "Houston Post" ceased operations in 1995. The "Houston Post" was owned by the family of former Lieutenant Governor Bill Hobby of Houston. The only other major publication to serve the city is the "Houston Press"—a free alternative weekly with a weekly readership of more than 300,000.
Infrastructure.
Healthcare.
Houston is the seat of the internationally renowned Texas Medical Center, which contains the world's largest concentration of research and healthcare institutions. All 49 member institutions of the Texas Medical Center are non-profit organizations. They provide patient and preventive care, research, education, and local, national, and international community well-being.
Employing more than 73,600 people, institutions at the medical center include 13 hospitals and two specialty institutions, two medical schools, four nursing schools, and schools of dentistry, public health, pharmacy, and virtually all health-related careers. It is where one of the first—and still the largest—air emergency service, Life Flight, was created, and a very successful inter-institutional transplant program was developed. More heart surgeries are performed at the Texas Medical Center than anywhere else in the world.
Some of the academic and research health institutions at the center include MD Anderson Cancer Center, Baylor College of Medicine, UT Health Science Center, Memorial Hermann Hospital, The Methodist Hospital, Texas Children's Hospital, and University of Houston College of Pharmacy.
The Baylor College of Medicine has annually been considered within the top ten medical schools in the nation; likewise, the MD Anderson Cancer Center has consistently ranked as one of the top two U.S. hospitals specializing in cancer care by "U.S. News & World Report" since 1990. The Menninger Clinic, a renowned psychiatric treatment center, is affiliated with Baylor College of Medicine and The Methodist Hospital System. With hospital locations nationwide and headquarters in Houston, the Triumph Healthcare hospital system is the third largest long term acute care provider nationally.
Transportation.
Highways.
The predominant form of transportation in Houston is the automobile with 71.7 percent of residents driving alone to work This is facilitated through Houston's freeway system, comprising 739.3 mi of freeways and expressways in a ten-county metropolitan area. However, the Texas Transportation Institute's annual Urban Mobility Report found that Houston had the fourth-worst congestion in the country with commuters spending an average of 58 hours in traffic in 2009.
Houston's highway system has a hub-and-spoke freeway structure serviced by multiple loops. The innermost loop is Interstate 610, which encircles downtown, the medical center, and many core neighborhoods with around a 8 mi diameter. Beltway 8 and its freeway core, the Sam Houston Tollway, form the middle loop at a diameter of roughly 23 mi. A proposed highway project, State Highway 99 (Grand Parkway), will form a third loop outside of Houston, totaling 180 miles in length and making an almost-complete circumference, with the exception of crossing the ship channel. As of June 2014, two of eleven segments of State Highway 99 have been completed to the west of Houston, and three northern segments, totaling 38 miles, are actively under construction and scheduled to open to traffic late in 2015. In addition to the Sam Houston Tollway loop mentioned above, the Harris County Toll Road Authority currently operates four spoke tollways: The Katy Managed Lanes of Interstate 10, the Hardy Toll Road, the Westpark Tollway, and the Fort Bend Parkway Extension. Other spoke roads either planned or under construction include Crosby Freeway, and the future Alvin Freeway.
Houston's freeway system is monitored by Houston TranStar—a partnership of four government agencies that are responsible for providing transportation and emergency management services to the region.
Transit systems.
The Metropolitan Transit Authority of Harris County (METRO) provides public transportation in the form of buses, light rail, and lift vans.
METRO began light rail service on January 1, 2004, with the inaugural track ("Red Line") running about 8 mi from the University of Houston–Downtown (UHD), which traverses through the Texas Medical Center and terminates at NRG Park. METRO is currently in the design phase of a 10-year expansion plan that will add five more lines. and expand the current Red Line. Amtrak, the national passenger rail system, provides service three times a week to Houston via the "Sunset Limited" (Los Angeles–New Orleans), which stops at a train station on the north side of the downtown area. The station saw 14,891 boardings and alightings in fiscal year 2008. In 2012, there was a 25 percent increase in ridership to 20,327 passengers embarking from the Houston Amtrak station.
Cycling.
Houston has the largest number of bike commuters in Texas with over 160 miles of dedicated bikeways. The city is currently in the process of expanding its on and off street bikeway network. A new Bicycle sharing system known as Houston B-Cycle currently operates 29 different stations in downtown and neighboring areas
Airports.
Houston is served by three airports, two of which are commercial that served 52 million passengers in 2007 and managed by the Houston Airport System. The Federal Aviation Administration and the state of Texas selected the "Houston Airport System as Airport of the Year" for 2005, largely because of its multi-year, $3.1 billion airport improvement program for both major airports in Houston.
The primary city airport is George Bush Intercontinental Airport (IAH), the tenth-busiest in the United States for total passengers, and twenty eight-busiest worldwide. Bush Intercontinental currently ranks fourth in the United States for non-stop domestic and international service with 182 destinations. In 2006, the United States Department of Transportation named IAH the fastest-growing of the top ten airports in the United States. The Houston Air Route Traffic Control Center stands on the George Bush Intercontinental Airport grounds.
Houston was the headquarters of Continental Airlines until its 2010 merger with United Airlines with headquarters in Chicago; regulatory approval for the merger was granted in October of that year. Bush Intercontinental became United Airline's largest airline hub. The airline retained a significant operational presence in Houston while offering more than 700 daily departures from the city. In early 2007, Bush Intercontinental Airport was named a model "port of entry" for international travelers by U.S. Customs and Border Protection.
The second-largest commercial airport is William P. Hobby Airport (named Houston International Airport until 1967) which operates primarily small to medium-haul domestic flights. Houston's aviation history is showcased in the 1940 Air Terminal Museum located in the old terminal building on the west side of the airport. Hobby Airport has been recognized with two awards for being one of the top five performing airports in the world and for customer service by Airports Council International.
Houston's third municipal airport is Ellington Airport (a former U.S. Air Force base) used by military, government, NASA, and general aviation sectors.
Pipelines.
Houston is the beginning or end point of numerous oil, gas, and products pipelines:
Sister cities.
The Houston Office of Protocol and International Affairs is the city's liaison to Houston's sister city associations and to the national governing organization, Sister Cities International. Through their official city-to-city relationships, these volunteer associations promote people-to-people diplomacy and encourage citizens to develop mutual trust and understanding through commercial, cultural, educational, and humanitarian exchanges.
Further reading.
</dl>

</doc>
<doc id="13776" url="http://en.wikipedia.org/wiki?curid=13776" title="Head (disambiguation)">
Head (disambiguation)

The head is the part of an animal that usually comprises the brain, eyes, ears, nose, and mouth.
Head may also refer to:

</doc>
<doc id="13777" url="http://en.wikipedia.org/wiki?curid=13777" title="Hard disk drive">
Hard disk drive

A hard disk drive (HDD), hard disk, hard drive or fixed disk is a data storage device used for storing and retrieving digital information using one or more rigid ("hard") rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order rather than sequentially. An HDD retains its data even when powered off.
Introduced by IBM in 1956, HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDD units, though most current units are manufactured by Seagate, Toshiba and Western Digital. Worldwide disk storage revenues were US $32 billion in 2013, down 3% from 2012.
The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 billion bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).
The two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as SATA (Serial ATA), USB or SAS (Serial attached SCSI) cables.
s of 2015[ [update]], the primary competing technology for secondary storage is flash memory in the form of solid-state drives (SSDs), but HDDs remain the dominant medium for secondary storage due to advantages in price per unit of storage and recording capacity. However, SSDs are replacing HDDs where speed, power consumption and durability are more important considerations.
History.
HDDs were introduced in 1956 as data storage for an IBM real-time transaction processing computer and were developed for use with general-purpose mainframe and minicomputers. The first IBM drive, the 350 RAMAC, was approximately the size of two refrigerators and stored five million six-bit characters (3.75 megabytes) on a stack of 50 disks.
In 1962 IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called "fixed disk" drives.
Some high-performance HDDs were manufactured with one head per track (e.g. IBM 2305) so that no time was lost physically moving the heads to a track. Known as fixed-head or head-per-track disk drives they were very expensive and are no longer in production.
In 1973, IBM introduced a new type of HDD codenamed "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.
Like the first removable pack drive, the first "Winchester" drives used platters 14 in in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a 5+1/4 in form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.
As the 1980s began, HDDs were a rare and very expensive additional feature in PCs, but by the late 1980s their cost had been reduced to the point where they were standard on all but the cheapest computers.
Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.
External HDDs remained popular for much longer on the Apple Macintosh. Every Mac made between 1986 and 1998 has a SCSI port on the back, making external expansion easy; also, "toaster" Compact Macs did not have easily accessible HDD bays (or, in the case of the Mac Plus, any hard drive bay at all), so on those models, external SCSI disks were the only reasonable option.
The 2011 Thailand floods damaged manufacturing plants, and impacted hard disk drive cost adversely in 2011-2013.
Driven by ever increasing areal density since their invention, HDDs have continuously improved their characteristics; a few highlights are listed in the table above. At the same time, market application expanded from mainframe computers of the late 1950s to most mass storage applications including computers and consumer applications such as storage of entertainment content.
Technology.
Magnetic recording.
An HDD records data by magnetizing a thin film of ferromagnetic material on a disk. Sequential changes in the direction of magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an encoding scheme, such as run-length limited encoding, which determines how the data is represented by the magnetic transitions.
A typical HDD design consists of a "spindle" that holds flat circular disks, also called platters, which hold the recorded data. The platters are made from a non-magnetic material, usually aluminium alloy, glass, or ceramic, and are coated with a shallow layer of magnetic material typically 10–20 nm in depth, with an outer layer of carbon for protection. For reference, a standard piece of copy paper is 0.07 -.
The platters in contemporary HDDs are spun at speeds varying from 4,200 rpm in energy-efficient portable devices, to 15,000 rpm for high-performance servers. The first HDDs spun at 1,200 rpm and, for many years, 3,600 rpm was the norm. As of December 2013, the platters in most consumer-grade HDDs spin at either 5,400 rpm or 7,200 rpm.
Information is written to and read from a platter as it rotates past devices called read-and-write heads that operate very close (often tens of nanometers) over the magnetic surface. The read-and-write head is used to detect and modify the magnetization of the material immediately under it.
In modern drives there is one head for each magnetic platter surface on the spindle, mounted on a common arm. An actuator arm (or access arm) moves the heads on an arc (roughly radially) across the platters as they spin, allowing each head to access almost the entire surface of the platter as it spins. The arm is moved using a voice coil actuator or in some older designs a stepper motor. Early hard disk drives wrote data at some constant bits per second, resulting in all tracks having the same amount of data per track but modern drives (since the 1990s) use zone bit recording—increasing the write speed from inner to outer zone and thereby storing more data per track in the outer zones.
In modern drives, the small size of the magnetic regions creates the danger that their magnetic state might be lost because of thermal effects, thermally induced magnetic instability which is commonly known as the "superparamagnetic limit." To counter this, the platters are coated with two parallel magnetic layers, separated by a 3-atom layer of the non-magnetic element ruthenium, and the two layers are magnetized in opposite orientation, thus reinforcing each other. Another technology used to overcome thermal effects to allow greater recording densities is perpendicular recording, first shipped in 2005, and as of 2007 the technology was used in many HDDs.
Components.
A typical HDD has two electric motors; a spindle motor that spins the disks and an actuator (motor) that positions the read/write head assembly across the spinning disks. The disk motor has an external rotor attached to the disks; the stator windings are fixed in place. Opposite the actuator at the end of the head support arm is the read-write head; thin printed-circuit cables connect the read-write heads to amplifier electronics mounted at the pivot of the actuator. The head support arm is very light, but also stiff; in modern drives, acceleration at the head reaches 550 "g".
The "actuator" is a permanent magnet and moving coil motor that swings the heads to the desired position. A metal plate supports a squat neodymium-iron-boron (NIB) high-flux magnet. Beneath this plate is the moving coil, often referred to as the "voice coil" by analogy to the coil in loudspeakers, which is attached to the actuator hub, and beneath that is a second NIB magnet, mounted on the bottom plate of the motor (some drives only have one magnet).
The voice coil itself is shaped rather like an arrowhead, and made of doubly coated copper magnet wire. The inner layer is insulation, and the outer is thermoplastic, which bonds the coil together after it is wound on a form, making it self-supporting. The portions of the coil along the two sides of the arrowhead (which point to the actuator bearing center) interact with the magnetic field, developing a tangential force that rotates the actuator. Current flowing radially outward along one side of the arrowhead and radially inward on the other produces the tangential force. If the magnetic field were uniform, each side would generate opposing forces that would cancel each other out. Therefore the surface of the magnet is half N pole, half S pole, with the radial dividing line in the middle, causing the two sides of the coil to see opposite magnetic fields and produce forces that add instead of canceling. Currents along the top and bottom of the coil produce radial forces that do not rotate the head.
The HDD's electronics control the movement of the actuator and the rotation of the disk, and perform reads and writes on demand from the disk controller. Feedback of the drive electronics is accomplished by means of special segments of the disk dedicated to servo feedback. These are either complete concentric circles (in the case of dedicated servo technology), or segments interspersed with real data (in the case of embedded servo technology). The servo feedback optimizes the signal to noise ratio of the GMR sensors by adjusting the voice-coil of the actuated arm. The spinning of the disk also uses a servo motor. Modern disk firmware is capable of scheduling reads and writes efficiently on the platter surfaces and remapping sectors of the media which have failed.
Error rates and handling.
Modern drives make extensive use of error correction codes (ECCs), particularly Reed–Solomon error correction. These techniques store extra bits, determined by mathematical formulas, for each block of data; the extra bits allow many errors to be corrected invisibly. The extra bits themselves take up space on the HDD, but allow higher recording densities to be employed without causing uncorrectable errors, resulting in much larger storage capacity. For example, a typical 1 TB hard disk with 512-byte sectors provides additional capacity of about 93 GB for the ECC data.
In the newest drives, as of 2009, low-density parity-check codes (LDPC) were supplanting Reed-Solomon; LDPC codes enable performance close to the Shannon Limit and thus provide the highest storage density available.
Typical hard disk drives attempt to "remap" the data in a physical sector that is failing to a spare physical sector provided by the drive's "spare sector pool" (also called "reserve pool"), while relying on the ECC to recover stored data while the amount of errors in a bad sector is still low enough. The S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology) feature counts the total number of errors in the entire HDD fixed by ECC (although not on all hard drives as the related S.M.A.R.T attributes "Hardware ECC Recovered" and "Soft ECC Correction" are not consistently supported), and the total number of performed sector remappings, as the occurrence of many such errors may predict an HDD failure.
The "No-ID Format", developed by IBM in the mid-1990s, contains information about which sectors are bad and where remapped sectors have been located.
Only a tiny fraction of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits, and another SAS enterprise disk from 2013 specifies similar error rates. Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat data corruption, specifies similar error rates in 2005.
The worst type of errors are those that go unnoticed, and are not even detected by the disk firmware or the host operating system. These errors are known as silent data corruption, some of which may be caused by hard disk drive malfunctions.
Future development.
HDD areal density's long term exponential growth has been similar to a 41% per year Moore's law rate; the rate was 60–100% per year beginning in the early 1990s and continuing until about 2005, an increase which Gordon Moore (1997) called "flabbergasting" and he speculated that HDDs had "moved at least as fast as the semiconductor complexity." However, the rate decreased dramatically around 2006 and, during 2011–2014, growth was in the annual range of 5–10%. Disk cost per byte improved nearly -45% per year during 1990–2010, and slowed after 2010 due to the Thailand floods and difficulty in migrating from perpendicular recording to newer technologies. Moore (2005) further observed that growth cannot continue forever.
Increasing areal density corresponds to an ever decreasing bit cell size. In 2013, a production desktop 3 TB HDD (with four platters) would have had an areal density of about 500 Gbit/in2 which would have amounted to a bit cell comprising about 18 magnetic grains (11 by 1.6 grains). Since the mid-2000s areal density progress has increasingly been challenged by a superparamagnetic trilemma involving grain size, grain magnetic strength and ability of the head to write. In order to maintain acceptable signal to noise smaller grains are required; smaller grains may self-reverse (thermal instability) unless their magnetic strength is increased, but known write head materials are unable to generate a magnetic field sufficient to write the medium. Several new magnetic storage technologies are being developed to overcome or at least abate this trilemma and thereby maintain the competitiveness of HDDs with respect to products such as flash memory-based solid-state drives (SSDs).
In 2013, Seagate introduced one such technology, shingled magnetic recording (SMR). Additionally, SMR comes with design complexities that may cause reduced write performance. Other new recording technologies that, as of 2015[ [update]], still remain under development include heat-assisted magnetic recording (HAMR), microwave-assisted magnetic recording (MAMR), two-dimensional magnetic recording (TDMR), bit-patterned recording (BPR), and "current perpendicular to plane" giant magnetoresistance (CPP/GMR) heads.
Depending upon assumptions on feasibility and timing of these technologies, the median forecast by industry observers and analysts for 2016 and beyond for areal density growth is 20% per year with a range of 10% to 40%. The ultimate limit for the BPR technology may be the superparamagnetic limit of a single particle that is estimated to be about two orders of magnitude higher than the 500 Gbits/in2 density represented by 2013 production desktop HDDs.
Capacity.
The capacity of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by a drive or system manufacturer; this can be caused by a combination of factors: the operating system using some space, different units used while calculating capacity, or data redundancy.
Calculation.
Modern hard disk drives appear to their interface as a contiguous set of logical blocks, so the gross drive capacity may be calculated by multiplying the number of blocks by the block size. This information is available from the manufacturer's specification and from the drive itself through use of special utilities invoking low level commands.
The gross capacity of older HDDs may be calculated as the product of the number of cylinders per zone, the number of bytes per sector (most commonly 512), and the count of zones of the drive. Some modern SATA drives also report cylinder-head-sector (CHS) values, but these are not actual physical parameters since the reported numbers are constrained by historic operating system interfaces. The C/H/S scheme has been replaced by logical block addressing. In some cases, to try to "force-fit" the CHS scheme to large-capacity drives, the number of heads was given as 64, although no modern drive has anywhere near 32 platters: the typical 2 TB hard disk as of 2013 has two 1 TB platters, and 4 TB drives use four platters.
In modern HDDs, spare capacity for defect management is not included in the published capacity; however, in many early HDDs a certain number of sectors were reserved as spares, thereby reducing the capacity available to end users.
For RAID subsystems, data integrity and fault-tolerance requirements also reduce the realized capacity. For example, a RAID1 subsystem will be about half the total capacity as a result of data mirroring. RAID5 subsystems with x drives, would lose 1/x of capacity to parity. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provides a great deal of fault-tolerance. Most RAID vendors use some form of checksums to improve data integrity at the block level. For many vendors, this involves using HDDs with sectors of 520 bytes per sector to contain 512 bytes of user data and eight checksum bytes or using separate 512-byte sectors for the checksum data.
In some systems, there may be hidden partitions used for system recovery that reduce the capacity available to the end user.
System use.
The presentation of a hard disk drive to its host is determined by the disk controller. The actual presentation may differ substantially from the drive's native interface, particularly in mainframes or servers. Modern HDDs, such as SAS and SATA drives, appear at their interfaces as a contiguous set of logical blocks that are typically 512 bytes long, though the industry is in the process of changing to the 4,096-byte logical blocks layout, known as the Advanced Format (AF).
The process of initializing these logical blocks on the physical disk platters is called "low-level formatting", which is usually performed at the factory and is not normally changed in the field. As a next step in preparing an HDD for use, "high-level formatting" writes partition and file system structures into selected logical blocks to make the remaining logical blocks available to the host's operating system and its applications. The file system uses some of the disk space to structure the HDD and organize files, recording their file names and the sequence of disk areas that represent the file. Examples of data structures stored on disk to retrieve files include the File Allocation Table (FAT) in the DOS file system and inodes in many UNIX file systems, as well as other operating system data structures (also known as metadata). As a consequence, not all the space on an HDD is available for user files, but this system overhead is usually negligible.
Units.
The total capacity of HDDs is given by manufacturers in SI-based units 
such as gigabytes (1 GB = 1,000,000,000 bytes) and terabytes (1 TB = 1,000,000,000,000 bytes). The practice of using SI-based prefixes (denoting powers of 1,000) in the hard disk drive and computer industries dates back to the early days of computing; by the 1970s, "million", "mega" and "M" were consistently used in the decimal sense for drive capacity. However, capacities of memory (RAM, ROM) and CDs are traditionally quoted using binary prefixes, meaning powers of 1,024.
Computers internally do not represent either hard drive or memory capacity in powers of 1,024; reporting it in this manner is a convention. Microsoft Windows family of operating systems uses the binary convention when reporting storage capacity, so a HDD offered by its manufacturer as a 1 TB drive is reported by these operating systems as a 931 GB HDD. Mac OS X 10.6 ("Snow Leopard") uses decimal convention when reporting HDD capacity.
The difference between the decimal and binary prefix interpretation caused some consumer confusion and led to class action suits against HDD manufacturers. The plaintiffs argued that the use of decimal prefixes effectively misled consumers while the defendants denied any wrongdoing or liability, asserting that their marketing and advertising complied in all respects with the law and that no class member sustained any damages or injuries.
Form factors.
IBM's first hard drive, the IBM 350, used a stack of fifty 24-inch platters and was of a size comparable to two large refrigerators. In 1962, IBM introduced its model 1311 disk, which used six 14-inch (nominal size) platters in a removable pack and was roughly the size of a washing machine. This became a standard platter size and drive form-factor for many years, used also by other manufacturers. The IBM 2314 used platters of the same size in an eleven-high pack and introduced the "drive in a drawer" layout, although the "drawer" was not the complete drive. 
Later drives were designed to fit entirely into a chassis that would mount in a 19-inch rack. Digital's RK05 and RL01 were early examples using single 14-inch platters in removable packs, the entire drive fitting in a 10.5-inch-high rack space (six rack units). In the mid-to-late 1980s the similarly sized Fujitsu Eagle, which used (coincidentally) 10.5-inch platters, was a popular product. 
Such large platters were never used with microprocessor-based systems. With increasing sales of microcomputers having built in floppy-disk drives (FDDs), HDDs that would fit to the FDD mountings became desirable. Thus HDD "Form factors", initially followed those of 8-inch, 5.25-inch, and 3.5-inch floppy disk drives. Because there were no smaller floppy disk drives, smaller HDD form factors developed from product offerings or industry standards.
s of 2012[ [update]], 2.5-inch and 3.5-inch hard disks were the most popular sizes.
By 2009 all manufacturers had discontinued the development of new products for the 1.3-inch, 1-inch and 0.85-inch form factors due to falling prices of flash memory, which has no moving parts.
While these sizes are customarily described by an approximately correct figure in inches, actual sizes have long been specified in millimeters.
Performance characteristics.
Time to access data.
The factors that limit the time to access the data on an HDD are mostly related to the mechanical nature of the rotating disks and moving heads. Seek time is a measure of how long it takes the head assembly to travel to the track of the disk that contains data. Rotational latency is incurred because the desired disk sector may not be directly under the head when data transfer is requested. These two delays are on the order of milliseconds each. The bit rate or data transfer rate (once the head is in the right position) creates delay which is a function of the number of blocks transferred; typically relatively small, but can be quite long with the transfer of large contiguous files. Delay may also occur if the drive disks are stopped to save energy.
An HDD's "Average Access Time" is its average seek time which technically is the time to do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.
Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk. Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.
Time to access data can be improved by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the amount of data under a set of heads, thereby potentially reducing seek activity for a given amount of data. The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.
Seek time.
Average seek time ranges from under 4 ms for high-end server drives to 15 ms for mobile drives, with the most common mobile drives at about 12 ms and the most common desktop type typically being around 9 ms. The first HDD had an average seek time of about 600 ms; by the middle of 1970s HDDs were available with seek times of about 25 ms. Some early PC drives used a stepper motor to move the heads, and as a result had seek times as slow as 80–120 ms, but this was quickly improved by voice coil type actuation in the 1980s, reducing seek times to around 20 ms. Seek time has continued to improve slowly over time.
Some desktop and laptop computer systems allow the user to make a tradeoff between seek performance and drive noise. Faster seek rates typically require more energy usage to quickly move the heads across the platter, causing louder noises from the pivot bearing and greater device vibrations as the heads are rapidly accelerated during the start of the seek motion and decelerated at the end of the seek motion. Quiet operation reduces movement speed and acceleration rates, but at a cost of reduced seek performance.
Latency.
Latency is the delay for the rotation of the disk to bring the required disk sector under the read-write mechanism. It depends on rotational speed of a disk, measured in revolutions per minute (rpm). Average rotational latency is shown in the table on the right, based on the statistical relation that the average latency in milliseconds for such a drive is one-half the rotational period.
Data transfer rate.
s of 2010[ [update]], a typical 7,200-rpm desktop HDD has a sustained "disk-to-buffer" data transfer rate up to 1,030 Mbits/sec. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the "buffer-to-computer" interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.
HDD data transfer rate depends upon the rotational speed of the platters and the data recording density. Because heat and vibration limit rotational speed, advancing density becomes the main method to improve sequential transfer rates. Higher speeds require a more powerful spindle motor, which creates more heat. While areal density advances by increasing both the number of tracks across the disk and the number of sectors per track, only the latter increases the data transfer rate for a given rpm. Since data transfer rate performance only tracks one of the two components of areal density, its performance improves at a lower rate.
Other considerations.
Other performance considerations include power consumption, audible noise, and shock resistance.
Access and interfaces.
HDDs are accessed over one of a number of bus types, including as of 2011[ [update]] parallel ATA (PATA, also called IDE or EIDE; described before the introduction of SATA as ATA), Serial ATA (SATA), SCSI, Serial Attached SCSI (SAS), and Fibre Channel. Bridge circuitry is sometimes used to connect HDDs to buses with which they cannot communicate natively, such as IEEE 1394, USB and SCSI.
Modern HDDs present a consistent interface to the rest of the computer, no matter what data encoding scheme is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.
Modern interfaces connect an HDD to a host bus interface adapter (today typically integrated into the "south bridge") with one data/control cable. Each drive also has an additional power cable, usually direct to the power supply unit.
Integrity and failure.
Due to the extremely close spacing between the heads and the disk surface, HDDs are vulnerable to being damaged by a head crash—a failure of the disk in which the head scrapes across the platter surface, often grinding away the thin magnetic film and causing data loss. Head crashes can be caused by electronic failure, a sudden power failure, physical shock, contamination of the drive's internal enclosure, wear and tear, corrosion, or poorly manufactured platters and heads.
The HDD's spindle system relies on air density inside the disk enclosure to support the heads at their proper "flying height" while the disk rotates. HDDs require a certain range of air densities in order to operate properly. The connection to the external environment and density occurs through a small hole in the enclosure (about 0.5 mm in breadth), usually with a filter on the inside (the "breather filter"). If the air density is too low, then there is not enough lift for the flying head, so the head gets too close to the disk, and there is a risk of head crashes and data loss. Specially manufactured sealed and pressurized disks are needed for reliable high-altitude operation, above about 3000 m. Modern disks include temperature sensors and adjust their operation to the operating environment. Breather holes can be seen on all disk drives—they usually have a sticker next to them, warning the user not to cover the holes. The air inside the operating drive is constantly moving too, being swept in motion by friction with the spinning platters. This air passes through an internal recirculation (or "recirc") filter to remove any leftover contaminants from manufacture, any particles or chemicals that may have somehow entered the enclosure, and any particles or outgassing generated internally in normal operation. Very high humidity present for extended periods of time can corrode the heads and platters.
For giant magnetoresistive (GMR) heads in particular, a minor head crash from contamination (that does not remove the magnetic surface of the disk) still results in the head temporarily overheating, due to friction with the disk surface, and can render the data unreadable for a short period until the head temperature stabilizes (so called "thermal asperity", a problem which can partially be dealt with by proper electronic filtering of the read signal).
When the logic board of a hard disk fails, the drive can often be restored to functioning order and the data recovered by replacing the circuit board of one of an identical hard disk. In the case of read-write head faults, they can be replaced using specialized tools in a dust-free environment. If the disk platters are undamaged, they can be transferred into an identical enclosure and the data can be copied or cloned onto a new drive. In the event of disk-platter failures, disassembly and imaging of the disk platters may be required. For logical damage to file systems, a variety of tools, including fsck on UNIX-like systems and CHKDSK on Windows, can be used for data recovery. Recovery from logical damage can require file carving.
A common expectation is that hard disk drives designed for server use will fail less frequently than consumer-grade drives usually used in desktop computers. A study by Carnegie Mellon University and an independent one by Google both found that the "grade" of a drive does not relate to the drive's failure rate.
A 2011 summary of research into SSD and magnetic disk failure patterns by Tom's Hardware summarized research findings as follows:
Manufacturers and sales.
More than 200 companies have manufactured HDDs over time. But consolidations have concentrated production into just three manufacturers today: Western Digital, Seagate, and Toshiba.
Worldwide revenues for disk storage were $32 billion in 2013, down about 3% from 2012. This corresponds to shipments of 552 million units in 2013 compared to 578 million in 2012 and 622 million in 2011. The estimated 2013 market shares are about 40–45% each for Seagate and Western Digital and 13–16% for Toshiba.
External hard disk drives.
External hard disk drives typically connect via USB; variants using USB 2.0 interface generally have slower data transfer rates when compared to internally mounted hard drives connected through SATA. Plug and play drive functionality offers system compatibility and features large storage options and portable design. s of 2015[ [update]], available capacities for external hard disk drives range from 500 GB to 6 TB.
External hard disk drives are usually available as pre-assembled integrated products, but may be also assembled by combining an external enclosure (with USB or other interface) with a separately purchased drive. They are available in 2.5-inch and 3.5-inch sizes; 2.5-inch variants are typically called "portable external drives", while 3.5-inch variants are referred to as "desktop external drives". "Portable" drives are packaged in smaller and lighter enclosures than the "desktop" drives; additionally, "portable" drives use power provided by the USB connection, while "desktop" drives require external power bricks.
Features such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost. There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.
Visual representation.
Hard disk drives are traditionally symbolized as a stylized stack of platters or as a cylinder, and are as such found in various diagrams; sometimes, they are depicted with small lights to indicate data access. In most modern graphical user environments (GUIs), hard disk drives are represented by an illustration or photograph of the drive enclosure.

</doc>
<doc id="13782" url="http://en.wikipedia.org/wiki?curid=13782" title="Hebrew calendar">
Hebrew calendar

The Hebrew or Jewish calendar (הַלּוּחַ הָעִבְרִי, "ha'luach ha'ivri") is a lunisolar calendar used today predominantly for Jewish religious observances. It determines the dates for Jewish holidays and the appropriate public reading of Torah portions, "yahrzeits" (dates to commemorate the death of a relative), and daily Psalm readings, among many ceremonial uses. In Israel, it is used for religious purposes, provides a time frame for agriculture and is an official calendar for civil purposes, although the latter usage has been steadily declining in favor of the Gregorian calendar.
The present Hebrew calendar is the product of evolution, including a Babylonian influence. Until the Tannaitic period (approximately 10–220 CE) the calendar employed a new crescent moon, with an additional month normally added every two or three years to correct for the difference between twelve lunar months and the solar year. When to add it was based on observation of natural agriculture-related events. Through the Amoraic period (200–500 CE) and into the Geonic period, this system was gradually displaced by the mathematical rules used today. The principles and rules were fully codified by Maimonides in the "Mishneh Torah" in the 12th century. Maimonides' work also replaced counting "years since the destruction of the Temple" with the modern creation-era "Anno Mundi."
The Hebrew lunar year is about eleven days shorter than the solar cycle and uses the 19-year Metonic cycle to bring it into line with the solar cycle, with the addition of an intercalary month every two or three years, for a total of seven times per 19 years. Even with this intercalation, the average Hebrew calendar year is longer by about 6 minutes and 40 seconds than the current mean solar year, so that every 217 years the Hebrew calendar will fall a day behind the current mean solar year; and about every 231 years it will fall a day behind the Gregorian calendar year.
The era used since the middle ages is the "Anno Mundi" epoch (Latin for "in the year of the world"; Hebrew: לבריאת העולם, "from the creation of the world"). As with "Anno Domini" ("A.D." or "AD"), the words or abbreviation for "Anno Mundi" ("A.M." or "AM") for the era should properly "precede" the date rather than follow it, although this is no longer always followed.
AM 5774 began at sunset on 4 September 2013 and ended on 24 September 2014. AM 5775 began at sunset on 24 September 2014 and ends at sunset on 13 September 2015. AM 5776 begins at sunset on 13 September 2015 and ends at sunset on 2 October 2016.
Components.
Day and hours.
The Jewish day is of no fixed length. The Jewish day is modeled on the reference to "...there was evening and there was morning..." in the Creation account in the first chapter of Genesis. Based on the classic Rabbinic interpretation of this text, a day in the Rabbinic Hebrew calendar runs from sunset (start of "the evening") to the next sunset. The time between sunset and the time when three stars are visible (known as 'tzait ha'kochavim') is known as 'bein hashmashot' and for some uses it is debated as what day it is. One complicating factor is that there is no clear cut sunrise or sunset time at the extreme latitudes during certain seasons. At higher latitudes in summer, when the sun does not sink below the horizon, a day is counted from midday to midday, and in the winter, when the sun does not rise above the horizon, from midnight to midnight.
There is no clock in the Jewish scheme, so that a civil clock is used. Though the civil clock, including the one in use in Israel, incorporates local adoptions of various conventions such as time zones, standard times and daylight saving, these have no place in the Jewish scheme. The civil clock is used only as a reference point – in expressions such as: "Shabbat starts at ...". The steady progression of sunset around the world and seasonal changes results in gradual civil time changes from one day to the next based on observable astronomical phenomena (the sunset) and not on man-made laws and conventions.
In Judaism, an hour is defined as 1/12 of the time from sunrise to sunset, so during the winter, an hour can be much less than 60 minutes, and during the summer, it can be much more than 60 minutes. A Judaic hour is known as a 'sha'ah z'manit' which means a timely hour.
Instead of the international date line convention, there are varying opinions as to where the day changes. One opinion uses the antimeridian of Jerusalem. (Jerusalem is 35°13’ east of the prime meridian, so the antimeridian is at 144°47' W, passing through eastern Alaska.) Other opinions exist as well.
Every hour is divided into 1080 "halakim" (singular: "helek") or parts. A part is 3⅓ seconds or 1/18 minute. The ultimate ancestor of the helek was a small Babylonian time period called a "barleycorn", itself equal to 1/72 of a Babylonian "time degree" (1° of celestial rotation). Actually, the barleycorn or "she" was the name applied to the smallest units of all Babylonian measurements, whether of length, area, volume, weight, angle, or time.
The weekdays start with Sunday (day 1, or "Yom Rishon") and proceed to Saturday (day 7), Shabbat. Since some calculations use division, a remainder of 0 signifies Saturday.
While calculations of days, months and years are based on fixed hours equal to 1/24 of a day, the beginning of each "halachic" day is based on the local time of sunset. The end of the Shabbat and other Jewish holidays is based on nightfall ("Tzeth haKochabim") which occurs some amount of time, typically 42 to 72 minutes, after sunset. According to Maimonides, nightfall occurs when three medium-sized stars become visible after sunset. By the 17th century this had become three second-magnitude stars. The modern definition is when the center of the sun is 7° below the geometric (airless) horizon, somewhat later than civil twilight at 6°. The beginning of the daytime portion of each day is determined both by dawn and sunrise. Most "halachic" times are based on some combination of these four times and vary from day to day throughout the year and also vary significantly depending on location. The daytime hours are often divided into "Sha`oth Zemaniyoth" or "Halachic hours" by taking the time between sunrise and sunset or between dawn and nightfall and dividing it into 12 equal hours. The nighttime hours are similarly divided into 12 equal portions, albeit a different amount of time than the "hours" of the daytime. The earliest and latest times for Jewish services, the latest time to eat Chametz on the day before Passover and many other rules are based on "Sha`oth Zemaniyoth". For convenience, the modern day using "Sha`oth Zemaniyoth" is often discussed as if sunset were at 6:00pm, sunrise at 6:00am and each hour were equal to a fixed hour. For example, "halachic" noon may be after 1:00pm in some areas during daylight saving time. Within the Mishnah, however, the numbering of the hours starts with the "first" hour after the start of the day.
Weeks.
Shevua [שבוע] is a weekly cycle of seven days, mirroring the seven-day period of the Book of Genesis in which the world is created. The names for the days of the week, like those in the Creation account, are simply the day number within the week, with Shabbat being the seventh day. Each day of the week runs from sunset to the following sunset and is figured locally.
Names of weekdays.
The Hebrew calendar follows a seven-day weekly cycle, which runs concurrently but independently of the monthly and annual cycles. The names for the days of the week are simply the day number within the week. In Hebrew, these names may be abbreviated using the numerical value of the Hebrew letters, for example יום א׳ ("Day 1", or Yom Rishon (יום ראשון)):
The names of the days of the week are modeled on the seven days mentioned in the Creation story. For example, "... And there was evening and there was morning, one day". "One day" (יוֹם אֶחָד) in Genesis 1:15 is translated in JPS as "first day", and in some other contexts (including KJV) as "day one". In subsequent verses the Hebrew refers to the days using ordinal numbers, e.g., 'second day', 'third day', and so forth, but with the sixth and seventh days the Hebrew includes the definite article ("the").
The Jewish Shabbat has a special role in the Jewish weekly cycle. There are many special rules which relate to the Shabbat, discussed more fully in the Talmudic tractate Shabbat.
In Hebrew, the word "Shabbat" (שַׁבָּת) can also mean "(Talmudic) week", so that in ritual liturgy a phrase like "Yom Reviʻi bəShabbat" means "the fourth day in the week".
Days of week of holidays.
The period from 1 Adar (or Adar II, in leap years) to 29 Marcheshvan contains all of the festivals specified in the Bible – Purim (14 Adar), Pesach (15 Nisan), Shavuot (6 Sivan), Rosh Hashanah (1 Tishrei), Yom Kippur (10 Tishrei), Sukkot (15 Tishrei), and Shemini Atzeret (22 Tishrei). This period is fixed, during which no adjustments are made.
There are additional rules in the Hebrew calendar to prevent certain holidays from falling on certain days of the week. (See Rosh Hashanah postponement, below.) These rules are implemented by adding an extra day to Marcheshvan (making it 30 days long) or by removing one day from Kislev (making it 29 days long). Accordingly, a common Hebrew calendar year can have a length of 353, 354 or 355 days, while a leap Hebrew calendar year can have a length of 383, 384 or 385 days.
Months.
The Hebrew calendar is a lunisolar calendar, meaning that months are based on lunar months, but years are based on solar years. The calendar year features twelve lunar months of twenty-nine or thirty days, with an intercalary lunar month added periodically to synchronize the twelve lunar cycles with the longer solar year. (These extra months are added seven times every nineteen years. See Leap months, below.) The beginning of each Jewish lunar month is based on the appearance of the new moon. Although originally the new lunar crescent had to be observed and certified by witnesses, the moment of the new moon is now approximated arithmetically.
The mean period of the lunar month (precisely, the synodic month) is very close to 29.5 days. Accordingly, the basic Hebrew calendar year is one of twelve lunar months alternating between 29 and 30 days:
In leap years (such as 5774) an additional month, Adar I (30 days) is added after Shevat, while the regular Adar is referred to as "Adar II."
The insertion of the leap month mentioned above is based on the requirement that Passover—the festival celebrating the Exodus from Egypt, which took place in the spring—always occurs in the [northern hemisphere's] spring season. Since the adoption of a fixed calendar, intercalations in the Hebrew calendar have been assigned to fixed points in a 19-year cycle. Prior to this, the intercalation was determined empirically:
The year may be intercalated on three grounds: 'aviv [i.e.the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone.
Importance of lunar months.
From very early times, the Mesopotamian lunisolar calendar was in wide use by the countries of the western Asia region. The structure, which was also used by the Israelites, was based on lunar months with the intercalation of an additional month to bring the cycle closer to the solar cycle.
 stresses the importance in Israelite religious observance of the new month (Hebrew: ראש חודש, Rosh Chodesh, "beginning of the month"): "... in your new moons, ye shall blow with the trumpets over your burnt-offerings..." Similarly in . "The beginning of the month" meant the appearance of a new moon, and in . "This month is to you"
According to the "Mishnah" and Tosefta, in the Maccabean, Herodian, and Mishnaic periods, new months were determined by the sighting of a new crescent, with two eyewitnesses required to testify to the Sanhedrin to having seen the new lunar crescent at sunset. The practice in the time of Gamaliel II (c. 100 CE) was for witnesses to select the appearance of the moon from a collection of drawings that depicted the crescent in a variety of orientations, only a few of which could be valid in any given month. These observations were compared against calculations.
At first the beginning of each Jewish month was signaled to the communities of Israel and beyond by fires lit on mountaintops, but after the Samaritans began to light false fires, messengers were sent. The inability of the messengers to reach communities outside Israel before mid-month High Holy Days (Succot and Passover) led outlying communities to celebrate scriptural festivals for two days rather than one, observing the second feast-day of the Jewish diaspora because of uncertainty of whether the previous month ended after 29 or 30 days.
In his work "Mishneh Torah" (1178), Maimonides included a chapter "Sanctification of the New Moon", in which he discusses the calendrical rules and their scriptural basis. He notes, "By how much does the solar year exceed the lunar year? By approximately 11 days. Therefore, whenever this excess accumulates to about 30 days, or a little more or less, one month is added and the particular year is made to consist of 13 months, and this is the so-called embolismic (intercalated) year. For the year could not consist of twelve months plus so-and-so many days, since it is said: throughout the months of the year (), which implies that we should count the year by months and not by days."
Names of months.
Both the Syrian calendar, currently used in the Arabic-speaking countries of the Fertile crescent, and the modern Assyrian calendar share many of the names for months with the Hebrew calendar, such as Nisan, Iyyar, Tammuz, Ab, Elul, Tishri and Adar, indicating a common origin. The origin is thought to be the Babylonian calendar. The modern Turkish calendar includes the names Subat (February), Nisan (April), Temmuz (July) and Eylul (September). The former name for October was Tesrin.
Biblical references to the pre-Jewish calendar include ten months identified by number rather than by name. In parts of the Torah portion "Noach" ("Noah") (specifically, , , ) it is implied that the months are thirty days long. There is also an indication that there were twelve months in the annual cycle (, ). Prior to the Babylonian exile, the names of only four months are referred to in the Tanakh:
All of these are believed to be Canaanite names. These names are only mentioned in connection with the building of the First Temple. Håkan Ulfgard suggests that the use of what are rarely used Canaanite (or in the case of Ethanim perhaps Northwest-semitic) names indicates that "the author is consciously utilizing an archaizing terminology, thus giving the impression of an ancient story...".
In a regular ("kesidran") year, Marcheshvan has 29 days and Kislev has 30 days. However, because of the Rosh Hashanah postponement rules (see below) Kislev may lose a day to have 29 days, and the year is called a short ("chaser") year, or Marcheshvan may acquire an additional day to have 30 days, and the year is called a full ("maleh") year. The calendar rules have been designed to ensure that Rosh Hashanah does not fall on a Sunday, Wednesday or Friday. This is to ensure that Yom Kippur does not directly precede or follow Shabbat, which would create practical difficulties, and that Hoshana Rabbah is not on a Shabbat, in which case certain ceremonies would be lost for a year. Hebrew names and romanized transliteration may somewhat differ, as they do for Marcheshvan (חשוון) or Kislev (כסלו): the Hebrew words shown here are those commonly indicated "e.g." in newspapers.
Leap months.
The solar year is about eleven days longer than twelve lunar months. The Bible does not directly mention the addition of "embolismic" or intercalary months. However, without the insertion of embolismic months, Jewish festivals would gradually shift outside of the seasons required by the Torah. This has been ruled as implying a requirement for the insertion of embolismic months to reconcile the lunar cycles to the seasons, which are integral to solar yearly cycles.
When the observational form of the calendar was in use, whether or not an embolismic month was announced after the "last month" (Adar) depended on 'aviv [i.e. the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone. It may be noted that in the Bible the name of the first month, "Aviv", literally means "spring". Thus, if Adar was over and Spring had not yet arrived, an additional month was observed.
Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle, which can be used to predict eclipses, forms the basis of the Greek and Hebrew calendars, and is used for the computation of the date of Easter each year
During leap years Adar I (or Adar Aleph — "first Adar") is added before the regular Adar. Adar I is actually considered to be the extra month, and has 30 days. Adar II (or Adar Bet — "second Adar") is the "real" Adar, and has the usual 29 days. For this reason, holidays such as Purim are observed in Adar II, not Adar I.
Hebrew astronomy.
Chronology was a chief consideration in the study of astronomy among the Jews; sacred time was based upon the cycles of the Sun and the Moon. The Talmud identified the twelve constellations of the zodiac with the twelve months of the Hebrew calendar. The correspondence of the constellations with their names in Hebrew and the months is as follows:
Jewish astrology.
The Talmud identified the 12 constellations of the zodiac with the 12 months of the Hebrew calendar. The correspondence of the constellations with their names in Hebrew and the months is as follows:
Some scholars identified the 12 signs of the zodiac with the 12 sons of Jacob/twelve tribes of Israel.
It should be noted that the 12 lunar months of the Hebrew Calendar are just that: based on the 12 lunar months of 29.53 days each and the lunar year of 354 days. The calendar originally alternated between 29 day months and 30 day months.
Years.
The Hebrew calendar year conventionally begins on Rosh Hashanah. However, other dates serve as the beginning of the year for different religious purposes.
There are three qualities that distinguish one year from another: whether it is a leap year or a common year, on which of four permissible days of the week the year begins, and whether it is a deficient, regular, or complete year. Mathematically, there are 24 (2×4×3) possible combinations, but only 14 of them are valid. Each of these patterns is called a "keviyah" (Hebrew קביעה for "a setting" or "an established thing"), and is encoded as a series of three Hebrew letters.
In Hebrew there are two common ways of writing the year number: with the thousands, called לפרט גדול ("major era"), and without the thousands, called לפרט קטן ("minor era").
Anno Mundi.
In 1178 CE, Maimonides wrote in the "Mishneh Torah", "Sanctification of the Moon" (11.16), that he had chosen the epoch from which calculations of all dates should be as "the third day of Nisan in this present year ... which is the year 4938 of the creation of the world" (March 22, 1178 CE). He included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, and beginning formal usage of the "anno mundi" era. From the 11th century, "anno mundi" dating became dominant throughout most of the world's Jewish communities. Today, the rules detailed in Maimonides' calendrical code are those generally used by Jewish communities throughout the world.
Since the codification by Maimonides in 1178 CE, the Jewish calendar has used the Anno Mundi epoch (Latin for “in the year of the world,” abbreviated "AM" or "A.M.;" Hebrew לבריאת העולם), sometimes referred to as the “Hebrew era”, to distinguish it from other systems based on some computation of creation, such as the Byzantine calendar.
There is also reference in the Talmud to years since the creation based on the calculation in the "Seder Olam Rabbah" of Rabbi Jose ben Halafta in about 160 CE. By his calculation, based on the Masoretic Text, Adam was created in 3760 BCE, later confirmed by the Muslim chronologist al-Biruni as 3448 years before the Seleucid era. An example is the c. 8th century Baraita of Samuel.
According to Rabbinic reckoning, the beginning of "year 1" is "not" Creation, but about one year before Creation, with the new moon of its first month (Tishrei) to be called "molad tohu" (the mean new moon of chaos or nothing). The Jewish calendar's epoch (reference date), 1 Tishrei AM 1, is equivalent to Monday, 7 October 3761 BC/BCE in the proleptic Julian calendar, the equivalent tabular date (same daylight period) and is about one year "before" the traditional Jewish date of Creation on 25 Elul AM 1, based upon the "Seder Olam Rabbah". Thus, adding 3760 before Rosh Hashanah or 3761 after to a Julian year number starting from 1 CE (AD 1) will yield the Hebrew year. For earlier years there may be a discrepancy (see: Missing years (Jewish calendar)).
The "Seder Olam Rabbah" also recognized the importance of the Jubilee and Sabbatical cycles as a long-term calendrical system, and attempted at various places to fit the Sabbatical and Jubilee years into its chronological scheme.
Previous systems.
Before the adoption of the current AM year numbering system, other systems were in use. In early times, the years were counted from some significant historic event. (e.g. ) During the period of the monarchy, it was the widespread practice in western Asia to use era year numbers according to the accession year of the monarch of the country involved. This practice was also followed by the united kingdom of Israel (e.g. ), kingdom of Judah (e.g. ), kingdom of Israel (e.g. ), Persia (e.g. ) and others. Besides, the author of Kings coordinated dates in the two kingdoms by giving the accession year of a monarch in terms of the year of the monarch of the other kingdom, (e.g. ) though some commentators note that these dates do not always synchronise. Other era dating systems have been used at other times. For example, Jewish communities in the Babylonian diaspora counted the years from the first deportation from Israel, that of Jehoiachin in 597 BCE, (e.g. ). The era year was then called "year of the captivity of Jehoiachin". (e.g. )
During the Hellenistic Maccabean period, Seleucid era counting was used, at least in the Greek-influenced area of Israel. The Books of the Maccabees used Seleucid era dating exclusively (e.g. , , , , ). Josephus writing in the Roman period also used Seleucid era dating exclusively. During the Talmudic era, from the 1st to the 10th century, the center of world Judaism was in the Middle East, primarily in the Talmudic Academies of Iraq and Palestine. Jews in these regions used Seleucid era dating (also known as the "Era of Contracts"). The Avodah Zarah states:
Rav Aha b. Jacob then put this question: How do we know that our Era [of Documents] is connected with the Kingdom of Greece at all? Why not say that it is reckoned from the Exodus from Egypt, omitting the first thousand years and giving the years of the next thousand? In that case, the document is really post-dated!<br> Said Rav Nahman: In the Diaspora the Greek Era alone is used. He [the questioner] thought that Rav Nahman wanted to dispose of him anyhow, but when he went and studied it thoroughly he found that it is indeed taught [in a Baraita]: In the Diaspora the Greek Era alone is used.
The use of the era of documents (i.e., Seleucid era) continued till the 16th century in the East, and was employed even in the 19th century among the Jews of Yemen.
Occasionally in Talmudic writings, reference was made to other starting points for eras, such as destruction era dating, being the number of years since the 70 CE destruction of the Second Temple. In the 8th and 9th centuries, as the center of Jewish life moved from Babylonia to Europe, counting using the Seleucid era "became meaningless". There is indication that Jews of the Rhineland in the early Middle Ages used the "years after the destruction of the Temple" (e.g., ).
New year.
 and set Aviv (now Nisan) as "the first of months":
Nisan 1 is referred to as the "ecclesiastical new year".
In ancient Israel, the start of the ecclesiastical new year for the counting of months and festivals (i.e. Nisan) was determined by reference to Passover. Passover is on 15 Nisan, () which corresponds to the full moon of Nisan. As Passover is a spring festival, it should fall on a full moon day around, and normally just after, the vernal (northward) equinox. If the twelfth full moon after the previous Passover is too early compared to the equinox, a leap month is inserted near the end of the previous year before the new year is set to begin. According to normative Judaism, the verses in require that the months be determined by a proper court with the necessary authority to sanctify the months. Hence the court, not the astronomy, has the final decision.
According to some Christian and Karaite sources, the tradition in ancient Israel was that 1 Nisan would not start until the barley is ripe, being the test for the onset of spring. If the barley was not ripe an intercalary month would be added before Nisan.
The day most commonly referred to as the "New Year" is 1 Tishrei, which actually begins in the seventh month of the ecclesiastical year. On that day the formal New Year for the counting of years (such as Shmita and Yovel), Rosh Hashanah ("head of the year") is observed. (see , which uses the phrase "beginning of the year".) This is the civil new year, and the date on which the year number advances. Certain agricultural practices are also marked from this date.
In the 1st century, Josephus stated that while –
Moses...appointed Nisan...as the first month for the festivals...the commencement of the year for everything relating to divine worship, but for selling and buying and other ordinary affairs he preserved the ancient order [i. e. the year beginning with Tishrei]."
Edwin Thiele has concluded that the ancient northern Kingdom of Israel counted years using the ecclesiastical new year starting on 1 Aviv (Nisan), while the southern Kingdom of Judah counted years using the civil new year starting on 1 Tishrei. The practice of the Kingdom of Israel was also that of Babylon, as well as other countries of the region. The practice of Judah is still followed.
In fact the Jewish calendar has a multiplicity of new years for different purposes. The use of these dates has been in use for a long time. The use of multiple starting dates for a year is comparable to different starting dates for civil "calendar years", "tax or fiscal years", "academic years", "religious cycles", etc. By the time of the redaction of the "Mishnah", (c. 200 CE), jurists had identified four new-year dates:
The 1st of Nisan is the new year for kings and feasts; the 1st of Elul is the new year for the tithe of cattle... the 1st of Tishri is the new year for years, of the years of release and jubilee years, for the planting and for vegetables; and the 1st of Shevat is the new year for trees-so the school of Shammai; and the school of Hillel say: On the 15th thereof.
The month of Elul is the new year for counting animal tithes ("ma'aser behemah"). "Tu Bishvat" ("the 15th of Shevat") marks the new year for trees (and agricultural tithes).
For the dates of the Jewish New Year see Jewish and Israeli holidays 2000-2050 or calculate using the section "Conversion between Jewish and civil calendars".
Leap years.
The Jewish calendar is based on the Metonic cycle of 19 years, of which 12 are common (non-leap) years of 12 months and 7 are leap years of 13 months. To determine whether a Jewish year is a leap year, one must find its position in the 19-year Metonic cycle. This position is calculated by dividing the Jewish year number by 19 and finding the remainder. For example, the present Jewish year divided by 19 results in a remainder of , indicating that it is year of the Metonic cycle. Since there is no year 0, a remainder of 0 indicates that the year is year 19 of the cycle.
Years 3, 6, 8, 11, 14, 17, and 19 of the Metonic cycle are leap years. To assist in remembering this sequence, some people use the mnemonic Hebrew word GUCHADZaT "גוחאדז"ט", where the Hebrew letters "gimel-vav-het aleph-dalet-zayin-tet" are used as Hebrew numerals equivalent to 3, 6, 8, 1, 4, 7, 9. The "keviyah" records whether the year is leap or common: פ for "p'shutah", meaning simple and indicating a common year, and מ indicating a leap year.
Another memory aid notes that intervals of the major scale follow the same pattern as do Jewish leap years, with "do" corresponding to year 19 (or 0): a whole step in the scale corresponds to two common years between consecutive leap years, and a half step to one common year between two leap years. This connection with the major scale is more plain in the context of 19 equal temperament: counting the tonic as 0, the notes of the major scale in 19 equal temperament are numbers 0 (or 19), 3, 6, 8, 11, 14, 17, the same numbers as the leap years in the Hebrew calendar. 
To determine whether year "n" of the calendar is a leap year, find the remainder on dividing [(7 × "n") + 1] by 19. If the remainder is 6 or less it is a leap year; if it is 7 or more it is not. For example, the remainder on dividing [(7 × ) + 1] by 19 is , so the year is a leap year. The remainder on dividing [(7 × 1) + 1] by 19 is 8, so the year 1 is a leap year.
Rosh Hashanah postponement.
To calculate the day on which Rosh Hashanah of a given year will fall, it is necessary first to calculate the expected molad (moment of lunar conjunction or new moon) of Tishrei in that year, and then to apply a set of rules to determine whether the first day of the year must be postponed. The molad can be calculated by multiplying the number of months that will have elapsed since some (preceding) molad whose weekday is known by the mean length of a (synodic) lunar month, which is 29 days, 12 hours, and 793 parts (there are 1080 "parts" in an hour, so that one part is equal to 31/3 seconds). The very first molad, the molad tohu, fell on Sunday evening at 11.11 formula_1, or in Jewish terms Day 2, 5 hours, and 204 parts.
In calculating the number of months that will have passed since the known molad that one uses as the starting point, one must remember to include any leap month(s) that falls within the elapsed interval, according to the cycle of leap years.
The two months whose numbers of days may be adjusted, Marcheshvan and Kislev, are the eighth and ninth months of the Hebrew year, whereas Tishrei is the seventh month (in the traditional counting of the months, even though it is the first month of a new calendar year). Any adjustments needed to postpone Rosh Hashanah must be made to the adjustable months in the year that precedes the year of which the Rosh Hashanah will be the first day.
Just four potential conditions are considered to determine whether the date of Rosh Hashanah must be postponed. These are called the Rosh Hashanah postponement rules, or "deḥiyyot":
The first of these rules (deḥiyyah "molad zaken") is referred to in the Talmud. Nowadays, molad zaken is used as a device to prevent the molad falling on the second day of the month. The second rule, (deḥiyyah "lo ADU"), is applied for religious reasons.
Another two rules are applied much less frequently and serve to prevent impermissible year lengths. Their names are Hebrew acronyms that refer to the ways they are calculated:
At the innovation of the sages, the calendar was arranged to ensure that Yom Kippur would not fall on a Friday or Sunday, and Hoshana Rabbah would not fall on Shabbat. These rules have been instituted because Shabbat restrictions also apply to Yom Kippur, so that if Yom Kippur were to fall on Friday, it would not be possible to make necessary preparations for Shabbat (such as candle lighting). Similarly, if Yom Kippur fell on a Sunday, it would not be possible to make preparations for Yom Kippur because the preceding day is Shabbat. Additionally, the laws of Shabbat override those of Hoshana Rabbah, so that if Hoshana Rabbah were to fall on Shabbat certain rituals that are a part of the Hoshana Rabbah service (such as carrying willows, which is a form of work) could not be performed.
To prevent Yom Kippur (10 Tishrei) from falling on a Friday or Sunday, Rosh Hashanah (1 Tishrei) cannot fall on Wednesday or Friday. Likewise, to prevent Hoshana Rabbah (21 Tishrei) from falling on a Saturday, Rosh Hashanah cannot fall a Sunday. This leaves only four days on which Rosh Hashanah can fall: Monday, Tuesday, Thursday, and Saturday, which are referred as the "four gates". Each day is associated with a number (its order in the week, beginning with Sunday as day 1). Numbers in Hebrew have been traditionally denominated by Hebrew letters. Thus the "keviyah" uses the letters ה ,ג ,ב and ז (representing 2, 3, 5, and 7, for Monday, Tuesday, Thursday, and Saturday) to denote the starting day of the year.
Deficient, regular, and complete years.
The postponement of the year is compensated for by adding a day to the second month or removing one from the third month. A Jewish common year can only have 353, 354, or 355 days. A leap year is always 30 days longer, and so can have 383, 384, or 385 days.
Whether a year is deficient, regular, or complete is determined by the time between two adjacent Rosh Hashanah observances and the leap year. While the "keviyah" is sufficient to describe a year, a variant specifies the day of the week for the first day of Pesach (Passover) in lieu of the year length.
A Metonic cycle equates to 235 lunar months in each 19-year cycle. This gives an average of 6939 days, 16 hours, and 595 parts for each cycle. But due to the Rosh Hashanah postponement rules (preceding section) a cycle of 19 Jewish years can be either 6939, 6940, 6941, or 6942 days in duration. Since none of these values is evenly divisible by seven, the Jewish calendar repeats exactly only following 36,288 Metonic cycles, or 689,472 Jewish years. There is a near-repetition every 247 years, except for an excess of about 50 minutes (905 parts).
History.
Mishnaic period.
The Tanakh contains several commandments related to the keeping of the calendar and the lunar cycle, and records changes that have taken place to the Hebrew calendar.
It has been noted that the procedures described in the Mishnah and Tosefta are all plausible procedures for regulating an empirical lunar calendar. Fire-signals, for example, or smoke-signals, are known from the pre-exilic Lachish ostraca. Furthermore, the Mishnah contains laws that reflect the uncertainties of an empirical calendar. Mishnah Sanhedrin, for example, holds that when one witness holds that an event took place on a certain day of the month, and another that the same event took place on the following day, their testimony can be held to agree, since the length of the preceding month was uncertain. Another Mishnah takes it for granted that it cannot be known in advance whether a year's lease is for twelve or thirteen months. Hence it is a reasonable conclusion that the Mishnaic calendar was actually used in the Mishnaic period.
The accuracy of the Mishnah's claim that the Mishnaic calendar was also used in the late Second Temple period is less certain. One scholar has noted that there are no laws from Second Temple period sources that indicate any doubts about the length of a month or of a year. This led him to propose that the priests must have had some form of computed calendar or calendrical rules that allowed them to know in advance whether a month would have 30 or 29 days, and whether a year would have 12 or 13 months.
Modern calendar.
Between 70 and 1178 CE, the observation-based calendar was gradually replaced by a mathematically calculated one. Except for the epoch year number, the calendar rules reached their current form by the beginning of the 9th century, as described by the Persian Muslim astronomer al-Khwarizmi (c. 780–850 CE) in 823.
One notable difference between the calendar of that era and the modern form was the date of the epoch (the fixed reference point at the beginning of year 1), which at that time was one year later than the epoch of the modern calendar.
Most of the present rules of the calendar were in place by 823, according to a treatise by al-Khwarizmi. Al-Khwarizmi's study of the Jewish calendar, "Risāla fi istikhrāj taʾrīkh al-yahūd" "Extraction of the Jewish Era" describes the 19-year intercalation cycle, the rules for determining on what day of the week the first day of the month Tishrī shall fall, the interval between the Jewish era (creation of Adam) and the Seleucid era, and the rules for determining the mean longitude of the sun and the moon using the Jewish calendar. Not all the rules were in place by 835.
In 921, Aaron ben Meïr proposed changes to the calendar. Though the proposals were rejected, they indicate that all of the rules of the modern calendar (except for the epoch) were in place before that date. In 1000, the Muslim chronologist al-Biruni described all of the modern rules of the Hebrew calendar, except that he specified three different epochs used by various Jewish communities being one, two, or three years later than the modern epoch.
There is a tradition, first mentioned by Hai Gaon (died 1038 CE), that Hillel b. R. Yehuda "in the year 670 of the Seleucid era" (i.e., 358–359 CE) was responsible for the new calculated calendar with a fixed intercalation cycle. Later writers, such as Nachmanides, explained Hai Gaon's words to mean that the entire computed calendar was due to Hillel b. Yehuda in response to persecution of Jews. Maimonides, in the 12th century, stated that the Mishnaic calendar was used "until the days of Abaye and Rava", who flourished c. 320–350 CE, and that the change came when "the land of Israel was destroyed, and no permanent court was left." Taken together, these two traditions suggest that Hillel b. Yehuda (whom they identify with the mid-4th-century Jewish patriarch Ioulos, attested in a letter of the Emperor Julian, and the Jewish patriarch Ellel, mentioned by Epiphanius) instituted the computed Hebrew Calendar because of persecution. H. Graetz linked the introduction of the computed calendar to a sharp repression following a failed Jewish insurrection that occurred during the rule of the Christian emperor Constantius and Gallus. A later writer, S. Lieberman, argued instead that the introduction of the fixed calendar was due to measures taken by Christian Roman authorities to prevent the Jewish patriarch from sending calendrical messengers.
Both the tradition that Hillel b. Yehuda instituted the complete computed calendar, and the theory that the computed calendar was introduced due to repression or persecution, have been questioned. Furthermore, two Jewish dates during post-Talmudic times (specifically in 506 and 776) are impossible under the rules of the modern calendar, indicating that its arithmetic rules were developed in Babylonia during the times of the Geonim (7th to 8th centuries). The Babylonian rules required the delay of the first day of Tishrei when the new moon occurred after noon.
The Talmuds do, however, indicate at least the beginnings of a transition from a purely empirical to a computed calendar. According to a statement attributed to Yose, an Amora who lived during the second half of the 3rd century, the feast of Purim, 14 Adar, could not fall on a Sabbath nor a Monday, lest 10 Tishrei (Yom Kippur) fall on a Friday or a Sunday. This indicates that, by the time of the redaction of the Jerusalem Talmud (c. 400 CE), there were a fixed number of days in all months from Adar to Elul, also implying that the extra month was already a second Adar added before the regular Adar. In another passage, a sage is reported to have counseled "those who make the computations" not to set the first day of Tishrei or the Day of the Willow on the sabbath. This indicates that there was a group who "made computations" and were in a position to control, to some extent, the day of the week on which Rosh Hashanah would fall.
Usage in contemporary Israel.
Early Zionist pioneers were impressed by the fact that the calendar preserved by Jews over many centuries in far-flung diasporas, as a matter of religious ritual, was geared to the climate of their original country: the Jewish New Year marks the transition from the dry season to the rainy one, and major Jewish holidays such as Sukkot, Passover, and Shavuot correspond to major points of the country's agricultural year such as planting and harvest.
Accordingly, in the early 20th century the Hebrew calendar was re-interpreted as an agricultural rather than religious calendar. The Kibbutz movement was especially inventive in creating new rituals fitting this interpretation.
After the creation of the State of Israel, the Hebrew calendar became one of the official calendars of Israel, along with the Gregorian calendar. Holidays and commemorations not derived from previous Jewish tradition were to be fixed according to the Hebrew calendar date. For example, the Israeli Independence Day falls on 5 Iyar, Jerusalem Reunification Day on 28 Iyar, and the Holocaust Commemoration Day on 27 Nisan.
Nevertheless, since the 1950s usage of the Hebrew calendar has steadily declined, in favor of the Gregorian calendar. At present, Israelis—except for a minority of the religiously observant—conduct their private and public life according to the Gregorian calendar, although the Hebrew calendar is still widely acknowledged, appearing in public venues such as banks (where it is legal for use on cheques and other documents, though only rarely do people make use of this option) and on the mastheads of newspapers.
The Jewish New Year (Rosh Hashanah) is a two-day public holiday in Israel. However, since the 1980s an increasing number of secular Israelis celebrate the Gregorian New Year (usually known as "Silvester Night"—"ליל סילבסטר") on the night between 31 December and 1 January. Prominent rabbis have on several occasions sharply denounced this practice, but with no noticeable effect on the secularist celebrants.
The disparity between the two calendars is especially noticeable with regard to commemoration of the assassinated Prime Minister Yitzchak Rabin. The official Day of Commemoration, instituted by a special Knesset law, is marked according to the Hebrew calendar – on 12 Marcheshvan. However, left-leaning Israelis, who revere Rabin as a martyr for the cause of peace and who are predominantly secular, hold their commemoration on 4 November. In some years the two competing Rabin Memorial Days are separated by as much as two weeks.
Wall calendars commonly used in Israel are hybrids. Most are organised according to Gregorian rather than Jewish months, but begin in September, when the Jewish New Year usually falls, and provide the Jewish date in small characters.
Other practices.
Outside of Rabbinic Judaism, evidence shows a diversity of practice.
Karaite calendar.
Karaites use the lunar month and the solar year, but the Karaite calendar differs from the current Rabbinic calendar in a number of ways. The Karaite calendar is identical to the Rabbinical calendar used before the Sanhedrin changed the Rabbinic calendar from the lunar, observation based calendar, to the current mathematically based calendar used in Rabbinic Judaism today.
In the lunar Karaite calendar, the beginning of each month, the Rosh Chodesh, can be calculated, but is confirmed by the observation in Israel of the first sightings of the new moon. This may result in an occasional variation of a maximum of one day, depending on the inability to observe the new moon. The day is usually "picked up" in the next month.
The addition of the leap month (Adar II) is determined by observing in Israel the ripening of barley at a specific stage (defined by Karaite tradition) (called aviv), rather than using the calculated and fixed calendar of Rabbinic Judaism. Occasionally this results in Karaites being one month ahead of other Jews using the calculated Rabbinic calendar. The "lost" month would be "picked up" in the next cycle when Karaites would observe a leap month while other Jews would not.
Furthermore, the seasonal drift of the Rabbinic calendar is avoided, resulting in the years affected by the drift starting one month earlier in the Karaite calendar.
Also, the four rules of postponement of the Rabbinic calendar are not applied, since they are not mentioned in the Tanakh. This can affect the dates observed for all the Jewish holidays in a particular year by one day.
In the Middle Ages many Karaite Jews outside Israel followed the calculated Rabbinic calendar, because it was not possible to retrieve accurate aviv barley data from the land of Israel. However, since the establishment of the State of Israel, and especially since the Six Day War, the Karaite Jews that have made "aliyah" can now again use the observational calendar.
The Qumran calendar.
Many of the Dead Sea (Qumran) Scrolls have references to a unique calendar, used by the people there, who are often assumed to be Essenes.
The year of this calendar used the ideal Mesopotamian calendar of twelve 30-day months, to which were added 4 days at the equinoxes and solstices (cardinal points), making a total of 364 days.
There was some ambiguity as to whether the cardinal days were at the beginning of the months or at the end, but the clearest calendar attestations give a year of four seasons, each having three months of 30, 30, and 31 days with the cardinal day the extra day at the end, for a total of 91 days, or exactly 13 weeks. Each season started on the 4th day of the week (Wednesday), every year. (Ben-Dov, "Head of All Years", pp. 16–17)
With only 364 days, it is clear that the calendar would after a few years be very noticeably different from the actual seasons, but there is nothing to indicate what was done about this problem. Various suggestions have been made by scholars. One is that nothing was done and the calendar was allowed to change with respect to the seasons. Another suggestion is that changes were made irregularly, only when the seasonal anomaly was too great to be ignored any longer. (Ben-Dov, "Head of All Years", pp. 19–20)
The writings often discuss the moon, but the calendar was not based on the movement of the moon any more than indications of the phases of the moon on a modern western calendar indicate that that is a lunar calendar.
The calendrical documents 4Q320 and 4Q321 from the Dead Sea Scrolls outlining the 364-day solar calendar, six-year cycle of priestly courses, and 354-day lunar year cycles may be found . In addition, an abbreviated Jubilee calendar from 4Q319 along with the priestly course serving on 1 Abib (the first day of the year) each year may be found .
Persian civil calendar.
Calendrical evidence for the postexilic Persian period is found in papyri from the Jewish colony at Elephantine, in Egypt. These documents show that the Jewish community of Elephantine used the Egyptian and Babylonian calendars.
The Sardica paschal table shows that the Jewish community of some eastern city, possibly Antioch, used a calendrical scheme that kept Nisan 14 within the limits of the Julian month of March. Some of the dates in the document are clearly corrupt, but they can be emended to make the sixteen years in the table consistent with a regular intercalation scheme. Peter, the bishop of Alexandria (early 4th century CE), mentions that the Jews of his city "hold their Passover according to the course of the moon in the month of Phamenoth, or according to the intercalary month every third year in the month of Pharmuthi", suggesting a fairly consistent intercalation scheme that kept Nisan 14 approximately between Phamenoth 10 (March 6 in the 4th century CE) and Pharmuthi 10 (April 5). Jewish funerary inscriptions from Zoar, south of the Dead Sea, dated from the 3rd to the 5th century, indicate that when years were intercalated, the intercalary month was at least sometimes a repeated month of Adar. The inscriptions, however, reveal no clear pattern of regular intercalations, nor do they indicate any consistent rule for determining the start of the lunar month.
In 1178, Maimonides included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, "Mishneh Torah". Today, the rules detailed in Maimonides' code are those generally used by Jewish communities throughout the world.
Astronomical calculations.
Synodic month – the molad interval.
A "new moon" (astronomically called a lunar conjunction and in Hebrew called a molad) is the moment at which the sun and moon are aligned horizontally with respect to a north-south line (technically, they have the same ecliptical longitude). The period between two new moons is a synodic month. The actual length of a synodic month varies from about 29 days 6 hours and 30 minutes (29.27 days) to about 29 days and 20 hours (29.83 days), a variation range of about 13 hours and 30 minutes. Accordingly, for convenience, a long-term average length called the mean synodic month (also called the molad interval) is used. The mean synodic month is formula_2 days, or 29 days, 12 hours, and 793 parts (44+1/18 minutes) (i.e. 29.530594 days), and is the same value determined by the Babylonians in the System B in about 300 BCE and was adopted by the Greek astronomer Hipparchus in the 2nd century BCE and by the Alexandrian astronomer Ptolemy in the "Almagest" four centuries later (who cited Hipparchus as his source). Its remarkable accuracy (less than one second from the true value) is thought to have been achieved using records of lunar eclipses from the 8th to 5th centuries BCE.
This value is as close to the correct value of 29.530589 days as it is possible for a value to come that is rounded off to whole parts (1/18 minute). The discrepancy makes the molad interval about 0.6 seconds too long. Put another way, if the molad is taken as the time of mean conjunction at some reference meridian, then this reference meridian is drifting slowly eastward. If this drift of the reference meridian is traced back to the mid-4th century, the traditional date of the introduction of the fixed calendar, then it is found to correspond to a longitude midway between the Nile and the end of the Euphrates. The modern molad moments match the mean solar times of the lunar conjunction moments near the meridian of Kandahar, Afghanistan, more than 30° east of Jerusalem.
Furthermore, the discrepancy between the molad interval and the mean synodic month is accumulating at an accelerating rate, since the mean synodic month is progressively shortening due to gravitational tidal effects. Measured on a strictly uniform time scale, such as that provided by an atomic clock, the mean synodic month is becoming gradually longer, but since the tides slow Earth's rotation rate even more, the mean synodic month is becoming gradually shorter in terms of mean solar time.
Seasonal drift.
The mean year of the current mathematically based Hebrew calendar is 365 days 5 hours 55 minutes and 25+25/57 seconds (365.2468 days) – computed as the molad/monthly interval of 29.530594 days × 235 months in a 19-year metonic cycle ÷ 19 years per cycle. In relation to the Gregorian calendar, the mean Gregorian calendar year is 365 days 5 hours 49 minutes and 12 seconds (365.2425 days), and the drift of the Hebrew calendar in relation to it is about a day every 231 years.
Implications for Jewish ritual.
Although the molad of Tishrei is the only molad moment that is not ritually announced, it is actually the only one that is relevant to the Hebrew calendar, for it determines the provisional date of Rosh Hashanah, subject to the Rosh Hashanah postponement rules. The other monthly molad moments are announced for mystical reasons. With the moladot on average almost 100 minutes late, this means that the molad of Tishrei lands one day later than it ought to in (100 minutes) ÷ (1440 minutes per day) = 5 of 72 years or nearly 7% of years.
Therefore, the seemingly small drift of the moladot is already significant enough to affect the date of Rosh Hashanah, which then cascades to many other dates in the calendar year and sometimes, due to the Rosh Hashanah postponement rules, also interacts with the dates of the prior or next year. The molad drift could be corrected by using a progressively shorter molad interval that corresponds to the actual mean lunar conjunction interval at the original molad reference meridian. Furthermore, the molad interval determines the calendar mean year, so using a progressively shorter molad interval would help correct the excessive length of the Hebrew calendar mean year, as well as helping it to "hold onto" the northward equinox for the maximum duration.
When the 19-year intercalary cycle was finalised in the 4th century, the earliest Passover (in year 16 of the cycle) coincided with the northward equinox, which means that Passover fell near the "first" full moon after the northward equinox, or that the northward equinox landed within one lunation before 16 days after the "molad" of "Nisan". This is still the case in about 80% of years, but in about 20% of years Passover is a month late by these criteria (as it was in AM 5765 and 5768, the 8th and 11th years of the 19-year cycle = Gregorian 2005 and 2008 CE). Presently this occurs after the "premature" insertion of a leap month in years 8, 11, and 19 of each 19-year cycle, which causes the northward equinox to land on exceptionally early Hebrew dates in such years. This problem will get worse over time, and so beginning in AM 5817 (2057 CE), year 3 of each 19-year cycle will also be a month late. If the calendar is not amended then Passover will start to land on or after the summer solstice around AM 16652 (12892 CE). (The exact year when this will begin to occur depends on uncertainties in the future tidal slowing of the Earth rotation rate, and on the accuracy of predictions of precession and Earth axial tilt.)
The seriousness of the spring equinox drift is widely discounted on the grounds that Passover will remain in the spring season for many millennia, and the text of the Torah is generally not interpreted as having specified tight calendrical limits. Of course, the Hebrew calendar also drifts with respect to the autumn equinox, and at least part of the harvest festival of Sukkot is already more than a month after the equinox in years 1, 9, and 12 of each 19-year cycle; beginning in AM 5818 (2057 CE), this will also be the case in year 4. (These are the same year numbers as were mentioned for the spring season in the previous paragraph, except that they get incremented at Rosh Hashanah.) This progressively increases the probability that Sukkot will be cold and wet, making it uncomfortable or impractical to dwell in the traditional "succah" during Sukkot. The first winter seasonal prayer for rain is not recited until "Shemini Atzeret", after the end of Sukkot, yet it is becoming increasingly likely that the rainy season in Israel will start before the end of Sukkot.
No equinox or solstice will ever be more than a day or so away from its mean date according to the solar calendar, while nineteen Jewish years average 6939d 16h 33m 031⁄3s compared to the 6939d 14h 26m 15s of nineteen mean tropical years. This discrepancy has mounted up to six days, which is why the earliest Passover currently falls on 26 March (as in AM 5773 / 2013 CE).
Worked example.
Given the length of the year, the length of each month is fixed as described above, so the real problem in determining the calendar for a year is determining the number of days in the year. In the modern calendar this is determined in the following manner.
The day of Rosh Hashanah and the length of the year are determined by the time and the day of the week of the Tishrei "molad", that is, the moment of the average conjunction. Given the Tishrei "molad" of a certain year, the length of the year is determined as follows:
First, one must determine whether each year is an ordinary or leap year by its position in the 19-year Metonic cycle. Years 3, 6, 8, 11, 14, 17, and 19 are leap years.
Secondly, one must determine the number of days between the starting Tishrei "molad" (TM1) and the Tishrei "molad" of the next year (TM2). For calendar descriptions in general the day begins at 6 p.m., but for the purpose of determining Rosh Hashanah, a "molad" occurring on or after noon is treated as belonging to the next day (the first "deḥiyyah"). All months are calculated as 29d, 12h, 44m, 31⁄3s long (MonLen). Therefore, in an ordinary year TM2 occurs 12 × MonLen days after TM1. This is usually 354 calendar days after TM1, but if TM1 is on or after 3:11:20 a.m. and before noon, it will be 355 days. Similarly, in a leap year, TM2 occurs 13 × MonLen days after TM1. This is usually 384 days after TM1, but if TM1 is on or after noon and before 2:27:162⁄3 p.m., TM2 will be only 383 days after TM1. In the same way, from TM2 one calculates TM3. Thus the four natural year lengths are 354, 355, 383, and 384 days.
However, because of the holiday rules, Rosh Hashanah cannot fall on a Sunday, Wednesday, or Friday, so if TM2 is one of those days, Rosh Hashanah in year 2 is postponed by adding one day to year 1 (the second "deḥiyyah"). To compensate, one day is subtracted from year 2. It is to allow these adjustments that the system allows 385-day years (long leap) and 353-day years (short ordinary) besides the four natural year lengths.
But how can year 1 be lengthened if it is already a long ordinary year of 355 days or year 2 be shortened if it is a short leap year of 383 days? That is why the third and fourth "deḥiyyah"s are needed.
If year 1 is already a long ordinary year of 355 days, there will be a problem if TM1 is on a Tuesday, as that means TM2 falls on a Sunday and will have to be postponed, creating a 356-day year. In this case, Rosh Hashanah in year 1 is postponed from Tuesday (the third "deḥiyyah"). As it cannot be postponed to Wednesday, it is postponed to Thursday, and year 1 ends up with 354 days.
On the other hand, if year 2 is already a short year of 383 days there will be a problem if TM2 is on a Wednesday. because Rosh Hashanah in year 2 will have to be postponed from Wednesday to Thursday and this will cause year 2 to be only 382 days long. In this case, year 2 is extended by one day by postponing Rosh Hashanah in year 3 from Monday to Tuesday (the fourth "deḥiyyah" ), and year 2 will have 383 days.
Rectifying the Hebrew calendar.
The attribution of the fixed arithmetic Hebrew calendar solely to Hillel II has, however, been questioned by a few authors, such as Sasha Stern, who claim that the calendar rules developed gradually over several centuries.
Given the importance in Jewish ritual of establishing the accurate timing of monthly and annual times, some futurist writers and researchers have considered whether a "corrected" system of establishing the Hebrew date is required. The mean year of the current mathematically based Hebrew calendar has "drifted" an average of 7–8 days late relative to the equinox relationship that it originally had. It is not possible, however, for any individual Hebrew date to be a week or more "late", because Hebrew months always begin within a day or two of the "molad" moment. What happens instead is that the traditional Hebrew calendar "prematurely" inserts a leap month one year before it "should have been" inserted, where "prematurely" means that the insertion causes the spring equinox to land more than 30 days before the latest acceptable moment, thus causing the calendar to run "one month late" until the time when the leap month "should have been" inserted prior to the following spring. This presently happens in 4 years out of every 19-year cycle (years 3, 8, 11, and 19), implying that the Hebrew calendar currently runs "one month late" more than 21% of the time.
Dr. Irv Bromberg has proposed a 353-year cycle of 4366 months, which would include 130 leap months, along with use of a progressively shorter "molad" interval, which would keep an amended fixed arithmetic Hebrew calendar from drifting for more than seven millennia. It takes about 31⁄2 centuries for the spring equinox to drift an average of 1⁄19th of a "molad" interval earlier in the Hebrew calendar. That is a very important time unit, because it can be cancelled by simply truncating a 19-year cycle to 11 years, omitting 8 years including three leap years from the sequence. That is the essential feature of the 353-year leap cycle ((9 × 19) + 11 + (9 × 19) = 353 years).
Religious questions abound about how such a system might be implemented and administered throughout the diverse aspects of the world Jewish community.
Conversion between Jewish and civil calendars.
The list below gives a time which can be used to determine the day the Jewish ecclesiastical (spring) year starts over a period of nineteen years:
Every nineteen years this time is 2 days, 16 hours, 33 1/18 minutes later in the week. That is either the same or the previous day in the civil calendar, depending on whether the difference in the day of the week is three or two days. If 29 February is included fewer than five times in the nineteen - year period the date will be later by the number of days which corresponds to the difference between the actual number of insertions and five. If the year is due to start on Sunday, it actually begins on the following Tuesday if the following year is due to start on Friday morning. If due to start on Monday, Wednesday or Friday it actually begins on the following day. If due to start on Saturday, it actually begins on the following day if the previous year was due to begin on Monday morning.
The table below lists, for a Jewish year commencing on 23 March, the civil date of the first day of each month. If the year does not begin on 23 March, each month's first day will differ from the date shown by the number of days that the start of the year differs from 23 March. The correct column is the one which shows the correct starting date for the following year in the last row. If 29 February falls within a Jewish month the first day of later months will be a day earlier than shown.
For long period calculations, dates should be reduced to the Julian calendar and converted back to the civil calendar at the end of the calculation. The civil calendar used here (Exigian) is correct to one day in 44,000 years and omits the leap day in centennial years which do not give remainder 200 or 700 when divided by 900. It is identical to the Gregorian calendar between 15 October 1582 CE and 28 February 2400 CE (both dates inclusive).
To find how many days the civil calendar is ahead of the Julian in any year from 301BC (the calendar is proleptic (assumed) up to 1582 CE) add 300 to the year, multiply the hundreds by 7, divide by 9 and subtract 4. Ignore any fraction of a day. When the difference between the calendars changes the calculated value applies on and from March 1 (civil date) for conversions to Julian. For earlier dates reduce the calculated value by one. For conversions to the civil date the calculated value applies on and from February 29 (Julian date). Again, for earlier dates reduce the calculated value by one. The difference is applied to the calendar one is converting into. A negative value indicates that the Julian date is ahead of the civil date. In this case it is important to remember that when calculating the civil equivalent of February 29 (Julian), February 29 is discounted. Thus if the calculated value is -4 the civil equivalent of this date is February 24. Before 1 CE use astronomical years rather than years BCE. The astronomical year is (year BCE) - 1.
Up to the 4th century CE these tables give the day of the Jewish month to within a day or so and the number of the month to within a month or so. From the 4th century the number of the month is given exactly and from the 9th century the day of the month is given exactly as well.
In the Julian calendar, every 76 years the Jewish year is due to start 5h 47 14/18m earlier, and 3d 18h 12 4/18m later in the week.
On what civil date does the eighth month begin in CE 20874-5?
20874=2026+(248x76). In (248x76) Julian years the Jewish year is due to start (248x3d 18h 12 4/18m) later in the week, which is 932d 2h 31 2/18m or 1d 2h 31 2/18m later after removing complete weeks. Allowing for the current difference of thirteen days between the civil and Julian calendars the Julian date is 13+(248x0d 5h 47 4/18m) earlier, which is 72d 21h 28 16/18m earlier. Convert back to the civil calendar by applying the formula.
So in 20874 CE the Jewish year is due to begin 87d 2h 31 2/18m later than in 2026 CE and 1d 2h 31 2/18m later in the week. In 20874 CE, therefore, the Jewish year is due to begin at 11.30 3/18 A.M. on Friday, 14 June. Because of the displacements it actually begins on Saturday, 15 June. Odd months have 30 days and even months 29, so the starting dates are 2, 15 July; 3, 13 August; 4, 12 September; 5, 11 October; 6, 10 November; 7, 9 December, and 8, 8 January.
The rules are based on the theory which Maimonides explains in his book "Rabbinical Astronomy" - no allowance is made for the secular (centennial) decrease of 1/2 second in the length of the mean tropical year and increase of about four yards in the distance between the earth and the moon resulting from tidal friction because astronomy was not sufficiently developed in the 12th century (when Maimonides wrote his book) to detect this.
Bibliography.
723–730.

</doc>
<doc id="13786" url="http://en.wikipedia.org/wiki?curid=13786" title="The Holocaust Industry">
The Holocaust Industry

The Holocaust Industry: Reflections on the Exploitation of Jewish Suffering is a 2000 book by Norman G. Finkelstein in which he argues that the American Jewish establishment exploits the memory of the Nazi Holocaust for political and financial gain, as well as to further the interests of Israel. According to Finkelstein, this "Holocaust industry" has corrupted Jewish culture and the authentic memory of the Holocaust.
Finkelstein on the book.
Finkelstein states that his consciousness of "the Nazi holocaust" is rooted in his parents' experiences in the Warsaw Ghetto; with the exception of his parents themselves, "every family member on both sides was exterminated by the Nazis". Nonetheless, during his childhood, no one ever asked any questions about what his mother and father had suffered. He suggests, "This was not a respectful silence. It was indifference." It was only after the establishment of "the Holocaust industry", he suggests, that outpourings of anguish over the plight of the Jews in World War II began. This ideology in turn served to endow Israel with a status as "'victim' state" despite its "horrendous" human rights record.
According to Finkelstein, his book is "an anatomy and an indictment of the Holocaust industry". He argues that "'The Holocaust' is an ideological representation of the Nazi holocaust".
In the foreword to the first paperback edition, Finkelstein notes that the first hardback edition had been a considerable hit in several European countries and many languages, but had been largely ignored in the United States. He sees "The New York Times" as the main promotional vehicle of the "Holocaust industry", and notes that the 1999 Index listed 273 entries for the Holocaust and just 32 entries for the entire continent of Africa.
Chapters.
The second (2003) edition contained 100 pages of new material, primarily in chapter 3 on the World Jewish Congress lawsuit against Swiss banks. Finkelstein set out to provide a guide to the relevant sections of the case. He feels that the presiding judge elected not to docket crucial documents, and that the Claims Resolution Tribunal could no longer be trusted. Finkelstein claims the CRT was on course to vindicate the Swiss banks before it changed tack in order to "protect the blackmailers' reputation".
Reviews and critiques.
Noam Chomsky and Alexander Cockburn praised the book and gave favorable reviews. In addition, the Holocaust historian Raul Hilberg, who is considered an authority on the history of Holocaust, is on record as praising Finkelstein's book: 
I refer now to the part of the book that deals with the claims against the Swiss banks, and the other claims pertaining to forced labor. I would now say in retrospect that he was actually conservative, moderate and that his conclusions are trustworthy. He is a well-trained political scientist, has the ability to do the research, did it carefully, and has come up with the right results. I am by no means the only one who, in the coming months or years, will totally agree with Finkelstein's breakthrough."
On the other hand, many have argued that "The Holocaust Industry" is an unscholarly work that promotes antisemitic stereotypes. For example, according to Israeli journalist Yair Sheleg, in August 2000, German historian Hans Mommsen called it "a most trivial book, which appeals to easily aroused anti-Semitic prejudices." Wolfgang Benz stated to "Le Monde": "It is impossible to learn anything from Finkelstein's book. At best, it is interesting for a psychotherapist." The reviewer of this daily added that Norman Finkelstein "hardly cares about nuance" and Rony Brauman wrote in the preface to the French edition ("L'Industrie de l'Holocauste", Paris, La Fabrique, 2001) that some assertions of N. Finkelstein (especially on the impact of the Six-days war) are wrong, others being pieces of "propaganda".
Historian Omer Bartov criticized Finkelstein's notion of Holocaust profiteers as a "novel variation of "'The Protocols of the Elders of Zion" '". In reviewing the first edition of the book, Bartov wrote:
Finkelstein was to later blame Bartov's review for the poor US sales of the book.
University of Chicago Professor Peter Novick, whose work Finkelstein described as providing the "initial stimulus" for "The Holocaust Industry", asserted in the July 28, 2000 "Jewish Chronicle" (London) that the book is replete with "false accusations", "egregious misrepresentations", "absurd claims" and "repeated mis-statements" ("A charge into darkness that sheds no light"). Finkelstein replied to the "hysterical" allegations by Novick on his homepage.
Hasia Diner has accused Peter Novick and Finkelstein of being "harsh critics of American Jewry from the left," and challenges the notion reflected in their books that American Jews did not begin to commemorate the Holocaust until post 1967.
Andrew Ross reviewing the book for Salon magazine wrote:
On the issue of reparations, he barely acknowledges the wrongs committed by the Swiss and German institutions — the burying of Jewish bank accounts, the use of slave labor — that gave rise to the recent reparations drive. The fear that the reparations will not wind up in the hands of those who need and deserve them most is a legitimate concern. But the idea that survivors have been routinely swindled by Jewish institutions is a gross distortion. The chief reason why survivors have so far seen nothing of the $1.25 billion Swiss settlement, reached in 1998, is that U.S. courts have yet to rule on a method of distribution. On other reparations and compensation settlements, the Claims Conference, a particular bete noire of Finkelstein, says that it distributed approximately $220 million to individual survivors in 1999 alone."
Finkelstein's response to the critics.
Finkelstein responded to his critics in the foreword to the second edition:
Mainstream critics allege that I conjured a "conspiracy theory" while those on the Left ridicule the book as a defense of "the banks". None, so far as I can tell, question my actual findings."
Other topics.
Fraudulent writings on the Holocaust.
Finkelstein describes two known frauds, that of "The Painted Bird" by Polish writer Jerzy Kosinski and "Fragments" by Binjamin Wilkomirski, and how they were defended by people even after they had been exposed. He identifies some of these people as members of the "Holocaust Industry", and notes that they also support each other. Elie Wiesel supported Kosinski; Israel Gutman and Daniel Goldhagen (see below) supported Wilkomirski; Wiesel and Gutman support Goldhagen.
Holocaust Industry defends itself.
Finkelstein has published heavy criticisms of several books in his career, as he did to "Hitler's Willing Executioners" by Daniel Johnah Goldhagen, which he calls "replete with gross misinterpretations of source material and internal contradictions", and says "the book is devoid of scholarly value". Independently, Ruth Bettina Birn (the world's leading authority on the archives that Goldhagen had consulted and chief historian for War Crimes with the Canadian Department of Justice) did the same - she and Finkelstein worked together on "A Nation on Trial: The Goldhagen Thesis and Historical Truth". Goldhagen refused the journal's invitation for a full rebuttal, and instead enlisted a London law firm to sue Birn and the Cambridge University Press. Protests were made to Birn's employer, calling her "a member of the perpetrator race" (she is German-born), prompting an official investigation of her.(p. 66) 
Other genocides.
Finkelstein scathingly compared the media treatment of the Holocaust and the media treatment of other genocides such as the Holodomor and the Armenian Genocide, particularly by members of what he calls "The Holocaust Industry". 1 to 1.5 million Armenians died in the years between 1915 and 1917/1923 - denial includes the claim that they were the result of a Civil War within World War I, or refusal to accept there were deaths. In 2001, Israeli Foreign Minister Shimon Peres went so far as to dismiss it as "allegations". However, by this time historical consensus was changing, and he was "angrily compared ... to a holocaust denier" by Israel Charny, executive director of the Institute on the Holocaust and Genocide in Jerusalem.
In August 2007, the Elie Wiesel Foundation for Humanity produced a letter signed by 53 Nobel Laureates re-affirming the Genocide Scholars' conclusion that the 1915 killings of Armenians constituted genocide. However, Wiesel's organization asserted there would be no legal "basis for reparations or territorial claims", anticipating Turkish anxieties that it could prompt financial or property claims. Abraham Foxman of the Anti-Defamation League announced: "Upon reflection, the consequences of those actions were indeed tantamount to genocide".
Other forms of Holocaust denial.
According to Finkelstein, Elie Wiesel characterizes any suggestion that he has profited from the "Holocaust Industry", or even any criticism at all, as Holocaust denial. Questioning a survivor's testimony, denouncing the role of Jewish collaborators, suggesting that Germans suffered during the bombing of Dresden or that any state except Germany committed crimes in World War II are all evidence of Holocaust denial – according to Deborah Lipstadt – and the most "insidious" forms of Holocaust denial are "immoral equivalencies", denying the uniqueness of The Holocaust. Finkelstein examines the implications of applying this standard to another member of the "Holocaust Industry", Daniel Goldhagen, who argued that Serbian actions in Kosovo "are, in their essence, different from those of Nazi Germany only in scale".
Holocaust deniers in real life.
According to Finkelstein, Deborah Lipstadt claims there is widespread Holocaust denial - yet in "Denying the Holocaust" her prime example is Arthur Butz, author of "The Hoax of the Twentieth Century". The chapter on him is entitled "Entering the Mainstream" - but Finkelstein considers that, were it not for the likes of Lipstadt, no one would ever have heard of Arthur Butz. Holocaust deniers have as much influence in the US as the Flat Earth Society (p. 69). Finkelstein believes there to be only one "truly mainstream" holocaust denier—Bernard Lewis, who was convicted in France of denying the Armenian genocide. Since Lewis is pro-Israel, "this instance ... raises no hackles in the United States."
Publishing history.
Publishing history of "The Holocaust Industry":

</doc>
<doc id="13787" url="http://en.wikipedia.org/wiki?curid=13787" title="Hermetic Order of the Golden Dawn">
Hermetic Order of the Golden Dawn

The Hermetic Order of the Golden Dawn (or, more commonly, The Golden Dawn) was an organization devoted to the study and practice of the occult, metaphysics, and paranormal activities during the late 19th and early 20th centuries. Known as a magical order, the Hermetic Order of the Golden Dawn was active in Great Britain and focused its practices on theurgy and spiritual development. Many present-day concepts of ritual and magic that are at the centre of contemporary traditions, such as Wicca and Thelema, were inspired by the Golden Dawn, which became one of the largest single influences on 20th-century Western occultism.
The three founders, William Robert Woodman, William Wynn Westcott, and Samuel Liddell MacGregor Mathers, were Freemasons and members of Societas Rosicruciana in Anglia (S.R.I.A.). Westcott appears to have been the initial driving force behind the establishment of the Golden Dawn.
The Golden Dawn system was based on hierarchy and initiation like the Masonic Lodges; however women were admitted on an equal basis with men. The "Golden Dawn" was the first of three Orders, although all three are often collectively referred to as the "Golden Dawn". The First Order taught esoteric philosophy based on the Hermetic Qabalah and personal development through study and awareness of the four Classical Elements as well as the basics of astrology, tarot divination, and geomancy. The Second or "Inner" Order, the "Rosae Rubeae et Aureae Crucis" (the Ruby Rose and Cross of Gold), taught proper magic, including scrying, astral travel, and alchemy. The Third Order was that of the "Secret Chiefs", who were said to be highly skilled; they supposedly directed the activities of the lower two orders by spirit communication with the Chiefs of the Second Order.
Influences.
Influences on Golden Dawn concepts and work include: Christian mysticism, Qabalah, Hermeticism, Ancient Egyptian religion, Theurgy, Freemasonry, Alchemy, Theosophy, Astrology, Eliphas Levi, Papus, John Dee & Edward Kelly, Enochian magic, and Renaissance grimoires, as well as Anna Kingsford & Frederick Hockley.
History.
Cipher Manuscripts.
The foundational documents of the original Order of the Golden Dawn, known as the Cipher Manuscripts, are written in English using Trithemius cipher. The manuscripts give the specific outlines of the Grade Rituals of the Order and prescribe a curriculum of graduated teachings that encompass the Hermetic Qabalah, astrology, occult tarot, geomancy, and alchemy.
According to the records of the Order, the manuscripts passed from Kenneth R. H. Mackenzie, a Masonic scholar, to the Rev. A. F. A. Woodford, whom British occult writer Francis King describes as the fourth founder (although Woodford died shortly after the Order was founded). The documents did not excite Woodford, and in February 1886 he passed them on to Freemason William Wynn Westcott, who managed to decode them in 1887. Westcott, pleased with his discovery, called on fellow Freemason Samuel Liddell MacGregor Mathers for a second opinion. Westcott asked for Mathers' help to turn the manuscripts into a coherent system for lodge work. Mathers in turn asked fellow Freemason William Robert Woodman to assist the two, and he accepted. Mathers and Westcott have been credited with developing the ritual outlines in the Cipher Manuscripts into a workable format. Mathers, however, is generally credited with the design of the curriculum and rituals of the Second Order, which he called the "Rosae Rubae et Aureae Crucis" ("Ruby Rose and Golden Cross" or the "RR et AC").
Founding of first temple.
In October 1887, Westcott claimed to have written to a German countess and prominent Rosicrucian named Anna Sprengel, whose address was said to have been found in the decoded Cipher Manuscripts. According to Westcott, Sprengel claimed the ability to contact certain supernatural entities, known as the Secret Chiefs, that were considered the authorities over any magical order or esoteric organization. Westcott purportedly received a reply from Sprengel granting permission to establish a Golden Dawn temple and conferring honorary grades of Adeptus Exemptus on Westcott, Mathers, and Woodman. The temple was to consist of the five grades outlined in the manuscripts.
In 1888, the Isis-Urania Temple was founded in London. In contrast to the S.R.I.A. and Masonry, women were allowed and welcome to participate in the Order in "perfect equality" with men. The Order was more of a philosophical and metaphysical teaching order in its early years. Other than certain rituals and meditations found in the Cipher manuscripts and developed further, "magical practices" were generally not taught at the first temple.
For the first four years, the Golden Dawn was one cohesive group later known as "the Outer Order" or "First Order." An "Inner Order" was established and became active in 1892. The Inner Order consisted of members known as "adepts," who had completed the entire course of study for the Outer Order. This group of adepts eventually became known as the Second Order.
Eventually, the Osiris temple in Weston-super-Mare, the Horus temple in Bradford (both in 1888), and the Amen-Ra temple in Edinburgh (1893) were founded. In 1893 Mathers founded the Ahathoor temple in Paris.
Secret Chiefs.
In 1891, Westcott's alleged correspondence with Anna Sprengel suddenly ceased. He claimed to have received word from Germany that she was either dead or that her companions did not approve of the founding of the Order and no further contact was to be made. If the founders were to contact the Secret Chiefs, apparently, it had to be done on their own. In 1892, Mathers professed that a link to the Secret Chiefs had been established. Subsequently, he supplied rituals for the Second Order, calling them the Red Rose and Cross of Gold. The rituals were based on the tradition of the tomb of Christian Rosenkreuz, and a "Vault of Adepts" became the controlling force behind the Outer Order. Later in 1916, Westcott claimed that Mathers also constructed these rituals from materials he received from Frater Lux ex Tenebris, a purported "Continental Adept".
Some followers of the Golden Dawn tradition believe that the Secret Chiefs were not human or supernatural beings but, rather, symbolic representations of actual or legendary sources of spiritual esotericism. The term came to stand for a great leader or teacher of a spiritual path or practice that found its way into the teachings of the Order.
Golden Age.
By the mid-1890s, the Golden Dawn was well established in Great Britain, with over one hundred members from every class of Victorian society. Many celebrities belonged to the Golden Dawn, such as actress Florence Farr, Irish revolutionary Maud Gonne, Irish writer William Butler Yeats, Welsh author Arthur Machen, and English authors Evelyn Underhill, and Aleister Crowley.
In 1896 or 1897, Westcott broke all ties to the Golden Dawn, leaving Mathers in control. It has been speculated that his departure was due to his having lost a number of occult-related papers in a hansom cab. Apparently, when the papers were found, Westcott's connection to the Golden Dawn was discovered and brought to the attention of his employers. He may have been told to either resign from the Order or to give up his occupation as coroner. After Westcott's departure, Mathers appointed Florence Farr to be Chief Adept in Anglia. Dr. Henry B. Pullen Burry succeeded Westcott as Cancellarius—one of the three Chiefs of the Order.
Mathers was the only active founding member after Westcott's departure. Due to personality clashes with other members and frequent absences from the center of Lodge activity in Great Britain, however, challenges to Mathers's authority as leader developed among the members of the Second Order.
Revolt.
Toward the end of 1899, the Adepts of the Isis-Urania and Amen-Ra temples had become dissatisfied with Mathers' leadership, as well as his growing friendship with Aleister Crowley. They had also become anxious to make contact with the Secret Chiefs themselves, instead of relying on Mathers as an intermediary. Within the Isis-Urania temple, disputes were arising between Farr's "The Sphere", a secret society within the Isis-Urania, and the rest of the Adepti Minores.
Crowley was refused initiation into the Adeptus Minor grade by the London officials. Mathers overrode their decision and quickly initiated him at the Ahathoor temple in Paris on January 16, 1900. Upon his return to the London temple, Crowley requested from Miss Cracknell, the acting secretary, the papers acknowledging his grade, to which he was now entitled. To the London Adepts, this was the final straw. Farr, already of the opinion that the London temple should be closed, wrote to Mathers expressing her wish to resign as his representative, although she was willing to carry on until a successor was found. Mathers believed Westcott was behind this turn of events and replied on February 16. On March 3, a committee of seven Adepts was elected in London, and requested a full investigation of the matter. Mathers sent an immediate reply, declining to provide proof, refusing to acknowledge the London temple, and dismissing Farr as his representative on March 23. In response, a general meeting was called on March 29 in London to remove Mathers as chief and expel him from the Order.
Splinters.
In 1901, W. B. Yeats privately published a pamphlet titled "Is the Order of R. R. & A. C. to Remain a Magical Order?"
After the Isis-Urania temple claimed its independence, there were even more disputes, leading to Yeats resigning. A committee of three was to temporarily govern, which included P.W. Bullock, M.W. Blackden and J. W. Brodie-Innes. After a short time, Bullock resigned, and Dr. Robert Felkin took his place.
In 1903, A.E. Waite and Blackden joined forces to retain the name Isis-Urania, while Felkin and other London members formed the Stella Matutina. Yeats remained in the Stella Matutina until 1921, while Brodie-Innes continued his Amen-Ra membership in Edinburgh.
Reconstruction.
Once Mathers realised that reconciliation was impossible, he made efforts to reestablish himself in London. The Bradford and Weston-super-Mare temples remained loyal to him, but their numbers were few. He then appointed Edward Berridge as his representative. According to Francis King, historical evidence shows that there were "twenty three members of a flourishing Second Order under Berridge-Mathers in 1913."
J.W. Brodie-Innes continued leading the Amen-Ra temple, deciding that the revolt was unjustified. By 1908, Mathers and Brodie-Innes were in complete accord. According to sources that differ regarding the actual date, sometime between 1901 and 1913 Mathers renamed the branch of the Golden Dawn remaining loyal to his leadership to Alpha et Omega. Brodie-Innes assumed command of the English and Scottish temples, while Mathers concentrated on building up his Ahathoor temple and extending his American connections. According to occultist Israel Regardie, the Golden Dawn had spread to the United States of America before 1900 and a Thoth-Hermes temple had been founded in Chicago. By the beginning of the First World War in 1914, Mathers had established two to three American temples.
Most temples of the Alpha et Omega and Stella Matutina closed or went into abeyance by the end of the 1930s, with the exceptions of two Stella Matutina temples: Hermes Temple in Bristol, which operated sporadically until 1970, and the Smaragdum Thallasses Temple (commonly referred to as Whare Ra) in Havelock North, New Zealand, which operated regularly until its closure in 1978.
Structure and grades.
Much of the hierarchical structure for the Golden dawn came from the Societas Rosicruciana in Anglia, which was itself derived from the Order of the Golden and Rosy Cross.
The paired numbers attached to the Grades relate to positions on the Tree of Life. The Neophyte Grade of "0=0" indicates no position on the Tree. In the other pairs, the first numeral is the number of steps up from the bottom (Malkuth), and the second numeral is the number of steps down from the top (Kether).
The First Order Grades were related to the four elements of Earth, Air, Water, and Fire, respectively. The Aspirant to a Grade received instruction on the metaphysical meaning of each of these Elements and had to pass a written examination and demonstrate certain skills to receive admission to that Grade.
The Portal Grade was an "Invisible" or in-between grade separating the First Order from the Second Order. The Circle of existing Adepts from the Second Order had to consent to allow an Aspirant to be initiated as an Adept and join the Second Order.
The Second Order was not, properly, part of the "Golden Dawn", but a separate Order in its own right, known as the R.R. et A.C. The Second Order directed the teachings of the First Order and was the governing force behind the First Order.
After passing the Portal, the Aspirant was instructed in the techniques of practical magic. When another examination was passed, and the other Adepts consented, the Aspirant attained the Grade of Adeptus Minor (5=6). There were also four sub-Grades of instruction for the Adeptus Minor, again relating to the four Outer Order grades.
A member of the Second Order had the power and authority to initiate aspirants to the First Order, though usually not without the permission of the Chiefs of his or her Lodge.
"The Golden Dawn" book.
The encyclopedic text "The Golden Dawn", by Israel Regardie, has been the most intensively used source for modern western occult and magical practice.
Contemporary Golden Dawn orders.
While no temples in the original chartered lineage of the Golden Dawn survived past the 1970s, several organizations have since revived its teachings and rituals. Among these, the following are notable:

</doc>
<doc id="13790" url="http://en.wikipedia.org/wiki?curid=13790" title="Hash function">
Hash function

A hash function is any function that can be used to map digital data of arbitrary size to digital data of fixed size. The values returned by a hash function are called hash values, hash codes, hash sums, or simply hashes. 
One use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. An example is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.
Hash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.
Uses.
Hash tables.
Hash functions are primarily used in hash tables, to quickly locate a data record (e.g., a dictionary definition) given its search key (the headword). Specifically, the hash function is used to map the search key to an index; the index gives the place in the hash table where the corresponding record should be stored. Hash tables, in turn, are used to implement associative arrays and dynamic sets.
Typically, the domain of a hash function (the set of possible keys) is larger than its range (the number of different table indexes), and so it will map several different keys to the same index. Therefore, each slot of a hash table is associated with (implicitly or explicitly) a set of records, rather than a single record. For this reason, each slot of a hash table is often called a "bucket", and hash values are also called "bucket indices".
Thus, the hash function only hints at the record's location — it tells where one should start looking for it. Still, in a half-full table, a good hash function will typically narrow the search down to only one or two entries.
Caches.
Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison.
Bloom filters.
Hash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.
Finding duplicate records.
When storing records in a large unsorted file, one may use a hash function to map each record to an index into a table "T", and to collect in each bucket "T"["i"] a list of the numbers of all records with the same hash value "i". Once the table is complete, any two duplicate records will end up in the same bucket. The duplicates can then be found by scanning every bucket "T"["i"] which contains two or more members, fetching those records, and comparing them. With a table of appropriate size, this method is likely to be much faster than any alternative approach (such as sorting the file and comparing all consecutive pairs).
Protecting data.
A hash value can be used to uniquely identify secret information. This requires that the hash function is collision resistant, which means that it is very hard to find data that generate the same hash value. These functions are categorized into cryptographic hash functions and provably secure hash functions. Functions in the second category are the most secure but also too slow for most practical purposes. Collision resistance is accomplished in part by generating very large hash values. For example SHA-1, one of the most widely used cryptographic hash functions, generates 160 bit values.
Finding similar records.
Hash functions can also be used to locate table records whose key is similar, but not identical, to a given key; or pairs of records in a large file which have similar keys. For that purpose, one needs a hash function that maps similar keys to hash values that differ by at most "m", where "m" is a small integer (say, 1 or 2). If one builds a table "T" of all record numbers, using such a hash function, then similar records will end up in the same bucket, or in nearby buckets. Then one need only check the records in each bucket "T"["i"] against those in buckets "T"["i"+"k"] where "k" ranges between −"m" and "m".
This class includes the so-called acoustic fingerprint algorithms, that are used to locate similar-sounding entries in large collection of audio files. For this application, the hash function must be as insensitive as possible to data capture or transmission errors, and to trivial changes such as timing and volume changes, compression, etc.
Finding similar substrings.
The same techniques can be used to find equal or similar stretches in a large collection of strings, such as a document repository or a genomic database. In this case, the input strings are broken into many small pieces, and a hash function is used to detect potentially equal pieces, as above.
The Rabin–Karp algorithm is a relatively fast string searching algorithm that works in O("n") time on average. It is based on the use of hashing to compare strings.
Geometric hashing.
This principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on. In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of "cells". The table is often an array with two or more indices (called a "grid file", "grid index", "bucket grid", and similar names), and the hash function returns an index tuple. This special case of hashing is known as geometric hashing or "the grid method". Geometric hashing is also used in telecommunications (usually under the name vector quantization) to encode and compress multi-dimensional signals.
Standard uses of hashing in cryptography.
Some standard applications that employ hash functions include authentication, message integrity (using an HMAC (Hashed MAC)), message fingerprinting, data corruption detection, and digital signature efficiency.
Properties.
Good hash functions, in the original sense of the term, are usually required to satisfy certain properties listed below. The exact requirements are dependent on the application, for example a hash function well suited to indexing data will probably be a poor choice for a cryptographic hash function.
Determinism.
A hash procedure must be deterministic—meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed, because that address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.
Uniformity.
A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of "collisions"—pairs of inputs that are mapped to the same hash value—increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.
Note that this criterion only requires the value to be "uniformly distributed", not "random" in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.
Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.
In other words, if a typical set of "m" records is hashed to "n" table slots, the probability of a bucket receiving many more than "m"/"n" records should be vanishingly small. In particular, if "m" is less than "n", very few buckets should have more than one or two records. (In an ideal "perfect hash function", no bucket should have more than one record; but a small number of collisions is virtually inevitable, even if "n" is much larger than "m" – see the birthday paradox).
When testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test.
Defined range.
It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. On the other hand, cryptographic hash functions produce much larger hash values, in order to ensure the computational complexity of brute-force inversion. For example SHA-1, one of the most widely used cryptographic hash functions, produces a 160-bit value.
Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. In cryptographic hash functions, these chunks are processed by a one-way compression function, with the last chunk being padded if necessary. In this case, their size, which is called "block size", is much bigger than the size of the hash value. For example, in SHA-1, the hash value is 160 bits and the block size 512 bits.
Variable range.
In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data "z", and the number "n" of allowed hash values.
A common solution is to compute a fixed hash function with a very large range (say, 0 to 232 − 1), divide the result by "n", and use the division's remainder. If "n" is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and "n" − 1, for any value of "n" that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of "n", e.g. odd or prime numbers.
We can allow the table size "n" to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let "n" be significantly less than 2"b". Consider a pseudorandom number generator (PRNG) function "P"(key) that is uniform on the interval [0, 2"b" − 1]. A hash function uniform on the interval [0, n-1] is "n" "P"(key)/2"b". We can replace the division by a (possibly faster) right bit shift: "nP"(key) » "b".
Variable range with minimal movement (dynamic hash function).
When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.
A hash function that will relocate the minimum number of records when the table is – where "z" is the key being hashed and "n" is the number of allowed hash values – such that "H"("z","n" + 1) = "H"("z","n") with probability close to "n"/("n" + 1).
Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property.
Extendible hashing uses a dynamic hash function that requires space proportional to "n" to compute the hash function, and it becomes a function of the previous keys that have been inserted.
Several algorithms that preserve the uniformity property but require time proportional to "n" to compute the value of "H"("z","n") have been invented.
Data normalization.
In some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.
Continuity.
"A hash function that is used to search for similar (as opposed to equivalent) data must be as continuous as possible; two inputs that differ by a little should be mapped to equal or nearly equal hash values."
Note that continuity is usually considered a fatal flaw for checksums, cryptographic hash functions, and other related concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in Nearest neighbor search.
Non-invertible.
In cryptographic applications, hash functions are typically expected to be practically non-invertible, meaning that it is not realistic to reconstruct the input datum x from its hash value h(x) alone without spending great amounts of computing time (see also One-way function).
Hash function algorithms.
For most types of hashing functions the choice of the function depends strongly on the nature of the input data, and their probability distribution in the intended application.
Trivial hash function.
If the datum to be hashed is small enough, one can use the datum itself (reinterpreted as an integer) as the hashed value. The cost of computing this "trivial" (identity) hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.
The meaning of "small enough" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer codice_1 and 32-bit floating-point codice_2 objects can simply use the value directly; whereas the 64-bit integer codice_3 and 64-bit floating-point codice_4 cannot use this method.
Other types of data can also use this perfect hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character ("A" for "a", "8" for "8", etc.). If each character is stored in 8 bits (as in ASCII or ISO Latin 1), the table has only 28 = 256 entries; in the case of Unicode characters, the table would have 17×216 = 1114112 entries.
The same technique can be used to map two-letter country codes like "us" or "za" to country names (262=676 table entries), 5-digit zip codes like 13083 to city names (100000 entries), etc. Invalid data values (such as the country code "xx" or the zip code 00000) may be left undefined in the table, or mapped to some appropriate "null" value.
Perfect hashing.
A hash function that is injective—that is, maps each valid input to a different hash value—is said to be perfect. With such a function one can directly locate the desired entry in a hash table, without any additional searching.
Minimal perfect hashing.
A perfect hash function for "n" keys is said to be minimal if its range consists of "n" "consecutive" integers, usually from 0 to "n"−1. Besides providing single-step lookup, a minimal perfect hash function also yields a compact hash table, without any vacant slots. Minimal perfect hash functions are much harder to find than perfect ones with a wider range.
Hashing uniformly distributed data.
If the inputs are bounded-length strings and each input may independently occur with uniform probability (such as telephone numbers, car license plates, invoice numbers, etc.), then a hash function needs to map roughly the same number of inputs to each hash value. For instance, suppose that each input is an integer "z" in the range 0 to "N"−1, and the output must be an integer "h" in the range 0 to "n"−1, where "N" is much larger than "n". Then the hash function could be "h" = "z" mod "n" (the remainder of "z" divided by "n"), or "h" = ("z" × "n") ÷ "N" (the value "z" scaled down by "n"/"N" and truncated to an integer), or many other formulas.
"h" = "z" mod "n" was used in many of the original random number generators, but was found to have a number of issues. One of which is that as "n" approaches "N", this function becomes less and less uniform.
Hashing data with other distributions.
These simple formulas will not do if the input values are not equally likely, or are not independent. For instance, most patrons of a supermarket will live in the same geographic area, so their telephone numbers are likely to begin with the same 3 to 4 digits. In that case, if "m" is 10000 or so, the division formula ("z" × "m") ÷ "M", which depends mainly on the leading digits, will generate a lot of collisions; whereas the remainder formula "z" mod "m", which is quite sensitive to the trailing digits, may still yield a fairly even distribution.
Hashing variable-length data.
When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, very characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.
In cryptographic hash functions, a Merkle–Damgård construction is usually used. In general, the scheme for hashing such data is to break the input into a sequence of small units (bits, bytes, words, etc.) and combine all the units "b"[1], "b"[2], …, "b"["m"] sequentially, as follows
 S ← S0; // "Initialize the state."
 for k in 1, 2, ..., m do // "Scan the input data units:"
 S ← F(S, b[k]); // "Combine data unit k into the state."
 return G(S, n) // "Extract the hash value from the state."
This schema is also used in many text checksum and fingerprint algorithms. The state variable "S" may be a 32- or 64-bit unsigned integer; in that case, "S0" can be 0, and "G"("S","n") can be just "S" mod "n". The best choice of "F" is a complex issue and depends on the nature of the data. If the units "b"["k"] are single bits, then "F"("S","b") could be, for instance
 if highbit(S) = 0 then
 return 2 * S + b
 else
 return (2 * S + b) ^ P
Here "highbit"("S") denotes the most significant bit of "S"; the '*' operator denotes unsigned integer multiplication with lost overflow; '^' is the bitwise exclusive or operation applied to words; and "P" is a suitable fixed word.
Special-purpose hash functions.
In many cases, one can design a special-purpose (heuristic) hash function that yields many fewer collisions than a good general-purpose hash function. For example, suppose that the input data are file names such as FILE0000.CHK, FILE0001.CHK, FILE0002.CHK, etc., with mostly sequential numbers. For such data, a function that extracts the numeric part "k" of the file name and returns "k" mod "n" would be nearly optimal. Needless to say, a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution.
Rolling hash.
In some applications, such as substring search, one must compute a hash function "h" for every "k"-character substring of a given "n"-character string "t"; where "k" is a fixed integer, and "n" is "k". The straightforward solution, which is to extract every such substring "s" of "t" and compute "h"("s") separately, requires a number of operations proportional to "k"·"n". However, with the proper choice of "h", one can use the technique of rolling hash to compute all those hashes with an effort proportional to "k" + "n".
Universal hashing.
A universal hashing scheme is a randomized algorithm that selects a hashing function "h" among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/"n", where "n" is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will however have more collisions than perfect hashing, and may require more operations than a special-purpose hash function. See also Unique Permutation Hashing.
Hashing with checksum functions.
One can adapt certain checksum or fingerprinting algorithms for use as hash functions. Some of those algorithms will map arbitrary long string data "z", with any typical real-world distribution—no matter how non-uniform and dependent—to a 32-bit or 64-bit string, from which one can extract a hash value in 0 through "n" − 1.
This method may produce a sufficiently uniform distribution of hash values, as long as the hash range size "n" is small compared to the range of the checksum or fingerprint function. However, some checksums fare poorly in the avalanche test, which may be a concern in some applications. In particular, the popular CRC32 checksum provides only 16 bits (the higher half of the result) that are usable for hashing. Moreover, each bit of the input has a deterministic effect on each bit of the CRC32, that is one can tell without looking at the rest of the input, which bits of the output will flip if the input bit is flipped; so care must be taken to use all 32 bits when computing the hash from the checksum.
Hashing with cryptographic hash functions.
Some cryptographic hash functions, such as SHA-1, have even stronger uniformity guarantees than checksums or fingerprints, and thus can provide very good general-purpose hashing functions.
In ordinary applications, this advantage may be too small to offset their much higher cost. However, this method can provide uniformly distributed hashes even when the keys are chosen by a malicious agent. This feature may help to protect services against denial of service attacks.
Hashing By Nonlinear Table Lookup.
Tables of random numbers (such as 256 random 32 bit integers) can provide high-quality nonlinear functions to be used
as hash functions or for other purposes such as cryptography. The key to be hashed would be split into 8-bit (one byte) parts and each part will be used as an index for the nonlinear table. The table values will be added by arithmetic or XOR addition to the hash output value. Because the table is just 1024 bytes in size, it will fit into the cache of modern microprocessors and allow for very fast execution of the hashing algorithm. As the table value is on average much longer than 8 bits, one bit of input will affect nearly all output bits. This is different from multiplicative hash functions where higher-value input bits do not affect lower-value output bits.
This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer number keys).
Efficient Hashing Of Strings.
Modern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these "wide word" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The remaining characters of the string which are smaller than the word length of the CPU must be handled differently (e.g. being processed one character at a time).
This approach has proven to speed up hash code generation by a factor of five or more on modern microprocessors of
a word size of 64 bit.
Another approach is to convert strings to a 32 or 64 bit numeric value and then apply a hash function. One method that avoids the problem of strings having great similarity ("Aaaaaaaaaa" and "Aaaaaaaaab") is to use a Cyclic redundancy check (CRC) of the string to compute a 32- or 64-bit value. While it is possible that two different strings will have the same CRC, the likelihood is very small and only requires that one check the actual string found to determine whether one has an exact match. CRCs will be different for strings such as "Aaaaaaaaaa" and "Aaaaaaaaab". Although, CRC codes can be used as hash values they are not cryptographically secure since they are not collision resistant.
Locality-sensitive hashing.
Locality-sensitive hashing (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in cryptography, as in this case the goal is to maximize the probability of "collision" of similar items rather than to avoid collisions.
One example of LSH is MinHash algorithm used for finding similar documents (such as web-pages):
Let "h" be a hash function that maps the members of "A" and "B" to distinct integers, and for any set "S" define "h"min("S") to be the member "x" of "S" with the minimum value of "h"("x"). Then "h"min("A") = "h"min("B") exactly when the minimum hash value of the union "A" ∪ "B" lies in the intersection "A" ∩ "B".
Therefore,
In other words, if "r" is a random variable that is one when "h"min("A") = "h"min("B") and zero otherwise, then "r" is an unbiased estimator of "J"("A","B"), although it has too high a variance to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.
Origins of the term.
The term "hash" comes by way of analogy with its non-technical meaning, to "chop and mix". Indeed, typical hash functions, like the mod operation, "chop" the input domain into many sub-domains that get "mixed" into the output range to improve the uniformity of the key distribution.
Donald Knuth notes that Hans Peter Luhn of IBM appears to have been the first to use the concept, in a memo dated January 1953, and that Robert Morris used the term in a survey paper in CACM which elevated the term from technical jargon to formal terminology.

</doc>
<doc id="13791" url="http://en.wikipedia.org/wiki?curid=13791" title="High jump">
High jump

The high jump is a track and field event in which competitors must jump over a horizontal bar placed at measured heights without the aid of certain devices. In its modern most practiced format, auxiliary weights and mounds have been used for assistance; rules have changed over the years. Over the centuries since, competitors have introduced increasingly more effective techniques to arrive at the current form.
Javier Sotomayor (Cuba) is the current men's record holder with a jump of 2.45 m ( ft  in) set in 1993, the longest standing record in the history of the men's high jump. Stefka Kostadinova (Bulgaria) has held the women's world record at 2.09 m ( ft  in) since 1987, also the longest-held record in the event.
Rules.
Jumpers must take off on one foot.
A jump is considered a failure if the bar is dislodged by the action of the jumper whilst jumping or the jumper touches the ground or breaks the plane of the near edge of the bar before clearance. The technique one uses for the jump must be almost flawless in order to have a chance of clearing a high bar.
Competitors may begin jumping at any height announced by the chief judge, or may pass, at their own discretion. Three consecutive missed jumps, at any height or combination of heights, will eliminate the jumper from competition.
The victory goes to the jumper who clears the greatest height during the final. If two or more jumpers tie for first place, the tie-breakers are: 1) The fewest misses at the height at which the tie occurred; and 2) The fewest misses throughout the competition.
If the event remains tied, the jumpers have a jump-off, beginning at the next greater height. Each jumper has one attempt. The bar is then alternately lowered and raised until only one jumper succeeds at a given height.
History.
The first recorded high jump event took place in Scotland in the 19th century. Early jumpers used either an elaborate straight-on approach or a scissors technique. In the latter, the bar was approached diagonally, and the jumper threw first the inside leg and then the other over the bar in a scissoring motion. Around the turn of the 20th century, techniques began to modernise, starting with the Irish-American Michael Sweeney's "Eastern cut-off". By taking off like the scissors, but extending his back and flattening out over the bar, Sweeney achieved a more economic clearance and raised the world record to 1.97 m ( ft  in) in 1895.
Another American, George Horine, developed an even more efficient technique, the "Western roll". In this style, the bar again is approached on a diagonal, but the inner leg is used for the take-off, while the outer leg is thrust up to lead the body sideways over the bar. Horine increased the world standard to 2.01 m ( ft  in) in 1912. His technique was predominant through the Berlin Olympics of 1936, in which the event was won by Cornelius Johnson at 2.03 m ( ft  in).
American and Soviet jumpers held the playing field for the next four decades, and they pioneered the evolution of the straddle technique. Straddle jumpers took off as in the Western roll, but rotated their (belly-down) torso around the bar, obtaining the most economical clearance up to that time. Straddle-jumper Charles Dumas was the first to clear 7 feet (2.13 m) in 1956, and American John Thomas pushed the world mark to 2.23 m ( ft  in) in 1960. Valeriy Brumel took over the event for the next four years. The elegant Soviet jumper radically sped up his approach run, took the record up to 2.28 m ( ft  in), and won the Olympic gold medal in 1964, before a motorcycle accident ended his career.
American coaches, including two-time NCAA champion Frank Costello of the University of Maryland, flocked to Russia to learn from Brumel and his coaches. However, it would be a solitary innovator at Oregon State University, Dick Fosbury, who would bring the high jump into the next century. Taking advantage of the raised, softer landing areas by then in use, Fosbury added a new twist to the outmoded Eastern Cut-off. He directed himself over the bar head and shoulders first, sliding over on his back and landing in a fashion which would likely have broken his neck in the old, sawdust landing pits. After he used this Fosbury flop to win the 1968 Olympic gold medal, the technique began to spread around the world, and soon "floppers" were dominating international high jump competitions. The last straddler to set a world record was Vladimir Yashchenko, who cleared 2.33 m ( ft  in) in 1977 and then 2.35 m ( ft  in) indoors in 1978.
Among renowned high jumpers following Fosbury's lead were: Americans Dwight Stones and his rival, 5 ft tall Franklin Jacobs of Paterson, NJ, who cleared 2.32 m ( ft  in), 0.59 m over his head (a feat equaled 27 years later by Sweden's Stefan Holm); Chinese record-setters Ni-chi Chin and Zhu Jianhua; Germans Gerd Wessig and Dietmar Mögenburg; Swedish Olympic medalist and world record holder Patrik Sjöberg; and female jumpers Iolanda Balaş of Romania, Ulrike Meyfarth of Germany and Italy's Sara Simeoni.
Technical aspects.
The approach.
The approach of the high jump may actually be more important than the take off. If a high jumper runs with bad timing or without enough aggression, clearing a high bar becomes more of a challenge. The approach requires a certain shape or curve, the right amount of speed, and the correct number of strides. The approach angle is also critical for optimal height.
Most great straddle jumpers have a run at angles of about 30 to 40 degrees. The length of the run is determined by the speed of the person's approach. A slower run requires about 8 strides. However, a faster high jumper might need about 13 strides. A greater run speed allows a greater part of the body's forward momentum to be converted upward .
The J type approach, favored by Fosbury floppers, allows for horizontal speed, the ability to turn in the air (centripetal force), and good take-off position. The approach should be a hard controlled stride so that a person does not fall from creating an angle with speed. Athletes should run tall and lean from the ankles on the curve and not the hips. Unlike the "classic" straddle technique, where the take-off foot is "planted" in the same spot at every height, flop-style jumpers must adjust their take-off as the bar is raised. Their J approach run must be adjusted slightly so that their take-off spot is slightly further out from the bar in order to allow their hips to clear the bar while still maintaining enough momentum to carry their legs across the bar. Jumpers attempting to reach record heights commonly fail when most of their energy is directed into the vertical effort, and they brush the bar off the standards with the backs of their legs as they stall-out in mid-air.
Drills can be practiced to solidify the approach. One drill is to run in a straight line (the linear part of the approach) and then run two to three circles spiraling into one another. Another is to run or skip a circle of any size, two to three times in a row. It is important to train to leap upwards without first leaning into the bar, allowing the momentum of the J approach to carry the body across the bar.
Declaring the winner.
In competition the winner is the person who cleared the highest height. In case of a tie, fewer failed attempts at that height are better: "i.e.", the jumper who makes a height on his/her first attempt is placed ahead of someone who clears the same height on the second or third attempt. If there still is a tie here, all the failed attempts at lower heights are added up, the one with the fewest number of total misses is declared the winner. If still tied a playoff is held. Starting height is the next larger height after the overjumped one. If all the competitors clear the height, the bar is raised 2 cm, and if they fail, the bar is lowered 2 cm. That continues until only one competitor succeeds in overjumping that height, he or she is declared the winner.
Athletes with most medals.
Athletes who have won multiple titles at the two most important competitions, the Olympic Games and the World Championships:
Kostadinova and Sotomayor are the only high jumpers to have been Olympic Champion, World Champion and broken the world record.
Height differentials.
All time lists of athletes with the highest recorded jumps above their own height.
Female two metres club.
s of 2014[ [update]], 65 different female athletes had ever been able to jump 2.00 m ( ft  in). The following table shows the only ten countries from which more than one athlete has cleared that mark.
National records.
Updated May 2015.

</doc>
