<doc id="18131" url="http://en.wikipedia.org/wiki?curid=18131" title="Los Angeles International Airport">
Los Angeles International Airport

Los Angeles International Airport (IATA: LAX, ICAO: KLAX, FAA LID: LAX) is the primary airport serving the Greater Los Angeles Area, the second-most populated metropolitan area in the United States. It is most often referred to by its IATA airport code LAX, with the letters pronounced individually. LAX is located in southwestern Los Angeles along the Pacific coast in the neighborhood of Westchester, 16 mi from Downtown Los Angeles. It is owned and operated by Los Angeles World Airports, an agency of the Los Angeles city government formerly known as the Department of Airports.
In 2014, LAX handled 70,622,212 passengers, far exceeding a previous record of 67.3 million travelers set in 2000. The airport holds the claim for "the world's busiest origin and destination (O & D) airport" in 2011, meaning it had the most non-connecting passengers, and the third busiest in the world by aircraft movements. It is also the only airport to rank among the top five U.S. airports for both passenger and cargo traffic.
While LAX is the busiest airport in the Greater Los Angeles Area, other airports, including Bob Hope Airport, John Wayne Airport, Long Beach Airport, and LA/Ontario International Airport, also serve the region. It is also notable for being one of few U.S. airports with four parallel runways.
LAX serves as a hub for American Airlines, Delta Air Lines, United Airlines, Alaska Airlines, Horizon Air, Great Lakes Airlines, and Virgin America. The airport also serves as a focus city for Southwest Airlines, Allegiant Air, Air New Zealand, Qantas, and Volaris. LAX is the only airport to serve as a hub for all three U.S. legacy airlines (American, Delta and United). LAX is also one of three airports in North America that will have scheduled direct air service to all six inhabited continents once Ethiopian Airlines starts flights in June. The other two airports are Toronto-Pearson and New York John F. Kennedy Airport.
History.
In 1928, the Los Angeles City Council selected 640 acre in the southern part of Westchester for a new airport for the city. The fields of wheat, barley and lima beans were converted into dirt landing strips without any terminal buildings. It was named Mines Field for William W. Mines, the real estate agent who arranged the deal. The first structure, Hangar No. 1, was erected in 1929 and is in the National Register of Historic Places.
Mines Field opened as the airport of Los Angeles in 1930 and the city purchased it to be a municipal airfield in 1937. The name became Los Angeles Airport in 1941 and Los Angeles International Airport in 1949. In the 1930s the main airline airports were Burbank Airport (then known as Union Air Terminal, and later Lockheed) in Burbank and the Grand Central Airport in Glendale. (In 1940 the airlines were all at Burbank except for Mexicana's three departures a week from Glendale; in late 1946 most airline flights moved to LAX, but Burbank always retained a few.)
Mines Field did not extend west of Sepulveda Boulevard; Sepulveda was rerouted circa 1950 to loop around the west ends of the extended east–west runways (now runways 25L and 25R), which by November 1950 were 6000 ft long. A tunnel was completed in 1953 allowing Sepulveda Boulevard to revert to straight and pass beneath the two runways; it was the first tunnel of its kind. For the next few years the two runways were 8500 ft long.
On July 10, 1956, Boeing's 707 prototype (the 367–80) visited LAX. The "Los Angeles Times" said it was its first appearance at a "commercial airport" outside the Seattle area.
The April 1957 Official Airline Guide showed 66 weekday departures on United Airlines, 32 American Airlines, 32 Western Airlines, 27 TWA, nine Southwest, five Bonanza Air Lines and three Mexicana Airlines; also 22 flights a week on Pan American World Airways and five a week on Scandinavian Airlines (the only direct flights from California to Europe).
American Airlines' 707-123s flew the first jet passengers out of LAX to New York in January 1959; the first wide-body jets were TWA's Boeing 747s to New York in early 1970.
In 1958, the architecture firm Pereira & Luckman was contracted to plan the re-design of the airport for the "jet age". The plan, developed with architects Welton Becket and Paul Williams, called for a series of terminals and parking structures in the central portion of the property, with these buildings connected at the center by a huge steel-and-glass dome. The plan was never realized, and the Theme Building was built on the site intended for the dome.
In the new terminal area west of Sepulveda Blvd that started opening in 1961, each terminal had a satellite building out in the middle of the tarmac, reached by underground tunnels from the ticketing area. United's satellites 7 and 8 were first to open, followed by 3, 4 and 5; satellite 2 opened as the international terminal several months later and satellite 6 was to be the last to open.
Since the 1920s, a neighborhood called Surfridge had been on the coastline west of the airport, part of the larger community of Palisades del Rey along with the neighborhood to the north now known as Playa del Rey. When the airlines switched to jet airliners during the 1960s and 1970s and Surfridge's residents complained about noise pollution, the city used its eminent domain powers to condemn and evacuate Surfridge. The government bulldozed the homes but did not bulldoze the streets, and the fenced-off "ghost" streets west of LAX are still there.
In 1981, LAX began a $700 million expansion in preparation for the 1984 Summer Olympics. The U-shaped roadway past the terminal entrances got a second level, with arriving passengers on the lower level and departing on the upper. Connector buildings between the ticketing areas and the satellite buildings were added, changing the layout to a "pier" design. Two new terminals (Terminal 1 and the Tom Bradley International Terminal) were built and Terminal 2, then two decades old, was rebuilt. Multi-story parking structures were also built in the center of the airport.
On July 8, 1982, groundbreaking for the two new terminals were conducted by Mayor Tom Bradley and World War II aviator General James Doolittle. The $123 million 963000 sqft International Terminal opened on June 11, 1984, and was named for Bradley.
On April 29, 1992, the airport closed for violence and cleanup after the 1992 Los Angeles Riots over the Rodney King beating.
The airport closed again on January 17, 1994 after the Northridge earthquake.
In 1996, a $29 million, 277 ft air traffic control tower was built near the Theme Building.
In 2000, before Los Angeles hosted the Democratic National Convention, fifteen glass pylons up to ten stories high were placed in a circle around the intersection of Sepulveda Boulevard and Century Boulevard, with more pylons of decreasing height following Century Boulevard eastward, evoking a sense of departure and arrival. Conceived by the designers at Selbert Perkins Design, the towers and 30 ft "LAX" letters are a gateway to the airport and offer a welcoming landmark for visitors. Illuminated from the inside, the pylons slowly cycle through a rainbow of colors that represents the multicultural makeup of Los Angeles and can be customized to celebrate events, holidays or a season. This was part of an overall face-lift that included new signage and various other cosmetic enhancements that was led by Ted Tokio Tanaka Architects. The LAX pylons underwent improvements in 2006, as stage lighting inside the cylinders was replaced with LED lights to conserve energy, make maintenance easier and enable on-demand cycling through various color effects.
LAX has been a hub for TWA, Air California, Braniff International, Continental Airlines, Delta Air Lines, Pacific Southwest Airlines, US Airways, Western Airlines, and the Flying Tiger Line.
Starting in the mid-1990s, under Mayors Richard Riordan and James Hahn, modernization and expansion plans for LAX were prepared, only to be stymied by a coalition of residents who live near the airport. They cited increased noise, pollution and traffic impacts of the project. In late 2005, newly elected Mayor Antonio Villaraigosa was able to reach a compromise, allowing some modernization to go forward while encouraging future growth among other facilities in the region.
It is illegal to limit the number of passengers that use an airport, but in December 2005 the city agreed to limit the passenger gates to 163. Once passenger usage hits 75 million, a maximum of two gates a year for up to five years will be closed, intending to limit growth to 79 million passengers a year. In exchange civil lawsuits were abandoned, to allow the city to complete badly needed improvements to the airport.
The airport is a hub for United Airlines, Alaska Airlines, American Airlines, and a focus city for Southwest Airlines, Allegiant Air, Air New Zealand, Qantas, Virgin America and Volaris. It also serves as an international gateway and hub for Delta Air Lines and houses a line maintenance facility for Delta's primary maintenance, repair and overhaul arm, Delta TechOps.
In 2008 plans were unveiled for a $4.11 billion renovation and improvement program to expand and rehabilitate the Tom Bradley International Terminal to accommodate the next generation of larger aircraft, as well as handle the growing number of flights to and from the Southern California region, and to develop the Central Terminal Area (CTA) of the airport to include streamlined passenger processing, public transportation and updated central utility plants. As of 2013, Los Angeles International Airport is the biggest airport in California. The multi-year project, originally projected to be completed in 2014, is ongoing as of February 2015, and is the largest public works project in Los Angeles history.
The "X" in LAX.
Before the 1930s, existing airports used a two-letter abbreviation based on the weather stations at the airports. At that time, "LA" served as the designation for Los Angeles Airport. But with the rapid growth in the aviation industry the designations expanded to three letters c. 1947, and "LA" became "LAX." The letter "X" has no specific meaning in this identifier. "LAX" is also used for the Port of Los Angeles in San Pedro and by Amtrak for Union Station in downtown Los Angeles.
Aircraft spotting.
The "Imperial Hill" area (also known as Clutter's Park) in El Segundo is a prime location for aircraft spotting. Another popular spotting location sits under the final approach for runways 24 L&R on a lawn next to the Westchester In-N-Out Burger on Sepulveda Boulevard. This is one of the few remaining locations in Southern California from which spotters may watch such a wide variety of low-flying commercial airliners from directly underneath a flight path.
Space Shuttle "Endeavour".
At 12:51 pm on Friday, September 21, 2012, a Shuttle Carrier Aircraft carrying the Space Shuttle "Endeavour" landed at LAX on runway 25L. An estimate of over 10,000 people saw the shuttle land in person. Interstate 105 was backed up for miles at a standstill. Imperial Highway was shut down for spectators. Along the way the shuttle passed many landmarks in the Los Angeles area including the Santa Monica Pier, Getty Center, NASA's Jet Propulsion Laboratory (JPL), Griffith Observatory, Malibu coastline, the Hollywood Sign, Universal Studios Hollywood, the Disneyland Resort, and Los Angeles City Hall. It was quickly taken off the Boeing 747 and was moved to a United Airlines hangar. The shuttle spent about a month in the hangar while it was prepared for transport to the California Science Center.
On Friday, October 12, "Endeavour" left the hangar at 2:00 a.m. Pacific Daylight Time and moved eastward on Manchester Boulevard on its way to the California Science Center. The shuttle transport vehicle (STV) was constructed of over 60 individual wheels and weighed over 12000 lb. It was designed to move the shuttle at a speed of 2 mph. This was both for safety concerns for the shuttle and so people could take many photos. As the entire planned route was only 12 miles, it should have only taken 6 hours to complete. Instead, the shuttle arrived in one piece on the morning of Sunday, October 14, 2012.
Theme Building.
The distinctive white googie "Theme Building", designed by Pereira & Luckman architect Paul Williams and constructed in 1961 by Robert E. McKee Construction Co., resembles a flying saucer that has landed on its four legs. A restaurant with a sweeping view of the airport is suspended beneath two arches that form the legs. The Los Angeles City Council designated the building a Los Angeles Historic-Cultural Monument in 1992. A $4 million renovation, with retro-futuristic interior and electric lighting designed by Walt Disney Imagineering, was completed before the "Encounter Restaurant" opened there in 1997. Visitors are able to take the elevator up to the roof of the "Theme Building", which closed after the September 11, 2001 attacks for security reasons and reopened to the public on weekends beginning on July 10, 2010. Additionally, a memorial to the victims of the 9/11 attacks is located on the grounds, as three of the four hijacked planes were originally destined for LAX.
Terminals.
LAX has nine passenger terminals arranged in the shape of the letter U or a horseshoe. The terminals are served by a shuttle bus. Terminals 4, 5, 6, 7, and 8 are all connected airside via an underground tunnel between Terminals 4, 5 and 6 and above-ground walkways between Terminals 6, 7, and 8. An additional airside shuttle bus operates among Terminals 4, 6, and the American Eagle remote terminal. There are no physical airside connections between any of the other terminals.
In addition to these terminals, there are 2000000 sqft of cargo facilities at LAX, and a heliport operated by Bravo Aviation. Qantas has a maintenance facility at LAX, even though it is not a hub.
Some inter-terminal connections require passengers to exit security, then walk or use a shuttle-bus to get to the other terminal, then re-clear security. A few LAX terminals provide airside connections, which allow connecting passengers to access other terminals without having to re-clear security. The following airside connections are possible:
In 2016 an Airside corridor will be available from Terminal 4 to the Tom Bradley International Terminal. This would allow airside connections from Terminals 8, 7, 6, 5 and 4 to the Tom Bradley International Terminal. A TSA screening area will be available in this connector to allow passengers to enter Terminal 4 after arriving on an international arrival in the Tom Bradley Terminal avoiding the main Terminal 4 security screening area, also allowing easier connections to Terminal 5, 6, 7 and 8.
Terminal 1.
Terminal 1 has 15 gates: Gates 1–3, 4A–4B, and 5–14, and houses Southwest Airlines. Terminal 1 was built in 1984. Terminal 1 is presently undergoing an extensive renovation financed by Southwest Airlines. This renovation will continue through 2018 and provide updates to security screening area, curbside dropoff, terminal areas and baggage handling.
Terminal 2.
Terminal 2 has 11 gates: Gates 21–21B, 22–22B, 23, 24–24B, and 25–28. It hosts most foreign airlines not using the Tom Bradley International Terminal along with a couple of domestic airlines: Aeroméxico, Air Canada, Air China, Avianca, Hawaiian Airlines, Sun Country Airlines, Virgin Atlantic, Volaris, and WestJet. Air China will move to the Tom Bradley International Terminal on July 1, 2015.
Terminal 2 was built in 1962, and was the original international terminal. It was completely torn down and rebuilt in stages between 1984 and 1988 at a cost of $94 million. The rebuilt terminal was designed by Leo A Daly. Terminal 2 has CBP (Customs and Border Protection) facilities to process arriving international passengers.
Former tenants of the terminal include Air New Zealand, Northwest Airlines, Pan American World Airways, and TACA International Airlines. Air New Zealand moved to the Tom Bradley International Terminal on December 3, 2014.
Terminal 3.
Terminal 3 has 12 gates: Gates 30, 31A–31B, 32, 33A–33B, 34–36, 37A–37B, and 38 (gate 39 was removed to make room for Virgin Australia Boeing 777 operations at gate 38). Terminal 3 opened in 1961 and was Trans World Airlines' terminal. The terminal was expanded in 1970 to accommodate widebody operations and between 1980 and 1987, which included a new passenger connector building and baggage system connected to the original satellite. It formerly housed some American Airlines flights after that airline acquired Reno Air and TWA in 1999 and 2001, respectively. Eventually, all American flights were moved to Terminal 4. As of November 2014, Allegiant Air, Frontier Airlines, JetBlue Airways, Spirit Airlines, and Virgin America use Terminal 3. US Airways moved operations to Terminal 6 on November 6, 2014.
Virgin America uses Terminal 2 for arrivals from Cancun. Virgin Australia check-in is in Terminal 3 and all departures and arrivals are at TBIT.
Tom Bradley International Terminal.
Tom Bradley International Terminal
The Tom Bradley International Terminal (TBIT) has 18 gates; nine on the north concourse and nine on the south concourse. Each gate is equipped with a Safegate Advanced - Visual Docking Guidance System. In addition, there are nine satellite gates for international flights located on the west side of LAX. Passengers are ferried to the west side gates by bus. The terminal exclusively hosts most of the major international airlines, with the exception of those housed in Terminal 2.
This terminal opened for the 1984 Summer Olympic Games and is named for Tom Bradley, the first African-American and longest-serving (20 years) mayor of Los Angeles, and champion of LAX. The terminal is located at the west end of the passenger terminal area between Terminals 3 and 4. Tom Bradley International Terminal hosts 27 airlines and handles 10 million passengers per year.
In 2010, modernization efforts resulted in additional space for inline baggage screening, three large alliance-aligned lounges plus one unaffiliated lounge (to replace the multiple airline specific lounges) and cosmetic upgrades in the departures and arrivals areas.
On November 17, 2008, Mayor Antonio Villaraigosa unveiled design concepts for LAX's Bradley West and Midfield Concourse projects. Los Angeles World Airports (LAWA), along with city officials, selected Fentress Architects in association with HNTB to develop a design concept for the modernization of LAX. The emphasis of the modernization is to improve the passenger experience and to keep Los Angeles competitive with other global cities.
On February 22, 2010, construction began on the $1.5 billion Bradley West project, part of the multi-year $4.11 billion LAX improvement and redevelopment projects. The project added over 1250000 sqft of shops, restaurants, and passenger lounges, as well as new security screening, customs, immigration, and baggage claim facilities. The terminal's existing two concourses will be demolished and replaced with a larger pair with 18 gates, nine of which will be able to accommodate the Airbus A380. The terminal opened in phases beginning on September 2012, and was completed in 2014.
On September 18, 2013, the new Tom Bradley International Terminal at LAX officially opened. Airlines started to use the new, smarter gates that can handle larger aircraft, with multiple gates configured for the Airbus A380. Inside, Southern California scenes and eccentric videos are played on seven huge multimedia screens. It is the largest immersive system in an airport and the most advanced multimedia environment in a North American airport. This system was created by Moment Factory. Lounge-like seating for relaxation, and the 25,000-square-foot duty-free shopping area are located in the Great Hall. Many restaurants and high-end shops are located there, such as Chloé, Burberry, Fred Segal, and Porsche amongst others. Franchises of popular LA restaurants including Umami Burger, 800 Degrees, Larder, Ink.Sack and Lucky Fish are among the restaurant selections available.
Accommodating the Airbus A380.
On March 25, 2007, Runway 7R/25L reopened after being shifted 55 ft south to prevent runway incursions and prepare the runway for the Airbus A380. Additional storm drains and enhanced runway lighting were added. Runway 25L is now 800 ft south of the parallel runway centerline to centerline, allowing a parallel taxiway between the runways; the taxiway was completed in 2008.
On September 18, 2006, Los Angeles World Airports started a $503 million facelift of the Tom Bradley International Terminal. Improvements included new paging, air conditioning and electrical systems, along with new elevators, escalators, baggage carousels, and a digital sign that automatically update flight information. With federal funding, explosives-detection technology was incorporated into the terminal's underground baggage system.
According to the "Los Angeles Times", in February 2007, many Pacific Rim carriers began reducing flights to LAX in favor of more modern airports, such as San Francisco International Airport, due to the aging Tom Bradley International Terminal.
On August 15, 2007, the Los Angeles City Council approved a $1.2 billion project to construct a new 10-gate terminal to handle international flights using the Airbus A380. Adding the first new gates built since the early 1980s, the new structure was to be built directly west of the Tom Bradley International Terminal on a site that was occupied mostly by aircraft hangars.
On March 19, 2007, the Airbus A380 made its debut at LAX, landing on runway 24L. Though LAX was originally to be the first U.S. city to see the A380, Airbus later decided to forgo LAX in favor of John F. Kennedy International Airport in New York City. After city officials fought for the super-jumbo jet to land at LAX, Airbus had to get two A380, where the A380 landed simultaneously in New York and LAX.
On March 31, 2008, the "Los Angeles Times" reported that foreign carriers were once again flocking to LAX's Tom Bradley International Terminal. The weaker dollar caused a surge in demand for U.S. travel, resulting in airlines either adding new destinations or increasing frequencies to existing routes. New airlines that introduced flights to LAX included Virgin Australia, Emirates Airlines, Turkish Airlines, and Iberia Airlines. Korean Air, Qantas, Air China, and Air France have all augmented their services to Los Angeles by adding new flights to existing routes. The influx of new flights comes amidst the renovation of the airport and consolidates LAX's status as the premiere international gateway to the Western United States.
Qantas launched service with the Airbus A380 on October 20, 2008, using the west side remote gates. Though initially deployed between LAX and Sydney, Qantas' A380 service was extended to the LAX-Melbourne route. This was followed shortly by Korean Air, which initiated nonstop A380 flights to Seoul-Incheon in October 2011. Air France has launched A380 flights between Paris-Charles de Gaulle and Los Angeles in May 2012. In addition, China Southern Airlines launched A380 service to Guangzhou in October 2012, representing an increase in capacity of 78% on the route. With the addition of these services, LAX boasted six daily A380 services. On October 15, British Airways' nonstop service from London Heathrow to LAX also became an A380 route.
Asiana Airlines launched an Airbus A380 service to LAX on Wednesday, August 20, 2014. This new A380 route makes Asiana the eighth A380 operator at LAX and the eleventh airline to operate an A380. Previously, Asiana deployed regional Asian A380 routes to Osaka, Hong Kong, and Bangkok.
s of 29 2015[ [update]], LAX has more A380 services than any other North American city, with twelve daily flights spread among nine operators: Emirates (Dubai), China Southern Airlines (Guangzhou), British Airways (London), Qantas (Sydney and Melbourne), Korean Air and Asiana Airlines (Seoul), Air France (Paris), Singapore Airlines (Singapore via Tokyo), and Lufthansa (Frankfurt).
Terminal 4.
Terminal 4 has 14 gates: Gates 40–41, 42A–42B, 43–45 (Gate 44 is for the bus to the American Eagle remote terminal, Gates 44A-44J), 46A–46B, 47A–47B, 48A–48B, and 49A, and houses American Airlines flights. Terminal 4 was built in 1961, was expanded in 1983 by adding a connector from the ticketing areas to the original satellite, and was renovated in 2002 at a cost of $400 million in order to improve the appearance and functionality of the facility. The renovation was designed by Rivers & Christian. An international arrivals facility was also added as part of the renovations.
American Eagle regional flights operate from the "American Eagle Terminal", a satellite terminal which is located just east of Terminal 8. Gate 44 serves as the shuttle bus stop at Terminal 4. The remote terminal is also connected by shuttle buses to Terminals 6 and TBIT because of Eagle's codesharing with Alaska Airlines, Qantas and US Airways.
Terminal 5.
Terminal 5 has 15 gates: Gates 50A–50B, 51A–51B, 52A–52B, 53A–53B, 54A–54B, 55A, 56–57, 58, and 59, and is used as Delta Air Lines' West Coast hub. Western Airlines occupied this terminal at its opening in 1962, and continued to do so until Western was merged with Delta on April 1, 1987. Terminal 5 was redesigned by Gensler, expanded to include a connector building between the original satellite and the ticketing facilities and remodeled from 1986 through early 1988. It was unofficially named 'Delta's Oasis at LAX' with the slogan 'Take Five at LAX' when construction was completed in the summer of 1988. Northwest Airlines moved all operations to Terminal 5 and Terminal 6 alongside Delta on June 30, 2009, as part of its merger with the airline. The terminal has a customs area in the arrivals floor, used for international flights served by Delta Air Lines.
Terminal 6.
Terminal 6 has 14 gates: Gates 60–63, 64A–64B, 65A-65B, 66, 67, 68A–68B, and 69A–69B. Parts of this terminal have changed little from its opening in 1961; in 1970, new gates were expanded from the main building, as is obvious from the rotunda at the end. Four of these gates have two jetways, which can accommodate large aircraft. An expansion of the terminal, including a connector of the original satellite to the ticketing areas, was completed in 1987.
Presently in use by Alaska Airlines, American Airlines (for US Airways operated flights), Copa and Great Lakes.
Continental Airlines originally built the Connector Building (which links the Ticketing and rotunda buildings). Prior to October 2014, United Airlines used the connector gates, supplementing its base at Terminal 7. Delta also leases some space from the Airport in Terminal 6, in addition to its base at Terminal 5. Most of the rotunda gates can feed arriving passengers into a sterile corridor that shunts them to Terminal 7's customs and immigration facility.
In April 2011, Alaska Airlines agreed to a deal with Los Angeles World Airports to renovate Terminal 6 and build an Alaska Airlines Board Room Lounge. The airline moved its flights to Terminal 6 on March 20, 2012, and Spirit Airlines was relocated to Terminal 3. US Airways moved operations to Terminal 6 on November 6, 2014.
Former tenants of the terminal include Continental Airlines until its merger with United in 2011 and Eastern Air Lines, which went bankrupt in 1991. The terminal also originally housed Pacific Southwest Airlines.
Terminal 7.
Terminal 7 has 11 gates: Gates 70A-70B, 71A-71B, 72, 73, 74, 75A-75B, 76, and 77. This terminal opened in 1962 and was expanded to accommodate widebody aircraft in 1970. The terminal was expanded in 1982 with the addition of a connector building, which today compromises of gates 70A-70B and 71A-71B. Four of these gates have two jetways, which accommodate large aircraft. Terminal 7 is used for United Airlines' domestic and international operations. The interior of the terminal was renovated between January 1998 and June 1999 at a cost of $250 million, was designed by HNTB, and was constructed by Hensel Phelps Construction. Added were new gate podiums, increased size of gate areas, relocated concessions, expanded restrooms, new flooring, and new signage. Also, the roof of the terminal was raised, and new, brighter light fixtures were added in order to provide more overall lighting. As of 2012, Terminal 7 is undergoing another facelift, with significant changes to concessions. The terminal also contains a United Club and an International First Class Lounge. The terminal has a customs area located on the arrivals floor, used by international flights served by Alaska Airlines and United Airlines.
Terminal 8.
Terminal 8 has nine gates: Gates 80–88. This terminal was originally constructed in 1961 as Concourse 8 and was redeveloped in 1982 and renamed Terminal 8. The terminal formerly served Shuttle by United flights. While at one point, United solely operated United Express flights from Terminal 8 and operated non-Express flights from Terminals 6 and 7, Terminal 8 is now used once again for some mainline flights.
Airlines and destinations.
LAX connects 87 domestic and 69 international destinations in North America, Latin America, Europe, the Middle East, Asia, and Oceania. Its most prominent airlines are American Airlines (15.71% of passenger traffic), Delta Air Lines (15.69%), and United Airlines (15.11%). Other airlines with a presence on a lesser scale include Southwest Airlines (11.10%) and Alaska Airlines (5.11%).
Passenger.
United Airlines/United Express operate the most departures from the airport, followed by American Airlines/American Eagle and Southwest Airlines. United also operates to the most destinations, followed by American and Alaska Airlines/Horizon. Delta, Qantas and United all operate nonstop services to the most transpacific destinations (three). Norwegian Air Shuttle (Operated by Norwegian Long Haul) serves the most destinations in Europe (three), while Alaska Airlines/Horizon serve the most destinations in Mexico (nine).
This table lists passenger flights served with a nonstop or direct flight with no change of aircraft carrying passengers originating in Los Angeles according to the airlines' published schedules, unless otherwise noted.
Traffic and statistics.
LAX handles more "origin and destination" (i.e. not connecting) passengers than any other airport in the world.
The airport handled 28,861,477 enplanements, the total number of passengers boarding an aircraft, in 2008. This makes LAX the third busiest airport in the United States in terms of enplanements.
It is the world's fifth-busiest airport by passenger traffic and fifteenth-busiest by cargo traffic, serving over 70.6 million passengers and 2 million tons of freight and mail in 2014. It is the busiest airport in the state of California, and the second-busiest airport by passenger boardings in the United States, based on final 2013 statistics.
In terms of international passengers, as of 2012, LAX is the third busiest in the United States. (behind JFK in New York City and MIA in Miami) and, as of 2006, 26th worldwide.
The number of aircraft operations (landings and takeoffs) has steadily increased to 636,706 in 2014, up from 614,917 in 2013, according to the Federal Aviation Administration. The Airports Council International places LAX at third most aircraft movements in the world, as of 2013.
Ground transportation.
Freeways and roads.
LAX's terminals are immediately west of the interchange between Century Boulevard and Sepulveda Boulevard (State Route 1). The 405 Freeway can be reached to the east via Century Boulevard, and the 105 Freeway can be reached to the south via Sepulveda Boulevard.
Bus.
The closest bus stops to the terminals are the pair of opposites on Sepulveda and Century, served by Torrance 8, Metro 117, Metro 232, Commuter Express 574 and Metro 40 (owl service only).
In addition, out of a number of bus systems, many routes (local, rapid and express) of the LACMTA Metro 232 to Long Beach, Line 8 of Torrance Transit, Line 109 of Beach Cities Transit, the Santa Monica Big Blue Bus system's Line 3 and Rapid 3 via Lincoln Blvd to Santa Monica and the Culver CityBus's Line 6 and Rapid 6 via Sepulveda Blvd to Culver City and UCLA all make stops at the LAX Transit Center in Parking Lot C. on 96th St., where shuttle bus "C" offers free connections to and from every LAX terminal, and at the Green Line Station, where shuttle bus "G" connects to and from the terminals.
FlyAway Bus.
The FlyAway Bus is a motorcoach shuttle service run by the LAWA, which currently travels between one of three off-airport areas: San Fernando Valley (Van Nuys), downtown Los Angeles (Union Station), and the Westside (Westwood). The Irvine FlyAway was discontinued on August 31, 2012. The shuttle service stops at every LAX terminal. The service hours vary based on the line. All lines use the regional system of High Occupancy Vehicle lanes to expedite their trips.
Metro Rail.
Shuttle bus "G" offers a free connection to and from the Aviation/LAX station on the Los Angeles Metro Rail Green Line. The line was originally intended to connect directly to the airport terminals, but budgetary restraints and opposition from local taxi and parking lot owners impeded its progress and won.
Airport Metro Connector.
Part of the long term master plan for LAX and the Los Angeles County Metropolitan Transportation Authority calls for a direct transit Metro Rail infill station on the under constructed Crenshaw/LAX Line to connect to an Automated people mover system called the Airport Metro Connector, connecting terminals 1-8 to the metro stations, it will decrease the need for shuttle bus services. Construction is planned to start in 2017 and to be completed by early 2024. Currently, shuttle bus "G" runs every 10–15 minutes (synched with the train schedule) from 5 am – 1:30 am.
Taxis and private shuttles.
Taxicab services are operated by nine city-authorized taxi companies and regulated by Authorized Taxicab Supervision Inc. (ATS). ATS maintains a taxicab holding lot under the 96th Street Bridge where, at peak periods, hundreds of cabs queue up to wait their turn to pull into the central terminal area to pick up passengers. A number of private shuttle companies also offer limousine and bus services to LAX Airport.
Coast Guard Air Station Los Angeles.
The airport also functions as a joint civil-military facility, providing a base for the United States Coast Guard and its Coast Guard Air Station Los Angeles facility, operating four HH-65 Dolphin helicopters, which covers Coast Guard operations in various Southern California locations, including Catalina Island. Missions include search and rescue (SAR), law enforcement, aids to navigation support (such as operating lighthouses) and various military operations. In addition, Coast Guard helicopters assigned to the air station deploy to Coast Guard cutters.
The Coast Guard is planning to close Coast Guard Air Station Los Angeles and move its aircraft and personnel to Naval Air Station Point Mugu, part of Naval Base Ventura County in Oxnard, California, when the lease on the existing facility ends in 2016.
Flight Path Learning Center & Museum.
The Flight Path Learning Center is a museum located at 6661 Imperial Highway and was formerly known as the "West Imperial Terminal". This building used to house some charter flights (Condor Airlines) and regular scheduled flights by MGM Grand Air. It sat empty for 10 years until it was re-opened as a learning center for LAX.
The center contains information on the history of aviation, several pictures of the airport, as well as aircraft scale models, flight attendant uniforms, and general airline memorabilia such as playing cards, china, magazines, signs, even a TWA gate information sign. The museum also offers school tours and a guest speaker program.
The museum's library contains an extensive collection of rare items such as aircraft manufacturer company newsletters/magazines, technical manuals for both military and civilian aircraft, industry magazines dating back to World War II and before, historic photographs and other invaluable references on aircraft operation and manufacturing.
The museum has on display "The Spirit of Seventy-Six," which is a DC-3 (DC-3-262, Serial No. 3269). After being in commercial airline service, the plane served as a corporate aircraft for Union Oil Company for 32 years. The plane was built in the Douglas Aircraft Company plant in Santa Monica in January 1941, which was a major producer of both commercial and military aircraft.
The museum claims to be "the only aviation museum and research center situated at a major airport and the only facility with a primary emphasis on contributions of civil aviation to the history and development of Southern California". There are other museums at major airports, however, including the Udvar-Hazy Center of the National Air and Space Museum adjacent to Washington Dulles Airport, the Royal Thai Air Force Museum at Don Muang Airport, the Suomen ilmailumuseo (Finnish Aviation Museum) at Helsinki-Vantaa Airport, the Frontier of Flight Museum at Dallas Love Field, the Tulsa Air and Space Museum & Planetarium at Tulsa International Airport and others.
Other facilities.
The airport has the administrative offices of Los Angeles World Airports.
Continental Airlines once had its corporate headquarters on the airport property. At a 1962 press conference in the office of Mayor of Los Angeles Sam Yorty, Continental Airlines announced that it planned to move its headquarters to Los Angeles in July 1963. In 1963 Continental's headquarters moved to a two-story, $2.3 million building on the grounds of the airport. The July 2009 "Continental Magazine" issue stated that the move "underlined Continental's western and Pacific orientation". On July 1, 1983 the airline's headquarters were relocated to the America Tower in the Neartown area of Houston.
In addition to Continental, Western Airlines and Flying Tiger Line also had their headquarters on the LAX property.
Incidents and accidents.
During its history there have been numerous incidents, but only the most notable are summarized below:
Planned modernization.
LAWA currently has several plans to modernize LAX. These include terminal and runway improvements, which will enhance the passenger experience, reduce overcrowding, and provide airport access to the latest class of very large passenger aircraft.
These improvements include:
LAWA is also planning to build and operate an automated people mover. This small train will include three stations in the central terminal area and three outside east of the terminals at a new intermodal transportation facility, connecting passengers between the central terminal area and the Metro Green Line, the future Metro Crenshaw Line, and regional and local bus lines and a consolidated car rental facility.
In popular culture.
Numerous films and television shows have been set or filmed partially at LAX, at least partly due to the airport's proximity to Hollywood studios. Film shoots at the Los Angeles airports, including LAX, produced $590 million for the Los Angeles region from 2002 to 2005.

</doc>
<doc id="18133" url="http://en.wikipedia.org/wiki?curid=18133" title="La Tène culture">
La Tène culture

The La Tène culture was a European Iron Age culture named after the archaeological site of La Tène on the north side of Lake Neuchâtel in Switzerland, where a rich cache of artifacts was discovered by Hansli Kopp in 1857.
La Tène culture developed and flourished during the late Iron Age (from 450 BCE to the Roman conquest in the 1st century BCE) in Belgium, eastern France, Switzerland, Austria, Southern Germany, the Czech Republic, Poland, Slovakia, Slovenia, Hungary and Romania. To the north extended the contemporary Jastorf culture of Northern Germany.
La Tène culture developed out of the early Iron Age Hallstatt culture without any definite cultural break, under the impetus of considerable Mediterranean influence from the Culture of Golasecca, the Greeks in pre-Roman Gaul and the Etruscans. Barry Cunliffe notes localization of La Tène culture during the 5th century when there arose "two zones of power and innovation: a Marne – Moselle zone in the west with trading links to the Po Valley via the central Alpine passes and the Golasecca culture, and a Bohemian zone in the east with separate links to the Adriatic via the eastern Alpine routes and the Venetic culture". A shift of settlement centres took place in the 4th century.
La Tène cultural material appeared over a large area, including parts of Ireland and Great Britain, northern Spain, Burgundy, and Austria. Elaborate burials also reveal a wide network of trade. In Vix, France, an elite woman of the 6th century BCE was buried with a very large bronze cauldron made in Greece. Exports from La Tène cultural areas to the Mediterranean cultures were based on salt, tin and copper, amber, wool and leather, furs and gold.
La Tène "homeland".
Though there is no agreement on the precise region in which La Tène culture first developed, there is a broad consensus that the center of the culture lay on the northwest edges of Hallstatt culture, north of the Alps, within the region between the valleys of the Marne and Moselle in the west and modern Bavaria and Austria in the east. In 1994 a prototypical ensemble of elite grave sites of the early 5th century BCE was excavated at Glauberg in Hesse, northeast of Frankfurt-am-Main, in a region that had formerly been considered peripheral to the La Tène sphere.
From their homeland, La Tène groups expanded in the 4th century to Hispania, the Po Valley, the Balkans, and even as far as Asia Minor, in the course of several major migrations. In the 4th century BCE, a Gallic army led by Brennus reached Rome and took the city. In the 3rd century BCE, Gallic bands entered Greece and threatened the oracle of Delphi, while another band settled Galatia in Asia Minor.
Periodization.
Extensive contacts through trade are recognized in foreign objects deposited in elite burials; stylistic influences on La Tène material culture can be recognized in Etruscan, Italic, Greek, Dacian and Scythian sources. Dateable Greek pottery at La Tène sites and dendrochronology and thermoluminescence help provide date ranges in an absolute chronology at some La Tène sites.
As with many archaeological periods, La Tène history was originally divided into "early" (6th century BCE), "middle" (c. 450–100 BCE), and "late" (1st century BCE) stages, with the Roman occupation effectively driving the culture underground and ending its development. A broad cultural unity was not paralleled by overarching social-political unifying structures, and the extent to which the material culture can be linguistically linked is debated.
Ethnology.
Our knowledge of this cultural area derives from three sources: from archaeological evidence, from Greek and Latin literary evidence, and more controversially, from ethnographical evidence suggesting some La Tène artistic and cultural survivals in traditionally Celtic regions of far western Europe. Some of the societies that are archaeologically identified with La Tène material culture were identified by Greek and Roman authors from the 5th century onwards as "Keltoi" ("Celts") and "Galli" ("Gauls"). Herodotus (iv.49) correctly placed "Keltoi" at the source of the Ister/Danube, in the heartland of La Tène material culture: "The Ister flows right across Europe, rising in the country of the Celts", whom however, apparently misunderstanding his source, he places "farthest to the west of any people of Europe"
Whether the usage of classical sources means that the whole of La Tène culture can be attributed to a unified Celtic people is difficult to assess; archaeologists have repeatedly concluded that language, material culture, and political affiliation do not necessarily run parallel. Frey notes (Frey 2004) that in the 5th century, "burial customs in the Celtic world were not uniform; rather, localised groups had their own beliefs, which, in consequence, also gave rise to distinct artistic expressions".
Material culture.
La Tène metalwork in bronze, iron and gold, developing technologically out of Hallstatt culture, is stylistically characterized by inscribed and inlaid intricate spirals and interlace, on fine bronze vessels, helmets and shields, horse trappings and elite jewelry, especially the neck rings called torcs and elaborate clasps called "fibulae". It is characterized by elegant, stylized curvilinear animal and vegetal forms, allied with the Hallstatt traditions of geometric patterning. The Early Style of La Tène art and culture mainly featured static, geometric decoration, while the transition to the Developed Style constituted a shift to movement-based forms, such as triskeles. Some subsets within the Developed Style contain more specific design trends, such as the recurrent serpentine scroll of the Waldalgesheim Style 
Initially La Tène folk lived in open settlements that were dominated by the chieftains’ towering hill forts. The development of towns—"oppida"—appears in mid-La Tène culture. La Tène dwellings were carpenter-built rather than of masonry. La Tène peoples also dug ritual shafts, in which votive offerings and even human sacrifices were cast. Severed heads appear to have held great power and were often represented in carvings. Burial sites included weapons, carts, and both elite and household goods, evoking a strong continuity with an afterlife.
Discovery.
La Tène is a village on the northern shore of Lake Neuchâtel, Switzerland. It is both an archaeological site and the eponymous site for the late Iron Age La Tène culture, also spelt "Latène" or "La-Tène".
In 1857, prolonged drought lowered the waters of the lake by about 2 m. On the northernmost tip of the lake, between the river Thielle and a point south of the village of Marin-Epagnier, Hansli Kopp, looking for antiquities for Colonel Frédéric Schwab, discovered several rows of wooden piles that still reached about 50 cm into the water. From among these, Kopp collected about forty iron swords.
The Swiss archaeologist Ferdinand Keller published his findings in 1868 in his influential first report on the Swiss pile dwellings ("Pfahlbaubericht"). In 1863 he interpreted the remains as a Celtic village built on piles. Eduard Desor, a geologist from Neuchâtel, started excavations on the lakeshore soon afterwards. He interpreted the site as an armory, erected on piles over the lake and later destroyed by enemy action. Another interpretation accounting for the presence of cast iron swords that had not been sharpened, was of a site of sacrifices.
With the first systematic lowering of the Swiss lakes from 1868 to 1883, the site fell completely dry. In 1880, Emile Vouga, a teacher from Marin-Epagnier, uncovered the wooden remains of two bridges (designated "Pont Desor" and "Pont Vouga") originally over 100 m long, that crossed the little Thielle River (today a nature reserve) and the remains of five houses on the shore. After Vouga had finished, F. Borel, curator of the Marin museum, began to excavate as well. In 1885 the canton asked the Société d'Histoire of Neuchâtel to continue the excavations, the results of which were published by Vouga in the same year.
All in all, over 2500 objects, mainly made from metal, have been excavated in La Tène. Weapons predominate, there being 166 swords (most without traces of wear), 270 lanceheads, and 22 shield bosses, along with 385 brooches, tools, and parts of chariots. Numerous human and animal bones were found as well.
Interpretations of the site vary. Some scholars believe the bridge was destroyed by high water, while others see it as a place of sacrifice after a successful battle (there are almost no female ornaments).
An exhibition marking the 150th anniversary of the discovery of the La Tène site was launched in June 2007 at the Musée Schwab in Bienne, Switzerland. It is scheduled to move to Zürich in 2008 and the Mont Beuvray in Burgundy in 2009.
Sites.
Some sites are:
Artifacts.
Some outstanding La Tène artifacts are:

</doc>
<doc id="18135" url="http://en.wikipedia.org/wiki?curid=18135" title="Lorenz curve">
Lorenz curve

In economics, the Lorenz curve is a graphical representation of the cumulative distribution function of the empirical probability distribution of wealth or income, and was developed by Max O. Lorenz in 1905 for representing inequality of the wealth distribution.
The curve is a graph showing the proportion of overall income or wealth assumed by the bottom "x"% of the people, although this is not rigorously true for a finite population (see below). It is often used to represent income distribution, where it shows for the bottom "x"% of households, what percentage ("y"%) of the total income they have. The percentage of households is plotted on the "x"-axis, the percentage of income on the "y"-axis. It can also be used to show distribution of assets. In such use, many economists consider it to be a measure of social inequality.
The concept is useful in describing inequality among the size of individuals in ecology and in studies of biodiversity, where the cumulative proportion of species is plotted against the cumulative proportion of individuals. It is also useful in business modeling: e.g., in consumer finance, to measure the actual percentage "y"% of delinquencies attributable to the "x"% of people with worst risk scores.
Explanation.
Points on the Lorenz curve represent statements like "the bottom 20% of all households have 10% of the total income." 
A perfectly equal income distribution would be one in which every person has the same income. In this case, the bottom "N"% of society would always have "N"% of the income. This can be depicted by the straight line "y" = "x"; called the "line of perfect equality."
By contrast, a perfectly unequal distribution would be one in which one person has all the income and everyone else has none. In that case, the curve would be at "y" = 0% for all "x" < 100%, and "y" = 100% when "x" = 100%. This curve is called the "line of perfect inequality."
The Gini coefficient is the ratio of the area between the line of perfect equality and the observed Lorenz curve to the area between the line of perfect equality and the line of perfect inequality. The higher the coefficient, the more unequal the distribution is. In the diagram on the right, this is given by the ratio "A"/("A+B"), where "A" and "B" are the indicated areas.
Definition and calculation.
The Lorenz curve can usually be represented by a function "L"("F"), where "F", the cumulative portion of the population, is represented by the horizontal axis, and "L", the cumulative portion of the total wealth or income, is represented by the vertical axis.
For a population of size "n", with a sequence of values "y""i", "i" = 1 to "n", that are indexed in non-decreasing order ( "y""i" ≤ "y""i"+1), the Lorenz curve is the continuous piecewise linear function connecting the points ( "F""i", "L""i" ), "i" = 0 to "n", where "F"0 = 0, "L"0 = 0, and for "i" = 1 to "n":
Note that the statement that the Lorenz curve gives the portion of the wealth or income held by a given portion of the population is only strictly true at the points defined above, but not at the points on the line segments between these points. For instance, in a population of 10 households, it doesn't make sense to say that 45% of them earn a certain portion of the total. If the population is modeled as a continuum then this subtlety disappears.
For a discrete probability function "f"("y"), let "y""i", "i" = 1 to "n", be the points with non-zero probabilities indexed in increasing order ( "y""i" < "y""i"+1). The Lorenz curve is the continuous piecewise linear function connecting the points ( "F""i", "L""i" ), "i" = 0 to "n", where "F"0 = 0, "L"0 = 0, and for "i" = 1 to "n":
For a probability density function "f"("x") with the cumulative distribution function "F"("x"), the Lorenz curve "L"("F"("x")) is given by:
where formula_8 denotes the average.
For a cumulative distribution function "F"("x") with inverse "x"("F"), the Lorenz curve "L"("F") is given by:
The inverse "x"("F") may not exist because the cumulative distribution function has intervals of constant values. However, the previous formula can still apply by generalizing the definition of "x"("F"):
For an example of a Lorenz curve, see Pareto distribution.
Properties.
A Lorenz curve always starts at (0,0) and ends at (1,1).
The Lorenz curve is not defined if the mean of the probability distribution is zero or infinite.
The Lorenz curve for a probability distribution is a continuous function. However, Lorenz curves representing discontinuous functions can be constructed as the limit of Lorenz curves of probability distributions, the line of perfect inequality being an example.
The information in a Lorenz curve may be summarized by the Gini coefficient and the Lorenz asymmetry coefficient.
The Lorenz curve cannot rise above the line of perfect equality. If the variable being measured cannot take negative values, the Lorenz curve:
Note however that a Lorenz curve for net worth would start out by going negative due to the fact that some people have a negative net worth because of debt.
The Lorenz curve is invariant under positive scaling. If X is a random variable, for any positive number "c" the random variable "c" X has the same Lorenz curve as X.
The Lorenz curve is flipped twice, once about F = 0.5 and once about "L" = 0.5, by negation. If X is a random variable with Lorenz curve "L"X("F"), then −X has the Lorenz curve:
The Lorenz curve is changed by translations so that the equality gap "F" − "L"("F") changes in proportion to the ratio of the original and translated means. If X is a random variable with a Lorenz curve "L" X ("F") and mean "μ" X , then for any constant "c" ≠ −"μ" X , X + "c" has a Lorenz curve defined by:
For a cumulative distribution function "F"("x") with mean "μ" and (generalized) inverse "x"("F"), then for any "F" with 0 < "F" < 1 :

</doc>
<doc id="18136" url="http://en.wikipedia.org/wiki?curid=18136" title="Literate programming">
Literate programming

Literate programming is an approach to programming introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.
The literate programming paradigm, as conceived by Knuth, represents a move away from writing programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts. Literate programs are written as an uninterrupted exposition of logic in an ordinary human language, much like the text of an essay, in which macros are included to hide abstractions and traditional source code.
Literate programming tools are used to obtain two representations from a literate source file: one suitable for further compilation or execution by a computer, the "tangled" code, and another for viewing as formatted documentation, which is said to be "woven" from the literate source. While the first generation of literate programming tools were computer language-specific, the later ones are language-agnostic and exist above the programming languages.
Concept.
A literate program is an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a "meta-language" on top of the underlying programming language.
A preprocessor is used to substitute arbitrary hierarchies, or rather "interconnected 'webs' of macros", to produce the compilable source code with one command ("tangle"), and documentation with another ("weave"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.
Advantages.
According to Knuth,
literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one's thoughts during a program's creation. The resulting documentation allows authors to restart their own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher "bird's eye view" of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.
Contrast with documentation generation.
Literate programming is very often misunderstood to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is backwards: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; in literate programming code is embedded in documentation, with the code following the structure of the documentation.
This misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are "literate programming tools". However, because these tools do not implement the "web of abstract concepts" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.
Example.
A classic example of literate programming is the literate implementation of the standard Unix codice_1 word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his "Literate Programming" book. The same example was later rewritten for the noweb literate programming tool. This example provides a good illustration of the basic elements of literate programming.
The following snippet of the codice_1 literate program shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new "operators" in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets ("codice_3") that indicate macros, the "codice_4" symbol which indicates the end of the code section in a noweb file. The "codice_5" symbol stands for the "root", topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as "codice_6", with the equal sign), so one literate program file can contain several files with machine source code.
Note also that the unraveling of the chunks can be done in any place in the literate program text file, not necessarily in the order they are sequenced in the enclosing chunk, but as is demanded by the logic reflected in the explanatory text that envelops the whole program.
Macros are not the same as "section names" in standard documentation. Literate programming macros can hide any chunk of code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as "codice_7", "codice_8" or "codice_9". This is illustrated by the following snippet of the codice_1 literate program.
In fact, macros can stand for any arbitrary chunk of code or other macros, and are thus more general than top-down or bottom-up "chunking", or than subsectioning. Knuth says that when he realized this, he began to think of a program as a "web" of various parts.
In a noweb literate program besides the free order of their exposition, the chunks behind macros, once introduced with "codice_11", can be grown later in any place in the file by simply writing "codice_6" and adding more content to it, as the following snippet illustrates ("plus" is added by the document formatter for readability, and is not in the code).
The documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The snippets of the literate codice_1 above show how an explanation of the program and its source code are interwoven. Such exposition of ideas creates the flow of thought that is like a literary work. Knuth wrote a "novel" which explains the code of the computer strategy game Colossal Cave Adventure.
Tools.
The first published literate programming environment was WEB, introduced by Donald Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's "TeX: The program", volume B of his 5-volume "Computers and Typesetting". Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems and can produce TeX and PDF documentation.
There are various other implementations of the literate programming concept:
Further reading.
</dl>

</doc>
<doc id="18137" url="http://en.wikipedia.org/wiki?curid=18137" title="Logistic map">
Logistic map

The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a seminal 1976 paper by the biologist Robert May, in part as a discrete-time demographic model analogous to the logistic equation first created by Pierre François Verhulst.
Mathematically, the logistic map is written
where:
This nonlinear difference equation is intended to capture two effects.
However, as a demographic model the logistic map has the pathological problem that some initial conditions and parameter values lead to negative population sizes. This problem does not appear in the older Ricker model, which also exhibits chaotic dynamics.
The formula_3 case of the logistic map is a nonlinear transformation of both the bit-shift map and the formula_4 case of the tent map.
Behavior dependent on "r".
The image below shows the amplitude and frequency content of some logistic map iterates for parameter values ranging from 2 to 4.
By varying the parameter "r", the following behavior is observed:
For any value of "r" there is at most one stable cycle. In this case, it is a globally stable cycle, which attracts almost all points.:13 For an "r" with a stable cycle of some period, there can be infinitely many unstable cycles of various periods.
A bifurcation diagram summarizes this. The horizontal axis shows the values of the parameter "r" while the vertical axis shows the possible long-term values of "x".
The bifurcation diagram is a self-similar: if you zoom in on the above-mentioned value "r" = 3.82843 and focus on one arm of the three, the situation nearby looks like a shrunk and slightly distorted version of the whole diagram. The same is true for all other non-chaotic points. This is an example of the deep and ubiquitous connection between chaos and fractals.
Chaos and the logistic map.
The relative simplicity of the logistic map makes it an excellent point of entry into a consideration of the concept of chaos. A rough description of chaos is that chaotic systems exhibit a great sensitivity to initial conditions—a property of the logistic map for most values of "r" between about 3.57 and 4 (as noted above). A common source of such sensitivity to initial conditions is that the map represents a repeated folding and stretching of the space on which it is defined. In the case of the logistic map, the quadratic difference equation (1) describing it may be thought of as a stretching-and-folding operation on the interval (0,1).
The following figure illustrates the stretching and folding over a sequence of iterates of the map. Figure (a), left, shows a two-dimensional Poincaré plot of the logistic map's state space for "r"=4, and clearly shows the quadratic curve of the difference equation (1). However, we can embed the same sequence in a three-dimensional state space, in order to investigate the deeper structure of the map. Figure (b), right, demonstrates this, showing how initially nearby points begin to diverge, particularly in those regions of "X""t" corresponding to the steeper sections of the plot.
This stretching-and-folding does not just produce a gradual divergence of the sequences of iterates, but an exponential divergence (see Lyapunov exponents), evidenced also by the complexity and unpredictability of the chaotic logistic map. In fact, exponential divergence of sequences of iterates explains the connection between chaos and unpredictability: a small error in the supposed initial state of the system will tend to correspond to a large error later in its evolution. Hence, predictions about future states become progressively (indeed, exponentially) worse when there are even very small errors in our knowledge of the initial state. This quality of unpredictability and apparent randomness led the logistic map equation to be used as a Pseudo-random number generator in early computers.
Since the map is confined to an interval on the real number line, its dimension is less than or equal to unity. Numerical estimates yield a correlation dimension of 0.500 ± 0.005 (Grassberger, 1983), a Hausdorff dimension of about 0.538 (Grassberger 1981), and an information dimension of 0.5170976... (Grassberger 1983) for r=3.5699456... (onset of chaos). Note: It can be shown that the correlation dimension is certainly between 0.4926 and 0.5024.
It is often possible, however, to make precise and accurate statements about the "likelihood" of a future state in a chaotic system. If a (possibly chaotic) dynamical system has an attractor, then there exists a probability measure that gives the long-run proportion of time spent by the system in the various regions of the attractor. In the case of the logistic map with parameter  "r" = 4  and an initial state in (0,1), the attractor is also the interval (0,1) and the probability measure corresponds to the beta distribution with parameters  "a" = 0.5  and  "b" = 0.5. Specifically, the invariant measure is formula_14. Unpredictability is not randomness, but in some circumstances looks very much like it. Hence, and fortunately, even if we know very little about the initial state of the logistic map (or some other chaotic system), we can still say something about the distribution of states a long time into the future, and use this knowledge to inform decisions based on the state of the system.
Solution in some cases.
The special case of "r" = 4 can in fact be solved exactly, as can the case with "r" = 2; however the general case can only be predicted statistically. 
The solution when "r" = 4 is,
where the initial condition parameter formula_16 is given by formula_17. For rational formula_16, after a finite number of iterations formula_2 maps into a periodic sequence. But almost all formula_16 are irrational, and, for irrational formula_16, formula_2 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2"n" shows the exponential growth of stretching, which results in sensitive dependence on initial conditions, while the squared sine function keeps formula_2 folded within the range [0, 1].
For "r" = 4 an equivalent solution in terms of complex numbers instead of trigonometric functions is
where formula_25 is either of the complex numbers
with modulus equal to 1. Just as the squared sine function in the trigonometric solution leads to neither shrinkage nor expansion of the set of points visited, in the latter solution this effect is accomplished by the unit modulus of formula_25.
By contrast, the solution when "r"=2 is
for formula_29. Since formula_30 for any value of formula_31 other than the unstable fixed point 0, the term formula_32 goes to 0 as "n" goes to infinity, so formula_2 goes to the stable fixed point formula_34
Finding cycles of any length when "r" = 4.
For the "r" = 4 case, from almost all initial conditions the iterate sequence is chaotic. Nevertheless, there exist an infinite number of initial conditions that lead to cycles, and indeed there exist cycles of length "k" for "all" integers "k" ≥ 1. We can exploit the relationship of the logistic map to the dyadic transformation (also known as the bit-shift map) to find cycles of any length. If "x" follows the logistic map formula_35 and "y" follows the dyadic transformation
then the two are related by
The reason that the dyadic transformation is also called the bit-shift map is that when "y" is written in binary notation, the map moves the binary point one place to the right (and if the bit to the left of the binary point has become a "1", this "1" is changed to a "0"). A cycle of length 3, for example, occurs if an iterate has a 3-bit repeating sequence in its binary expansion (which is not also a one-bit repeating sequence): 001, 010, 100, 110, 101, or 011. The iterate 001001001... maps into 010010010..., which maps into 100100100..., which in turn maps into the original 001001001...; so this is a 3-cycle of the bit shift map. And the other three binary-expansion repeating sequences give the 3-cycle 110110110... → 101101101... → 011011011... → 110110110... Either of these 3-cycles can be converted to fraction form: for example, the first-given 3-cycle can be written as 1/7 → 2/7 → 4/7 → 1/7. Using the above translation from the bit-shift map to the "r" = 4 logistic map gives the corresponding logistic cycle .611260467... → .950484434... → .188255099... → .611260467... . We could similarly translate the other bit-shift 3-cycle into its corresponding logistic cycle. Likewise, cycles of any length "k" can be found in the bit-shift map and then translated into the corresponding logistic cycles.
However, since almost all numbers in [0, 1) are irrational, almost all initial conditions of the bit-shift map lead to the non-periodicity of chaos. This is one way to see that the logistic "r" = 4 map is chaotic for almost all initial conditions.
Amount of cycles of (minimal) length "k" for logistic map with "r" = 4 (tent map with formula_4) is a known integer sequence (sequence in OEIS): 2, 1, 2, 3, 6, 9, 18, 30, 56, 99, 186, 335, 630, 1161 ... It tells us that logistic map with "r" = 4 has 2 fixed points, 1 cycle of length 2, 2 cycles of length 3 and so on. This sequence takes a particularly simple form for prime "k": formula_39. For example: formula_40 is the number of cycles of length 13.

</doc>
<doc id="18138" url="http://en.wikipedia.org/wiki?curid=18138" title="Levant">
Levant

The Levant (; Arabic: المشرق /ʔal-maʃriq/) is an approximate historical geographical term referring to a large area in the eastern Mediterranean. In its widest historical sense, the Levant included all of the eastern Mediterranean with its islands, that is, it included all of the countries along the eastern Mediterranean shores, extending from Greece to Egypt. The term "Levant" entered English in the late 15th century from French. It derives from the Italian "levante", meaning “rising,” implying the rising of the sun in the east. As such, it is broadly equivalent to the Arabic term "Mashriq," 'the land where the sun rises'. The western counterpart in Arabic is the Maghreb.
In the 13th and 14th centuries CE the term "levante" was used for Italian maritime commerce in the eastern Mediterranean, including Greece, Anatolia, Syria-Palestine, and Egypt, that is, the lands east of Venice. Eventually the term was restricted to the Muslim countries of Syria-Palestine and Egypt. In 1581 England set up the Levant Company to monopolize commerce with the Ottoman Empire.
The name "Levant States" was used to refer to the French mandate over Syria and Lebanon after World War I. This is probably the reason why the term "Levant" has come to be used synonymously with Syria-Palestine. Some scholars misunderstood the term thinking that it derives from the name of Lebanon. Today the term is typically used in conjunction with prehistoric or ancient historical references. It has the same meaning as Syria-Palestine or Greater Syria (Arabic: الشام /ʔaʃ-ʃaːm/), that is, it means an area bounded by the Taurus Mountains of Anatolia in the North, the Mediterranean Sea in the west, and the north Arabian Desert and Mesopotamia in the east. It does not include Anatolia (the former Asia Minor, now Asian Turkey; although at times Cilicia may be included), the Caucasus Mountains, or any part of the Arabian Peninsula proper. The Sinai Peninsula (Asian Egypt) is sometimes included, though more considered an intermediate, peripheral or marginal area forming a land bridge between the Levant and northern African Egypt.
The Levant has been described as the "crossroads of western Asia, the eastern Mediterranean and northeast Africa", and the "northwest of the Arabian plate".
Etymology.
The term "Levant", which appeared in English in 1497, originally meant the East in general or "Mediterranean lands east of Italy". It is borrowed from the French "levant" 'rising', referring to the rising of the sun in the east, or the point where the sun rises. The phrase is ultimately from the Latin word "levare," meaning 'lift, raise'. Similar etymologies are found in Greek Ἀνατολή ("Anatolē", "cf." Anatolia), in Germanic "Morgenland" (literally, "morning land"), in Italian (as in "Riviera di Levante", the portion of the Liguria coast east of Genoa), in Hungarian "Kelet", in Spanish and Catalan "Levante" and "Llevant", ("the place of rising"), and in Hebrew ("mizrah"). Most notably, "Orient" and its Latin source "oriens" meaning "east", is literally "rising", deriving from Latin "orior" "rise".
The notion of the Levant has undergone a dynamic process of historical evolution in usage, meaning, and understanding. While the term "Levantine" originally referred to the European residents of the eastern Mediterranean region, it later came to refer to regional "native" and "minority" groups.
The term became current in English in the 16th century, along with the first English merchant adventurers in the region; English ships appeared in the Mediterranean in the 1570s, and the English merchant company signed its agreement ("capitulations") with the Grand Turk in 1579 (Braudel). The English Levant Company was founded in 1581 to trade with the Ottoman Empire, and in 1670 the French was founded for the same purpose. At this time, the Far East was known as the "Upper Levant".
In 19th-century travel writing, the term incorporated eastern regions under then current or recent governance of the Ottoman empire, such as Greece. In 19th-century archaeology, it referred to overlapping cultures in this region during and after prehistoric times, intending to reference the place instead of any one culture. The French mandates of Syria and Lebanon (1920–1946) were called the Levant states.
Geography and modern-day use of the term.
Today, "Levant" is typically used by archaeologists and historians with reference to the prehistory and the ancient and medieval history of the region, as when discussing the Crusades. The term is also occasionally employed to refer to modern events, peoples, states or parts of states in the same region, namely Cyprus, Egypt, Iraq, Israel, Jordan, Lebanon, Palestine, Syria, and Turkey are sometimes considered Levant countries (compare with Near East, Middle East, Eastern Mediterranean and Western Asia). Several researchers include the island of Cyprus in Levantine studies, including the Council for British Research in the Levant, the UCLA Near Eastern Languages and Cultures department, "Journal of Levantine Studies" and the UCL Institute of Archaeology, the last of which has dated the connection between Cyprus and mainland Levant to the early Iron Age. Archaeologists seeking a neutral orientation that is neither biblical nor national have used terms such as Syro-Palestinian archaeology and archaeology of the southern Levant.
While the usage of the term "Levant" in academia has been relegated to the fields of archeology and literature, there is a recent attempt to reclaim the notion of the Levant as a category of analysis in political and social sciences. Two academic journals were recently launched: "", published by the Van Leer Jerusalem Institute and "The Levantine Review", published by Boston College.
People, religion and culture.
The populations of the Levant share not only the geographic position, but cuisine, some customs, and a very long history. The largest religious group in the Levant are the Muslims and the largest cultural-linguistic group are primarily Arab due to Arabization of the region over the centuries, but there are also many other groups.
The majority of Levantines are Sunni, Salafi, nondenominational or Shia Muslim. There are also Yazidi Kurds, Alawites, Nizari, Druze and Ismailis.
Until the creation of the modern State of Israel in 1948, Jews lived throughout the southern Levant alongside Muslims and Christians; since then, almost all have been expelled from their homes and sought refuge in Israel. 
There are many Levantine Christian groups such as Greek and Oriental Orthodox, Maronite, Roman Catholic, and Protestant. Armenians mostly belong to the Armenian Apostolic Church. There are Levantines or Franco-Levantines who are mostly Roman Catholic. There are also Circassians, Turks, Samaritans, and Nawars. There are Assyrian peoples belonging to the Assyrian Church of the East (autonomous) and the Chaldean Catholic Church (Catholic).
Language.
Most Levantine populations speak Levantine Arabic, also known as Mediterranean Arabic (شامي). In Israel, the primary language is Hebrew, while Arabic is also an official language. In Cyprus, the majority language is Greek, followed by Turkish, and then a dialect of Levantine Arabic, Cypriot Maronite Arabic. Some communities and populations speak Greek, Armenian, Circassian, French, English or other languages in addition to Levantine Arabic.
Dance.
A dance native to the Levant is known as the Dabke, a folk dance of possible Canaanite or Phoenician origin. It is marked by synchronized jumping, stamping, and movement, similar to tap dancing. One version is performed by men, another by women.
See also.
Overlapping regional designations
Sub-regional designations
Other

</doc>
<doc id="18139" url="http://en.wikipedia.org/wiki?curid=18139" title="League of Nations mandate">
League of Nations mandate

A League of Nations mandate was a legal status for certain territories transferred from the control of one country to another following World War I, or the legal instruments that contained the internationally agreed-upon terms for administering the territory on behalf of the League. These were of the nature of both a treaty and a constitution, which contained minority rights clauses that provided for the rights of petition and adjudication by the International Court. The mandate system was established under Article 22 of the Covenant of the League of Nations, entered into on 28 June 1919. With the dissolution of the League of Nations after World War II, it was stipulated at the Yalta Conference that the remaining Mandates should be placed under the trusteeship of the United Nations, subject to future discussions and formal agreements. Most of the remaining mandates of the League of Nations (with the exception of South-West Africa) thus eventually became United Nations Trust Territories.
Generalities.
All of the territories subject to League of Nations mandates were previously controlled by states defeated in World War I, principally Imperial Germany and the Ottoman Empire. The mandates were fundamentally different from the protectorates in that the Mandatory power undertook obligations to the inhabitants of the territory and to the League of Nations.
The process of establishing the mandates consisted of two phases: 
Treaties.
The divestiture of Germany's overseas colonies, along with three territories disentangled from its European homeland area (the Free City of Danzig, Memel Territory, and Saar), was accomplished in the Treaty of Versailles (1919), with the territories being allotted among the Allies on May 7 of that year. Ottoman territorial claims were first addressed in the Treaty of Sèvres (1920) and finalized in the Treaty of Lausanne (1923). The Turkish territories were allotted among the Allied Powers at the San Remo conference in 1920.
Hidden agendas and objections.
Peace treaties have played an important role in the formation of the modern law of nations. Many rules that govern the relations between states have been introduced and codified in the terms of peace treaties. The first twenty-six articles of the contained the Covenant of the League of Nations. It contained the international machinery for the enforcement of the terms of the treaty. Article 22 established a system of Mandates to administer former colonies and territories.
Legitimacy of the allocations.
Article 22 was written two months before the signing of the peace treaty, before it was known what communities, peoples, or territories were related to sub-paragraphs 4, 5, and 6. The treaty was signed, and the peace conference had been adjourned, before a formal decision was made. The mandates were arrangements guaranteed by, or arising out of the general treaty which stipulated that mandates were to be exercised on behalf of the League.
The treaty contained no provision for the mandates to be allocated on the basis of decisions taken by four members of the League acting in the name of the so-called "Principal Allied and Associated Powers". The decisions taken at the conferences of the Council of Four were not made on the basis of consultation or League unanimity as stipulated by the Covenant. As a result, the actions of the conferees were viewed by some as having no legitimacy.
In testimony before the Senate Committee on Foreign Relations a former US State Department official who had been a member of the American Commission at Paris, testified that the United Kingdom and France had simply gone ahead and arranged the world to suit themselves. He pointed out that the League of Nations could do nothing to alter their arrangements, since the League could only act by unanimous consent of its members - including the UK and France.
United States Secretary of State Robert Lansing was a member of the American Commission to Negotiate Peace at Paris in 1919. He explained that the system of mandates was a device created by the Great Powers to conceal their division of the spoils of war under the color of international law. If the former German and Ottoman territories had been ceded to the victorious powers directly, their economic value would have been credited to offset the Allies' claims for war reparations.
Article 243 of the treaty instructed the Reparations Commission that non-mandate areas of the Saar and Alsace-Lorraine were to be reckoned as credits to Germany in respect of its reparation obligations.
Legitimacy of the provisions.
Under the plan of the US Constitution the Congress was delegated the power to declare or define the Law of Nations in cases where its terms might be vague or indefinite. The US Senate refused to ratify the Covenant of the League of Nations. The legal issues surrounding the rule by force and the lack of self-determination under the system of mandates were cited by the Senators who withheld their consent. The US government subsequently entered into individual treaties to secure legal rights for its citizens, to protect property rights and businesses interests in the mandates, and to preclude the mandatory administration from altering the terms of the mandates without prior US approval.
The United States filed a formal protest because the preamble of the mandates indicated to the League that they had been approved by the Principal Allied and Associated Powers, when, in fact, that was not the case.
The Official Journal of the League of Nations, dated June 1922, contained a statement by Lord Balfour (UK) in which he explained that the League's authority was strictly limited. The article related that the 'Mandates were not the creation of the League, and they could not in substance be altered by the League. The League's duties were confined to seeing that the specific and detailed terms of the mandates were in accordance with the decisions taken by the Allied and Associated Powers, and that in carrying out these mandates the Mandatory Powers should be under the supervision—not under the control—of the League.'
Types of mandates.
The League of Nations decided the exact level of control by the Mandatory power over each mandate on an individual basis. However, in every case the Mandatory power was forbidden to construct fortifications or raise an army within the territory of the mandate, and was required to present an annual report on the territory to the League of Nations.
The mandates were divided into three distinct groups based upon the level of development each population had achieved at that time.
Class A mandates.
The first group, or "Class A mandates", were territories formerly controlled by the Ottoman Empire that were deemed to "... have reached a stage of development where their existence as independent nations can be provisionally recognized subject to the rendering of administrative advice and assistance by a Mandatory until such time as they are able to stand alone. The wishes of these communities must be a principal consideration in the selection of the Mandatory." 
The Class A mandates were:
Class B mandates.
The second group of mandates, or "Class B mandates", were all former "Schutzgebiete" (German territories) in West and Central Africa which were deemed to require a greater level of control by the mandatory power: "...the Mandatory must be responsible for the administration of the territory under conditions which will guarantee freedom of conscience and religion." The mandatory power was forbidden to construct military or naval bases within the mandates. 
The Class B mandates were:
Class C mandates.
The "Class C mandates", including South West Africa and certain of the South Pacific Islands, were considered to be "best administered under the laws of the Mandatory as integral portions of its territory"
The Class C mandates were former German possessions:
Rules of establishment.
According to the Council of the League of Nations, meeting of August 1920: "draft mandates adopted by the Allied and Associated Powers would not be definitive until they had been considered and approved by the League ... the legal title held by the mandatory Power must be a double one: one conferred by the Principal Powers and the other conferred by the League of Nations,"
Three steps were required to establish a Mandate under international law:
(1) The Principal Allied and Associated Powers confer a mandate on one of their number or on a third power; (2) the principal powers officially notify the council of the League of Nations that a certain power has been appointed mandatory for such a certain defined territory; and (3) the council of the League of Nations takes official cognisance of the appointment of the mandatory power and informs the latter that it [the council] considers it as invested with the mandate, and at the same time notifies it of the terms of the mandate, after assertaining whether they are in conformance with the provisions of the covenant."
The U.S. State Department "Digest of International Law" says that the terms of the Treaty of Lausanne provided for the application of the principles of state succession to the "A" Mandates. The Treaty of Versailles (1920) provisionally recognized the former Ottoman communities as independent nations. It also required Germany to recognize the disposition of the former Ottoman territories and to recognize the new states laid down within their boundaries. The terms of the Treaty of Lausanne required the newly created states that acquired the territory detached from the Ottoman Empire to pay annuities on the Ottoman public debt and to assume responsibility for the administration of concessions that had been granted by the Ottomans. The treaty also let the States acquire, without payment, all the property and possessions of the Ottoman Empire situated within their territory. The treaty provided that the League of Nations was responsible for establishing an arbital court to resolve disputes that might arise and stipulated that its decisions were final.
A disagreement regarding the legal status and the portion of the annuities to be paid by the "A" mandates was settled when an Arbitrator ruled that some of the mandates contained more than one State:The difficulty arises here how one is to regard the Asiatic countries under the British and French mandates. Iraq is a Kingdom in regard to which Great Britain has undertaken responsibilities equivalent to those of a Mandatory Power. Under the British mandate, Palestine and Transjordan have each an entirely separate organisation. We are, therefore, in the presence of three States sufficiently separate to be considered as distinct Parties. France has received a single mandate from the Council of the League of Nations, but in the countries subject to that mandate, one can distinguish two distinct States: Syria and the Lebanon, each State possessing its own constitution and a nationality clearly different from the other.
Later history.
After the United Nations was founded in 1945 and the League of Nations was disbanded, all but one of the mandated territories that remained under the control of the mandatory power became United Nations trust territories, a roughly equivalent status. In each case, the colonial power that held the mandate on each territory became the administering power of the trusteeship, except that Japan, which had been defeated in World War II, lost its mandate over the South Pacific islands, which became a "strategic trust territory" known as the Trust Territory of the Pacific Islands under United States administration.
The sole exception to the transformation of League of Nations mandates into UN trusteeships was that South Africa refused to place South-West Africa under trusteeship. Instead, South Africa proposed that it be allowed to annex South-West Africa, a proposal rejected by the United Nations General Assembly. The International Court of Justice held that South Africa continued to have international obligations under the mandate for South-West Africa. The territory finally attained independence in 1990 as Namibia, after a long guerrilla war of independence against the apartheid regime.
Nearly all the former League of Nations mandates had become sovereign states by 1990, including all of the former United Nations Trust Territories with the exception of a few successor entities of the gradually dismembered Trust Territory of the Pacific Islands (formerly Japan's South Pacific Trust Mandate). These exceptions include the Northern Mariana Islands which is a commonwealth in political union with the United States with the status of unincorporated organized territory. The Northern Mariana Islands does elect its own governor to serve as territorial head of government, but it remains a U.S. territory with its head of state being the President of the United States and federal funds to the Commonwealth administered by the Office of Insular Affairs of the United States Department of the Interior.
Remnant Micronesia and the Marshall Islands, the heirs of the last territories of the Trust, attained final independence on 22 December 1990. (The UN Security Council ratified termination of trusteeship, effectively dissolving trusteeship status, on 10 July 1987). The Republic of Palau, split off from the Federated States of Micronesia, became the last to get its independence effectively on 1 October 1994.

</doc>
<doc id="18142" url="http://en.wikipedia.org/wiki?curid=18142" title="Loudon Classic">
Loudon Classic

The Loudon Classic, held at the New Hampshire Motor Speedway (formerly Bryar Motorsport Park and New Hampshire International Speedway) is the longest running motorcycle race in the United States, and is held every year on Father's Day. While it is popularly known as Laconia, the location of the race was moved from Belknap Recreation Area to Loudon in 1964.

</doc>
<doc id="18143" url="http://en.wikipedia.org/wiki?curid=18143" title="Lincoln, New Hampshire">
Lincoln, New Hampshire

Lincoln is a town in Grafton County, New Hampshire, United States. It is the second-largest town by area in New Hampshire. The population was 1,662 at the 2010 census. The town is home to the New Hampshire Highland Games and to a portion of Franconia Notch State Park. Set in the White Mountains, large portions of the town are within the White Mountain National Forest. The Appalachian Trail crosses in the northeast. Lincoln is the location of the Loon Mountain ski resort and associated recreation-centered development.
The primary settlement in town, where 993 people resided at the 2010 census, is defined as the Lincoln census-designated place (CDP) and is located along New Hampshire Route 112 east of Interstate 93. The town also includes the village of North Lincoln and the former village sites of Stillwater and Zealand (sometime known as Pullman) in the town's remote eastern and northern sections respectively, which are now within the White Mountain National Forest.
History.
In 1764, Colonial Governor Benning Wentworth granted 32456 acre to a group of approximately 70 land investors from Connecticut. Lincoln was named after Henry Fiennes Pelham-Clinton, 2nd Duke of Newcastle, 9th Earl of Lincoln – a cousin of the Wentworth governors. He held the position of comptroller of customs for the port of London under George II and George III, which was important to trade between America and England.
The town was settled about 1782. The 1790 census indicates that it had 22 inhabitants. Rocky soil yielded poor farming, but the area's abundant timber, combined with water power to run sawmills on the Pemigewasset River and its East Branch, helped Lincoln develop into a center for logging. By 1853, the Merrimack River Lumber Company was operating. The railroad transported freight, and increasingly brought tourists to the beautiful mountain region. In 1892, James E. Henry bought approximately 100000 acre of virgin timber and established a logging enterprise at what is today the center of Lincoln. In 1902, he built a pulp and paper mill. He erected The Lincoln House hotel in 1903, although a 1907 fire would nearly raze the community. Until he died in 1912, Henry controlled his company town, installing relatives in positions of civic authority.
In 1917, Henry's heirs sold the business to the Parker Young Company, which in turn sold it to the Marcalus Manufacturing Company in 1946. Franconia Paper took over in 1950, producing 150 tons of paper a day until bankruptcy in 1971, at which time new river classification standards discouraged further papermaking in Lincoln.
Tourism is today the principal business. Nearby Loon Mountain has long drawn skiers, and in recent years has attempted to convert itself into a four-season attraction. The Flume is one of the most visited attractions in the state. Discovered in 1808, it is a natural canyon extending 800 ft at the base of Mount Liberty. Walls of Conway granite rise to a height of 70 to 90 feet (21 to 27 m) and are only 12 to 20 feet (2.5 to 6.0 m) apart.
Geography.
According to the United States Census Bureau, the town has a total area of 130.9 sqmi, of which 130.3 sqmi is land and 0.6 sqmi is water, comprising 0.43% of the town. It is the second-largest town in area in New Hampshire, after Pittsburg.
Lincoln is drained by the Pemigewasset River and its East Branch. Lincoln lies almost fully within the Merrimack River watershed, with the western edge of town in the Connecticut River watershed. Kancamagus Pass, elevation 2860 ft, is on the Kancamagus Highway at the eastern boundary. The highest point in Lincoln is either the summit of Mount Carrigain, at 4700 ft above sea level, plus or minus 10 ft, or the summit of Mount Bond at 4698 ft.
Demographics.
As of the census of 2000, there were 1,271 people, 583 households, and 324 families residing in the town. The population density was 9.7 people per square mile (3.8/km²). There were 2,339 housing units at an average density of 17.9 per square mile (6.9/km²). The racial makeup of the town was 97.40% White, 0.39% Native American, 0.87% Asian, 0.16% from other races, and 1.18% from two or more races. 0.71% of the population were Hispanic or Latino of any race.
There were 583 households out of which 22.8% had children under the age of 18 living with them, 43.4% were married couples living together, 8.9% had a female householder with no husband present, and 44.4% were non-families. 35.7% of all households were made up of individuals and 12.7% had someone living alone who was 65 years of age or older. The average household size was 2.18 and the average family size was 2.83.
In the town the population was spread out with 19.8% under the age of 18, 7.1% from 18 to 24, 27.9% from 25 to 44, 27.9% from 45 to 64, and 17.4% who were 65 years of age or older. The median age was 43 years. For every 100 females there were 99.2 males. For every 100 females age 18 and over, there were 96.7 males.
The median income for a household in the town was $28,523, and the median income for a family was $44,063. Males had a median income of $25,263 versus $22,784 for females. The per capita income for the town was $17,999. About 3.4% of families and 8.0% of the population were below the poverty line, including 5.3% of those under age 18 and 5.5% of those age 65 or over.

</doc>
<doc id="18145" url="http://en.wikipedia.org/wiki?curid=18145" title="List of laser applications">
List of laser applications

Many scientific, military, medical and commercial laser applications have been developed since the invention of the laser in 1958. The coherency, high monochromaticity, and ability to reach extremely high powers are all properties which allow for these specialized applications.
Scientific.
In science, lasers are used in many ways, including:
Lasers may also be indirectly used in spectroscopy as a micro-sampling system, a technique termed Laser ablation (LA), which is typically applied to ICP-MS apparatus resulting in the powerful LA-ICP-MS.
The principles of laser spectroscopy are discussed by Demtröder
and the use of tunable lasers in spectroscopy are described in Tunable Laser Applications.
Spectroscopy.
Most types of laser are an inherently pure source of light; they emit near-monochromatic light with a very well defined range of wavelengths. By careful design of the laser components, the purity of the laser light (measured as the "linewidth") can be improved more than the purity of any other light source. This makes the laser a very useful source for spectroscopy. The high intensity of light that can be achieved in a small, well collimated beam can also be used to induce a nonlinear optical effect in a sample, which makes techniques such as Raman spectroscopy possible. Other spectroscopic techniques based on lasers can be used to make extremely sensitive detectors of various molecules, able to measure molecular concentrations in the parts-per-1012 (ppt) level. Due to the high power densities achievable by lasers, beam-induced atomic emission is possible: this technique is termed Laser induced breakdown spectroscopy (LIBS).
Heat Treatment.
Heat treating with lasers allows selective surface hardening against wear with little or no distortion of the component. Because this eliminates much part reworking that is currently done, the laser system's capital cost is recovered in a short time. An inert, absorbent coating for laser heat treatment has also been developed that eliminates the fumes generated by conventional paint coatings during the heat-treating process with CO2 laser beams.
One consideration crucial to the success of a heat treatment operation is control of the laser beam irradiance on the part surface. The optimal irradiance distribution is driven by the thermodynamics of the laser-material interaction and by the part geometry.
Typically, irradiances between 500-5000 W/cm^2 satisfy the thermodynamic constraints and allow the rapid surface heating and minimal total heat input required. For general heat treatment, a uniform square or rectangular beam is one of the best options. For some special applications or applications where the heat treatment is done on an edge or corner of the part, it may be better to have the irradiance decrease near the edge to prevent melting.
Lunar laser ranging.
When the Apollo astronauts visited the moon, they planted retroreflector arrays to make possible the Lunar Laser Ranging Experiment. Laser beams are focused through large telescopes on Earth aimed toward the arrays, and the time taken for the beam to be reflected back to Earth measured to determine the distance between the Earth and Moon with high accuracy.
Photochemistry.
Some laser systems, through the process of mode locking, can produce extremely brief pulses of light - as short as picoseconds or femtoseconds (10−12 - 10−15 seconds). Such pulses can be used to initiate and analyse chemical reactions, a technique known as "photochemistry". The short pulses can be used to probe the process of the reaction at a very high temporal resolution, allowing the detection of short-lived intermediate molecules. This method is particularly useful in biochemistry, where it is used to analyse details of protein folding and function.
Laser barcode scanners.
Laser barcode scanners are ideal for applications that require high speed reading of linear codes or stacked symbols. From small products for embedded OEM applications to rugged laser barcode scanners for industrial use, Microscan offers a wide range of quality products to read linear barcodes and stacked symbols, with features such as high speed reading, wide field of view, symbol reconstruction, and aggressive decoding technology.
Laser cooling.
A technique that has recent success is "laser cooling". This involves atom trapping, a method where a number of atoms are confined in a specially shaped arrangement of electric and magnetic fields. Shining particular wavelengths of laser light at the ions or atoms slows them down, thus "cooling" them. As this process is continued, they all are slowed and have the same energy level, forming an unusual arrangement of matter known as a Bose–Einstein condensate.
Nuclear fusion.
Some of the world's most powerful and complex arrangements of multiple lasers and optical amplifiers are used to produce extremely high intensity pulses of light of extremely short duration. These pulses are arranged such that they impact pellets of tritium–deuterium simultaneously from all directions, hoping that the squeezing effect of the impacts will induce atomic fusion in the pellets. This technique, known as "inertial confinement fusion", so far has not been able to achieve "breakeven", that is, so far the fusion reaction generates less power than is used to power the lasers, but research continues.
Microscopy.
Confocal laser scanning microscopy and Two-photon excitation microscopy make use of lasers to obtain blur-free images of thick specimens at various depths. Laser capture microdissection use lasers to procure specific cell populations from a tissue section under microscopic visualization.
Additional laser microscopy techniques include harmonic microscopy, four-wave mixing microscopy and interferometric microscopy.
Military.
Military uses of lasers include applications such as target designation and ranging, defensive countermeasures, communications and directed energy weapons.
Directly as an energy weapon.
Directed energy weapons are being developed, such as Boeing's Airborne Laser which was constructed inside a Boeing 747. Designated the YAL-1, it was intended to kill short- and intermediate-range ballistic missiles in their boost phase.
High power boost-phase intercept laser systems use a complex system of lasers to find, track and destroy intercontinental ballistic missiles (ICBM). In this type of system a chemical laser, one in which the laser operation is powered by an energetic chemical reaction, is used as the main weapon beam (see Airborne Laser). The Mobile Tactical High-Energy Laser (MTHEL) is another defensive laser system under development; this is envisioned as a field-deployable weapon system able to track incoming artillery projectiles and cruise missiles by radar and destroy them with a powerful deuterium fluoride laser.
Another example of direct use of a laser as a defensive weapon was researched for the Strategic Defense Initiative (SDI, nicknamed "Star Wars"), and its successor programs. This project would use ground-based or space-based laser systems to destroy incoming intercontinental ballistic missiles (ICBMs). The practical problems of using and aiming these systems were many; particularly the problem of destroying ICBMs at the most opportune moment, the "boost phase" just after launch. This would involve directing a laser through a large distance in the atmosphere, which, due to optical scattering and refraction, would bend and distort the laser beam, complicating the aiming of the laser and reducing its efficiency.
Another idea from the SDI project was the "nuclear-pumped X-ray laser". This was essentially an orbiting atomic bomb, surrounded by laser media in the form of glass rods; when the bomb exploded, the rods would be bombarded with highly-energetic gamma-ray photons, causing spontaneous and stimulated emission of X-ray photons in the atoms making up the rods. This would lead to optical amplification of the X-ray photons, producing an X-ray laser beam that would be minimally affected by atmospheric distortion and capable of destroying ICBMs in flight. The X-ray laser would be a strictly one-shot device, destroying itself on activation. Some initial tests of this concept were performed with underground nuclear testing; however, the results were not encouraging. Research into this approach to missile defense was discontinued after the SDI program was cancelled.
Defensive countermeasures.
Defensive countermeasure applications can range from compact, low power infrared countermeasures to high power, airborne laser systems. IR countermeasure systems use lasers to confuse the seeker heads on infrared homing missiles.
Disorientation.
Some weapons simply use a laser to disorient a person. One such weapon is the Thales Green Laser Optical Warner.
Guidance.
Laser guidance is a technique of guiding a missile or other projectile or vehicle to a target by means of a laser beam.
Targeting.
Target designator.
Another military use of lasers is as a "laser target designator". This is a low-power laser pointer used to indicate a target for a precision-guided munition, typically launched from an aircraft. The guided munition adjusts its flight-path to home in to the laser light reflected by the target, enabling a great precision in aiming. The beam of the laser target designator is set to a pulse rate that matches that set on the guided munition to ensure munitions strike their designated targets and do not follow other laser beams which may be in use in the area. The laser designator can be shone onto the target by an aircraft or nearby infantry. Lasers used for this purpose are usually infrared lasers, so the enemy cannot easily detect the guiding laser light.
Firearms.
Laser sight.
The laser has in most firearms applications been used as a tool to enhance the targeting of other weapon systems. For example, a "laser sight" is a small, usually visible-light laser placed on a handgun or a rifle and aligned to emit a beam parallel to the barrel. Since a laser beam has low divergence, the laser light appears as a small spot even at long distances; the user places the spot on the desired target and the barrel of the gun is aligned (but not necessarily allowing for bullet drop, windage, distance between the direction of the beam and the axis of the barrel, and the target mobility while the bullet travels).
Most laser sights use a red laser diode. Others use an infrared diode to produce a dot invisible to the naked human eye but detectable with night vision devices. The firearms adaptive target acquisition module LLM01 laser light module combines visible and infrared laser diodes. In the late 1990s, green diode pumped solid state laser (DPSS) laser sights (532 nm) became available. Modern laser sights are small and light enough for attachment to the firearms.
In 2007, LaserMax, a company specializing in manufacturing lasers for military and police firearms, introduced the first mass-production green laser available for small arms. This laser mounts to the underside of a handgun or long arm on the accessory rail. The green laser is supposed to be more visible than the red laser in bright lighting conditions because, for the same wattage, green light appears brighter than red light.
Eye-targeted lasers.
A non-lethal laser weapon was developed by the U.S. Air Force to temporarily impair an adversary's ability to fire a weapon or to otherwise threaten enemy forces. This unit illuminates an opponent with harmless low-power laser light and can have the effect of dazzling or disorienting the subject or causing him to flee. Several types of dazzlers are now available, and some have been used in combat.
There remains the possibility of using lasers to blind, since this requires such lower power levels, and is easily achievable in a man-portable unit. However, most nations regard the deliberate permanent blinding of the enemy as forbidden by the rules of war (see Protocol on Blinding Laser Weapons). Although several nations have developed blinding laser weapons, such as China's ZM-87, none of these are believed to have made it past the prototype stage.
In addition to the applications that crossover with military applications, a widely known law enforcement use of lasers is for lidar to measure the speed of vehicles.
Holographic weapon sight.
A Holographic weapon sight uses a laser diode to illuminate a hologram of a reticle built into a flat glass optical window of the sight. The user looks through the optical window and sees a cross hair reticle image superimposed at a distance on the field of view.
Industrial and commercial.
Industrial laser applications can be divided into two categories depending on the power of the laser: material processing and micro-material processing.
In material processing, lasers with average optical power above 1 kilowatt are used mainly for industrial materials processing applications. Beyond this power threshold there are thermal issues related to the optics that separate these lasers from their lower-power counterparts. Laser systems in the 50-300W range are used primarily for pumping, plastic welding and soldering applications. Lasers above 300W are used in brazing, thin metal welding, and sheet metal cutting applications. The required brightness (as measured in by the beam parameter product) is higher for cutting applications than for brazing and thin metal welding. High power applications, such as hardening, cladding, and deep penetrating welding, require multiple kW of optical power, and are used in a broad range of industrial processes.
Micro material processing is a category that includes all laser material processing applications under 1 kilowatt. The use of lasers in Micro Materials Processing has found broad application in the development and manufacturing of screens for smartphones, tablet computers, and LED TVs.
A detailed list of industrial and commercial laser applications includes:
Surveying and ranging.
In surveying and construction, the laser level is affixed to a tripod, leveled and then spun to illuminate a horizontal plane. The laser beam projector employs a rotating head with a mirror for sweeping the laser beam about a vertical axis. If the mirror is not self-leveling, it is provided with visually readable level vials and manually adjustable screws for orienting the projector. A staff carried by the operator is equipped with a movable sensor, which can detect the laser beam and gives a signal when the sensor is in line with the beam (usually an audible beep). The position of the sensor on the graduated staff allows comparison of elevations between different points on the terrain.
A tower-mounted laser level is used in combination with a sensor on a wheel tractor-scraper in the process of land laser leveling to bring land (for example, an agricultural field) to near-flatness with a slight grade for drainage. The laser line level was invented in 1996 by Steve J. Orosz, Jr.[1] This type of level does not require a heavy motor to create the illusion of a line from a dot, rather, it uses a lens to transform the dot into a line.

</doc>
<doc id="18148" url="http://en.wikipedia.org/wiki?curid=18148" title="Left-arm orthodox spin">
Left-arm orthodox spin

Left-arm orthodox spin also known as Slow Left Arm Orthodox is a type of Left Arm Finger Leg spin bowling in the sport of cricket.
Left-arm orthodox spin is bowled by a left-arm bowler using the fingers to spin the ball from right to left of the cricket pitch (from the bowler's perspective).
Left arm orthodox spin bowlers generally attempt to drift the ball in the air into a right-handed batsman, and then turn it away from the batsman (towards off-stump) upon landing on the pitch. The drift and turn in the air are attacking techniques. The left-arm orthodox spin like an off break or off spin is also a bowling action.
The major variations of a left-arm spinner are the topspinner (which turns less and bounces higher in the cricket pitch), the arm ball (which does not turn at all, drifts into a right-handed batsman in the direction of the bowler's arm movement; also called a 'floater') and the left-arm spinner's version of a doosra (which turns the other way). The left-arm unorthodox spin like a leg break or leg spin is also a bowling action.

</doc>
<doc id="18151" url="http://en.wikipedia.org/wiki?curid=18151" title="Laser construction">
Laser construction

A laser is constructed from three principal parts:
Pump source.
The "pump source" is the part that provides energy to the laser system. Examples of pump sources include electrical discharges, flashlamps, arc lamps, light from another laser, chemical reactions and even explosive devices. The type of pump source used principally depends on the "gain medium", and this also determines how the energy is transmitted to the medium. A helium–neon (HeNe) laser uses an electrical discharge in the helium-neon gas mixture, a Nd:YAG laser uses either light focused from a xenon flash lamp or diode lasers, and excimer lasers use a chemical reaction.
Gain medium / Laser medium.
The "gain medium" is the major determining factor of the wavelength of operation, and other properties, of the laser. "Gain media" in different materials have linear spectra or wide spectra. "Gain media" with wide spectra allow tuning of the laser frequency. First wide "tunable" crystal laser with tunabulity more octave represent on photo 3 http://spie.org/x39922.xml . There are hundreds if not thousands of different gain media in which laser operation has been achieved (see list of laser types for a list of the most important ones). The gain medium is excited by the pump source to produce a population inversion, and it is in the gain medium that spontaneous and stimulated emission of photons takes place, leading to the phenomenon of optical gain, or amplification.
Examples of different gain media include:
Optical resonator.
The "optical resonator", or "optical cavity", in its simplest form is two parallel mirrors placed around the gain medium which provide feedback of the light. The mirrors are given optical coatings which determine their reflective properties. Typically one will be a high reflector, and the other will be a partial reflector. The latter is called the output coupler, because it allows some of the light to leave the cavity to produce the laser's output beam.
Light from the medium, produced by spontaneous emission, is reflected by the mirrors back into the medium, where it may be amplified by stimulated emission. The light may reflect from the mirrors and thus pass through the gain medium many hundreds of times before exiting the cavity. In more complex lasers, configurations with four or more mirrors forming the cavity are used. The design and alignment of the mirrors with respect to the medium is crucial to determining the exact operating wavelength and other attributes of the laser system.
Other optical devices, such as spinning mirrors, modulators, filters, and absorbers, may be placed within the optical resonator to produce a variety of effects on the laser output, such as altering the wavelength of operation or the production of pulses of laser light.
Some lasers do not use an optical cavity, but instead rely on very high optical gain to produce significant amplified spontaneous emission (ASE) without needing feedback of the light back into the gain medium. Such lasers are said to be superluminescent, and emit light with low coherence but high bandwidth. Since they do not use optical feedback, these devices are often not categorized as lasers.

</doc>
<doc id="18152" url="http://en.wikipedia.org/wiki?curid=18152" title="Logical conjunction">
Logical conjunction

In logic and mathematics, and is the truth-functional operator of logical conjunction; the "and" of a set of operands is true if and only if "all" of its operands are true. The logical connective that represents this operator is typically written as formula_1 or formula_2. 
""A" and "B"" is true only if "A" is true and "B" is true. 
An operand of a conjunction is a conjunct.
Related concepts in other fields are:
Notation.
And is usually expressed with an infix operator: in mathematics and logic, ∧; in electronics, formula_2; and in programming languages, & or and. In Jan Łukasiewicz's prefix notation for logic, the operator is K, for Polish "koniunkcja".
Definition.
Logical conjunction is an operation on two logical values, typically the values of two propositions, that produces a value of "true" if and only if both of its operands are true.
The conjunctive identity is 1, which is to say that AND-ing an expression with 1 will never change the value of the expression. In keeping with the concept of vacuous truth, when conjunction is defined as an operator or function of arbitrary arity, the empty conjunction (AND-ing over an empty set of operands) is often defined as having the result 1.
Truth table.
The truth table of formula_4:
Introduction and elimination rules.
As a rule of inference, conjunction introduction is a classically valid, simple argument form. The argument form has two premises, "A" and "B". Intuitively, it permits the inference of their conjunction.
or in logical operator notation:
Here is an example of an argument that fits the form "conjunction introduction":
Conjunction elimination is another classically valid, simple argument form. Intuitively, it permits the inference from any conjunction of either element of that conjunction.
...or alternately,
In logical operator notation:
...or alternately,
Properties.
commutativity: yes
associativity: yes
distributivity: with various operations, especially with "or"
idempotency: yes<br>
monotonicity: yes
truth-preserving: yes<br>
When all inputs are true, the output is true.
falsehood-preserving: yes<br>
When all inputs are false, the output is false.
Walsh spectrum: (1,-1,-1,1)
Nonlinearity: 1 (the function is bent)
If using binary values for true (1) and false (0), then "logical conjunction" works exactly like normal arithmetic multiplication.
Applications in computer engineering.
In high-level computer programming and digital electronics, logical conjunction is commonly represented by an infix operator, usually as a keyword such as "codice_1", an algebraic multiplication, or the ampersand symbol "codice_2". Many languages also provide short-circuit control structures corresponding to logical conjunction.
Logical conjunction is often used for bitwise operations, where codice_3 corresponds to false and codice_4 to true:
The operation can also be applied to two binary words viewed as bitstrings of equal length, by taking the bitwise AND of each pair of bits at corresponding positions. For example:
This can be used to select part of a bitstring using a bit mask. For example, codice_15  =  codice_16 extracts the fifth bit of an 8-bit bitstring.
In computer networking, bit masks are used to derive the network address of a subnet within an existing network from a given IP address, by ANDing the IP address and the subnet mask.
Logical conjunction "codice_1" is also used in SQL operations to form database queries.
The Curry-Howard correspondence relates logical conjunction to product types.
Set-theoretic correspondence.
The membership of an element of an intersection set in set theory is defined in terms of a logical conjunction: "x" ∈ "A" ∩ "B" if and only if ("x" ∈ "A") ∧ ("x" ∈ "B"). Through this correspondence, set-theoretic intersection shares several properties with logical conjunction, such as associativity, commutativity, and idempotence.
Natural language.
As with other notions formalized in mathematical logic, the logical conjunction "and" is related to, but not the same as, the grammatical conjunction "and" in natural languages.
English "and" has properties not captured by logical conjunction. For example, "and" sometimes implies order. For example, "They got married and had a child" in common discourse means that the marriage came before the child. The word "and" can also imply a partition of a thing into parts, as "The American flag is red, white, and blue." Here it is not meant that the flag is "at once" red, white, and blue, but rather that it has a part of each color.

</doc>
<doc id="18153" url="http://en.wikipedia.org/wiki?curid=18153" title="Logical connective">
Logical connective

In logic, a logical connective (also called a logical operator) is a symbol or word used to connect two or more sentences (of either a formal or a natural language) in a grammatically valid way, such that the sense of the compound sentence produced depends only on the original sentences.
The most common logical connectives are binary connectives (also called dyadic connectives) which join two sentences which can be thought of as the function's operands. Also commonly, negation is considered to be a unary connective.
Logical connectives along with quantifiers are the two main types of logical constants used in formal systems such as propositional logic and predicate logic. Semantics of a logical connective is often, but not always, presented as a truth function.
A logical connective is similar to but not equivalent to a conditional operator. 
In language.
Natural language.
In the grammar of natural languages two sentences may be joined by a grammatical conjunction to form a "grammatically" compound sentence. Some but not all such grammatical conjunctions are truth functions. For example, consider the following sentences:
The words "and" and "so" are "grammatical" conjunctions joining the sentences (A) and (B) to form the compound sentences (C) and (D). The "and" in (C) is a "logical" connective, since the truth of (C) is completely determined by (A) and (B): it would make no sense to affirm (A) and (B) but deny (C). However, "so" in (D) is not a logical connective, since it would be quite reasonable to affirm (A) and (B) but deny (D): perhaps, after all, Jill went up the hill to fetch a pail of water, not because Jack had gone up the hill at all.
Various English words and word pairs express logical connectives, and some of them are synonymous. Examples (with the name of the relationship in parentheses) are:
The word "not" (negation) and the phrases "it is false that" (negation) and "it is not the case that" (negation) also express a logical connective – even though they are applied to a single statement, and do not connect two statements.
Formal languages.
In formal languages, truth functions are represented by unambiguous symbols. These symbols are called "logical connectives", "logical operators", "propositional operators", or, in classical logic, "truth-functional connectives". See well-formed formula for the rules which allow new well-formed formulas to be constructed by joining other well-formed formulas using truth-functional connectives.
Logical connectives can be used to link more than two statements, so one can speak about "n-ary logical connective".
Common logical connectives.
List of common logical connectives.
Commonly used logical connectives include
Alternative names for biconditional are "iff", "xnor" and "bi-implication".
For example, the meaning of the statements "it is raining" and "I am indoors" is transformed when the two are combined with logical connectives:
For statement "P" = "It is raining" and "Q" = "I am indoors".
It is also common to consider the "always true" formula and the "always false" formula to be connective:
History of notations.
Some authors used letters for connectives at some time of the history: u. for conjunction (German's "und" for "and") and o. for disjunction (German's "oder" for "or") in earlier works by Hilbert (1904); N"p for negation, K"pq for conjunction, A"pq for disjunction, C"pq for implication, E"pq" for biconditional in Łukasiewicz (1929); cf. Polish notation.
Redundancy.
Such logical connective as converse implication ← is actually the same as material conditional with swapped arguments, so the symbol for converse implication is redundant. In some logical calculi (notably, in classical logic) certain essentially different compound statements are logically equivalent. A less trivial example of a redundancy is the classical equivalence between ¬"P" ∨ "Q" and "P" → "Q". Therefore, a classical-based logical system does not need the conditional operator "→" if "¬" (not) and "∨" (or) are already in use, or may use the "→" only as a syntactic sugar for a compound having one negation and one disjunction.
There are sixteen Boolean functions associating the input truth values P and Q with four-digit binary outputs. These correspond to possible choices of binary logical connectives for classical logic. Different implementation of classical logic can choose different functionally complete subsets of connectives.
One approach is to choose a "minimal" set, and define other connectives by some logical form, like in the example with material conditional above.
The following are the minimal functionally complete sets of operators in classical logic whose arities do not exceed 2:
See more details about functional completeness in classical logic at Functional completeness in truth function.
Another approach is to use on equal rights connectives of a certain convenient and functionally complete, but "not minimal" set. This approach requires more propositional axioms and each equivalence between logical forms must be either an axiom or provable as a theorem.
But intuitionistic logic has the situation more complicated. Of its five connectives {∧, ∨, →, ¬, ⊥} only negation ¬ has to be reduced to other connectives (see details). Neither of conjunction, disjunction and material conditional has an equivalent form constructed of other four logical connectives.
Properties.
Some logical connectives possess properties which may be expressed in the theorems containing the connective. Some of those properties that a logical connective may have are:
For classical and intuitionistic logic, the "=" symbol means that corresponding implications "…→…" and "…←…" for logical compounds can be both proved as theorems, and the "≤" symbol means that "…→…" for logical compounds is a consequence of corresponding "…→…" connectives for propositional variables. Some many-valued logics may have incompatible definitions of equivalence and order (entailment).
Both conjunction and disjunction are associative, commutative and idempotent in classical logic, most varieties of many-valued logic and intuitionistic logic. The same is true about distributivity of conjunction over disjunction and disjunction over conjunction, as well as for the absorption law.
In classical logic and some varieties of many-valued logic, conjunction and disjunction are dual, and negation is self-dual, the latter is also self-dual in intuitionistic logic. 
Order of precedence.
As a way of reducing the number of necessary parentheses, one may introduce precedence rules: formula_63 has higher precedence than formula_1, formula_1 higher than formula_18, and formula_18 higher than formula_3. So for example, formula_84 is short for formula_85.
Here is a table that shows a commonly used precedence of logical operators.
However not all authors use the same order; for instance, an ordering in which disjunction is lower precedence than implication or bi-implication has also been used. Sometimes precedence between conjunction and disjunction is unspecified requiring to provide it explicitly in given formula with parentheses. The order of precedence determines which connective is the "main connective" when interpreting a non-atomic formula.
Computer science.
A truth-functional approach to logical operators is implemented as logic gates in digital circuits. Practically all digital circuits (the major exception is DRAM) are built up from NAND, NOR, NOT, and transmission gates; see more details in Truth function in computer science. Logical operators over bit vectors (corresponding to finite Boolean algebras) are bitwise operations.
But not every usage of a logical connective in computer programming has a Boolean semantic. For example, lazy evaluation is sometimes implemented for "P" ∧ "Q" and "P" ∨ "Q", so these connectives are not commutative if some of expressions P, Q has side effects. Also, a conditional, which in some sense corresponds to the material conditional connective, is essentially non-Boolean because for codice_1 the consequent Q is not executed if the antecedent P is false (although a compound as a whole is successful ≈ "true" in such case). This is closer to intuitionist and constructivist views on the material conditional, rather than to classical logic's ones.

</doc>
<doc id="18154" url="http://en.wikipedia.org/wiki?curid=18154" title="Propositional calculus">
Propositional calculus

Propositional calculus (also called propositional logic, sentential calculus, or sentential logic) is the branch of mathematical logic concerned with the study of propositions (whether they are true or false) and formed by other propositions with the use of logical connectives, and how their value depends on the truth value of their components. Logical connectives are found in natural languages. In English for example, some examples are "and" (conjunction), "or" (disjunction), "not” (negation) and "if" (but only when used to denote material conditional).
The following is an example of a very simple inference within the scope of propositional logic:
Both premises and the conclusions are propositions. The premises are taken for granted and then with the application of modus ponens (an inference rule) the conclusion follows.
As propositional logic is not concerned with the structure of propositions beyond the point where they can't be decomposed anymore by logical connectives, this inference can be restated replacing those "atomic" statements with statement letters, which are interpreted as variables representing statements:
The same can be stated succinctly in the following way:
When P is interpreted as “It's raining” and Q as “it's cloudy” the above symbolic expressions can be seen to exactly correspond with the original expression in natural language. Not only that, but they will also correspond with any other inference of this "form", which will be valid on the same basis that this inference is.
Propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. A system of inference rules and axioms allows certain formulas to be derived. These derived formulas are called theorems and may be interpreted to be true propositions. A constructed sequence of such formulas is known as a "derivation" or "proof" and the last formula of the sequence is the theorem. The derivation may be interpreted as proof of the proposition represented by the theorem.
When a formal system is used to represent formal logic, only statement letters are represented directly. The natural language propositions that arise when they're interpreted are outside the scope of the system, and the relation between the formal system and its interpretation is likewise outside the formal system itself.
Usually in truth-functional propositional logic, formulas are interpreted as having either a truth value of "true" or a truth value of "false". Truth-functional propositional logic and systems isomorphic to it, are considered to be zeroth-order logic.
History.
Although propositional logic (which is interchangeable with propositional calculus) had been hinted by earlier philosophers, it was developed into a formal logic by Chrysippus in the 3rd century BC and expanded by the Stoics. The logic was focused on propositions. This advancement was different from the traditional syllogistic logic which was focused on terms. However, later in antiquity, the propositional logic developed by the Stoics was no longer understood . Consequently, the system was essentially reinvented by Peter Abelard in the 12th century.
Propositional logic was eventually refined using symbolic logic. The 17th/18th century philosopher Gottfried Leibniz has been credited with being the founder of symbolic logic for his work with the calculus ratiocinator. Although his work was the first of its kind, it was unknown to the larger logical community. Consequently, many of the advances achieved by Leibniz were reachieved by logicians like George Boole and Augustus De Morgan completely independent of Leibniz.
Just as propositional logic can be considered an advancement from the earlier syllogistic logic, Gottlob Frege's predicate logic was an advancement from the earlier propositional logic. One author describes predicate logic as combining "the distinctive features of syllogistic logic and propositional logic." Consequently, predicate logic ushered in a new era in logic's history; however, advances in propositional logic were still made after Frege, including Natural Deduction, Truth-Trees and Truth-Tables. Natural deduction was invented by Gerhard Gentzen and Jan Łukasiewicz. Truth-Trees were invented by Evert Willem Beth. The invention of truth-tables, however, is of controversial attribution.
Within works by Frege and Bertrand Russell, one finds ideas influential in bringing about the notion of truth tables. The actual 'tabular structure' (being formatted as a table), itself, is generally credited to either Ludwig Wittgenstein or Emil Post (or both, independently). Besides Frege and Russell, others credited with having ideas preceding truth-tables include Philo, Boole, Charles Sanders Peirce, and Ernst Schröder. Others credited with the tabular structure include Łukasiewicz, Schröder, Alfred North Whitehead, William Stanley Jevons, John Venn, and Clarence Irving Lewis. Ultimately, some have concluded, like John Shosky, that "It is far from clear that any one person should be given the title of 'inventor' of truth-tables.".
Terminology.
In general terms, a calculus is a formal system that consists of a set of syntactic expressions ("well-formed formulas"), a distinguished subset of these expressions (axioms), plus a set of formal rules that define a specific binary relation, intended to be interpreted to be logical equivalence, on the space of expressions.
When the formal system is intended to be a logical system, the expressions are meant to be interpreted to be statements, and the rules, known to be "inference rules", are typically intended to be truth-preserving. In this setting, the rules (which may include axioms) can then be used to derive ("infer") formulas representing true statements from given formulas representing true statements.
The set of axioms may be empty, a nonempty finite set, a countably infinite set, or be given by axiom schemata. A formal grammar recursively defines the expressions and well-formed formulas of the language. In addition a semantics may be given which defines truth and valuations (or interpretations).
The language of a propositional calculus consists of
A "well-formed formula" is any atomic formula, or any formula that can be built up from atomic formulas by means of operator symbols according to the rules of the grammar.
Mathematicians sometimes distinguish between propositional constants, propositional variables, and schemata. Propositional constants represent some particular proposition, while propositional variables range over the set of all atomic propositions. Schemata, however, range over all propositions. It is common to represent propositional constants by A, B, and C, propositional variables by P, Q, and R, and schematic letters are often Greek letters, most often φ, ψ, and χ.
Basic concepts.
The following outlines a standard propositional calculus. Many different formulations exist which are all more or less equivalent but differ in the details of:
Any given proposition may be represented with a letter called a 'propositional constant', analogous to representing a number by a letter in mathematics, for instance, . All propositions require exactly one of two truth-values: true or false. For example, let P be the proposition that it is raining outside. This will be true (P) if it is raining outside and false otherwise (¬"P").
It is extremely helpful to look at the truth tables for these different operators, as well as the method of analytic tableaux.
Closure under operations.
Propositional logic is closed under truth-functional connectives. That is to say, for any proposition φ, ¬"φ" is also a proposition. Likewise, for any propositions φ and ψ, "φ" ∧ "ψ" is a proposition, and similarly for disjunction, conditional, and biconditional. This implies that, for instance, "φ" ∧ "ψ" is a proposition, and so it can be conjoined with another proposition. In order to represent this, we need to use parentheses to indicate which proposition is conjoined with which. For instance, "P" ∧ "Q" ∧ "R" is not a well-formed formula, because we do not know if we are conjoining "P" ∧ "Q" with R or if we are conjoining P with "Q" ∧ "R". Thus we must write either ("P" ∧ "Q") ∧ "R" to represent the former, or "P" ∧ ("Q" ∧ "R") to represent the latter. By evaluating the truth conditions, we see that both expressions have the same truth conditions (will be true in the same cases), and moreover that any proposition formed by arbitrary conjunctions will have the same truth conditions, regardless of the location of the parentheses. This means that conjunction is associative, however, one should not assume that parentheses never serve a purpose. For instance, the sentence "P" ∧ ("Q" ∨ "R") does not have the same truth conditions of ("P" ∧ "Q") ∨ "R", so they are different sentences distinguished only by the parentheses. One can verify this by the truth-table method referenced above.
Note: For any arbitrary number of propositional constants, we can form a finite number of cases which list their possible truth-values. A simple way to generate this is by truth-tables, in which one writes P, Q, ..., Z, for any list of k propositional constants—that is to say, any list of propositional constants with k entries. Below this list, one writes 2"k" rows, and below P one fills in the first half of the rows with true (or T) and the second half with false (or F). Below Q one fills in one-quarter of the rows with T, then one-quarter with F, then one-quarter with T and the last quarter with F. The next column alternates between true and false for each eighth of the rows, then sixteenths, and so on, until the last propositional constant varies between T and F for each row. This will give a complete listing of cases or truth-value assignments possible for those propositional constants.
Argument.
The propositional calculus then defines an "argument" to be a set of propositions. A valid argument is a set of propositions, the last of which follows from—or is implied by—the rest. All other arguments are invalid. The simplest valid argument is modus ponens, one instance of which is the following set of propositions:
This is a set of three propositions, each line is a proposition, and the last follows from the rest. The first two lines are called premises, and the last line the conclusion. We say that any proposition C follows from any set of propositions formula_6, if C must be true whenever every member of the set formula_6 is true. In the argument above, for any P and Q, whenever "P" → "Q" and P are true, necessarily Q is true. Notice that, when P is true, we cannot consider cases 3 and 4 (from the truth table). When "P" → "Q" is true, we cannot consider case 2. This leaves only case 1, in which Q is also true. Thus Q is implied by the premises.
This generalizes schematically. Thus, where φ and ψ may be any propositions at all,
Other argument forms are convenient, but not necessary. Given a complete set of axioms (see below for one such set), modus ponens is sufficient to prove all other argument forms in propositional logic, thus they may be considered to be a derivative. Note, this is not true of the extension of propositional logic to other logics like first-order logic. First-order logic requires at least one additional rule of inference in order to obtain completeness.
The significance of argument in formal logic is that one may obtain new truths from established truths. In the first example above, given the two premises, the truth of Q is not yet known or stated. After the argument is made, Q is deduced. In this way, we define a deduction system to be a set of all propositions that may be deduced from another set of propositions. For instance, given the set of propositions formula_9, we can define a deduction system, Γ, which is the set of all propositions which follow from A. Reiteration is always assumed, so formula_10. Also, from the first element of A, last element, as well as modus ponens, R is a consequence, and so formula_11. Because we have not included sufficiently complete axioms, though, nothing else may be deduced. Thus, even though most deduction systems studied in propositional logic are able to deduce formula_12, this one is too weak to prove such a proposition.
Generic description of a propositional calculus.
A propositional calculus is a formal system formula_13, where:
The "language" of formula_15, also known as its set of "formulas", "well-formed formulas", is inductively defined by the following rules:
Repeated applications of these rules permits the construction of complex formulas. For example:
Example 1. Simple axiom system.
Let formula_37, where formula_14, formula_39, formula_25, formula_26 are defined as follows:
Example 2. Natural deduction system.
Let formula_58, where formula_14, formula_39, formula_25, formula_26 are defined as follows:
In the following example of a propositional calculus, the transformation rules are intended to be interpreted as the inference rules of a so-called "natural deduction system". The particular system presented here has no initial points, which means that its interpretation for logical applications derives its theorems from an empty axiom set.
Our propositional calculus has ten inference rules. These rules allow us to derive other true formulas given a set of formulas that are assumed to be true. The first nine simply state that we can infer certain well-formed formulas from other well-formed formulas. The last rule however uses hypothetical reasoning in the sense that in the premise of the rule we temporarily assume an (unproven) hypothesis to be part of the set of inferred formulas to see if we can infer a certain other formula. Since the first nine rules don't do this they are usually described as "non-hypothetical" rules, and the last one as a "hypothetical" rule.
In describing the transformation rules, we may introduce a metalanguage symbol formula_70. It is basically a convenient shorthand for saying "infer that". The format is formula_71, in which Γ is a (possibly empty) set of formulas called premises, and ψ is a formula called conclusion. The transformation rule formula_71 means that if every proposition in Γ is a theorem (or has the same truth value as the axioms), then ψ is also a theorem. Note that considering the following rule Conjunction introduction, we will know whenever Γ has more than one formula, we can always safely reduce it into one formula using conjunction. So for short, from that time on we may represent Γ as one formula instead of a set. Another omission for convenience is when Γ is an empty set, in which case Γ may not appear.
Proofs in propositional calculus.
One of the main uses of a propositional calculus, when interpreted for logical applications, is to determine relations of logical equivalence between propositional formulas. These relationships are determined by means of the available transformation rules, sequences of which are called "derivations" or "proofs".
In the discussion to follow, a proof is presented as a sequence of numbered lines, with each line consisting of a single formula followed by a "reason" or "justification" for introducing that formula. Each premise of the argument, that is, an assumption introduced as an hypothesis of the argument, is listed at the beginning of the sequence and is marked as a "premise" in lieu of other justification. The conclusion is listed on the last line. A proof is complete if every line follows from the previous ones by the correct application of a transformation rule. (For a contrasting approach, see proof-trees).
Example of a proof.
Interpret formula_110 as "Assuming A, infer A". Read formula_111 as "Assuming nothing, infer that A implies A", or "It is a tautology that A implies A", or "It is always true that A implies A".
Soundness and completeness of the rules.
The crucial properties of this set of rules are that they are "sound" and "complete". Informally this means that the rules are correct and that no other rules are required. These claims can be made more formal as follows.
We define a "truth assignment" as a function that maps propositional variables to true or false. Informally such a truth assignment can be understood as the description of a possible state of affairs (or possible world) where certain statements are true and others are not. The semantics of formulas can then be formalized by defining for which "state of affairs" they are considered to be true, which is what is done by the following definition.
We define when such a truth assignment A satisfies a certain well-formed formula with the following rules:
With this definition we can now formalize what it means for a formula φ to be implied by a certain set S of formulas. Informally this is true if in all worlds that are possible given the set of formulas S the formula φ also holds. This leads to the following formal definition: We say that a set S of well-formed formulas "semantically entails" (or "implies") a certain well-formed formua φ if all truth assignments that satisfy all the formulas in S also satisfy φ.
Finally we define "syntactical entailment" such that φ is syntactically entailed by S if and only if we can derive it with the inference rules that were presented above in a finite number of steps. This allows us to formulate exactly what it means for the set of inference rules to be sound and complete:
Soundness: If the set of well-formed formulas S syntactically entails the well-formed formula φ then S semantically entails φ.
Completeness: If the set of well-formed formulas S semantically entails the well-formed formula φ then S syntactically entails φ.
For the above set of rules this is indeed the case.
Sketch of a soundness proof.
Notational conventions: Let G be a variable ranging over sets of sentences. Let A, B and C range over sentences. For "G syntactically entails A" we write "G proves A". For "G semantically entails A" we write "G implies A".
We want to show: ("A")("G") (if G proves A, then G implies A).
We note that "G proves A" has an inductive definition, and that gives us the immediate resources for demonstrating claims of the form "If G proves A, then ...". So our proof proceeds by induction.
Notice that Basis Step II can be omitted for natural deduction systems because they have no axioms. When used, Step II involves showing that each of the axioms is a (semantic) logical truth.
The Basis steps demonstrate that the simplest provable sentences from G are also implied by G, for any G. (The proof is simple, since the semantic fact that a set implies any of its members, is also trivial.) The Inductive step will systematically cover all the further sentences that might be provable—by considering each case where we might reach a logical conclusion using an inference rule—and shows that if a new sentence is provable, it is also logically implied. (For example, we might have a rule telling us that from "A" we can derive "A or B". In III.a We assume that if A is provable it is implied. We also know that if A is provable then "A or B" is provable. We have to show that then "A or B" too is implied. We do so by appeal to the semantic definition and the assumption we just made. A is provable from G, we assume. So it is also implied by G. So any semantic valuation making all of G true makes A true. But any valuation making A true makes "A or B" true, by the defined semantics for "or". So any valuation which makes all of G true makes "A or B" true. So "A or B" is implied.) Generally, the Inductive step will consist of a lengthy but simple case-by-case analysis of all the rules of inference, showing that each "preserves" semantic implication.
By the definition of provability, there are no sentences provable other than by being a member of G, an axiom, or following by a rule; so if all of those are semantically implied, the deduction calculus is sound.
Sketch of completeness proof.
We adopt the same notational conventions as above.
We want to show: If G implies A, then G proves A. We proceed by contraposition: We show instead that if G does not prove A then G does not imply A.
QED
Another outline for a completeness proof.
If a formula is a tautology, then there is a truth table for it which shows that each valuation yields the value true for the formula. Consider such a valuation. By mathematical induction on the length of the subformulas, show that the truth or falsity of the subformula follows from the truth or falsity (as appropriate for the valuation) of each propositional variable in the subformula. Then combine the lines of the truth table together two at a time by using "(P is true implies S) implies ((P is false implies S) implies S)". Keep repeating this until all dependencies on propositional variables have been eliminated. The result is that we have proved the given tautology. Since every tautology is provable, the logic is complete.
Interpretation of a truth-functional propositional calculus.
An interpretation of a truth-functional propositional calculus formula_112 is an assignment to each propositional symbol of formula_112 of one or the other (but not both) of the truth values truth (T) and falsity (F), and an assignment to the connective symbols of formula_112 of their usual truth-functional meanings. An interpretation of a truth-functional propositional calculus may also be expressed in terms of truth tables.
For formula_115 distinct propositional symbols there are formula_116 distinct possible interpretations. For any particular symbol formula_117, for example, there are formula_118 possible interpretations:
For the pair formula_117, formula_122 there are formula_123 possible interpretations:
Since formula_112 has formula_129, that is, denumerably many propositional symbols, there are formula_130, and therefore uncountably many distinct possible interpretations of formula_112.
Interpretation of a sentence of truth-functional propositional logic.
If φ and ψ are formulas of formula_112 and formula_133 is an interpretation of formula_112 then:
Some consequences of these definitions:
Alternative calculus.
It is possible to define another version of propositional calculus, which defines most of the syntax of the logical operators by means of axioms, and which uses only one inference rule.
Axioms.
Let φ, χ, and ψ stand for well-formed formulas. (The well-formed formulas themselves would not contain any Greek letters, but only capital Roman letters, connective operators, and parentheses.) Then the axioms are as follows:
Inference rule.
The inference rule is modus ponens:
Meta-inference rule.
Let a demonstration be represented by a sequence, with hypotheses to the left of the turnstile and the conclusion to the right of the turnstile. Then the deduction theorem can be stated as follows:
This deduction theorem (DT) is not itself formulated with propositional calculus: it is not a theorem of propositional calculus, but a theorem about propositional calculus. In this sense, it is a meta-theorem, comparable to theorems about the soundness or completeness of propositional calculus.
On the other hand, DT is so useful for simplifying the syntactical proof process that it can be considered and used as another inference rule, accompanying modus ponens. In this sense, DT corresponds to the natural conditional proof inference rule which is part of the first version of propositional calculus introduced in this article.
The converse of DT is also valid:
in fact, the validity of the converse of DT is almost trivial compared to that of DT:
The converse of DT has powerful implications: it can be used to convert an axiom into an inference rule. For example, the axiom AND-1,
can be transformed by means of the converse of the deduction theorem into the inference rule
which is conjunction elimination, one of the ten inference rules used in the first version (in this article) of the propositional calculus.
Example of a proof.
The following is an example of a (syntactical) demonstration, involving only axioms THEN-1 and THEN-2:
Prove: formula_167 (Reflexivity of implication).
Proof:
Equivalence to equational logics.
The preceding alternative calculus is an example of a Hilbert-style deduction system. In the case of propositional systems the axioms are terms built with logical connectives and the only inference rule is modus ponens. Equational logic as standardly used informally in high school algebra is a different kind of calculus from Hilbert systems. Its theorems are equations and its inference rules express the properties of equality, namely that it is a congruence on terms that admits substitution.
Classical propositional calculus as described above is equivalent to Boolean algebra, while intuitionistic propositional calculus is equivalent to Heyting algebra. The equivalence is shown by translation in each direction of the theorems of the respective systems. Theorems formula_176 of classical or intuitionistic propositional calculus are translated as equations formula_177 of Boolean or Heyting algebra respectively. Conversely theorems formula_178 of Boolean or Heyting algebra are translated as theorems formula_179 of classical or intuitionistic calculus respectively, for which formula_180 is a standard abbreviation. In the case of Boolean algebra formula_178 can also be translated as formula_182, but this translation is incorrect intuitionistically.
In both Boolean and Heyting algebra, inequality formula_183 can be used in place of equality. The equality formula_178 is expressible as a pair of inequalities formula_183 and formula_186. Conversely the inequality formula_183 is expressible as the equality formula_188, or as formula_189. The significance of inequality for Hilbert-style systems is that it corresponds to the latter's deduction or entailment symbol formula_70. An entailment
is translated in the inequality version of the algebraic framework as
Conversely the algebraic inequality formula_183 is translated as the entailment
The difference between implication formula_195 and inequality or entailment formula_183 or formula_194 is that the former is internal to the logic while the latter is external. Internal implication between two terms is another term of the same kind. Entailment as external implication between two terms expresses a metatruth outside the language of the logic, and is considered part of the metalanguage. Even when the logic under study is intuitionistic, entailment is ordinarily understood classically as two-valued: either the left side entails, or is less-or-equal to, the right side, or it is not.
Similar but more complex translations to and from algebraic logics are possible for natural deduction systems as described above and for the sequent calculus. The entailments of the latter can be interpreted as two-valued, but a more insightful interpretation is as a set, the elements of which can be understood as abstract proofs organized as the morphisms of a category. In this interpretation the cut rule of the sequent calculus corresponds to composition in the category. Boolean and Heyting algebras enter this picture as special categories having at most one morphism per homset, i.e., one proof per entailment, corresponding to the idea that existence of proofs is all that matters: any proof will do and there is no point in distinguishing them.
Graphical calculi.
It is possible to generalize the definition of a formal language from a set of finite sequences over a finite basis to include many other sets of mathematical structures, so long as they are built up by finitary means from finite materials. What's more, many of these families of formal structures are especially well-suited for use in logic.
For example, there are many families of graphs that are close enough analogues of formal languages that the concept of a calculus is quite easily and naturally extended to them. Indeed, many species of graphs arise as "parse graphs" in the syntactic analysis of the corresponding families of text structures. The exigencies of practical computation on formal languages frequently demand that text strings be converted into pointer structure renditions of parse graphs, simply as a matter of checking whether strings are well-formed formulas or not. Once this is done, there are many advantages to be gained from developing the graphical analogue of the calculus on strings. The mapping from strings to parse graphs is called "parsing" and the inverse mapping from parse graphs to strings is achieved by an operation that is called "traversing" the graph.
Other logical calculi.
Propositional calculus is about the simplest kind of logical calculus in current use. It can be extended in several ways. (Aristotelian "syllogistic" calculus, which is largely supplanted in modern logic, is in "some" ways simpler – but in other ways more complex – than propositional calculus.) The most immediate way to develop a more complex logical calculus is to introduce rules that are sensitive to more fine-grained details of the sentences being used.
First-order logic (a.k.a. first-order predicate logic) results when the "atomic sentences" of propositional logic are broken up into terms, variables, predicates, and quantifiers, all keeping the rules of propositional logic with some new ones introduced. (For example, from "All dogs are mammals" we may infer "If Rover is a dog then Rover is a mammal".) With the tools of first-order logic it is possible to formulate a number of theories, either with explicit axioms or by rules of inference, that can themselves be treated as logical calculi. Arithmetic is the best known of these; others include set theory and mereology. Second-order logic and other higher-order logics are formal extensions of first-order logic. Thus, it makes sense to refer to propositional logic as "zeroth-order logic", when comparing it with these logics.
Modal logic also offers a variety of inferences that cannot be captured in propositional calculus. For example, from "Necessarily p" we may infer that p. From p we may infer "It is possible that p". The translation between modal logics and algebraic logics concerns classical and intuitionistic logics but with the introduction of a unary operator on Boolean or Heyting algebras, different from the Boolean operations, interpreting the possibility modality, and in the case of Heyting algebra a second operator interpreting necessity (for Boolean algebra this is redundant since necessity is the De Morgan dual of possibility). The first operator preserves 0 and disjunction while the second preserves 1 and conjunction.
Many-valued logics are those allowing sentences to have values other than "true" and "false". (For example, "neither" and "both" are standard "extra values"; "continuum logic" allows each sentence to have any of an infinite number of "degrees of truth" between "true" and "false".) These logics often require calculational devices quite distinct from propositional calculus. When the values form a Boolean algebra (which may have more than two or even infinitely many values), many-valued logic reduces to classical logic; many-valued logics are therefore only of independent interest when the values form an algebra that is not Boolean.
Solvers.
Finding solutions to propositional logic formulas is an NP-complete problem. However, practical methods exist (e.g., DPLL algorithm, 1962; Chaff algorithm, 2001) that are very fast for many useful cases. Recent work has extended the SAT solver algorithms to work with propositions containing arithmetic expressions; these are the SMT solvers.

</doc>
<doc id="18155" url="http://en.wikipedia.org/wiki?curid=18155" title="Lazy evaluation">
Lazy evaluation

In programming language theory, lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing). The sharing can reduce the running time of certain functions by an exponential factor over other non-strict evaluation strategies, such as call-by-name. 
The benefits of lazy evaluation include: 
Lazy evaluation is often combined with memoization, as described in Jon Bentley's "Writing Efficient Programs". After a function's value is computed for that parameter or set of parameters, the result is stored in a lookup table that is indexed by the values of those parameters; the next time the function is called, the table is consulted to determine whether the result for that combination of parameter values is already available. If so, the stored result is simply returned. If not, the function is evaluated and another entry is added to the lookup table for reuse.
Lazy evaluation can lead to reduction in memory footprint, since values are created when needed. However, lazy evaluation is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate. Lazy evaluation can introduce space leaks.
The opposite of lazy evaluation is eager evaluation, sometimes known as strict evaluation. Eager evaluation is the evaluation strategy employed in most programming languages.
History.
Lazy evaluation was introduced for lambda calculus by Christopher Wadsworth and for programming languages independently by Peter Henderson & James H. Morris and Daniel P. Friedman & David S. Wise.
Applications.
Delayed evaluation is used particularly in functional programming languages. When using delayed evaluation, an expression is not evaluated as soon as it gets bound to a variable, but when the evaluator is forced to produce the expression's value. That is, a statement such as codice_1 (i.e. the assignment of the result of an expression to a variable) clearly calls for the expression to be evaluated and the result placed in codice_2, but what actually is in codice_2 is irrelevant until there is a need for its value via a reference to codice_2 in some later expression whose evaluation could itself be deferred, though eventually the rapidly growing tree of dependencies would be pruned to produce some symbol rather than another for the outside world to see.
Delayed evaluation has the advantage of being able to create calculable infinite lists without infinite loops or size matters interfering in computation. For example, one could create a function that creates an infinite list (often called a "stream") of Fibonacci numbers. The calculation of the "n"-th Fibonacci number would be merely the extraction of that element from the infinite list, forcing the evaluation of only the first n members of the list.
For example, in the Haskell programming language, the list of all Fibonacci numbers can be written as:
In Haskell syntax, "codice_5" prepends an element to a list, codice_6 returns a list without its first element, and codice_7 uses a specified function (in this case addition) to combine corresponding elements of two lists to produce a third.
Provided the programmer is careful, only the values that are required to produce a particular result are evaluated. However, certain calculations may result in the program attempting to evaluate an infinite number of elements; for example, requesting the length of the list or trying to sum the elements of the list with a fold operation would result in the program either failing to terminate or running out of memory.
Control structures.
In many common "eager" languages, "if" statements evaluate in a lazy fashion.
 if a then b else c
evaluates (a), then if and only if (a) evaluates to true does it evaluate (b), otherwise it evaluates (c). That is, either (b) or (c) will not be evaluated. Conversely, in an eager language the expected behavior is that
 define f(x, y) = 2 * x
 set k = f(d, e)
will still evaluate (e) when computing the value of f(d, e) even though (e) is unused in function f. However, user-defined control structures depend on exact syntax, so for example
 define g(a, b, c) = if a then b else c
 l = g(h, i, j)
(i) and (j) would both be evaluated in an eager language. While in
 l' = if h then i else j
(i) or (j) would be evaluated, but never both.
Lazy evaluation allows control structures to be defined normally, and not as primitives or compile-time techniques. If (i) or (j) have side effects or introduce run time errors, the subtle differences between (l) and (l') can be complex. It is usually possible to introduce user-defined lazy control structures in eager languages as functions, though they may depart from the language's syntax for eager evaluation: Often the involved code bodies (like (i) and (j)) need to be wrapped in a function value, so that they are executed only when called.
Short-circuit evaluation of Boolean control structures is sometimes called "lazy".
Working with infinite data structures.
Many languages offer the notion of "infinite data-structures". These allow definitions of data to be given in terms of infinite ranges, or unending recursion, but the actual values are only computed when needed. Take for example this trivial program in Haskell:
In the function numberFromInfiniteList, the value of infinity is an infinite range, but until an actual value (or more specifically, a specific value at a certain index) is needed, the list is not evaluated, and even then it is only evaluated as needed (that is, until the desired index.)
Other uses.
In computer windowing systems, the painting of information to the screen is driven by "expose events" which drive the display code at the last possible moment. By doing this, windowing systems avoid computing unnecessary display content updates.
Another example of laziness in modern computer systems is copy-on-write page allocation or demand paging, where memory is allocated only when a value stored in that memory is changed.
Laziness can be useful for high performance scenarios. An example is the Unix mmap function, which provides "demand driven" loading of pages from disk, so that only those pages actually touched are loaded into memory, and unneeded memory is not allocated.
MATLAB implements "copy on edit", where arrays which are copied have their actual memory storage replicated only when their content is changed, possibly leading to an "out of memory" error when updating an element afterwards instead of during the copy operation.
Implementation.
Some programming languages delay evaluation of expressions by default, and some others provide functions or special syntax to delay evaluation. In Miranda and Haskell, evaluation of function arguments is delayed by default. In many other languages, evaluation can be delayed by explicitly suspending the computation using special syntax (as with Scheme's "codice_8" and "codice_9" and OCaml's "codice_10" and "codice_11") or, more generally, by wrapping the expression in a thunk. The object representing such an explicitly delayed evaluation is called a "lazy future." Perl 6 uses lazy evaluation of lists, so one can assign infinite lists to variables and use them as arguments to functions, but unlike Haskell and Miranda, Perl 6 doesn't use lazy evaluation of arithmetic operators and functions by default.
Laziness and eagerness.
Controlling eagerness in lazy languages.
In lazy programming languages such as Haskell, although the default is to evaluate expressions only when they are demanded, it is possible in some cases to make code more eager—or conversely, to make it more lazy again after it has been made more eager. This can be done by explicitly coding something which forces evaluation (which may make the code more eager) or avoiding such code (which may make the code more lazy). "Strict" evaluation usually implies eagerness, but they are technically different concepts.
However, there is an optimisation implemented in some compilers called strictness analysis, which, in some cases, allows the compiler to infer that a value will always be used. In such cases, this may render the programmer's choice of whether to force that particular value or not, irrelevant, because strictness analysis will force strict evaluation.
In Haskell, marking constructor fields strict means that their values will always be demanded immediately. The codice_12 function can also be used to demand a value immediately and then pass it on, which is useful if a constructor field should generally be lazy. However, neither of these techniques implements "recursive" strictness—for that, a function called codice_13 was invented.
Also, pattern matching in Haskell 98 is strict by default, so the codice_14 qualifier has to be used to make it lazy. 
Simulating laziness in eager languages.
In Python 2.x the codice_15 function computes a list of integers. The entire list is stored in memory when the first assignment statement is evaluated, so this is an example of eager or immediate evaluation:
In Python 3.x the codice_15 function returns a special range object which computes elements of the list on demand. Elements of the range object are only generated when they are needed (e.g., when codice_17 is evaluated in the following example), so this is an example of lazy or deferred evaluation:
From version 2.2 forward, Python manifests lazy evaluation by implementing iterators (lazy sequences) unlike tuple or list sequences. For instance (Python 2):
In the .NET Framework it is possible to do lazy evaluation using the class System.Lazy<T>. The class can be easily exploited in F# using the lazy keyword, while the force method will force the evaluation. There are also specialized collections like Microsoft.FSharp.Collections.Seq that provide built-in support for lazy evaluation. 
let fibonacci = Seq.unfold (fun (x, y) -> Some(x, (y, x + y))) (0I,1I)
fibonacci |> Seq.nth 1000
In C# and VB.NET, the class System.Lazy<T> is directly used. 
public int Sum()
 int a = 0;
 int b = 0; 
 Lazy<int> x = new Lazy<int>(() => a + b);
 a = 3;
 b = 5;
 return x.Value; // returns 8
Or with a more practical example: 
// recursive calculation of the n'th fibonacci number
public int Fib(int n)
 return (n == 1)? 1 : (n == 2)? 1 : Fib(n-1) + Fib(n-2);
public void Main()
 Console.WriteLine("Which Fibonacci number do you want to calculate?");
 int n = Int32.Parse(Console.Readline()); 
 Lazy<int> fib = new Lazy<int>(() => Fib(n)); // function is prepared, but not executed
 bool execute; 
 if(n > 100)
 Console.WriteLine("This can take some time. Do you really want to calculate this large number? [y/n]");
 execute = (Console.Readline() == "y"); 
 else execute = true;
 if(execute) Console.WriteLine(fib.Value); // number is only calculated if needed
Another way is to use the yield keyword: 
// eager evaluation 
public IEnumerable<int> Fibonacci(int x)
 IList<int> fibs = new List<int>();
 int prev = -1;
 int next = 1;
 for (int i = 0; i < x; i++)
 int sum = prev + next;
 prev = next;
 next = sum;
 fibs.Add(sum); 
 return fibs;
// lazy evaluation 
public IEnumerable<int> LazyFibonacci(int x)
 int prev = -1;
 int next = 1;
 for (int i = 0; i < x; i++)
 int sum = prev + next;
 prev = next;
 next = sum;
 yield return sum;

</doc>
<doc id="18156" url="http://en.wikipedia.org/wiki?curid=18156" title="Lemuridae">
Lemuridae

Lemuridae is a family of strepsirrhine primates native to Madagascar, and one of five families commonly known as lemurs. These animals were once thought to be the evolutionary predecessors of monkeys and apes, but this is no longer considered correct.
Characteristics.
Lemurids are medium-sized arboreal primates, ranging from 32 to 56 cm in length, excluding the tail, and weighing from 0.7 to 5 kg. They have long, bushy tails and soft, woolly fur of varying coloration. The hindlegs are slightly longer than the forelegs, although not enough to hamper fully quadrupedal movement (unlike the sportive lemurs). Most species are highly agile, and regularly leap several metres between trees. They have a good sense of smell and binocular vision. Unlike most other lemurs, all but one species of lemurid (the Ring-tailed Lemur) lack a tapetum lucidum, a reflective layer in the eye that improves night vision.
Lemurids are herbivorous, eating fruit, leaves, and, in some cases, nectar. For the most part, they have the dental formula: 2.1.3.32.1.3.3
With most lemurids, the mother gives birth to one or two young after a gestation period of between 120 and 140 days, depending on species. The ruffed lemur species are the only lemurids that have true litters, consisting of anywhere from two to six offspring. They are generally sociable animals, living in groups of up to thirty individuals in some species. In some cases, such as the Ring-tailed Lemur, the groups are long-lasting, with distinct dominance hierarchies, while in others, such as the Common Brown Lemur, the membership of the groups varies from day to day, and seems to have no clear social structure.
Classification.
The family Lemuridae contains 21 extant species in five genera.
FAMILY LEMURIDAE
This family was once broken into two subfamilies, Hapalemurinae (bamboo lemurs and the greater bamboo lemur) and Lemurinae (the rest of the family), but molecular evidence and the similarity of the scent glands have since placed the ring-tailed lemur with the bamboo lemurs and the greater bamboo Lemur.
Lemur species in the "Eulemur" genus are known to interbreed, despite having dramatically different chromosome numbers. Red-fronted (2N=60) and Collared (2N=50–52) Brown Lemurs were found to hybridize at Berenty Reserve, Madagascar.

</doc>
<doc id="18157" url="http://en.wikipedia.org/wiki?curid=18157" title="Lucent">
Lucent

Lucent Technologies, Inc. was an American multinational telecommunications equipment company headquartered in Murray Hill, New Jersey, in the United States. It was established on September 30, 1996 through the divestiture of the former AT&T Technologies business unit of AT&T Corporation, which included Western Electric and Bell Labs.
Lucent was merged with Alcatel SA of France in a merger of equals on December 1, 2006, forming Alcatel-Lucent.
Name.
Lucent means "light-bearing" in Latin. The name was applied in 1996 at the time of the split from AT&T.
The name was widely criticised, as the logo was to be, both internally and externally. Corporate communications and business cards included the strapline 'Bell Labs Innovations' in a bid to retain the prestige of the internationally famous research lab, within a new business under an as-yet unknown name.
This same linguistic root also gives Lucifer, "the light bearer" (from lux, 'light', and ferre, 'to bear'), who is also a character in Dante's epic poem "Inferno". Shortly after the Lucent renaming in 1996, the Plan 9 project released a development of their work as the Inferno OS in 1997. This extended the 'Lucifer' and Dante references as a series of punning names for the components of Inferno - Dis, Limbo, Charon and Styx.
When the rights to Inferno were sold in 2000, the company Vita Nuova Holdings was formed to represent them. This continues the Dante theme, although moving away from his "Divine Comedy" to the poem "La Vita Nuova".
Logo.
The Lucent logo, the Innovation Ring, was designed by Landor Associates, a prominent San Francisco-based branding consultancy. One source inside Lucent says that the logo is actually a Zen Buddhist symbol for "eternal truth", the Enso, turned 90 degrees and modified. Another source says it represents the mythic ouroboros, a snake holding its tail in its mouth. Lucent's logo also has been said to represent constant re-creating and re-thinking. Carly Fiorina picked the logo because her mother was a painter and she rejected the sterile geometric logos of most high tech companies.
After the logo was compared in the media to the ring a coffee mug leaves on paper, a "Dilbert" comic strip showed Dogbert as an overpaid consultant designing a new company logo; he takes a piece of paper that his coffee cup was sitting on and calls it the "Brown Ring of Quality". A telecommunication commentator referred to the logo as "a big red zero" and predicted financial losses.
History.
Two of the primary reasons AT&T Corporation chose to spin off its equipment manufacturing business was to permit it to profit from sales to competing telecommunications providers; these customers had previously shown reluctance to purchase from a direct competitor. Bell Labs brought prestige to the new company, as well as the revenue from thousands of patents. Lucent also operated from 666 Fifth Avenue, NY.
At the time of its spinoff, Lucent was placed under the leadership of Henry Schacht, who was brought in to oversee its transition from an arm of AT&T into an independent corporation. Richard McGinn, who was serving as President and COO, succeeded Schacht as CEO in 1997 while Schacht remained chairman of the board. Lucent became a "darling" stock of the investment community in the late 1990s, rising from a split-adjusted spinoff price of $7.56/share to a high of $84. Its market capitalization reached a high of $258 billion, and it was at the time the most widely held company with 5.3 million shareholders.
In 1997, Lucent acquired Milpitas-based voice mail market leader Octel Communications Corporation for $2.1 billion, a move which immediately rendered the Business Systems Group profitable. By 1999 Lucent stock continued to soar and in that year Lucent acquired Ascend Communications, an Alameda, California–based manufacturer of communications equipment for US$24 billion. Lucent held discussions to acquire Juniper Networks but decided instead to build its own routers internally.
However at the start of 2000, Lucent's "private bubble burst", while other competitors like Nortel Networks and Alcatel were still going strong as it would be many months before the rest of the telecom industry bubble collapsed. Previously Lucent had 14 straight quarters where it exceeded analysts' expectations, leading to high expectations for the 15th quarter, ending Dec. 31, 1999. On January 6, 2000, Lucent made the first of a string of announcements that it had missed its quarterly estimates, as CEO Rich McGinn grimly announced that Lucent had run into special problems during that quarter—including disruptions in its optical networking business—and reported flat revenues and a big drop in profits. That caused the stock to plunge by 28%, shaving $64 billion off of the company's market capitalization. When it was later revealed that it had used dubious accounting and sales practices to generate some of its earlier quarterly numbers, Lucent fell from grace. It was said that "Rich McGinn that couldn't accept Lucent's fall from its early triumphs." He described himself once as imposing "audacious" goals on his managers, believing the stretch for performance would produce dream results. Henry Schacht defended the corporate culture that McGinn created and also noted that McGinn did not sell any Lucent shares while serving as CEO. In November 2000, it also disclosed to the Securities and Exchange Commission that it had a $125 million accounting error for the third quarter of 2000, and by December 2000 it reported it had overstated its revenues for its latest quarter by nearly $700 million. Although no wrongdoing was found on his part, McGinn was forced to resign as CEO and he was replaced by Schacht on an interim basis. Subsequently, its CFO, Deborah Hopkins, left the company in May 2001 with Lucent's stock at $9.06 whereas at the time she was hired it was at $46.82.
In 2001 there were merger discussions between Lucent and Alcatel, which would have seen Lucent acquired at its current market price without a premium and the newly combined entity would have been headquartered in Murray Hill. However these negotiations collapsed when Schacht insisted on an equal 7-7 split of the merged company's board of directors, while Alcatel chief executive officer Serge Tchuruk wanted 8 of the 14 board seats for Alcatel due to it being in a stronger position. The failure of the merger talks caused Lucent's share price to collapse, and by October 2002 the stock price had bottomed at 55 cents per share.
Patricia Russo, formerly Lucent's EVP of the Corporate Office who then left for Eastman Kodak to serve as COO, was named permanent Chairman and CEO of Lucent in 2002, succeeding Schacht who remained on the Board of Directors.
In April 2000, Lucent sold its Consumer Products unit to VTech and Consumer Phone Services. In October 2000, Lucent spun off its Business Systems arm into Avaya, Inc., and in June 2002, it spun off its microelectronics division into Agere Systems. The spinoffs of enterprise networking and wireless, the industry's key growth businesses from 2003 onward, meant that Lucent no longer had the capacity to serve this market.
Lucent was reduced to 30,500 employees, down from about 165,000 employees at its zenith. The layoffs of so many experienced employees meant that the company was in a weakened position and unable to reestablish itself when the market recovered in 2003. By early 2003 Lucent's market value was $15.6 billion (which includes $6.8 billion of current value for two companies that Lucent had recently spun off, Avaya and Agere Systems), make the shares worth around $2.13, a far cry from its dotcom bubble peak of around $84, when Lucent was worth $258 billion.
Lucent continued to be active in the areas of telephone switching, optical, data and wireless networking.
On April 2, 2006, Lucent announced a merger agreement with Alcatel, which was 1.5 times the size of Lucent. Serge Tchuruk became non-executive chairman, and Russo served as CEO of the newly merged company, Alcatel-Lucent, until they were both forced to resign at the end of 2008. The merger failed to produce the expected synergies, and there were significant write-downs of Lucent's assets that Alcatel purchased.
Operations.
Divisions.
Lucent was divided into several core groups:
Murray Hill facility.
The Murray Hill anechoic chamber, built in 1940, is the world's oldest wedge-based anechoic chamber. The interior room measures approximately 30 ft high by 28 ft wide by 32 ft deep. The exterior concrete and brick walls are about 3 ft thick to keep outside noise from entering the chamber. The chamber absorbs over 99.995% of the incident acoustic energy above 200 Hz. At one time the Murray Hill chamber was cited in the Guinness Book of World Records as the world's quietest room. It is possible to hear the sounds of skeletal joints and heart beats very prominently.
The Murray Hill facility was the global headquarters for Lucent Technologies. The Murray Hill facility also has the largest copper-roof in the world. When Lucent Technologies was experiencing financial troubles in 2000 and 2001, one out of every three fluorescent lights was turned off in the facility. The same was done in the Naperville and Allentown, Pennsylvania facilities for a while. The facility had a cricket field and featured a nearby station from which enthusiasts could control RC airplanes and helicopters.

</doc>
<doc id="18158" url="http://en.wikipedia.org/wiki?curid=18158" title="Lupercalia">
Lupercalia

Lupercalia was a very ancient, possibly pre-Roman pastoral festival, observed on February 13 through 15, to avert evil spirits and purify the city, releasing health and fertility. Lupercalia subsumed Februa, an earlier-origin spring cleansing ritual held on the same date, which gives the month of February "(Februarius)" its name.
The name "Lupercalia" was believed in antiquity to evince some connection with the Ancient Greek festival of the Arcadian Lykaia (from Ancient Greek: "λύκος" — "lukos", "wolf", Latin "lupus") and the worship of "Lycaean Pan", assumed to be a Greek equivalent to Faunus, as instituted by Evander.
In Roman mythology, "Lupercus" is a god sometimes identified with the Roman god Faunus, who is the Roman equivalent of the Greek god Pan. Lupercus is the god of shepherds. His festival, celebrated on the anniversary of the founding of his temple on February 15, was called the Lupercalia. His priests wore goatskins. The historian Justin mentions an image of "the Lycaean god, whom the Greeks call Pan and the Romans Lupercus," nude save for the girdle of goatskin, which stood in the Lupercal, the cave where Romulus and Remus were suckled by a she-wolf. There, on the Ides of February (in February the ides is the 13th), a goat and a dog were sacrificed, and salt mealcakes prepared by the Vestal Virgins were burnt.
Late Republic and Empire.
Plutarch described Lupercalia:
Lupercalia, of which many write that it was anciently celebrated by shepherds, and has also some connection with the Arcadian Lycaea. At this time many of the noble youths and of the magistrates run up and down through the city naked, for sport and laughter striking those they meet with shaggy thongs. And many women of rank also purposely get in their way, and like children at school present their hands to be struck, believing that the pregnant will thus be helped in delivery, and the barren to pregnancy.
The Lupercalia festival was partly in honor of Lupa, the she-wolf who suckled the infant orphans, Romulus and Remus, the founders of Rome, explaining the name of the festival, Lupercalia, or "Wolf Festival." The festival was celebrated near the cave of Lupercal on the Palatine Hill (the central hill where Rome was traditionally founded), to expiate and purify new life in the Spring. A known Lupercalia festival of 44 BC attests to the continuity of the festival but the Lupercal cave may have fallen into disrepair, and was later rebuilt by Augustus. It has been tentatively identified with a cavern discovered in 2007, 50 ft below the remains of Augustus' palace.
The rites were directed by the "Luperci", the "brothers of the wolf ("lupus")", a corporation of "sacerdotes" (priests) of Faunus, dressed only in a goatskin, whose institution is attributed either to the Arcadian Evander, or to Romulus and Remus. The Luperci were divided into two collegia, called "Quinctiliani" (or "Quinctiale"s) and "Fabiani", from the gens Quinctilia (or Quinctia) and gens Fabia; at the head of each of these colleges was a "magister". In 44 BC, a third college, the "Julii", was instituted in honor of Julius Caesar, the first magister of which was Mark Antony. Antony offered Caesar a crown during the festival, an act that was widely interpreted as a sign that Caesar aspired to make himself king and was gauging the reaction of the crowd. In imperial times the members were usually of equestrian standing.
The festival began with the sacrifice by the Luperci (or the flamen dialis) of two male goats and a dog. Next two young patrician Luperci were led to the altar, to be anointed on their foreheads with the sacrificial blood, which was wiped off the bloody knife with wool soaked in milk, after which they were expected to smile and laugh.
The sacrificial feast followed, after which the Luperci cut thongs from the skins of the animals, which were called februa, dressed themselves in the skins of the sacrificed goats, in imitation of Lupercus, and ran round the walls of the old Palatine city, the line of which was marked with stones, with the thongs in their hands in two bands, striking the people who crowded near. Girls and young women would line up on their route to receive lashes from these whips. This was supposed to ensure fertility, prevent sterility in women and ease the pains of childbirth.
The Lupercalia in the 5th century.
By the 5th century, when the public performance of pagan rites had been outlawed, a nominally Christian Roman populace still clung to the Lupercalia in the time of Pope Gelasius I (494–96). It had been literally degraded since the 1st century, when in 44 BC the consul Mark Antony did not scruple to run with the Luperci; now the upper classes left the festivities to the rabble. Whatever the fortunes of the rites in the meantime, in the last decade of the 5th century they prompted Pope Gelasius I's taunt to the senators who were intent on preserving them: "If you assert that this rite has salutary force, celebrate it yourselves in the ancestral fashion; run nude yourselves that you may properly carry out the mockery." The remark was addressed to the senator Andromachus by Gelasius in an extended literary epistle that was virtually a diatribe against the Lupercalia. Gelasius finally abolished the Lupercalia after a long dispute.
Some authors claim that Gelasius replaced Lupercalia with the "Feast of the Purification of the Blessed Virgin Mary," but researcher Oruch says that there is no written record of Gelasius ever intending a replacement of Lupercalia. Some researchers, such as Kellog and Cox, have made a separate claim that the modern customs of Saint Valentine's Day originate from Lupercalia customs. Other researchers have rejected this claim: they say there is no proof that the modern customs of Saint Valentine's Day originate from Lupercalia customs, and the claim seems to originate from misconceptions about festivities.
References in art.
Horace's Ode III, 18 describes Lupercalia.
William Shakespeare's play "Julius Caesar" begins during the Lupercalia, with the tradition described above. Mark Antony is instructed by Caesar to strike his wife Calpurnia, in the hope that she will be able to conceive:
CAESAR (to Calpurnia)
 "Stand you directly in Antonius' way,<br>When he doth run his course. Antonius!
ANTONY
 "Caesar, my lord?
CAESAR
 "Forget not, in your speed, Antonius,<br> To touch Calpurnia; for our elders say,<br>The barren touched in this holy chase,<br>Shake off their sterile curse.

</doc>
<doc id="18162" url="http://en.wikipedia.org/wiki?curid=18162" title="Lists of atheists">
Lists of atheists

Atheism is, in a broad sense, the rejection of belief in the existence of deities. In a narrower sense, atheism is specifically the position that there are no deities. Most inclusively, atheism is simply the absence of belief that any deities exist. This is a compilation of the various lists of atheists with articles in Wikipedia. Living persons in these lists are people whose atheism is relevant to their notable activities or public life, and who have publicly identified themselves as atheists.

</doc>
<doc id="18163" url="http://en.wikipedia.org/wiki?curid=18163" title="List of Buddhists">
List of Buddhists

This is a list of notable Buddhists, encompassing all the major branches of the religion, and including interdenominational and eclectic Buddhist practitioners. This list includes both formal teachers of Buddhism, and people notable in other areas who are publicly Buddhist or who have espoused Buddhism.
Historical Buddhist thinkers and founders of schools.
Individuals are grouped by nationality, except in cases where their influence was felt elsewhere. Gautama Buddha and his immediate disciples ('Buddhists') are listed separately from later Indian Buddhist thinkers, teachers and contemplatives.
Modern teachers.
Zen teachers.
American
Chinese
Dominican Republic
European
Japanese
Korean
Malaysian
Taiwanese
Vietnamese

</doc>
<doc id="18166" url="http://en.wikipedia.org/wiki?curid=18166" title="List of agnostics">
List of agnostics

Listed here are persons who have identified themselves as theologically agnostic. Also included are individuals who have expressed the view that the veracity of a god's existence is unknown or inherently unknowable.

</doc>
<doc id="18167" url="http://en.wikipedia.org/wiki?curid=18167" title="Linked list">
Linked list

In computer science, a linked list is a data structure consisting of a group of nodes which together represent a sequence. Under the simplest form, each node is composed of data and a reference (in other words, a "link") to the next node in the sequence; more complex variants add additional links. This structure allows for efficient insertion or removal of elements from any position in the sequence.Linear collection of data elements is called node pointers
<br>
Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists (the abstract data type), stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement the other data structures directly without using a list as the basis of implementation.
The principal benefit of a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while an array has to be declared in the source code, before compiling and running the program. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.
On the other hand, simple linked lists by themselves do not allow random access to the data, or any form of efficient indexing. Thus, many basic operations — such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted — may require sequential scanning of most or all of the list elements. The advantages and disadvantages of using linked lists are given below.
History.
Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language. IPL was used by the authors to develop several early artificial intelligence programs, including the Logic Theory Machine, the General Problem Solver, and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first UNESCO International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in "Programming the Logic Theory Machine" by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM Turing Award in 1975 for having "made basic contributions to artificial intelligence, the psychology of human cognition, and list processing".
The problem of machine translation for natural language processing led Victor Yngve at Massachusetts Institute of Technology (MIT) to use linked lists as data structures in his COMIT programming language for computer research in the field of linguistics. A report on this language entitled "A programming language for mechanical translation" appeared in Mechanical Translation in 1958.
LISP, standing for list processor, was created by John McCarthy in 1958 while he was at MIT and in 1960 he published its design in a paper in the Communications of the ACM, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I". One of LISP's major data structures is the linked list.
By the early 1960s, the utility of both linked lists and languages which use these structures as their primary data representation was well established. Bert Green of the MIT Lincoln Laboratory published a review article entitled "Computer languages for symbol manipulation" in IRE Transactions on Human Factors in Electronics in March 1961 which summarized the advantages of the linked list approach. A later review article, "A Comparison of list-processing computer languages" by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.
Several operating systems developed by Technical Systems Consultants (originally of West Lafayette Indiana, and later of Chapel Hill, North Carolina) used singly linked lists as file structures. A directory entry pointed to the first sector of a file, and succeeding portions of the file were located by traversing pointers. Systems using this technique included Flex (for the Motorola 6800 CPU), mini-Flex (same CPU), and Flex9 (for the Motorola 6809 CPU). A variant developed by TSC for and marketed by Smoke Signal Broadcasting in California, used doubly linked lists in the same manner.
The TSS/360 operating system, developed by IBM for the System 360/370 machines, used a double linked list for their file system catalog. The directory structure was similar to Unix, where a directory could contain files and other directories and extend to any depth.
Basic concepts and nomenclature.
Each record of a linked list is often called an 'element' or 'node'.
The field of each node that contains the address of the next node is usually called the 'next link' or 'next pointer'. The remaining fields are known as the 'data', 'information', 'value', 'cargo', or 'payload' fields.
The 'head' of a list is its first node. The 'tail' of a list may refer either to the rest of the list after the head, or to the last node in the list. In Lisp and some derived languages, the next node may be called the 'cdr' (pronounced "could-er") of the list, while the payload of the head node may be called the 'car'.
Singly linked list.
Singly linked lists contain nodes which have a data field as well as a 'next' field, which points to the next node in line of nodes. Operations that can be performed on singly linked lists include insertion, deletion and traversal.
<br>
Doubly linked list.
In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. The two links may be called 'forward('s') and 'backwards', or 'next' and 'prev'('previous').
<br>
A technique known as XOR-linking allows a doubly linked list to be implemented using a single link field in each node. However, this technique requires the ability to do bit operations on addresses, and therefore may not be available in some high-level languages.
Multiply linked list.
In a 'multiply linked list', each node contains two or more link fields, each field being used to connect the same set of data records in a different order (e.g., by name, by department, by date of birth, etc.). While doubly linked lists can be seen as special cases of multiply linked list, the fact that the two orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.
Circular Linked list.
In the last node of a list, the link field often contains a null reference, a special value used to indicate the lack of further nodes. A less common convention is to make it point to the first node of the list; in that case the list is said to be 'circular' or 'circularly linked'; otherwise it is said to be 'open' or 'linear'.
<br>
In the case of a circular doubly linked list, the only change that occurs is that the end, or "tail", of the said list is linked back to the front, or "head", of the list and vice versa.
Sentinel nodes.
In some implementations, an extra 'sentinel' or 'dummy' node may be added before the first data record or after the last one. This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a "first" and "last" node.
Empty lists.
An empty list is a list that contains no data records. This is usually the same as saying that it has zero nodes. If sentinel nodes are being used, the list is usually said to be empty when it has only sentinel nodes.
Hash linking.
The link fields need not be physically part of the nodes. If the data records are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the data records.
List handles.
Since a reference to the first node gives access to the whole list, that reference is often called the 'address', 'pointer', or 'handle' of the list. Algorithms that manipulate linked lists usually get such handles to the input lists and return the handles to the resulting lists. In fact, in the context of such algorithms, the word "list" often means "list handle". In some situations, however, it may be convenient to refer to a list by a handle that consists of two links, pointing to its first and last nodes.
Combining alternatives.
The alternatives listed above may be arbitrarily combined in almost every way, so one may have circular doubly linked lists without sentinels, circular singly linked lists with sentinels, etc.
Tradeoffs.
As with most choices in computer programming and design, no method is well suited to all circumstances. A linked list data structure might work well in one case, but cause problems in another. This is a list of some of the common tradeoffs involving linked list structures.
Linked lists vs. dynamic arrays.
A "dynamic array" is a data structure that allocates all elements contiguously in memory, and keeps a count of the current number of elements. If the space reserved for the dynamic array is exceeded, it is reallocated and (possibly) copied, an expensive operation.
Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have indexed a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can "delete" an element from an array in constant time by somehow marking its slot as "vacant", this causes fragmentation that impedes the performance of iteration.
Moreover, arbitrarily many elements may be inserted into a linked list, limited only by the total memory available; while a dynamic array will eventually fill up its underlying array data structure and will have to reallocate — an expensive operation, one that may not even be possible if memory is fragmented, although the cost of reallocation can be averaged over insertions, and the cost of an insertion due to reallocation would still be amortized O(1). This helps with appending elements at the array's end, but inserting into (or removing from) middle positions still carries prohibitive costs due to data moving to maintain contiguity. An array from which many elements are removed may also have to be resized in order to avoid wasting too much space.
On the other hand, dynamic arrays (as well as fixed-size array data structures) allow constant-time random access, while linked lists allow only sequential access to elements. Singly linked lists, in fact, can be easily traversed in only one direction. This makes linked lists unsuitable for applications where it's useful to look up an element by its index quickly, such as heapsort. Sequential access on arrays and dynamic arrays is also faster than on linked lists on many machines, because they have optimal locality of reference and thus make good use of data caching.
Another disadvantage of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data items such as characters or boolean values, because the storage overhead for the links may exceed by a factor of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data). It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.
Some hybrid solutions try to combine the advantages of the two representations. Unrolled linked lists store several elements in each list node, increasing cache performance while decreasing memory overhead for references. CDR coding does both these as well, by replacing references with the actual data referenced, which extends off the end of the referencing record.
A good example that highlights the pros and cons of using dynamic arrays vs. linked lists is by implementing a program that resolves the Josephus problem. The Josephus problem is an election method that works by having a group of people stand in a circle. Starting at a predetermined person, you count around the circle "n" times. Once you reach the "n"th person, take them out of the circle and have the members close the circle. Then count around the circle the same "n" times and repeat the process, until only one person is left. That person wins the election. This shows the strengths and weaknesses of a linked list vs. a dynamic array, because if you view the people as connected nodes in a circular linked list then it shows how easily the linked list is able to delete nodes (as it only has to rearrange the links to the different nodes). However, the linked list will be poor at finding the next person to remove and will need to search through the list until it finds that person. A dynamic array, on the other hand, will be poor at deleting nodes (or elements) as it cannot remove one node without individually shifting all the elements up the list by one. However, it is exceptionally easy to find the "n"th person in the circle by directly referencing them by their position in the array.
The list ranking problem concerns the efficient conversion of a linked list representation into an array. Although trivial for a conventional computer, solving this problem by a parallel algorithm is complicated and has been the subject of much research.
A balanced tree has similar memory access patterns and space overhead to a linked list while permitting much more efficient indexing, taking O(log n) time instead of O(n) for a random access. However, insertion and deletion operations are more expensive due to the overhead of tree manipulations to maintain balance. Schemes exist for trees to automatically maintain themselves in a balanced state: AVL trees or red-black trees.
Singly linked linear lists vs. other lists.
While doubly-linked and circular lists have advantages over singly-linked linear lists, linear lists offer some advantages that make them preferable in some situations.
A singly linked linear list is a recursive data structure, because it contains a pointer to a "smaller" object of the same type. For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.
Linear singly linked lists also allow tail-sharing, the use of a common final portion of sub-list as the terminal portion of two different lists. In particular, if a new node is added at the beginning of a list, the former list remains available as the tail of the new one — a simple example of a persistent data structure. Again, this is not true with the other variants: a node may never belong to two different circular or doubly linked lists.
In particular, end-sentinel nodes can be shared among singly linked non-circular lists. The same end-sentinel node may be used for "every" such list. In Lisp, for example, every proper list ends with a link to a special node, denoted by codice_1 or codice_2, whose codice_3 and codice_4 links point to itself. Thus a Lisp procedure can safely take the codice_3 or codice_4 of "any" list.
The advantages of the fancy variants are often limited to the complexity of the algorithms, not in their efficiency. A circular list, in particular, can usually be emulated by a linear list together with two variables that point to the first and last nodes, at no extra cost.
Doubly linked vs. singly linked.
Double-linked lists require more space per node (unless one uses XOR-linking), and their elementary operations are more expensive; but they are often easier to manipulate because they allow fast and easy sequential access to the list in both directions. In a doubly linked list, one can insert or delete a node in a constant number of operations given only that node's address. To do the same in a singly linked list, one must have the "address of the pointer" to that node, which is either the handle for the whole list (in case of the first node) or the link field in the "previous" node. Some algorithms require access in both directions. On the other hand, doubly linked lists do not allow tail-sharing and cannot be used as persistent data structures.
Circularly linked vs. linearly linked.
A circularly linked list may be a natural option to represent arrays that are naturally circular, e.g. the corners of a polygon, a pool of buffers that are used and released in FIFO ("first in, first out") order, or a set of processes that should be time-shared in round-robin order. In these applications, a pointer to any node serves as a handle to the whole list.
With a circular list, a pointer to the last node gives easy access also to the first node, by following one link. Thus, in applications that require access to both ends of the list (e.g., in the implementation of a queue), a circular structure allows one to handle the structure by a single pointer, instead of two.
A circular list can be split into two circular lists, in constant time, by giving the addresses of the last node of each piece. The operation consists in swapping the contents of the link fields of those two nodes. Applying the same operation to any two nodes in two distinct lists joins the two list into one. This property greatly simplifies some algorithms and data structures, such as the quad-edge and face-edge.
The simplest representation for an empty "circular" list (when such a thing makes sense) is a null pointer, indicating that the list has no nodes. Without this choice, many algorithms have to test for this special case, and handle it separately. By contrast, the use of null to denote an empty "linear" list is more natural and often creates fewer special cases.
Using sentinel nodes.
Sentinel node may simplify certain list operations, by ensuring that the next or previous nodes exist for every element, and that even empty lists have at least one node. One may also use a sentinel node at the end of the list, with an appropriate data field, to eliminate some end-of-list tests. For example, when scanning the list looking for a node with a given value "x", setting the sentinel's data field to "x" makes it unnecessary to test for end-of-list inside the loop. Another example is the merging two sorted lists: if their sentinels have data fields set to +∞, the choice of the next output node does not need special handling for empty lists.
However, sentinel nodes use up extra space (especially in applications that use many short lists), and they may complicate other operations (such as the creation of a new empty list).
However, if the circular list is used merely to simulate a linear list, one may avoid some of this complexity by adding a single sentinel node to every list, between the last and the first data nodes. With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link. The list handle should then be a pointer to the last data node, before the sentinel, if the list is not empty; or to the sentinel itself, if the list is empty.
The same trick can be used to simplify the handling of a doubly linked linear list, by turning it into a circular doubly linked list with a single sentinel node. However, in this case, the handle should be a single pointer to the dummy node itself.
Linked list operations.
When manipulating linked lists in-place, care must be taken to not use values that you have invalidated in previous assignments. This makes algorithms for inserting or deleting linked list nodes somewhat subtle. This section gives pseudocode for adding or removing nodes from singly, doubly, and circularly linked lists in-place. Throughout we will use "null" to refer to an end-of-list marker or sentinel, which may be implemented in a number of ways.
Linearly linked lists.
Singly linked lists.
Our node data structure will have two fields. We also keep a variable "firstNode" which always points to the first node in the list, or is "null" for an empty list.
 record "Node"
 data; "// The data being stored in the node"
 "Node" next "// A reference to the next node, null for last node"
 record "List"
 "Node" firstNode "// points to first node of list; null for empty list"
Traversal of a singly linked list is simple, beginning at the first node and following each "next" link until we come to the end:
 node := list.firstNode
 while node not null
 "(do something with node.data)"
 node := node.next
The following code inserts a node after an existing node in a singly linked list. The diagram shows how it works. Inserting a node before an existing one cannot be done directly; instead, one must keep track of the previous node and insert a node after it.
 function insertAfter("Node" node, "Node" newNode) "// insert newNode after node"
 newNode.next := node.next
 node.next := newNode
Inserting at the beginning of the list requires a separate function. This requires updating "firstNode".
 function insertBeginning("List" list, "Node" newNode) "// insert node before current first node"
 newNode.next := list.firstNode
 list.firstNode := newNode
Similarly, we have functions for removing the node "after" a given node, and for removing a node from the beginning of the list. The diagram demonstrates the former. To find and remove a particular node, one must again keep track of the previous element.
 function removeAfter("Node" node) "// remove node past this one"
 obsoleteNode := node.next
 node.next := node.next.next
 destroy obsoleteNode
 function removeBeginning("List" list) "// remove first node"
 obsoleteNode := list.firstNode
 list.firstNode := list.firstNode.next "// point past deleted node"
 destroy obsoleteNode
Notice that codice_7 sets codice_8 to codice_9 when removing the last node in the list.
Since we can't iterate backwards, efficient codice_10 or codice_11 operations are not possible.
Appending one linked list to another can be inefficient unless a reference to the tail is kept as part of the List structure, because we must traverse the entire first list in order to find the tail, and then append the second list to this. Thus, if two linearly linked lists are each of length formula_1, list appending has asymptotic time complexity of formula_2. In the Lisp family of languages, list appending is provided by the codice_12 procedure.
Many of the special cases of linked list operations can be eliminated by including a dummy element at the front of the list. This ensures that there are no special cases for the beginning of the list and renders both codice_13 and codice_7 unnecessary. In this case, the first useful data in the list will be found at codice_15.
Circularly linked list.
In a circularly linked list, all nodes are linked in a continuous circle, without using "null." For lists with a front and a back (such as a queue), one stores a reference to the last node in the list. The "next" node after the last node is the first node. Elements can be added to the back of the list and removed from the front in constant time.
Circularly linked lists can be either singly or doubly linked.
Both types of circularly linked lists benefit from the ability to traverse the full list beginning at any given node. This often allows us to avoid storing "firstNode" and "lastNode", although if the list may be empty we need a special representation for the empty list, such as a "lastNode" variable which points to some node in the list or is "null" if it's empty; we use such a "lastNode" here. This representation significantly simplifies adding and removing nodes with a non-empty list, but empty lists are then a special case.
Algorithms.
Assuming that "someNode" is some node in a non-empty circular singly linked list, this code iterates through that list starting with "someNode":
 function iterate(someNode)
 if someNode ≠ null
 node := someNode
 do
 do something with node.value
 node := node.next
 while node ≠ someNode
Notice that the test "while node ≠ someNode" must be at the end of the loop. If the test was moved to the beginning of the loop, the procedure would fail whenever the list had only one node.
This function inserts a node "newNode" into a circular linked list after a given node "node". If "node" is null, it assumes that the list is empty.
 function insertAfter("Node" node, "Node" newNode)
 if node = null
 newNode.next := newNode
 else
 newNode.next := node.next
 node.next := newNode
Suppose that "L" is a variable pointing to the last node of a circular linked list (or null if the list is empty). To append "newNode" to the "end" of the list, one may do
 insertAfter(L, newNode)
 L := newNode
To insert "newNode" at the "beginning" of the list, one may do
 insertAfter(L, newNode)
 if L = null
 L := newNode
Linked lists using arrays of node.
Languages that do not support any type of reference can still create links by replacing pointers with array indices. The approach is to keep an array of records, where each record has integer fields indicating the index of the next (and possibly previous) node in the array. Not all nodes in the array need be used. If records are also not supported, parallel arrays can often be used instead.
As an example, consider the following linked list record that uses arrays instead of pointers:
 record "Entry" {
 "integer" next; "// index of next entry in array"
 "integer" prev; "// previous entry (if double-linked)"
 "string" name;
 "real" balance;
A linked list can be build by creating an array of these structures, and an integer variable to store the index of the first element.
 "integer" listHead
 "Entry" Records[1000]
Links between elements are formed by placing the array index of the next (or previous) cell into the Next or Prev field within a given element. For example:
In the above example, codice_16 would be set to 2, the location of the first entry in the list. Notice that entry 3 and 5 through 7 are not part of the list. These cells are available for any additions to the list. By creating a codice_17 integer variable, a free list could be created to keep track of what cells are available. If all entries are in use, the size of the array would have to be increased or some elements would have to be deleted before new entries could be stored in the list.
The following code would traverse the list and display names and account balance:
 i := listHead
 while i ≥ 0 "// loop through the list"
 print i, Records[i].name, Records[i].balance "// print entry"
 i := Records[i].next
When faced with a choice, the advantages of this approach include:
This approach has one main disadvantage, however: it creates and manages a private memory space for its nodes. This leads to the following issues:
For these reasons, this approach is mainly used for languages that do not support dynamic memory allocation. These disadvantages are also mitigated if the maximum size of the list is known at the time the array is created.
Language support.
Many programming languages such as Lisp and Scheme have singly linked lists built in. In many functional languages, these lists are constructed from nodes, each called a "cons" or "cons cell". The cons has two fields: the "car", a reference to the data for that node, and the "cdr", a reference to the next node. Although cons cells can be used to build other data structures, this is their primary purpose.
In languages that support abstract data types or templates, linked list ADTs or templates are available for building linked lists. In other languages, linked lists are typically built using references together with records.
Internal and external storage.
When constructing a linked list, one is faced with the choice of whether to store the data of the list directly in the linked list nodes, called "internal storage", or merely to store a reference to the data, called "external storage". Internal storage has the advantage of making access to the data more efficient, requiring less storage overall, having better locality of reference, and simplifying memory management for the list (its data is allocated and deallocated at the same time as the list nodes).
External storage, on the other hand, has the advantage of being more generic, in that the same data structure and machine code can be used for a linked list no matter what the size of the data is. It also makes it easy to place the same data in multiple linked lists. Although with internal storage the same data can be placed in multiple lists by including multiple "next" references in the node data structure, it would then be necessary to create separate routines to add or delete cells based on each field. It is possible to create additional linked lists of elements that use internal storage by using external storage, and having the cells of the additional linked lists store references to the nodes of the linked list containing the data.
In general, if a set of data structures needs to be included in linked lists, external storage is the best approach. If a set of data structures need to be included in only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available. Likewise, if different sets of data that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.
Another approach that can be used with some languages involves having different data structures, but all have the initial fields, including the "next" (and "prev" if double linked list) references in the same location. After defining separate structures for each type of data, a generic structure can be defined that contains the minimum amount of data shared by all the other structures and contained at the top (beginning) of the structures. Then generic routines can be created that use the minimal structure to perform linked list type operations, but separate routines can then handle the specific data. This approach is often used in message parsing routines, where several types of messages are received, but all start with the same set of fields, usually including a field for message type. The generic routines are used to add new messages to a queue when they are received, and remove them from the queue in order to process the message. The message type field is then used to call the correct routine to process the specific type of message.
Example of internal and external storage.
Suppose you wanted to create a linked list of families and their members. Using internal storage, the structure might look like the following:
 record "member" { "// member of a family"
 "member" next;
 "string" firstName;
 "integer" age;
 record "family" { "// the family itself"
 "family" next;
 "string" lastName;
 "string" address;
 "member" members "// head of list of members of this family"
To print a complete list of families and their members using internal storage, we could write:
 aFamily := Families "// start at head of families list"
 while aFamily ≠ null "// loop through list of families"
 print information about family
 aMember := aFamily.members "// get head of list of this family's members"
 while aMember ≠ null "// loop through list of members"
 print information about member
 aMember := aMember.next
 aFamily := aFamily.next
Using external storage, we would create the following structures:
 record "node" { "// generic link structure"
 "node" next;
 "pointer" data "// generic pointer for data at node"
 record "member" { "// structure for family member"
 "string" firstName;
 "integer" age
 record "family" { "// structure for family"
 "string" lastName;
 "string" address;
 "node" members "// head of list of members of this family"
To print a complete list of families and their members using external storage, we could write:
 famNode := Families "// start at head of families list"
 while famNode ≠ null "// loop through list of families"
 aFamily := (family) famNode.data "// extract family from node"
 print information about family
 memNode := aFamily.members "// get list of family members"
 while memNode ≠ null "// loop through list of members"
 aMember := (member)memNode.data "// extract member from node"
 print information about member
 memNode := memNode.next
 famNode := famNode.next
Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type. This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure ("node"), and this language does not have parametric types.
As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.
Speeding up search.
Finding a specific element in a linked list, even if it is sorted, normally requires O("n") time (linear search). This is one of the primary disadvantages of linked lists over other data structures. In addition to the variants discussed above, below are two simple ways to improve search time.
In an unordered list, one simple heuristic for decreasing average search time is the "move-to-front heuristic", which simply moves an element to the beginning of the list once it is found. This scheme, handy for creating simple caches, ensures that the most recently used items are also the quickest to find again.
Another common approach is to "index" a linked list using a more efficient external data structure. For example, one can build a red-black tree or hash table whose elements are references to the linked list nodes. Multiple such indexes can be built on a single list. The disadvantage is that these indexes may need to be updated each time a node is added or removed (or at least, before that index is used again).
Random access lists.
A random access list is a list with support for fast random access to read or modify any element in the list. One possible implementation is a skew binary random access list using the skew binary number system, which involves a list of trees with special properties; this allows worst-case constant time head/cons operations, and worst-case logarithmic time random access to an element by index. Random access lists can be implemented as persistent data structures.
Random access lists can be viewed as immutable linked lists in that they likewise support the same O(1) head and tail operations.
A simple extension to random access lists is the min-list, which provides an additional operation that yields the minimum element in the entire list in constant time (without mutation complexities).
Related data structures.
Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported.
The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list.
A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node.
An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list.
A hash table may use linked lists to store the chains of items that hash to the same position in the hash table.
A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index.
A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.

</doc>
<doc id="18168" url="http://en.wikipedia.org/wiki?curid=18168" title="Logic gate">
Logic gate

In electronics, a logic gate is an idealized or physical device implementing a Boolean function; that is, it performs a logical operation on one or more logical inputs, and produces a single logical output. Depending on the context, the term may refer to an ideal logic gate, one that has for instance zero rise time and unlimited fan-out, or it may refer to a non-ideal physical device (see Ideal and real op-amps for comparison).
Logic gates are primarily implemented using diodes or transistors acting as electronic switches, but can also be constructed using vacuum tubes, electromagnetic relays (relay logic), fluidic logic, pneumatic logic, optics, molecules, or even mechanical elements. With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic.
Logic circuits include such devices as multiplexers, registers, arithmetic logic units (ALUs), and computer memory, all the way up through complete microprocessors, which may contain more than 100 million gates. In modern practice, most gates are made from field-effect transistors (FETs), particularly MOSFETs (metal–oxide–semiconductor field-effect transistors).
Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates.
In reversible logic, Toffoli gates are used.
Electronic gates.
To build a functionally complete logic system, relays, valves (vacuum tubes), or transistors can be used. The simplest family of logic gates using bipolar transistors is called resistor-transistor logic (RTL). Unlike simple diode logic gates (which do not have a gain element), RTL gates can be cascaded indefinitely to produce more complex logic functions. RTL gates were used in early integrated circuits. For higher speed and better density, the resistors used in RTL were replaced by diodes resulting in diode-transistor logic (DTL). Transistor-transistor logic (TTL) then supplanted DTL. As integrated circuits became more complex, bipolar transistors were replaced with smaller field-effect transistors (MOSFETs); see PMOS and NMOS. To reduce power consumption still further, most contemporary chip implementations of digital systems now use CMOS logic. CMOS uses complementary (both n-channel and p-channel) MOSFET devices to achieve a high speed with low power dissipation.
For small-scale logic, designers now use prefabricated logic gates from families of devices such as the TTL 7400 series by Texas Instruments, the CMOS 4000 series by RCA, and their more recent descendants. Increasingly, these fixed-function logic gates are being replaced by programmable logic devices, which allow designers to pack a large number of mixed logic gates into a single integrated circuit. The field-programmable nature of programmable logic devices such as FPGAs has removed the 'hard' property of hardware; it is now possible to change the logic design of a hardware system by reprogramming some of its components, thus allowing the features or function of a hardware implementation of a logic system to be changed.
Electronic logic gates differ significantly from their relay-and-switch equivalents. They are much faster, consume much less power, and are much smaller (all by a factor of a million or more in most cases). Also, there is a fundamental structural difference. The switch circuit creates a continuous metallic path for current to flow (in either direction) between its input and its output. The semiconductor logic gate, on the other hand, acts as a high-gain voltage amplifier, which sinks a tiny current at its input and produces a low-impedance voltage at its output. It is not possible for current to flow between the output and the input of a semiconductor logic gate.
Another important advantage of standardized integrated circuit logic families, such as the 7400 and 4000 families, is that they can be cascaded. This means that the output of one gate can be wired to the inputs of one or several other gates, and so on. Systems with varying degrees of complexity can be built without great concern of the designer for the internal workings of the gates, provided the limitations of each integrated circuit are considered.
The output of one gate can only drive a finite number of inputs to other gates, a number called the 'fanout limit'. Also, there is always a delay, called the 'propagation delay', from a change in input of a gate to the corresponding change in its output. When gates are cascaded, the total propagation delay is approximately the sum of the individual delays, an effect which can become a problem in high-speed circuits. Additional delay can be caused when a large number of inputs are connected to an output, due to the distributed capacitance of all the inputs and wiring and the finite amount of current that each output can provide.
Symbols.
There are two sets of symbols for elementary logic gates in common use, both defined in ANSI/IEEE Std 91-1984 and its supplement ANSI/IEEE Std 91a-1991. The "distinctive shape" set, based on traditional schematics, is used for simple drawings, and derives from MIL-STD-806 of the 1950s and 1960s. It is sometimes unofficially described as "military", reflecting its origin. The "rectangular shape" set, based on ANSI Y32.14 and other early industry standards, as later refined by IEEE and IEC, has rectangular outlines for all types of gate and allows representation of a much wider range of devices than is possible with the traditional symbols. The IEC standard, IEC 60617-12, has been adopted by other standards, such as EN 60617-12:1999 in Europe, BS EN 60617-12:1999 in the United Kingdom, and DIN EN 60617-12:1998 in Germany.
The mutual goal of IEEE Std 91-1984 and IEC 60617-12 was to provide a uniform method of describing the complex logic functions of digital circuits with schematic symbols. These functions were more complex than simple AND and OR gates. They could be medium scale circuits such as a 4-bit counter to a large scale circuit such as a microprocessor.
IEC 617-12 and its successor IEC 60617-12 do not explicitly show the "distinctive shape" symbols, but do not prohibit them. These are, however, shown in ANSI/IEEE 91 (and 91a) with this note: "The distinctive-shape symbol is, according to IEC Publication 617, Part 12, not preferred, but is not considered to be in contradiction to that standard." IEC 60617-12 correspondingly contains the note (Section 2.1) "Although non-preferred, the use of other symbols recognized by official national standards, that is distinctive shapes in place of symbols [list of basic gates], shall not be considered to be in contradiction with this standard. Usage of these other symbols in combination to form complex symbols (for example, use as embedded symbols) is discouraged." This compromise was reached between the respective IEEE and IEC working groups to permit the IEEE and IEC standards to be in mutual compliance with one another.
A third style of symbols was in use in Europe and is still widely used in European academia. See the column "DIN 40700" in .
In the 1980s, schematics were the predominant method to design both circuit boards and custom ICs known as gate arrays. Today custom ICs and the field-programmable gate array are typically designed with Hardware Description Languages (HDL) such as Verilog or VHDL.
The two input exclusive-OR is true only when the two input values are "different", false if they are equal, regardless of the value. If there are more than two inputs, the gate generates a true at its output if the number of trues at its input is "odd" (). In practice, these gates are built from combinations of simpler logic gates.
Universal logic gates.
Charles Sanders Peirce (winter of 1880–81) showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but his work on it was unpublished until 1933. The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called "Peirce's arrow". Consequently, these gates are sometimes called "universal logic gates".
De Morgan equivalent symbols.
By use of De Morgan's laws, an "AND" function is identical to an "OR" function with negated inputs and outputs. Likewise, an "OR" function is identical to an "AND" function with negated inputs and outputs. A NAND gate is equivalent to an OR gate with negated inputs, and a NOR gate is equivalent to an AND gate with negated inputs.
This leads to an alternative set of symbols for basic gates that use the opposite core symbol ("AND" or "OR") but with the inputs and outputs negated. Use of these alternative symbols can make logic circuit diagrams much clearer and help to show accidental connection of an active high output to an active low input or vice versa. Any connection that has logic negations at both ends can be replaced by a negationless connection and a suitable change of gate or vice versa. Any connection that has a negation at one end and no negation at the other can be made easier to interpret by instead using the De Morgan equivalent symbol at either of the two ends. When negation or polarity indicators on both ends of a connection match, there is no logic negation in that path (effectively, bubbles "cancel"), making it easier to follow logic states from one symbol to the next. This is commonly seen in real logic diagrams - thus the reader must not get into the habit of associating the shapes exclusively as OR or AND shapes, but also take into account the bubbles at both inputs and outputs in order to determine the "true" logic function indicated.
A De Morgan symbol can show more clearly a gate's primary logical purpose and the polarity of its nodes that are considered in the "signaled" (active, on) state. Consider the simplified case where a two-input NAND gate is used to drive a motor when either of its inputs are brought low by a switch. The "signaled" state (motor on) occurs when either one OR the other switch is on. Unlike a regular NAND symbol, which suggests AND logic, the De Morgan version, a two negative-input OR gate, correctly shows that OR is of interest. The regular NAND symbol has a bubble at the output and none at the inputs (the opposite of the states that will turn the motor on), but the De Morgan symbol shows both inputs and output in the polarity that will drive the motor.
De Morgan's theorem is most commonly used to implement logic gates as combinations of only NAND gates, or as combinations of only NOR gates, for economic reasons.
Data storage.
Logic gates can also be used to store data. A storage element can be constructed by connecting several gates in a "latch" circuit. More complicated designs that use clock signals and that change only on a rising or falling edge of the clock are called edge-triggered "flip-flops". The combination of multiple flip-flops in parallel, to store a multiple-bit value, is known as a register. When using any of these gate setups the overall system has memory; it is then called a sequential logic system since its output can be influenced by its previous state(s).
These logic circuits are known as computer memory. They vary in performance, based on factors of speed, complexity, and reliability of storage, and many different types of designs are used based on the application.
Three-state logic gates.
A three-state logic gate is a type of logic gate that can have three different outputs: high (H), low (L) and high-impedance (Z). The high-impedance state plays no role in the logic, which is strictly binary. These devices are used on buses of the CPU to allow multiple chips to send data. A group of three-states driving a line with a suitable control circuit is basically equivalent to a multiplexer, which may be physically distributed over separate devices or plug-in cards.
In electronics, a high output would mean the output is sourcing current from the positive power terminal (positive voltage). A low output would mean the output is sinking current to the negative power terminal (zero voltage). High impedance would mean that the output is effectively disconnected from the circuit.
History and development.
The binary number system was refined by Gottfried Wilhelm Leibniz (published in 1705) and he also established that by using the binary system, the principles of arithmetic and logic could be combined. In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as an AND logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of "Tractatus Logico-Philosophicus" (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935–38). Claude E. Shannon introduced the use of Boolean algebra in the analysis and design of switching circuits in 1937. Active research is taking place in molecular logic gates.
Implementations.
Since the 1990s, most logic gates are made in CMOS technology (i.e. NMOS and PMOS transistors are used). Often millions of logic gates are packaged in a single integrated circuit.
There are several logic families with different characteristics (power consumption, speed, cost, size) such as: RDL (resistor-diode logic), RTL (resistor-transistor logic), DTL (diode-transistor logic), TTL (transistor-transistor logic) and CMOS (complementary metal oxide semiconductor). There are also sub-variants, e.g. standard CMOS logic vs. advanced types using still CMOS technology, but with some optimizations for avoiding loss of speed due to slower PMOS transistors.
Non-electronic implementations are varied, though few of them are used in practical applications. Many early electromechanical digital computers, such as the Harvard Mark I, were built from relay logic gates, using electro-mechanical relays. Logic gates can be made using pneumatic devices, such as the Sorteberg relay or mechanical logic gates, including on a molecular scale. Logic gates have been made out of DNA (see DNA nanotechnology) and used to create a computer called MAYA (see MAYA II). Logic gates can be made from quantum mechanical effects (though quantum computing usually diverges from boolean design). Photonic logic gates use non-linear optical effects.
In principle any method that leads to a gate that is functionally complete (for example, either a NOR or a NAND gate) can be used to make any kind of digital logic circuit. Note that the use of 3-state logic for bus systems is not needed, and can be replaced by digital multiplexers.

</doc>
<doc id="18171" url="http://en.wikipedia.org/wiki?curid=18171" title="Linear search">
Linear search

In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.
Linear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed.
Analysis.
For a list with "n" items, the best case is when the value is equal to the first element of the list, in which case only one comparison is needed. The worst case is when the value is not in the list (or occurs only once at the end of the list), in which case "n" comparisons are needed.
If the value being sought occurs "k" times in the list, and all orderings of the list are equally likely, the expected number of comparisons is
For example, if the value being sought occurs once in the list, and all orderings of the list are equally likely, the expected number of comparisons is formula_2. However, if it is "known" that it occurs once, then at most "n" - 1 comparisons are needed, and the expected number of comparisons is 
(for example, for "n" = 2 this is 1, corresponding to a single if-then-else construct).
Either way, asymptotically the worst-case cost and the expected cost of linear search are both O("n").
Non-uniform probabilities.
The performance of linear search improves if the desired value is more likely to be near the beginning of the list than to its end. Therefore, if some values are much more likely to be searched than others, it is desirable to place them at the beginning of the list.
In particular, when the list items are arranged in order of decreasing probability, and these probabilities are geometrically distributed, the cost of linear search is only O(1). If the table size "n" is large enough, linear search will be faster than binary search, whose cost is O(log "n").
Application.
Linear search is usually very simple to implement, and is practical when the list has only a few elements, or when performing a single search in an unordered list.
When many values have to be searched in the same list, it often pays to pre-process the list in order to use a faster method. For example, one may sort the list and use binary search, or build any efficient search data structure from it. Should the content of the list change frequently, repeated re-organization may be more trouble than it is worth.
As a result, even though in theory other search algorithms may be faster than linear search (for instance binary search), in practice even on medium-sized arrays (around 100 items or less) it might be infeasible to use anything else. On larger arrays, it only makes sense to use other, faster search methods if the data is large enough, because the initial time to prepare (sort) the data is comparable to many linear searches 
Pseudocode.
Forward iteration.
This pseudocode describes a typical variant of linear search, where the result of the search is supposed to be either the location of the list item where the desired value was found; or an invalid location "Λ", to indicate that the desired element does not occur in the list.
 For each item in the list:
 if that item has the desired value,
 stop the search and return the item's location.
 Return "Λ".
In this pseudocode, the last line is executed only after all list items have been examined with none matching.
If the list is stored as an array data structure, the location may be the index of the item found (usually between 1 and "n", or 0 and "n"−1). In that case the invalid location "Λ" can be any index before the first element (such as 0 or −1, respectively) or after the last one ("n"+1 or "n", respectively).
If the list is a simply linked list, then the item's location is its reference, and "Λ" is usually the null pointer.
Recursive version.
Linear search can also be described as a recursive algorithm:
 LinearSearch(value, list)
 if the list is empty, return "Λ";
 else
 if the first item of the list has the desired value, return its location;
 else return LinearSearch(value, remainder of the list)
Searching in reverse order.
Linear search in an array is usually programmed by stepping up an index variable until it reaches the last index. This normally requires two comparison instructions for each list item: one to check whether the index has reached the end of the array, and another one to check whether the item has the desired value. In many computers, one can reduce the work of the first comparison by scanning the items in reverse order.
Suppose an array "A" with elements indexed 1 to "n" is to be searched for a value "x". The following
pseudocode performs a forward search, returning "n + 1" if the value is not found:
 Set "i" to 1.
 Repeat this loop:
 If "i" > "n", then exit the loop.
 If "A"["i"] = "x", then exit the loop.
 Set "i" to "i" + 1.
 Return "i".
The following pseudocode searches the array in the reverse order, returning 0 when the element is not found:
 Set "i" to "n".
 Repeat this loop:
 If "i" ≤ 0, then exit the loop.
 If "A"["i"] = "x", then exit the loop.
 Set "i" to "i" − 1.
 Return "i".
Most computers have a conditional branch instruction that tests the sign of a value in a register, or the sign of the result of the most recent arithmetic operation. One can use that instruction, which is usually faster than a comparison against some arbitrary value (requiring a subtraction), to implement the command "If "i" ≤ 0, then exit the loop".
This optimization is easily implemented when programming in machine or assembly language. That branch instruction is not directly accessible in typical high-level programming languages, although many compilers will be able to perform that optimization on their own.
Using a sentinel.
Another way to reduce the overhead is to eliminate all checking of the loop index. This can be done by inserting the desired item itself as a sentinel value at the far end of the list, as in this pseudocode:
 Set "A"["n + 1"] to "x". 
 Set "i" to 1.
 Repeat this loop:
 If "A"["i"] = "x", then exit the loop.
 Set "i" to "i" + 1.
 Return "i".
With this stratagem, it is not necessary to check the value of "i" against the list length "n": even if "x" was not in "A" to begin with, the loop will terminate when "i" = "n + 1". However this method is possible only if the array slot "A"["n + 1"] exists but is not being otherwise used. Similar arrangements could be made if the array were to be searched in reverse order, and element "A(0)" were available.
Although the effort avoided by these ploys is tiny, it is still a significant component of the overhead of performing each step of the search, which is small. Only if many elements are likely to be compared will it be worthwhile considering methods that make fewer comparisons but impose other requirements.
Linear search on an ordered list.
For ordered lists that must be accessed sequentially, such as linked lists or files with variable-length records lacking an index, the average performance can be improved by giving up at the first element which is greater than the unmatched target value, rather than examining the entire list.
If the list is stored as an ordered array, then binary search is almost always more efficient than linear search as with "n" > 8, say, unless there is some reason to suppose that most searches will be for the small elements near the start of the sorted list.

</doc>
<doc id="18172" url="http://en.wikipedia.org/wiki?curid=18172" title="Land mine">
Land mine

A land mine is an explosive device, concealed under or on the ground and designed to destroy or disable enemy targets, ranging from combatants to vehicles and tanks, as they pass over or near the device. Such devices are typically detonated automatically by way of pressure from the target stepping or driving on it, though other detonation mechanisms may be possible. The device may cause damage either by a direct blast or by fragments that are thrown by the blast.
The name originates from the ancient practice of military mining, where tunnels were dug under enemy fortifications or troop formations by sappers. These killing tunnels ("mines") were at first collapsed to destroy targets located above, but they were later filled with explosives and detonated in order to cause even greater devastation.
Nowadays, in common parlance, land mines generally refer to devices specifically manufactured as anti-personnel or anti-vehicle weapons. Though many types of improvised explosive devices ("IEDs") can technically be classified as land mines, the term "land mine" is typically reserved for manufactured devices designed to be used by recognized military services, whereas "IED" is used for makeshift devices assembled by paramilitary, insurgent, or terrorist groups.
The use of land mines is controversial because of their potential as indiscriminate weapons. They can remain dangerous many years after a conflict has ended, harming the economy and civilians of many developing nations. With pressure from a number of campaign groups organised through the International Campaign to Ban Landmines, a global movement to prohibit their use led to the 1997 Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on their Destruction. Currently, a total of 161 nations are party to the "Ottawa treaty".
Use.
Land mines were designed for two main uses: to create defensive tactical barriers, channeling attacking forces into predetermined fire zones or slowing an invading force's progress to allow reinforcements to arrive; and to act as passive area-denial weapons (to deny the enemy use of valuable terrain, resources or facilities when active defense of the area is not desirable or possible).
Land mines are currently used in large quantities mostly for this first purpose, thus their widespread use in the demilitarized zones (DMZs) of likely flashpoints such as Cyprus, Afghanistan and Korea. As of 2013, the only governments that still laid land mines were Myanmar in its internal conflict, and Syria in its civil war.
Land mines continue to kill or injure at least 4,300 people every year, even decades after the ends of the conflicts for which they were placed.
History.
Pre-modern development.
Jiao Yu in the preface to his "Huolongjing Quanzhi", written in 1412 AD, claimed that the 3rd-century Chancellor Zhuge Liang of the Kingdom of Shu, had used not only "fire weapons" but land mines in the Battle of Hulugu Valley against the forces of Sima Yi and his son Sima Zhao of the Kingdom of Wei. This claim is dubious, as gunpowder warfare did not develop in China until the advent of the flamethrower (Pen Huo Qi) in the 10th century, while the land mine was not seen in China until the late 13th century.
Explosive land mines.
East Asia.
Explosive land mines were being used in 1277 AD by the Song Dynasty Chinese against an assault of the Mongols, who were besieging a city in southern China. The invention of this detonated "enormous bomb" was credited to one Lou Qianxia of the 13th century. The famous 14th-century Chinese text of the "Huolongjing", which was the first to describe hollow cast iron cannonball shells filled with gunpowder, was also the first to describe the invention of the land mine in greater detail than references found in texts written beforehand.
This mid 14th century work compiled during the late Yuan Dynasty and early Ming Dynasty (before 1375, when its co-editor Liu Ji died) stated that mines were made of cast iron and were spherical in shape, filled with either 'magic gunpowder', 'poison gunpowder', or 'blinding and burning gunpowder', any one of these compositions being suitable for use. The wad of the mine was made of hard wood, carrying three different fuses in case of defective connection to the touch hole.
In those days, the Chinese relied upon command signals and carefully timed calculation of enemy movements into the minefield, since a long fuse had to be ignited by hand from the ambushers in a somewhat far-off location lying in wait. The "Huolongjing" also describes land mines that were set off by enemy movement, called the 'ground-thunder explosive camp', one of the 'self-trespassing' (zifan) types, as the text says:
These mines are mostly installed at frontier gates and passes. Pieces of bamboo are sawn into sections nine feet in length, all septa in the bamboo being removed, save only the last; and it is then bandaged round with fresh cow-hide tape. Boiling oil is next poured into (the tube) and left there for some time before being removed. The fuse starts from the bottom (of the tube), and (black powder) is compressed into it to form an explosive mine. The gunpowder fills up eight-tenths of the tube, while lead or iron pellets take up the rest of the space; then the open end is sealed with wax. A trench five feet in depth is dug (for the mines to be concealed). The fuse is connected to a firing device which ignites them when disturbed.
The "Huolongjing" describes the trigger device used for this as a 'steel wheel', which directed sparks of flame onto the connection of fuses running to the multiple-laid land mines underneath the carefully hidden trap. Further description of how this flint device operated was not made until a Chinese text of 1606 AD revealed that a weight drive (common in medieval clockworks) had been used to work the 'steel wheel'.
The way in which the Chinese land mine trigger worked was a system of two steel wheels rotated by a falling weight, the chord of which was wound around their axle, and when the enemy stepped onto the disguised boards they released the pins that dropped the weights. In terms of global significance, the first wheellock musket in Europe was sketched by Leonardo da Vinci around 1500 AD, although no use of metal flint for gunpowder weapons were known before that point in Europe.
Besides the use of steel wheels providing sparks for the fuses, there were other methods used as well, such as the 'underground sky-soaring thunder'. The Ming Dynasty (1368–1644) text of the "Wubei Zhi" (Treatise on Armament Technology), written by Mao Yuanyi in 1628, outlined the use of land mines that were triggered by the heat of a slow-burning incandescent material in an underground bowl placed directly above the train of fuses leading to the mines buried 3 ft beneath. The booby trap of this mine system had a mound where weapons of halberds, pikes, and lances were dug in, meant to entice the enemy to walk up the small mound and claim their stolen prize of war booty.
When the weapons were removed from the mound, this movement disturbed the bowl beneath them where the butt ends of the staffs were, which in turn ignited the fuses. According to the "Wubei Huolongjing" volume of the 17th century, the formula for this slow-burning incandescent material allowed it to burn continuously for 20 to 30 days without going out. This formula included 1 lb of white sandal wood powder, 3 oz of iron rust (ferric oxide), 5 oz of 'white' charcoal powder (from quicklime), 2 oz of willow charcoal powder, 6 oz of dried, ground, and powdered red dates, and 3 oz of bran.
The Chinese also made use of the naval mine at sea and on the rivers of China and elsewhere in maritime battles.
Europe and the United States.
The first known land mine in Europe was created by Pedro Navarro (d. 1528), a Spanish soldier, who used it in the settles of the Italian castles, in the beginning of the sixteenth century.
At Augsburg in 1573, a German military engineer by the name of Samuel Zimmermann invented an extremely effective mine known as the "Fladdermine". It consisted of a fougasse (or later, sometimes a "shell fougasse", that is, a fougasse loaded not with stones but with early black powder mortar shells, similar to large black powder hand grenades) activated by a snaphance or flintlock mechanism connected to a tripwire on the surface. Combining the effects of a tripwire activated bounding fragmentation mine with a cluster bomb, it was devastating to massed attackers but required high maintenance due to the susceptibility of black powder to dampness. Consequently it was mainly employed in the defenses of major fortifications, in which role it continued to be used until the 1870s.
In Europe in the early eighteenth century, improvised land mines or booby traps were constructed in the form of bombs buried in shallow wells in the earth and covered with scrap metal and/or gravel to serve as shrapnel. Known in French as "fougasse", the term is sometimes still used in the present day to describe such devices. This technique was used in several European wars of the eighteenth century, the American Revolution, and the American Civil War.
The first modern mechanically fused high explosive anti-personnel land mines were created by Confederate troops of Brigadier General Gabriel J. Rains during the Battle of Yorktown in 1862. As a Captain, Rains had earlier employed explosive booby traps during the Seminole Wars in Florida in 1840. Both mechanically and electrically fused "land torpedoes" were employed, although by the end of the war mechanical fuses had been found to be generally more reliable. Many of these designs were improvised in the field, especially from explosive shells, but by the end of the war nearly 2,000 standard pattern "Rains mines" had been deployed.
Improved designs of mines were created in Imperial Germany, circa 1912, and were copied and manufactured by all major participants in the First World War. Both sides employed land mines (defensively) and tunnel mines (offensively). Well before the war was over, the British were manufacturing land mines that contained poison gas instead of explosives. Poison gas mines were manufactured at least until the 1980s in the Soviet Union. The United States was known to have at least experimented with the concept in the 1950s.
Nuclear mines have also been developed, both land and naval varieties. An example is the British Blue Peacock project, while another was the U.S. Medium Atomic Demolition Munition.
Characteristics and functioning.
A typical land mine includes the following components:
Firing mechanisms and initiating actions.
A land mine can be triggered by a number of things including pressure, movement, sound, magnetism and vibration. Anti-personnel mines commonly use the pressure of a person's foot as a trigger, but tripwires are also frequently employed. Most modern anti-vehicle mines use a magnetic trigger to enable it to detonate even if the tires or tracks did not touch it. Advanced mines are able to sense the difference between friendly and enemy types of vehicles by way of a built-in signature catalog. This will theoretically enable friendly forces to use the mined area while denying the enemy access.
Many mines combine the main trigger with a touch or tilt trigger to prevent enemy engineers from defusing it. Land mine designs tend to use as little metal as possible to make searching with a metal detector more difficult; land mines made mostly of plastic have the added advantage of being very inexpensive.
Some types of modern mines are designed to self-destruct, or chemically render themselves inert after a period of weeks or months to reduce the likelihood of civilian casualties at the conflict's end. These self-destruct mechanisms are not absolutely reliable, and most land mines laid historically are not equipped in this manner.
Anti-handling devices.
Anti-handling devices detonate the mine if someone attempts to lift, shift or disarm it. The intention is to hinder deminers by discouraging any attempts to clear minefields. There is a degree of overlap between the function of a boobytrap and an anti-handling device insofar as some mines have optional fuze pockets into which standard pull or pressure-release boobytrap firing devices can be screwed. Alternatively, some mines may mimic a standard design, but actually be specifically intended to kill deminers, such as the MC-3 and PMN-3 variants of the PMN mine. Anti-handling devices can be found on both anti-personnel mines and anti-tank mines, either as an integral part of their design or as improvised add-ons. For this reason, the standard render safe procedure for mines is often to destroy them on site without attempting to lift them.
Anti-tank mines.
Anti-tank mines were created not long after the invention of the tank in the First World War. At first improvised, purpose-built designs were developed. Set off when a tank passes, they attack the tank at one of its weaker areas — the tracks. They are designed to immobilize or destroy vehicles and their occupants. In U.S. military terminology destroying the vehicles is referred to as a catastrophic kill (k-kill) while only disabling its movement is referred to as a mobility kill (m-kill).
Anti-tank mines are typically larger than anti-personnel mines and require more pressure to detonate. The high trigger pressure, normally requiring 100 kg prevents them from being set off by infantry or smaller vehicles of lesser importance. More modern anti-tank mines use shaped charges to focus and increase the armor penetration of the explosives.
Anti-personnel mines.
Anti-personnel mines are designed to kill or injure enemy combatants as opposed to destroying vehicles. They are often designed to injure rather than kill in order to increase the logistical support (evacuation, medical) burden on the opposing force. Some types of anti-personnel mines can also damage the tracks or wheels of armored vehicles.
Under the Ottawa Treaty, signatory countries undertake not to manufacture, stockpile or use anti-personnel mines. As of 2009, it has been signed/accessioned by 156 countries. Thirty-eight countries, including the People's Republic of China, Russian Federation and the United States, are not party to the Convention.
Warfare.
In military science, minefields are considered a defensive or harassing weapon, used to slow the enemy down, to help deny certain terrain to the enemy, to focus enemy movement into kill zones, or to reduce morale by randomly attacking material and personnel. In some engagements during World War II, anti-tank mines accounted for half of all vehicles disabled.
Since combat engineers with mine-clearing equipment can clear a path through a minefield relatively quickly, mines are usually considered effective only if covered by fire.
The extents of minefields are often marked with warning signs and cloth tape, to prevent friendly troops and non-combatants from entering them. Of course, sometimes terrain can be denied using dummy minefields. Most forces carefully record the location and disposition of their own minefields, because warning signs can be destroyed or removed, and minefields should eventually be cleared. Minefields may also have marked or unmarked safe routes to allow friendly movement through them.
Placing minefields without marking and recording them for later removal is considered a war crime under Protocol II of the Convention on Certain Conventional Weapons, which is itself an annex to the Geneva Conventions.
Artillery and aircraft scatterable mines allow minefields to be placed in front of moving formations of enemy units, including the reinforcement of minefields or other obstacles that have been breached by enemy engineers. They can also be used to cover the retreat of forces disengaging from the enemy, or for interdiction of supporting units to isolate front line units from resupply. In most cases these minefields consist of a combination of anti-tank and anti-personnel mines, with the anti-personnel mines making removal of the anti-tank mines more difficult. Mines of this type used by the United States are designed to self-destruct after a preset period of time, reducing the requirement for mine clearing to only those mines whose self-destruct system did not function. Some designs of these scatterable mines require an electrical charge (capacitor or battery) to detonate. After a certain period of time, either the charge dissipates, leaving them effectively inert or the circuitry is designed such that upon reaching a low level, the device is triggered, thus destroying the mine.
Guerrilla warfare.
None of the conventional tactics and norms of mine warfare applies when they are employed in a guerrilla role:
One example where such tactics were employed is in the various Southern African conflicts during the 1970s and 1980s, specifically Angola, Mozambique, Namibia, South Africa and Zimbabwe.
Laying mines.
Minefields may be laid by several means. The preferred, but most labour-intensive, way is to have engineers bury the mines, since this will make the mines practically invisible and reduce the number of mines needed to deny the enemy an area. Mines can be laid by specialized mine-laying vehicles. Mine-scattering shells may be fired by artillery from a distance of several tens of kilometers.
Mines may be dropped from helicopters or airplanes, or ejected from cluster bombs or cruise missiles.
Anti-tank minefields can be scattered with anti-personnel mines to make clearing them manually more time-consuming; and anti-personnel minefields are scattered with anti-tank mines to prevent the use of armored vehicles to clear them quickly. Some anti-tank mine types are also able to be triggered by infantry, giving them a dual purpose even though their main and official intention is to work as anti-tank weapons.
Some minefields are specifically booby-trapped to make clearing them more dangerous. Mixed anti-personnel and anti-tank minefields, anti-personnel mines "under" anti-tank mines, and fuses separated from mines have all been used for this purpose. Often, single mines are backed by a secondary device, designed to kill or maim personnel tasked with clearing the mine.
Multiple anti-tank mines have been buried in stacks of two or three with the bottom mine fuzed, in order to multiply the penetrating power. Since the mines are buried, the ground directs the energy of the blast in a single direction — through the bottom of the target vehicle or on the track.
Another specific use is to mine an aircraft runway immediately after it has been bombed in order to delay or discourage repair. Some cluster bombs combine these functions. One example is the British JP233 cluster bomb which includes munitions to damage (crater) the runway as well as anti-personnel mines in the same cluster bomb.
Demining.
Metal detectors were first used for demining, after their invention by the Polish officer Józef Kosacki. His invention, known as the Polish mine detector, was used by the Allies alongside mechanical methods, to clear the German mine fields during the Second Battle of El Alamein when 500 units were shipped to Field Marshal Montgomery's Eighth Army.
The Nazis used captured civilians who were chased across minefields to detonate the explosives. According to Laurence Rees, "Curt von Gottberg, the SS-Obergruppenfuhrer who, during 1943, conducted another huge anti-partisan action called Operation Kottbus on the eastern border of Belorussia, reported that 'approximately two to three thousand local people were blown up in the clearing of the minefields'."
Whereas the placing and arming of mines is relatively inexpensive and simple, the process of detecting and removing them is typically expensive, slow, and dangerous. This is especially true of irregular warfare where mines were used on an ad hoc basis in unmarked areas. Anti-personnel mines are most difficult to find, due to their small size and the fact that many are made almost entirely of non-metallic materials specifically to escape detection.
Manual clearing remains the most effective technique for clearing mine fields, although hybrid techniques involving the use of animals and robots are being developed. Animals are desirable due to their strong sense of smell, which is more than capable of detecting a land mine. Animals like rats and dogs can also differentiate between other metal objects and land mines because they can be trained to detect the explosive agent itself.
Other techniques involve the use of geo-location technologies. A joint team of researchers at the University of New South Wales and Ohio State University is working to develop a system based on multi-sensor integration.
The laying of land mines has inadvertently led to a positive development in the Falkland Islands. Mine fields laid near the sea during the Falklands War have become favorite places for penguins, which do not weigh enough to detonate the mines. Therefore, they can breed safely, free of human intrusion. These odd sanctuaries have proven so popular and lucrative for ecotourism that efforts exist to prevent removal of the mines.
Norwegian NGO Norwegian People's Aid is one organisation involved in the safe removal of land mines.
Anti-personnel mine ban.
The use of land mines is controversial because they are indiscriminate weapons, harming soldier and civilian alike. They remain dangerous after the conflict in which they were deployed has ended, killing and injuring civilians and rendering land impassable and unusable for decades. To make matters worse, many factions have not kept accurate records (or any at all) of the exact locations of their minefields, making removal efforts painstakingly slow. These facts pose serious difficulties in many developing nations where the presence of mines hampers resettlement, agriculture, and tourism. The International Campaign to Ban Landmines campaigned successfully to prohibit their use, culminating in the 1997 Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on their Destruction, known informally as the Ottawa Treaty.
The Treaty came into force on March 1, 1999. The treaty was the result of the leadership of the Government of Canada working with the "International Campaign to Ban Landmines", launched in 1992. The campaign and its leader, Jody Williams, won the Nobel Peace Prize in 1997 for its efforts.
The treaty does not include anti-tank mines, cluster bombs or claymore-type mines operated in command mode and focuses specifically on anti-personnel mines, because these pose the greatest long term (post-conflict) risk to humans and animals since they are typically designed to be triggered by any movement or pressure of only a few kilograms, whereas anti-tank mines require much more weight (or a combination of factors that would exclude humans). Existing stocks must be destroyed within four years of signing the treaty.
Signatories of the Ottawa Treaty agree that they will not use, develop, manufacture, stockpile or trade in anti-personnel land mines. There were originally 122 signatories in 1997; currently[ [update]], it has been signed by 155 countries and ratified by 153. Another 40 have yet to sign on. The United States is not one of the signatories, based on lacking an exception for the DMZ of Korea.
There is a clause in the treaty, Article 3, which permits countries to retain land mines for use in training or development of countermeasures. 64 countries have taken this option.
As an alternative to an outright ban, 10 countries follow regulations that are contained in a 1996 amendment of Protocol II of the Convention on Conventional Weapons (CCW). The countries are China, Finland, India, Israel, Morocco, Pakistan, South Korea, Sri Lanka, and the United States.
Manufacturers.
The ICBL has identified the following countries as manufacturing land mines as of August 2004. None are signatories of the Ottawa Treaty.
Of other states which are thought to have manufactured mines recently:
References.
</dl>

</doc>
<doc id="18173" url="http://en.wikipedia.org/wiki?curid=18173" title="List of libertarian political parties">
List of libertarian political parties

Many countries and subnational political entities have libertarian political parties. Although these parties may describe themselves as "libertarian," their ideologies differ considerably and not all of them support all elements of the libertarian agenda.

</doc>
<doc id="18175" url="http://en.wikipedia.org/wiki?curid=18175" title="Loa">
Loa

Loa (also spelled "Lwa" or "L'wha") are the spirits of Haitian Vodou and Louisiana Voodoo. They are also referred to as Mystères and the Invisibles and are intermediaries between Bondye (French: Bon Dieu, meaning "good God")—the Supreme Creator, who is distant from the world—and humanity. Unlike saints or angels however, they are not simply prayed to, they are served. They are each distinct beings with their own personal likes and dislikes, distinct sacred rhythms, songs, dances, ritual symbols, and special modes of service. Contrary to popular belief, the loa are not deities in and of themselves; they are intermediaries for, and dependent on, a distant Bondye.
Syncretism.
Enslaved Fon and Ewe peoples in Haiti and Louisiana syncretized the Loa with the Roman Catholic Saints—Vodoun altars will frequently display images of Catholic saints. For example, Papa Legba is syncretized with St. Peter or St. Lazarus.
Syncretism also works the other way in Haitian Vodou and many Catholic saints have become Loa in their own right, most notably Philomena, St. Michael the Archangel, St. Jude, and John the Baptist.
Rituals.
In a ritual the Loa are summoned by the "Houngan" (Priest), "Mambo" (Priestess), or "Bokor" (Sorcerers) to take part in the service, receive offerings, and grant requests. The loa arrive in the peristyle (ritual space) by mounting (possessing) a horse (ritualist) - who is said to be "ridden". This can be quite a violent occurrence as the participant can flail about or convulse before falling to the ground, but some loa, such as Ayizan, will mount their "horses" very quietly.
Certain loa display very distinctive behaviour by which they can be recognized, specific phrases, and specific actions. As soon as a loa is recognized, the symbols appropriate to them will be given to them. For example, Erzulie Freda will be given a mirror and a comb, fine cloth or jewelry; Legba will be given his cane, straw hat, and pipe; Baron Samedi will be given his top hat, sunglasses, and a cigar.
Once the loa have arrived, fed, been served, and possibly given help or advice, they leave the peristyle. Certain loa can become obstinate, for example the Ghede are notorious for wanting just one more smoke, or one more drink, but it is the job of the Houngan or Mambo to keep the spirits in line while ensuring they are adequately provided for.
Nations of Loa.
There are many families or "nanchons" (nations) of Loa: Rada (also Radha), Petro (also Pethro, Petwo), Nago, Kongo, and Ghede (also Guede, or Gede), among others.
Rada loa.
The Rada loa are generally the older, more beneficent loa. They include Legba, Loko, Ayizan, Anaisa Pye, Dhamballah Wedo and Ayida-Weddo, Erzulie Freda, La Sirène, and Agwé. Their traditional colour is white (as opposed to the specific colours of individual loa).
Petro loa.
The Petro loa are generally the more fiery, occasionally aggressive and warlike loa, and are associated with Haiti and the New World. They include Ezili Dantor, Marinette, and Met Kalfu (Maitre Carrefour, "Master Crossroads"). Their traditional colour is red.
Kongo loa.
Originating from the Congo region, these loa include the many Simbi loa. It also includes Marinette, a fierce and much feared female loa.
Nago loa.
Originating from Yorubaland, this nation includes many of the Ogoun loa.
Ghede loa.
The Ghede are the spirits of the dead. They are traditionally led by the Barons (La Croix, Samedi, Cimitière, Kriminel), and Maman Brigitte. The Ghede as a family are loud, rude (although rarely to the point of real insult), sexual, and usually a lot of fun. As those who have lived already, they have nothing to fear, and frequently will display how far past consequence and feeling they are when they come through in a service—eating glass, raw chillis, and anointing their sensitive areas with chilli rum, for example. Their traditional colours are black and purple.

</doc>
<doc id="18178" url="http://en.wikipedia.org/wiki?curid=18178" title="Labour economics">
Labour economics

Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income.
In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work).
Macro and micro analysis of labour markets.
There are two sides to labour economics. Labour economics can generally be seen as the application of microeconomic or macroeconomic techniques to the labour market. Microeconomic techniques study the role of individuals and individual firms in the labour market. Macroeconomic techniques look at the interrelations between the labour market, the goods market, the money market, and the foreign trade market. It looks at how these interactions influence macro variables such as employment levels, participation rates, aggregate income and Gross Domestic Product.
The macroeconomics of labour markets.
The labor force' is defined as the number of people of working age, who are either employed or actively looking for work. The participation rate is the number of people in the labour force divided by the size of the adult civilian noninstitutional population (or by the population of working age that is not institutionalised). The nonlabour force includes those who are not looking for work, those who are institutionalised such as in prisons or psychiatric wards, stay-at home spouses, children, and those serving in the military. The unemployment level is defined as the labour force minus the number of people currently employed. The unemployment rate is defined as the level of unemployment divided by the labour force. The employment rate is defined as the number of people currently employed divided by the adult population (or by the population of working age). In these statistics, self-employed people are counted as employed.
Variables like employment level, unemployment level, labour force, and unfilled vacancies are called stock variables because they measure a quantity at a point in time. They can be contrasted with flow variables which measure a quantity over a duration of time. Changes in the labour force are due to flow variables such as natural population growth, net immigration, new entrants, and retirements from the labour force. Changes in unemployment depend on: inflows made up of non-employed people starting to look for jobs and of employed people who lose their jobs and look for new ones; and outflows of people who find new employment and of people who stop looking for employment. When looking at the overall macroeconomy, several types of unemployment have been identified, including:
Neoclassical microeconomics of labour markets.
Neo-classical economists view the labour market as similar to other markets in that the forces of supply and demand jointly determine price (in this case the wage rate) and quantity (in this case the number of people employed).
However, the labour market differs from other markets (like the markets for goods or the financial market) in several ways. Perhaps the most important of these differences is the function of supply and demand in setting price and quantity. In markets for goods, if the price is high there is a tendency in the long run for more goods to be produced until the demand is satisfied. With labour, overall supply cannot effectively be manufactured because people have a limited amount of time in the day, and people are not manufactured.
The labour market also acts as a non-clearing market: according to neoclassical theory most markets have a point of equilibrium without excess surplus or demand, but the labour market is expected to have a persistent level of unemployment. Contrasting the labour market to other markets also reveals persistent compensating differentials among similar workers. The standard competitive assumption leads to clear conclusions: workers earn their marginal product of labour.
Neoclassical microeconomic model — Supply.
Households are suppliers of labour. In microeconomics theory, people are assumed to be rational and seeking to maximize their utility function. In this labour market model, their utility function is determined by the choice between income and leisure. However, they are constrained by the working hours available to them.
Let w denote hourly wage.
Let k denote total working hours.
Let L denote working hours.
Let π denote other incomes or benefits.
Let A denote leisure hours.
The utility function and budget constraint can be expressed as following:
This can be shown in a graph that illustrates the trade-off between allocating your time between leisure activities and income generating activities. The linear constraint line indicates that there are only 24 hours in a day, and individuals must choose how much of this time to allocate to leisure activities and how much to working (If multiple days are being considered the maximum number of hours that could be allocated towards leisure or work is about 16 due to the necessity of sleep). This allocation decision is informed by the indifference curve labelled IC. The curve indicates the combinations of leisure and work that will give the individual a specific level of utility. The point where the highest indifference curve is just tangent to the constraint line (point A), illustrates the short-run equilibrium for this supplier of labour services.
"The Income/Leisure trade-off in the short run"
If the preference for consumption is measured by the value of income obtained, rather than work hours, this diagram can be used to show a variety of interesting effects. This is because the slope of the budget constraint becomes the wage rate. The point of optimisation (point A) reflects the equivalency between the wage rate and the marginal rate of substitution, leisure for income (the slope of the indifference curve). Because the marginal rate of substitution, leisure for income, is also the ratio of the marginal utility of leisure (MUL) to the marginal utility of income (MUY), one can conclude:
"Effects of a wage increase"
If wages increase, this individual's constraint line pivots up from X,Y1 to X,Y2. He/she can now purchase more goods and services. His/her utility will increase from point A on IC1 to point B on IC2.
To understand what effect this might have on the decision of how many hours to work, you must look at the income effect and substitution effect.
The wage increase shown in the previous diagram can be decomposed into two separate effects. The pure income effect is shown as the movement from point A to point C in the next diagram. Consumption increases from YA to YC and — assuming leisure is a normal good — leisure time increases from XA to XC (employment time decreases by the same amount; XA to XC).
"The Income and Substitution effects of a wage increase"
But that is only part of the picture. As the wage rate rises, the worker will substitute leisure hours for work hours, that is, will work more hours to take advantage of the higher wage rate, or in other words substitute away from leisure because of its higher opportunity cost. This substitution effect is represented by the shift from point C to point B. The net impact of these two effects is shown by the shift from point A to point B. The relative magnitude of the two effects depends on the circumstances. In some cases the substitution effect is greater than the income effect (in which case more time will be allocated to working), but in other cases the income effect will be greater than the substitution effect (in which case less time is allocated to working). The intuition behind this latter case is that the worker has reached the point where his marginal utility of leisure outweighs his marginal utility of income. To put it in less formal (and less accurate) terms: there is no point in earning more money if you don't have the time to spend it.
"The Labour Supply curve"
If the substitution effect is greater than the income effect, the labour supply curve (diagram to the left) will slope upwards to the right, as it does at point E for example. This individual will continue to increase his supply of labour services as the wage rate increases up to point F where he is working HF hours (each period of time). Beyond this point he will start to reduce the amount of labour hours he supplies (for example at point G he has reduced his work hours to HG) because the marginal utility of extra leisure exceeds the marginal utility of extra income above wage F. Where the supply curve is sloping upwards to the right (positive wage elasticity of labour supply), the substitution effect is greater than the income effect. Where it slopes upwards to the left (negative elasticity), the income effect is greater than the substitution effect. The direction of slope may change more than once for some individuals, and the labour supply curve is likely to be different for different individuals.
Other variables that affect this decision include taxation, welfare, work environment, and income as a signal of ability or social contribution.
Neoclassical microeconomic model — Demand.
This article has examined the labour supply curve which illustrates at every wage rate the maximum quantity of hours a worker will be willing to supply to the economy per period of time. Economists also need to know the maximum quantity of hours an employer will demand at every wage rate. To understand the quantity of hours demanded per period of time it is necessary to look at product production: labour demand is a derived demand, it is derived from the output levels in the goods market. Other aggregate methods of assessing demand include survey metrics and sources of real-time Labor Market Information.
A firm's labour demand is based on its marginal physical product of labour (MPPL). This is defined as the additional output (or physical product) that results from an increase of one unit of labour (or from an infinitesimal increase in labour). (If you are not familiar with these concepts, you might want to look at production theory basics before continuing with this article)
Labour demand is a derived demand; that is, hiring labour is not desired for its own sake but rather because it aids in producing output, which contributes to an employer's revenue and hence profits. The demand for an additional amount of labour depends on the Marginal Revenue Product (MRP) and the marginal cost (MC) of the worker. The MRP is calculated by multiplying the price of the end product or service by the Marginal Physical Product of the worker. If the MRP is greater than a firm's Marginal Cost, then the firm will employ the worker since doing so will increase profit. The firm only employs however up to the point where MRP=MC, and not beyond, in neoclassical economic theory.
The MRP of the worker is affected by other inputs to production with which the worker can work (e.g. machinery), often aggregated under the term "capital". It is typical in economic models for greater availability of capital for a firm to increase the MRP of the worker, all else equal. Education and training are counted as "human capital". Since the amount of physical capital affects MRP, and since financial capital flows can affect the amount of physical capital available, MRP and thus wages can be affected by financial capital flows within and between countries, and the degree of capital mobility within and between countries.
"The Marginal Physical Product of Labour"
According to neoclassical theory, over the relevant range of outputs, the marginal physical product of labour is declining (law of diminishing returns). That is, as more and more units of labour are employed, their additional output begins to decline. This is reflected by the slope of the MPPL curve in the diagram to the right. If the marginal physical product of labour is multiplied by the value of the output that it produces, we obtain the Value of marginal physical product of labour:
The value of marginal physical product of labour (formula_3) is the value of the additional output produced by an additional unit of labour. This is illustrated in the diagram by the VMPPL curve that is above the MPPL.
In perfectly competitive industries, the VMPPL is in identity with the marginal revenue product of labour (MRPL). This is because in competitive markets price is equal to marginal revenue, and marginal revenue product is defined as the marginal physical product times the marginal revenue from the output (MRP = MPP * MR). The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run.
Neoclassical microeconomic model — Equilibrium.
"A firm's labour demand in the short run (D) and an horizontal supply curve (S)"
The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run. In competitive markets, a firm faces a perfectly elastic supply of labour which corresponds with the wage rate and the marginal resource cost of labour (W = SL = MFCL). In imperfect markets, the diagram would have to be adjusted because MFCL would then be equal to the wage rate divided by marginal costs. Because optimum resource allocation requires that marginal factor costs equal marginal revenue product, this firm would demand L units of labour as shown in the diagram.
The demand for labour of this firm can be summed with the demand for labour of all other firms in the economy to obtain the aggregate demand for labour. Likewise, the supply curves of all the individual workers (mentioned above) can be summed to obtain the aggregate supply of labour. These supply and demand curves can be analysed in the same way as any other industry demand and supply curves to determine equilibrium wage and employment levels.
Wage differences exist, particularly in mixed and fully/partly flexible labour markets. For example, the wages of a doctor and a port cleaner, both employed by the NHS, differ greatly. There are various factors concerning this phenomenon. This includes the MRP of the worker. A doctor's MRP is far greater than that of the port cleaner. In addition, the barriers to becoming a doctor are far greater than that of becoming a port cleaner. To become a doctor takes a lot of education and training which is costly, and only those who excel in academia can succeed in becoming doctors. The port cleaner however requires relatively less training. The supply of doctors is therefore significantly less elastic than that of port cleaners. Demand is also inelastic as there is a high demand for doctors and medical care is a necessity, so the NHS will pay higher wage rates to attract the profession.
Information approaches.
In many real-life situations the assumption of perfect information is unrealistic. The firm does not necessarily know how hard a worker is working or how productive they are. This provides an incentive for workers to shirk from providing their full effort — since it is difficult for the employer to identify the hard-working and the shirking employees, there is no incentive to work hard and productivity falls overall, leading to more workers being hired and a lower unemployment rate.
One solution used recently (stock options) grants employees the chance to benefit directly from the firm's success. However, this solution has attracted criticism as executives with large stock option packages have been suspected of acting to over-inflate share values to the detriment of the long-run welfare of the firm. Another solution, foreshadowed by the rise of temporary workers in Japan and the firing of many of these workers in response to the financial crisis of 2008, is more flexible job contracts and terms that encourage employees to work less than full-time by partially compensating for the loss of hours, relying on workers to adapt their working time in response to job requirements and economic conditions instead of the employer trying to determine how much work is needed to complete a given task and overestimating.
Another aspect of uncertainty results from the firm's imperfect knowledge about worker ability. If a firm is unsure about a worker's ability, it pays a wage assuming that the worker's ability is the average of similar workers. This wage undercompenstates high ability workers and may drive them away from the labour market. Such phenomenon is called adverse selection and can sometimes lead to market collapse.
There are many ways to overcome adverse selection in labour market. One important mechanism is called signalling, pioneered by Michael Spence. In his classical paper on job signalling, Spence showed that even if formal education does not increase productivity, high ability workers may still acquire it just to signal their abilities. Employers can then use education as a signal to infer worker ability and pay higher wages to better educated workers.
Search models.
One of the major research achievements of the last 20 years has been the development of a framework with dynamic search, matching, and bargaining.
Personnel economics: hiring and incentives.
At the micro level, one sub-discipline eliciting increased attention in recent decades is analysis of internal labour markets, that is, "within" firms (or other organisations), studied in personnel economics from the perspective of personnel management. By contrast, external labour markets "imply that workers move somewhat fluidly between firms and wages are determined by some aggregate process where firms do not have significant discretion over wage setting." The focus is on "how firms establish, maintain, and end employment relationships and on how firms provide incentives to employees," including models and empirical work on incentive systems and as constrained by economic efficiency and risk/incentive tradeoffs relating to personnel compensation.
Criticisms.
Many sociologists, political economists, and heterodox economists claim that labour economics tends to lose sight of the complexity of individual employment decisions. These decisions, particularly on the supply side, are often loaded with considerable emotional baggage and a purely numerical analysis can miss important dimensions of the process, such as social benefits of a high income or wage rate regardless of the marginal utility from increased consumption or specific economic goals.
From the perspective of mainstream economics, neoclassical models are not meant to serve as a full description of the psychological and subjective factors that go into a given individual's employment relations, but as a useful approximation of human behavior in the aggregate, which can be fleshed out further by the use of concepts such as information asymmetry, transaction costs, contract theory etc.
Also missing from most labour market analyses is the role of unpaid labour. Even though this type of labour is unpaid it can nevertheless play an important part in society. The most dramatic example is child raising. However, over the past 25 years an increasing literature, usually designated as the economics of the family, has sought to study within household decision making, including joint labour supply, fertility, child raising, as well as other areas of what is generally referred to as home production.
Wage slavery.
The labour market, as institutionalised under today's market economic systems, has been criticised, especially by both mainstream socialists and anarcho-syndicalists, who utilise the term wage slavery as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.
According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book "On the Limits of State Action", classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the labourer works under external control, "we may admire what he does, but we despise what he is." Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.
The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy," politics will be "the shadow cast on society by big business". Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.
As per anthropologist David Graeber, the earliest wage labour contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued that most of the techniques of human organisation employed on factory workers during the industrial revolution were first developed on slave plantations.
Additionally, Marxists posit that labour-as-commodity, which is how they regard wage labour, provides an absolutely fundamental point of attack against capitalism. "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatisation of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."

</doc>
<doc id="18179" url="http://en.wikipedia.org/wiki?curid=18179" title="Lammas">
Lammas

In some English-speaking countries in the Northern Hemisphere, August 1 is Lammas Day (Anglo-Saxon "hlaf-mas", "loaf-mass"), the festival of the wheat harvest, and is the first harvest festival of the year. On this day it was customary to bring to church a loaf made from the new crop, which began to be harvested at Lammastide.
The loaf was blessed, and in Anglo-Saxon England it might be employed afterwards to work magic: a book of Anglo-Saxon charms directed that the lammas bread be broken into four bits, which were to be placed at the four corners of the barn, to protect the garnered grain.
In many parts of England, tenants were bound to present freshly harvested wheat to their landlords on or before the first day of August. In the "Anglo-Saxon Chronicle", where it is referred to regularly, it is called "the feast of first fruits". The blessing of first fruits was performed annually in both the Eastern and Western Churches on the first or the sixth of August (the latter being the feast of the Transfiguration of Christ).
Lammas has coincided with the feast of St. Peter in Chains, commemorating St. Peter's miraculous deliverance from prison, but in the liturgical reform of 1969, the feast of St. Alphonsus Liguori was transferred to this day, the day of St. Alphonsus' death.
History.
In medieval times the feast was sometimes known in England and Scotland as the "Gule of August", but the meaning of "gule" is unclear. Ronald Hutton suggests following the 18th-century Welsh clergyman antiquary John Pettingall that it is merely an Anglicisation of "Gŵyl Awst", the Welsh name of the "feast of August". "OED" and most etymological dictionaries give it a more circuitous origin similar to "gullet"; from O.Fr. "goulet", dim. of "goule", "throat, neck," from L. "gula" "throat,". 
Several antiquaries beginning with John Brady offered a back-construction to its being originally known as "Lamb-mass", under the undocumented supposition that tenants of the Cathedral of York, dedicated to St. Peter ad Vincula, of which this is the feast, would have been required to bring a live lamb to the church, or, with John Skinner, "because Lambs then grew out of season." This is a folk etymology, of which "OED" notes that it was "subsequently felt as if from ".
For many villeins, the wheat must have run low in the days before Lammas, and the new harvest began a season of plenty, of hard work and company in the fields, reaping in teams. Thus there was a spirit of celebratory play.
In the medieval agricultural year, Lammas also marked the end of the hay harvest that had begun after Midsummer. At the end of hay-making a sheep would be loosed in the meadow among the mowers, for him to keep who could catch it.
In Shakespeare's "Romeo and Juliet" (1.3.19) it is observed of Juliet, "Come Lammas Eve at night shall she [Juliet] be fourteen." Since Juliet was born Lammas eve, she came before the harvest festival, which is significant since her life ended before she could reap what she had sown and enjoy the bounty of the harvest, in this case full consummation and enjoyment of her love with Romeo.
Another well-known cultural reference is the opening of "The Battle of Otterburn": "It fell about the Lammas tide when the muir-men win their hay".
William Hone speaks in "The Every-Day Book" (1838) of a later festive Lammas day sport common among Scottish farmers near Edinburgh. He says that they "build towers...leaving a hole for a flag-pole in the centre so that they may raise their colours." When the flags over the many peat-constructed towers were raised, farmers would go to others' towers and attempt to "level them to the ground." A successful attempt would bring great praise. However, people were allowed to defend their towers, and so everyone was provided with a "tooting-horn" to alert nearby country folk of the impending attack and the battle would turn into a "brawl." According to Hone, more than four people had died at this festival and many more were injured. At the day's end, races were held, with prizes given to the townspeople.
Neopaganism.
Lughnasadh or Lammas is also the name used for one of the eight sabbats in the Neopagan Wheel of the Year. It is the first of the three autumn harvest festivals, the other two being the autumn equinox (also called Mabon) and Samhain. In the Northern Hemisphere it takes place around August 1, while in the Southern Hemisphere it is celebrated around February 1.
Other uses.
Lammas is one of the Scottish quarter days.
"Lammas leaves" or "Lammas growth" refers to a second crop of leaves produced in high summer by some species of trees in temperate countries to replace those lost to insect damage. They often differ slightly in shape, texture and/or hairiness from the earlier leaves.
In popular culture.
The Doctor Who serial, The Image of the Fendahl, takes place on Lammas Eve.
In the Inspector Morse episode "Day of the Devil", Lammas Day is presented as a Satanic (un)holy day, "the Devil's day".

</doc>
<doc id="18182" url="http://en.wikipedia.org/wiki?curid=18182" title="Longmeadow, Massachusetts">
Longmeadow, Massachusetts

Longmeadow is a town in Hampden County, Massachusetts, United States. The population was 15,784 at the 2010 census.
History.
Longmeadow was first settled in 1644, and officially incorporated on October 17, 1783. The town was originally farmland within the limits of Springfield. It remained relatively pastoral until the street railway was built "circa" 1910, when the population tripled over a fifteen-year period. After Interstate 91 was built in the wetlands on the west side of town, population tripled again between 1960 and 1975.
During the 19th and early 20th centuries, Longmeadow was best known as the site from which Longmeadow brownstone was mined. Several famous American buildings, including Princeton University's Neo-Gothic library are made of Longmeadow brownstone. In 1894, the more populous and industrialized "East Village" portion of the town containing the brownstone quarries split off to become East Longmeadow.
Longmeadow Country Club: Designed by famed golf course architect Donald Ross in 1922, the LCC was the proving ground for golf equipment designed and manufactured by the Spalding Co. of Chicopee. Bobby Jones, a consultant for Spalding, was also a member in standing at LCC and filmed a number of his instruction videos at LCC in the 1930s.
Geography.
Longmeadow is located in the western part of the state, just south of the city of Springfield, and is bordered on the west by the Connecticut River, to the east by East Longmeadow and to the south by Enfield, Connecticut. It extends approximately 3 mi north to south and 2.5 mi east to west. It is approximately 20 mi north of Hartford.
More than 30% of the town is permanent open space. Conservation areas on the west side of town include more than 750 acre bordering the Connecticut River. The area supports a wide range of wildlife including deer, beaver, wild turkeys, foxes, and eagles. Springfield's Forest Park (Springfield), which at 735 acre is the largest city park in New England, forms the northern border of the town. The private Twin Hills and public Franconia golf courses, plus town athletic fields and conservation land, cover nearly 2/3 of the eastern border of the town. Two large public parks, the Longmeadow Country Club, and three conservation areas account for the bulk of the remaining formal open space. Almost 20% of the houses in town are in proximity to a "dingle", a tree-lined steep sided sandy ravine with a wetland at the bottom that provides a privacy barrier between yards.
Longmeadow has a town common located along U.S. Route 5, on the west side of town. It is about 0.75-mile (1.2 kilometers) long. Roughly 100 houses date back before 1900, most of which are in the historic district near the town green. Houses along the photogenic main street are set back farther than in most towns of similar residential density. The town has three recently remodeled elementary schools, two secondary schools, and one high school. The commercial center of town is an area called "The Longmeadow Shops," including restaurants and clothing stores.
According to the United States Census Bureau, the town has a total area of 9.5 sqmi, of which 9.0 sqmi is land and 0.5 sqmi (5.05%) is water.
Demographics.
As of the census of 2000, there were 15,633 people, 5,734 households, and 4,432 families residing in the town. The population density was 1732.5 PD/sqmi. There were 5,879 housing units at an average density of 651.5 /sqmi. The racial makeup of the town was 95.42% White, 0.69% African American, 0.05% Native American, 2.90% Asian, 0.06% Pacific Islander, 0.26% from other races, and 0.62% from two or more races. Hispanic or Latino of any race were 1.09% of the population.
There were 5,734 households out of which 37.1% had children under the age of 18 living with them, 69.1% were married couples living together, 6.4% had a female householder with no husband present, and 22.7% were non-families. 20.4% of all households were made up of individuals and 14.0% had someone living alone who was 65 years of age or older. The average household size was 2.66 and the average family size was 3.09.
In the town the population was spread out with 26.8% under the age of 18, 4.6% from 18 to 24, 22.0% from 25 to 44, 28.7% from 45 to 64, and 17.8% who were 65 years of age or older. The median age was 43 years. For every 100 females there were 87.7 males. For every 100 females age 18 and over, there were 82.0 males.
The median income for a household in the town was $89,586, and the median income for a family was $105,578. Males had a median income of $68,238 versus $40,890 for females. The per capita income for the town was $38,949. About 1.0% of families and 2.1% of the population were below the poverty line, including 0.3% of those under age 18 and 8.3% of those age 65 or over.
Government.
The town government consists of a Select Board with five members, elected by the town. The public school system is governed by the School Committee. The School Committee is made up of seven voting member elected by the town, the superintendent of schools, two assistant-superidents, a secretary, and a student representative.
Education.
The Longmeadow public school system comprises six schools. Blueberry Hill School, Center School, and Wolf Swamp Road School are K−5 elementary schools. Williams Middle School and Glenbrook Middle School serve grades 6–8. Longmeadow High School serves all students in the town between grades 9 and 12. The town's elementary schools have been recently rebuilt, statements of interest for improvements to the two middle schools and Longmeadow High School were filed with the Massachusetts School Building Authority in 2007. In 2010, the voters of Longmeadow approved a 2.5% budget override to support the construction of a new 78 million dollar high school. The town received an estimated 34 million dollars in state funds to be used towards the new construction The new High School was completed and opened to students on February 26, 2013. After students and faculty had moved into the new school, the demolition of the old school was begun. The demolition was completed by June 2013. The school had its grand opening in September 2013 with both the brand new school and renovated business & administration wing open.
Longmeadow also hosts three private parochial schools, the (LYA), Heritage Academy and . LYA was established in 1946 in response to the Greater Springfield Jewish community's need for a quality Jewish day school. In 1999, LYA became the first Jewish day school to be accredited by the New England Association of Schools and Colleges (NEASC). The more than 120 students that the school serves each year from across the spectrum of Jewish life includes orthodox, conservative, reform and unaffiliated families. St. Mary's School, located behind St. Mary's church, serves Catholic students grades Pre-K through Grade 8.
Approximately 50% of the students at Longmeadow High School participate in the music program. The choruses have won gold medals at the MICCA competition. The jazz ensemble has won numerous gold medals as well, but no longer competes. The wind ensemble and symphony orchestra have had the honor of performing in Indianapolis, Boston (Boston Symphony Hall), and New York (Carnegie Hall). In 2010, Longmeadow was awarded The American Prize in Orchestral Performance. The music program's crowning achievement has been receiving three national Grammy Awards based on the high level of excellence maintained throughout all groups in the music program.
Longmeadow also contains the 46 acre primary campus for Bay Path University, a private undergraduate and graduate institution.

</doc>
<doc id="18183" url="http://en.wikipedia.org/wiki?curid=18183" title="Relative direction">
Relative direction

The most common relative directions are left, right, forward(s), backward(s), up, and down. No absolute direction corresponds to any of the relative directions. This is a consequence of the translational invariance of the laws of physics: nature, loosely speaking, behaves the same no matter what direction one moves. As demonstrated by the Michelson-Morley null result, there is no absolute inertial frame of reference. There are definite relationships between the relative directions, however. Left and right, forward and backward, and up and down are three pairs of complementary directions, each pair orthogonal to both of the others. Relative directions are also known as egocentric coordinates.
Traditions and conventions.
Since definitions of left and right based on the geometry of the natural environment are unwieldy, in practice, the meaning of relative direction words is conveyed through tradition, acculturation, education, and direct reference. One common definition of up and down uses gravity and the planet Earth as a frame of reference. Since there is a very noticeable force of gravity acting between the Earth and any other nearby object, down is defined as that direction which an object moves in reference to the Earth when the object is allowed to fall freely. Up is then defined as the opposite direction of down. Another common definition uses a human body, standing upright, as a frame of reference. In that case, up is defined as the direction from feet to head, perpendicular to the surface of the Earth. In most cases, up is a directionally oriented position generally opposite to that of the pull of gravity.
In situations where a common frame of reference is needed, it is most common to use an egocentric view. A simple example is road signage. Another example is stage blocking, where "stage left" "stage right" "stage up" and "stage down" are, by convention, defined from the actor's point of view, but up and down stage do not follow gravitational conventions of up and down. An example of a non-egocentric view is page layout, where the relative terms "upper half" "left margin," etc. are defined in terms of the observer but employed in reverse for a type compositor, returning to an egocentric view. In medicine and science, where precise definitions are crucial, relative directions (left and right) are the sides of the organism, not those of the observer. The same is true in heraldry, where left and right in a coat of arms is treated as if the shield was being held by the armiger. To avoid confusion, Latin terminology is employed: "dexter" and "sinister" for right and left. Proper right and proper left are terms mainly used to describe images, and overcome the potential confusion that a figure's right or "proper right" hand is on the left hand as the viewer of a frontal image sees it.
Forward and backward may be defined by referring to an object's or person's motion. Forward is defined as the direction in which the object is moving. Backward is then defined as the opposite direction to forward. Alternatively, 'forward' may be the direction pointed by the observer's nose, defining 'backward' as the direction from the nose to the sagittal border in the observer's skull. With respect to a ship 'forward' would indicate the relative position of any object lying in the direction the ship is pointing. For symmetrical objects, it is also necessary to define forward and backward in terms of expected direction. Many mass transit trains are built symmetrically with paired control booths, and definitions of forward, backward, left, and right are temporary.
Given significant distance from the magnetic poles, one can figure which hand is which using a magnetic compass and the sun. Facing the sun, before noon, the north pointer of the compass points to the "left" hand. After noon, it points to the "right".
Geometry of the natural environment.
The right-hand rule is one common way to relate the three principal directions. For many years a fundamental question in physics was whether a left-hand rule would be equivalent. Many natural structures, including human bodies, follow a certain "handedness", but it was widely assumed that nature did not distinguish the two possibilities. This changed with the discovery of parity violations in particle physics. If a sample of cobalt-60 atoms is magnetized so that they spin counterclockwise around some axis, the beta radiation resulting from their nuclear decay will be preferentially directed opposite that axis. Since counter-clockwise may be defined in terms of up, forward, and right, this experiment unambiguously differentiates left from right using only natural elements: if they were reversed, or the atoms spun clockwise, the radiation would follow the spin axis instead of being opposite to it.
Nautical terminology.
Bow, stern, port, and starboard are nautical terms that convey an impersonal relative direction in the context of the moving frame of persons aboard a ship. The need for impersonal terms is most clearly seen in a rowing shell where the majority of the crew face aft ("backwards") and the oars to their right are actually on the port side.
Cultures not using relative directions.
Most human cultures use relative directions for reference, but there are exceptions. The Australian Aboriginal people the Guugu Yimithirr have no words denoting the egocentric directions in their language; instead, they exclusively refer to cardinal directions, even when describing small-scale spaces. For instance, if they wanted someone to move over on the car seat to make room, they might say "move a bit to the east". To tell someone where exactly they left something in their house, they might say, "I left it on the southern edge of the western table." Or they might warn a person to "look out for that big ant just north of your foot". Other peoples "from Polynesia to Mexico and from Namibia to Bali" similarly have predominantly "geographic languages". American Sign Language makes heavy use of geographical direction through absolute orientation.
Left-right confusion.
Left-right confusion is the difficulty some people have in distinguishing the difference between the directions left and right. According to research by John R. Clarke (Drexel University) it affects about 15% of the population. These people can usually normally perform daily activities such as driving according to signs and navigating according to a map, but will often take a wrong turn when told to turn left or right and may have difficulties performing actions that require precise understanding of directional commands, such as ballroom dancing.

</doc>
<doc id="18184" url="http://en.wikipedia.org/wiki?curid=18184" title="Lizard">
Lizard

Lizards are a widespread group of squamate reptiles, with approximately over 6,000 species, ranging across all continents except Antarctica, as well as most oceanic island chains. The group, traditionally recognized as the suborder Lacertilia, is defined as all extant members of the Lepidosauria (reptiles with overlapping scales) that are neither sphenodonts (i.e., tuatara) nor snakes – they form an evolutionary grade. While the snakes are recognized as falling phylogenetically within the Toxicofera clade from which they evolved, the sphenodonts are the sister group to the squamates, the larger monophyletic group, which includes both the lizards and the snakes.
Biology.
Lizards typically have feet and external ears, while snakes lack both of these characteristics. However, because they are defined negatively as excluding snakes, lizards have no unique distinguishing characteristic as a group. Lizards and snakes share a movable quadrate bone, distinguishing them from the sphenodonts, which have more primitive and solid diapsid skulls. Many lizards can detach their tails to escape from predators, an act called autotomy. Vision, including color vision, is particularly well developed in most lizards, and most communicate with body language or bright colors on their bodies, as well as with pheromones. Lizards are the most speciose among extant reptiles, comprising about 60% of all living species.
The adult length of species within the suborder ranges from a few centimeters for chameleons such as "Brookesia micra" and geckos such as "Sphaerodactylus ariasae" to nearly 3 m in the case of the largest living varanid lizard, the Komodo dragon. Some extinct varanids reached great size. The extinct aquatic mosasaurs reached 17 m, and the giant monitor "Megalania" is estimated to have reached up to 7 m long.
The name Sauria was coined by James Macartney (1802); it was the Latinisation of the French name "Sauriens", coined by Alexandre Brongniart (1800) for an order of reptiles in the classification proposed by the author, containing lizards and crocodilians, later discovered not to be each other's closest relatives. Later authors used the term "Sauria" in a more restricted sense, i.e. as a synonym of Lacertilia, a suborder of Squamata that includes all lizards but excludes snakes. This classification is rarely used today because Sauria so-defined is a paraphyletic group. It was defined as a clade by Jacques Gauthier, Arnold G. Kluge and Timothy Rowe (1988) as the group containing the most recent common ancestor of archosaurs and lepidosaurs (the groups containing crocodiles and lizards, as per Mcartney's original definition) and all its descendants. A different definition was formulated by Michael deBraga and Olivier Rieppel (1997) who defined Sauria as the clade containing the most recent common ancestor of Choristodera, Archosauromorpha and Lepidosauromorpha and all their descendants. However, neither of these uses have gained wide acceptance among researchers specializing in lizards.
Physiology.
Sight is very important for most lizards, both for locating prey and for communication, and, as such, many lizards have highly acute color vision. Most lizards rely heavily on body language, using specific postures, gestures, and movements to define territory, resolve disputes, and entice mates. Some species of lizards also use bright colors, such as the iridescent patches on the belly of "Sceloporus". These colors would be highly visible to predators, so are often hidden on the underside or between scales and only revealed when necessary.
The particular innovation in this respect is the dewlap, a brightly colored patch of skin on the throat, usually hidden between scales. When a display is needed, a lizard can erect the hyoid bone of its throat, resulting in a large vertical flap of brightly colored skin beneath the head which can be then used for communication. Anoles are particularly famous for this display, with each species having specific colors, including patterns only visible under ultraviolet (UV) light, as many lizards can see UV light.
Shedding and regenerating tails.
Lizard tails are often a different and dramatically more vivid color than the rest of the body so as to encourage potential predators to strike for the tail first.
Many lizard species (including geckos, skinks, and others) are capable of shedding part of their tails through a process called autotomy. This is an example of the pars pro toto principle, sacrificing "a part for the whole", and is employed by lizards to allow them to escape when captured by the tail by a predator. The detached tail writhes and wiggles, creating a deceptive sense of continued struggle, distracting the predator's attention from the fleeing prey animal.
The lizard will partially regenerate its tail over a period of weeks. A 2014 research identified 326 genes involving the regeneration of lizard tails. The new section will contain cartilage rather than bone, and the skin may be distinctly discolored compared to the rest of the body.
Evolution and relationships.
The retention of the basic 'reptilian' amniote body form by lizards makes it tempting to assume any similar animal, alive or extinct, is also a lizard. However, this is not the case, and lizards as squamates are part of a well-defined group.
The earliest amniotes were superficially lizard-like, but had solid, box-like skulls, with openings only for eyes and nostrils, termed the anapsid condition. Turtles retain, or have re-evolved, this skull form. Early anapsids later gave rise to two new groups with additional holes in their skulls to make room for and anchor larger jaw muscles. The synapsids, with a single fenestra, gave rise to the large, but generally lizard-like pelycosaurs, which include "Dimetrodon", a group which again gave rise to the therapsids, including the cynodonts, from which the modern mammals would evolve.
The modern "Sphenodon" retains the basic lepidosaur skull, distinguishing it from true lizards in spite of superficial similarities. Squamates, including snakes and all true lizards, further lightened the skull by eliminating the lower margin of the lower skull opening.
The earliest known fossil remains of a lizard belong to the iguanian species "Tikiguania estesi", found in the Tiki Formation of India, which dates to the Carnian stage of the Triassic period, about 220 million years ago. However, doubt has been raised over the age of "Tikiguania" because it is almost indistinguishable from modern agamid lizards. The "Tikiguania" remains may instead be late Tertiary or Quaternary in age, having been washed into much older Triassic sediments. Lizards are most closely related to a group called Rhynchocephalia, which includes the tuatara. Rhynchocephalians first appeared in the Late Triassic, so it can be inferred that the lizard-rhynchocephalian divergence occurred at this time and that the earliest lizards appeared in the Triassic.
Mitochondrial phylogenetics suggest that the first lizards evolved in the late Permian. Most evolutionary relationships within the squamates are not yet completely worked out, with the relationship of snakes to other groups being the most problematic. On the basis of morphological data, iguanid lizards were thought to have diverged from other squamates very early on, but recent molecular phylogenies, both from mitochondrial and nuclear DNA, do not support this. Because snakes have a faster molecular clock than other squamates, and few early snake and snake ancestor fossils have been found, resolving the relationship between snakes and other squamate groups is difficult.
Lizard diversification.
Lacertilia comprises four generally recognized suborders, Iguania, Gekkota, Amphisbaenia and Autarchoglossa, with the "blind skinks" in the family Dibamidae having an uncertain position. While traditionally excluded from the lizards, the snakes are usually classified as a clade with a similar subordinal rank.
Iguania.
The suborder Iguania, found in Africa, southern Asia, Australia, the New World and the islands of the west Pacific, forms the sister group to the remainder of the squamata. The various species are largely arboreal, and have primitively fleshy, non-prehensile tongues, some even have scales, but this condition is obviously highly modified in the chameleons. This clade includes the following families:
Gekkota.
Active hunters, the Gekkota include three families comprising the distinctive cosmopolitan geckos and the legless, flap-footed lizards of Australia and New Guinea. Like snakes, the flap-footed lizards and most geckos lack eyelids. Unlike snakes, they use their tongues to clean their often highly developed eyes. While gecko feet have unique surfaces that allow them to cling to glass and run on ceilings, 
the flap-foot has lost its limbs. The three families of this suborder are:
Relationship with humans.
Most lizard species are harmless to humans. Only the largest lizard species, the Komodo dragon, which reaches 3.3 m (11 ft) in length and weighs up to 166 kg (365 lb), has been known to stalk, attack, and, on occasion, kill humans. An eight-year-old Indonesian boy died from blood loss after an attack in 2007. The venoms of the Gila monster and beaded lizard are not usually deadly, but they can inflict extremely painful bites due to powerful jaws.
Numerous species of lizard are kept as pets, including bearded dragons, iguanas, anoles, and geckos (such as the popular leopard gecko). Some lizards have an affinity for humans, but many are suspicious or skittish around them. Lizards that bite humans are very rare. Lizards are predominantly insectivorous, but some eat fruit, or vegetables. Live crickets and worms are the most typical foods for pet lizards, though the crested gecko (not a friendly lizard to humans) can feed entirely on fruit.
Lizard symbolism plays important, though rarely predominant, roles in some cultures (e.g., Tarrotarro in Australian Aboriginal mythology). The Moche people of ancient Peru worshipped animals and often depicted lizards in their art. According to a popular legend in Maharashtra, a common Indian monitor, with ropes attached, was used to scale the walls of the Sinhagad fort in the Battle of Sinhagad.
Green iguanas are eaten in Central America, where they are referred to sometimes as "chicken of the tree" after their habit of resting in trees and their supposed chicken-like taste, and spiny-tailed lizards are eaten in Africa. In North Africa, "Uromastyx" species are considered "dhaab" or 'fish of the desert' and eaten by nomadic tribes.
Classification.
Suborder Lacertilia (Sauria) – (lizards) 

</doc>
<doc id="18185" url="http://en.wikipedia.org/wiki?curid=18185" title="List of deists">
List of deists

This is a partial list of people who have been categorized as deists, the belief in a deity based on natural religion only, or belief in religious truths discovered by people through a process of reasoning, independent of any revelation through scripture or prophets. They have been selected for their influence on Deism, or for their fame in other areas.

</doc>
<doc id="18186" url="http://en.wikipedia.org/wiki?curid=18186" title="List of Hindus">
List of Hindus

The following is a list of notable people who are or were Hindus.

</doc>
<doc id="18187" url="http://en.wikipedia.org/wiki?curid=18187" title="Book of Leviticus">
Book of Leviticus

The Book of Leviticus (; from Greek Λευιτικός, "Leuitikos", meaning "relating to the Levites") is the third book of the Hebrew Bible, and the third of five books of the Torah (or Pentateuch). Its Hebrew name, Hebrew: ויקרא‎, "Vayikra"/"Wayiqra", comes from its first word, "And He called." The English name is from the Latin "Leviticus", taken in turn from Greek and a reference to the Levites, the tribe of Aaron, from whom the priests descended. The book, however, addresses all the people of Israel (1:2) though some passages address the priests specifically (6:8). Most of its chapters (1–7, 11–27) consist of God's speeches to Moses which he is commanded to repeat to the Israelites. This takes place within the story of the Israelites' Exodus after they escaped Egypt and reached Mt. Sinai (Exodus 19:1). The book of Exodus narrates how Moses led the Israelites in building the Tabernacle (Exodus 35–40) based on God's instructions (Exodus 25–31). Then in Leviticus, God tells the Israelites and their priests how to make offerings in the Tabernacle and how to conduct themselves while camped around the holy tent sanctuary. Leviticus takes place during the month or month-and-a-half between the completion of the Tabernacle (Exodus 40:17) and the Israelites' departure from Sinai (Numbers 1:1, 10:11).
The instructions of Leviticus emphasize ritual, legal and moral practices rather than beliefs. Nevertheless, they reflect the world view of the creation story in Genesis 1 that God wishes to live with humans. The book teaches that faithful performance of the sanctuary rituals can make that possible, so long as the people avoid sin and impurity whenever possible. The rituals, especially the sin and guilt offerings, provide the means to gain forgiveness for sins (Leviticus 4–5) and purification from impurities (Leviticus 11–16) so that God can continue to live in the Tabernacle in the midst of the people.
The traditional view is that Leviticus was compiled by Moses, or that the material in it goes back to his time, but internal clues suggest that the book developed much later in Israel's history and was completed either near the end of the Judean monarchy in the late seventh century BCE or in the exilic and post-exilic period of the sixth and fifth centuries BCE. Scholars debate whether it was written primarily for Jewish worship in exile that centered on reading or preaching, or was aimed instead at worshipers at temples in Jerusalem and Samaria. Scholars are practically unanimous that the book had a long period of growth, that it includes some material of considerable antiquity, and that it reached its present form in the Persian period (538–332 BCE).
Structure.
"(The outlines provided by commentaries are similar, though not identical; compare those of Wenham, Hartley, Milgrom, and Watts)
I. Laws on sacrifice (1:1–7:38)
II. Institution of the priesthood (8:1–10:20)
III. Uncleanliness and its treatment (11:1–15:33)
IV. Day of Atonement: purification of the tabernacle from the effects of uncleanliness and sin (ch. 16)
V. Prescriptions for practical holiness (the Holiness Code, chs. 17–26)
V. Redemption of votive gifts (ch. 27)
Summary.
Chapters 1–5 describe the various sacrifices from the sacrificers' point of view, although the priests are essential for handling the blood. Chapters 6–7 go over much the same ground, but from the point of view of the priest, who, as the one actually carrying out the sacrifice and dividing the "portions", needs to know how this is to be done. Sacrifices are to be divided between God, the priest, and the offerers, although in some cases the entire sacrifice is a single portion consigned to God—i.e., burnt to ashes.
Chapters 8–10 describe the consecration by Moses of Aaron and his sons as the first priests, the first sacrifices, and God's destruction of two of Aaron's sons for ritual offenses. The purpose is to underline the character of altar priesthood (i.e., those priests empowered to offer sacrifices to God) as an Aaronite privilege, and the responsibilities and dangers of their position.
With sacrifice and priesthood established, chapters 11–15 instruct the lay people on purity (or cleanliness). Eating certain animals produces uncleanliness, as does giving birth; certain skin diseases (but not all) are unclean, as are certain conditions affecting walls and clothing (mildew and similar conditions); and genital discharges, including female menses and male gonorrhea, are unclean. The reasoning behind the food rules are obscure; for the rest the guiding principle seems to be that all these conditions involve a loss of "life force", usually but not always blood.
Leviticus 16 concerns the Day of Atonement. This is the only day on which the High Priest is to enter the holiest part of the sanctuary, the holy of holies. He is to sacrifice a bull for the sins of the priests, and a goat for the sins of the laypeople. A second goat is to be sent into the desert to "Azazel", bearing the sins of the whole people. Azazel may be a wilderness-demon, but its identity is mysterious.
Chapters 17–26 are the Holiness code. It begins with a prohibition on all slaughter of animals outside the Temple, even for food, and then prohibits a long list of sexual contacts and also child sacrifice. The "holiness" injunctions which give the code its name begin with the next section: penalties are imposed for the worship of Molech, consulting mediums and wizards, cursing one's parents and engaging in unlawful sex. Priests are instructed on mourning rituals and acceptable bodily defects. Blasphemy is to be punished with death, and rules for the eating of sacrifices are set out; the calendar is explained, and rules for sabbatical and Jubilee years set out; and rules are made for oil lamps and bread in the sanctuary; and rules are made for slavery. The code ends by telling the Israelites they must choose between the law and prosperity on the one hand, or, on the other, horrible punishments, the worst of which will be expulsion from the land.
Chapter 27 is a disparate and probably late addition telling about persons and things dedicated to the Lord and how vows can be redeemed instead of fulfilled.
Line 74 edits arise from the following sources:;
Composition.
The majority of scholars agree that the Pentateuch received its final form during the Persian period (538–332 BCE). Nevertheless, they also agree that Leviticus had a long period of growth, with many additions and editings, before reaching that form.
The entire book of Leviticus is composed of Priestly literature. Most scholars see chapters 1–16 (the Priestly code) and chapters 17–26 (the Holiness code) as the work of two related schools, but while the Holiness material employs the same technical terms as the Priestly code, it broadens their meaning from pure ritual to the theological and moral, turning the ritual of the Priestly code into a model for the relationship of Israel to God: as the tabernacle is made holy by the presence of the Lord and kept apart from uncleanliness, so He will dwell among Israel when Israel is purified (made holy) and separated from other peoples.
The ritual instructions in the Priestly code apparently grew from priests giving instruction and answering questions about ritual matters; the Holiness code (or H) used to be regarded as a separate document later incorporated into Leviticus, but it seems better to think of the Holiness authors as editors who worked with the Priestly code and actually produced Leviticus as we now have it.
Themes.
Ritual.
Many scholars argue that the rituals of Leviticus have a theological meaning concerning Israel's relationship with its God. Jacob Milgrom has been especially influential in spreading this view. He maintained that the priestly regulations in Leviticus expressed a rational system of theological thought. The writers expected them to be put into practice in Israel’s temple, so the rituals would express this theology as well, as well as ethical concern for the poor. Milgrom also argued that the book’s purity regulations (chaps. 11-15) are based in ethical thinking. Many other interpreters have followed Milgrom in exploring the theological and ethical implications of Leviticus’s regulations (e.g. Marx, Balentine), though some have questioned how systematic they really are. Ritual, therefore, is not a series of actions undertaken for their own sake, but a means of maintaining the relationship between God, the world, and humankind.
Sacrifice.
The Priestly theology of sacrifice begins with the Creation, when humankind is not given permission to eat meat (Genesis 1:26–30); after the Flood God gives permission to men to slaughter animals and eat their meat, but the animals are to be offered as sacrifices (Genesis 9:3–4). Sacrifice is in a sense a gift (offering) to God, but also involves the transfer of the offering from the everyday to the sacred; those who eat meat are eating a sanctified meal, and God's share in this is the "pleasing odour" released as the offering (incense or meat) is burnt.
In Leviticus, sacrifice is to be offered only by priests. This does not conform with the picture given elsewhere in the bible, where sacrifices are offered by a wide range of people (e.g. Manoah the judge, Samuel and Elijah the prophets, and kings Saul, David and Solomon, none of whom are priests) and the general impression is that any head of family could make a sacrifice. Most of these sacrifices are burnt offerings, and there is no mention of sin offerings. For these reasons there is a widespread scholarly view that the sacrificial rules of Leviticus 1–16 were introduced after the Babylonian exile, when circumstances allowed the priestly writers to describe the rituals so as to express their worldview of an idealised Israel living its life as a holy community in observance of the priestly prescriptions. Other scholars think these changes came about earlier, during Judah's monarchy.
Kehuna (Jewish Priesthood).
The main function of the priests is service at the altar, and only the sons of Aaron are priests in the full sense. (Ezekiel also distinguishes between altar-priests and lower Levites, but in Ezekiel the altar-priests are called sons of Zadok instead of sons of Aaron; many scholars see this as a remnant of struggles between different priestly factions in First Temple times, resolved by the Second Temple into a hierarchy of Aaronite altar-priests and lower-level Levites, including singers, gatekeepers and the like).
In chapter 10, God kills Nadab and Abihu, the oldest sons of Aaron, for offering "strange incense". Fortunately, Aaron has two sons left. Commentators have read various messages in the incident: a reflection of struggles between priestly factions in the post–Exilic period (Gerstenberger); or a warning against offering incense outside the Temple, where there might be the risk of invoking strange gods (Milgrom). In any case, the sanctuary has been polluted by the bodies of the two dead priests, leading into the next theme, holiness.
Uncleanliness and purity.
Ritual purity is essential for an Israelite to be able to approach God and remain part of the community. Uncleanliness threatens holiness; Chapters 11–15 review the various causes of uncleanliness and describe the rituals which will restore cleanliness; cleanliness is to be maintained through observation of the rules on sexual behaviour, family relations, land ownership, worship, sacrifice, and observance of holy days.
Yahweh dwells with Israel in the holy of holies. All of the priestly ritual is focused on Yahweh and the construction and maintenance of a holy space, but sin generates impurity, as do everyday events such as childbirth; impurity pollutes the holy dwelling place. Failure to ritually purify the sacred space could result in God leaving, which would be disastrous.
Atonement.
Through sacrifice the priest "makes atonement" for sin and the offerer is forgiven (but only if God accepts the sacrifice—forgiveness comes only from God). Atonement rituals involve blood, poured or sprinkled, as the symbol of the life of the victim: the blood has the power to wipe out or absorb the sin. The role of atonement is reflected structurally in two-part division of the book: chapters 1–16 call for the establishment of the institution for atonement, and chapters 17–27 call for the life of the atoned community in holiness.
Holiness.
The consistent theme of chapters 17–26 is the repeated phrase, "Be holy, for I the Lord your God am holy." Holiness in ancient Israel had a different meaning than in contemporary usage: it might have been regarded as the "god-ness" of God, an invisible but physical and potentially dangerous force. Specific objects, or even days, can be holy, but they derive holiness from being connected with God—the seventh day, the tabernacle, and the priests all derive their holiness from God. As a result, Israel had to maintain its own holiness in order to live safely alongside God.
The need for holiness is directed to the possession of the Promised Land (Canaan), where the Jews will become a holy people: "You shall not do as they do in the land of Egypt where you dwelt, and you shall not do as they do in the land of Canaan to which I am bringing you...You shall do my ordinances and keep my statutes...I am the Lord, your God" (ch. 18:3).
Subsequent tradition.
Leviticus, as part of the Torah, became the law book of Jerusalem's second temple as well as of the Samaritan temple. Evidence of its influence was found among the Dead Sea Scrolls, which included fragments of seventeen manuscripts of Leviticus dating from the third to the first centuries BCE. Many other Qumran scrolls cite the book, especially the Temple Scroll and 4QMMT.
Leviticus's instructions for animal offerings have not been observed by Jews or Christians since the first century CE. Because of the destruction of the temple in Jerusalem in 70 CE, Jewish worship has focused on prayer and the study of Torah. Nevertheless, Leviticus constitutes a major source of Jewish law and is traditionally the first book taught to children in the Rabbinic system of education. There are two main Midrashim on Leviticus—the halakhic one (Sifra) and a more aggadic one (Vayikra Rabbah).
In the New Testament, the letter to the Hebrews in particular uses ideas and images from Leviticus to describe Christ as the high priest who offers his own blood as a sin offering. Therefore, Christians do not make animal offerings either, as Gordon Wenham summarized: "With the death of Christ the only sufficient "burnt offering" was offered once and for all, and therefore the animal sacrifices which foreshadowed Christ's sacrifice were made obsolete."
Christians generally have the view that the New Covenant supersedes (i.e., replaces) the Old Testament's ritual laws, which includes many of the rules in Leviticus. Christians therefore have usually not observed Leviticus' rules regarding diet, purity, and agriculture. Christian teachings have differed, however, as to where to draw the line between ritual and moral regulations.
External links.
Online versions of Leviticus:
Related article:
Free Online Bibliography on Leviticus:

</doc>
<doc id="18188" url="http://en.wikipedia.org/wiki?curid=18188" title="L. Frank Baum">
L. Frank Baum

Lyman Frank Baum (May 15, 1856 – May 6, 1919), better known by his pen name L. Frank Baum, was an American author chiefly known for his children's books, particularly "The Wonderful Wizard of Oz". He wrote thirteen novel sequels, nine other fantasy novels, and a host of other works (55 novels in total, plus four "lost" novels, 83 short stories, over 200 poems, an unknown number of scripts, and many miscellaneous writings), and made numerous attempts to bring his works to the stage and screen. His works anticipated such century-later commonplaces as television, augmented reality, laptop computers ("The Master Key"), wireless telephones ("Tik-Tok of Oz"), women in high risk, action-heavy occupations ("Mary Louise in the Country"), and the ubiquity of advertising on clothing ("Aunt Jane's Nieces at Work").
Childhood and early life.
Baum was born in Chittenango, New York, in 1856, into a devout Methodist family. He had German, Scots-Irish, and English ancestry, and was the seventh of nine children of Cynthia Ann (née Stanton) and Benjamin Ward Baum, only five of whom survived into adulthood. "Lyman" is the name of his father's brother but having always disliked it he preferred his middle name, "Frank".
Benjamin Baum was a wealthy businessman, ultimately for providing barrels during the Pennsylvania oil rush. Baum grew up on his parents' expansive estate, Rose Lawn, that he fondly recalled as a sort of paradise. Rose Lawn was located in Mattydale, New York. As a young, sickly and daydreaming child, he was tutored at home with his siblings. From 12 years of age, he spent two miserable years at Peekskill Military Academy, but after being severely disciplined for daydreaming he had a possible psychogenic heart attack and was allowed to return home.
Baum started writing early in life, possibly prompted by his father buying him a cheap printing press. He had always been close to his younger brother Henry (Harry) Clay Baum who helped in the production of "The Rose Lawn Home Journal". The brothers published several issues of the journal, which included advertisements; they may have sold issues. By 17 years, Baum established a second amateur journal, "The Stamp Collector", printed an 11-page pamphlet called "Baum's Complete Stamp Dealers' Directory", and started a stamp dealership with friends.
At 20 years, Baum took on the then national craze—the breeding of fancy poultry. He specialized in raising of the Hamburg. In March 1880 he established a monthly trade journal, "The Poultry Record", and in 1886, when Baum was 30 years old, his first book was published: "The Book of the Hamburgs: A Brief Treatise upon the Mating, Rearing, and Management of the Different Varieties of Hamburgs".
Baum had a flair for being the spotlight of fun in the household, including during times of financial difficulties. His selling of fireworks made the Fourth of July memorable. His skyrockets, Roman candles, and fireworks filled the sky, while many people around the neighborhood would gather in front of the house to watch the displays. Christmas was even more festive. Baum dressed as Santa Claus for the family. His father would place the Christmas tree behind a curtain in the front parlor so that Baum could talk to everyone while he decorated the tree without people managing to see him. He maintained this tradition all his life.
Career.
Theater.
Baum embarked on his lifetime infatuation, and wavering financial success, with the theater. A local theatrical company duped him into replenishing their stock of costumes on the promise of leading roles to follow his way. Disillusioned, Baum left the theatre — temporarily — and went to work as a clerk in his brother-in-law's dry goods company in Syracuse. From this experience may have evolved his story, "The Suicide of Kiaros", first published in the literary journal, "The White Elephant". A fellow clerk one day was found locked in a store room dead probably from suicide. Baum could never long stay away from the stage. He played roles in plays, performing under the stage names of Louis F. Baum and George Brooks.
In 1880, his father built him a theatre in Richburg, New York, and Baum set about writing plays and gathering a company to act in them. "The Maid of Arran", a melodrama with songs based on William Black's novel "A Princess of Thule", proved a modest success. Baum not only wrote the play but composed songs for it (making it a prototypical musical, as its songs relate to the narrative), and acted in the leading role. His aunt, Katharine Gray, played his character's aunt. She was the founder of Syracuse Oratory School, and Baum advertised his services in her catalog to teach theatre, including stage business, playwriting, directing, and translating (French, German, and Italian), revision, and operettas, though he was not employed to do so. On November 9, 1882, Baum married Maud Gage, a daughter of Matilda Joslyn Gage, a famous women's suffrage and radical feminist activist. While Baum was touring with "The Maid of Arran", the theatre in Richburg caught fire during a production of Baum's ironically-titled parlor drama, "Matches", destroying not only the theatre, but the only known copies of many of Baum's scripts, including "Matches", as well as costumes.
The South Dakota years.
In July 1888, Baum and his wife moved to Aberdeen, Dakota Territory, where he opened a store, "Baum's Bazaar". His habit of giving out wares on credit led to the eventual bankrupting of the store, so Baum turned to editing a local newspaper, "The Aberdeen Saturday Pioneer", where he wrote a column, "Our Landlady". Following the death of Sitting Bull at the hands of a federal agent, Baum urged the wholesale extermination of all America's native peoples in a column he wrote on December 20, 1890. On January 3, 1891 he reverted to the subject in an editorial response to the Wounded Knee Massacre: 
"The Pioneer has before declared that our only safety depends upon the total extirmination ["sic"] of the Indians. Having wronged them for centuries, we had better, in order to protect our civilization, follow it up by one more wrong and wipe these untamed and untamable creatures from the face of the earth."
A recent analysis of these editorials has challenged their literal interpretation, suggesting that the actual intent of Baum was to generate sympathy for the Indians via obnoxious argument, ostensibly promoting the contrary position.
Baum's description of Kansas in "The Wonderful Wizard of Oz" is based on his experiences in drought-ridden South Dakota. During much of this time, Matilda Joslyn Gage was living in the Baum household. While Baum was in South Dakota, he sang in a quartet that included a man who would become one of the first Populist (People's Party) Senators in the U.S., James Kyle.
Writing.
After Baum's newspaper failed in 1891, he, Maud and their four sons moved to Humboldt Park section of Chicago, where Baum took a job reporting for the "Evening Post". Beginning in 1897, for several years he edited a magazine for advertising agencies focused on window displays in stores. The major department stores created elaborate Christmas time fantasies, using clockwork mechanisms that made people and animals appear to move. In 1900, Baum published a book about window displays in which he stressed the importance of mannequins in drawing customers. He also had to work as a traveling salesman.
In 1897, he wrote and published "Mother Goose in Prose", a collection of Mother Goose rhymes written as prose stories, and illustrated by Maxfield Parrish. "Mother Goose" was a moderate success, and allowed Baum to quit his door-to-door sales job (which had had a negative impact on his health). In 1899 Baum partnered with illustrator W. W. Denslow, to publish "Father Goose, His Book", a collection of nonsense poetry. The book was a success, becoming the best-selling children's book of the year.
"The Wonderful Wizard of Oz".
In 1900, Baum and Denslow (with whom he shared the copyright) published "The Wonderful Wizard of Oz" to much critical acclaim and financial success. The book was the best-selling children's book for two years after its initial publication. Baum went on to write thirteen more novels based on the places and people of the Land of Oz.
"The Wizard of Oz: Fred R. Hamlin's Musical Extravaganza".
Two years after "Wizard"'s publication, Baum and Denslow teamed up with composer Paul Tietjens and director Julian Mitchell to produce a musical stage version of the book under Fred R. Hamlin. Baum and Tietjens had worked on a musical of "The Wonderful Wizard of Oz" in 1901 and based closely upon the book, but it was rejected. This stage version, the first to use the shortened title "The Wizard of Oz", opened in Chicago in 1902, then ran on Broadway for 293 stage nights from January to October 1903. It returned to Broadway in 1904, where it played from March to May and again from November to December. It successfully toured the United States with much of the same cast, as was done in those days, until 1911, and then became available for amateur use. The stage version starred David C. Montgomery and Fred Stone as the Tin Woodman and Scarecrow respectively, which shot the pair to instant fame. The stage version differed quite a bit from the book, and was aimed primarily at adults. Toto was replaced with Imogene the Cow, and Tryxie Tryfle, a waitress, and Pastoria, a streetcar operator, were added as fellow cyclone victims. The Wicked Witch of the West was eliminated entirely in the script, and the plot became about how the four friends, being allied with the usurping Wizard, were hunted as traitors to Pastoria II, the rightful King of Oz. It is unclear how much control or influence Baum had on the script; it appears that many of the changes were written by Baum against his wishes due to contractual requirements with Hamlin. Jokes in the script, mostly written by Glen MacDonough, called for explicit references to President Theodore Roosevelt, Senator Mark Hanna, Rev. Andrew Danquer and oil magnate John D. Rockefeller. Although use of the script was rather free-form, the line about Hanna was ordered dropped as soon as Hamlin got word of his death in 1904.
Beginning with the success of the stage version, most subsequent versions of the story, including newer editions of the novel, have been titled "The Wizard of Oz", rather than using the full, original title. In more recent years, restoring the full title has become increasingly common, particularly to distinguish the novel from the Hollywood film.
Baum wrote a new Oz book, "The Marvelous Land of Oz", with a view to making it into a stage production, which was titled "The Woggle-Bug", but since Montgomery and Stone balked at appearing when the original was still running, the Scarecrow and Tin Woodman were omitted from this adaptation, which was seen as a self-rip-off by critics and proved to be a major flop before it could reach Broadway. He also worked for years on a musical version of "Ozma of Oz", which eventually became "The Tik-Tok Man Of Oz". This did fairly well in Los Angeles, but not well enough to convince producer Oliver Morosco to mount a production in New York. He also began a stage version of "The Patchwork Girl of Oz", but this was ultimately realized as a film".
Later life and work.
With the success of "Wizard" on page and stage, Baum and Denslow hoped lightning would strike a third time and in 1901 published "Dot and Tot of Merryland". The book was one of Baum's weakest, and its failure further strained his faltering relationship with Denslow. It would be their last collaboration. Baum would work primarily with John R. Neill on his fantasy work beginning in 1904, but Baum met Neill few times (all before he moved to California) and often found Neill's art not humorous enough for his liking, and was particularly offended when Neill published "The Oz Toy Book: Cut-outs for the Kiddies" without authorization.
Several times during the development of the Oz series, Baum declared that he had written his last Oz book and devoted himself to other works of fantasy fiction based in other magical lands, including "The Life and Adventures of Santa Claus" and "Queen Zixi of Ix". However, persuaded by popular demand, letters from children, and the failure of his new books, he returned to the series each time. Even so, his other works remained very popular after his death, with "The Master Key" appearing on "St. Nicholas Magazine"'s survey of readers' favorite books well into the 1920s.
In 1905, Baum declared plans for an Oz amusement park. In an interview, he mentioned buying Pedloe Island off the coast of California to turn it into an Oz park. Trouble is, not only is there no evidence that he purchased such an island, no one has ever been able to find any island whose name even resembles Pedloe in that area. Nevertheless, Baum stated to the press that he had discovered a Pedloe Island off the coast of California and that he had purchased it to be "the Marvelous Land of Oz," intending it to be "a fairy paradise for children." Eleven-year-old Dorothy Talbot of San Francisco was reported to be ascendant to the throne on March 1, 1906, when the Palace of Oz was expected to be completed. Baum planned to live on the island, with administrative duties handled by the princess and her all-child advisers. Plans included statues of the Scarecrow, Tin Woodman, Jack Pumpkinhead, and H.M. Woggle-Bug, T.E. Baum abandoned his Oz park project after the failure of "The Woggle-Bug", which was playing at the Garrick Theatre in 1905.
Because of his lifelong love of theatre, he financed elaborate musicals, often to his financial detriment. One of Baum's worst financial endeavors was his "The Fairylogue and Radio-Plays" (1908), which combined a slideshow, film, and live actors with a lecture by Baum as if he were giving a travelogue to Oz. However, Baum ran into trouble and could not pay his debts to the company who produced the films. He did not get back to a stable financial situation for several years, after he sold the royalty rights to many of his earlier works, including "The Wonderful Wizard of Oz". This resulted in the M.A. Donahue Company publishing cheap editions of his early works with advertising that purported that Baum's newer output was inferior to the less expensive books they were releasing. Baum had shrewdly transferred most of his property, except for his clothing, his library (mostly of children's books, such as the fairy tales of Andrew Lang, whose portrait he kept in his study), and his typewriter (all of which he successfully argued were essential to his occupation), into Maud's name, as she handled the finances, anyway, and thus lost much less than he could have.
Baum made use of several pseudonyms for some of his other, non-Oz books. They include:
Baum also anonymously wrote "The Last Egyptian: A Romance of the Nile". He continued theatrical work with Harry Marston Haldeman's men's social group, The Uplifters, for which he wrote several plays for various celebrations. He also wrote the group's parodic by-laws. The group, which also included Will Rogers, was proud to have had Baum as a member and posthumously revived many of his works despite their ephemeral intent. Although many of these play's titles are known, only "The Uplift of Lucifer" is known to survive (it was published in a limited edition in the 1960s). Prior to that, his last produced play was "The Tik-Tok Man of Oz" (based on "Ozma of Oz" and the basis for "Tik-Tok of Oz"), a modest success in Hollywood that producer Oliver Morosco decided did not do well enough to take to Broadway. Morosco, incidentally, quickly turned to film production, as would Baum.
In 1914, having moved to Hollywood years earlier, Baum started his own film production company, The Oz Film Manufacturing Company, which came as an outgrowth of the Uplifters. He served as its president, and principal producer and screenwriter. The rest of the board consisted of Louis F. Gottschalk, Harry Marston Haldeman, and Clarence R. Rundel. The films were directed by J. Farrell MacDonald, with casts that included Violet MacMillan, Vivian Reed, Mildred Harris, Juanita Hansen, Pierre Couderc, Mai Welles, Louise Emmons, J. Charles Haydon, and early appearances by Harold Lloyd and Hal Roach. Silent film actor Richard Rosson appeared in one of the films, whose younger brother Harold Rosson photographed "The Wizard of Oz" (1939). After little success probing the unrealized children's film market, Baum came clean about who wrote "The Last Egyptian" and made a film of it (portions of which are included in "Decasia"), but the Oz name had, for the time being, become box office poison and even a name change to Dramatic Feature Films and transfer of ownership to Frank Joslyn Baum did not help. Unlike with "The Fairylogue and Radio-Plays", Baum invested none of his own money in the venture, but the stress probably took its toll on his health.
Death.
On May 5, 1919, Baum suffered a stroke. The following day he slipped into a coma but briefly awoke and spoke his last words to his wife, "Now we can cross the Shifting Sands." Frank died on May 6, 1919. He was buried in Glendale's Forest Lawn Memorial Park Cemetery.
His final Oz book, "Glinda of Oz", was published on July 10, 1920, a year after his death. The Oz series was continued long after his death by other authors, notably Ruth Plumly Thompson, who wrote an additional nineteen Oz books.
Baum's beliefs.
Literary.
Baum's avowed intentions with the Oz books, and other fairy tales, was to re-tell tales such as are found in the works of the Brothers Grimm and Hans Christian Andersen; make them in an American vein, update them, avoid stereotypical characters such as dwarfs or genies, and remove the association of violence and moral teachings. Although the first books contained a fair amount of violence, it decreased with the series; in "The Emerald City of Oz", Ozma objected to doing violence even to the Nomes who threaten Oz with invasion. His introduction is often cited as the beginnings of the sanitization of children's stories, although he did not do a great deal more than eliminate harsh moral lessons. His stories still include decapitations, eye removals, maimings of all kinds, and other violent acts, but the tone is very different from Grimm or Andersen.
Another traditional element that Baum intentionally omitted was the emphasis on romance. He considered romantic love to be uninteresting for young children, as well as largely incomprehensible. In "The Wonderful Wizard of Oz", the only element of romance lay in the backstory of the Tin Woodman and his love Nimmie Amee, which explains his condition and does not otherwise affect the tale, and that of Gayelette and the enchantment of the Winged Monkeys; the only other stories with such elements were "The Scarecrow of Oz" and "Tik-Tok of Oz", both based on dramatizations, which Baum regarded warily until his readers accepted them.
Political.
Women's suffrage advocate.
Sally Roesch Wagner of The Matilda Joslyn Gage Foundation has published a pamphlet titled "The Wonderful Mother of Oz" describing how Matilda Gage's radical feminist politics were sympathetically channeled by Baum into his Oz books. Much of the politics in the Republican "Aberdeen Saturday Pioneer" dealt with trying to convince the populace to vote for women's suffrage. Baum was the secretary of Aberdeen's Woman's Suffrage Club. When Susan B. Anthony visited Aberdeen, she stayed with the Baums. Nancy Tystad Koupal notes an apparent loss of interest in editorializing after Aberdeen failed to pass the bill for women's enfranchisement.
Some of Baum's contacts with suffragists of his day seem to have inspired much of his second Oz story, "The Marvelous Land of Oz". In this story, General Jinjur leads the girls and women of Oz, armed with knitting needles, in a revolt; they succeed, and make the men do the household chores. Jinjur proves to be an incompetent ruler, but a female advocating gender equality is ultimately placed on the throne. His Edith Van Dyne stories, including the "Aunt Jane's Nieces", "The Flying Girl" and its sequel, and his girl sleuth Josie O'Gorman from The Bluebird Books, depict girls and young women engaging in traditionally masculine activities.
Editorials about Native Americans.
During the period surrounding the 1890 Ghost Dance movement and Wounded Knee Massacre, Baum wrote two editorials about Native Americans for the "Aberdeen Saturday Pioneer" which have provoked controversy in recent times because of his assertion that the safety of White settlers depended on the wholesale genocide of American Indians. Sociologist Robert Venables has argued that Baum was not using sarcasm in the editorials.
The first piece was published on December 20, 1890, five days after the killing of the Lakota Sioux holy man, Sitting Bull (who was being held in custody at the time). Following is the complete text of the editorial:
Following the December 29, 1890, massacre, Baum wrote a second editorial, published on January 3, 1891:
These two short editorials continue to haunt his legacy. In 2006, two descendants of Baum apologized to the Sioux nation for any hurt their ancestor had caused.
The short story, "The Enchanted Buffalo", claims to be a legend of a tribe of bison, and states that a key element made it into legends of Native American tribes. "Father Goose, His Book" contains poems such as "There Was a Little Nigger Boy" and "Lee-Hi-Lung-Whan." In "The Last Egyptian", Lord Roane uses "nigger" to insult the title character, while in "The Daring Twins", set in the American South, the only character to use the term is a boy from Boston complaining that his mother uses their money to help "naked niggers in Africa." Baum mentions his characters' distaste for a Hopi snake dance in "Aunt Jane's Nieces and Uncle John", but also deplores the horrible situation of Indian Reservations. "Aunt Jane's Nieces on the Ranch" has a hard-working Mexican present himself as an exception to reiterate Anglo stereotypes of Mexican laziness. Baum's mother-in-law, Woman's Suffrage leader Matilda Joslyn Gage, had great influence over Baum's views. Gage was initiated into the Wolf Clan and admitted into the Iroquois Council of Matrons for her outspoken respect and sympathy for Native American people; it would seem unlikely that Baum could have harbored animosity for them in his mature years.
The interpretation of the Indian editorials has been explored in the context of satire and reverse psychology, highlighting their ironic inconsistencies. Analysis of Baum literature, both subsequent to and contemporary with the editorials, appears to reveal sympathy with the plight of the Indians, suggesting that in these editorials “…he was not advocating holocaust, he was deploring it, at the moment it was occurring and in the midst of it … (he) found himself surrounded not by bloodthirsty redskins, but rather by his subscribers, bloodthirsty frontier rednecks.”
Political imagery in "The Wizard of Oz".
Although numerous political references to the "Wizard" appeared early in the 20th century, the first full-fledged interpretation of the novel as an extended political allegory of the politics and characters of the 1890s appeared in a scholarly article by Henry Littlefield, an upstate New York high school history teacher. Special attention was paid to the Populist metaphors and debates over silver and gold. As a Republican and avid supporter of Women's Suffrage, it is thought that Baum personally did not support the political ideals of either the Populist movement of 1890–92 or the Bryanite-silver crusade of 1896–1900. He published a poem in support of William McKinley.
Since 1964 many scholars, economists and historians have expanded on Littlefield's interpretation, pointing to multiple similarities between the characters (especially as depicted in Denslow's illustrations) and stock figures from editorial cartoons of the period. Littlefield himself wrote to "The New York Times" letters to the editor section spelling out that his theory had no basis in fact, but that his original point was, "not to label Baum, or to lessen any of his magic, but rather, as a history teacher at Mount Vernon High School, to invest turn-of-the-century America with the imagery and wonder I have always found in his stories."
Baum's newspaper had addressed politics in the 1890s, and Denslow was an editorial cartoonist as well as an illustrator of children's books. A series of political references are included in the 1902 stage version, such as references by name to the President and a powerful senator, and to John D. Rockefeller for providing the oil needed by the Tin Woodman. Scholars have found few political references in Baum's Oz books after 1902.
When Baum himself was asked whether his stories had hidden meanings, he always replied that they were written to "please children".
Religion.
Originally a Methodist, Baum joined the Episcopal Church in Aberdeen to participate in community theatricals. Later, he and his wife, encouraged by Matilda Joslyn Gage, became members of the Theosophical Society in 1892. Baum's beliefs are often reflected in his writing. The only mention of a church in his Oz books is the porcelain one which the Cowardly Lion breaks in the Dainty China Country in "The Wonderful Wizard of Oz". The Baums believed in God, but felt that religious decisions should be made by mature minds and not religious authorities. As a result, they sent their older sons to "Ethical Culture Sunday School" in Chicago, which taught morality, not religion.
Bibliography.
 "This literature-related list is ; you can help by [ expanding it]".
Short stories.
This list omits those stories that appeared in "Our Landlady", "American Fairy Tales", "Animal Fairy Tales", "Little Wizard Stories of Oz", and "Queer Visitors from the Marvelous Land of Oz".
Miscellanea.
Baum has been credited as the editor of "In Other Lands Than Ours" (1907), a collection of letters written by his wife Maud Gage Baum.
Plays and adaptations.
Including those listed here and on the Oz books page, Michael Patrick Hearn has identified 42 titles of stage plays associated with Baum, some probably redundant or reflective of alternate drafts, many for works that Baum may never have actually started. Listed below are those either known to have been performed (such as the lost plays of his youth) or that exist in at least fragmentary or treatment form.
"The Wizard of Oz" on screen and back to stage.
Following early film treatments in 1910 and 1925 and Baum's own venture The Oz Film Manufacturing Company, Metro Goldwyn Mayer made the story into the now classic movie "The Wizard of Oz" (1939), starring Judy Garland as Dorothy Gale. It was only MGM's second feature-length film in three-strip Technicolor (the first having been "Sweethearts" (1938), based on the Victor Herbert operetta). Among other changes the film was given an all-a-dream ending. (Baum used this technique only once, in "Mr. Woodchuck", and in that case the title character explicitly told the dreamer numerous times that she was dreaming.)
A completely new Tony Award-winning Broadway musical with an African-American cast, "The Wiz", was staged in 1975 with Stephanie Mills as Dorothy. It was the basis for a 1978 film by the same title starring Diana Ross as an adult Dorothy and Michael Jackson as the Scarecrow.
"The Wizard of Oz" continues to inspire new versions, such as Disney's "Return to Oz" (1985), "The Muppets' Wizard of Oz", "Tin Man" (a re-imagining of the story televised in late 2007 on the Sci Fi Channel), and a variety of animated productions. Today's most successful Broadway show, "Wicked", provides a backstory to the two Oz witches used in the classic MGM film. Gregory Maguire, author of the novel, "Wicked", on which the musical is based, chose to honor L. Frank Baum by naming his main character Elphaba—a phonetic take on Baum's initials.
The film "Oz the Great and Powerful" (2013) serves as an homage to MGM's film, "The Wizard of Oz" (1939), and stars James Franco, Mila Kunis, Rachel Weisz, and Michelle Williams.

</doc>
<doc id="18189" url="http://en.wikipedia.org/wiki?curid=18189" title="Lake Ladoga">
Lake Ladoga

Lake Ladoga (Russian: Ла́дожское о́зеро, "Ladozhskoye ozero"; ] or Russian: Ла́дога, "Ladoga"; ]; Finnish: "Laatokka" [earlier in Finnish "Nevajärvi"]; Livvi-Karelian: "Luadogu"; Veps: "Ladog, Ladoganjärv") is a freshwater lake located in the Republic of Karelia and Leningrad Oblast in northwestern Russia just outside the outskirts of Saint Petersburg. It is the largest lake in Europe, and the 15th largest freshwater lake by area in the world.
Etymology.
In one of Nestor's chronicles from the 12th century he mentions a lake called "the Great Nevo" (a clear link to the Neva River; also compare Finnish: "neva" "bog, quagmire"). In ancient Norse sagas and Hanseatic treaties they mention a city made of lakes called Aldoga (compare Finnish: "aalto" - wave).
Since the beginning of the 14th century the name of the lake was commonly called Ladoga, which was named after the city of Ladoga which in turn was named after the eponymous inflow in the lower reaches of the Volkhov River (Finnish: "Alodejoki" - river in the low areas). Other theories about the origin of the name comes from Karelian: "Aalto" - wave, here Karelian: "Aaltokas" - wavy; from the Russian dialectal word "алодь", meaning "open lake, extensive water field".
According to Jackson, T.N., "So far it can be considered that almost for granted, that the name of Ladoga first referred to the river, then the city, and only then the lake." Therefore, it considers the primary hydronym Ladoga, from Finnish: "*Alode-jogi (joki)" "the lower river". From the river was the city Old Norse: "Aldeigja", and already it was borrowed Slavic population and transformed using metathesis "ald → lad" from Old East Slavic: Ладога. The Old Norse intermediary word between Finnish and Old Russian word is fully confirmed by archeology: the Scandinavians first appeared in Ladoga in the early 750's, that is, a couple of decades before the Slavs.
Eugene Helimski by contrast, offers an etymology rooted in German. In his opinion, the primary name of the lake - Old Norse: "*Aldauga" "old source", similar to the open sea (the Old Open-Sea-Like-Source). This hydronym is associated with the name of the Neva River (which flows from Lake Ladoga) which comes German - the "new". Through the intermediate form *Aldaugja word gave Old Norse: "Aldeigja" "Ladoga (city)".
Geography.
The lake has an average surface area of 17,891 km² (excluding the islands). Its north-to-south length is 219 km and its average width is 83 km; the average depth is 51 m, although it reaches a maximum of 230 m in the north-western part. Basin area: 276,000 km², volume: 837 km³ (earlier estimated as 908 km³);. There are around 660 islands, with a total area of about 435 km². Ladoga is, on average, 5 m above sea level. Most of the islands, including the famous Valaam archipelago, Kilpola and Konevets, are situated in the northwest of the lake.
Separated from the Baltic Sea by the Karelian Isthmus, it drains into the Gulf of Finland via the Neva River.
Lake Ladoga is navigable, being a part of the Volga-Baltic Waterway connecting the Baltic Sea with the Volga River. The Ladoga Canal bypasses the lake in the southern part, connecting the Neva to the Svir.
The basin of Lake Ladoga includes about 50,000 lakes and 3,500 rivers longer than 10 km. About 85% of the water inflow is due to tributaries, 13% is due to precipitation, and 2% is due to underground waters.
"Ladoga Lacus", a methane lake on Saturns Moon Titan is named after the lake.
Geological history.
Deglaciation following the Weichsel glaciation took place in the Lake Ladoga basin between 12,500 and 11,500 radiocarbon years BP. Lake Ladoga was initially part of the Baltic Ice Lake (70–80 m. above present sea-level), a historical freshwater stage of Baltic Sea. It is possible, though not certain, that Ladoga was isolated from it during regression of the subsequent Yoldia Sea brackish stage (10,200–9,500 BP). The isolation threshold should be at Heinjoki to the east of Vyborg, where the Baltic Sea and Ladoga were connected by a strait or a river outlet at least until the formation of the River Neva, and possibly even much later, until the 12th century AD or so.
At 9,500 BP, Lake Onega, previously draining into the White Sea, started emptying into Ladoga via the River Svir. Between 9,500 and 9,100 BP, during the transgression of Ancylus Lake, the next freshwater stage of the Baltic, Ladoga certainly became part of it, even if they hadn't been connected immediately before. During the Ancylus Lake subsequent regression, around 8,800 BP Ladoga became isolated.
Ladoga slowly transgressed in its southern part due to uplift of the Baltic Shield in the north. It has been hypothesized, but not proven, that waters of the Litorina Sea, the next brackish-water stage of the Baltic, occasionally invaded Ladoga between 7,000 and 5,000 BP. Around 5,000 BP the waters of the Saimaa Lake penetrated Salpausselkä and formed a new outlet, River Vuoksi, entering Lake Ladoga in the northwestern corner and raising its level by 1–2 m.
The River Neva originated when the Ladoga waters at last broke through the threshold at Porogi into the lower portions of Izhora River, then a tributary of the Gulf of Finland, between 4,000 and 2,000 BP. According to some new data, it happened at 3,100 radiocarbon years BP (3,410–3,250 calendar years BP).
Wildlife.
The Ladoga is rich with fish. 48 forms (species and infraspecific taxa) of fish have been encountered in the lake, including roach, carp bream, zander, European perch, ruffe, endemic variety of smelt, two varieties of "Coregonus albula" (vendace), eight varieties of "Coregonus lavaretus", a number of other "Salmonidae" as well as, albeit rarely, endangered European sea sturgeon. Commercial fishing was once a major industry but has been hurt by overfishing. After the war, between 1945–1954, the total annual catch increased and reached a maximum of 4,900 tonnes. However, unbalanced fishery led to the drastic decrease of catch in 1955–1963, sometimes to 1,600 tonnes per year. Trawling has been forbidden in Lake Ladoga since 1956 and some other restrictions were imposed. The situation gradually recovered, and in 1971–1990 the catch ranged between 4,900 and 6,900 tonnes per year, about the same level as the total catch in 1938. Fish farms and recreational fishing are developing. 
It has its own endemic Ringed Seal subspecies known as the Ladoga Seal.
Since the beginning of the 1960s Ladoga has become considerably eutrophicated.
Nizhnesvirsky Natural Reserve is situated along the shore of Lake Ladoga immediately to the north of the mouth of the River Svir.
History.
In the Middle Ages, the lake formed a vital part of the trade route from the Varangians to the Greeks, with the Norse emporium at Staraya Ladoga defending the mouth of the Volkhov since the 8th century. In the course of the Swedish–Novgorodian Wars, the area was disputed between the Novgorod Republic and Sweden. In the early 14th century, the fortresses of Korela (Kexholm) and Oreshek (Nöteborg) were established along the banks of the lake.
The ancient Valaam Monastery was founded on the island of Valaam, the largest in Lake Ladoga, abandoned between 1611–1715, magnificently restored in the 18th century, and evacuated to Finland during the Winter War in 1940. In 1989 the monastic activities in the Valaam were resumed. Other historic cloisters in the vicinity are the Konevets Monastery, which sits on the Konevets island, and the Alexander-Svirsky Monastery, which preserves fine samples of medieval Muscovite architecture.
During the Ingrian War, a fraction of the Ladoga coast was occupied by Sweden. In 1617, by the Treaty of Stolbovo, the northern and western coast was ceded by Russia to Sweden. In 1721, after the Great Northern War, it was restituted to Russia by the Treaty of Nystad. Later, in 1812–1940 the lake was shared between Finland and Russia. According to the conditions of the 1920 Tartu Peace Treaty militarization of the lake was severely restricted. However, both Soviet Russia and Finland had flotillas in Ladoga (see also Finnish Ladoga Naval Detachment). After the Winter War (1939–40) according to the Moscow Peace Treaty, Ladoga, previously shared with Finland, became an internal basin of the Soviet Union.
During the Continuation War (1941–44) not only Finnish and Soviet, but also German and Italian vessels operated there (see also Naval Detachment K and Regia Marina). Under these circumstances, during much of the Siege of Leningrad (1941–44), Lake Ladoga provided the only access to the besieged city because a section of the eastern shore remained in Soviet hands. Supplies were transported into Leningrad with trucks on winter roads over the ice, the "Road of Life", and by boat in the summer. After World War II, Finland lost the Karelia region to the USSR, and all Finnish citizens were evacuated from the ceded territory. Ladoga became an internal Soviet basin once again. The northern shore, Ladoga Karelia with the town of Sortavala, is now part of the Republic of Karelia. The western shore, Karelian Isthmus, became part of Leningrad Oblast.
Since 1996 the is an annual competition for 4x4 vehicles and ATVs that travels through over 1200 kilometers of the mud, swamp and bog that surround Lake Ladoga.

</doc>
<doc id="18190" url="http://en.wikipedia.org/wiki?curid=18190" title="Language family">
Language family

A language family is a group of languages related through descent from a common ancestor, called the proto-language of that family. The term 'family' reflects the tree model of language origination in historical linguistics, which makes use of a metaphor comparing languages to people in a biological family tree, or in a subsequent modification, to species in a phylogenetic tree of evolutionary taxonomy. No actual biological relationship between speakers is implied by the metaphor.
Estimates of the number of living languages vary from 5,000 to 8,000, depending on the precision of one's definition of "language", and in particular on how one classifies dialects. The 2013 edition of Ethnologue catalogs just over 7,000 living human languages. A "living language" is simply one that is used as the primary form of communication of a group of people. There are also many dead and extinct languages, as well as some that are still insufficiently studied to be classified, or even unknown outside their respective speech communities.
Membership of languages in a language family is established by comparative linguistics. Sister languages are said to have a "genetic" or "genealogical" relationship. The latter is older, but has been revived in recent years to better distinguish the relationships between languages from the genetic relationships between people.
The evidence of linguistic relationship is found in observable shared characteristics that are not attributed to contact or borrowing. 
Genealogically related languages present shared retentions, that is, features of the proto-language (or reflexes of such features) that cannot be explained by chance or borrowing (convergence). Membership in a branch or group within a language family is established by shared innovations, that is, common features of those languages that are not found in the common ancestor of the entire family. For example, Germanic languages are "Germanic" in that they share vocabulary and grammatical features that are not believed to have been present in the Proto-Indo-European language. These features are believed to be innovations that took place in Proto-Germanic, a descendant of Proto-Indo-European that was the source of all Germanic languages.
Structure of a family.
Language families can be divided into smaller phylogenetic units, conventionally referred to as "branches" of the family because the history of a language family is often represented as a tree diagram. A family is a monophyletic unit; that is, all its members derive from a common ancestor, and all attested descendants of that ancestor are included in the family. (In this way, the term "family" is analogous to the biological term "clade".) Some taxonomists restrict the term "family" to a certain level, but there is little consensus in how to do so. Those who affix such labels also subdivide branches into "groups", and groups into "complexes". A top-level (largest) family is often called a "phylum" or "stock". The term "macrofamily" or "superfamily" is sometimes applied to proposed groupings of language families whose status as phylogenetic units is generally considered to be unsubstantiated by accepted historical linguistic methods.
For example, the Celtic, Germanic, Slavic, Romance, and Indo-Iranian language families are branches of a larger Indo-European language family. There is a remarkably similar pattern shown by the linguistic tree and the genetic tree of human ancestry 
that was verified statistically. Languages interpreted in terms of the putative phylogenetic tree of human languages are transmitted to a great extent vertically (i.e. by ancestry) as opposed to horizontally (i.e. by spatial diffusion).
Dialect continua.
Some closely knit language families, and many branches within larger families, take the form of dialect continua, in which there are no clear-cut borders that make it possible to unequivocally identify, define, or count individual languages within the family. However, when the differences between the speech of different regions at the extremes of the continuum are so great that there is no mutual intelligibility between them, as occurs for Arabic, the continuum cannot meaningfully be seen as a single language. A speech variety may also be considered either a language or a dialect depending on social or political considerations. Thus different sources give sometimes wildly different accounts of the number of languages within a family. Classifications of the Japonic family, for example, range from one language (a language isolate) to nearly twenty.
Isolates.
Most of the world's languages are known to be related to others. Those that have no known relatives (or for which family relationships are only tentatively proposed) are called language isolates, essentially language families consisting of a single language. An example is Basque. In general, it is assumed that language isolates have relatives, or had relatives at some point in their history, but at a time depth too great for linguistic comparison to recover them.
A language isolated in its own branch within a family, such as Armenian within Indo-European, is often also called an isolate, but the meaning of "isolate" in such cases is usually clarified. For instance, Armenian may be referred to as an "Indo-European isolate". By contrast, so far as is known, the Basque language is an absolute isolate: It has not been shown to be related to any other language despite numerous attempts. A language may be said to be an isolate currently but not historically if related but now extinct relatives are attested. The Aquitanian language, spoken in Roman times, may have been an ancestor of Basque, but it could also have been a sister language to the ancestor of Basque. In the latter case, Basque and Aquitanian would form a small family together. (Ancestors are not considered to be distinct members of a family.)
Proto-languages.
The common ancestor of a language family is seldom known directly, since most languages have a relatively short recorded history. However, it is possible to recover many features of a proto-language by applying the comparative method—a reconstructive procedure worked out by 19th century linguist August Schleicher. This can demonstrate the validity of many of the proposed families in the list of language families. For example, the reconstructible common ancestor of the Indo-European language family is called "Proto-Indo-European". Proto-Indo-European is not attested by written records, and so it is conjectured to have been spoken before the invention of writing.
Sometimes, however, a proto-language "can" be identified with a historically known language. For instance, dialects of Old Norse are the proto-language of Norwegian, Swedish, Danish, Faroese and Icelandic. Likewise, the Appendix Probi depicts Proto-Romance, a language almost unattested due to the prestige of Classical Latin, a highly stylised literary register not representative of the speech of ordinary people.
Other classifications of languages.
Sprachbund.
Shared innovations, acquired by borrowing or other means, are not considered genetic and have no bearing with the language family concept. It has been asserted, for example, that many of the more striking features shared by Italic languages (Latin, Oscan, Umbrian, etc.) might well be "areal features". However, very similar-looking alterations in the systems of long vowels in the West Germanic languages greatly postdate any possible notion of a proto-language innovation (and cannot readily be regarded as "areal", either, since English and continental West Germanic were not a linguistic area). In a similar vein, there are many similar unique innovations in Germanic, Baltic and Slavic that are far more likely to be areal features than traceable to a common proto-language. But legitimate uncertainty about whether shared innovations are areal features, coincidence, or inheritance from a common ancestor, leads to disagreement over the proper subdivisions of any large language family.
A sprachbund is a geographic area having several languages that feature common linguistic structures. The similarities between those languages are caused by language contact, not by chance or common origin, and are not recognized as criteria that define a language family. An example of a sprachbund would be the Indian Subcontinent.
Contact languages.
The concept of language families is based on the historical observation that languages develop dialects, which over time may diverge into distinct languages. However, linguistic ancestry is less clear-cut than familiar biological ancestry, in which species do not crossbreed. It is more like the evolution of microbes, with extensive lateral gene transfer: Quite distantly related languages may affect each other through language contact, which in extreme cases may lead to languages with no single ancestor, whether they be creoles or mixed languages. In addition, a number of sign languages have developed in isolation and appear to have no relatives at all. Nonetheless, such cases are relatively rare and most well-attested languages can be unambiguously classified as belonging to one language family or another, even if this family's relation to other families is not known.
Further reading.
</dl>

</doc>
<doc id="18194" url="http://en.wikipedia.org/wiki?curid=18194" title="Looe Island">
Looe Island

Looe Island (Cornish: Enys Lann-Managh, meaning "island of the monk's enclosure"), also known as St George's Island, and historically St Michael's Island is a small island a mile from the mainland town of Looe in Cornwall, England.
According to local legend, Joseph of Arimathea landed here with the child Christ. Some scholars, including Glyn Lewis, suggest the island could be as Ictis, the location described by Diodorus Siculus as a centre for the tin trade in pre-Roman Britain.
The waters around the island are a marine nature reserve owned by Cornwall Wildlife Trust and form part of the Looe Voluntary Marine Conservation Area (VMCA). First established in 1995, the Looe VCMA covers nearly 5 km of coastline and aims to protect the coastal and marine wildlife around Looe.
History.
People have been living on Looe Island since the Iron Age. Evidence of early habitation includes pieces of Roman amphora as well as stone boat anchors and Roman coins. In the Dark Ages, the island was used a seat of early Christian settlement. The child Jesus was believed to have visited the Island with his uncle, Joseph of Arimathea, who traded with the Cornish tin traders. Therefore Looe Island became a place of pilgrimage for early Christians and a small thatched roofed chapel was built there during this time.
In the later Medieval period, the island came under the control of Glastonbury Abbey. Lammana Priory was a priory on the mainland directly aligned to a small chapel on the Island consisting of two Benedictine monks until 1289 when the property was sold to a local landowner. The priory was replaced by a chapel served by a secular priest until the Dissolution of the Monasteries in 1536 when it became property of the Crown. From the 13th to the 16th centuries the island was known as St Michael's Island. After 1584 it became known as St George's Island.
Through the 17th and 18th centuries the island was a popular haunt for smugglers avoiding the British government's revenue cutters out of Plymouth and Falmouth. The Old Guildhall Museum in Looe hold information and research about the smuggling families of Looe Island and information is also available the more recent publications about the island.
In the 20th century, Looe island was owned (and inhabited) by two sisters, Babs and Evelyn Atkins, who wrote two books: "We Bought An Island" (1976, ISBN 0-245-52940-3) and its sequel "Tales From Our Cornish Island" (1986, ISBN 0-245-54265-5). They chronicle the purchase of the island and what it was like to live there. Evelyn died in 1997 at the age of 87; Babs continued to live on the island until her death in 2004, at the age of 86. On her death, the island was bequeathed to the Cornwall Wildlife Trust; it will be preserved as a nature reserve in perpetuity. The adjoining islet, formerly known as Little Island, now renamed Trelawny Island and connected by a small bridge, was bequeathed by Babs back to the Trelawny family, who had owned Looe Island from 1743 to 1921. The full history of the island is explained at length in "Island Life: A History of Looe Island", published in 2006, and the role of the island today is briefly described in "Looe Island Then and Now" published in 2014.
Geography.
It is about 22.5 acres in area and a mile (1.6 km) in circumference. Its highest point is 47 m above sea level. The island, like much of south west England, has a mild climate with frost and snow being rare.
The island is owned and managed by a charity, the Cornwall Wildlife Trust. This is a non-profit making venture, the landing fees and other income being devoted to conserving the island's natural beauty and providing facilities. The island is open during the summer to day visitors arriving by the Trust's boat. After a short welcome talk visitors are directed to the small visitor centre from where they can pick up a copy of the self-guided trail. Visitors have some two hours on the Island and all trips are subject to tides and weather/sea state. While it is normally accessible only by the Cornwall Wildlife Trust's boat, at low spring tides it is possible for the journey to be made by foot across the rocky sea floor.
Media appearances.
In 2008, Channel 4's archaeology series "Time Team" visited the island to carry out an investigation into its early Christian history. They excavated the sites of Christian chapels built on both the island and on the mainland opposite. During their dig they found the remains of a Benedictine chapel that was built in c.1139 by monks from Glastonbury Abbey, a reliquary, graves and the remains of much earlier Romano-British chapels built of wood with dating evidence suggesting use by Christians before the reign of Constantine the Great.
In 1994/95 Andrew Hugill composed Island Symphony, an electro-acoustic piece utilising sampled sounds sourced over the net plus recorded natural sounds from the island itself.

</doc>
<doc id="18195" url="http://en.wikipedia.org/wiki?curid=18195" title="LaTeX">
LaTeX

LaTeX ( or shortening of Lamport TeX) is a word processor and document markup language. It is distinguished by typical word processors such as Microsoft Office and OpenOffice in that the writer uses plain text as opposed to formatted text, relying on markup tagging conventions to define the general structure of a document (such as article, book, and letter), to stylise text throughout a document (such as bold and italic), and to add citations and cross-referencing. A TeX distribution such as TeXlive or MikTeX is used to produce an output file (such as PDF or DVI) suitable for printing or digital distribution.
LaTeX is used for the communication and publication of scientific documents in many fields, including mathematics, physics, computer science, statistics, economics, and political science. It also has a prominent role in the preparation and publication of books and articles that contain complex multilingual materials, such as Sanskrit and Arabic. LaTeX uses the TeX typesetting program for formatting its output, and is itself written in the TeX macro language.
LaTeX is widely used in academia. LaTeX can be used as a standalone document preparation system, or as an intermediate format. In the latter role, for example, it is often used as part of a pipeline for translating DocBook and other XML-based formats to PDF. The typesetting system offers programmable desktop publishing features and extensive facilities for automating most aspects of typesetting and desktop publishing, including numbering and cross-referencing of tables and figures, chapter and section headings, the inclusion of graphics, page layout, indexing and bibliographies.
Like TeX, LaTeX started as a writing tool for mathematicians and computer scientists, but from early in its development it has also been taken up by scholars who needed to write documents that include complex math expressions or non-Latin scripts, such as Arabic, Sanskrit and Chinese.
LaTeX is intended to provide a high-level language that accesses the power of TeX. LaTeX comprises a collection of TeX macros and a program to process LaTeX documents. Because the plain TeX formatting commands are elementary, it provides authors with ready-made commands for formatting and layout requirements such as chapter headings, footnotes, cross-references and bibliographies.
LaTeX was originally written in the early 1980s by Leslie Lamport at SRI International. The current version is LaTeX2e (styled as LaTeX2ε). LaTeX is a free software and is distributed under the LaTeX Project Public License (LPPL).
Typesetting system.
LaTeX follows the design philosophy of separating presentation from content, so that authors can focus on the content of what they are writing without attending simultaneously to its visual appearance. In preparing a LaTeX document, the author specifies the logical structure using simple, familiar concepts such as "chapter", "section", "table", "figure", etc., and lets the LaTeX system worry about the formatting and layout of these structures. It therefore encourages the separation of layout from content while still allowing manual typesetting adjustments where needed. This concept is similar to the mechanism by which many word processors allow styles to be defined globally for an entire document or the use of Cascading Style Sheets to style HTML. The LaTeX system is a markup language that also handles typesetting and rendering.
LaTeX can be arbitrarily extended by using the underlying macro language to develop custom formats. Such macros are often collected into "packages," which are available to address special formatting issues such as complicated mathematical content or graphics. Indeed, in the example below, the codice_1 environment is provided by the codice_2 package.
In order to create a document in LaTeX, you first write a file, say foobar.tex, using your preferred text editor. Then you give your foobar.tex file as input to the TeX program (with the LaTeX macros loaded), and TeX writes out a file suitable for viewing onscreen or printing. This write-format-preview cycle is one of the chief ways in which working with LaTeX differs from what-you-see-is-what-you-get word-processing. It is similar to the code-compile-execute cycle familiar to computer programmers. Today, many LaTeX-aware editing programs make this cycle a simple matter of pressing a single key, while showing the output preview on the screen beside the input window. Some online Latex editors automatically refresh the preview.
Examples.
The example below shows the LaTeX input and corresponding output:
</syntaxhighlight>
Note how the equation for formula_1 was typeset by the markup:
The square root is denoted by "\sqrt{"argument"}" and fractions by "\frac{"numerator"} {"denominator"}".
Pronouncing and writing "LaTeX".
LaTeX is usually pronounced or in English (that is, not with the /ks/ pronunciation English speakers normally associate with "X", but with a /k/).
The characters T, E, X in the name come from capital Greek letters tau, epsilon, and chi, as the name of TeX derives from the Greek: τέχνη (skill, art, technique); for this reason, TeX's creator Donald Knuth promotes a pronunciation of () (that is, with a voiceless velar fricative as in Modern Greek, similar to the ch in loch). Lamport, on the other hand, has said he does not favor or discourage any pronunciation for LaTeX.
The name is traditionally printed in running text with a special typographical logo: LaTeX.
In media where the logo cannot be precisely reproduced in running text, the word is typically given the unique capitalization "LaTeX". The TeX, LaTeX and XeTeX logos can be rendered via pure CSS and XHTML for use in graphical web browsers following the specifications of the internal \LaTeX macro.
Licensing.
LaTeX is typically distributed along with plain TeX. It is distributed under a free software license, the LaTeX Project Public License (LPPL). The LPPL is not compatible with the GNU General Public License, as it requires that modified files must be clearly differentiable from their originals (usually by changing the filename); this was done to ensure that files that depend on other files will produce the expected behavior and avoid dependency hell. The LPPL is DFSG compliant as of version 1.3. As free software, LaTeX is available on most operating systems including UNIX (Solaris, HP-UX, AIX), BSD (FreeBSD, Mac OS X, NetBSD, OpenBSD), Linux (Red Hat, Debian GNU/Linux, Arch, Gentoo), Microsoft Windows (9x, XP, Vista, 7, 8), DOS, RISC OS, AmigaOS and Plan9.
Related software.
As a macro package, LaTeX provides a set of macros for TeX to interpret. There are many other macro packages for TeX, including Plain TeX, GNU Texinfo, AMSTeX, and ConTeXt.
When TeX "compiles" a document, it follows (from the user's point of view) the following processing sequence: Macros → TeX → Driver → Output. Different implementations of each of these steps are typically available in TeX distributions. Traditional TeX will output a DVI file, which is usually converted to a PostScript file. More recently, Hàn Thế Thành and others have written a new implementation of TeX called pdfTeX, which also outputs to PDF and takes advantage of features available in that format. The XeTeX engine developed by Jonathan Kew merges modern font technologies and Unicode with TeX.
The default font for LaTeX is Knuth's Computer Modern, which gives default documents created with LaTeX the same distinctive look as those created with plain TeX. XeTeX allows the use of OpenType and TrueType (that is, outlined) fonts for output files.
There are also many editors for LaTeX.
Versions.
LaTeX2e is the current version of LaTeX, since it replaced LaTeX 2.09 in 1994. s of 2014[ [update]], a future version called LaTeX3, started in the early 1990s, is still in development. Planned features include improved syntax, hyperlink support, a new user interface, access to arbitrary fonts, and new documentation.
There are numerous commercial implementations of the entire TeX system. System vendors may add extra features like additional typefaces and telephone support. LyX is a free, WYSIWYM visual document processor that uses LaTeX for a back-end. TeXmacs is a free, WYSIWYG editor with similar functionalities as LaTeX but a different typesetting engine. Other WYSIWYG editors that produce LaTeX include Scientific Word on MS Windows.
A number of community-supported TeX distributions are available, including TeX Live (multiplatform), teTeX (deprecated in favor of TeX Live, UNIX), fpTeX (deprecated), MiKTeX (Windows), proTeXt (Windows), MacTeX (TeX Live with the addition of Mac specific programs), gwTeX (Mac OS X), OzTeX (Mac OS Classic), AmigaTeX (no longer available) and PasTeX (AmigaOS, available on the Aminet repository).
Compatibility.
LaTeX documents (*.tex) can be opened with any text editor. They consist of plain text and do not contain hidden formatting codes or binary instructions. Additionally, TeX documents can be shared by rendering the LaTeX file to Rich Text Format (.rtf) or XML. This can be done using the free software programs LaTeX2RTF or TeX4ht. LaTeX can also be rendered to PDF files using the LaTeX extension pdfLaTeX. LaTeX files containing Unicode text can be processed into PDFs by the LaTeX extension XeLaTeX.
Further reading.
</dl>

</doc>
<doc id="18196" url="http://en.wikipedia.org/wiki?curid=18196" title="List of saints">
List of saints

This is an incomplete list of Christian saints in alphabetical order by Christian name, but if necessary by surname, the place, or attribute part of the name as well.
There are 810 canonized Roman Catholic saints, although some incorrectly give numbers in the thousands. Among the Eastern Orthodox and Oriental Orthodox Communions, the numbers may be even higher, since there is no fixed process of "canonization" and each individual jurisdiction within the two Orthodox communions independently maintains parallel lists of saints that have only partial overlap. Note that 78 popes are considered saints.
The Anglican Communion have only ever canonized one saint—King Charles I of England (see Society of King Charles the Martyr). However, it recognizes pre-Reformation saints, as does the United Methodist Church. Persons who have led lives of celebrated sanctity or missionary zeal are included in the Calendar of the Prayer Book "without thereby enrolling or commending such persons as saints of the Church". Similarly, any individuals commemorated in the Lutheran calendar of saints will be listed as well.
Wikipedia contains calendars of saints for particular denominations, listed by the day of the year on which they are traditionally venerated, as well as a chronological list of saints and blesseds, listed by their date of death.
Notes.
4 "Common Worship" has "Commemoration".
6 Eastern Rite Catholic Churches only. 
7 Russian Orthodox Church only.

</doc>
<doc id="18198" url="http://en.wikipedia.org/wiki?curid=18198" title="Lebesgue measure">
Lebesgue measure

In measure theory, the Lebesgue measure, named after French mathematician Henri Lebesgue, is the standard way of assigning a measure to subsets of "n"-dimensional Euclidean space. For "n" = 1, 2, or 3, it coincides with the standard measure of length, area, or volume. In general, it is also called n"-dimensional volume, n"-volume, or simply volume. It is used throughout real analysis, in particular to define Lebesgue integration. Sets that can be assigned a Lebesgue measure are called Lebesgue measurable; the measure of the Lebesgue measurable set "A" is denoted by λ("A").
Henri Lebesgue described this measure in the year 1901, followed the next year by his description of the Lebesgue integral. Both were published as part of his dissertation in 1902.
The Lebesgue measure is often denoted "dx", but this should not be confused with the distinct notion of a volume form.
Definition.
Given a subset formula_1, with the length of an (open, closed, semi-open) interval formula_2 given by formula_3, the Lebesgue outer measure formula_4 is defined as
The Lebesgue measure of E is given by its Lebesgue outer measure formula_6 if, for every formula_7,
Intuition.
The first part of the definition states that the subset formula_9 of the real numbers is reduced to its outer measure by coverage by sets of intervals. Each of these sets of intervals formula_10 covers formula_9 in the sense that when the intervals are combined together by union, they form a superset of formula_9. Moreover, the intervals in each set are disjoint, and there is a countable infinity of these intervals. For each set, the total length is calculated by adding the lengths of this infinity of disjoint intervals. This total length of any interval set can easily overestimate the measure of formula_9, because formula_9 is a subset of the union of the intervals, and so the intervals may include points which are not in formula_9. The Lebesgue outer measure emerges as the greatest lower bound (infimum) of the lengths from among all possible such sets. Intuitively, it is the total length of those interval sets which fit formula_9 most tightly.
That characterizes the Lebesgue outer measure. Whether this outer measure translates to the Lebesgue measure proper depends on an additional condition. This condition is tested by taking subsets of the real numbers formula_17 using formula_9 as an instrument to split formula_17 into two partitions: the part of formula_17 which intersects with formula_9 and the remaining part of formula_17 which is not in formula_9: the set difference of formula_17 and formula_9. These partitions of formula_17 are subject to the outer measure. If for all possible such subsets formula_17 of the real numbers, the partitions of formula_17 cut apart by formula_9 have outer measures which add up to the outer measure of formula_17, then the outer Lebesgue measure of formula_9 gives its Lebesgue measure. Intuitively, this condition means that the set formula_9 must not have some curious properties which causes a discrepancy in the measure of another set when formula_9 is used as a "mask" to "clip" that set, hinting at the existence of sets for which the Lebesgue outer measure does not give the Lebesgue measure. (Such sets are, in fact, not Lebesgue-measurable.)
Properties.
The Lebesgue measure on R"n" has the following properties:
All the above may be succinctly summarized as follows:
The Lebesgue measure also has the property of being σ-finite.
Null sets.
A subset of R"n" is a "null set" if, for every ε > 0, it can be covered with countably many products of "n" intervals whose total volume is at most ε. All countable sets are null sets.
If a subset of R"n" has Hausdorff dimension less than "n" then it is a null set with respect to "n"-dimensional Lebesgue measure. Here Hausdorff dimension is relative to the Euclidean metric on R"n" (or any metric Lipschitz equivalent to it). On the other hand a set may have topological dimension less than "n" and have positive "n"-dimensional Lebesgue measure. An example of this is the Smith–Volterra–Cantor set which has topological dimension 0 yet has positive 1-dimensional Lebesgue measure.
In order to show that a given set "A" is Lebesgue measurable, one usually tries to find a "nicer" set "B" which differs from "A" only by a null set (in the sense that the symmetric difference ("A" − "B") formula_43("B" − "A") is a null set) and then show that "B" can be generated using countable unions and intersections from open or closed sets.
Construction of the Lebesgue measure.
The modern construction of the Lebesgue measure is an application of Carathéodory's extension theorem. It proceeds as follows.
Fix "n" ∈ N. A box in R"n" is a set of the form
where "bi" ≥ "ai", and the product symbol here represents a Cartesian product. The volume of this box is defined to be
For "any" subset "A" of R"n", we can define its outer measure "λ"*("A") by:
We then define the set "A" to be Lebesgue measurable if for every subset "S" of R"n",
These Lebesgue measurable sets form a σ-algebra, and the Lebesgue measure is defined by for any Lebesgue measurable set "A".
The existence of sets that are not Lebesgue measurable is a consequence of a certain set-theoretical axiom, the axiom of choice, which is independent from many of the conventional systems of axioms for set theory. The Vitali theorem, which follows from the axiom, states that there exist subsets of R that are not Lebesgue measurable. Assuming the axiom of choice, non-measurable sets with many surprising properties have been demonstrated, such as those of the Banach–Tarski paradox.
In 1970, Robert M. Solovay showed that the existence of sets that are not Lebesgue measurable is not provable within the framework of Zermelo–Fraenkel set theory in the absence of the axiom of choice (see Solovay's model).
Relation to other measures.
The Borel measure agrees with the Lebesgue measure on those sets for which it is defined; however, there are many more Lebesgue-measurable sets than there are Borel measurable sets. The Borel measure is translation-invariant, but not complete.
The Haar measure can be defined on any locally compact group and is a generalization of the Lebesgue measure (R"n" with addition is a locally compact group).
The Hausdorff measure is a generalization of the Lebesgue measure that is useful for measuring the subsets of R"n" of lower dimensions than "n", like submanifolds, for example, surfaces or curves in R³ and fractal sets. The Hausdorff measure is not to be confused with the notion of Hausdorff dimension.
It can be shown that there is no infinite-dimensional analogue of Lebesgue measure.

</doc>
<doc id="18201" url="http://en.wikipedia.org/wiki?curid=18201" title="Lake Champlain">
Lake Champlain

Lake Champlain (French: "Lac Champlain") is a natural freshwater lake in North America, located mainly within the borders of the United States (states of Vermont and New York) but partially situated across the Canada-United States border in the Canadian province of Quebec.
The New York portion of the Champlain Valley includes the eastern portions of Clinton County and Essex County. Most of this area is part of the Adirondack Park. There are recreational opportunities in the park and along the relatively undeveloped coastline of Lake Champlain. The cities of Plattsburgh, New York and Burlington, Vermont are to the north of the lake, and the village of Ticonderoga, New York is located in the southern part of the region. The Quebec portion is located in the regional county municipalities of Le Haut-Richelieu and Brome-Missisquoi.
Geology.
The Champlain Valley is the northernmost unit of a landform system known as the Great Appalachian Valley, which stretches from Quebec to Alabama. The Champlain Valley is a physiographic section of the larger Saint Lawrence Valley, which in turn is part of the larger Appalachian physiographic division.
It is one of numerous large lakes located in an arc from Labrador through the northern United States and into the Northwest Territories of Canada. Although it is smaller than each of the Great Lakes: Ontario, Erie, Huron, Superior, or Michigan, Lake Champlain is a large body of fresh water. Approximately 1269 km2 in area, the lake is roughly 201 km long, and 23 km across at its widest point. The maximum depth is approximately 400 ft. The lake varies seasonally from about 95 to above mean sea level.
Hydrology.
Lake Champlain is situated in the Lake Champlain Valley between the Green Mountains of Vermont and the Adirondack Mountains of New York, drained northward by the 106 mi-long Richelieu River into the St. Lawrence River at Sorel-Tracy, Quebec northeast and downstream of Montreal. It also receives the waters from the 32 mi-long Lake George, so its basin collects waters from the northwestern slopes of the Green Mountains of Vermont and the northernmost eastern peaks of the Adirondack Mountains of New York.
The lake drains nearly half of Vermont. About 250,000 people get their drinking water from the lake.
The lake is fed by Otter Creek, the Winooski, Poultney, Missisquoi, and Lamoille Rivers in Vermont, and the Ausable, Chazy, Boquet, Saranac and La Chute rivers in New York.
It is connected to the Hudson River by the Champlain Canal.
Portions of the lake freeze each winter, and in some winters the entire lake surface freezes, referred to as "closing". The lake temperature reaches an average of 70 °F in July and August.
Chazy Reef.
The Chazy Reef is an extensive Ordovician carbonate rock formation which extends from Tennessee to Quebec and Newfoundland. It occurs in prominent outcropping at Goodsell Ridge, Isle La Motte, the northernmost island in Lake Champlain.
The oldest reefs are around "The Head" of the south end of the island; slightly younger reefs are found at the Fisk Quarry; and the youngest (the famous coral reefs) are located in fields to the north. Together, these three sites provide a unique narrative of events which took place over 450 million years ago in the ocean in the Southern Hemisphere, long before the emergence of Lake Champlain 20 thousand years ago.
History.
The lake was named after the French explorer Samuel de Champlain, who encountered it in 1609. While the ports of Burlington, Vermont; Port Henry, New York; and Plattsburgh, New York today are primarily used by small craft, ferries and lake cruise ships, they were of substantial commercial and military importance in the 18th and 19th centuries.
A variety of Native American names for the lake were recorded by historians. Many historical works give "Caniaderi Guarunte" as the Iroquois name for the lake (meaning: mouth or door of the country); the lake was an important northern gateway to their lands. A number of other sources give "Petonbowk" (meaning the lake in between) as the Abenaki name in their Algonquian language for the lake. The St. Francis/Sokoki Abenaki Band, who make their home along the "Masipskiwibi" River (in Missisquoi language, "Crooked River") in northwestern Vermont, call the lake "Bitawbagok", which has the same meaning as "Petonbowk". Some early 21st-century articles appeared during the Champlain Quadricentennial (2009) claiming "Ondakina" as the “local” native name for the lake, but none cites a verifiable source.
Colonial America and the Revolutionary War.
New France allocated concessions all along lake Champlain to French settlers, and built forts to defend the waterways.
In colonial times, Lake Champlain was used as a water passage (or, in winter, ice) between the Saint Lawrence and the Hudson valleys. Travelers found it easier to journey by boats and sledges on the lake rather than to go overland on the unpaved and frequently mud-bound roads of the time. The northern tip of the lake at Saint-Jean-sur-Richelieu, Quebec (known as St. John in colonial times under British rule) is a short distance from Montreal. The southern tip at Whitehall (Skenesborough in revolutionary times) is a short distance from Saratoga, Glens Falls, and Albany, New York.
Forts were built at Ticonderoga and Crown Point (Fort St. Frederic) to control passage on the lake in colonial times. Important battles were fought at Ticonderoga in 1758 and 1775. During the Revolutionary War, the British and Americans conducted a frenetic shipbuilding race through the Spring and Summer of 1776 at opposite ends of the lake, fighting a significant naval engagement on October 11 at the Battle of Valcour Island. While it was a tactical defeat for the Americans and the small fleet led by Benedict Arnold was almost entirely destroyed, the Americans gained a strategic victory. The British invasion was delayed long enough so that the approach of winter prevented the fall of these forts until the following year. In this period, the Continental Army gained strength and was victorious at Saratoga.
War of 1812.
During the War of 1812, British and American forces faced each other in the Battle of Lake Champlain, also known as the Battle of Plattsburgh, fought on September 11, 1814. This ended the final British invasion of the northern states during the War of 1812. It was fought just prior to the signing of the Treaty of Ghent, and the American victory denied the British any leverage to demand exclusive control over the Great Lakes or territorial gains against the New England states.
Three US Naval ships have been named after this battle, including the USS "Lake Champlain" (CV-39), the USS "Lake Champlain" (CG-57), and a cargo ship used during World War I.
Following the War of 1812, the US Army began construction on "Fort Blunder", an unnamed fortification built at the northernmost end of Lake Champlain to protect against attacks from British Canada. Its nickname came from a surveying error: the initial phase of construction on the fort turned out to be taking place on a point .75 mi north of the Canadian border. Once this error was spotted, construction was abandoned. Locals scavenged materials used in the abandoned fort for use in their own homes and public buildings. 
By the Webster-Ashburton Treaty of 1842, the US-Canadian border was adjusted northward to include the strategically important site of "Fort Blunder" on the US side. In 1844, work was begun to replace the remains of the 1812-era fort with a massive new Third System masonry fortification known as Fort Montgomery. Portions of this fort are still standing.
Modern history.
In the early 19th century, the construction of the Champlain Canal connected Lake Champlain to the Hudson River system, allowing north-south commerce by water from New York City to Montreal and Atlantic Canada.
In 1909, 65,000 people celebrated the 300th anniversary of the French discovery of the lake. Attending dignitaries included President William Howard Taft, along with representatives from France, Canada and the United Kingdom.
In 1929, then-New York Governor Franklin Roosevelt and Vermont Governor John Weeks, dedicated the first bridge to span the lake, built from Crown Point to Chimney Point. This bridge lasted until December 2009. Severe deterioration was found, and the bridge was demolished and replaced with the Lake Champlain Bridge, which opened in November 2011.
On February 19, 1932, boats were able to sail on Lake Champlain. It was the first time that the lake was known to be free of ice during the winter at that time.
Lake Champlain briefly became the nation's sixth Great Lake on March 6, 1998, when President Clinton signed Senate Bill 927. This bill, which reauthorized the National Sea Grant Program, contained a line declaring Lake Champlain to be a Great Lake. This status enabled its neighboring states to apply for additional federal research and education funds allocated to these national resources. Following a small uproar, the Great Lake status was rescinded on March 24 (although New York and Vermont universities continue to receive funds to monitor and study the lake).
"Champ", Lake Champlain monster.
In 1609 Samuel de Champlain wrote that he saw a lake monster five feet (1.5 m) long, as thick as a man's thigh, with silver-gray scales a dagger could not penetrate. The alleged monster had 2.5 foot jaws with sharp and dangerous teeth. Native Americans claimed to have seen similar monsters 8 to. This mysterious creature is likely the original Lake Champlain monster.:20 The monster has been memorialized in sports teams names and mascots: the Vermont Lake Monsters and mascot (Champ) of the state's minor league baseball team. A Vermont Historical Society publication recounts the story and offers possible explanations for accounts of the so-called monster: "floating logs, schools of large sturgeons diving in a row, or flocks of black birds flying close to the water."
Ecology.
A pollution prevention, control, and restoration plan for Lake Champlain was first endorsed in October 1996 by the governors of New York and Vermont, and the regional administrators of the EPA (United States Environmental Protection Agency). In April 2003, the plan was updated and Quebec signed onto it. The plan is being implemented by the Lake Champlain Basin Program and its partners at the state, provincial, federal and local level. It is renowned as a model for interstate and international cooperation. Its primary goals are to reduce phosphorus inputs to Lake Champlain; reduce toxic contamination; minimize the risks to humans from water-related health hazards; and control the introduction, spread, and impact of non-native nuisance species to preserve the integrity of the Lake Champlain ecosystem.
Agricultural and urban runoff from the watershed or drainage basin is the primary source of excess phosphorus, which exacerbates algae blooms in Lake Champlain. The most problematic blooms have been cyanobacteria, commonly called blue-green algae, in the northeastern part of the Lake, primarily Missisquoi Bay.
To reduce phosphorus runoff to this part of the lake, Vermont and Quebec agreed to reduce their inputs by 60% and 40%, respectively, by an agreement signed in 2002. While agricultural sources (manure and fertilizers) are the primary sources of phosphorus (about 70%) in the Missisquoi basin, runoff from developed land and suburbs is estimated to contribute about 46% of the phosphorus runoff basin-wide to Lake Champlain, and agricultural lands contributed about 38%.
In 2002, the cleanup plan noted that the lake had the capacity to absorb 110 MT of phosphorus each year. In 2009, a judge noted that 218 MT were still flowing in annually, more than twice what the lake could handle. Sixty municipal and industrial sewage plants discharge processed waste from the Vermont side.
In 2008, the EPA expressed concerns to the State of Vermont that the Lake's cleanup was not progressing fast enough to meet the original cleanup goal of 2016. The State, however, cites its Clean and Clear Action Plan as a model that will produce positive results for Lake Champlain.
In 2007, Vermont banned phosphates for dishwasher use starting in 2010. This will prevent an estimated 2 - from flowing into the lake. While this represents 0.6% of the phosphate pollution, it took $1.9 million to remove the pollutant rom treated wastewater, an EPA requirement.
Despite concerns about pollution, Lake Champlain is safe for swimming, fishing, and boating. It is considered a world-class fishery for salmonid species (Lake trout and Atlantic salmon) and bass. About 81 fish species live in the Lake, and more than 300 bird species rely on it for habitat and as a resource during migrations.
By 2008 at least six institutions monitoring lake water health: 
1) In 2002 the Conservation Law Foundation appointed a "lakekeeper," who reviews the state's pollution controls, 
2) Friends of Missisquoi Bay was formed in 2003, 
3) the Lake Champlain Committee, 
4) Vermont Water Resources Board hired a water quality expert in 2008 to write water quality standards and create wetland protection rules, 
5) In 2007 the Vermont Agency of Natural Resources appoints a "Lake czar" to oversee pollution control. Clean and Clear, an agency of the Vermont state government established in 2004; and 
6) the Nature Conservancy, a non-profit group, focuses on biodiversity and ecosystem health.
In 2001, scientists estimated that farming contributed 38% of the phosphorus runoff. By 2010, results of environmentally conscious farming practices, enforced by law, had made any positive contribution to lake cleanliness. A federally funded study was started to analyze this problem and to arrive at a solution.
Biologists have been trying to control lampreys in the lake since 1985 or earlier. Lampreys are native to the area, but have expanded in population to such an extent that they wounded nearly all Lake trout in 2006 and 70-80% of salmon. The use of pesticides against the lamprey has reduced their casualties of other fish to 35% of salmon and 31% of lake trout. The goal was 15% of salmon and 25% of lake trout.
The federal and state governments originally budgeted $18 million for lake programs for 2010. This was later supplemented by an additional $6.5 million from the federal government.
Railroad.
Historically four significant railroad crossings were built over the lake. As of 2011, only one remains.
Now called Colchester Park, the main three-mile (5 km) causeway has been adapted and preserved as a recreation area for cyclists, runners, and anglers. Two smaller marble rock-landfill causeways were also erected as part of this line that connected Grand Isle to North Hero, Vermont and from North Hero to Alburgh.:257
Natural history.
In 2010, the estimate of cormorant population, now classified as a nuisance species because they take so much of the lake fish, ranged from 14,000 to 16,000. A Fish and Wildlife commissioner said that the ideal population would be 3,300 or about 3 per 1 km2. Cormorants had disappeared from the lake (and all northern lakes) due to the use of DDT in the 1940s and 1950s, which made their eggs more fragile and reduced breeding populations.
Ring-billed gulls are also considered a nuisance. Measures have been taken to reduce their population. Authorities are trying to encourage the return of black crowned night herons, cattle egrets, and great blue herons, which disappeared during the time DDT was being widely used.
Infrastructure.
Lake crossings.
The Alburgh Peninsula (also known as the Alburgh Tongue), extending south from the Quebec shore of the lake into Vermont, is accessible from the rest of the state only via Canada. This is a distinction shared with Point Roberts, Washington, and the Northwest Angle in Minnesota as well as Province Point (see below). Unlike the other three cases, this is no longer of practical significance because highway bridges across the lake provide access to the peninsula from within the United States (from three directions).
Province Point, Vermont.
A few kilometres to the northeast of the town of East Alburgh, Vermont, Province Point is the southernmost tip of a small promontory approximately acres ( ha) in size . The promontory is cut through by the US-Canadian border; as such the area is a practical exclave of the United States contiguous with Canada.
Mainland.
Two roadways cross over the lake, connecting Vermont and New York.
Since November 2011, the Lake Champlain Bridge has crossed the southern part of the lake, connecting Chimney Point in Vermont with Crown Point, New York. It replaced Champlain Bridge, which was closed in 2009 because of severe structural problems found that could have resulted in a collapse. In 2009, the bridge had been used by 3,400 drivers per day, and driving around the southern end of the lake added two hours to the trip. Ferry service was re-established to take some of the traffic burden. On December 28, 2009, the bridge was destroyed in a controlled demolition. A new bridge was rapidly constructed by a joint state commitment, opening on November 7, 2011. 
To the north, US 2 runs from Rouses Point, New York to Grand Isle County, Vermont in the town of Alburgh, before continuing south along a chain of islands towards Burlington. To the east, Vermont Route 78 runs from an intersection with US 2 in Alburgh through East Alburgh to Swanton. The US 2-VT 78 route technically runs from the New York mainland to an extension of the mainland between two arms of the lake and then to the Vermont mainland, but it provides a direct route across the two main arms of the northern part of the lake.
Ferry.
North of Ticonderoga, New York, the lake widens appreciably; ferry service is operated by the Lake Champlain Transportation Company at:
While the old bridge was being demolished and the new one constructed, Lake Champlain Transportation
Company operated a free, 24-hour ferry from just south of the bridge to Chimney
Point in Vermont at the expense of the states of New York and Vermont at a cost to the states of about $10 per car.
The most southerly crossing is the Fort Ticonderoga Ferry, connecting Ticonderoga, New York with Shoreham, Vermont just north of the historic fort.
Railroad.
The Swanton, VT, to East Alburg, Vermont, rail trestle.
Waterways.
Lake Champlain has been connected to the Erie Canal via the Champlain Canal since the canal's official opening September 9, 1823, the same day as the opening of the Erie Canal from Rochester on Lake Ontario to Albany. It connects to the St. Lawrence River via the Richelieu River, with the Chambly Canal bypassing rapids on the river since 1843. Together with these waterways the lake is part of the Lakes to Locks Passage. The Lake Champlain Seaway, a project to use the lake to bring ocean-going ships from New York City to Montreal, was proposed in the late 19th century and considered as late as the 1960s, but rejected for various reasons.
Surroundings.
Major cities.
Burlington, Vermont (pop. 42,217, 2010 Census) is the largest city on the lake. The 2nd and 3rd most populated cities/towns are (Plattsburgh, New York, and Colchester, Vermont, respectively) combined. The fourth-largest community is the city of South Burlington.
Islands.
Lake Champlain contains roughly 80 islands, three of which comprise four entire Vermont towns (most of Grand Isle County). The largest islands:
Aids to navigation.
All active navigational aids on the American portion of the lake are maintained by USCG Burlington station, along with those on international Lake Memphremagog to the east.
Aids to navigation on the Canadian portion of the lake are maintained by the Canadian Coast Guard.
Parks.
There are a number of parks in the Lake Champlain region of both Vermont and New York.
Two on the New York side of the lake include Point Au Roche State Park, which have hiking and cross country skiing trails. A public beach is located on park grounds, and the Ausable Point State Park. The Cumberland Bay State Park is located on Cumberland Head, with a campground, city beach, and sports fields.
There are various parks along the lake on the Vermont side, including Sand Bar State Park in Milton, featuring a 2000 ft natural sand beach, swimming,canoe and kayak rentals, food concession, picnic grounds and a play area. At 226 acre, Grand Isle State Park contains camping facilities, a sand volleyball court, a nature walk trail, a horseshoe pit and a play area. Burlington's Waterfront Park is a revitalized industrial area.
Public safety.
Coast Guard Station Burlington provides "Search and Rescue, Law Enforcement and Ice Rescue services 24 hours a day, 365 days a year." Services are also provided by local, and state, and federal governments bordering on the lake, including the US Border Patrol, Royal Canadian Mounted Police, Vermont State Police, New York State Police Marine Detail, and Vermont Fish and Wildlife wardens.

</doc>
<doc id="18203" url="http://en.wikipedia.org/wiki?curid=18203" title="Lambda calculus">
Lambda calculus

Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. First formulated by Alonzo Church to formalize the concept of effective computability, lambda calculus found early successes in the area of computability theory, such as a negative answer to Hilbert's Entscheidungsproblem. Lambda calculus is a conceptually simple universal model of computation (Turing showed in 1937 that Turing machines equated the lambda calculus in expressiveness). The name derives from the Greek letter lambda (λ) used to denote binding a variable in a function. The letter itself is arbitrary and has no special meaning. Lambda calculus is taught and used in computer science because of its usefulness in showcasing functional thinking and iterative reduction.
Because of the importance of the notion of variable binding and substitution, there is not just one system of lambda calculus, and in particular there are "typed" and "untyped" variants. Historically, the most important system was the untyped lambda calculus, in which function application has no restrictions (so the notion of the domain of a function is not built into the system). In the Church–Turing Thesis, the untyped lambda calculus is claimed to be capable of computing all effectively calculable functions. The typed lambda calculus is a variety that restricts function application, so that functions can only be applied if they are capable of accepting the given input's "type" of data.
Today, the lambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. It is still used in the area of computability theory, although Turing machines are also an important model for computation. Lambda calculus has played an important role in the development of the theory of programming languages. Counterparts to lambda calculus in computer science are functional programming languages, which essentially implement the lambda calculus (augmented with some constants and datatypes). Beyond programming languages, the lambda calculus also has many applications in proof theory. A major example of this is the Curry–Howard correspondence, which gives a correspondence between different systems of typed lambda calculus and systems of formal logic.
Lambda calculus in history of mathematics.
The lambda calculus was introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. The original system was shown to be logically inconsistent in 1935 when Stephen Kleene and J. B. Rosser developed the Kleene–Rosser paradox.
Subsequently, in 1936 Church isolated and published just the portion relevant to computation, what is now called the untyped lambda calculus. In 1940, he also introduced a computationally weaker, but logically consistent system, known as the simply typed lambda calculus.
Informal description.
Motivation.
Computable functions are a fundamental concept within computer science and mathematics. The λ-calculus provides a simple semantics for computation, enabling properties of computation to be studied formally. The λ-calculus incorporates two simplifications that make this semantics simple.
The first simplification is that the λ-calculus treats functions "anonymously", without giving them explicit names. For example, the function 
can be rewritten in "anonymous form" as 
(read as "the pair of formula_3 and formula_4 is mapped to formula_5"). Similarly, 
can be rewritten in anonymous form as formula_7, where the input is simply mapped to itself.
The second simplification is that the λ-calculus only uses functions of a single input. An ordinary function that requires two inputs, for instance the formula_8 function, can be reworked into an equivalent function that accepts a single input, and as output returns "another" function, that in turn accepts a single input. For example, 
can be reworked into 
This method, known as currying, transforms a function that takes multiple arguments into a chain of functions each with a single argument.
Function application of the formula_8 function to the arguments (5, 2), yields at once
whereas evaluation of the curried version requires one more step
to arrive at the same result.
The lambda calculus.
The lambda calculus consists of a language of lambda terms, which is defined by a certain formal syntax, and a set of transformation rules, which allow manipulation of the lambda terms. These transformation rules can be viewed as an equational theory or as an operational definition.
As described above, all functions in the lambda calculus are anonymous functions, having no names. They only accept one input variable, with currying used to implement functions with several variables.
Lambda terms.
The syntax of the lambda calculus defines some expressions as valid lambda calculus expression and some as invalid, just as some strings of characters are valid C programs and some are not. A valid lambda calculus expression is called a "lambda term".
The following three rules give an inductive definition that can be applied to build all syntactically valid lambda terms:
Nothing else is a lambda term. Thus a lambda term is valid if and only if it can be obtained by repeated application of these three rules. However, some parentheses can be omitted according to certain rules. For example, the outermost parentheses are usually not written. "See" #Notation, below.
A lambda abstraction formula_26 is a definition of an anonymous function that is capable of taking a single input formula_3 and substituting it into the expression formula_20. 
It thus defines an anonymous function that takes x and returns t. For example formula_29 is a lambda abstraction for the function formula_30 using the term formula_31 for formula_20. The definition of a function with a lambda abstraction merely "sets up" the function but does not invoke it. The abstraction binds the variable formula_3 in the term formula_20.
An application formula_35 represents the application of a function formula_20 to an input formula_24, that is, it represents the act of calling function formula_20 on input formula_24 to produce formula_40.
There is no concept in lambda calculus of variable declaration. In a definition such as formula_41 (i.e. formula_42), the lambda calculus treats formula_4 as a variable that is not yet defined. The lambda abstraction formula_41 is syntactically valid, and represents a function that adds its input to the yet-unknown formula_4.
Bracketing may be used and may be needed to disambiguate terms. For example, formula_46 and formula_47 denote different terms (although coincidentally reduce to the same value.)
Functions that operate on functions.
In lambda calculus, functions are taken to be 'first class values', so functions may be used as the inputs, or be returned as outputs from other functions.
For example, formula_48 represents the identity function, formula_49, and formula_50 represents the identity function applied to formula_4. Further, formula_52 represents the constant function formula_53, the function that always returns formula_4, no matter the input. In lambda calculus, function application is regarded as left-associative, so that formula_55 means formula_56.
There are several notions of "equivalence" and "reduction" that allow lambda terms to be "reduced" to "equivalent" lambda terms.
Alpha equivalence.
A basic form of equivalence, definable on lambda terms, is alpha equivalence. It captures the intuition that the particular choice of a bound variable, in a lambda abstraction, does not (usually) matter.
For instance, formula_48 and formula_58 are alpha-equivalent lambda terms, and they both represent the same function (the identity function). 
The terms formula_3 and formula_4 are not alpha-equivalent, because they are not bound in a lambda abstraction.
In many presentations, it is usual to identify alpha-equivalent lambda terms.
The following definitions are necessary in order to be able to define beta reduction.
Free variables.
The free variables of a term are those variables not bound by a lambda abstraction. The set of free variables of an expression is defined inductively:
For example, the lambda term representing the identity formula_48 has no free variables, but the function formula_41 has a single free variable, formula_4.
Capture-avoiding substitutions.
Suppose formula_20, formula_24 and formula_74 are lambda terms and formula_3 and formula_4 are variables.
The notation formula_77 indicates substitution of formula_74 for formula_3 in formula_20 in a "capture-avoiding" manner. This is defined so that:
For example, formula_92, and formula_93.
The freshness condition (requiring that formula_4 is not in the free variables of formula_74) is crucial in order to ensure that substitution does not change the meaning of functions.
For example, a substitution is made that ignores the freshness condition: formula_96. This substitution turns the constant function formula_97 into the identity formula_48 by substitution.
In general, failure to meet the freshness condition can be remedied by alpha-renaming with a suitable fresh variable.
For example, switching back to our correct notion of substitution, in formula_99 the lambda abstraction can be renamed with a fresh variable formula_100, to obtain formula_101, and the meaning of the function is preserved by substitution.
Beta reduction.
The beta reduction rule states that an application of the form formula_102 reduces to the term formula_103. The notation formula_104 is used to indicate that formula_105 beta reduces to formula_106.
For example, for every formula_24, formula_108. This demonstrates that formula_109 really is the identity.
Similarly, formula_110, which demonstrates that formula_111 is a constant function.
The lambda calculus may be seen as an idealised functional programming language, like Haskell or Standard ML.
Under this view, beta reduction corresponds to a computational step. This step can be repeated by additional beta conversions until there are no more applications left to reduce. In the untyped lambda calculus, as presented here, this reduction process may not terminate.
For instance, consider the term formula_112.
Here formula_113.
That is, the term reduces to itself in a single beta reduction, and therefore the reduction process will never terminate.
Another aspect of the untyped lambda calculus is that it does not distinguish between different kinds of data.
For instance, it may be desirable to write a function that only operates on numbers. However, in the untyped lambda calculus, there is no way to prevent a function from being applied to truth values, strings, or other non-number objects.
Formal definition.
Definition.
Lambda expressions are composed of
The set of lambda expressions, Λ, can be defined inductively:
Instances of rule 2 are known as abstractions and instances of rule 3 are known as applications.
Notation.
To keep the notation of lambda expressions uncluttered, the following conventions are usually applied.
Free and bound variables.
The abstraction operator, λ, is said to bind its variable wherever it occurs in the body of the abstraction. Variables that fall within the scope of an abstraction are said to be "bound". All other variables are called "free". For example, in the following expression y is a bound variable and x is free: λ"y"."x" "x" "y". Also note that a variable is bound by its "nearest" abstraction. In the following example the single occurrence of x in the expression is bound by the second lambda: λ"x"."y" (λ"x"."z" "x")
The set of "free variables" of a lambda expression, M, is denoted as FV(M) and is defined by recursion on the structure of the terms, as follows:
An expression that contains no free variables is said to be "closed". Closed lambda expressions are also known as combinators and are equivalent to terms in combinatory logic.
Reduction.
The meaning of lambda expressions is defined by how expressions can be reduced.
There are three kinds of reduction:
We also speak of the resulting equivalences: two expressions are "β-equivalent", if they can be β-converted into the same expression, and α/η-equivalence are defined similarly.
The term "redex", short for "reducible expression", refers to subterms that can be reduced by one of the reduction rules. For example, (λ"x".M) N is a beta-redex in expressing the substitution of N for x in M; if "x" is not free in M, λ"x".M "x" is an eta-redex. The expression to which a redex reduces is called its reduct; using the previous example, the reducts of these expressions are respectively M["x":=N] and M.
α-conversion.
Alpha-conversion, sometimes known as alpha-renaming, allows bound variable names to be changed. For example, alpha-conversion of λ"x"."x" might yield λ"y"."y". Terms that differ only by alpha-conversion are called "α-equivalent". Frequently, in uses of lambda calculus, α-equivalent terms are considered to be equivalent.
The precise rules for alpha-conversion are not completely trivial. First, when alpha-converting an abstraction, the only variable occurrences that are renamed are those that are bound to the same abstraction. For example, an alpha-conversion of λ"x".λ"x"."x" could result in λ"y".λ"x"."x", but it could "not" result in λ"y".λ"x"."y". The latter has a different meaning from the original.
Second, alpha-conversion is not possible if it would result in a variable getting captured by a different abstraction. For example, if we replace "x" with "y" in λ"x".λ"y"."x", we get λ"y".λ"y"."y", which is not at all the same.
In programming languages with static scope, alpha-conversion can be used to make name resolution simpler by ensuring that no variable name masks a name in a containing scope (see alpha renaming to make name resolution trivial).
In the De Bruijn index notation, any two alpha-equivalent terms are literally identical.
Substitution.
Substitution, written "E"["V" := "R"], is the process of replacing all free occurrences of the variable "V" in the expression "E" with expression "R".
Substitution on terms of the λ-calculus is defined by recursion on the structure of terms, as follows (note: x and y are only variables while M and N are any λ expression).
To substitute into a lambda abstraction, it is sometimes necessary to α-convert the expression. For example, it is not correct for (λ"x"."y")["y" := "x"] to result in (λ"x"."x"), because the substituted "x" was supposed to be free but ended up being bound. The correct substitution in this case is (λ"z"."x"), up to α-equivalence. Notice that substitution is defined uniquely up to α-equivalence.
β-reduction.
Beta-reduction captures the idea of function application. Beta-reduction is defined in terms of substitution: the beta-reduction of  ((λ"V"."E") "E′")  is "E"["V" := "E′"].
For example, assuming some encoding of 2, 7, ×, we have the following β-reduction: ((λ"n"."n"×2) 7) → 7×2.
η-conversion.
Eta-conversion expresses the idea of extensionality, which in this context is that two functions are the same if and only if they give the same result for all arguments. Eta-conversion converts between λ"x".("f" "x") and "f" whenever "x" does not appear free in "f".
Normal forms and confluence.
For the untyped lambda calculus, β-reduction as a rewriting rule is neither strongly normalising nor weakly normalising.
However, it can be shown that β-reduction is confluent. (Of course, we are working up to α-conversion, i.e. we consider two normal forms to be equal, if it is possible to α-convert one into the other.)
Therefore, both strongly normalising terms and weakly normalising terms have a unique normal form. For strongly normalising terms, any reduction strategy is guaranteed to yield the normal form, whereas for weakly normalising terms, some reduction strategies may fail to find it.
Encoding datatypes.
The basic lambda calculus may be used to model booleans, arithmetic, data structures and recursion, as illustrated in the following sub-sections.
Arithmetic in lambda calculus.
There are several possible ways to define the natural numbers in lambda calculus, but by far the most common are the Church numerals, which can be defined as follows:
and so on. Or using the alternative syntax presented above in "Notation":
A Church numeral is a higher-order function—it takes a single-argument function "f", and returns another single-argument function. The Church numeral "n" is a function that takes a function "f" as argument and returns the "n"-th composition of "f", i.e. the function "f" composed with itself "n" times. This is denoted "f"("n") and is in fact the "n"-th power of "f" (considered as an operator); "f"(0) is defined to be the identity function. Such repeated compositions (of a single function "f") obey the laws of exponents, which is why these numerals can be used for arithmetic. (In Church's original lambda calculus, the formal parameter of a lambda expression was required to occur at least once in the function body, which made the above definition of 0 impossible.)
We can define a successor function, which takes a number "n" and returns "n" + 1 by adding another application of "f",where '(mf)x' means the function 'f' is applied 'm' times on 'x':
Because the "m"-th composition of "f" composed with the "n"-th composition of "f" gives the "m"+"n"-th composition of "f", addition can be defined as follows:
PLUS can be thought of as a function taking two natural numbers as arguments and returning a natural number; it can be verified that
and
are β-equivalent lambda expressions. Since adding "m" to a number "n" can be accomplished by adding 1 "m" times, an equivalent definition is:
Similarly, multiplication can be defined as
Alternatively
since multiplying "m" and "n" is the same as repeating the add "n" function "m" times and then applying it to zero.
Exponentiation has a rather simple rendering in Church numerals, namely
The predecessor function defined by PRED "n" = "n" − 1 for a positive integer "n" and PRED 0 = 0 is considerably more difficult. The formula
can be validated by showing inductively that if "T" denotes (λ"g".λ"h"."h" ("g" "f")), then T("n")(λ"u"."x") = (λ"h"."h"("f"("n"−1)("x"))) for "n" > 0. Two other definitions of PRED are given below, one using conditionals and the other using pairs. With the predecessor function, subtraction is straightforward. Defining
SUB "m" "n" yields "m" − "n" when "m" > "n" and 0 otherwise.
Logic and predicates.
By convention, the following two definitions (known as Church booleans) are used for the boolean values TRUE and FALSE:
Then, with these two λ-terms, we can define some logic operators (these are just possible formulations; other expressions are equally correct):
We are now able to compute some logic functions, for example:
and we see that AND TRUE FALSE is equivalent to FALSE.
A "predicate" is a function that returns a boolean value. The most fundamental predicate is ISZERO, which returns TRUE if its argument is the Church numeral 0, and FALSE if its argument is any other Church numeral:
The following predicate tests whether the first argument is less-than-or-equal-to the second:
and since "m" = "n", if LEQ "m" "n" and LEQ "n" "m", it is straightforward to build a predicate for numerical equality.
The availability of predicates and the above definition of TRUE and FALSE make it convenient to write "if-then-else" expressions in lambda calculus. For example, the predecessor function can be defined as:
which can be verified by showing inductively that "n" (λ"g".λ"k".ISZERO ("g" 1) "k" (PLUS ("g" "k") 1)) (λ"v".0) is the add "n" − 1 function for "n" > 0.
Pairs.
A pair (2-tuple) can be defined in terms of TRUE and FALSE, by using the Church encoding for pairs. For example, PAIR encapsulates the pair ("x","y"), FIRST returns the first element of the pair, and SECOND returns the second.
A linked list can be defined as either NIL for the empty list, or the PAIR of an element and a smaller list. The predicate NULL tests for the value NIL. (Alternatively, with NIL := FALSE, the construct "l" (λ"h".λ"t".λ"z".deal_with_head_"h"_and_tail_"t") (deal_with_nil) obviates the need for an explicit NULL test).
As an example of the use of pairs, the shift-and-increment function that maps ("m", "n") to ("n", "n" + 1) can be defined as
which allows us to give perhaps the most transparent version of the predecessor function:
Recursion and fixed points.
Recursion is the definition of a function using the function itself; on the face of it, lambda calculus does not allow this (we can't refer to a value which is yet to be defined, inside the lambda term defining that same value, as all functions are anonymous in lambda calculus). However, this impression is misleading: in  (λ"x"."x" "x") "y"  both "x"‍ ‍'s refer to the same lambda term, "y", so it is possible for a lambda expression – here "y" – to be arranged to receive itself as its argument value, through self-application.
Consider for instance the factorial function F("n") recursively defined by
In the lambda expression which is to represent this function, a "parameter" (typically the first one) will be assumed to receive the lambda expression itself as its value, so that calling it – applying it to an argument – will amount to recursion. Thus to achieve recursion, the intended-as-self-referencing argument (called "r" here) must always be passed to itself within the function body, at a call point:
The self-application achieves replication here, passing the function's lambda expression on to the next invocation as an argument value, making it available to be referenced and called there.
This solves it but requires re-writing each recursive call as self-application. We would like to have a generic solution, without a need for any re-writes:
Given a lambda term with first argument representing recursive call (e.g. G here), the "fixed-point" combinator FIX will return a self-replicating lambda expression representing the recursive function (here, F). The function does not need to be explicitly passed to itself at any point, for the self-replication is arranged in advance, when it is created, to be done each time it is called. Thus the original lambda expression (FIX G) is re-created inside itself, at call-point, achieving self-reference.
In fact, there are many possible definitions for this FIX operator, the simplest of them being:
In the lambda calculus, Y "g"  is a fixed-point of "g", as it expands to:
Now, to perform our recursive call to the factorial function, we would simply call (Y G) "n",  where "n" is the number we are calculating the factorial of. Given "n" = 4, for example, this gives:
Every recursively defined function can be seen as a fixed point of some suitably defined function closing over the recursive call with an extra argument, and therefore, using Y, every recursively defined function can be expressed as a lambda expression. In particular, we can now cleanly define the subtraction, multiplication and comparison predicate of natural numbers recursively.
Standard terms.
Certain terms have commonly accepted names:
Typed lambda calculus.
A typed lambda calculus is a typed formalism that uses the lambda-symbol (formula_114) to denote anonymous function abstraction. In this context, types are usually objects of a syntactic nature that are assigned to lambda terms; the exact nature of a type depends on the calculus considered (see kinds below). From a certain point of view, typed lambda calculi can be seen as refinements of the untyped lambda calculus but from another point of view, they can also be considered the more fundamental theory and "untyped lambda calculus" a special case with only one type.
Typed lambda calculi are foundational programming languages and are the base of typed functional programming languages such as ML and Haskell and, more indirectly, typed imperative programming languages. Typed lambda calculi play an important role in the design of type systems for programming languages; here typability usually captures desirable properties of the program, e.g. the program will not cause a memory access violation.
Typed lambda calculi are closely related to mathematical logic and proof theory via the Curry–Howard isomorphism and they can be considered as the internal language of classes of categories, e.g. the simply typed lambda calculus is the language of Cartesian closed categories (CCCs).
Computable functions and lambda calculus.
A function "F": N → N of natural numbers is a computable function if and only if there exists a lambda expression "f" such that for every pair of "x", "y" in N, "F"("x")="y" if and only if "f" "x" =β "y",  where "x" and "y" are the Church numerals corresponding to "x" and "y", respectively and =β meaning equivalence with beta reduction. This is one of the many ways to define computability; see the Church-Turing thesis for a discussion of other approaches and their equivalence.
Undecidability of equivalence.
There is no algorithm that takes as input two lambda expressions and outputs TRUE or FALSE depending on whether or not the two expressions are equivalent. This was historically the first problem for which undecidability could be proven. As is common for a proof of undecidability, the proof shows that no computable function can decide the equivalence. Church's thesis is then invoked to show that no algorithm can do so.
Church's proof first reduces the problem to determining whether a given lambda expression has a "normal form". A normal form is an equivalent expression that cannot be reduced any further under the rules imposed by the form. Then he assumes that this predicate is computable, and can hence be expressed in lambda calculus. Building on earlier work by Kleene and constructing a Gödel numbering for lambda expressions, he constructs a lambda expression "e" that closely follows the proof of Gödel's first incompleteness theorem. If "e" is applied to its own Gödel number, a contradiction results.
Lambda calculus and programming languages.
As pointed out by Peter Landin's 1965 paper , sequential procedural programming languages can be understood in terms of the lambda calculus, which provides the basic mechanisms for procedural abstraction and procedure (subprogram) application.
Lambda calculus reifies "functions" and makes them first-class objects, which raises implementation complexity when it is implemented.
Anonymous functions.
For example in Lisp the 'square' function can be expressed as a lambda expression as follows:
The above example is an expression that evaluates to a first-class function. The symbol codice_1 creates an anonymous function, given a list of parameter names, codice_2 — just a single argument in this case, and an expression that is evaluated as the body of the function, codice_3. The Haskell example is identical. Anonymous functions are sometimes called lambda expressions.
For example Pascal and many other imperative languages have long supported passing subprograms as arguments to other subprograms through the mechanism of function pointers. However, function pointers are not a sufficient condition for functions to be first class datatypes, because a function is a first class datatype if and only if new instances of the function can be created at run-time. And this run-time creation of functions is supported in Smalltalk, Javascript, and more recently in Scala, Eiffel ("agents"), C# ("delegates") and C++11, among others.
Reduction strategies.
Whether a term is normalising or not, and how much work needs to be done in normalising it if it is, depends to a large extent on the reduction strategy used. The distinction between reduction strategies relates to the distinction in functional programming languages between eager evaluation and lazy evaluation.
Applicative order is not a normalising strategy. The usual counterexample is as follows: define Ω = ωω where ω = λ"x"."xx". This entire expression contains only one redex, namely the whole expression; its reduct is again Ω. Since this is the only available reduction, Ω has no normal form (under any evaluation strategy). Using applicative order, the expression KIΩ = (λ"x".λ"y"."x") (λ"x"."x")Ω is reduced by first reducing Ω to normal form (since it is the rightmost redex), but since Ω has no normal form, applicative order fails to find a normal form for KIΩ.
In contrast, normal order is so called because it always finds a normalising reduction, if one exists. In the above example, KIΩ reduces under normal order to "I", a normal form. A drawback is that redexes in the arguments may be copied, resulting in duplicated computation (for example, (λ"x"."xx") ((λ"x"."x")"y") reduces to ((λ"x"."x")"y") ((λ"x"."x")"y") using this strategy; now there are two redexes, so full evaluation needs two more steps, but if the argument had been reduced first, there would now be none).
The positive tradeoff of using applicative order is that it does not cause unnecessary computation, if all arguments are used, because it never substitutes arguments containing redexes and hence never needs to copy them (which would duplicate work). In the above example, in applicative order (λ"x"."xx") ((λ"x"."x")"y") reduces first to (λ"x"."xx")"y" and then to the normal order "yy", taking two steps instead of three.
Most "purely" functional programming languages (notably Miranda and its descendents, including Haskell), and the proof languages of theorem provers, use "lazy evaluation", which is essentially the same as call by need. This is like normal order reduction, but call by need manages to avoid the duplication of work inherent in normal order reduction using "sharing". In the example given above, (λ"x"."xx") ((λ"x"."x")"y") reduces to ((λ"x"."x")"y") ((λ"x"."x")"y"), which has two redexes, but in call by need they are represented using the same object rather than copied, so when one is reduced the other is too.
A note about complexity.
While the idea of beta reduction seems simple enough, it is not an atomic step, in that it must have a non-trivial cost when estimating computational complexity. To be precise, one must somehow find the location of all of the occurrences of the bound variable "V" in the expression "E", implying a time cost, or one must keep track of these locations in some way, implying a space cost. A naïve search for the locations of "V" in "E" is "O"("n") in the length "n" of "E". This has led to the study of systems that use explicit substitution. Sinot's director strings offer a way of tracking the locations of free variables in expressions.
Parallelism and concurrency.
The Church–Rosser property of the lambda calculus means that evaluation (β-reduction) can be carried out in "any order", even in parallel. This means that various nondeterministic evaluation strategies are relevant. However, the lambda calculus does not offer any explicit constructs for parallelism. One can add constructs such as Futures to the lambda calculus. Other process calculi have been developed for describing communication and concurrency.
Semantics.
The fact that lambda calculus terms act as functions on other lambda calculus terms, and even on themselves, led to questions about the semantics of the lambda calculus. Could a sensible meaning be assigned to lambda calculus terms? The natural semantics was to find a set "D" isomorphic to the function space "D" → "D", of functions on itself. However, no nontrivial such "D" can exist, by cardinality constraints because the set of all functions from "D" to "D" has greater cardinality than "D", unless "D" is a singleton set.
In the 1970s, Dana Scott showed that, if only continuous functions were considered, a set or domain "D" with the required property could be found, thus providing a model for the lambda calculus.
This work also formed the basis for the denotational semantics of programming languages.
Further reading.
Monographs/textbooks for graduate students:
"Some parts of this article are based on material from FOLDOC, used with ."

</doc>
<doc id="18208" url="http://en.wikipedia.org/wiki?curid=18208" title="Lossy compression">
Lossy compression

In information technology, "lossy" compression is the class of data encoding methods that uses inexact approximations (or partial data discarding) for representing the content that has been encoded. Such compression techniques are used to reduce the amount of data that would otherwise be needed to store, handle, and/or transmit the represented content. The different versions of the photo of the cat at the right demonstrate how the approximation of an image becomes progressively coarser as more details of the data that made up the original image are removed. The amount of data reduction possible using lossy compression can often be much more substantial than what is possible with lossless data compression techniques. 
Using well-designed lossy compression technology, a substantial amount of data reduction is often possible before the result is sufficiently degraded to be noticed by the user. Even when the degree of degradation becomes noticeable, further data reduction may often be desirable for some applications (e.g., to make real-time communication possible through a limited bit-rate channel, to reduce the time needed to transmit the content, or to reduce the necessary storage capacity).
Lossy compression is most commonly used to compress multimedia data (audio, video, and still images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file that can then be used to produce compressed files for different purposes; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.
Lossy and lossless compression.
It is possible to compress many types of digital data in a way that reduces the size of a computer file needed to store it, or the bandwidth needed to transmit it, with no loss of the full information contained in the original file. A picture, for example, is converted to a digital file by considering it to be an array of dots and specifying the color and brightness of each dot. If the picture contains an area of the same color, it can be compressed without loss by saying "200 red dots" instead of "red dot, red dot, ...(197 more times)..., red dot."
The original data contains a certain amount of information, and there is a lower limit to the size of file that can carry all the information. Basic information theory says that there is an absolute limit in reducing the size of this data. When data is compressed, its entropy increases, and it cannot increase indefinitely. As an intuitive example, most people know that a compressed ZIP file is smaller than the original file, but repeatedly compressing the same file will not reduce the size to nothing. Most compression algorithms can recognize when further compression would be pointless and would in fact increase the size of the data.
In many cases, files or data streams contain more information than is needed for a particular purpose. For example, a picture may have more detail than the eye can distinguish when reproduced at the largest size intended; likewise, an audio file does not need a lot of fine detail during a very loud passage. Developing lossy compression techniques as closely matched to human perception as possible is a complex task. Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid trade-off for the reduced data.
Transform coding.
More generally, some forms of lossy compression can be thought of as an application of "transform coding" – in the case of multimedia data, "perceptual coding:" it transforms the raw data to a domain that more accurately reflects the information content. For example, rather than expressing a sound file as the amplitude levels over time, one may express it as the frequency spectrum over time, which corresponds more accurately to human audio perception.
While data reduction (compression, be it lossy or lossless) is a main goal of transform coding, it also allows other goals: one may represent data more accurately for the original amount of space – for example, in principle, if one starts with an analog or high-resolution digital master, an MP3 file of a given size should provide a better representation than a raw uncompressed audio in WAV or AIFF file of the same size. This is because uncompressed audio can only reduce file size by lowering bit rate or depth, whereas compressing audio can reduce size while maintaining bit rate and depth. This compression becomes a selective loss of the least significant data, rather than losing data across the board. Further, a transform coding may provide a better domain for manipulating or otherwise editing the data – for example, equalization of audio is most naturally expressed in the frequency domain (boost the bass, for instance) rather than in the raw time domain.
From this point of view, perceptual encoding is not essentially about "discarding" data, but rather about a "better representation" of data.
Another use is for backward compatibility and graceful degradation: in color television, encoding color via a luminance-chrominance transform domain (such as YUV) means that black-and-white sets display the luminance, while ignoring the color information.
Another example is chroma subsampling: the use of color spaces such as YIQ, used in NTSC, allow one to reduce the resolution on the components to accord with human perception – humans have highest resolution for black-and-white (luma), lower resolution for mid-spectrum colors like yellow and green, and lowest for red and blues – thus NTSC displays approximately 350 pixels of luma per scanline, 150 pixels of yellow vs. green, and 50 pixels of blue vs. red, which are proportional to human sensitivity to each component.
Information loss.
Lossy compression formats suffer from generation loss: repeatedly compressing and decompressing the file will cause it to progressively lose quality.
This is in contrast with lossless data compression, where data will not be lost via the use of such a procedure. 
Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.
There are two basic lossy compression schemes:
In some systems the two techniques are combined, with transform codecs being used to compress the error signals generated by the predictive stage.
Lossy versus lossless.
The advantage of lossy methods over lossless methods is that in some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application. 
Lossy methods are most often used for compressing sound, images or videos. This is because these types of data are intended for human interpretation where the mind can easily "fill in the blanks" or see past very minor errors or inconsistencies – ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test.
Transparency.
When a user acquires a lossily compressed file, (for example, to reduce download time) the retrieved file can be quite different from the original at the bit level while being indistinguishable to the human ear or eye for most practical purposes. Many compression methods focus on the idiosyncrasies of human physiology, taking into account, for instance, that the human eye can see only certain wavelengths of light. The psychoacoustic model describes how sound can be highly compressed without degrading perceived quality. Flaws caused by lossy compression that are noticeable to the human eye or ear are known as compression artifacts.
Compression ratio.
The compression ratio (that is, the size of the compressed file compared to that of the uncompressed file) of lossy video codecs is nearly always far superior to that of the audio and still-image equivalents.
Transcoding and editing.
An important caveat about lossy compression (formally transcoding), is that editing lossily compressed files causes digital generation loss from the re-encoding. This can be avoided by only producing lossy files from (lossless) originals and only editing (copies of) original files, such as images in raw image format instead of JPEG.
If data which has been compressed lossily is decoded and compressed losslessly, the size of the result can be comparable with the size of the data before lossy compression, but the data already lost cannot be recovered.
When deciding to use lossy conversion without keeping the original, one should remember that format conversion may be needed in the future to achieve compatibility with software or devices (format shifting), or to avoid paying patent royalties for decoding or distribution of compressed files.
Editing of lossy files.
By modifying the compressed data directly without decoding and re-encoding, some editing of lossily compressed files without degradation of quality is possible. Editing which reduces the file size as if it had been compressed to a greater degree, but without more loss than this, is sometimes also possible.
JPEG.
The primary programs for lossless editing of JPEGs are codice_1, and the derived codice_2 (which also preserves Exif information), and (which provides a Windows interface).
These allow the image to be
While unwanted information is destroyed, the quality of the remaining portion is unchanged.
Some other transforms are possible to some extent, such as joining images with the same encoding (composing side by side, as on a grid) or pasting images (such as logos) onto existing images (both via ), or scaling.
Some changes can be made to the compression without re-encoding:
The freeware Windows-only IrfanView has some lossless JPEG operations in its codice_3 plugin.
Metadata.
Metadata, such as ID3 tags, Vorbis comments, or Exif information, can usually be modified or removed without modifying the underlying data.
Downsampling/compressed representation scalability.
One may wish to downsample or otherwise decrease the resolution of the represented source signal and the quantity of data used for its compressed representation without re-encoding, as in bitrate peeling, but this functionality is not supported in all designs, as not all codecs encode data in a form that allows less important detail to simply be dropped.
Some well known designs that have this capability include JPEG 2000 for still images and H.264/MPEG-4 AVC based Scalable Video Coding for video. Such schemes have also been standardized for older designs as well, such as JPEG images with progressive encoding, and MPEG-2 and MPEG-4 Part 2 video, although those prior schemes had limited success in terms of adoption into real-world common usage.
Without this capacity, which is often the case in practice, to produce a representation with lower resolution or lower fidelity than a given one, one needs to start with the original source signal and encode, or start with a compressed representation and then decompress and re-encode it (transcoding), though the latter tends to cause digital generation loss.
Another approach is to encode the original signal at several different bitrates, and their either choose which to use (as when streaming over the internet – as in RealNetworks' "SureStream" – or offering varying downloads, as at Apple's iTunes Store), or broadcast several, where the best that is successfully received is used, as in various implementations of hierarchical modulation. Similar techniques are used in mipmaps, pyramid representations, and more sophisticated scale space methods.
Some audio formats feature a combination of a lossy format and a lossless correction which when combined reproduce the original signal; the correction can be stripped, leaving a smaller, lossily compressed, file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.
Methods.
Other data.
Researchers have (semi-seriously) performed lossy compression on text by either using a thesaurus to substitute short words for long ones, or generative text techniques, although these sometimes fall into the related category of lossy data conversion.
Lowering resolution.
A general kind of lossy compression is to lower the resolution of an image, as in image scaling, particularly decimation.
One may also remove less "lower information" parts of an image, such as by seam carving.
Many media transforms, such as Gaussian blur, are, like lossy compression, irreversible: the original signal cannot be reconstructed from the transformed signal. However, in general these will have the same size as the original, and are not a form of compression.
Lowering resolution has practical uses, as the NASA New Horizons craft will transmit thumbnails of its encounter with Pluto-Charon before it sends the higher resolution images. 
Another solution for slow connections is the usage of Image interlacing which progressively defines the image. Thus a partial transmission is enough to preview the final image, in a lower resolution version, without creating a scaled and a full version too.

</doc>
<doc id="18209" url="http://en.wikipedia.org/wiki?curid=18209" title="Lossless compression">
Lossless compression

Lossless data compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy data compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).
Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).
Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.
Lossless compression techniques.
Most lossless compression programs do two things in sequence: the first step generates a "statistical model" for the input data, and the second step uses this model to map input data to bit sequences in such a way that "probable" (e.g. frequently encountered) data will produce shorter output than "improbable" data.
The primary encoding algorithms used to produce bit sequences are Huffman coding (also used by DEFLATE) and arithmetic coding. Arithmetic coding achieves compression rates close to the best possible for a particular statistical model, which is given by the information entropy, whereas Huffman compression is simpler and faster but produces poor results for models that deal with symbol probabilities close to 1.
There are two primary ways of constructing statistical models: in a "static" model, the data is analyzed and a model is constructed, then this model is stored with the compressed data. This approach is simple and modular, but has the disadvantage that the model itself can be expensive to store, and also that it forces using a single model for all data being compressed, and so performs poorly on files that contain heterogeneous data. "Adaptive" models dynamically update the model as the data is compressed. Both the encoder and decoder begin with a trivial model, yielding poor compression of initial data, but as they learn more about the data, performance improves. Most popular types of compression used in practice now use adaptive coders.
Lossless compression methods may be categorized according to the type of data they are designed to compress. While, in principle, any general-purpose lossless compression algorithm ("general-purpose" meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. Many of the lossless compression techniques used for text also work reasonably well for indexed images.
Text and image.
Statistical modeling algorithms for text (or text-like binary data such as executables) include:
Multimedia.
These techniques take advantage of the specific characteristics of images such as the common phenomenon of contiguous 2-D areas of similar tones.
Every pixel but the first is replaced by the difference to its left neighbor. This leads to small values having a much higher probability than large values.
This is often also applied to sound files, and can compress files that contain mostly low frequencies and low volumes.
For images, this step can be repeated by taking the difference to the top pixel, and then in videos, the difference to the pixel in the next frame can be taken.
A hierarchical version of this technique takes neighboring pairs of data points, stores their difference and sum, and on a higher level with lower resolution continues with the sums. This is called discrete wavelet transform. JPEG2000 additionally uses data points from other pairs and multiplication factors to mix them into the difference. These factors must be integers, so that the result is an integer under all circumstances. So the values are increased, increasing file size, but hopefully the distribution of values is more peaked. 
The adaptive encoding uses the probabilities from the previous sample in sound encoding, from the left and upper pixel in image encoding, and additionally from the previous frame in video encoding. In the wavelet transformation, the probabilities are also passed through the hierarchy.
Historical legal issues.
Many of these methods are implemented in open-source and proprietary tools, particularly LZW and its variants. Some algorithms are patented in the United States and other countries and their legal usage requires licensing by the patent holder. Because of patents on certain kinds of LZW compression, and in particular licensing practices by patent holder Unisys that many developers considered abusive, some open source proponents encouraged people to avoid using the Graphics Interchange Format (GIF) for compressing still image files in favor of Portable Network Graphics (PNG), which combines the LZ77-based deflate algorithm with a selection of domain-specific prediction filters. However, the patents on LZW expired on June 20, 2003.
Many of the lossless compression techniques used for text also work reasonably well for indexed images, but there are other techniques that do not work for typical text that are useful for some images (particularly simple bitmaps), and other techniques that take advantage of the specific characteristics of images (such as the common phenomenon of contiguous 2-D areas of similar tones, and the fact that color images usually have a preponderance of a limited range of colors out of those representable in the color space).
As mentioned previously, lossless sound compression is a somewhat specialized area. Lossless sound compression algorithms can take advantage of the repeating patterns shown by the wave-like nature of the data – essentially using autoregressive models to predict the "next" value and encoding the (hopefully small) difference between the expected value and the actual data. If the difference between the predicted and the actual data (called the "error") tends to be small, then certain difference values (like 0, +1, −1 etc. on sample values) become very frequent, which can be exploited by encoding them in few output bits.
It is sometimes beneficial to compress only the differences between two versions of a file (or, in video compression, of successive images within a sequence). This is called delta encoding (from the Greek letter Δ, which in mathematics, denotes a difference), but the term is typically only used if both versions are meaningful outside compression and decompression. For example, while the process of compressing the error in the above-mentioned lossless audio compression scheme could be described as delta encoding from the approximated sound wave to the original sound wave, the approximated version of the sound wave is not meaningful in any other context.
Lossless compression methods.
By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. For this reason, many different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain.
Some of the most common lossless compression algorithms are listed below.
Video.
See this list of lossless video codecs.
Cryptography.
Cryptosystems often compress data (the "plaintext") "before" encryption for added security. When properly implemented, compression greatly increases the unicity distance by removing patterns that might facilitate cryptanalysis. However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. Thus, cryptosystems must utilize compression algorithms whose output does not contain these predictable patterns.
Genetics.
Genetics compression algorithms (not to be confused with genetic algorithms) are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and specific algorithms adapted to genetic data. In 2012, a team of scientists from Johns Hopkins University published the first genetic compression algorithm that does not rely on external genetic databases for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities.
Executables.
Self-extracting executables contain a compressed application and a decompressor. When executed, the decompressor transparently decompresses and runs the original application. This is especially often used in demo coding, where competitions are held for demos with strict size limits, as small as 1k.
This type of compression is not strictly limited to binary executables, but can also be applied to scripts, such as JavaScript.
Lossless compression benchmarks.
Lossless compression algorithms and their implementations are routinely tested in head-to-head benchmarks. There are a number of better-known compression benchmarks. Some benchmarks cover only the data compression ratio, so winners in these benchmarks may be unsuitable for everyday use due to the slow speed of the top performers. Another drawback of some benchmarks is that their data files are known, so some program writers may optimize their programs for best performance on a particular data set. The winners on these benchmarks often come from the class of context-mixing compression software.
The benchmarks listed in the 5th edition of the "Handbook of Data Compression" (Springer, 2009) are:
Matt Mahoney, in his February 2010 edition of the free booklet "Data Compression Explained", additionally lists the following:
 publishes a chart summary of the "frontier" in compression ratio and time.
Limitations.
Lossless data compression algorithms cannot guarantee compression for all input data sets. In other words, for any lossless data compression algorithm, there will be an input data set that does not get smaller when processed by the algorithm, and for any lossless data compression algorithm that makes at least one file smaller, there will be at least one file that it makes larger. This is easily proven with elementary mathematics using a counting argument, as follows:
Any lossless compression algorithm that makes some files shorter must necessarily make some files longer, but it is not necessary that those files become "very much" longer. Most practical compression algorithms provide an "escape" facility that can turn off the normal coding for files that would become longer by being encoded. In theory, only a single additional bit is required to tell the decoder that the normal coding has been turned off for the entire input; however, most encoding algorithms use at least one full byte (and typically more than one) for this purpose. For example, DEFLATE compressed files never need to grow by more than 5 bytes per 65,535 bytes of input.
In fact, if we consider files of length N, if all files were equally probable, then for any lossless compression that reduces the size of some file, the expected length of a compressed file (averaged over all possible files of length N) must necessarily be "greater" than N. So if we know nothing about the properties of the data we are compressing, we might as well not compress it at all. A lossless compression algorithm is useful only when we are more likely to compress certain types of files than others; then the algorithm could be designed to compress those types of data better.
Thus, the main lesson from the argument is not that one risks big losses, but merely that one cannot always win. To choose an algorithm always means implicitly to select a "subset" of all files that will become usefully shorter. This is the theoretical reason why we need to have different compression algorithms for different kinds of files: there cannot be any algorithm that is good for all kinds of data.
The "trick" that allows lossless compression algorithms, used on the type of data they were designed for, to consistently compress such files to a shorter form is that the files the algorithms are designed to act on all have some form of easily modeled redundancy that the algorithm is designed to remove, and thus belong to the subset of files that that algorithm can make shorter, whereas other files would not get compressed or even get bigger. Algorithms are generally quite specifically tuned to a particular type of file: for example, lossless audio compression programs do not work well on text files, and vice versa.
In particular, files of random data cannot be consistently compressed by any conceivable lossless data compression algorithm: indeed, this result is used to "define" the concept of randomness in algorithmic complexity theory.
It's provably impossible to create an algorithm that can losslessly compress any data. While there have been many claims through the years of companies achieving "perfect compression" where an arbitrary number "N" of random bits can always be compressed to "N" − 1 bits, these kinds of claims can be safely discarded without even looking at any further details regarding the purported compression scheme. Such an algorithm contradicts fundamental laws of mathematics because, if it existed, it could be applied repeatedly to losslessly reduce any file to length 0. Allegedly "perfect" compression algorithms are often derisively referred to as "magic" compression algorithms for this reason.
On the other hand, it has also been proven that there is no algorithm to determine whether a file is incompressible in the sense of Kolmogorov complexity. Hence it's possible that any particular file, even if it appears random, may be significantly compressed, even including the size of the decompressor. An example is the digits of the mathematical constant "pi", which appear random but can be generated by a very small program. However, even though it cannot be determined whether a particular file is incompressible, a simple theorem about incompressible strings shows that over 99% of files of any given length cannot be compressed by more than one byte (including the size of the decompressor).
Mathematical background.
Abstractly, a compression algorithm can be viewed as a function on sequences (normally of octets). Compression is successful if the resulting sequence is shorter than the original sequence (and the instructions for the decompression map). For a compression algorithm to be lossless, the compression map must form a bijection between "plain" and "compressed" bit sequences.
The pigeonhole principle prohibits a bijection between the collection of sequences of length "N" and any subset of the collection of sequences of length "N"−1. Therefore it is not possible to produce an algorithm that reduces the size of every possible input sequence.
Psychological background.
Most everyday files are relatively 'sparse' in an information entropy sense, and thus, most lossless algorithms a layperson is likely to apply on regular files compress them relatively well. This may, through misapplication of intuition, lead some individuals to conclude that a well-designed compression algorithm can compress "any" input, thus, constituting a "magic compression algorithm".
Points of application in real compression theory.
Real compression algorithm designers accept that streams of high information entropy cannot be compressed, and accordingly, include facilities for detecting and handling this condition. An obvious way of detection is applying a raw compression algorithm and testing if its output is smaller than its input. Sometimes, detection is made by heuristics; for example, a compression application may consider files whose names end in ".zip", ".arj" or ".lha" uncompressible without any more sophisticated detection. A common way of handling this situation is quoting input, or uncompressible parts of the input in the output, minimising the compression overhead. For example, the zip data format specifies the 'compression method' of 'Stored' for input files that have been copied into the archive verbatim.
The Million Random Number Challenge.
Mark Nelson, frustrated over many cranks trying to claim having invented a magic compression algorithm appearing in comp.compression, has constructed a 415,241 byte binary file of highly entropic content, and issued a public challenge of $100 to anyone to write a program that, together with its input, would be smaller than his provided binary data yet be able to reconstitute ("decompress") it without error.
The FAQ for the comp.compression newsgroup contains a challenge by Mike Goldman offering $5,000 for a program that can compress random data. Patrick Craig took up the challenge, but rather than compressing the data, he split it up into separate files all of which ended in the number "5", which was not stored as part of the file. Omitting this character allowed the resulting files (plus, in accordance with the rules, the size of the program that reassembled them) to be smaller than the original file. However, no actual compression took place, and the information stored in the names of the files was necessary to reassemble them in the correct order in the original file, and this information was not taken into account in the file size comparison. The files themselves are thus not sufficient to reconstitute the original file; the file names are also necessary. Patrick Craig agreed that no meaningful compression had taken place, but argued that the wording of the challenge did not actually require this. A full history of the event, including discussion on whether or not the challenge was technically met, is on Patrick Craig's web site.

</doc>
<doc id="18210" url="http://en.wikipedia.org/wiki?curid=18210" title="Larry Niven">
Larry Niven

Laurence van Cott Niven (; born April 30, 1938) — known as Larry Niven — is an American science fiction writer. His best-known work is "Ringworld" (1970), which received Hugo, Locus, Ditmar, and Nebula awards. The Science Fiction Writers of America named him the 2015 recipient of the Damon Knight Memorial Grand Master Award. His work is primarily hard science fiction, using big science concepts and theoretical physics. It also often includes elements of detective fiction and adventure stories. His fantasy includes the series "The Magic Goes Away", rational fantasy dealing with magic as a non-renewable resource.
Biography.
Niven was born in Los Angeles. He briefly attended the California Institute of Technology and graduated with a Bachelor of Arts in mathematics (with a minor in psychology) from Washburn University, Topeka, Kansas, in 1962. He did a year of graduate work in mathematics at the University of California at Los Angeles. On September 6, 1969, he married Marilyn Joyce "Fuzzy Pink" Wisowaty, a science fiction and Regency literature fan. He is an agnostic.
Work.
Niven is the author of numerous science fiction short stories and novels, beginning with his 1964 story "The Coldest Place". In this story, the coldest place concerned is the dark side of Mercury, which at the time the story was written was thought to be tidally locked with the Sun (it was found to rotate in a 2:3 resonance after Niven received payment for the story, but before it was published).
In addition to the Nebula award in 1970 and the Hugo and Locus awards in 1971 for "Ringworld", Niven won the Hugo Award for Best Short Story for "Neutron Star" in 1967. He won the same award in 1972, for "Inconstant Moon", and in 1975 for "The Hole Man". In 1976, he won the Hugo Award for Best Novelette for "The Borderland of Sol".
Niven has written scripts for three science fiction television series: the original "Land of the Lost" series; "", for which he adapted his early story "The Soft Weapon"; and "The Outer Limits", for which he adapted his story "Inconstant Moon" into an episode of the same name.
Niven has also written for the DC Comics character Green Lantern including in his stories hard science fiction concepts such as universal entropy and the redshift effect.
He has included limited psi gifts (mind over matter) in some characters in his stories; like Gil Hamilton's psychic arm which can only reach as far as a corporeal arm could, though it can, for example, reach through solid materials and manipulate objects on the other side, or Matt Keller's ability to make people not see him in "A Gift From Earth".
Many of Niven's stories—sometimes called the Tales of Known Space— take place in his Known Space universe, in which humanity shares the several habitable star systems nearest to the Sun with over a dozen alien species, including the aggressive feline Kzinti and the very intelligent but cowardly Pierson's Puppeteers, which are frequently central characters. The "Ringworld" series is part of the Tales of Known Space, and Niven has shared the setting with other writers at least since a 1988 anthology, "The Man-Kzin Wars" (Baen Books, jointly edited with Jerry Pournelle and Dean Ing).
Niven has also written a logical fantasy series "The Magic Goes Away", which utilizes an exhaustible resource called "mana" to power a rule-based "technological" magic. "The Draco Tavern" series of short stories take place in a more light-hearted science fiction universe, and are told from the point of view of the proprietor of an omni-species bar. The whimsical "Svetz" series consists of a collection of short stories, "The Flight of the Horse", and a novel, "Rainbow Mars", which involve a nominal time machine sent back to retrieve long-extinct animals, but which travels, in fact, into alternative realities and brings back mythical creatures such as a Roc and a Unicorn. Much of his writing since the 1970s has been in collaboration, particularly with Jerry Pournelle and Steven Barnes, but also Brenda Cooper and Edward M. Lerner.
Influence.
Niven's most famous contribution to the SF genre comes from his novel "Ringworld", in which he envisions a Ringworld: a band of material, roughly a million miles wide, of approximately the same diameter as Earth's orbit, rotating around a star. The idea's genesis came from Niven's attempts to imagine a more efficient version of a Dyson Sphere, which could produce the effect of surface gravity through rotation. Given that spinning a Dyson Sphere would result in the atmosphere pooling around the equator, the Ringworld removes all the extraneous parts of the structure, leaving a spinning band landscaped on the sun-facing side, with the atmosphere and inhabitants kept in place through centrifugal force and 1000 mile high perimeter walls (rim walls). After publication of "Ringworld", Dan Alderson and Ctien, two fannish friends of Niven, analyzed the structure and told Niven that the Ringworld was dynamically unstable such that when the center of rotation drifts away from the central sun, gravity will pull the ring into contact with the star. Niven used this as a core plot element in the sequel novel, "The Ringworld Engineers".
This idea proved influential, serving as an alternative to a full Dyson Sphere that required fewer assumptions (such as artificial gravity) and allowed a day/night cycle to be introduced (through the use of a smaller ring of "shadow squares", rotating between the ring and its sun). This was further developed by Iain M. Banks in his Culture series, which features about 1/100th ringworld–size megastructures called Orbitals that orbit a star rather than encircling it entirely (actual "Rings" and Dyson "Spheres" are also mentioned but are much rarer). Alastair Reynolds also uses ringworlds in his 2008 novel "House of Suns". The Ringworld-like namesake of the "Halo" video game series is the eponymous Halo megastructure/superweapon.
The original release of "" paid homage to Larry Niven on a card called "Nevinyrral's Disk", with Nevinyrral being "Larry Niven" spelled backwards. Subsequent sets have featured no new cards featuring Nevinyrral, although the character is sporadically quoted on the flavor text of various cards. "Netrunner" paid a similar homage to Larry Niven with the card "Nevinyrral".
Policy involvement.
According to author Michael Moorcock, in 1967 Niven was among those Science Fiction Writers of America members who voiced opposition to the Vietnam War. However, in 1968 Niven's name appeared in a pro-war ad in "Galaxy Science Fiction" magazine.
Niven was an adviser to Ronald Reagan on the creation of the Strategic Defense Initiative antimissile policy, as part of the Citizens' Advisory Council on National Space Policy – as covered in the BBC documentary "Pandora's Box" by Adam Curtis. The council also convinced Vice President Dan Quayle to support the Single-stage-to-orbit (SSTO) concept for a reusable space ship that led to the building of the DC-X.
In 2007, Niven, in conjunction with a group of science fiction writers known as SIGMA, led by Pournelle, began advising the U.S. Department of Homeland Security as to future trends affecting terror policy and other topics.
Other works.
One of Niven's best known humorous works is "Man of Steel, Woman of Kleenex", in which he uses real-world physics to underline the difficulties of Superman and a human woman (Lois Lane or Lana Lang) mating.
Niven appeared in the 1980 science documentary film "Target...Earth?".
Niven's Laws.
Larry Niven is also known in science fiction fandom for "Niven's Law": "There is no cause so right that one cannot find a fool following it". Over the course of his career Niven has added to this first law a list of Niven's Laws which he describes as "how the Universe works" as far as he can tell.
Bibliography.
Tales of Known Space.
Ringworld companion series (with Edward M. Lerner)
Ringworld
Man-Kzin Wars
Heorot.
Heorot (with Steven Barnes and Jerry Pournelle)
with Steven Barnes.
Dream Park

</doc>
<doc id="18212" url="http://en.wikipedia.org/wiki?curid=18212" title="Linux distribution">
Linux distribution

A Linux distribution (often called a distro for short) is an operating system made as a software collection based on the Linux kernel and, often, on a package management system. Linux users usually obtain their operating system by downloading one of the Linux distributions, which are available for a wide variety of systems ranging from embedded devices (for example, OpenWrt) and personal computers to powerful supercomputers (for example, Rocks Cluster Distribution).
A typical Linux distribution comprises a Linux kernel, GNU tools and libraries, additional software, documentation, a window system (the most common being the X Window System), a window manager, and a desktop environment. Most of the included software is free and open-source software made available both as compiled binaries and in source code form, allowing modifications to the original software. Usually, Linux distributions optionally include some proprietary software that may not be available in source code form, such as binary blobs required for some device drivers. Almost all Linux distributions are Unix-like; the most notable exception is Android, which does not include a command-line interface and programs made for typical Linux distributions.
A Linux distribution may also be described as a particular assortment of application and utility software (various GNU tools and libraries, for example), packaged together with the Linux kernel in such a way that its capabilities meet the needs of many users. The software is usually adapted to the distribution and then packaged into software packages by the distribution's maintainers. The software packages are available online in so-called repositories, which are storage locations usually distributed around the world. Beside glue components, such as the distribution installers (for example, Debian-Installer and Anaconda) or the package management systems, there are only very few packages that are originally written from the ground up by the maintainers of a Linux distribution.
More than six hundred Linux distributions exist; over three hundred of those are in active development, constantly being revised and improved. Because of the huge availability of software, distributions have taken a wide variety of forms, including those suitable for use on desktops, servers, laptops, netbooks, mobile phones and tablets, as well as minimal environments typically for use in embedded systems. There are commercially backed distributions, such as Fedora (Red Hat), openSUSE (SUSE) and Ubuntu (Canonical Ltd.), and entirely community-driven distributions, such as Debian, Slackware, Gentoo and Arch Linux. Most distributions come ready to use and pre-compiled for a specific instruction set, while some distributions (such as Gentoo) are distributed mostly in source code form and compiled locally during installation.
History.
Linus Torvalds developed the Linux kernel and distributed its first version, 0.01, in 1991. Linux was initially distributed as source code only, and later as a pair of downloadable floppy disk images – one bootable and containing the Linux kernel itself, and the other with a set of GNU utilities and tools for setting up a file system. Since the installation procedure was complicated, especially in the face of growing amounts of available software, distributions sprang up to simplify this.
Early distributions included the following:
The two oldest and still active distribution projects started in 1993. The SLS distribution was not well maintained, so in July 1993 a new distribution, called Slackware and based on SLS, was released by Patrick Volkerding. Also dissatisfied with SLS, Ian Murdock set to create a free distribution by founding Debian, which had its first release in December 1993.
Users were attracted to Linux distributions as alternatives to the DOS and Microsoft Windows operating systems on IBM PC compatible computers, Mac OS on the Apple Macintosh, and proprietary versions of Unix. Most early adopters were familiar with Unix from work or school. They embraced Linux distributions for their low (if any) cost, and availability of the source code for most or all of the software included.
Originally, the distributions were simply a convenience, but later they became the usual choice even for Unix or Linux experts.
To date, Linux has proven more popular in the server market, primarily for Web and database servers (for example, in the LAMP stack), and in embedded devices market than in the desktop market.
Components.
Many Linux distributions provide an installation system akin to that provided with other modern operating systems. On the other hand, some distributions, including Gentoo Linux, provide only the binaries of a basic kernel, compilation tools, and an installer; the installer compiles all the requested software for the specific architecture of the user's computer, using these tools and the provided source code.
Package management.
Distributions are normally segmented into "packages". Each package contains a specific application or service. Examples of packages are a library for handling the PNG image format, a collection of fonts or a web browser.
The package is typically provided as compiled code, with installation and removal of packages handled by a package management system (PMS) rather than a simple file archiver. Each package intended for such a PMS contains meta-information such as a package description, version, and "dependencies". The package management system can evaluate this meta-information to allow package searches, to perform an automatic upgrade to a newer version, to check that all dependencies of a package are fulfilled, and/or to fulfill them automatically.
Although Linux distributions typically contain much more software than proprietary operating systems, it is normal for local administrators to also install software not included in the distribution. An example would be a newer version of a software application than that supplied with a distribution, or an alternative to that chosen by the distribution (for example, KDE Plasma Workspaces rather than GNOME or vice versa for the user interface layer). If the additional software is distributed in source-only form, this approach requires local compilation. However, if additional software is locally added, the "state" of the local system may fall out of synchronization with the state of the package manager's database. If so, the local administrator will be required to take additional measures to ensure the entire system is kept up to date. The package manager may no longer be able to do so automatically.
Most distributions install packages, including the kernel and other core operating system components, in a predetermined configuration. Few now require or even permit configuration adjustments at first install time. This makes installation less daunting, particularly for new users, but is not always acceptable. For specific requirements, much software must be carefully configured to be useful, to work correctly with other software, or to be secure, and local administrators are often obliged to spend time reviewing and reconfiguring assorted software.
Some distributions go to considerable lengths to specifically adjust and customize most or all of the software included in the distribution. Not all do so. Some distributions provide configuration tools to assist in this process.
By replacing "everything" provided in a distribution, an administrator may reach a "distribution-less" state: everything was retrieved, compiled, configured, and installed locally. It is possible to build such a system from scratch, avoiding a distribution altogether. One needs a way to generate the first binaries until the system is "self-hosting". This can be done via compilation on another system capable of building binaries for the intended target (possibly by cross-compilation). For example, see Linux From Scratch.
Types and trends.
Broadly, Linux distributions may be:
The diversity of Linux distributions is due to technical, organizational, and philosophical variation among vendors and users. The permissive licensing of free software means that any user with sufficient knowledge and interest can customize an existing distribution or design one to suit his or her own needs.
Installation-free distributions (live CD/USB).
A "live" distribution is a Linux distribution that can be booted from removable storage media such as optical discs or USB flash drives, instead of being installed on and booted from a hard disk drive. The portability of installation-free distributions makes them advantageous for applications such as demonstrations, borrowing someone else's computer, rescue operations, or as installation media for a standard distribution.
When the operating system is booted from a read-only medium such as a CD or DVD, any user data that needs to be retained between sessions cannot be stored on the boot device but must be written to another storage device, such as a USB flash drive or a hard disk drive.
Many Linux distributions provide a "live" form in addition to their conventional form, which is a network-based or removable-media image intended to be used only for installation; such distributions include SUSE, Ubuntu, Linux Mint, MEPIS and Fedora. Some distributions, including Knoppix, Puppy Linux, Devil-Linux, SuperGamer, SliTaz GNU/Linux and , are designed primarily for live use. Additionally, some minimal distributions can be run directly from as little space as one floppy disk without the need to change the contents of the system's hard disk drive.
Examples.
Popular distributions.
Well-known Linux distributions include: 
DistroWatch attempts to include every known distribution of Linux, whether currently active or not; it also maintains a ranking of distributions based on its own site's page views, as a measure of relative popularity.
Niche distributions.
Other distributions target specific niches, such as:
Android.
Whether Google's Android counts as a Linux distribution is a matter of definition. It uses the Linux kernel, so the Linux Foundation and Chris DiBona, Google's open source chief, agree that Android is a Linux distribution; others, such as Google engineer Patrick Brady, disagree by noting the lack of support for many GNU tools in Android, including the glibc.
Interdistribution issues.
The Free Standards Group is an organization formed by major software and hardware vendors that aims to improve interoperability between different distributions. Among their proposed standards are the Linux Standard Base, which defines a common ABI and packaging system for Linux, and the Filesystem Hierarchy Standard which recommends a standard filenaming chart, notably the basic directory names found on the root of the tree of any Linux filesystem. Those standards, however, see limited use, even among the distributions developed by members of the organization.
The diversity of Linux distributions means that not all software runs on all distributions, depending on what libraries and other system attributes are required. Packaged software and software repositories are usually specific to a particular distribution, though cross-installation is sometimes possible on closely related distributions.
Tools for choosing a distribution.
There are tools available to help people select an appropriate distribution, such as several versions of the Linux Distribution Chooser, and the universal package search tool "whohas". There are easy ways to try out several Linux distributions before deciding on one: Multi Distro is a Live CD that contains nine space-saving distributions.
Virtual machines such as VirtualBox and VMware Workstation virtualize hardware allowing users to test live media on a virtual machine.
Details and interest rankings of Linux distributions are available on DistroWatch and a fairly comprehensive list of live CDs is available at livecdlist.com. OSDir.com offers screenshots as a way to get a first impression of various distributions.
Installation.
There are many ways to install a Linux distribution. The most common method of installing Linux is by booting from an optical disk that contains the installation program and installable software. Such a disk can be burned from a downloaded ISO image, purchased alone for a low price, provided as a cover disk with a magazine, shipped for free by request, or obtained as part of a box set that may also include manuals and additional commercial software.
Early Linux distributions were installed using sets of floppies but this has been abandoned by all major distributions. Nowadays most distributions offer CD and DVD sets with the vital packages on the first disc and less important packages on later ones. They usually also allow installation over a network after booting from either a set of floppies or a CD with only a small amount of data on it.
New users tend to begin by partitioning a hard drive in order to keep their previously installed operating system. The Linux distribution can then be installed on its own separate partition without affecting previously saved data.
In a Live CD setup, the computer boots the entire operating system from CD without first installing it on the computer's hard disk. Some distributions have a Live CD "installer", where the computer boots the operating system from the disk, and then proceeds to install it onto the computer's hard disk, providing a seamless transition from the OS running from the CD to the OS running from the hard disk.
Both servers and personal computers that come with Linux already installed are available from vendors including Hewlett-Packard, Dell and System76.
On embedded devices, Linux is typically held in the device's firmware and may or may not be consumer-accessible. 
Anaconda, one of the more popular installers, is used by Red Hat Enterprise Linux, Fedora and other distributions to simplify the installation process. Debian, Ubuntu and many others use Debian-Installer.
Installation via an existing operating system.
Some distributions let the user install Linux on top of their current system, such as WinLinux or coLinux. Linux is installed to the Windows hard disk partition, and can be started from inside Windows itself
Virtual machines (such as VirtualBox or VMware) also make it possible for Linux to be run inside another OS. The VM software simulates a separate computer onto which the Linux system is installed. After installation, the virtual machine can be booted as if it were an independent computer.
Various tools are also available to perform full dual-boot installations from existing platforms without a CD, most notably:
Proprietary software.
Some specific proprietary software products are not available in any form for Linux. This includes many popular computer games, although in recent years some game manufacturers have begun making their software available for Linux. Emulation and API-translation projects like Wine and CrossOver make it possible to run non-Linux-based software on Linux systems, either by emulating a proprietary operating system or by translating proprietary API calls (e.g., calls to Microsoft's Win32 or DirectX APIs) into native Linux API calls. A virtual machine can also be used to run a proprietary OS (like Microsoft Windows) on top of Linux.
OEM contracts.
Computer hardware is usually sold with an operating system other than Linux already installed by the original equipment manufacturer (OEM). In the case of IBM PC compatibles the OS is usually Microsoft Windows; in the case of Apple Macintosh computers it has always been a version of Apple's OS, currently OS X; Sun Microsystems sold SPARC hardware with the Solaris installed; video game consoles such as the Xbox, PlayStation, and Wii each have their own proprietary OS. This limits Linux's market share: consumers are unaware that an alternative exists, they must make a conscious effort to use a different operating system, and they must either perform the actual installation themselves, or depend on support from a friend, relative, or computer professional.
However, it is possible to buy hardware with Linux already installed. Lenovo, Hewlett-Packard, Dell, Affordy, and System76 all sell general-purpose Linux laptops, and custom-order PC manufacturers will also build Linux systems (but possibly with the Windows key on the keyboard). Fixstars Solutions (formerly Terra Soft) sells Macintosh computers and PlayStation 3 consoles with Yellow Dog Linux installed.
It is more common to find embedded devices sold with Linux as the default manufacturer-supported OS, including the Linksys NSLU2 NAS device, TiVo's line of personal video recorders, and Linux-based cellphones (including Android smartphones), PDAs, and portable music players. 
The end user license agreement (EULA) for Apple gives the consumer the opportunity to reject the license and obtain a refund. The current Microsoft Windows license lets the manufacturer determine the refund policy. With previous versions of Windows, it was possible to obtain a refund if the manufacturer failed to provide the refund by litigation in the small claims courts. On 15 February 1999, a group of Linux users in Orange County, California held a "Windows Refund Day" protest in an attempt to pressure Microsoft into issuing them refunds. In France, the Linuxfrench and AFUL (French speaking Libre Software Users' Association) organizations along with free software activist Roberto Di Cosmo started a "Windows Detax" movement, which led to a 2006 petition against "racketiciels" (translation: Racketware) with 39,415 signatories and the DGCCRF branch of the French government filing several complaints against bundled software. On March 24, 2014, a new international petition was launched by AFUL on the Avaaz platform, translated into several languages and supported by many organizations around the world.

</doc>
<doc id="18213" url="http://en.wikipedia.org/wiki?curid=18213" title="Los Angeles Dodgers">
Los Angeles Dodgers

The Los Angeles Dodgers are a professional baseball team located in Los Angeles, California. The Dodgers are members of the National League (NL) West division of Major League Baseball (MLB). Established in 1883, the team originated in Brooklyn, New York, where it was known by a number of nicknames before becoming the Brooklyn Dodgers definitively by 1932. The team moved to Los Angeles before the 1958 season. They played their first four seasons in Los Angeles at the Los Angeles Memorial Coliseum before moving to their current home of Dodger Stadium, the third-oldest ballpark in Major League Baseball (after Fenway Park and Wrigley Field).
The Dodgers have won six World Series titles and 21 National League pennants. Eight Cy Young Award winners have pitched for the Dodgers, winning a total of twelve Cy Young Awards (both MLB records). The team has also produced 12 Rookie of the Year Award winners, including four consecutive from 1979 to 1982 and five consecutive from 1992 to 1996, the longest consecutive streaks in Major League Baseball.
History.
In the 20th century, the team, then known as the Robins, won league pennants in 1916 and 1920, losing the World Series both times, first to Boston and then Cleveland. In 1941, as the Dodgers, they captured their third National League pennant, only to lose again to the New York Yankees. This marked the onset of the Dodgers–Yankees rivalry, as the Dodgers would face them in their next six World Series appearances. Led by Jackie Robinson, the first black Major League Baseball player of the modern era; and three-time National League Most Valuable Player Roy Campanella, also signed out of the Negro Leagues, the Dodgers captured their first World Series title in 1955 by defeating the Yankees for the first time, a story notably described in the 1972 book "The Boys of Summer".
Following the 1957 season the team left Brooklyn. In just their second season in Los Angeles, the Dodgers won their second World Series title, beating the Chicago White Sox in six games in 1959. Spearheaded by the dominant pitching style of Sandy Koufax and Don Drysdale, the Dodgers captured three pennants in the 1960s and won two more World Series titles, sweeping the Yankees in four games in 1963, and edging the Minnesota Twins in seven in 1965. The 1963 sweep was their second victory against the Yankees, and their first against them as a Los Angeles team. The Dodgers won four more pennants in 1966, 1974, 1977 and 1978, but lost in each World Series appearance. They went on to win the World Series again in 1981, thanks to pitching sensation Fernando Valenzuela. The early 1980s were affectionately dubbed "Fernandomania." In 1988, another pitching hero, Orel Hershiser, again led them to a World Series victory, aided by one of the most memorable home runs of all time, by their injured star outfielder Kirk Gibson coming off the bench to pinch hit with two outs in the bottom of the ninth inning of game 1, in his only appearance of the series.
The Dodgers share a fierce rivalry with the San Francisco Giants, the oldest rivalry in baseball, dating back to when the two franchises played in New York City. Both teams moved west for the 1958 season. The Brooklyn Dodgers and Los Angeles Dodgers have collectively appeared in the World Series 18 times, while the New York Giants and San Francisco Giants have collectively appeared 20 times and have been invited 21 times. The Giants have won two more World Series (8); the Dodgers have won 21 National League pennants, while the Giants hold the record with 23. Although the two franchises have enjoyed near equal success, the city rivalries are rather lopsided and in both cases, a team's championships have predated to the other's first one in that particular location. When the two teams were based in New York, the Giants won five World Series championships, and the Dodgers one. After the move to California, the Dodgers have won five in Los Angeles, the Giants have won three in San Francisco.
Team history.
Brooklyn Dodgers.
The Dodgers were originally founded in 1883 as the Brooklyn Atlantics, taking the name of a defunct team that had played in Brooklyn before them. The team joined the American Association in 1884 and won the AA championship in 1889 before joining the National League in 1890. They promptly won the NL Championship their first year in the League. The team was known alternatively as the Bridegrooms, Grooms, Superbas, Robins, and Trolley Dodgers before officially becoming the Dodgers in the 1930s.
In Brooklyn, the Dodgers won the NL pennant several times (1890, 1899, 1900, 1916, 1920, 1941, 1947, 1949, 1952, 1953, 1955, 1956) and the World Series in 1955. After moving to Los Angeles, the team won National League pennants in 1959, 1963, 1965, 1966, 1974, 1977, 1978, 1981, and 1988, with World Series championships in 1959, 1963, 1965, 1981, 1988. In all, the Dodgers have appeared in 18 World Series: 9 in Brooklyn and 9 in Los Angeles.
Jackie Robinson.
For most of the first half of the 20th century, no Major League Baseball team employed an African American player. Jackie Robinson became the first African American to play for a Major League Baseball team when he played his first major league game on April 15, 1947, as a member of the Brooklyn Dodgers. This was mainly due to general manager Branch Rickey's efforts. The deeply religious Rickey's motivation appears to have been primarily moral, although business considerations were also a factor. Rickey was a member of The Methodist Church, the antecedent denomination to The United Methodist Church of today, which was a strong advocate for social justice and active later in the Civil Rights movement.
This event was the harbinger of the integration of professional sports in the United States, the concomitant demise of the Negro Leagues, and is regarded as a key moment in the history of the American Civil Rights movement. Robinson was an exceptional player, a speedy runner who sparked the team with his intensity. He was the inaugural recipient of the Rookie of the Year award, which is now named the Jackie Robinson Award in his honor. The Dodgers' willingness to integrate, when most other teams refused to, was a key factor in their 1947–1956 success. They won six pennants in those 10 years with the help of Robinson, three-time MVP Roy Campanella, Cy Young Award winner Don Newcombe, Jim Gilliam and Joe Black. Robinson would eventually go on to become the first African-American elected to the Baseball Hall of Fame in 1962.
Move to Los Angeles.
Real estate businessman Walter O'Malley had acquired majority ownership of the Dodgers in 1950, when he bought the shares of his co-owners, Branch Rickey and the estate of James L. Smith. Before long he was working to buy new land in Brooklyn to build a more accessible and better arrayed ballpark than Ebbets Field. Beloved as it was, Ebbets Field was no longer well-served by its aging infrastructure and the Dodgers could no longer sell out the park even in the heat of a pennant race (despite largely dominating the league from 1946 to 1957).
O'Malley wanted to build a new, state of the art stadium in Brooklyn. But City Planner Robert Moses and other New York politicians refused to let him build the Brooklyn stadium he wanted. During the 1955 season he announced that the team would play seven regular season games and one exhibition game at Jersey City's Roosevelt Stadium in 1956. He expected that this move would put pressure on the city's politicians to build the Dodgers the park he wanted in Brooklyn. Yet Moses and the others considered this an empty threat, and did not believe O'Malley would go through with moving the team from New York City. That is when Los Angeles came into the picture.
After teams began to travel to and from games by air instead of train, it became possible to include locations in the far west. When Los Angeles officials attended the 1956 World Series looking to entice a team to move to the City of Angels, they were not even considering the Dodgers. Their original target had been the Washington Senators (who would in fact move to Bloomington, suburban Minneapolis, to become the Minnesota Twins in 1961). When O'Malley heard that LA was looking for a club, he sent word to the Los Angeles officials that he was interested in talking. LA offered him what New York would not: a chance to buy land suitable for building a ballpark, and own that ballpark, giving him complete control over all revenue streams. When the news came out, NY Mayor Robert F. Wagner, Jr. and Moses made a feeble effort to save the Dodgers, offering to build a ballpark on the World's Fair Grounds in Queens. Wagner was already on shaky ground, as the New York Giants were getting ready to move out of the crumbling Polo Grounds. However, O'Malley was interested in his park only under his conditions, and the plans for a new stadium in Brooklyn seemed like a pipe dream. Walter O'Malley was left with the difficult decision to move the Dodgers to California, convincing Giants owner Horace Stoneham to move to San Francisco instead of Minneapolis to keep the Giants-Dodgers rivalry alive on the West Coast. There was no turning back: the Dodgers were heading for Hollywood.
The Dodgers played their final game at Ebbets Field on September 24, 1957, which the Dodgers won 2–0 over the Pittsburgh Pirates.
Los Angeles Dodgers.
The Dodgers were the first Major League Baseball team to ever play in Los Angeles. On April 18, 1958, the Dodgers played their first LA game, defeating the former New York and now new San Francisco Giants, 6–5, before 78,672 fans at the Los Angeles Memorial Coliseum. Catcher Roy Campanella, left partially paralyzed in an off-season accident, was never able to play in Los Angeles.
Construction on Dodger Stadium was completed in time for Opening Day 1962. With its clean, simple lines and its picturesque setting amid hills and palm trees, the ballpark quickly became an icon of the Dodgers and their new California lifestyle. O'Malley was determined that there would not be a bad seat in the house, achieving this by cantilevered grandstands that have since been widely imitated. More importantly for the team, the stadium's spacious dimensions, along with other factors, gave defense an advantage over offense and the Dodgers moved to take advantage of this by assembling a team that would excel with its pitching.
Since moving to Los Angeles, the Dodgers have won nine more National League Championships and five World Series rings.
Other historical notes.
The team's nickname.
The Dodgers' official history tells us that, "[t]he term "Trolley Dodgers" was attached to the Brooklyn ballclub due to the complex maze of trolley cars that weaved its way through the borough of Brooklyn."
In 1892, the city of Brooklyn (Brooklyn was an independent city until annexed by New York City in 1898) began replacing its slow-moving, horse-drawn trolley lines with the faster, more powerful electric trolley lines. Within less than three years, by the end of 1895, electric trolley accidents in Brooklyn had resulted in more than 130 deaths and maimed well over 500 people. Brooklyn's high-profile, the significant number of widely-reported accidents, and a trolley strike in early 1895, combined to create a strong association in the public's mind between Brooklyn and trolley dodging.
Sportswriters started using the name "trolley dodgers" to refer to the Brooklyn team early in the 1895 season. The name was shortened to, on occasion, the "Brooklyn Dodgers" as early as 1898.
Sportswriters in the early 20th century began referring to the Dodgers as the "Bums", in reference to the team's fans and possibly because of the "street character" nature of Jack Dawkins, the "Artful Dodger" in Charles Dickens' "Oliver Twist".
Other team names used by the franchise were the Atlantics, Grays, Grooms, Bridegrooms, Superbas and Robins. All of these nicknames were used by fans and sportswriters to describe the team, but not in any official capacity. The team's legal name was the Brooklyn Base Ball Club. However, the Trolley Dodger nickname was used throughout this period, simultaneously with these other nicknames, by fans and sportswriters of the day. The team did not use the name in any formal sense until 1932, when the word "Dodgers" appeared on team jerseys. The "conclusive shift" came in 1933, when both home and road jerseys for the team bore the name "Dodgers".
Examples of how the many popularized names of the team were used are available from newspaper articles before 1932. A New York Times article describing a game in 1916 starts out: "Jimmy Callahan, pilot of the Pirates, did his best to wreck the hopes the Dodgers have of gaining the National League pennant", but then goes on to comment: "the only thing that saved the Superbas from being toppled from first place was that the Phillies lost one of the two games played". What is interesting about the use of these two nicknames is that most baseball statistics sites and baseball historians generally now refer to the pennant-winning 1916 Brooklyn team as the Robins. A 1918 New York Times article uses the nickname in its title: "Buccaneers Take Last From Robins", but the subtitle of the article reads: "Subdue The Superbas By 11 To 4, Making Series An Even Break".
Another example of the use of the many nicknames is found on the program issued at Ebbets Field for the 1920 World Series which identifies the matchup in the series as "Dodgers vs. Indians" despite the fact that the Robins nickname had been in consistent use for around six years. The "Robins" nickname was derived from the name of their Hall of Fame manager, Wilbert Robinson, who led the team from 1914 to 1931
Uniforms.
The Dodgers uniforms have remained relatively unchanged for over 70 years. The home jersey is white with "Dodgers" written in script across the chest in Dodger Blue. The road jersey is grey with "Los Angeles" written in script across the chest in Dodger Blue. The word "Dodgers" was first used on the front of the team's home jersey in 1933; the uniform was white with red pinstripes, and the stylized B on the left shoulder. The Dodgers also wore green outlined uniforms and green caps throughout the 1937 season but reverted to blue the following year.
The current design was adopted in 1939, and has remained the same ever since with only minor cosmetic changes. Since 1952, the home uniform has had a red uniform number under the "Dodgers" script. The road jerseys also have a red uniform number under the script. The most obvious change is the removal of "Brooklyn" from the road jerseys and the replacement of the stylized "B" with the interlocking "LA" on the caps in 1958. In 1970, the Dodgers removed the city name from the road jerseys and had "Dodgers" on both the home and away uniforms. The city script returned to the road jerseys in 1999, and the tradition-rich Dodgers flirted with an alternate uniform for the first time since 1944 (when all-blue satin uniforms were introduced). These 1999 alternate jerseys had a royal blue top with the "Dodgers" script in white across the chest, and the red number on the front. These were worn with white pants and a new cap with silver brim, top button and Dodger logo. These alternates proved unpopular and the team abandoned them after only one season. In 2014, the Dodgers introduced an alternate road jersey: a grey version of the home jersey (w/the Dodgers script).
Asian players.
The Dodgers have been groundbreaking in their signing of players from Asia; mainly, Japan, South Korea, and Taiwan. Former owner Peter O'Malley began reaching out in 1980 by starting clinics in China and South Korea, building baseball fields in two Chinese cities, and in 1998 becoming the first major league team to open an office in Asia. The Dodgers were the second team to start a Japanese player in recent history, pitcher Hideo Nomo, the first team to start a South Korean player, pitcher Chan Ho Park, and the first Taiwanese player, Chin-Feng Chen. In addition, they were the first team to send out three Asian pitchers, from different Asian countries, in one game: Park, Hong-Chih Kuo of Taiwan, and Takashi Saito of Japan. In the 2008 season, the Dodgers had the most Asian players on its roster of any major league team with five. They included Japanese pitchers Takashi Saito and Hiroki Kuroda; South Korean pitcher Chan Ho Park; and Taiwanese pitcher Hong-Chih Kuo and infielder Chin-Lung Hu. In 2005, the Dodgers' Hee Seop Choi became the first Asian player to compete in the Home Run Derby. For the 2013 season, the Dodgers signed starting pitcher Hyun-Jin Ryu with a six-year, $36,000,000 contract, after posting a bid of nearly $27,000,000 to acquire him from the KBO's Hanhwa Eagles.
Rivalries.
The Dodgers' rivalry with the San Francisco Giants dates back to the 19th century, when the two teams were based in New York; the rivalry with the New York Yankees took place when the Dodgers were based in New York, but was revived with their East Coast/West Coast World Series battles in 1963, 1977, 1978, and 1981. The Dodgers also had a heated rivalry with the Cincinnati Reds during the 1970s, 1980s and early 1990s. The rivalry with the Los Angeles Angels of Anaheim and the San Diego Padres dates back to the Angels' and Padres' respective inaugural seasons (Angels in 1961, Padres in 1969). Regional proximity is behind the rivalries with both the Angels and the Padres.
San Francisco Giants.
The Dodgers–Giants rivalry is one of the longest-standing rivalries in American baseball.
The feud between the Dodgers and the San Francisco Giants began in the late 19th century when both clubs were based in New York City, with the Dodgers playing in Brooklyn and the Giants playing at the Polo Grounds in Manhattan. After the 1957 season, Dodgers owner Walter O'Malley moved the team to Los Angeles for financial and other reasons. Along the way, he managed to convince Giants owner Horace Stoneham—who was considering moving his team to Minnesota—to preserve the rivalry by bringing his team to California as well. New York baseball fans were stunned and heartbroken by the move. Given that the cities of Los Angeles and San Francisco have been bitter rivals in economic, cultural, and political arenas for over a century and a half, the new venue in California became fertile ground for its transplantation.
Each team's ability to endure for over a century while moving across an entire continent, as well as the rivalry's leap from a cross-city to a cross-state engagement, have led to the rivalry being considered one of the greatest in sports history.
Unlike many other historic baseball match-ups in which one team remains dominant for most of their history, the Dodgers–Giants rivalry has exhibited a persistent balance in the respective successes of the two teams. While the Giants have more wins in franchise history, and lead all NL teams with 23 National League pennants, the Dodgers are second, having won 21; the Giants have won eight World Series titles, while the Dodgers have won six. The 2010 World Series was the Giants' first championship since moving to California, while the Dodgers' last title came in the 1988 World Series.
Los Angeles Angels of Anaheim.
This rivalry refers to a series of games played with the Los Angeles Angels of Anaheim. The series takes its name from the massive freeway system in the greater Los Angeles metropolitan area, the home of both teams; one could travel from one team's stadium to the other simply by traveling along Interstate 5. The term is akin to "Subway Series" which refers to meetings between New York City baseball teams. The term "Freeway Series" also inspired the official name of the regions' NHL rivalry: the "Freeway Face-Off".
San Diego Padres.
The rivalry between Dodgers and San Diego Padres is a divisional rivalry since both play in the NL West. Also, the two teams are both based in major cities of Southern California, and both separated by the Interstate 5. One of the most notable games between the two is the one on April 11, 2013 when they got into a bench-clearing brawl after Dodgers pitcher Zach Greinke hit Padres Carlos Quentin with a pitch. The rivalry has escalated slightly after the Dodgers traded fan favorite, Matt Kemp, to the Padres on December 11, 2014. The decision to trade Kemp was highly unfavorable with Dodger fans, as he was viewed by many as an unofficial captain for the team.
Historical rivalry.
New York Yankees.
The Dodgers–Yankees rivalry is one of the most well-known rivalries in Major League Baseball. The two teams have met eleven times in the World Series, more times than any other pair from the American and National Leagues. The initial significance was embodied in the two teams' proximity in New York City, when the Dodgers initially played in Brooklyn. After the Dodgers moved to Los Angeles in 1958, the rivalry retained its significance as the two teams represented the dominant cities on each coast of the United States, and since the 1980s, the two largest cities in the United States.
Although the rivalry's significance arose from the two teams' numerous World Series meetings, the Yankees and Dodgers have not met in the World Series since 1981. They would not play each other in a non-exhibition game until 2004, when they played a three-game interleague series. Their last meeting was in July 2013, when they split a two-game series in Los Angeles.
Fan support.
The Dodgers have a loyal fanbase, evidenced by the fact that the Dodgers were the first MLB team to attract more than 3 million fans in a season (in 1978), and accomplished that feat six more times before any other franchise did it once. The Dodgers drew at least 3 million fans for 15 consecutive seasons from 1996 to 2010, the longest such streak in all of MLB. On July 3, 2007, Dodgers management announced that total franchise attendance, dating back to 1901, had reached 175 million, a record for all professional sports. In 2007, the Dodgers set a franchise record for single-season attendance, attracting over 3.8 million fans. In 2009, the Dodgers led MLB in total attendance. The Dodger baseball cap is consistently in the top three in sales. During the 2011-2012 season, Frank McCourt, the owner of the Dodgers at that time, was going through a rough divorce with his wife over who should be the owner of the Dodger team. Instead, Frank McCourt paid $131 million to his wife as part of the divorce settlement. As a result, the team payroll was financially low for a big-budget team crippling the Dodgers in the free-agent market. Collectively, the team performance waned due to the distracting drama in the front office resulting in low attendance numbers.
Given the team's proximity to Hollywood, numerous celebrities can often be seen attending home games at Dodger Stadium. Celebrities such as Magic Johnson, Tiger Woods, Alyssa Milano, and Shia Labeouf are known to sit at field box seats behind home plate where they sign autographs for fellow Dodger fans. Actor Bryan Cranston is a lifelong Dodger fan.
The Dodgers set the world record for the largest attendance for a single baseball game during an exhibition game against the Boston Red Sox on March 28, 2008 at the Los Angeles Memorial Coliseum in honor of the Dodgers 50th anniversary, with 115,300 fans in attendance. All proceeds from the game benefited the official charity of the Dodgers, ThinkCure! which supports cancer research at Children's Hospital Los Angeles and City of Hope.
Radio and television.
Vin Scully has called Dodgers games since 1950. His longtime partners were Jerry Doggett (1956–1987) and Ross Porter (1977–2004). In 1976, he was selected by Dodgers fans as the Most Memorable Personality (on the field or off) in the team's history. He is also a recipient of the Baseball Hall of Fame's Ford C. Frick Award for broadcasters (inducted in 1982). Unlike the modern style in which multiple sportscasters have an on-air conversation (usually with one functioning as play-by-play announcer and the other[s] as color commentator), Scully, Doggett and Porter generally called games solo, trading with each other inning-by-inning. In the 1980s and 1990s, Scully would call the entire radio broadcast except for the third and seventh inning, allowing the other Dodger commentators to broadcast an inning.
When Doggett retired after the 1987 season, he was replaced by Hall-of-Fame Dodgers pitcher Don Drysdale, who previously broadcast games for the California Angels and Chicago White Sox. Drysdale died in his hotel room following a heart attack before a game in Montreal in 1993. This was a difficult broadcast for Scully and Porter who could not mention it on-air until Drysdale's family had been notified and the official announcement made. He was replaced by former Dodgers outfielder Rick Monday. Porter's tenure ended after the 2004 season, after which the format of play-by-play announcers and color commentators was installed, led by Monday and newcomer Charley Steiner. Scully, however, continues to announce solo.
Scully calls roughly 100 games per season (all home games and road games in California and Arizona) for both flagship radio station KLAC and on television for SportsNet LA. Scully is simulcast for the first three innings of each of his appearances, then announces only for the TV audience. If Scully is calling the game, Steiner takes over play-by-play on radio beginning with the fourth inning, with Monday as color commentator. If Scully is not calling the game, Steiner and Orel Hershiser call the entire game on television while Monday and Kevin Kennedy do the same on radio. In the event the Dodgers are in post-season play, Scully calls the first three and last three innings of the radio broadcast alone and Steiner and Monday handle the middle innings.
The Dodgers also broadcast on radio in Spanish, and the play-by-play is voiced by another Frick Award winner, Jaime Jarrín, who has been with the Dodgers since 1959. The color analyst for some games is former Dodger pitcher Fernando Valenzuela, for whom Jarrin once translated post-game interviews. The Spanish-language radio flagship station is KTNQ.
Achievements.
Ford C. Frick Award recipients.
Names in bold received the award based primarily on their work as Dodgers broadcasters.
 Played as Dodgers
Retired numbers.
Koufax, Campanella, and Robinson were the first Dodgers to have their numbers retired, in a ceremony at Dodger Stadium on June 4, 1972. This was the year in which Koufax was inducted into the Baseball Hall of Fame; Robinson and Campanella were already Hall-of-Famers.
Alston's number was retired in the year following his retirement as the Dodgers manager, six years before he was inducted into the Hall of Fame.
Gilliam died suddenly in 1978 after a 28-year career with the Dodgers organization. The Dodgers retired his number two days after his death, prior to Game 1 of the 1978 World Series. He is the only non-Hall-of-Famer to have his number retired by the Dodgers.
Beginning in 1980, the Dodgers have retired the numbers of longtime Dodgers (Snider, Reese, Drysdale, Lasorda, and Sutton) during the seasons in which each was inducted into the Hall of Fame.
In 1997, 50 years after he broke the color barrier and 25 years after the Dodgers retired his number, Robinson's No.42 was retired throughout Major League Baseball. Robinson is the only major league baseball player to have this honor bestowed upon him. Starting in the 2007 season, Jackie Robinson Day (April 15, commemorating Opening Day of Robinson's rookie season of 1947) has featured many or all players and coaches wearing the number 42 as a tribute to Robinson.
The Dodgers have not issued No.34 since the departure of Fernando Valenzuela in 1991, although it has not been officially retired.
Personnel.
Managers.
Since 1884, the Dodgers have used a total of 30 Managers, the most current being Don Mattingly, who was appointed at the conclusion of the 2010 season as the successor to Joe Torre.
The managers of the Los Angeles Dodgers (1958–present) are as follows:
Public address announcers.
From the Dodgers' move to Los Angeles from Brooklyn in 1958, the Dodgers employed a handful of well-known public address announcers; the most famous of which was John Ramsey, who served as the PA voice of the Dodgers from 1958 until his retirement in 1982; as well as announcing at other venerable Los Angeles venues, including the Los Angeles Memorial Coliseum and Sports Arena, and the Forum. Ramsey died in 1990.
From 1958 to 1982, Doug Moore, a local businessman; Philip Petty, an Orange County Superior Court Judge; and Dennis Packer; served as back-up voices for John Ramsey for the Dodgers, California Angels, Los Angeles Chargers, USC football and Los Angeles Rams. Packer was Ramsey's primary backup for the Los Angeles Lakers and Los Angeles Kings until Ramsey's retirement from the Forum in 1978. Thereafter, Packer became the public address announcer for the Lakers, Kings, indoor soccer and indoor tennis events at the Forum.
Nick Nickson, a radio broadcaster for the Los Angeles Kings, replaced John Ramsey as the Dodger Stadium public address announcer in 1983 and served in that capacity through the 1989 season to work with the Kings full-time.
Dennis Packer and Pete Arbogast were emulators of John Ramsey, using the same stentorian style of announcing Ramsey was famous for. Packer and Arbogast shared the stadium announcing chores for the 1994 FIFA World Cup matches at the Rose Bowl. Arbogast won the Dodgers job on the day that Ramsey died on January 25, 1990, by doing a verbatim imitation of Ramsey's opening and closing remarks that were standard at each game. He left following the 1993 season to concentrate with his duties as the radio voice of USC sports. Arbogast's replacement was Mike Carlucci, who remained as the Dodgers' PA voice until 2001.
Through 2014, the Dodgers public address announcer was Eric Smith, who also announces for the Los Angeles Clippers and USC Trojans.
On April 3, 2015 the Dodgers announced that former radio broadcaster Todd Leitz would become their new public address announcer. Leitz was an anchor and news reporter in Los Angeles at KNX 1070 AM for 10 years, and a news reporter at KABC 790 for two years.
Other.
Vin Scully is permanently honored in the Hall's "Scribes & Mikemen" exhibit as a result of winning the Ford C. Frick Award in 1982. As with all Frick Award recipients, he is not officially considered an inducted member of the Hall of Fame.
Sue Falsone, served as the first female physical therapist in Major League baseball, and from 2012 to 2013, was the first female head athletic trainer.
External links.
class="navbox collapsible autocollapse" style="width:100;"
!colspan="3" style="background-color:#DCDCDC;"

</doc>
<doc id="18214" url="http://en.wikipedia.org/wiki?curid=18214" title="Louis Andriessen">
Louis Andriessen

Louis Andriessen (]; born 6 June 1939) is a Dutch composer and pianist based in Amsterdam. He was a composition teacher at the Royal Conservatory of The Hague. He was recipient of the Gaudeamus International Composers Award in 1959.
Life and career.
Andriessen was born in Utrecht into a musical family, the son of the composer Hendrik Andriessen (1892–1981), brother of composers Jurriaan Andriessen (1925–1996) and Caecilia Andriessen (born 1931), and nephew of Willem Andriessen (1887–1964).
Andriessen originally studied with his father and Kees van Baaren at the Royal Conservatory of The Hague, before embarking upon two years of study with Italian composer Luciano Berio in Milan and Berlin. He later joined the faculty of the Royal Conservatory. See: .
In 1969 Andriessen co-founded STEIM in Amsterdam. He also helped found the instrumental groups Orkest de Volharding and Hoketus, both of which performed compositions of the same names. He later became closely involved in the ongoing Schonberg and Asko ensembles and inspired the formation of the British ensemble Icebreaker.
Andriessen, a widower, was married to guitarist Jeanette Yanikian (1935–2008). They were a couple for over 40 years and were married in 1996.
Style and notable works.
Andriessen's early works show experimentation with various contemporary trends: post war serialism ("Series", 1958), pastiche ("Anachronie I", 1966–67), and tape ("Il Duce", 1973). His reaction to what he perceived as the conservatism of much of the Dutch contemporary music scene quickly moved him to form a radically alternative musical aesthetic of his own. Since the early 1970s he has refused to write for conventional symphony orchestras and has instead opted to write for his own idiosyncratic instrumental combinations, which often retain some traditional orchestral instruments alongside electric guitars, electric basses, and congas.
Andriessen's mature music combines the influences of jazz, American minimalism, Igor Stravinsky and Claude Vivier. His harmonic writing eschews the consonant modality of much minimalism, preferring post war European dissonance, often crystallised into large blocks of sound. Large scale pieces such as "De Staat" ['Republic'] (1972–76), for example, are influenced by the energy of the big band music of Count Basie and Stan Kenton and the repetitive procedures of Steve Reich, both combined with bright, clashing dissonances. Andriessen's music is thus anti-Germanic and anti-Romantic, and marks a departure from post war European serialism and its offshoots. He has also played a role in providing alternatives to traditional performance practice techniques, often specifying forceful, rhythmic articulations, and amplified, non-vibrato, singing.
Other notable works include "Workers Union" (1975), a melodically indeterminate piece "for any loud sounding group of instruments"; "Mausoleum" (1979) for 2 baritones and large ensemble; "De Tijd" ['Time'] (1979–81) for female singers and ensemble; "De Snelheid" ['Velocity'] (1982–83), for 3 amplified ensembles; "De Materie" ['Matter'] (1984–88), a large four-part work for voices and ensemble; collaborations with filmmaker and librettist Peter Greenaway on the film "M is for Man, Music, Mozart" and the operas "Rosa: A Horse Drama" (1994) and "Writing to Vermeer" (1998); and the recent "La Passione" (2000–02) for female voice, violin and ensemble.

</doc>
<doc id="18217" url="http://en.wikipedia.org/wiki?curid=18217" title="Leonard Peltier">
Leonard Peltier

Leonard Peltier (born September 12, 1944) is a Native American activist and member of the American Indian Movement (AIM). In 1977 he was convicted and sentenced to two consecutive terms of life imprisonment for first degree murder in the shooting of two Federal Bureau of Investigation (FBI) agents during a 1975 conflict on the Pine Ridge Indian Reservation.
Peltier's indictment and conviction have been the subject of much controversy; Amnesty International placed his case under the "Unfair Trials" category of its "Annual Report: USA 2010".
Peltier is incarcerated at the United States Penitentiary, Coleman in Florida. Peltier's next scheduled parole hearing will be in July 2024. Barring appeals, parole or presidential pardon, his projected release date is October 11, 2040.
Early life and education.
Peltier was born in Grand Forks, North Dakota, the eleventh of thirteen children, to Leo Peltier and Alvina Robideau. His father was three-quarters Chippewa and one-quarter French, and his mother was Lakota Sioux on her mother's side and Chippewa on her father's. Peltier's parents divorced when he was four years old. At this time, Leonard and his sister Betty Ann were taken to live with their paternal grandparents Alex and Mary Dubois-Peltier in the Turtle Mountain Indian Reservation of the Turtle Mountain Chippewa near Belcourt, North Dakota.
In September 1953, at the age of nine, Leonard was enrolled at the Wahpeton Indian School in Wahpeton, North Dakota, an Indian boarding school run by the Bureau of Indian Affairs (BIA). He graduated at Wahpeton in May 1957, and attended the Flandreau Indian School in Flandreau, South Dakota. After dropping out in the ninth grade, he returned to the Turtle Mountain Reservation to live with his father.
Career and activism.
In 1965, Peltier relocated to Seattle, Washington. He worked for several years and became the owner of an auto body station. In the city, Peltier became involved in a variety of causes championing Native American civil rights, and eventually joined the American Indian Movement.
In the early 1970s, he learned about the factional tensions at the Pine Ridge Indian Reservation in South Dakota between supporters of Richard Wilson, elected tribal chairman in 1972, and traditionalist members of the tribe. Wilson had created a private militia, known as the Guardians of the Oglala Nation (GOON), whose members were reputed to have attacked political opponents. Protests over a failed impeachment hearing of Wilson contributed to the AIM and Lakota armed takeover of Wounded Knee in February 1973, which resulted in a 71-day siege by federal forces, known as the Wounded Knee Incident. They demanded the resignation of Wilson. Peltier, however, spent most of the occupation in a Milwaukee jail charged with attempted murder. When Peltier secured bail at the end of April, he took part in an AIM protest outside the federal building in Milwaukee and was on his way to Wounded Knee with the group to deliver supplies when the incident ended.
The takeover did not end Wilson's leadership, the actions of the GOONs or the violence; over sixty murders of AIM members and their supporters occurred on Pine Ridge during the next three years. In 1975 Peltier traveled to the Pine Ridge reservation as a member of AIM to try to help reduce the continuing violence among political opponents. At the time, he was a fugitive, with a warrant issued in Milwaukee, Wisconsin. It charged him with unlawful flight to avoid prosecution for the attempted murder of an off-duty Milwaukee police officer, a crime of which he was later acquitted.
Shootout at Pine Ridge.
On June 26, 1975, Special Agents Jack R. Coler and Ronald A. Williams of the Federal Bureau of Investigation (FBI) were on the Pine Ridge Reservation searching for a young man named Jimmy Eagle, who was wanted for questioning in connection with the recent assault and robbery of two local ranch hands. Eagle had been involved in a physical altercation with a friend, during which he had stolen a pair of leather cowboy boots. At approximately 11:50 a.m., Williams and Coler, driving two separate unmarked cars, spotted, reported, and followed a red pick-up truck which matched the description of Eagle's.
Soon after his initial report, Williams radioed into a local dispatch that he and Coler had come under high-powered rifle fire from the occupants of the vehicle and were unable to return fire with their .38 Special revolvers. Williams radioed that they would be killed if reinforcements did not arrive. He next radioed that he was hit. FBI Special Agent Gary Adams was the first to respond to Williams' call for assistance, and he also came under intense gun fire; he was unable to reach Coler and Williams.
The FBI, BIA, and the local police spent the afternoon waiting for other law enforcement officers. At 2:30 p.m., a BIA rifleman fatally shot Joe Stuntz, an AIM member who had taken part in the shootout. At 4:31 p.m., authorities recovered the bodies of Williams and Coler from their vehicles. At 6:30 p.m. they ignited tear gas and stormed the Jumping Bull houses, where they found Stuntz's body clad in Coler's green FBI field jacket, which he appeared to have taken from the agent's car. The two FBI Agents were later confirmed to have died on 26 June 1975. Stuntz appeared to have died later, during subsequent shooting.
The FBI reported that Williams had received a defensive wound to his right hand (as he attempted to shield his face) from a bullet which passed through his hand into his head, killing him instantly. Williams received two gunshot injuries, to his body and foot, prior to the contact shot that killed him. Coler, incapacitated from earlier bullet wounds, had been shot twice in the head. In total, 125 bullet holes were found in the agents' vehicles, many from a .223 Remington (5.56 mm) rifle.
Leonard Peltier provided numerous alibis, to different people, about his activities on the morning of the attacks. In an interview with the author Peter Matthiessen ("In the Spirit of Crazy Horse" 1983), Peltier described working on a car in Oglala, claiming to have driven back to the Jumping Bull Compound about an hour before the shooting started. In an interview with Lee Hill, he described being woken up in the tent city at the ranch by the sound of gunshots. To Harvey Arden, for "Prison Writings", he described enjoying a beautiful morning before he heard the firing.
Aftermath.
On September 5, 1975, Williams' handgun and shells from both agents' handguns were found in a vehicle near a residence where Dino Butler was arrested. On September 9, 1975, Peltier purchased a Plymouth station wagon in Denver, Colorado. The FBI sent out descriptions of the vehicle and a recreational vehicle (RV) in which Peltier and associates were believed to be traveling. An Oregon State Trooper stopped the vehicles and ordered the driver of the RV to exit; but, after a brief exchange of gunfire, the driver escaped on foot. Authorities later identified the driver as Peltier. Coler's handgun was found in a bag under the front seat of the RV, where authorities later reported finding Peltier's thumb print. On December 22, 1975, Peltier was named to the FBI Ten Most Wanted Fugitives list.
On September 10, 1975, a station wagon exploded on the Kansas Turnpike near Wichita. A burned AR-15 rifle was recovered, along with Agent Coler's .38 Special revolver. The car was loaded with weapons and explosives, which apparently ignited when placed too close to a hole in the exhaust pipe. Injured in the blast were Robert Robideau, Norman Charles, and Michael Anderson, who were all members of AIM.
Peltier fled to Hinton, Alberta, where he hid in a friend's cabin. On February 6, 1976, he was arrested and extradited from Canada based on an affidavit signed by Myrtle Poor Bear, a local Native American woman. She claimed to have been Peltier's girlfriend at the time and to have witnessed the murders. But, according to Peltier and others at the scene, Poor Bear did not know Peltier, nor was she present at the time of the shooting. She later claimed that she was pressured and threatened by FBI agents into giving the statements. Poor Bear attempted to testify about the FBI's intimidation at Peltier's trial; however, the judge barred her testimony on the grounds of mental incompetence.
Peltier fought extradition to the United States, even as Bob Robideau and Darrelle “Dino” Butler, AIM members also present on the Jumping Bull compound at the time of the shootings, were found not guilty on the grounds of self-defense by a federal jury in Cedar Rapids, Iowa. Peltier returned too late to be tried with Robideau and Butler, and he was subsequently tried separately. 
Peltier's trial was held in Fargo, North Dakota, where a jury convicted Peltier of the murders of Coler and Williams. Unlike the trial for Butler and Robideau, the jury was informed that the two FBI agents were killed by close-range shots to their heads, when they were already defenseless due to previous gunshot wounds. They also saw autopsy and crime scene photographs of the two agents, which had not been shown to the jury at Cedar Rapids. In April 1977, Peltier was convicted and sentenced to two consecutive life sentences. Upon hearing the appeals case on February 11, 1986, Federal Appeals Judge Gerald W. Heaney, concluded, "When all is said and done ... a few simple but very important facts remain. The casing introduced into evidence had in fact been extracted from the Wichita AR-15." In his 1999 memoir, Peltier admitted that he fired at the agents, but denies that he fired the fatal shots that killed them.
A cartridge case from the Wichita AR-15 was found in the trunk of Agent Coler's car, and admitted as evidence at Peltier's trial in Fargo, North Dakota. Also admitted as evidence was the fact that no person involved in shooting at the agents, other than Peltier, possessed an AR-15 rifle.
The journalist Scott Anderson said that in a 1995 interview with Peltier, he sought answers to the contradictions he had found in Peltier's accounts of the incident on 26 June 1975. When asked about the guns he carried that day, Peltier listed a .30-30, a .303, a .306, a .250 and a .22, but he did not remember the AR-15.
The former United States Attorney General Ramsey Clark has served "pro bono" as one of Peltier's lawyers and has aided in filing a series of appeals on Peltier's behalf. In all appeals, the conviction and sentence have been affirmed by the 8th Circuit Court of Appeals. The last two appeals were "Peltier v. Henman", 997 F. 2d 461 in July 1993 and "United States v. Peltier", 446 F.3d 911 (8th Cir. 2006) (Peltier IV) in 2006.
Doubts about legal proceedings.
Numerous doubts have been raised over Peltier's guilt and the fairness of his trial, based on allegations and inconsistencies regarding the FBI and prosecution's handling of this case:
Post-trial debate.
Peltier's conviction sparked great controversy and has drawn criticism from a number of sources. Numerous appeals have been filed on his behalf; none of the resulting rulings has been made in his favor. Peltier is considered by the AIM to be a political prisoner and has received support from individuals and groups including Nelson Mandela, Rigoberta Menchú, Amnesty International, the United Nations High Commissioner for Human Rights, the Zapatista Army of National Liberation, Tenzin Gyatso (the 14th Dalai Lama), Zack de la Rocha, the European Parliament, the Belgian Parliament, the Italian Parliament, the Kennedy Memorial Center for Human Rights, Archbishop Desmond Tutu, and Rev. Jesse Jackson.
Peltier's supporters have given two different rationales for overturning the conviction. One argument asserts that Peltier did not commit the murders, and that he either had no knowledge of the murders (as he told CNN in 1999), or that he has knowledge implicating others which he will never reveal, or (as told in Peter Matthiessen's "In the Spirit of Crazy Horse (book)", 1983) that he approached and searched the agents but did not execute them. The other rationale holds that the murders (no matter who committed them) occurred during a war-like atmosphere on the reservation in which FBI agents were terrorizing residents in the wake of the Wounded Knee Incident in 1973.
The film "Incident at Oglala" (1992) included the AIM activist Robert Robideau saying the FBI agents had been shot by a 'Mr X'. When Peltier was interviewed about 'Mr X', he said he knew who the man was. In 1995 Dino Butler, in an interview with E.K. Caldwell of "News From Indian Country", said that 'Mr X' had been invented as the murderer in an attempt to achieve Peltier's release. In a 2001 interview with "News From Indian Country", Bernie Lafferty said that she had witnessed Peltier's referring to his murder of one of the agents.
21st-century developments.
Near the end of the Clinton administration in 2000, rumors began circulating that Bill Clinton was considering granting Peltier clemency. Opponents campaigned against that, culminating in a protest outside the White House by about 500 FBI agents and families, and a letter opposing clemency from FBI director Louis Freeh. Clinton did not grant or deny Peltier clemency. In January 2009, President George W. Bush denied Peltier's clemency petition before leaving office.
In 2002, Peltier filed a civil rights lawsuit in the U.S. District Court for the District of Columbia against the FBI, Louis Freeh, and FBI agents who had participated in the campaign against his clemency petition, alleging that they "engaged in a systematic and officially sanctioned campaign of misinformation and disinformation." On March 22, 2004, the suit was dismissed.
Editorial about deaths of agents and Aquash.
In January 2002 in the "News from Indian Country", the publisher Paul DeMain wrote an editorial that an "unnamed delegation" told him, "Peltier was responsible for the close range execution of the [FBI] agents. ..." DeMain described the delegation as "grandfathers and grandmothers, AIM activists, Pipe carriers and others who have carried a heavy unhealthy burden within them that has taken its toll." DeMain said he was told the motive for the execution-style murder of the AIM activist Anna Mae Aquash in December 1975 "allegedly was her knowledge that Leonard Peltier had shot the two agents, as he was convicted." DeMain did not accuse Peltier of participation in the Aquash murder. In 2003 two Native American men were indicted and later convicted for the murder.
On May 1, 2003, Peltier sued DeMain for libel for similar statements about the case published on March 10, 2003, in "News from Indian Country". On May 25, 2004, Peltier withdrew the suit after he and DeMain settled the case. DeMain issued the following statement:
I do not believe that Leonard Peltier received a fair trial in connection with the murders of which he was convicted. Certainly he is entitled to one. Nor do I believe, according to the evidence and testimony I now have, that Mr. Peltier had any involvement in the death of Anna Mae Aquash.
DeMain did not retract his allegations that Peltier was guilty of the murders of the FBI agents and that the motive for Aquash's murder was the fear that she might inform on the activist.
Indictments and trials for the murder of Aquash.
Bruce Ellison, Leonard Peltier's lawyer since the 1970s, invoked his Fifth Amendment rights against self-incrimination and refused to testify at the 2003 federal grand jury hearings on charges against Arlo Looking Cloud and John Graham for the murder of Aquash. Ellison also refused to testify at Looking Cloud's trial in 2004. During the trial, the federal prosecutor named Ellison as a co-conspirator in the Aquash case. Witnesses said that Ellison participated in interrogating Aquash about being an informant on December 11, 1975, shortly before her murder.
In February 2004, Fritz Arlo Looking Cloud, an Oglala Sioux, was tried and convicted for the murder of Aquash. In Looking Cloud's trial, the federal prosecution argued that AIM's suspicion of Aquash stemmed from her having heard Peltier admit to the murders. Darlene "Kamook" Nichols, former wife of the AIM leader Dennis Banks, was a witness for the prosecution. She testified that in late 1975, Peltier told her and a small group of AIM fugitive activists about shooting the FBI agents. At the time all were fleeing law enforcement after the Pine Ridge shootout. The other fugitives included her sister Bernie Nichols, her husband Dennis Banks, and Anna Mae Aquash, among several others. Bernie Nichols-Lafferty testified with a similar account of Peltier's statement.
Earlier in 1975, the AIM member Douglass Durham had been revealed to be an FBI agent and dismissed from the organization. AIM leaders were fearful of infiltration. Other witnesses have testified that, once Aquash was suspected of being an informant, Peltier interrogated her while holding a gun to her head. Peltier and David Hill were said to have Aquash participate in bomb-making so that her fingerprints would be on the bombs. Prosecutors alleged in court documents that the trio planted these bombs at two power plants on the Pine Ridge reservation on Columbus Day 1975.
During the trial, Nichols acknowledged receiving $42,000 from the FBI in connection with her cooperation on the case. She said it was compensation for travel expenses to collect evidence and moving expenses to be farther from her ex-husband Dennis Banks, whom she feared because she had implicated him as a witness. Peltier has claimed that Kamook Nichols committed perjury with her testimony.
On June 26, 2007, the Supreme Court of British Columbia ordered the extradition of John Graham to the United States to stand trial for his alleged role in the murder of Aquash. He was eventually tried by the state of South Dakota in 2010. During his trial, Darlene "Kamook" Ecoffey said Peltier told both her and Aquash that he had killed the FBI agents in 1975. Ecoffey testified under oath, "He (Peltier) held his hand like this," she said, pointing her index finger like a gun, "and he said 'that (expletive) was begging for his life but I shot him anyway.'" Graham was convicted of murder as the gunman who shot Aquash and sentenced to life imprisonment.
Presidential candidate.
Peltier was the candidate for the Peace and Freedom Party in the 2004 Presidential race. While numerous states have laws that prohibit prison inmates convicted of felonies from voting (Maine and Vermont are exceptions), the United States Constitution has no prohibition against felons being elected to Federal offices, including President. The Peace and Freedom Party secured ballot status for Peltier only in California, where his presidential candidacy received 27,607 votes, approximately 0.2% of the vote in that state.
Ruling on FBI documents.
In a February 27, 2006, decision, U.S. District Judge William Skretny ruled that the FBI did not have to release five of 812 documents relating to Peltier and held at their Buffalo field office. He ruled that the particular documents were exempted on the grounds of "national security and FBI agent/informant protection." In his opinion Judge Skretny wrote, "Plaintiff has not established the existence of bad faith or provided any evidence contradicting (the FBI's) claim that the release of these documents would endanger national security or would impair this country's relationship with a foreign government." In response, Michael Kuzma, a member of Peltier's defense team, said, "We're appealing. It's incredible that it took him 254 days to render a decision." Kuzma further said, "The pages we were most intrigued about revolved around a teletype from Buffalo ... a three-page document that seems to indicate that a confidential source was being advised by the FBI not to engage in conduct that would compromise attorney-client privilege." Peltier's supporters have tried to obtain more than 100,000 pages of documents from FBI field offices, claiming that the files should have been turned over at the time of his trial or following a Freedom of Information Act (FOIA) request filed soon after.
On July 20, 2012, a federal judge refused a request by Kuzma to review more than 900 pages of FBI documents related to Frank Blackhorse, who was among the approximately 24 AIM members or supporters the FBI identified as having participated in the fatal shootout on June 26, 1975. Blackhorse was arrested with Peltier but faced no extradition effort.
2007 political controversy.
In 2007, billionaire David Geffen, a Peltier supporter, shifted his financial support from Hillary Clinton's presidential campaign to that of Barack Obama. Geffen said he switched his support because he was disillusioned by Bill Clinton's refusal to pardon Peltier, although he had pardoned Marc Rich.
Beaten in Canaan.
On January 13, 2009, Peltier was severely beaten by fellow inmates at the United States Penitentiary, Canaan, where he had been transferred from USP Lewisburg. He was sent back to Lewisburg, where he remained until the fall of 2011 when he was transferred to a federal penitentiary in Florida. Today Leonard Peltier is housed at Coleman Federal Correctional Complex in Coleman, Florida.
In popular culture.
Music.
Silent Bear releases "Freedom For Leonard Peltier ( Bring Him Home)" featuring Pete Seeger on his CD, The Green Lion, July 2014

</doc>
<doc id="18221" url="http://en.wikipedia.org/wiki?curid=18221" title="LambdaMOO">
LambdaMOO

LambdaMOO is an online community of the variety called a MOO. It is the oldest MOO today.
"LambdaMOO" was founded in late 1990 or early 1991 by Pavel Curtis at Xerox PARC. Now hosted in the state of Washington, it is operated and administered entirely on a volunteer basis. Guests are allowed, and membership is free to anyone with an e-mail address.
"LambdaMOO" gained some notoriety when Julian Dibbell wrote a book called "My Tiny Life" describing his experiences there. Over its history, "LambdaMOO" has been highly influential in the examination of virtual-world social issues.
History.
LambdaMOO has its roots in the 1978–1980 work by Roy Trubshaw and Richard Bartle to create and expand the concept of Multi-User Dungeon (MUD) – virtual communities. Around 1987–1988, the expansion of the global internet allowed more users to experience the MUD. Pavel Curtis at Xerox Parc noted that they were "almost exclusively for recreational purposes." Curtis determined to explore whether the MUD could be non-recreational. He developed "LambdaMOO" software to run on the LambdaMOO server, which implements the MOO programming language. This software was subsequently made available to the public. Several starter databases, known as cores, are available for MOOs; "LambdaMOO" itself uses the LambdaCore database. The "Lambda" name is from Curtis's own username on earlier MUD systems.
LambdaMOO can refer to the software, the server, or the community of users.
Joining.
To join LambdaMOO requires a working email. Once you submit your email, you must wait a few days before you receive a randomized character name and password. In the meantime, you can play LambdaMOO as a guest.
Once you get your randomized character name you are able to log into LambdaMOO and change your character name and password. After you choose your unique character name you are able to select your gender as well as editing your characters physical description and personality. LambdaMOO allows you a lot of freedom when it comes to customizing your character. You are offered a wide range of genders and are able to write anything you want in a paragraph about yourself. After you customize these settings you are equipped to begin exploring LambdaMOO.
Geography.
"LambdaMOO" central geography was based on Pavel Curtis's California home. New players and guests traditionally connected in "The Coat Closet", but a second area, "The Linen Closet" (specially programmed as a silent area) was later added as an alternative connection point. The coat closet opens onto the center of the house in The Living Room, a common hangout and place for conversation; its fixtures include a fireplace (where things can be roasted), The Living Room Couch (which periodically causes players' objects to 'fall through' to underneath the couch), and a pet Cockatoo who repeats overheard phrases (which is often found with its beak gagged). From time to time the Cockatoo is replaced with a more seasonal creature: a Turkey near Thanksgiving, a Raven near Halloween, et cetera.
To the north of the Living Room is the Entrance Hall, the Front Yard, and a limited residential area along LambdaStreet. There is an extensive subterranean complex located down the manhole, including a sewage system.
Players walking to the far west along LambdaStreet may be given the option to 'jump off the end of the world', which disables access to their account for three months. To the south of the Living Room is a pool deck, a hot tub, and some of the extensive grounds of the mansion, featuring gardens, hot air balloon landing pads, open fields, fishing holes, and the like.
To the northwest of the living room are the laundry room, garage, dining room, smoking room, drawing room, housekeeper's quarters, and kitchen; a popular command allows players in the living room to push others into the kitchen and ask them to "fetch me a cup of tea"; since players can prevent themselves from being moved in such a fashion, this command is more often used on new users, who may have difficulty finding their way back to the Living Room. (There is direct access to the kitchen leading northwest from the living room, but as with the actual house you must head north, east, and then south from the Kitchen to return.)
To the east of the entry hall, hallways provide access to some individual rooms, the Linen Closet, and to the eastern wing of the house. In the eastern wing can be found the Library of online books, the Museum of generic objects (which account-holders may create instances of), and an extensive area for the "LambdaMOO" RPG.
Since the creation of the original LambdaMOO map, many users have expanded the MOO by making additional rooms with the command "@dig."
Politics.
While most MOOs are run by administrative fiat, in summer of 1993 "LambdaMOO" implemented a petition/ballot mechanism, allowing the community to propose and vote on new policies and other administrative actions. A petition may be created by anyone eligible to participate in politics (those who have maintained accounts at the MOO for at least 30 days), can be signed by other players, and may then be submitted for administrative 'vetting'. Once vetted, the petition has a limited time to collect enough signatures to become valid and be made into a ballot. Ballots are subsequently voted on; those with a 66% approval rating are passed and will be implemented. This system suffered quite a lot of evolution and eventually passed into a state where wizards took back the power they'd passed into the hands of the people, but still maintain the ballot system as a way for the community to express its opinions.
Demographics.
While the population of "LambdaMOO" once numbered close to 10,000 with over 300 actively connected at any time, these days it is rare to see more than a few dozen actively participating connected players at one time.
As of November 13, 2005, "LambdaMOO" had 10 official wizards (administrators) and approximately 2,900 general users. Of these, approximately 1410 players reported themselves as male, and 916 as female; the remaining players either stayed with the default neuter gender, or deliberately chose another. ("LambdaMOO" supports custom designations of gender, and comes with the following presets: neuter, male, female, either, Spivak, splat, plural, egotistical, royal, and 2nd-person).

</doc>
<doc id="18223" url="http://en.wikipedia.org/wiki?curid=18223" title="Lorica segmentata">
Lorica segmentata

The lorica segmentata ("segmented cuirass") is a type of personal armour used by soldiers of the Roman Empire, consisting of metal strips ("girth hoops" fashioned into circular bands), fastened to internal leather straps. The Latin name was first used in the 16th century (the ancient form is unknown, although it is possible that the Romans referred to the armour as "lorica laminata"-("laminated cuirass")).
The plates of lorica segmentata armour were soft iron inside and (some at least) were mild steel on the outside, making the plates hardened against damage without becoming brittle. This was a deliberate act, called case hardening, and is carried out by enriching the surface iron with carbon from organic materials packed tightly around the piece which is then heated in a forge. 
The strips were arranged horizontally on the body, overlapping downwards, and they surrounded the torso in two halves, being fastened at the front and back. The upper body and shoulders were protected by additional strips ("shoulder guards") and breast- and backplates. The form of the armour allowed it to be stored very compactly, since it was possible to separate it into four sections each of which would collapse on itself into a compact mass. The fitments that closed the various plate sections together (buckles, lobate hinges, hinged straps, tie-hooks, tie-rings, etc.) were made of brass. In later variants dating from around 75–80 A.D., the fastenings of the armour were simplified. Bronze hinges were removed in favour of simple rivets, belt fastenings utilised small hooks, and the lowest two girdle plates were replaced by one broad plate.
History.
During the time of their use, this style of armour evolved and changed, the currently recognised types being the Kalkriese, Corbridge and Newstead types, named after their places of discovery. There was, however, a considerable overlap between these types in use and the Corbridge and Newstead types are often found at the same site (e.g. at Caerleon in Wales, Carnuntum in Austria, Carlisle in England and León in Spain). It is possible that there was a fourth type, covering the body with segmented armour joined to scale shoulder defences. However, this is only known from one badly damaged statue originating at Alba Iulia in Romania. The currently accepted range for the use of the armour is from about 9 B.C. (Dangstetten) to the late 3rd century A.D. (León). Its use was geographically widespread but mail may have been more common at all times.
Uses.
The question as to precisely who used the armour is debated. There is a clear difference in armour between the two corps shown on Trajan's Column. This is a monument erected in 113 in Rome to commemorate the conquest of Dacia by Emperor Trajan (ruled 98–117): its bas-reliefs are a key source for Roman military equipment. "Auxilia" are generally shown wearing mail ("lorica hamata") cuirasses and carrying oval shields. Legionaries are uniformly depicted wearing "lorica segmentata" and with curved rectangular shields. On this basis, it has been supposed that lorica segmentata was used by legionaries only. However, some historians consider Trajan's Column to be inaccurate as a historical source due to its inaccurate and stylised portrayal of Roman armour: "...it is probably safest to interpret the Column reliefs as ‘impressions’, rather than accurate representations." 
The view that "auxilia" were light troops originates from Vegetius' comment that ""auxilia" are always joined as light troops with the legions in the line". It is true that some specialist units in the "auxilia", such as Syrian archers and Numidian cavalry wore light armour (or none). But they were a small minority of the "auxilia". Most auxiliary "cohortes" contained heavy infantry similar to legionaries.
However, on another Trajanic monument (the Adamclisi "Tropaeum") the "lorica segmentata" does not appear at all, and legionaries and "auxilia" alike are depicted wearing either mail or scales ("lorica squamata"). Some experts are of the opinion that the Adamclisi monument is a more accurate portrayal of normality, with the "segmentata" used rarely, maybe only for set-piece battles and parades. This viewpoint considers the figures in Trajan's Column to be highly stereotyped, in order to distinguish clearly between different types of troops. In any event, both corps were equipped with the same weapons: "gladius" (a close-combat stabbing sword) and javelins, although the type of javelin known as "pilum" seems to have been provided to legionaries only. Goldsworthy points out that the equipment of both corps were roughly equal in weight.
In recent years archaeology has found fittings of "lorica segmentata" in many fort sites that are thought to have been garrisoned by only auxiliary troops, "i.e.", where the legions were not based. If the legions were, indeed, broken up and distributed around all these small bases, then it implies a tactical use of the legions that has not previously been considered. Hitherto, the legions were regarded as shock troops employed only "en masse" and not broken up into detachments. M.C. Bishop, however, has argued that we need to examine the way in which the various troop types were armed and deduce from this what their battle roles were, rather than trying to consider who-wore-what. Succinctly put, the legions were armed and trained for close-order combat while the auxiliary forces, just as numerous, were more accustomed to open order fighting, although they could be employed as the legions were (e.g. at Mons Graupius) if circumstances demanded this.
Last uses and disappearance.
During the 3rd century, all "peregrini" were granted Roman citizenship, and therefore legionaries lost their social superiority. The "lorica segmentata" eventually disappeared from Roman use, most likely due to its high cost and difficult maintenance despite its good qualities, although it appears to have still been in use into the early 4th century, being depicted in the Arch of Constantine erected in 315 during the reign of Constantine I to commemorate his military achievements. (However, it has been argued that these depictions are from an earlier monument by Marcus Aurelius, from which Constantine incorporated portions into his Arch.) Recently there has been evidence of a "lorica segmentata" found in Spain, dating from the third century. What is more surprising is that it was found in an area where, as far as we know, only "limitanei" operated. ("Limitanei" are frontier troops, from Latin limes, boundary or frontier, while the mobile field armies are the "comitatenses"; it used to be thought they were inferior second-line units or militia; their quality did not diminish until the mid-5th century.)
Popular culture.
The Lorica Segmentata has come to be viewed as synonymous with the Roman legions in pop culture depictions. This tendency even extends to time periods where its employment is so early or late as to be anachronistic.

</doc>
<doc id="18224" url="http://en.wikipedia.org/wiki?curid=18224" title="Known Space">
Known Space

Known Space is the fictional setting of about a dozen science fiction novels and several collections of short stories written by Larry Niven. It has also become a shared universe in the spin-off "Man-Kzin Wars" anthologies. ISFDB catalogs all works set in the fictional universe that includes Known Space under the series name Tales of Known Space, which was the title of a 1975 collection of Niven's short stories. The first-published work in the series, which was Niven's first published piece was "The Coldest Place", in the December 1964 issue of ""If" magazine, edited by Frederik Pohl. This was the first-published work in the 1975 collection.
The stories span approximately one thousand years of future history, from the first human explorations of the Solar System to the colonization of dozens of nearby systems. Late in the series, Known Space is an irregularly shaped "bubble" about 60 light-years across.
Within the Tales of Known Space, the epithet "Known Space" refers to a relatively small region in the Milky Way galaxy, one centered on Earth. In the future that the series depicts, spanning roughly the third millennium, humans have explored this region and colonized many of its worlds. Contact has been made with other species, such as the two-headed Pierson's Puppeteers and the aggressive felinoid Kzinti. Stories in the Known Space series include events and places outside of the region called "Known Space" such as the Ringworld, the Pierson's Puppeteers' Fleet of Worlds and the Pak homeworld.
The Tales were originally conceived as two separate series, the "Belter" stories set roughly from 2000 to 2350 CE and the "Neutron Star"/"Ringworld" stories set in 2651 CE and later. The earlier, Belter period features solar-system colonization and slower-than-light travel with fusion-powered and Bussard ramjet ships. The later, Neutron Star period features faster-than-light ships using "hyperdrive". Niven implicitly joined the two settings as a single fictional universe in the short story "A Relic of the Empire" ("If", December 1966), by using background elements of the Slaver civilization from the "Belter" series as a plot element in the faster-than-light setting. In the late 1980s—having written almost no Tales of Known Space in more than a decade—Niven opened the 300-year gap in the Known Space timeline as a shared universe, and the stories of the "Man-Kzin Wars" volumes fill in that history, bridging the two settings.
Overview.
Species.
In the process of exploring space, humankind encounters several intelligent alien species, including the following (in alphabetical order):
Also figuring in some stories are dolphins and other intelligent cetaceans, and various offshoots of "Homo sapiens" including the associate lineage of the hominids of the Ringworld. Most life in Known Space shares similar biochemistries, since they evolved from the Thrintun practice of seeding barren worlds with food yeast which they used to feed their slaves. Over a billion years, the Thrintun food yeast evolved into the different life forms in Known Space.
Locations.
One aspect of the "Known Space" universe is that most of the early human colonies are on planets suboptimal for "Homo sapiens". During the first phase of human interstellar colonization (i.e. before humanity acquired FTL), simple robotic probes were sent to nearby stars to assess their planets for habitation. The programming of these probes was flawed: they sent back a "good for colonization" message if they found a habitable "point", rather than a habitable "planet". Sleeper ships containing human colonists were sent to the indicated star systems. Too often, those colonists had to make the best of a bad situation.
Technology.
The series features a number of "superscience" inventions which figure as plot devices. Stories earlier in the timeline feature technology such as Bussard ramjets, Drouds (wires capable of directly stimulating the pleasure centers of the brain) and explore how organ transplantation technology enables the new crime of "organlegging" (as well as the general sociological effects of widespread transplant technology), while later stories feature hyperdrive, invulnerable starship hulls, stasis fields, molecular monofilaments, transfer booths (teleporters used only on planetary surfaces), the lifespan-extending drug boosterspice, and the tasp which is an extension of the wirehead development which works without direct contact.
The impact of inventions and technology on society is a recurring, if not central theme in Niven's work: for example, addiction to electric brain stimulation resulting in "wireheads", or the secondary and tertiary effects of an invention such as teleportation on social behavior, problems, and mores.
The milieu can be viewed as representing the last gasp of Campbell-era science fiction, as the iconoclastic, counterculture influences of "new wave" science fiction of the sixties play no part in most of the stories. However, there are notable exceptions in the "Gil the ARM" stories; and "Jigsaw Man" first appeared in Harlan Ellison's landmark "new wave" anthology, "Dangerous Visions".
Boosterspice.
Boosterspice is a compound that increases the longevity and reverses aging of human beings. With the use of boosterspice, humans can easily live hundreds of years and, theoretically, indefinitely.
Developed by the Institute of Knowledge on Jinx, it is said to be made from genetically engineered ragweed (although early stories have it ingested in the form of edible seeds). In "Ringworld's Children", it is suggested boosterspice may actually be adapted from Tree-of-Life, without the symbiotic virus that enabled hominids to metamorphose from Pak Breeder stage to Pak Protector stage (mutated Pak breeders were the ancestors of both "Homo sapiens" and the hominids of the Ringworld).
On the Ringworld, there is an analogous (and apparently more potent) compound developed from Tree-of-Life, but they are mutually incompatible; in "The Ringworld Engineers", Louis Wu learns that the character Halrloprillalar died when in ARM custody after leaving the Ringworld, as a result of having taken boosterspice after having used the Ringworld equivalent. Boosterspice only works on "Homo sapiens", whereas the Tree-of-Life compound will work on any hominid descended from the Pak.
Hyperdrive.
Faster Than Light (FTL) propulsion, or hyperdrive, was obtained from the Outsiders at the end of the First Man-Kzin War. In addition to winning the war for humanity, it allowed the re-integration of all the human colonies, which were previously separated by distance. Standard hyperdrive covers a distance of one light-year every three days (121.75 x c). A more advanced Quantum II Hyperdrive introduced later is able to cover the same distance in one and a quarter minutes (420,768 x c).
In Niven's first novel, "World of Ptavvs", the hyperdrive used by the Thrintun required a ship to be going faster than 93% of the speed of light. However, this is the only time that Hyperdrive is described this way.
In the vast majority of "Known Space" material, hyperdrive requires that a ship be outside a star's gravity well to use. Ships which activate hyperdrive close to a star are likely to disappear without a trace. This effect is regarded as a limitation based on the laws of physics. In Niven's novel "Ringworld's Children" the Ringworld itself is converted into a gigantic Quantum II hyperdrive and launched into hyperspace while within its star's gravity well. "Ringworld's Children" reveals that there is life in hyperspace around gravity wells and that hyperspace predators eat spaceships which appear in hyperspace close to large masses, thus explaining why a structure as large as the Ringworld can safely engage the hyperdrive in a star's gravity well.
One phenomenon travellers in hyperspace can experience is the so-called 'blind spot' should they look through a porthole or camera screen, giving the impression that the walls around the porthole or sides of the camera view screen are expanding to 'cover up the outside'. The phenomenon is the result of hyperspace being so fundamentally different from 'normal/Einstein' space that a traveller's senses can not truly comprehend it, and instead the observer 'sees' a form of nothingness that can be hypnotic and dangerous.
Staring too long into the 'blind' spot can be insanity inducing, so as a precaution all view ports on ships are blinded when a ship enters hyperspace.
Invulnerable Hulls.
The Puppeteer firm, General Products, produces an invulnerable starship hull, known simply as a General Products Hull. The hulls are impervious to any type of matter or energy, with the exception of antimatter (which destroys the hull), gravitation, and visible light (which pass through the hull). While invulnerable themselves, this is no guarantee that the contents are likewise protected. For example, though a high speed impact with the surface of a planet or star may cause no harm to the hull, the occupants will be crushed if they are not protected by additional measures such as a stasis field or a gravity compensating field.
In "Fleet of Worlds", the characters tour a General Products factory and receive clues that allow them to destroy a General Products hull from the inside using only a high-powered interstellar communications laser. In "Juggler of Worlds", the Puppeteers, attempting to surmise how this was done without antimatter, identify another technique which can be used to destroy the otherwise invulnerable hulls, one which does suggest some potential defense options.
Organ Transplantation.
On Earth in the mid 21st century it became possible to transplant any organ from any person to another, with the exception of brain and central nervous system tissue. Individuals were categorized according to their so-called "rejection spectrum" which allowed doctors to counter any immune system responses to the new organs, allowing transplants to "take" for life. It also enabled the crime of "organlegging" which lasted well into the 24th century.
Stasis Fields.
A Slaver stasis field creates a bubble of space/time disconnected from the entropy gradient of the rest of the universe. Time slows effectively to a stop for an object in stasis, at a ratio of some billions of years outside to a second inside. An object in stasis is invulnerable to anything occurring outside the field, as well as being preserved indefinitely. A stasis field may be recognized by its perfectly reflecting surface, so perfect in fact that it reflects 100% of all radiation and particles, including neutrinos. However one stasis field cannot exist inside another. This is used in World of Ptavvs where humans develop a stasis field technology and realize that a mirrored artifact known as the "Silver Statue" must be actually an alien in a stasis field. They place it with a human envoy, who is a telepath, and envelop both in field. By doing this, they unleash the last living member of the Slaver species on the world.
Stepping Disks.
Stepping Disks are a fictional teleportation technology. They were invented by the Pierson's Puppeteers, and their existence is not generally known to other races until the events of "The Ringworld Engineers".
The stepping disks are an outgrowth and improvement of the transfer booth technology used by humans and other Known Space races. Unlike the booths, the disks do not require an enclosed chamber, and somehow can differentiate between solid masses and air, for example. They also have a far greater range than transfer booths, extending several Astronomical Units.
Several limitations to stepping disks are mentioned in the Ringworld novels. If there is a difference in velocity between two disks, any matter transferred between them must be accelerated by the disk accordingly. If there is not enough energy to do so, the transfer cannot take place. This becomes a problem with disks that are a significant distance apart on the Ringworld surface, as they will have different velocities: same speed, different direction.
Transfer Booths.
Transfer Booths are an inexpensive form of teleportation. They are similar in appearance to an old style telephone booth: one enters, "dials" one's desired destination, and is immediately deposited in a corresponding booth at the destination. They are inexpensive: a trip anywhere on Earth costs only a "tenth-star" (presumably equivalent to a dime). Introduced by one of Gregory Pelton's ancestors, apparently bought from and based on Puppeteer technology.
Paranormal abilities.
Some individuals in the stories display limited paranormal or "psionic" abilities. Gil Hamilton can move objects with his mind using his phantom arm, which he gained after losing an arm in an asteroid mining accident. When he finally had the arm replaced from an organ bank on Earth, the ability persisted. "Plateau Eyes" (introduced in "A Gift From Earth") is an ability to hide in plain sight, by causing others not to notice you. Population control is tight on Earth, but these abilities can gain the possessor a license to have more children. The Pierson's Puppeteers engineer a lottery for child licenses on Earth to increase the occurrence of "Luck", which they think is a paranormal ability humans have that has enabled them to defeat races such as the Kzinti. In "Ringworld", the character Teela Brown is said to be the ultimate expression of this ability.
Organizations.
ARM.
The ARM is the police force of the United Nations. ARM originated as an acronym for "Amalgamation of Regional Militia", though this is not a term in current usage by the time of the "Known Space" novels. An agent of the ARM, Gil Hamilton, is the protagonist of Niven's sci-fi detective stories, a series-within-a-series gathered in the collection "Flatlander" (Confusingly, "Flatlander" is also the name of an unrelated "Known Space" story.)
Their basic function is to enforce mandatory birth control on overcrowded Earth, and restrict research which might lead to dangerous weapons. In short, the ARM hunts down women who have illegal pregnancies and suppresses all new technologies. They also hunt organleggers, especially in the era of the "organ bank problem". Among the many technologies they control and outlaw are all trained forms of armed and unarmed combat. By the 25th century, ARM agents were kept in an artificially induced state of paranoid schizophrenia to enhance their usefulness as law enforcement officials, which led to them sometimes being referred to as "Schizes". Agents with natural tendencies toward paranoia were medicated into docility during their off duty hours, through the aforementioned science of psychistry (see "Madness Has Its Place" and "Juggler of Worlds").
Their jurisdiction is limited to the Earth-Moon system; other human colonies have their own militia. Nevertheless, in many "Known Space" stories, ARM agents operate or exert influence in other human star systems through the "Bureau of Alien Affairs" (see "In the Hall of the Mountain King", "Procrustes", "The Borderland of Sol", and "Neutron Star"). These interventions begin following the Man-Kzin Wars and the introduction of hyperdrive, presumably as part of a general re-integration of human societies.
Stories in Known Space.
The Tales of Known Space were first published primarily as short stories or serials in science fiction magazines. Generally the short fiction was subsequently released in one or more collections and the serial novels as books. Some of the shorter novels (novellas) published in magazines were expanded as, or incorporated in, book-length novels. Due to the large number of stories, it is particularly difficult for a completionist fan to read every story in the series. There are also two or three short stories which share common themes and some background elements with "Known Space" stories, but which are not considered a part of the "Known Space" universe: "One Face" (1965) and "Bordered in Black" (1966) —both in the 1979 collection "Convergent Series"— and possibly "The Color of Sunfire", published online and listed here.
In the "Known Space" stories, Niven had created a number of technological devices (GP hull, stasis field, Ringworld material) which, combined with the "Teela Brown gene", made it very difficult to construct engaging stories beyond a certain date — the combination of factors made it tricky to produce any kind of creditable threat/problem without complex contrivances. Niven demonstrated this, to his own satisfaction, with "Safe at Any Speed" (1967). He used the setting for much less short fiction after 1968 and much less for novels after two published in 1980. Late in that decade, however, Niven invited other authors to participate in a series of shared-universe novels, with the Man-Kzin Wars as their setting. The first volume was published in 1988.
"Ringworld" (1970) won the annual Nebula, Hugo, and Locus best novel awards.
"Protector" (1973) and "The Ringworld Engineers" (1980) were nominated for the Hugo and Locus Awards.
Playground.
Niven has described his fiction as "playground equipment", encouraging fans to speculate and extrapolate on the events described. Debates have been made, for example, on who built the Ringworld (Pak Protectors and the Outsiders being the traditional favorites, but see "Ringworld's Children" for a possibly definitive answer), and what happened to the Tnuctipun. However, Niven also states that this is "not" an invitation to violate his copyrights, so fans should try to avoid publishing works that are too obviously based in the "Known Space" universe without Niven's given permission.
Niven was also reported to have said that "Known Space should be seen as a possible future history told by people that may or may not have all their facts right."
The author also published an "outline" for a story which would "destroy" the Known Space Series (or more precisely, reveal much of the Known Space background to be an in-universe hoax), in an article entitled . Although the article is written as though Niven intended to write the story, he later wrote that the article was only an elaborate joke, and he never intended to write such a novel. The article itself notes that the outline was made obsolete by the publication of "Ringworld". "Down in Flames" was a result of a conversation between Norman Spinrad and Niven in 1968, but at the time of its first publication in 1977 some of the concepts were invalidated by Niven's writings between '68 and '77. (A further edited version of the outline was published in "N-Space" in 1990.)

</doc>
<doc id="18225" url="http://en.wikipedia.org/wiki?curid=18225" title="LeRoy Homer, Jr.">
LeRoy Homer, Jr.

LeRoy Wilton Homer, Jr. (August 27, 1965 – September 11, 2001) was the First Officer of United Airlines Flight 93, which was hijacked as part of the September 11 attacks in 2001, and crashed into a field near Shanksville, Pennsylvania, killing all 37 passengers and seven crew members.
Biography.
Homer, son of a West German and an American soldier who was stationed in West Germany, grew up on Long Island in New York where he always dreamed of flying. As a child, he assembled model airplanes, collected aviation memorabilia and read books on aviation. He was 15 years old when he started flight instruction in a Cessna 152. Working part-time jobs after school to pay for flying lessons, he completed his first solo trip at the age of 16 and obtained his private pilot's certificate in 1983.
Homer was graduated from Ss. Cyril and Methodius School in 1979 and St. John the Baptist Diocesan High School in 1983.
He entered the United States Air Force Academy as a member of the class of 1987. As an upperclassman, he was a member of Cadet Squadron 31. He graduated on May 27, 1987, and was commissioned as a second lieutenant in the U.S. Air Force.
After completing his USAF pilot training in 1988, he was assigned to McGuire Air Force Base in New Jersey, flying a Lockheed C-141 Starlifter. While on active duty, he served in the Gulf War and later supported operations in Somalia. He received many commendations, awards and medals during his military career. In 1993, he was named the Twenty-First Air Force "Aircrew Instructor of the Year". Homer achieved the rank of captain before his honorable discharge from active duty in 1995 and his acceptance of a Reserve Commission in order to continue his career as an Air Force officer.
Homer continued his military career as a member of the U.S. Air Force Reserve, initially as a C-141 instructor pilot with the 356th Airlift Squadron at Wright-Patterson Air Force Base in Ohio, then subsequently as an Academy Liaison Officer, recruiting potential candidates for both the Air Force Academy and the Air Force Reserve Officer Training Corps. During his time in the Air Force Reserve, he achieved the rank of major.
He continued his flying career by joining United Airlines in May 1995. His first assignment was Second Officer on the Boeing 727. He then upgraded to First Officer on the Boeing 757/Boeing 767 in 1996, where he remained until September 11, 2001.
He married Melodie on May 24, 1998, and his first child, Laurel, was born in late November 2000. They resided together in Marlton, New Jersey.
September 11 attacks.
On September 11, 2001, Homer was flying with Captain Jason M. Dahl on United Airlines Flight 93 from Newark, New Jersey, to San Francisco. The plane was hijacked by four al-Qaeda terrorists as part of the September 11 attacks. He was removed from the cockpit, while Jason Dahl remained until he could no longer be heard on the cockpit voice recorder. While struggling for control of the plane after the hijackers had breached the cockpit, Homer managed to transmit to the ground twice, screaming "Mayday! Mayday! Mayday! Get out of here!"(Cleveland central ATC audio tramission). After learning of the earlier crashes at the World Trade Center and the Pentagon, the crew and passengers tried to foil the hijacking and reclaim the aircraft. During this struggle, the hijackers were not able to disengage the autopilot. Realizing they needed help, they were heard to say "bring back the pilot" (CVR transcripts). However the uprising of crew and passengers and knowing they would not make it to their intended target, which was the US Capitol,they instead chose to crash the plane into a field near Shanksville, Pennsylvania.
Homer received many awards and citations posthumously, including honorary membership in the historic Tuskegee Airmen; the Congress of Racial Equality's Dr. Martin Luther King, Jr. Award; the Southern Christian Leadership Conference Drum Major for Justice Award; and the Westchester County Trailblazer Award.
He is survived by his wife, Melodie, his daughter, Laurel, along with his mother, seven sisters, his brother, and other family.
At the National 9/11 Memorial, Homer, Jr. is memorialized at the South Pool, on Panel S-67, along with other crew and passengers on Flight 93.

</doc>
<doc id="18227" url="http://en.wikipedia.org/wiki?curid=18227" title="LGB">
LGB

LGB may refer to:

</doc>
<doc id="18230" url="http://en.wikipedia.org/wiki?curid=18230" title="La Jetée">
La Jetée

La Jetée (]) ("The Jetty," here referring to an outdoor viewing pier at an airport), is a 1962 French science fiction featurette by Chris Marker. Constructed almost entirely from still photos, it tells the story of a post-nuclear war experiment in time travel. It is 28 mins long, black and white. It won the Prix Jean Vigo for short film.
The 1995 science fiction film "12 Monkeys" was inspired by, and borrows several concepts directly from, "La Jetée".
Plot.
A man (Davos Hanich) is a prisoner in the aftermath of World War III in post-apocalyptic Paris where survivors live underground in the "Palais de Chaillot" galleries. Scientists research time travel, hoping to send test subjects to different time periods "to call past and future to the rescue of the present". They have difficulty finding subjects who can mentally withstand the shock of time travel. The scientists eventually settle upon the prisoner; his key to the past is a vague but obsessive memory from his pre-war childhood of a woman (Hélène Chatelain) he had seen on the observation platform ("the jetty") at Orly Airport shortly before witnessing a startling incident there. He had not understood exactly what happened but knew he had seen a man die.
After several attempts, he reaches the pre-war period. He meets the woman from his memory, and they develop a romantic relationship. After his successful passages to the past, the experimenters attempt to send him into the far future. In a brief meeting with the technologically advanced people of the future, he is given a power unit sufficient to regenerate his own destroyed society.
Upon his return, with his mission accomplished, he discerns that he is to be executed by his jailers. He is contacted by the people of the future, who offer to help him escape to their time permanently; but he asks instead to be returned to the pre-war time of his childhood, hoping to find the woman again. He is returned and does find her, on the jetty at the airport. However, as he rushes to her, he notices an agent of his jailers who has followed him and realizes the agent is about to kill him. In his final moments, he comes to understand that the incident he witnessed as a child, which has haunted him ever since, was his own death.
Production.
"La Jetée" is constructed almost entirely from optically printed photographs playing out as a photomontage of varying rhythm. It contains only one brief shot (of the woman mentioned above sleeping and suddenly waking up) originating on a motion-picture camera, this due to the fact that Marker could only afford to hire one for an afternoon. The stills were taken with a Pentax Spotmatic and the motion-picture segment was shot with a 35mm Arriflex. The film has no dialogue aside from small sections of muttering in German and people talking in an airport terminal. The story is told by a voice-over narrator. The scene in which the hero and the woman look at a cut-away trunk of a tree is a reference to Alfred Hitchcock's 1958 film "Vertigo" which Marker also references in his 1983 film "Sans soleil".
Influence and legacy.
Terry Gilliam's "12 Monkeys" (1995) was inspired by and takes several concepts directly from "La Jetée" (acknowledging this debt in the opening credits). In 2015, the SyFy Channel released a television show also titled "12 Monkeys" that is "Based on "La Jetée"" in the closing credits. In 1996, Zone Books released a book which reproduced the film's original images along with the script in both English and French; re-released in 2008, it is now out of print. The 2003 short film, "La puppé", is both an homage to and a parody of "La Jetée". The video for Sigue Sigue Sputnik's 1989 single "Dancerama" is also an homage to "La Jetée". The film is one of the influences in the video for David Bowie's "Jump They Say" (1993). The music video for Isis's "In Fiction", from 2004's "Panopticon", drew comparisons with "La Jetée". The song "Last Night at the Jetty" by Panda Bear has lyrics inspired by the themes of the film.
"The Time Traveler's Wife" (2009) also takes inspiration in the relationship between the woman and the time traveller. In 2010, "Time" ranked "La Jetée" first in its list of "Top 10 time-travel movies". Kode9 (music, script) in collaboration with Ms. Haptic (narration, script), Marcel Weber (aka MFO) (images, script) and Lucy Benson (images, script) created an homage to "La Jetée" in 2011, for the Unsound Festival. The plot of the homage centers around the "woman" instead of the "man" and is a "reimagining" rather than a "remix" in that it features a completely new, original script that further develops the narrative whilst remaining true to the original plot. The two stories function in harmony with one another. The images and music of "Her Ghost" are almost exclusively sourced from the original film, however they are significantly reworked so as to create an original piece. A live performance of "Her Ghost" was part of the at Centre Pompidou in Paris 2013. In 2012, in correspondence with the Sight & Sound Poll, the British Film Institute deemed "La Jetée" as the 50th greatest film of all time.
Home media release.
In Region 2, the film is available with English subtitles in the "La Jetée/Sans soleil" digipack released by Arte Video. In Region 1, the Criterion Collection has released a "La Jetée/Sans soleil" combination DVD / Blu-ray, which features the option of hearing the English or French narration.

</doc>
<doc id="18232" url="http://en.wikipedia.org/wiki?curid=18232" title="Little penguin">
Little penguin

The little penguin ("Eudyptula minor") is the smallest species of penguin. It grows to an average of 33 cm in height and 43 cm in length, though specific measurements vary by subspecies. It is found on the coastlines of southern Australia and New Zealand, with possible records from Chile. In Australia, they are often called fairy penguins. In New Zealand, they are more commonly known as little blue penguins or blue penguins, owing to their slate-blue plumage. They are also known by their Māori name: kororā.
Taxonomy.
The little penguin was first described by German naturalist Johann Reinhold Forster in 1781. There are several subspecies but a precise classification of these is still a matter of dispute. The holotypes of the subspecies "Eudyptula minor variabilis" and "Eudyptula minor chathamensis" are in the collection of the Museum of New Zealand Te Papa Tongarewa. The white-flippered penguin is sometimes considered a subspecies, sometimes a distinct species, and sometimes a morph. As the Australian and Otago (southeastern coast of South Island) little penguins may be a distinct species to which the specific name "minor" would apply, the white-flippered birds indeed belong to a distinct species, although not exactly as originally assumed.
Mitochondrial and nuclear DNA evidence suggests the split between "Eudyptula" and "Spheniscus" occurred around 25 million years ago, with the ancestors of the white-flippered and little penguins diverging about 2.7 million years ago.
Description.
Like those of all penguins, the little penguin's wings have developed into flippers used for swimming. The little penguin typically grows to between 30 and 33 cm (12 to 13 inches) tall and usually weighs about 1.5 kilogram on average (3.3 pounds). The head and upperparts are blue in colour, with slate-grey ear coverts fading to white underneath, from the chin to the belly. The flippers are blue. The dark grey-black beak is 3–4 cm long, the irises pale silvery- or bluish-grey or hazel, and the feet pink above with black soles and webbing. An immature individual will have a shorter bill and lighter upperparts.
Like most seabirds, they have a long lifespan. The average for the species is 6.5 years, but flipper ringing experiments show in very exceptional cases up to 25 years in captivity.
Distribution and habitat.
The little penguin breeds along the entire coastline of New Zealand, the Chatham Islands, and southern Australia (including roughly 20,000 pairs on Babel Island). Australian colonies exist in New South Wales, Victoria, Tasmania, South Australia and Western Australia. Little penguins have also been reported from Chile (where they are known as Pingüino pequeño or Pingüino azul) (Isla Chañaral 1996, Playa de Santo Domingo, San Antonio, 16 March 1997) and South Africa, but it is unclear whether these birds were vagrants. As new colonies continue to be discovered, rough estimates of the world population are around 350,000-600,000 animals.
New Zealand.
Overall, little penguin populations in New Zealand have been decreasing. Some colonies have gone extinct and others continue to be at risk. Some new colonies have been established in urban areas. The species is not considered endangered in New Zealand, with the exception of the white-flippered subspecies found only on Banks Peninsula and nearby Motunau Island. Since the 1960s, the mainland population has declined by 60-70%; though there has been a small increase on Motunau Island.
Australia.
Australian little penguin colonies primarily exist on offshore islands where they are protected from feral terrestrial predators and human disturbance. Colonies are found from Port Stephens in northern New South Wales around the southern coast to Fremantle, Western Australia.
New South Wales.
An endangered population of little penguins exists at Manly, North Sydney Harbor. The population is protected under the NSW Threatened Species Conservation Act 1995 and has been managed in accordance with a Recovery Plan since the year 2000. The population once numbered in the hundreds, but has decreased to around 60 pairs of birds. The decline is believed to be mainly due to loss of suitable habitat, attacks by foxes and dogs and disturbance at nesting sites.
The largest colony in New South Wales is on Montague Island. Up to 8000 breeding pairs are known to nest there each year.
Jervis Bay Territory.
A population of approximately 5,000 breeding pairs exists on Bowen Island. The colony has increased from 500 pairs in 1979 and 1500 pairs in 1985. During this time, the island was privately leased. The island was vacated in 1986 and is currently controlled by the federal government.
South Australia.
In South Australia, many little penguin colony declines have been identified across the state. In some cases, colonies have declined to extinction (including the Neptune Islands, West Island, Wright Island, Pullen Island and several colonies on western Kangaroo Island), while others have declined from thousands of animals to few (Granite Island and Kingscote). A report released in 2011 presented evidence supporting the listing of the statewide population or the more closely monitored sub-population from St. Vincent's Gulf as Vulnerable under South Australia's "National Parks & Wildlife Act 1972". As of 2014, the little penguin is not listed as a species of conservation concern, despite ongoing declines at many colonies.
Tasmania.
Tasmanian little penguin population estimates range from 110,000–190,000 breeding pairs of which less than 5% are found on mainland Tasmania. Ever-increasing human pressure is predicted to result in the extinction of colonies on mainland Tasmania.
Victoria.
The largest colony of little penguins in Victoria is located at Phillip Island, where the nightly 'parade' of penguins across Summerland Beach has been a major tourist destination, and more recently a major conservation effort, since the 1920s. Phillip Island is home to an estimated 32,000 breeding pairs (70,000 birds). Little penguins can also be seen in the vicinity of the St Kilda, Victoria pier and breakwater. The breakwater is home to a colony of little penguins which have been the subject of a conservation study since 1986.
Little penguin habitats also exist at a number of offshore locations, including London Arch and The Twelve Apostles along the Great Ocean Road, Wilson's Promontory and Gabo Island.
Western Australia.
The largest colony of little penguins in Western Australia is believed to be located on Penguin Island. An estimated 1,000 pairs nest there during the winter. An account of little penguins on Bellinger Island published in 1928 numbered them in their thousands. Visiting naturalists in November 1986 estimated the colony at 20 breeding pairs. The colony's present status is unknown. The account named another substantial colony 12 miles from Bellinger Island and the same distance from Cape Pasley. Little penguins are known to breed on some islands of the Recherche Archipelago, including Woody Island where day-tripping tourists can view the animals.
Threats.
Predation.
Threats to little penguin populations include predation (both adult and nest predation) by a variety of terrestrial animals including cats, dogs, rats, foxes, large reptiles, ferrets and stoats. Due to their diminutive size and the introduction of new predators, some colonies have been reduced in size by as much as 98% in just a few years, such as the small colony on Middle Island, near Warrnambool, Victoria, which was reduced from approximately 600 penguins in 2001 to less than 10 in 2005. Because of this threat of colony collapse, conservationists pioneered an experimental technique using Maremma Sheepdogs to protect the colony and fend off would-be predators.
Uncontrolled dogs or feral cats can have sudden and severe impacts on penguin colonies (more than the penguin's natural predators) and may kill many individuals. Examples of colonies affected by dog attacks include Manly, New South Wales, Penneshaw, South Australia, Red Chapel Beach, Tasmania, Penguin Island, Western Australia and Little Kaiteriteri Beach, New Zealand.
A suspected stoat or ferret attack at Doctor's Point near Dunedin, New Zealand claimed the lives of 29 little blue penguins in November 2014.
Prey availability.
Variation in prey abundance and distribution from year to year causes young birds to be washed up dead from starvation or in weak condition.
Predator management.
Little penguins in the wild are sometimes preyed upon by New Zealand fur seals. A study conducted by researchers from the South Australian Research and Development Institute found that roughly 40 percent of seal droppings in South Australia's Granite Island area contained little penguin remains.
They are also preyed upon by White-bellied sea eagles. These large birds-of-prey are endangered in South Australia and not considered a threat to colony viability.
On land, little penguins are vulnerable to attack from domestic and feral dogs and cats. Attacks on Kangaroo Island, at Manly in Tasmania and in New Zealand have resulted in significant impacts to several populations. Management strategies to mitigate the risk of attack include establishing dog-free zones near penguin colonies and introducing regulations to ensure dogs to remain on leashes at all times in adjacent areas.
Little penguins on Middle Island in Warrnambool, Victoria were subject to heavy predation by foxes, which were able to reach the island at low tide by a tidal sand bridge. The deployment of Maremma sheepdogs to protect the penguin colony has deterred the foxes and enabled the penguin population to rebound. This is in addition to the support from groups of volunteers who work to protect the penguins from attack at night. The first Maremma sheepdog to prove the concept was Oddball, whose story inspired a feature film of the same name. The film is scheduled for release in 2015.
In Sydney, snipers have been deployed to protect a colony of little penguins. This effort is in addition to support from local volunteers who work to protect the penguins from attack at night.
Interactions with fishing.
Some little penguins are drowned when amateur fishermen set gill nets near penguin colonies. Discarded fishing line can also present an entanglement risk and contact can result in physical injury, reduced mobility or drowning. In 2014, a group of 25 dead little penguins were found on Altona Beach in Victoria. Necropsies concluded that the animals had died after becoming entangled in net fishing equipment, prompting community calls for a ban on net fishing in Port Phillip Bay.
In the 20th century, little penguins were intentionally caught by fishermen to use as baits in pots for catching crayfish (Southern rock lobster) or by line fishermen. Colonies targeted for this purpose included Bruny Island, Tasmania and West Island, South Australia.
Oil spills.
Oil spills can be lethal for penguins and other sea birds. Oil is toxic when ingested and penguins' buoyancy and the insulative quality of their plumage is damaged by contact with oil. Little penguin populations have been significantly affected during two major oil spills at sea: the Iron Baron oil spill off Tasmania's north coast in 1995 and the grounding of the Rena off New Zealand in 2011.
Plastic pollution.
Plastics are swallowed by little penguins, who mistake them for prey items. They present a choking hazard and also occupying space in the animal's stomach. Indigestible material in a penguin's stomach can contribute to malnutrition or starvation. Other larger plastic items, such as bottle packaging rings can become entangled around penguins' necks, affecting their mobility.
Human development.
The impacts of human habitation in proximity to little penguin colonies include collisions with vehicles, direct harassment, burning and clearing of vegetation and housing development.
Behaviour.
Little penguins are diurnal and like many penguin species, spend the largest part of their day swimming and foraging at sea. During the breeding and chick rearing seasons, little penguins will leave their nest at sunrise, forage for food throughout the day and return to their nests just after dusk. Little penguins preen their feathers to keep them waterproof. They do this by rubbing a tiny drop of oil onto every feather from a special gland above the tail.
Diet.
These birds feed by hunting small clupeoid fishes, cephalopods and crustaceans, for which they travel and dive quite extensively. In New Zealand, important prey items include arrow squid, slender sprat, Graham's gudgeon, red cod and ahuru. Since the year 2000, the little penguins of Port Phillip Bay's diet has consisted mainly of barracouta, anchovy, and arrow squid. Sardines previously featured more prominently in southern Australian little penguin diets prior to mass sardine mortality events of the 1990s. These mass mortality events affected sardine stocks over 5,000 kilometres of coastline.
They are generally inshore feeders. The use of data loggers has provided information of the diving behaviour of little penguins. 50% of their dives go no deeper than 2 m and the mean diving time is 21 seconds. Yet, they are able to dive as deep as 20 m and remained submerged as long as 60 seconds. Little penguins play an important role in the ecosystem as not only a predator to parasites but also a host. Recent studies have shown a new species of feather mite that feeds on the preening oil on the feathers of the penguin.
Reproduction.
Little penguins mature at different ages. The female matures at 2 years old. The male, however, matures at 3 years old. Little penguins only remain faithful to their partner in breeding seasons and whilst hatching eggs. At other times of the year they do tend to swap burrows. They exhibit site fidelity to their nesting colonies and nesting sites over successive years.
Little penguins can breed as isolated pairs, in colonies, or semi-colonially. Nests are situated close to the sea in burrows excavated by the birds or other species, or in caves, rock crevices, under logs or in or under a variety of man-made structures including nest boxes, pipes, stacks of wood or timber, and buildings. They are monogamous within a breeding season, and share incubation and chick rearing duties. They are the only species of penguin capable of producing more than one clutch of eggs per breeding season, but few populations do so. The one or two white or lightly mottled brown eggs are laid from July to mid-November, and with rarer second (or even third) clutches beginning as late as December. Incubation takes up to 36 days. Chicks are brooded for 18–38 days, and fledge after 7–8 weeks.
Little penguins typically return to their colonies to feed their chicks at dusk. The birds will tend to come ashore in small groups to provide some defence against predators which might pick off individuals one by one. In Australia, the strongest colonies are usually on cat-free and fox-free islands. However, the population on Granite Island (which is a fox, cat and dog-free island) has been severely depleted, from around 2000 penguins in 2001 down to 146 in 2009.
Relationship with humans.
Little penguins have long been a curiosity to humans, and to children in particular. Captive animals are often exhibited in zoos. Historically, the animals have also been used as bait to catch Southern rock lobster, captured for amusement and eaten by ship-wrecked sailors and castaways to avoid starvation. They have also been the victims of malicious attacks by humans and incidental bycatch by fishermen using nets. The sites of many breeding colonies have developed into tourist destinations which provide an economic boost for coastal and island communities in Australia and New Zealand. These locations also often provide facilities and volunteer staff to support population surveys, habitat improvement works and little penguin research programs.
Nocturnal Tours.
South of Perth, Western Australia, visitors to Penguin Island are able to view penguins in a natural environment. Less than one hour from the centre of the city, it is possible to see little penguins in all months, including visiting sensitive areas where they remain on land for extended periods for the purposes of moulting.
At Phillip Island, Victoria, a viewing area has been established at the Phillip Island Nature Park to allow visitors to view the nightly "penguin parade". Lights and concrete stands have been erected to allow visitors to see but not photograph the birds interacting in their colony.
In Otago, New Zealand town of Oamaru, where visitors may view the birds returning to their colony at dusk. In Oamaru it is not uncommon for penguins to nest within the cellars and foundations of local shorefront properties, especially in the old historic precinct of the town. More recently, little penguin viewing facilities have been established at Pilots Beach, Otago Peninsula and Dunedin in New Zealand. Here visitors are guided by volunteer wardens to watch penguins returning to their burrows at dusk.
Visitors to Kangaroo Island, South Australia, have nightly opportunities to observe penguins at the Kangaroo Island Marine Centre in Kingscote and at the Penneshaw Penguin Centre. Granite Island at Victor Harbor, South Australia continues to offer guided tours at dusk, despite its colony dropping from thousands in the 1990s to dozens in 2014. There is also a Penguin Centre located on the island where the penguins can be viewed in captivity.
In Bicheno, Tasmania, evening penguin viewing tours are offered by a local tour operator at a rookery on private land.
Habitat restoration.
Several efforts have been made to improve breeding sites on Kangaroo Island, including augmenting habitat with artificial burrows and revegetation work. The Knox School's habitat restoration efforts were filmed and broadcast in 2008 by "Totally Wild".
Zoological exhibits.
Australia.
Exhibits currently exist at the Adelaide Zoo, Melbourne Zoo, the National Zoo & Aquarium in Canberra, Perth Zoo and the Taronga Zoo in Sydney.
A colony of little penguins is also exhibited at Sea World, on the Gold Coast, Queensland, Australia. In early March, 2007, 25 of the 37 penguins died from an unknown toxin following a change of gravel in their enclosure. It is still not known what caused the deaths of the little penguins, and it was decided not to return the 12 surviving penguins to the same enclosure in which the penguins became ill. A new enclosure for the little penguin colony was opened at Sea World in 2008.
New Zealand.
Exhibits currently exist at the Auckland Zoo, and at the National Aquarium of New Zealand.
North America.
A colony of little blue penguins exists at the New England Aquarium in Boston, Massachusetts. The penguins are one of three species on exhibit and are part of the Association of Zoos and Aquarium's Species Survival Plan for little blue penguins.
Mascots & logos.
Linus Torvalds, the original creator of Linux (a popular operating system kernel), was once pecked by a little penguin while on holiday in Australia. Reportedly, this encounter encouraged Torvalds to select Tux as the official Linux mascot.
A Linux kernel programming challenge called the Eudyptula Challenge has attracted thousands of persons, its creator(s) use the name "Little Penguin".
Penny the Little Penguin was the mascot for the 2007 FINA World Swimming Championships held in Melbourne, Victoria.

</doc>
<doc id="18233" url="http://en.wikipedia.org/wiki?curid=18233" title="Lake Balaton">
Lake Balaton

Lake Balaton is a freshwater lake in the Transdanubian region of Hungary. It is the largest lake in Central Europe, and one of the region's foremost tourist destinations. The Zala River provides the largest inflow of water to the lake, and the canalised Sió is the only outflow.
The mountainous region of the northern shore is known both for its historic character and as a major wine region, while the flat southern shore is known for its resort towns. Balatonfüred and Hévíz developed early as resorts for the wealthy, but it was not until the late 19th century when landowners, ruined by "Phylloxera" attacking their grape vines, began building summer homes to rent out to the burgeoning middle classes.
Name.
In Hungarian, the lake is known simply as "Balaton", or "the Balaton". 
It was called by the Romans "Lacus Pelsodis" or "Pelso". The name is a substrate, probably Indo-European (Latin *"palud"- Rumantsch "paltaun"), later influenced by the Slavic "blato" meaning 'mud' or 'swamp' (from earlier Proto-Slavic "boltьno", Slovene: "Blatno jezero", Slovak: "Blatenské jazero"). Slavic prince Pribina began to build in January 846 a fortress as his seat of power and several churches in the region of Lake Balaton, in a territory of modern Zalavár surrounded by forests and swamps along the river Zala. His well fortified castle and capital of Balaton Principality that became known as "Blatnohrad" or "Moosburg" ("Swamp Fortress") served as a bulwark both against the Bulgarians and the Moravians.
The Romans called the lake "Lacus Pelso" ("Lake Pelso"). "Pelso" derives from a local name for the lake, perhaps from the Illyrian language, as the Illyrians once populated the region. 
The German name for the lake is "Plattensee". It is unlikely that the Germans named the lake so for being shallow since the adjective "platt" is a Greek loanword that was borrowed via French and entered the general German vocabulary in the 17th century. It is also noteworthy that the average depth of Balaton (3.2m) is not extraordinary for the area (cf. the average depth of the neighbouring Neusiedler See, which is roughly 1m).
Climate.
Lake Balaton affects the local area precipitation every year. The area receives approximately two to three inches (5–7 cm) more precipitation than most of Hungary, resulting in more cloudy days and less extreme temperatures. The lake's surface freezes during winters. The microclimate around Lake Balaton has also made the region ideal for viniculture. The lake, acting as a mirror, greatly increases the amount of sunlight that the grapevines of the region receive. The Mediterranean-like climate, combined with the soil (containing volcanic rock), has made the region notable for its production of wines since the Roman period two thousand years ago.
History.
While a few settlements on Lake Balaton, including Balatonfüred and Hévíz, have long been resort centres for the Hungarian aristocracy, it was only in the late 19th century that the Hungarian middle class began to visit the lake. The construction of railways in 1861 and 1909 increased tourism substantially, but the post-war boom of the 1950s was much larger.
The last major German offensive of World War II, Operation Frühlingserwachen, was conducted in the region of Lake Balaton in March 1945, being referred to as "the Lake Balaton Offensive" in many British histories of the war. The battle was a German attack by Sepp Dietrich's Sixth Panzer Army and the Hungarian Third Army between 6 March and 16 March 1945, and in the end, resulted in a Red Army victory. Several Ilyushin Il-2 wrecks have been pulled out of the lake after having been shot down during the later months of the war.
During the 1960s and 1970s, Balaton became a major tourist destination for ordinary working Hungarians and especially for subsidised holiday excursions for union members. It also attracted many East Germans and other residents of the Eastern Bloc. West Germans could also visit, making Balaton a common meeting place for families and friends separated by the Berlin Wall until 1989. The collapse of Communism after 1991 and the dismantling of the unions saw the gradual but steady reduction in numbers of lower-paid Hungarian visitors.
Tourism.
The major resorts around the lake are Siófok, Keszthely, and Balatonfüred. Zamárdi, another resort town on the southern shore, has been the site of Balaton Sound, a notable electronic music festival since 2007. Balatonkenese has hosted numerous traditional gastronomic events. Siófok is known for attracting young people to it because of its large clubs. Keszthely is the site of the Festetics Palace and Balatonfüred is a historical bathing town which hosts the annual Anna Ball.
The peak tourist season extends from June until the end of August. The average water temperature during the summer is 25 °C, which makes bathing and swimming popular on the lake. Most of the beaches consist of either grass, rocks, or the silty sand that also makes up most of the bottom of the lake. Many resorts have artificial sandy beaches and all beaches have step access to the water. Other tourist attractions include sailing, fishing, and other water sports, as well as visiting the countryside and hills, wineries on the north coast, and nightlife on the south shore. The Tihany Peninsula is a historical district. Badacsony is a volcanic mountain and wine-growing region as well as a lakeside resort. The lake is almost completely surrounded by separated bike lanes to facilitate bicycle tourism.
Although the peak season at the lake is the summer, Balaton is also frequented during the winter, when visitors go ice-fishing or even skate, sledge, or ice-sail on the lake if it freezes over.
Sármellék International Airport provides air service to Balaton (although most service is only seasonal).
Other resort towns include: Balatonalmádi, Balatonboglár, Balatonlelle, Fonyód and Vonyarcvashegy.
Towns and villages.
North shore.
From east to west:
Balatonfőkajár - Balatonakarattya - Balatonkenese - Balatonfűzfő - Balatonalmádi - Alsóörs - Paloznak - Csopak - Balatonarács - Balatonfüred - Tihany - Aszófő - Örvényes - Balatonudvari - Fövenyes - Balatonakali - Zánka - Balatonszepezd - Szepezdfürdő - Révfülöp - Pálköve - Ábrahámhegy - Balatonrendes - Badacsonytomaj - Badacsony - Badacsonytördemic - Szigliget - Balatonederics - Balatongyörök - Vonyarcvashegy - Gyenesdiás - Keszthely
South shore.
From east to west:
Balatonakarattya - Balatonaliga - Balatonvilágos - Sóstó - Szabadifürdő - Siófok - Széplak - Zamárdi - Szántód - Balatonföldvár - Balatonszárszó - Balatonszemes - Balatonlelle - Balatonboglár - Fonyód - Bélatelep - Balatonfenyves - Balatonmáriafürdő - Balatonkeresztúr - Balatonberény - Fenékpuszta
Panorama from Balaton and Keszthely

</doc>
<doc id="18234" url="http://en.wikipedia.org/wiki?curid=18234" title="Libro de los juegos">
Libro de los juegos

The Libro de los Juegos, ("Book of games"), or Libro de acedrex, dados e tablas, ("Book of chess, dice and tables", in Old Spanish) was commissioned by Alfonso X of Castile, Galicia and León and completed in his scriptorium in Toledo in 1283, is an exemplary piece of Alfonso’s medieval literary legacy.
The book consists of ninety-seven leaves of parchment, many with color illustrations, and contains 150 miniatures. The text is a treatise that addresses the playing of three games: a game of skill, or chess; a game of chance, or dice; and a third game, backgammon, which combines elements of both skill and chance. The book contains the earliest known description of some of these games, including many games imported from the Arab kingdoms. These games are discussed in the final section of the book at both an astronomical and astrological level. Examining further, the text can also be read as an allegorical initiation tale and as a metaphysical guide for leading a balanced, prudent, and virtuous life. In addition to the didactic, although not overly moralistic, aspect of the text, the manuscript’s illustrations reveal a rich cultural, social, and religious complexity.
It is one of the most important documents for researching the history of board games. The only known original is held in the library of the monastery of San Lorenzo del Escorial near Madrid in Spain. The book is bound in sheepskin and is 40 cm high and 28 cm wide (16 in × 11 in). A 1334 copy is held in the library of the Spanish Royal Academy of History in Madrid.
Background.
Alfonso was likely influenced by his contact with scholars in the Arab world. Unlike many contemporary texts on the topic, he does not engage the games in the text with moralistic arguments; instead, he portrays them in an astrological context. He conceives of gaming as a dichotomy between the intellect and chance. The book is divided into three parts reflecting this: the first on chess (a game purely of abstract strategy), the second on dice (with outcomes controlled strictly by chance), and the last on tables (combining elements of both). The text may have been influenced by Frederick II's text on falconry.
Chess.
The Libro de juegos contains an extensive collection of writings on chess, with over 100 chess problems and variants. Among its more notable entries is a depiction of what Alfonso calls the "ajedrex de los quatro tiempos" ("chess of the four seasons"). This game is a chess variant for four players, described as representing a conflict between the four elements and the four humors. The chessmen are marked correspondingly in green, red, black, and white, and pieces are moved according to the roll of dice. Alfonso also describes a game entitled "astronomical chess", played on a board of seven concentric circles, divided radially into twelve areas, each associated with a constellation of the Zodiac.
Tables.
The book describes the rules for a number of games in the tables family. One notable entry is "todas tablas", which has an identical starting position to modern backgammon and follows the same rules for movement and bearoff. Alfonso also describes a variant played on a board with seven points in each table. Players rolled seven-sided dice to determine the movement of pieces, an example of Alfonso's preference for the number seven.
Art.
The miniatures in the "Libro de juegos" vary between half- and full-page illustrations. The half-page miniatures typically occupy the upper half of a folio, with text explaining the game "problem" solved in the image occupying the bottom half. The back or second (verso) side of Folio 1, in a half-page illustration, depicts the initial stages of the creation of the "Libro de juegos", accompanied by text on the bottom half of the page, and the front or first (recto) side of Folio 2 depicts the transmission of the game of chess from an Indian Philosopher-King to three followers. The full-page illustrations are almost exclusively on the verso side of later folios and are faced by accompanying text on the recto side of the following folio. The significance of the change in miniature size and placement may indicate images of special emphasis, could merely function as a narrative or didactic technique, or could indicate different artisans at work in Alfonso’s scriptorium as the project developed over time.
Having multiple artisans working on the "Libro de juegos" would have been a typical practice for medieval chanceries and scriptoria, where the labor of producing a manuscript was divided amongst individuals of varying capacities, for example the positions of scribe, draftsman, and apprentice cutting pages. But in addition to performing different tasks, various artisans could have labored at the same job, such as the work of illustration in the "Libro de juegos", thereby revealing a variety hands or styles. The "Libro de Juegos" offers such evidence in the difference in size between the half- and full-page illustrations in addition to changes in framing techniques amongst the folios: geometrical frames with embellished corners, architectural frames established by loosely perspectival rooftops and colonnades, and games played under tents. Other stylistic variances are found in figural representation, in facial types, and in a repertoire of different postures assumed by the players in different folios in the manuscript.
For example, in a comparison of two miniatures, found on Folios 53v and 76r, examples of these different styles are apparent, although the trope of a pair of gamers is maintained. In Folio 53v, two men are playing chess, both wearing turbans and robes. Although they may be seated on rugs on the ground, as suggested by the ceramic containers that are placed on or front of the rug near the man on the right side of the board, the figures’ seated positions, which are full frontal with knees bent at right angles, suggests that they are seated on stools or perhaps upholstered benches. The figures’ robes display a Byzantine conservatism, with their modeled three-dimensionality and allusion to a Classical style, yet the iconic hand gestures are reminiscent of a Romanesque energy and theatricality. Although the figures are seated with their knees and torsos facing front, their shoulders and heads rotate in three-quarter profile toward the center of the page, the chess board, and each other. The proximal, inner arm of each player (the arm that is closest to the board) is raised in a speaking gesture; the distal, outside arms of the players are also raised and are bent at the elbows, creating a partial crossing of each player’s torso as the hands lift in speaking gestures. The faces reveal a striking specificity of subtle detail, particular to a limited number of miniatures throughout the "Libro de juegos", perhaps indicative of a particular artist’s hand. These details include full cheeks, realistic wrinkles around the eyes and across the brow, and a red, full-lipped mouth that hints at the Gothic affectations in figural representation coming out of France during the late twelfth and early thirteenth centuries.
The style in the miniature in Folio 76v is markedly different from the style in Folio 53v. In this case, the framed miniature contains two men, perhaps Spanish, with uncovered wavy light brown hair that falls to the jaw line. The men seem young, as the player on the left has no facial hair and his face is unlined. In both folios, both pairs of players are playing backgammon and seem to be well-dressed, although there is no addition of gold detailing to their robes as seen in the wardrobes of aristocratic players in other miniatures. These players are seated on the ground, leaning on pillows that are placed next to a backgammon board. In this miniature, the figure on the left side of the board faces the reader, while the figure on the right leans in to the board with his back to the reader. In other words, each player is leaning on his left elbow, using his right hand to reach across his body to play. In the miniatures of this style, the emphasis seems to be more on the posture of the player than the detail of their faces; this crossed, lounging style is only found in the folios of the "Libro de tablas", the third section of the "Libro de juegos" which explicates the game of backgammon, again perhaps indicative of the work of a particular artist.
Other visual details contemporaneous of Alfonso’s court and social and cultural milieu infuse the "Libro de juegos". Although some of the miniatures are framed by simple rectangles with corners embellished by the golden castles and lions of Castile and León, other are framed by medieval Spanish architectural motifs, including Gothic and Mudéjar arcades of columns and arches. At times, the figural depictions are hierarchical, especially in scenes with representations of Alfonso, where the king is seated on a raised throne while dictating to scribes or meting out punishments to gamblers. Yet a contemporary atmosphere of Spanish "convivencia" is evoked by the inclusion nobility, rogues, vagrants, young and old, men, women, Christian, Muslim, and Jewish characters. Alfonso himself is depicted throughout the text, both as participant and spectator and as an older man and as a younger. The pages are filled with many social classes and ethnicities in various stages of solving the challenges presented by games.
Iconography.
The "Libro de juegos" can be divided into three parts: the games and problems it explores textually, the actual illuminations themselves, and the metaphysical allegories, where an analysis of the texts and illuminations reveals the movements of the macrocosmos of the universe and the microcosmos of man. The symbolism within the medieval illuminations, as explained by the accompanying texts, reveal allusions to medieval literature, art, science, law and philosophy. Intended as a didactic text, the manuscript functions as a manual that documents and explains how and why one plays games ranging from pure, intellectual strategy (chess), to games of pure chance (dice), to games that incorporate both elements (backgammon). Conceivably, Alfonso hoped to elucidate for himself how to better play the game of life, while also providing a teaching tool for others. The game of "ajedrex", or chess, is not the only game explicated in the "Libro de Juegos", but it does occupy the primary position in the text and is given the most attention to detail.
In the thirteenth century, chess had been played in Europe for almost two hundred years, having been introduced into Europe by Arabs around the year 1000. The Arabs had become familiar with the game as early as the eighth century when the Islamic empire conquered Persia, where the game of chess was alleged to have been originated. It is said that a royal advisor had invented the game in order to teach his king prudence without having to overtly correct him. As Arab contact with the West expanded, so too did the game and its various permutations, and by the twelfth century, chess was becoming an entertaining diversion among a growing population of Europeans, including some scholars, clergy, the aristocracy, and the merchant classes; thus, by the thirteenth century, the iconography and symbolism associated with chess would have been accessible and familiar to Alfonso and his literate court culture, who may have had access to the private library, and manuscripts, of Alfonso, including the "Libro de juegos".
The "Libro de juegos" manuscript was a Castilian translation of Arabic texts, which were themselves translations of Persian manuscripts. The visual trope portrayed in the "Libro de juegos" miniatures is seen in other European transcriptions of the Arabic translations, most notably the German Carmina Burana Manuscript: two figures, one on either side of the board, with the board tilted up to reveal to the readers the moves made by the players. The juxtaposition of chess and dice in Arabic tradition, indicating the opposing values of skill (chess) and ignorance (dice), was given a different spin in Alfonso’s manuscript, however. As Alfonso elucidates in the opening section of the "Libro de Juegos", the "Libro de ajedrex" (Book of chess) demonstrates the value of the intellect, the "Libro de los dados" (Book of dice) illustrates that chance has supremacy over pure intellect, and the" Libro de las tablas" (Book of tables) celebrates a conjoined use of both intellect and chance. Further, the iconographic linkage between chess and kingship in the Western tradition continued to evolve and became symbolic of kingly virtues, including skill, prudence, and intelligence.
Significance.
Most of the work accomplished in Alfonso’s scriptorium consisted of translations into the Castilian vernacular from Arabic translations of Greek texts or classical Jewish medicinal texts. As a result, very few original works were produced by this scholar-king, relative to the huge amount of work that was translated under his auspices. This enormous focus on translation was perhaps an attempt by Alfonso to continue the legacy of academic openness in Castile, initiated by Islamic rulers in Córdoba, where the emirates had also employed armies of translators in order to fill their libraries with Arabic translations of classic Greek texts. Alfonso was successful in promoting Castilian society and culture through his emphasis on the use of Galaico-Portuguese and Castilian, in academic, juridical, diplomatic, literary, and historical works. This emphasis, on languages other than Romance languages, also had the effect of reducing the universality of his translated works and original academic writings, as Latin was the "lingua franca" in both Iberia and Europe; yet Alfonso never desisted in his promotion of the Castilian vernacular.
Legacy.
In 1217, Alfonso had captured the Kingdom of Murcia, on the Mediterranean coast south of Valencia, for his father, King Alfonso IX, thereby unifying the kingdoms of Castile and León, bringing together the northern half of the Iberian Peninsula under one Christian throne. With the Christian re-conquest of the Peninsula underway, inroads into Islamic territories were successfully incorporating lands previously held by the "taifa" kingdoms. The arts and sciences prospered in the Kingdom of Castile under the confluence of Latin and Arabic traditions of academic curiosity as Alfonso sponsored scholars, translators, and artists of all three religions of the Book (Jewish, Christian, and Muslim) in his chanceries and scriptoria. Clerical and secular scholars from Europe turned their eyes to Iberian Peninsula as the arts and sciences prospered in an early Spanish "renaissance" under the patronage of Alfonso X, who was continuing the tradition of (relatively) enlightened and tolerant "convivencia" established by the Muslim emirate several centuries earlier.
As an inheritor of a dynamic mixture of Arabic and Latin culture, Alfonso was steeped in the rich heritage of humanistic philosophy, and the production of his "Libro de juegos" reveals the compendium of world views that comprised the eclectic thirteenth century admixture of faith and science. According to this approach, man’s actions could be traced historically and his failures and successes could be studied as lessons to be applied to his future progress. These experiences can be played out and studied as they are lived, or as game moves played and analyzed in the pages of the "Libro de juegos". It is a beautiful and luxurious document, rich not only in workmanship but also in the amount of scholarship of multiple medieval disciplines that are integrated in its pages.

</doc>
<doc id="18236" url="http://en.wikipedia.org/wiki?curid=18236" title="Lithium citrate">
Lithium citrate

Lithium citrate (Li3C6H5O7) (brand names Litarex, Demalit) is a chemical compound of lithium and citrate that is used as a mood stabilizer in psychiatric treatment of manic states and bipolar disorder. There is extensive pharmacology of lithium, the active component of this salt.
Lithia water contains various lithium salts, including the citrate. An early version of Coca-Cola available in pharmacies' soda fountains called Lithia Coke was a mixture of Coca-Cola syrup and lithia water. The soft drink 7Up was originally named "Bib-Label Lithiated Lemon-Lime Soda" when it was formulated in 1929 because it contained lithium citrate. The beverage was a patent medicine marketed as a cure for hangover. Lithium citrate was removed from 7Up in 1948.

</doc>
<doc id="18237" url="http://en.wikipedia.org/wiki?curid=18237" title="Lithium carbonate">
Lithium carbonate

Lithium carbonate is an inorganic compound, the lithium salt of carbonate with the formula Li2CO3. This white salt is widely used in the processing of metal oxides. For the treatment of bipolar disorder, it is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Uses.
Lithium carbonate is an important industrial chemical. It forms low-melting fluxes with silica and other materials. Glasses derived from lithium carbonate are useful in ovenware. Lithium carbonate is a common ingredient in both low-fire and high-fire ceramic glaze. Its alkaline properties are conducive to changing the state of metal oxide colorants in glaze particularly red iron oxide (Fe2O3). Cement sets more rapidly when prepared with lithium carbonate, and is useful for tile adhesives. When added to aluminium trifluoride, it forms LiF which gives a superior electrolyte for the processing of aluminium. It is also used in the manufacture of most lithium-ion battery cathodes, which are made of lithium cobalt oxide.
Medical uses.
In 1843, lithium carbonate was used as a new solvent for stones in the bladder. In 1859, some doctors recommended a therapy with lithium salts for a number of ailments, including gout, urinary calculi, rheumatism, mania, depression, and headache. In 1949, John Cade discovered the antimanic effects of lithium ions. This finding led lithium, specifically lithium carbonate, to be used to treat mania associated with bipolar disorder.
Lithium carbonate is used to treat mania, the elevated phase of bipolar disorder. Lithium ions interfere with ion transport processes (see “sodium pump”) that relay and amplify messages carried to the cells of the brain. Mania is associated with irregular increases in protein kinase C (PKC) activity within the brain. Lithium carbonate and sodium valproate, another drug traditionally used to treat the disorder, act in the brain by inhibiting PKC’s activity and help to produce other compounds that also inhibit the PKC. Despite these findings, a great deal remains unknown regarding lithium's mood-controlling properties.
Use of lithium salts exhibit a number of risks and side effects, especially at higher doses. Lithium intoxication affects the central nervous and renal systems and is potentially lethal.
Properties and reactions.
Unlike sodium carbonate, which forms at least three hydrates, lithium carbonate exists only in the anhydrous form. Its solubility in water is low relative to other lithium salts. The isolation of lithium from aqueous extracts of lithium ores capitalizes on this poor solubility. Its apparent solubility increases 10-fold under a mild pressure of carbon dioxide; this effect is due to the formation of the metastable bicarbonate, which is more soluble:
The extraction of lithium carbonate at high pressures of CO2 and its precipitation upon depressuring is the basis of the Quebec process.
Lithium carbonate can also be purified by exploiting its diminished solubility in hot water. Thus, heating a saturated aqueous solution causes crystallization of Li2CO3.
Lithium carbonate, and other carbonates of group 1, do not decarboxylate readily. Li2CO3 decomposes at temperatures around 1300°C.
Production.
Lithium is extracted from primarily two sources: pegmatite crystals and lithium salt from brine pools. About 30,000 tons were produced in 1989. It also exists as the rare mineral zabuyelite.
Lithium carbonate is generated by combining lithium peroxide with carbon dioxide. This reaction is the basis of certain air purifiers, e.g., in spacecraft, used to absorb carbon dioxide: 

</doc>
<doc id="18238" url="http://en.wikipedia.org/wiki?curid=18238" title="Lunar Roving Vehicle">
Lunar Roving Vehicle

The Lunar Roving Vehicle (LRV) or lunar rover was a battery-powered four-wheeled rover used on the Moon in the last three missions of the American Apollo program (15, 16, and 17) during 1971 and 1972. It was popularly known as the moon buggy, a play on the phrase "dune buggy".
The LRV was transported to the Moon on the Apollo Lunar Module (LM) and, once unpacked on the surface, could carry one or two astronauts, their equipment, and lunar samples. The three LRVs remain on the Moon.
History.
The concept of a lunar rover predated Apollo, with a 1952–1954 series in "Collier's Weekly" magazine by Wernher von Braun and others, "Man Will Conquer Space Soon!" In this, von Braun described a six-week stay on the Moon, featuring 10-ton tractor trailers for moving supplies.
In 1956, Mieczyslaw G. Bekker published two books on land locomotion. At the time, Bekker was a University of Michigan professor and a consultant to the U.S. Army Tank-Automotive Command's Land Locomotion Laboratory. The books provided much of the theoretical base for future lunar vehicle development.
Early lunar mobility studies.
In the February 1964 issue of "Popular Science", von Braun, then director of NASA's Marshall Space Flight Center (MSFC), discussed the need for a lunar surface vehicle, and revealed that studies had been underway at MSFC in conjunction with Lockheed, Bendix, Boeing, General Motors, Brown Engineering, Grumman, and Bell Aerospace. 
Beginning in the early 1960s, a series of studies centering on lunar mobility were conducted under MSFC. This began with the Lunar Logistics System (LLS), followed by the Mobility Laboratory (MOLAB), then the Lunar Scientific Survey Module (LSSM), and finally the Mobility Test Article (MTA). In early planning for the Apollo Program, it had been assumed that two Saturn V launch vehicles would be used for each lunar mission: one for sending the crew aboard a Lunar Surface Module (LSM) to lunar orbit, landing, and returning, and a second for sending an LSM-Truck (LSM-T) with all of the equipment, supplies, and transport vehicle for use by the crew while on the surface. All of the first MSFC studies were based on this dual-launch assumption, allowing a large, heavy, roving vehicle.
The LLS studies were begun by Grumman and Northrop in the fall of 1962; these were designs for pressurized cabin vehicles with electric motors for each wheel. At about this same time, Bendix and Boeing were conducting internal studies on lunar transportation systems. Bekker, now with General Motors Defense Research Laboratories (GMDRL) at Santa Barbara, California, was completing a study for NASA's Jet Propulsion Laboratory on a small, unmanned lunar roving vehicle for the Surveyor Program. Ferenc Pavlics, originally from Hungary, used a wire-mesh design for "resilient wheels," a design that would be followed in future small rovers.
In early 1963, NASA selected MSFC for studies in an Apollo Logistics Support System (ALSS). Following reviews of all earlier efforts, this resulted in a 10-volume report. Included was the need for a pressurized vehicle in the 6490 - weight range, accommodating two men with their expendables and instruments for traverses up to two weeks in duration. This was called a Mobility Laboratory (MOLAB). In June 1964, MSFC awarded contracts for MOLAB studies and Mobility Test Articles (MTAs) to Bendix and to Boeing, with GMDRL as vehicle technology subcontractor. Bell Aerospace was already under contract for studies of Lunar Flying Vehicles.
Even as ALSS was underway, MSFC was examining a less ambitious surface exploration activity, the Local Scientific Surface Module (LSSM). This would be composed of a fixed, habitable shelter-laboratory (SHELAB) with a small lunar-traversing vehicle (LTV) that could either carry one man or be remotely controlled. LSSM would be carried on an LSM-T, thus still requiring a dual launch. The Propulsion and Vehicle Engineering (P&VE) support contractor Hayes International made a preliminary study of the shelter and vehicle. Also, for the potential need of a MOLAB-like vehicle in future, enlarged lunar explorations, the MOLAB efforts were continued for some time, resulting in several full-scale MTAs. 
With pressure from Congress to hold down Apollo costs, Saturn V production was reduced, allowing only a single booster per mission. It would then be necessary for any roving vehicle to be carried on the same Lunar Module as transporting the astronauts. In November 1964, ALSS was put on indefinite hold, but Bendix and Boeing were given study contracts for small rovers under the LSSM program. The name of the Lunar Excursion Module was changed to simply the Lunar Module, indicating that the capability for powered "excursions" away from a lunar-lander base did not yet exist. There could be no SHELAB — the astronauts would work out of the LM — and the LTV accommodating two persons took the name Local Scientific Surface Module (LSSM). MSFC was also examining unmanned robotic rovers that could be controlled from the Earth. 
From the start of MSFC, Huntsville, Alabama-based Brown Engineering Company (BECO) had participated in all of its lunar mobility efforts. In 1965, BECO became the prime support contractor for MSFC's P&VE Laboratory. With an urgent need to determine the feasibility of a two-man LSSM, von Braun bypassed the usual procurement process and had P&VE's Advanced Studies Office directly task BECO to design, build, and test a MTA for the vehicle. While Bendix and Boeing would continue with work leading to LSSM concepts and designs, the MTA was vital for MSFC human factors studies involving spacesuit-clad astronauts interfacing with power, telemetry, navigation, and life-support equipment on the rover. Eduardo SanJuan, an immigrant from the Philippines who had led the earlier study by Hayes International, joined BECO to lead the LSSM MTA development .
In designing the LSSM MTA, full use was made of all earlier small-rover studies, and commercially available components were incorporated wherever possible. The selection of wheels was of great importance, and almost nothing was known at that time about the lunar surface. The MSFC Space Sciences Laboratory (SSL) was responsible for predicting surface properties. BECO was also the prime support contractor for the SSL and set up a test area to examine a wide variety of wheel-surface conditions. To simulate Pavlics' "resilient wheel," a four-foot-diameter inner tube wrapped with nylon ski rope was used. On the MTA, each wheel had a small electric motor, with overall power provided by standard truck batteries. A roll bar gave protection from overturn accidents.
In early 1966, BECO's MTA became available for examining human factors and other testing. MSFC built a small test track with craters and rock debris where the LSSM and MOLAB MTAs were compared; it was soon obvious that a small rover would be best for the proposed missions. The vehicle was also operated in remote mode to determine characteristics in tests that might be dangerous to the operator, such as acceleration, bounce-height, and turn-over tendency as it traveled at higher speeds and over simulated obstacles. The LSSM performance under one-sixth gravity was obtained through flights on a KC-135A aircraft in a Reduced Gravity parabolic maneuver; among other things, the need for a very soft wheel and suspension combination was shown. Although Pavlics' wire-mesh wheels were not available for the MTA, testing of these was conducted on various soils at the Waterways Experiment Station of the U.S. Army Corps of Engineers at Vicksburg, Mississippi. Later, when wire-mesh wheels were tested on low-g flights, the need for wheel fenders to reduce dust contamination was found. The LSSM MTA was extensively tested at the U.S. Army's Yuma Proving Ground in Arizona, as well as the Army's Aberdeen Proving Ground in Maryland.
Apollo Lunar Roving Vehicle.
During 1965 and 1967, the Summer Conference on Lunar Exploration and Science brought together leading scientists to assess NASA's planning for exploring the Moon and to make recommendations. One of their findings was that the LSSM was critical to a successful program and should be given major attention. At MSFC, von Braun established the Lunar Roving Task team, and in May 1969, NASA selected the Lunar Roving Vehicle (LRV) for use in manned lunar missions and approved the Manned Lunar Rover Vehicle Program as a MSFC hardware development. Saverio F. "Sonny" Morea was named the LRV program manager.
On 11 July 1969, just before the successful Moon landing of Apollo 11, a request for proposal for the final development and building the Apollo LRV was released by MSFC. Boeing, Bendix, Grumman, and Chrysler submitted proposals. Following three months of proposal evaluation and negotiations, Boeing was selected as the Apollo LRV prime contractor on 28 October 1969. Boeing would manage the LRV project under Henry Kudish in Huntsville, Alabama. As a major subcontractor, General Motors' Defense Research Laboratories in Santa Barbara, California, would furnish the mobility system (wheels, motors, and suspension); this effort would be led by Ferenc Pavlics. Boeing in Seattle, Washington, would furnish the electronics and navigation system. Vehicle testing would take place at the Boeing facility in Kent, Washington, and the chassis manufacturing and overall assembly would be at the Boeing facility in Huntsville.
The first cost-plus-incentive-fee contract to Boeing was for $19,000,000 and called for delivery of the first LRV by 1 April 1971. Cost overruns, however, led to a final cost of $38,000,000, which was about the same as NASA's original estimate. Four lunar rovers were built, one each for Apollo missions 15, 16, and 17; and one used for spare parts after the cancellation of further Apollo missions. Other LRV models were built: a static model to assist with human factors design; an engineering model to design and integrate the subsystems; two one-sixth gravity models for testing the deployment mechanism; a one-gravity trainer to give the astronauts instruction in the operation of the rover and allow them to practice driving it; a mass model to test the effect of the rover on the LM structure, balance, and handling; a vibration test unit to study the LRV's durability and handling of launch stresses; and a qualification test unit to study integration of all LRV subsystems. A paper by Savero Morea gives details of the LRV system and its development.
LRVs were used for greater surface mobility during the Apollo J-class missions, "Apollo 15", "Apollo 16", and "Apollo 17". The rover was first used on 31 July 1971, during the Apollo 15 mission. This greatly expanded the range of the lunar explorers. Previous teams of astronauts were restricted to short walking distances around the landing site due to the bulky space suit equipment required to sustain life in the lunar environment. The range, however, was operationally restricted to remain within walking distance of the lunar module, in case the rover broke down at any point. The rovers were designed with a top speed of about 8 mph, although Eugene Cernan recorded a maximum speed of 11.2 mph, giving him the (unofficial) lunar land-speed record.
The LRV was developed in only 17 months and performed all its functions on the Moon with no major anomalies. Scientist-astronaut Harrison Schmitt of Apollo 17 said, "The Lunar Rover proved to be the reliable, safe and flexible lunar exploration vehicle we expected it to be. Without it, the major scientific discoveries of Apollo 15, 16, and 17 would not have been possible; and our current understanding of lunar evolution would not have been possible."
The LRVs experienced some minor problems. The rear fender extension on the Apollo 16 LRV was lost during the mission's second extra-vehicular activity (EVA) at station 8 when John Young bumped into it while going to assist Charles Duke. The dust thrown up from the wheel covered the crew, the console, and the communications equipment. High battery temperatures and resulting high power consumption ensued. No repair attempt was mentioned.
The fender extension on the Apollo 17 LRV broke when accidentally bumped by Eugene Cernan with a hammer handle. Cernan and Schmitt taped the extension back in place, but due to the dusty surfaces, the tape did not adhere and the extension was lost after about one hour of driving, causing the astronauts to be covered with dust. For their second EVA, a replacement "fender" was made with some EVA maps, duct tape, and a pair of clamps from inside the Lunar Module that were nominally intended for the moveable overhead light. This repair was later undone so that the clamps could be taken inside for the return launch. The maps were brought back to Earth and are now on display at the National Air and Space Museum. The abrasion from the dust is evident on some portions of the makeshift fender.
The color TV camera mounted on the front of the LRV could be remotely operated by Mission Control in pan and tilt axes as well as zoom. This allowed far better television coverage of the EVA than the earlier missions. On each mission, at the conclusion of the astronauts' stay on the surface, the commander drove the LRV to a position away from the Lunar Module so that the camera could record the ascent stage launch. The camera operator in Mission Control experienced difficulty in timing the various delays so that the LM ascent stage was in frame through the launch. On the third and final attempt (Apollo 17), the launch and ascent were successfully tracked.
NASA's rovers, left behind, are among the artificial objects on the Moon, as are the Soviet Union's unmanned rovers, Lunokhod 1 and Lunokhod 2.
Features and specifications.
The Apollo Lunar Roving Vehicle was an electric-powered vehicle designed to operate in the low-gravity vacuum of the Moon and to be capable of traversing the lunar surface, allowing the Apollo astronauts to extend the range of their surface extravehicular activities. Three LRVs were used on the Moon, one on Apollo 15 by astronauts David Scott and Jim Irwin, one on Apollo 16 by John Young and Charles Duke, and one on Apollo 17 by Eugene Cernan and Harrison Schmitt. The mission commander served as the driver, occupying the left-hand seat of each LRV. Features are available in papers by Morea, Baker, and Kudish.
Mass and payload.
The Lunar Roving Vehicle had a mass of 210 kg, and was designed to hold a payload of 490 kg. This resulted in weights in the approximately one-sixth g on the lunar surface of 35 kgf empty and 116 kgf fully loaded. The frame was 10 ft long with a wheelbase of 7.5 ft. The height of the vehicle was 3.6 ft. The frame was made of 2219 aluminium alloy tubing welded assemblies and consisted of a three-part chassis that was hinged in the center so it could be folded up and hung in the Lunar Module Quadrant 1 bay. It had two side-by-side foldable seats made of tubular aluminium with nylon webbing and aluminum floor panels. An armrest was mounted between the seats, and each seat had adjustable footrests and a Velcro seat belt. A large mesh dish antenna was mounted on a mast on the front center of the rover. The suspension consisted of a double horizontal wishbone with upper and lower torsion bars and a damper unit between the chassis and upper wishbone. Fully loaded, the LRV had a ground clearance of 14 in.
Wheels and power.
The wheels were designed and manufactured by General Motors Defense Research Laboratories in Santa Barbara, California. Ferenc Pavlics was given special recognition by NASA for developing the "resilient wheel". They consisted of a spun aluminum hub and a 32 in diameter, 9 in wide tire made of zinc-coated woven 0.033 in diameter steel strands attached to the rim and discs of formed aluminum. Titanium chevrons covered 50% of the contact area to provide traction. Inside the tire was a 25.5 in diameter bump stop frame to protect the hub. Dust guards were mounted above the wheels. Each wheel had its own electric drive made by Delco, a direct current (DC) series-wound motor capable of 0.25 hp at 10,000 rpm, attached to the wheel via an 80:1 harmonic drive, and a mechanical brake unit.
Maneuvering capability was provided through the use of front and rear steering motors. Each series-wound DC steering motor was capable of 0.1 hp. The front and rear wheels would turn in opposite directions to achieve a tight turning radius of 10 ft, or could be decoupled so only front or rear would be used for steering. They could free-wheel in case of drive failure.
Power was provided by two 36-volt silver-zinc potassium hydroxide non-rechargeable batteries with a capacity of 121 A·h each (a total of 242 A·h), yielding a range of 57 mi. These were used to power the drive and steering motors and also a 36-volt utility outlet mounted on the front of the LRV to power the communications relay unit or the TV camera. LRV batteries and electronics were passively cooled, using change-of-phase wax thermal capacitor packages and reflective, upward-facing radiating surfaces. While driving, radiators were covered with mylar blankets to minimize dust accumulation. When stopped, the astronauts would open the blankets, and manually remove excess dust from the cooling surfaces with hand brushes.
Control and navigation.
A T-shaped hand controller situated between the two seats controlled the four drive motors, two steering motors, and brakes. Moving the stick forward powered the LRV forward, left and right turned the vehicle left or right, and pulling backwards activated the brakes. Activating a switch on the handle before pulling back would put the LRV into reverse. Pulling the handle all the way back activated a parking brake. The control and display modules were situated in front of the handle and gave information on the speed, heading, pitch, and power and temperature levels.
Navigation was based on continuously recording direction and distance through use of a directional gyro and odometer and feeding this data to a computer that would keep track of the overall direction and distance back to the LM. There was also a Sun-shadow device that could give a manual heading based on the direction of the Sun, using the fact that the Sun moved very slowly in the sky.
Usage.
Each rover was used on three traverses, one per day over the three-day course of each mission, with the individual performances logged as follows:
An operational constraint on the use of the LRV was that the astronauts must be able to walk back to the LM if the LRV were to fail at any time during the EVA (called the "Walkback Limit"). Thus, the traverses were limited in the distance they could go at the start and at any time later in the EVA. Therefore, they went to the farthest point away from the LM and worked their way back to it so that, as the life support consumables were depleted, their remaining walk back distance was equally diminished. This constraint was relaxed during the longest traverse on Apollo 17, based on the demonstrated reliability of the LRV and spacesuits on previous missions. A paper by Burkhalter and Sharp provides details on usage.
Deployment.
Deployment of the LRV from the LM's Quadrant 1 bay by the astronauts was achieved with a system of pulleys and braked reels using ropes and cloth tapes. The rover was folded and stored in the bay with the underside of the chassis facing out. One astronaut would climb the egress ladder on the LM and release the rover, which would then be slowly tilted out by the second astronaut on the ground through the use of reels and tapes. As the rover was let down from the bay, most of the deployment was automatic. The rear wheels folded out and locked in place. When they touched the ground, the front of the rover could be unfolded, the wheels deployed, and the entire frame let down to the surface by pulleys.
The rover components locked into place upon opening. Cabling, pins, and tripods would then be removed and the seats and footrests raised. After switching on all the electronics, the vehicle was ready to back away from the LM.
Current locations.
A total of four flight-ready rovers were manufactured. Three were transported to and left on the moon via the Apollo 15, 16, and 17 missions, with the fourth rover used for spare parts on the first three following the cancellation of Apollo 18. Since only the upper stages of the lunar excursion modules could return to lunar orbit from the surface, the vehicles, along with the lower stages were abandoned. As a result, the only lunar rovers on display are test vehicles, trainers, and mock-ups. The rover used on Apollo 15 was left on the lunar surface at Hadley-Apennine (
). The rover used on Apollo 16 was left on the lunar surface at Descartes (
). The rover used on Apollo 17 was left on the lunar surface at Taurus-Littrow (
) and was seen by the Lunar Reconnaissance Orbiter during passes in 2009 and 2011.
Several rovers were created for testing, training, or validation purposes. The engineering mockup is on display at the Museum of Flight in Seattle, Washington. The Qualification Test Unit is on display at the National Air and Space Museum in Washington, D.C. The rover used for vibration testing is on display in the Davidson Saturn V Center at the Marshall Space Flight Center in Huntsville, Alabama. Additional test units are on display at the Johnson Space Center in Houston, Texas, and the Kennedy Space Center Visitors Complex in Cape Canaveral, Florida. Replicas of rovers are on display at the National Museum of Naval Aviation in Pensacola, Florida, the Evergreen Aviation & Space Museum in McMinnville, Oregon, and the Kansas Cosmosphere and Space Center in Hutchinson, Kansas. A replica on loan from the Smithsonian Institution is on display at the attraction at Epcot at the Walt Disney World Resort near Orlando, Florida.

</doc>
<doc id="18240" url="http://en.wikipedia.org/wiki?curid=18240" title="Lake Kickapoo">
Lake Kickapoo

Lake Kickapoo is a reservoir in Archer County, Texas. Created in 1947. It has a surface area of 6,200 acre. Named after the Kickapoo tribe native to the area.
One of the nine Air Force Space Surveillance System (formerly NAVSPASUR) sites is located at Lake Kickapoo (). It is the master transmitter and the most powerful continuous wave (CW) station in the world, at 768 kW radiated power.

</doc>
<doc id="18243" url="http://en.wikipedia.org/wiki?curid=18243" title="Land (disambiguation)">
Land (disambiguation)

Land is the solid surface of the Earth that is not covered by water. It may also refer to:
In music:
As a synonym for a region belonging to a people:
As a geographical place:
As a division of a country:
Other usages:

</doc>
<doc id="18245" url="http://en.wikipedia.org/wiki?curid=18245" title="Labyrinth">
Labyrinth

In Greek mythology, the Labyrinth (Greek λαβύρινθος "labyrinthos",) was an elaborate structure designed and built by the legendary artificer Daedalus for King Minos of Crete at Knossos. Its function was to hold the Minotaur eventually killed by the hero Theseus. Daedalus had so cunningly made the Labyrinth that he could barely escape it after he built it. 
In colloquial English, "labyrinth" is generally synonymous with "maze", but many contemporary scholars observe a distinction between the two: "maze" refers to a complex branching (multicursal) puzzle with choices of path and direction; while a single-path (unicursal) "labyrinth" has only a single path to the center. A labyrinth in this sense has an unambiguous route to the center and back and is not difficult to navigate.
Although early Cretan coins occasionally exhibit multicursal patterns, the unicursal seven-course "Classical" design became associated with the Labyrinth on coins as early as 430 BC, and became widely used to represent the Labyrinth – even though both logic and literary descriptions make it clear that the Minotaur was trapped in a complex branching maze. Even as the designs became more elaborate, visual depictions of the Labyrinth from Roman times until the Renaissance are almost invariably unicursal. Branching mazes were reintroduced only when garden mazes became popular during the Renaissance.
Labyrinths appeared as designs on pottery or basketry, as body art, and in etchings on walls of caves or churches. The Romans created many primarily decorative labyrinth designs on walls and floors in tile or mosaic. Many labyrinths set in floors or on the ground are large enough that the path can be walked. They have been used historically both in group ritual and for private meditation.
Ancient labyrinths.
"Labyrinth" is a word of pre-Greek (Minoan) origin, which the Greeks used for the palace of Knossos in Crete, and it is derived from the Lydian word "labrys" ("double-edged axe"). This was a symbol of royal power, which suggests that the labyrinth was originally the royal Minoan palace in Crete and meant "palace of the double-axe" (the suffix -nth as in Korinth). This designation cannot be limited to the palace of Knossos, because the same symbols were discovered in other palaces of Crete.
Pliny's "Natural History" mentions four ancient labyrinths: the Cretan labyrinth, an Egyptian labyrinth, a Lemnian labyrinth, and an Italian labyrinth.
'Labrys' was a cult-word introduced from Anatolia. In Labraunda of Caria the double-axe accompanies the storm-god Zeus Labraundos (Ζεὺς Λαβρυάνδις). It also accompanies the Hurrian god of sky and storm Teshub (his Hittite and Luwian name was Tarhun). A lot of these symbols were found in the Minoan palaces in Crete, and they usually accompanied goddesses. It seems that the double-axe was the symbol of the beginning ("arche") of the creation.
The goddess of the double-axe probably presided over the Minoan palaces, and especially over the palace of Knossos. The Linear B (Mycenaean) inscription 𐀅𐁆𐀪𐀵𐀍𐄀𐀡𐀴𐀛𐀊 on tablet ΚΝ Gg 702, is interpreted as "da-pu2-ri-to-jo,po-ti-ni-ja" ("labyrinthoio potnia", "Mistress of the labyrinth), and she was undoubtedly the goddess of the palace. The word "daburinthos" ("labyrinthos") may possibly show the same equivocation between initial "d-" and "l-" as is found in the variation of the early Hittite royal name "Tabarna" / "Labarna" (where written "t-" may represent phonetic "d-").
The complex palace of Knossos in Crete is usually implicated, though the actual dancing ground, depicted in frescoes at Knossos, has not been found. Something was being shown to visitors as a labyrinth at Knossos in the 1st century AD (Philostratos, "De vita Apollonii Tyanei" iv.34).
The labyrinth is the referent in the familiar Greek patterns of the endlessly running meander, to give the "Greek key" its common modern name. In the 3rd century BC, coins from Knossos were still struck with the labyrinth symbol. The predominant labyrinth form during this period is the simple seven-circuit style known as the "classical" labyrinth.
The term "labyrinth" came to be applied to any unicursal maze, whether of a particular circular shape (illustration) or rendered as square. At the center, a decisive turn brought one out again. In Plato's dialogue "Euthydemus", Socrates describes the labyrinthine line of a logical argument:
"Then it seemed like falling into a labyrinth: we thought we were at the finish, but our way bent round and we found ourselves as it were back at the beginning, and just as far from that which we were seeking at first." ... Thus the present-day notion of a labyrinth as a place where one can lose [his] way must be set aside. It is a confusing path, hard to follow without a thread, but, provided [the traverser] is not devoured at the midpoint, it leads surely, despite twists and turns, back to the beginning.
Cretan labyrinth.
Knossos has been supposed since Classical times to be the site of the labyrinth. When the Bronze Age site at Knossos was excavated by explorer Arthur Evans, he found various bull motifs, including an image of a man leaping over the horns of a bull, as well as depictions of a labrys carved into the walls. On the strength of a passage in the "Iliad", it has been suggested that the palace was the site of a dancing-ground made for Ariadne by the craftsman Daedalus, where young men and women, of the age of those sent to Crete as prey for the Minotaur, would dance together. By extension, in popular legend the palace is associated with the myth of the Minotaur.
In the 2000s, archaeologists explored other potential sites of the labyrinth. Oxford University geographer Nicholas Howarth believes that 'Evans’s hypothesis that the palace of Knossos is also the Labyrinth must be treated sceptically.' Howarth and his team conducted a search of an underground complex known as the Skotino cave but concluded that it was formed naturally. Another contender is a series of underground tunnels at Gortyn, accessed by a narrow crack but expanding into interlinking caverns. Unlike the Skotino cave, these caverns have smooth walls and columns, and appear to have been at least partially man-made. This site corresponds to an unusual labyrinth symbol on a 16th-century map of Crete contained in a book of maps in the library of Christ Church, Oxford. A map of the caves themselves was produced by the French in 1821. The site was also used by German soldiers to store ammunition during the Second World War. Howarth's investigation was shown on a documentary produced for the National Geographic Channel.
Herodotus' Egyptian labyrinth.
Even more generally, "labyrinth" might be applied to any extremely complicated maze-like structure. Herodotus, in Book II of his "Histories", describes as a "labyrinth" a building complex in Egypt, "near the place called the City of Crocodiles", that he considered to surpass the pyramids:
It has twelve covered courts — six in a row facing north, six south — the gates of the one range exactly fronting the gates of the other. Inside, the building is of two storeys and contains three thousand rooms, of which half are underground, and the other half directly above them. I was taken through the rooms in the upper storey, so what I shall say of them is from my own observation, but the underground ones I can speak of only from report, because the Egyptians in charge refused to let me see them, as they contain the tombs of the kings who built the labyrinth, and also the tombs of the sacred crocodiles. The upper rooms, on the contrary, I did actually see, and it is hard to believe that they are the work of men; the baffling and intricate passages from room to room and from court to court were an endless wonder to me, as we passed from a courtyard into rooms, from rooms into galleries, from galleries into more rooms and thence into yet more courtyards. The roof of every chamber, courtyard, and gallery is, like the walls, of stone. The walls are covered with carved figures, and each court is exquisitely built of white marble and surrounded by a colonnade.
During the 19th century, the remains of the Labyrinth were discovered "11½ miles from the pyramid of Hawara, in the province of Faioum." The Labyrinth was likely modified and added upon "at various times. The names of more than one king have been found there, the oldest" name being that of Amenemhat III. "It is unnecessary to imagine more than that it was monumental, and a monument of more than one king of Egypt."
In 1898, the "Harpers Dictionary of Classical Antiquities" described the structure as "the largest of all the temples of Egypt, the so-called Labyrinth, of which, however, only the foundation stones have been preserved."
Herodotus' description of the Egyptian Labyrinth inspired some central scenes in Bolesław Prus' 1895 historical novel, "Pharaoh".
Pliny's Lemnian labyrinth.
Pliny the Elder's "Natural History" (36.90) lists the legendary Smilis, reputed to be a contemporary of Daedalus, together with the historical mid-sixth-century BC architects and sculptors Rhoikos and Theodoros as two of the makers of the Lemnian labyrinth, which Andrew Stewart regards as "evidently a misunderstanding of the Samian temple's location "en limnais" ['in the marsh']."
Pliny's Italian labyrinth.
According to Pliny, the tomb of the great Etruscan general Lars Porsena contained an underground maze. Pliny's description of the exposed portion of the tomb is intractable; Pliny, it seems clear, had not observed this structure himself, but is quoting the historian and Roman antiquarian Varro.
Ancient labyrinths outside Europe.
At about the same time as the appearance of the Greek labyrinth, an essentially identical pattern appeared in Native American culture, the Tohono O'odham people labyrinth which features I'itoi, the "Man in the Maze". The Tonoho O'odham pattern has two distinct differences from the Greek: it is radial in design, and the entrance is at the top, where traditional Greek labyrinths have the entrance at the bottom (see below).
A prehistoric petroglyph on a riverbank in Goa shows the same pattern and has been dated to circa 2500 BC. Other examples have been found among cave art in northern India and on a dolmen shrine in the Nilgiri Mountains, but are difficult to date accurately. Early labyrinths in India all follow the Classical pattern; some have been described as plans of forts or cities.
Labyrinths appear in Indian manuscripts and Tantric texts from the 17th century onward. They are often called "Chakravyuha" in reference to an impregnable battle formation described in the ancient Mahabharata epic. Lanka, the capital city of mythic Rāvana, is described as a labyrinth in the 1910 translation of Al-Beruni's "India" (c. 1030 CE) p. 306 (with a diagram on the following page).
By the White Sea, notably on the Solovetsky Islands, there have been preserved more than 30 stone labyrinths. The most remarkable monument is the Stone labyrinths of Bolshoi Zayatsky Island - a group of 13–14 stone labyrinths on 0.4 km2 area of one small island. These labyrinths are thought to be 2,000–3,000 years old.
Labyrinth as pattern.
In antiquity, the less complicated labyrinth pattern familiar from medieval examples was already developed. In Roman floor mosaics, the simple classical labyrinth is framed in the meander border pattern, squared off as the medium requires, but still recognisable. Often an image of the Minotaur appears in the centre of these mosaic labyrinths. Roman meander patterns gradually developed in complexity towards the fourfold shape that is now familiarly known as the medieval form. The labyrinth retains its connection with death and a triumphant return: at Hadrumentum in North Africa (now Sousse), a Roman family tomb has a fourfold labyrinth mosaic floor with a dying minotaur in the center and a mosaic inscription: "Enclosed here, he loses life" (Kern 169; Kerényi fig.31).
Medieval labyrinths and turf mazes.
When the early humanist Benzo d'Alessandria visited Verona before 1310, he noted the ""Laberinthum" which is now called the Arena"; perhaps he was seeing the "cubiculi" beneath the arena's missing floor.
The full flowering of the medieval labyrinth came about from the twelfth through fourteenth centuries with the grand pavement labyrinths of the gothic cathedrals, notably Chartres, Reims and Amiens in northern France. These labyrinths may have originated as symbolic allusion to the Holy City; and some modern thinkers have theorized that prayers and devotions may have accompanied the perambulation of their intricate paths. Although some books (in particular guidebooks) suggest that the mazes on cathedral floors served as substitutes for pilgrimage paths, the earliest attested use of the phrase "chemin de Jerusalem" (path to Jerusalem) dates to the late 18th century when it was used to describe mazes at Reims and Saint-Omer. The accompanying ritual, supposedly involving pilgrims following the maze on their knees while praying, may have been practiced at Chartres during the 17th century. However, no contemporary evidence supports the idea that labyrinths had such a purpose for early Christians. The cathedral labyrinths are thought to be the inspiration for the many turf mazes in the UK, such as survive at Wing, Hilton, Alkborough, and Saffron Walden.
Over the same general period, some 500 or more non-ecclesiastical labyrinths were constructed in Scandinavia. These labyrinths, generally in coastal areas, are marked out with stones, most often in the simple 7- or 11-course classical forms. They often have names which translate as "Troy Town". They are thought to have been constructed by fishing communities: trapping malevolent trolls or winds in the labyrinth's coils might ensure a safe fishing expedition. There are also stone labyrinths on the Isles of Scilly, although none is known to date from before the nineteenth century.
There are examples of labyrinths in many disparate cultures. The symbol has appeared in various forms and media (petroglyphs, classic-form, medieval-form, pavement, turf, and basketry) at some time throughout most parts of the world, from Native North and South America to Australia, Java, India, and Nepal.
Modern labyrinths.
In recent years, there has been a resurgence of interest in the labyrinth symbol, which has inspired a revival in labyrinth building.
Countless video games depict mazes and labyrinths.
On bobsled, luge, and skeleton tracks, a labyrinth is where there are three to four curves in succession without a straight line in between any of the turns.
In modern imagery, the labyrinth of Daedalus is often represented by a multicursal maze, in which one may become lost.
The Argentine writer Jorge Luis Borges was entranced with the idea of the labyrinth, and used it extensively in his short stories (such as "The House of Asterion" in "The Aleph"). His use of it has inspired other authors' works (e.g. Umberto Eco's "The Name of the Rose", Mark Z. Danielewski's "House of Leaves"). Additionally, Roger Zelazny's fantasy series, "The Chronicles of Amber", features a labyrinth, called "the Pattern", which grants those who walk it the power to move between parallel worlds. The avant-garde multi-screen film, "In the Labyrinth", presents a search for meaning in a symbolic modern labyrinth. In Rick Riordan's series Percy Jackson & the Olympians, the events of the fourth novel "The Battle of the Labyrinth" predominantly take place within the labyrinth of Daedalus, which has followed the heart of the West to settle beneath the United States. Australian author Sara Douglass incorporated some labyrinthine ideas in her series The Troy Game, in which the Labyrinth on Crete is one of several in the ancient world, created with the cities as a source of magical power. Lawrence Durrell's "The Dark Labyrinth" depicts travelers trapped underground in Crete.
The labyrinth is also treated in contemporary fine arts. Examples include Piet Mondrian's "Dam and Ocean" (1915), Joan Miró's "Labyrinth" (1923), Pablo Picasso's "Minotauromachia" (1935), M. C. Escher's "Relativity" (1953), Friedensreich Hundertwasser's "Labyrinth" (1957), Jean Dubuffet's "Logological Cabinet" (1970), Richard Long's "Connemara sculpture" (1971), Joe Tilson's "Earth Maze" (1975), Richard Fleischner's "Chain Link Maze" (1978), István Orosz's "Atlantis Anamorphosis" (2000), Dmitry Rakov's "Labyrinth" (2003), and Labyrinthine projection by contemporary American artist Mo Morales (2000). The Italian painter Davide Tonato has dedicated many of his artistic works to the labyrinth theme.
In February 2013 it was announced that Mark Wallinger has created a set of 270 enamel plaques of unicursal labyrinth designs, one for every tube station, to mark the 150th anniversary of the London Underground; each will be numbered according to its position in the route taken by the contestants in the 2009 Guinness World Record Tube Challenge.
Cultural meanings.
Prehistoric labyrinths are believed to have served as traps for malevolent spirits or as defined paths for ritual dances. In medieval times, the labyrinth symbolized a hard path to God with a clearly defined center (God) and one entrance (birth). In their cross-cultural study of signs and symbols, "Patterns that Connect", Carl Schuster and Edmund Carpenter present various forms of the labyrinth and suggest various possible meanings, including not only a sacred path to the home of a sacred ancestor, but also, perhaps, a representation of the ancestor him/herself: "...many [New World] Indians who make the labyrinth regard it as a sacred symbol, a beneficial ancestor, a deity. In this they may be preserving its original meaning: the ultimate ancestor, here evoked by two continuous lines joining its twelve primary joints."
One can think of labyrinths as symbolic of pilgrimage; people can walk the path, ascending toward salvation or enlightenment. Many people could not afford to travel to holy sites and lands, so labyrinths and prayer substituted for such travel. Later, the religious significance of labyrinths faded, and they served primarily for entertainment, though recently their spiritual aspect has seen a resurgence.
Many newly made labyrinths exist today, in churches and parks. Modern mystics use labyrinths to help them achieve a contemplative state. Walking among the turnings, one loses track of direction and of the outside world, and thus quiets the mind. The Labyrinth Society provides a locator for modern labyrinths all over the world.
In addition, the labyrinth can serve as a metaphor for situations that are difficult to be extricated from, as an image that suggests getting lost in a subterranean dungeon-like world. Octavio Paz titled his book on Mexican identity "The Labyrinth of Solitude", describing the Mexican condition as orphaned and lost.
Christian use.
Labyrinths have on various occasions been used in Christian tradition as a part of worship. The earliest known example is from a fourth-century pavement at the Basilica of St Reparatus, at Orleansville, Algeria, with the words "Sancta Eclesia" ["sic"] at the centre, though it is unclear how it might have been used in worship.
In medieval times, labyrinths began to appear on church walls and floors around 1000 C.E.. The most famous medieval labyrinth, with great influence on later practice, was created in Chartres Cathedral. The purpose of the labyrinths is not clear, though there are surviving descriptions of French clerics performing a ritual Easter dance along the path on Easter Sunday. Some books (guidebooks in particular) suggest that mazes on cathedral floors originated in the medieval period as alternatives to pilgrimage to the Holy Land, but the earliest attested use of the phrase "chemin de Jerusalem" (path to Jerusalem) dates to the late 18th century when it was used to describe mazes at Reims and Saint-Omer. The accompanying ritual, depicted in Romantic illustrations as involving pilgrims following the maze on their knees while praying, may have been practiced at Chartres during the 17th century.
The use of labyrinths has recently been revived in some contexts of Christian worship. For example, a labyrinth was set up on the floor of St Paul's Cathedral for a week in March 2000.

</doc>
<doc id="18246" url="http://en.wikipedia.org/wiki?curid=18246" title="Lyon &amp; Healy">
Lyon &amp; Healy

Lyon & Healy is an American harp manufacturer based in Chicago, Illinois. Their Chicago headquarters and manufacturing facility contains a showroom and concert hall. George W. Lyon and Patrick J. Healy began the company in 1864 as a sheet music shop. By the end of the 19th century, they manufactured a wide range of musical instruments—including guitars, mandolins, banjos, and various brass and percussion instruments.
Lyon & Healy harps are widely played by professional musicians, since they are one of the few makers of harps for orchestral use—which are known as "concert harps" or "pedal harps". Lyon & Healy also makes smaller "folk harps" or lever harps (based on traditional Irish and Scottish instruments) that use levers to change string pitch instead of pedals. In the 1980s, Lyon & Healy also began to manufacture electroacoustic harps and, later, solid body electric harps.
History.
George W. Lyon and Patrick J. Healy founded the company in 1864, after they moved from Boston to start a sheet music shop for music publisher Oliver Ditson. Determining Lyon & Healy's history is complicated because its building and company records were destroyed in two fires, including the Great Chicago Fire of 1871. Two smaller fires did little damage to the firm and did not result in data loss.
Company letters and trade catalogs don't provide exact dates that would reveal when Lyon & Healy began manufacturing instruments. An article in the "Musical Courier" states that Lyon & Healy began manufacturing instruments in 1885. Clearly, Lyon & Healy was making fretted string instruments in the 1880s, with Washburn (guitars, mandolins, banjos, and zithers) as their premier line. By the 1900s, if not earlier, Lyon & Healy might well have been manufacturing bowed string instruments.
Lyon & Healy also made various percussion instruments. Later, Lyon & Healy began manufacturing brass instruments, possibly as early as the 1890s. Lyon & Healy also repaired instruments, and offered engraving services. Complicating matters still further, Lyon & Healy engraved instruments that it retailed but did not actually manufacture. In its 1892 catalog, it claimed that it manufactured 100,000 instruments annually.
The company is known to have made other instruments, including reed organs and pianos. Lyon & Healy evidently began manufacturing these instruments around 1876 in its factories in Chicago and nearby cities. George W. Lyon patented his cottage upright in 1878 and it was sold under the Lyon & Healy name.
Lyon retired in 1889, and Healy died in 1905.
Lyon & Healy built their first harp in 1889. Healy wanted to develop a harp better suited to the rigors of the American climate than available European models. They successfully produced a harp notable for its strength, pitch reliability, and freedom from unwanted vibration. Previously, most harps in North America where made by small groups of craftsmen in France, England, Ireland, or Italy.
In 1890, Lyon & Healy introduced the Style 23 Harp, still a popular and recognizable design. It has 47 strings, highly decorative floral carving on the top of the column, base, and feet, and a fleur de lis pattern at the bottom of the column. It is available in a gold version. It is 74 in tall, and weighs about 37 kg. Lyon & Healy produces one of the most ornate and elaborate harps in the world, the Louis XV, which includes carvings of leaves, flowers, scrolls, and shells along its neck and kneeblock, as well the soundboard edges.
In the 1890s the company—which used the slogan,"Everything in music"—began building pipe organs. In 1894 Robert J. Bennett came to Lyon & Healy from the Hutchens company of Boston to head their organ department. The largest surviving Lyon & Healy pipe organ is at the Our Lady of Sorrows Basilica in Chicago. It is a large organ of four manuals and 57 ranks of pipes.
They also made small pipe organs. An example survives at St. Mary's Catholic Church in Aspen Colorado. It is a two manual tracker with a 30 note straight pedalboard and 7 ranks. It is believed to have been built around 1900, and can still be pumped by hand.
By the 1900s, Lyon & Healy was one of the largest music publishers in the world, and was a major producer of musical instruments. However, In late 1920s, Lyon & Healy sold its brass musical instrument manufacturing branch (see "New Langwill Index"). In the 1970s, the firm concentrated solely upon making and selling harps.
In 1928, Lyon & Healy introduced one of the most unusual harps ever mass-produced, the "Salzedo Model". The company designed it in collaboration with the harpist Carlos Salzedo. It an Art Deco style instrument that incorporates bold red and white lines on the soundboard to create a stylized and distinct appearance.
In the 1960s, Lyon & Healy introduced a smaller lever harp, the "Troubadour", a 36-string harp for young beginners with smaller hands, and for casual players. This harp stands 65.5 in, and weighs 17 kg.
In the late 1970s, Steinway & Sons (then owned by CBS) purchased Lyon & Healy and soon after closed all retail stores—that sold sheet music and musical instruments, and their education departments—to focus on harp production.
By 1985, Lyon & Healy also made folk harps, also known as "Irish harps", which are even smaller than the Troubadour. The "Shamrock model folk harp" has 34 strings. It stands 55 in tall with its legs. The legs can be removed so the player can hold the instrument lap—style on the knees. It weighs about 10 kg. It features Celtic designs on the soundboard. An Irish or folk harp player is sometimes called a "harper" rather than "harpist". 
DePaul University now owns the Wabash building. Lyon & Healy harps are still in Chicago, Illinois, at 168 North Ogden Avenue. The building was once home to the recording studios of Orlando R. Marsh.
The firm still exists in one of its buildings producing a range of harps, including a relatively new addition an electronic harp.
Craftsmanship.
Wood in harp construction varies by instrument, but Sitka Spruce (Picea sitchensis) is the most common soundboard wood. Various Lyon & Healy guitars, mandolins, and many other instrument types reside in major musical instrument museums in the U.S. and Europe.
Lyon and Healy now primarily manufactures four types of harps—the "level harp", "petite pedal harp", "semi-grande pedal harp", and "concert grand harp". They also make limited numbers of "special harps" called "concert grands". Lyon & Healy makes electric lever harps in nontraditional colors such as pink, green, blue, and red.

</doc>
<doc id="18247" url="http://en.wikipedia.org/wiki?curid=18247" title="Index of philosophy articles (A–C)">
Index of philosophy articles (A–C)


</doc>
<doc id="18271" url="http://en.wikipedia.org/wiki?curid=18271" title="Lamborghini">
Lamborghini

Automobili Lamborghini S.p.A. (]) is an Italian brand and manufacturer of luxury sports cars and, formerly, SUVs, which is owned by the Volkswagen Group through its subsidiary brand division Audi. Lamborghini's production facility and headquarters are located in Sant'Agata Bolognese, Italy. In 2011, Lamborghini's 831 employees produced 1,711 vehicles.
Ferruccio Lamborghini, an Italian manufacturing magnate, founded Automobili Ferruccio Lamborghini S.p.A. in 1963 to compete with established marques, including Ferrari. The company gained wide acclaim in 1966 for the Miura sports coupé, which established rear mid-engine, rear wheel drive as the standard layout for high-performance cars of the era. Lamborghini grew rapidly during its first decade, but sales plunged in the wake of the 1973 worldwide financial downturn and the oil crisis. The firm's ownership changed three times after 1973, including a bankruptcy in 1978. American Chrysler Corporation took control of Lamborghini in 1987 and sold it to Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation in 1994. In 1998, Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group where it was placed under the control of the group's Audi division.
New products and model lines were introduced to the brand's portfolio and brought to the market and saw an increased productivity for the brand Lamborghini. In the late 2000s, during the worldwide financial crisis and the subsequent economic crisis, Lamborghini's sales saw a drop of nearly 50 percent.
Lamborghini produces sports cars and V12 engines for offshore powerboat racing. Lamborghini currently produces the V12-powered Aventador and the V10-powered Huracán.
History.
Manufacturing magnate Italian Ferruccio Lamborghini founded the company in 1963 with the objective of producing a refined grand touring car to compete with offerings from established marques such as Ferrari. The company's first models were released in the mid-1960s and were noted for their refinement, power and comfort. Lamborghini gained wide acclaim in 1966 for the Miura sports coupé, which established rear mid-engine, rear wheel drive as the standard layout for high-performance cars of the era.
Lamborghini grew rapidly during its first ten years, but sales plunged in the wake of the 1973 worldwide financial downturn and the oil crisis. Ferruccio Lamborghini sold ownership of the company to Georges-Henri Rossetti and René Leimer and retired in 1974. The company went bankrupt in 1978, and was placed in the receivership of brothers Jean-Claude and Patrick Mimran in 1980. The Mimrans purchased the company out of receivership by 1984 and invested heavily in the company's expansion. Under the Mimrans' management, Lamborghini's model line was expanded from the Countach to include the Jalpa sports car and the LM002 high performance off-road vehicle.
The Mimrans sold Lamborghini to the Chrysler Corporation in 1987. After replacing the Countach with the Diablo and discontinuing the Jalpa and the LM002, Chrysler sold Lamborghini to Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation in 1994. In 1998, Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group where it was placed under the control of the group's Audi division. New products and model lines were introduced to the brand's portfolio and brought to the market and saw an increased productivity for the brand Lamborghini. In the late 2000s, during the worldwide financial crisis and the subsequent economic crisis, Lamborghini's sales saw a drop of nearly 50 percent.
Products.
Automobiles.
As of the 2015 model year, Lamborghini's automobile product range consists of two model lines, both of which are mid-engine two-seat sports cars. The V12-powered Aventador line consists of the LP 700–4 coupé and roadster. The V10-powered Huracán line currently includes only the LP 610-4 coupé.
Marine engines.
Motori Marini Lamborghini produces a large V12 marine engine block for use in World Offshore Series Class 1 powerboats. A Lamborghini branded marine engine displaces approximately 8171 cc and outputs approximately 940 hp.
Lamborghini motorcycle.
In the mid-1980s, Lamborghini produced a limited-production run of a 1,000 cc sports motorcycle. UK weekly newspaper "Motor Cycle News" reported in 1994 – when featuring an example available through an Essex motorcycle retailer - that 24 examples were produced with a Lamborghini alloy frame having adjustable steering head angle, Kawasaki GPz1000RX engine/transmission unit, Ceriani front forks and "Marvic" wheels. The bodywork was plastic and fully integrated with front fairing merged into fuel tank and seat cover ending in a rear tail-fairing. The motorcycles were designed by Lamborghini stylists and produced by French business "Boxer Bikes".
Branded merchandise.
Lamborghini licenses its brand to manufacturers that produce a variety of Lamborghini-branded consumer goods including scale models, clothing, accessories, bags and electronics.
Motorsport.
In contrast to his rival Enzo Ferrari, Ferruccio Lamborghini had decided early on that there would be no factory-supported racing of Lamborghinis, viewing motorsport as too expensive and too draining on company resources. This was unusual for the time, as many sports car manufacturers sought to demonstrate the speed, reliability, and technical superiority through motorsport participation. Enzo Ferrari in particular was known for considering his road car business mostly a source of funding for his participation in motor racing. Ferruccio's policy led to tensions between him and his engineers, many of whom were racing enthusiasts; some had previously worked at Ferrari. When Dallara, Stanzani, and Wallace began dedicating their spare time to the development of the P400 prototype, they designed it to be a road car with racing potential, one that could win on the track and also be driven on the road by enthusiasts. When Ferruccio discovered the project, he allowed them to go ahead, seeing it as a potential marketing device for the company, while insisting that it would not be raced. The P400 went on to become the Miura. The closest the company came to building a true race car under Lamborghini's supervision were a few highly modified prototypes, including those built by factory test driver Bob Wallace, such as the Miura SV-based "Jota" and the Jarama S-based "Bob Wallace Special".
In the mid-1970s, while Lamborghini was under the management of Georges-Henri Rossetti, Lamborghini entered into an agreement with BMW to develop, then manufacture 400 cars for BMW in order to meet Group 4 homologation requirements. BMW lacked experience developing a mid-engined vehicle and believed that Lamborghini's experience in that area would make Lamborghini an ideal choice of partner. Due to Lamborghini's shaky finances, Lamborghini fell behind schedule developing the car's structure and running gear. When Lamborghini failed to deliver working prototypes on time, BMW took the program in house, finishing development without Lamborghini. BMW contracted with Baur to produce the car, which BMW named the M1, delivering the first vehicle in October 1978.
In 1985, Lamborghini's British importer developed the Countach QVX, in conjunction with Spice Engineering, for the 1986 Group C championship season. One car was built, but lack of sponsorship caused it to miss the season. The QVX competed in only one race, the non-championship 1986 Southern Suns 500 km race at Kyalami in South Africa, driven by Tiff Needell. Despite the car finishing better than it started, sponsorship could once again not be found and the programme was cancelled.
Lamborghini was an engine supplier in Formula One between the 1989 and 1993 Formula One seasons. It supplied engines to Larrousse (1989–1990,1992–1993), Lotus (1990), Ligier (1991), Minardi (1992), and to the Modena team in 1991. While the latter is commonly referred to as a factory team, the company saw themselves as a supplier, not a backer. The 1992 Larrousse–Lamborghini was largely uncompetitive but noteworthy in its tendency to spew oil from its exhaust system. Cars following closely behind the Larrousse were commonly coloured yellowish-brown by the end of the race. Lamborghini's best result was achieved with Larrousse at the 1990 Japanese Grand Prix, when Aguri Suzuki finished third on home soil.
In late 1991, a Lamborghini Formula One motor was used in the Konrad KM-011 Group C sports car, but the car only lasted a few races before the project was canceled. The same engine, re-badged a Chrysler, Lamborghini's then-parent company, was tested by McLaren towards the end of the 1993 season, with the intent of using it during the 1994 season. Although driver Ayrton Senna was reportedly impressed with the engine's performance, McLaren pulled out of negotiations, choosing a Peugeot engine instead, and Chrysler ended the project.
Two racing versions of the Diablo were built for the Diablo Supertrophy, a single-model racing series held annually from 1996 to 1999. In the first year, the model used in the series was the Diablo SVR, while the Diablo 6.0 GTR was used for the remaining three years. Lamborghini developed the Murciélago R-GT as a production racing car to compete in the FIA GT Championship, the Super GT Championship and the American Le Mans Series in 2004. The car's highest placing in any race that year was the opening round of the FIA GT Championship at Valencia, where the car entered by Reiter Engineering finished third from a fifth-place start. In 2006, during the opening round of the Super GT championship at Suzuka, a car run by the Japan Lamborghini Owners Club garnered the first victory (in class) by an R-GT. A GT3 version of the Gallardo has been developed by Reiter Engineering. A Murciélago R-GT entered by All-Inkl.com racing, driven by Christophe Bouchut and Stefan Mücke, won the opening round of the FIA GT Championship held at Zhuhai International Circuit, achieving the first major international race victory for Lamborghini.
Complete Formula One results.
() (results in bold indicate pole position) 
Marketing.
Brand identity.
The world of bullfighting is a key part of Lamborghini's identity. In 1962, Ferruccio Lamborghini visited the Seville ranch of Don Eduardo Miura, a renowned breeder of Spanish fighting bulls. Lamborghini, a Taurus himself, was so impressed by the majestic Miura animals that he decided to adopt a raging bull as the emblem for the automaker he would open shortly.
Vehicle nomenclature.
After producing two cars with alphanumeric designations, Lamborghini once again turned to the bull breeder for inspiration. Don Eduardo was filled with pride when he learned that Ferruccio had named a car for his family and their line of bulls; the fourth Miura to be produced was unveiled to him at his ranch in Seville.
The automaker would continue to draw upon the bullfighting connection in future years. The Islero was named for the Miura bull that killed the famed bullfighter Manolete in 1947. "Espada" is the Spanish word for sword, sometimes used to refer to the bullfighter himself. The Jarama's name carried a special double meaning; though it was intended to refer only to the historic bullfighting region in Spain, Ferruccio was concerned about confusion with the also historic Jarama motor racing track.
After christening the Urraco after a bull breed, in 1974, Lamborghini broke from tradition, naming the Countach not for a bull, but for contacc (pronounced ]), a Piedmontese expletive. Legend has it that stylist Nuccio Bertone uttered the word in surprise when he first laid eyes on the Countach prototype, "Project 112". The LM002 (LM for Lamborghini Militaire) sport utility vehicle and the Silhouette (named after the popular racing category of the time) were other exceptions to the tradition.
The Jalpa of 1982 was named for a bull breed; Diablo, for the Duke of Veragua's ferocious bull famous for fighting an epic battle against "El Chicorro" in Madrid in 1869; Murciélago, the legendary bull whose life was spared by "El Lagartijo" for his performance in 1879; Gallardo, named for one of the five ancestral castes of the Spanish fighting bull breed; and Reventón, the bull that defeated young Mexican "torero" Félix Guzmán in 1943. The Estoque concept of 2008 was named for the estoc, the sword traditionally used by "matadors" during bullfights.
Concept vehicles.
Throughout its history, Lamborghini has envisioned and presented a variety of concept cars, beginning in 1963 with the very first Lamborghini prototype, the 350GTV. Other famous models include Bertone's 1967 Marzal, 1974 Bravo, and 1980 Athon, Chrysler's 1987 Portofino, the Italdesign-styled Cala from 1995, the Zagato-built Raptor from 1996.
A retro-styled Lamborghini Miura concept car, the first creation of chief designer Walter de'Silva, was presented in 2006. President and CEO Stephan Winkelmann denied that the concept would be put into production, saying that the Miura concept was "a celebration of our history, but Lamborghini is about the future. Retro design is not what we are here for. So we won’t do the [new] Miura.”
At the 2008 Paris Motor Show, Lamborghini revealed the Estoque, a four-door sedan concept. Although there had been much speculation regarding the Estoque's eventual production, Lamborghini management has not made a decision regarding production of what might be the first four-door car to roll out of the Sant'Agata factory.
At the 2010 Paris Motor Show, Lamborghini unveiled the Sesto Elemento. The concept car is made almost entirely of carbon fibre making it extremely light, weighing only 999 kg. The Sesto Elemento shares the same V10 engine found in the Lamborghini Gallardo. Lamborghini hopes to signal a shift in the company's direction from making super cars focused on top speed to producing more agile, track focused cars with the Sesto Elemento. The concept car can reach 0–62 in 2.5 seconds and can reach a top speed of over 180 mph.
At the 2012 Geneva Motor Show, Lamborghini unveiled the Aventador J – a roofless, windowless version of the Lamborghini Aventador. The Aventador J uses the same 700 hp engine and seven-speed transmission as the standard Aventador.
At the 2012 Beijing Motor Show, Lamborghini unveiled the Urus SUV. This is the first SUV By Lamborghini since the LM002.
As part of the celebration of 50 years of Lamborghini, the company unveiled the Egoista. Egoista is for one person's driving and only one of Egoista is to be made.
At the 2014 Paris Motor Show, Lamborghini unveiled the Asterion LPI910-4 hybrid concept car. This is the first hybrid Lamborghini in the history of the company ("Asterion" was the traditional proper name of another hybrid – namely, the half-man, half-bull Minotaur of Greek legend). Utilizing the Huracán's 5.2 litre V10 producing 607 horsepower, along with one electric motor mounted on the transaxle and an additional two on the front axle, developing an additional 300 horsepower. This puts the power at a combined figure of 907 horsepower. 0–100 km/h is claimed to be "just above 3 seconds," with a claimed top speed of 185 mph.
Corporate affairs.
Structure.
As of 2011, Lamborghini is structured as a wholly owned subsidiary of AUDI AG named Automobili Lamborghini S.p.A.
Automobili Lamborghini S.p.A. controls five principal subsidiaries: Ducati Motor Holding S.p.A., a manufacturer of motorcycles; Italdesign Giugiaro S.p.A., a 90.1%-owned design and prototyping firm that provides services to the entire Volkswagen Group; MML S.p.A. (Motori Marini Lamborghini), a manufacturer of marine engine blocks; and VOLKSWAGEN GROUP ITALIA S.p.A. (formerly AUTOGERMA S.p.A.), which sells Audi and other Volkswagen Group vehicles in Italy.
Sales results.
By sales, the most important markets in 2004 for Lamborghini's sports cars are the U.S. (41%), Germany (13%), Great Britain (9%) and Japan (8%). Prior to the launch of the Gallardo in 2003, Lamborghini produced approximately 400 vehicles per year; in 2011 Lamborghini produced 1,711 vehicles.
Licensing.
Automóviles Lamborghini Latinoamérica.
Automóviles Lamborghini Latinoamérica S.A. de C.V. (Lamborghini Automobiles of Latin America Public Limited Company) is an authorized distributor and manufacturer of Lamborghini-branded vehicles and merchandise in Latin America and South America.
In 1995, Indonesian corporation MegaTech, Lamborghini's owner at the time, entered into distribution and license agreements with Mexican businessman Jorge Antonio Fernandez Garcia. The agreements give Automóviles Lamborghini Latinoamérica S.A. de C.V. the exclusive distributorship of Lamborghini vehicles and branded merchandise in Latin America and South America. Under the agreements, Automóviles Lamborghini is also allowed to manufacture Lamborghini vehicles and market them worldwide under the Lamborghini brand.
Automóviles Lamborghini has produced two rebodied versions of the Diablo called the Eros and the Coatl. Automóviles Lamborghini plans to start producing auto parts in the Argentine province of Santiago del Estero in 2011, followed by production of cars in 2012.
Museo Lamborghini.
This two story museum is attached to the headquarters, and covers the history of Lamborghini cars and sport utility vehicles, showcasing a variety of modern and vintage models. The museum uses displays of cars, engines and photos to provide a history and review important milestones of Lamborghini.
External links.
class="navbox collapsible autocollapse"
!colspan="32" class="navbox-title"

</doc>
<doc id="18272" url="http://en.wikipedia.org/wiki?curid=18272" title="LaGrand case">
LaGrand case

The LaGrand case was a legal action heard before the International Court of Justice (ICJ) which concerned the Vienna Convention on Consular Relations. In the case the ICJ found that its own temporary court orders were legally binding and that the rights contained in the convention could not be denied by the application of domestic legal procedures.
Background.
On January 7, 1982, brothers Karl-Heinz and Walter Bernhard LaGrand bungled an armed bank robbery in Marana, Arizona, United States, killing a man and severely injuring a woman in the process. They were subsequently charged and convicted of murder and sentenced to death. The LaGrands were German nationals, having been born in Germany. While they had both lived in the United States since they were four and five, respectively, neither had officially obtained U.S. citizenship. As foreigners the LaGrands should have been informed of their right to consular assistance, under the Vienna Convention, from their state of nationality, Germany. However the Arizona authorities failed to do this even after they became aware that the LaGrands were German nationals. The LaGrand brothers later contacted the German consulate of their own accord, having learned of their right to consular assistance. They appealed their sentences and convictions on the grounds that they were not informed of their right to consular assistance, and that with consular assistance they might have been able to mount a better defense. The federal courts rejected their argument on grounds of procedural default, which provides that issues cannot be raised in federal court appeals unless they have first been raised in state courts.
Diplomatic efforts, including pleas by German ambassador Jürgen Chrobog and German Member of Parliament Claudia Roth, and the recommendation of Arizona's clemency board, failed to sway Arizona Governor Jane Dee Hull, who insisted that the executions be carried out. Karl LaGrand was subsequently executed by the state of Arizona on February 24, 1999, by lethal injection. Walter LaGrand was then executed March 3, 1999, by lethal gas.
The case.
Germany then initiated legal action in the International Court of Justice against the United States regarding Walter LaGrand. Hours before Walter LaGrand was due to be executed, Germany applied for the Court to grant a provisional court order, requiring the United States to delay the execution of Walter LaGrand, which the court granted.
Germany then initiated action in the U.S. Supreme Court for enforcement of the provisional order. In its judgment, the U.S. Supreme Court held that it lacked jurisdiction with respect to Germany's complaint against Arizona due to the Eleventh Amendment of the U.S. constitution, which prohibits federal courts from hearing lawsuits of foreign states against a U.S. state. With respect to Germany's case against the United States, it held that the doctrine of procedural default was not incompatible with the Vienna Convention, and that even if procedural default did conflict with the Vienna Convention it had been overruled by later federal law – the Antiterrorism and Effective Death Penalty Act of 1996, which explicitly legislated the doctrine of procedural default. (Subsequent federal legislation overrides prior self-executing treaty provisions, "Whitney v. Robertson", 124 U.S. (1888)).
The U.S. Solicitor General sent a letter to the Supreme Court, as part of these proceedings, arguing that provisional measures of the International Court of Justice are not legally binding. The United States Department of State also conveyed the ICJ's provisional measure to the Governor of Arizona without comment. The Arizona clemency board recommended a stay to the governor, on the basis of the pending ICJ case; but the governor of Arizona ignored the recommendation and Walter LaGrand was executed on March 3, 1999. As of 2013 this is the last use of lethal gas in the U.S., although five states still permit its use in varying circumstances. 
Germany then modified its complaint in the case before the ICJ, alleging furthermore that the U.S. violated international law by failing to implement the provisional measures. In opposition to the German submissions, the United States argued that the Vienna Convention did not grant rights to individuals, only to states; that the convention was meant to be exercised subject to the laws of each state party, which in the case of the United States meant subject to the doctrine of procedural default; and that Germany was seeking to turn the ICJ into an international court of criminal appeal.
ICJ decision.
On June 27, 2001, the ICJ, rejecting all of the United States' arguments, ruled in favor of Germany. The ICJ held that the Vienna Convention on Consular Relations of 24 April 1963 (Vienna Convention) granted rights to individuals on the basis of its plain meaning, and that domestic laws could not limit the rights of the accused under the convention, but only specify the means by which those rights were to be exercised. The ICJ also found that its own provisional measures were legally binding. The nature of provisional measures has been a subject of great dispute in international law; the English text of the Statute of the International Court of Justice implies they are not binding, while the French text implies that they are. Faced with a contradiction between two equally authentic texts of the statute, the court considered which interpretation better served the objects and purposes of the statute, and hence found that they are binding. This was the first time in the court's history it had ruled as such.
The court also found that the United States violated the Vienna Convention through its application of procedural default. The court was at pains to point out that it was not passing judgment on the doctrine itself, but only its application to cases involving the Vienna Convention.

</doc>
<doc id="18273" url="http://en.wikipedia.org/wiki?curid=18273" title="Lotus 1-2-3">
Lotus 1-2-3

Lotus 1-2-3 is a spreadsheet program from Lotus Software (now part of IBM). It was the IBM PC's first "killer application", was hugely popular in the 1980s and contributed significantly to the success of the IBM PC. 
The first spreadsheet, Visicalc, had helped launch the Apple II as one of the earliest personal computers in business use. With IBM's entry into the market, VisiCalc was slow to respond, and when they did, they launched what was essentially a straight port of their existing system in spite of the greatly expanded hardware capabilities. Lotus' solution was marketed as a three-in-one, integrated solution, which handled spreadsheet calculations, database functionality, and graphical charts. Thus the name "1-2-3", though how much database capability was debatable given Lotus' spare memory. 1-2-3 quickly overtook VisiCalc, as well as MultiPlan and SuperCalc, two VisiCalc competitors.
1-2-3 was the spreadsheet standard throughout the 1980s and into the 1990s, part of a suite of three office automation products that included dBASE and WordPerfect, to build a complete business platform. With the acceptance of Windows 3.0, the market for desktop software grew even more. None of the major spreadsheet developers had seriously considered the graphical user interface to supplement their DOS offerings, and so they responded slowly to Microsoft's own graphical based products, Excel and Word. Lotus was passed by Microsoft in the early 1990s and never recovered. IBM purchased Lotus and continued to sell Lotus offerings, only officially ending sales in 2013. 
History.
VisiCalc.
VisiCalc was launched in 1979 on the Apple II and immediately became a best-seller. Compared to earlier programs, VisiCalc allowed one to easily construct free-form calculation systems for practically any purpose, the limitations being primarily memory and speed related. The application was so compelling that there were numerous stories of people buying Apple II's to run the program. VisiCalc's runaway success on the Apple led to direct bug compatible ports to other platforms, including the Atari 8-bit family, Commodore PET and many others. This included the IBM PC when it launched in 1981, where it quickly became another best-seller, with an estimated 300,000 sales in the first six months on the market.
There were well known problems with VisiCalc, and several competitors appeared to address some of these issues. One early example was 1980's SuperCalc, which solved the problem of circular references, while a slightly later example was Microsoft Multiplan from 1981, which offered larger sheets and other improvements. In spite of these, and others, VisiCalc continued to outsell them all.
Beginnings.
The Lotus Development Corporation was founded by Mitchell Kapor, a friend of the developers of VisiCalc. 1-2-3 was originally written by Jonathan Sachs, who had written two spreadsheet programs previously while working at Concentric Data Systems, Inc. To aid its growth, in the UK, and possibly elsewhere, Lotus 1-2-3 was the very first computer software to use television consumer advertising.
1-2-3 was released on January 26, 1983, and immediately overtook Visicalc in sales. Unlike Microsoft Multiplan, it stayed very close to the model of VisiCalc, including the "A1" letter and number cell notation, and slash-menu structure. It was cleanly programmed and relatively bug-free, gained speed from being written completely in x86 assembly language (this remained the case for all DOS versions until 3.0, when Lotus switched to C) and wrote directly to video memory rather than use the slow DOS and/or BIOS text output functions.
The reliance on the specific hardware of the IBM PC led to 1-2-3 being utilized as one of the two stress test applications, with Microsoft Flight Simulator, for true 100% compatibility when PC clones started to appear in the early 1980s. 1-2-3 required two disk drives and at least 192K of memory, which made it incompatible with the IBM PCjr; Lotus produced a version for the PCjr that was on two cartridges but otherwise identical.
By early 1984 the software was a killer app for the IBM PC and compatibles, while hurting sales of computers that could not run it. "They're looking for 1-2-3. Boy, are they looking for 1-2-3!" "InfoWorld" wrote. Noting that computer purchasers did not want PC compatibility as much as compatibility with certain PC software, the magazine suggested "let's tell it like it is. Let's not say 'PC compatible,' or even 'MS-DOS compatible.' Instead, let's say '1-2-3 compatible.'" An Apple II software company promised that its spreadsheet had "the power of 1-2-3". Because spreadsheets use large amounts of memory, 1‐2‐3 helped popularize greater RAM capacities in PCs, and especially the advent of expanded memory, which allowed greater than 640k to be accessed.
Rivals.
Lotus 1-2-3 inspired imitators, the first of which was Mosaic Software's "The Twin", written in the fall of 1985 largely in the C language, followed by VP-Planner, which was backed by Adam Osborne. These were able to not only read 1-2-3 files, but also execute many or most macro programs by incorporating the same command structure. Copyright law had first been understood to only cover the source code of a program. After the success of lawsuits which claimed that the very "look and feel" of a program were covered, Lotus sought to ban any program which had a compatible command and menu structure. Program commands had not been considered to be covered before, but the commands of 1-2-3 were embedded in the words of the menu displayed on the screen. 1-2-3 won its case against Mosaic Software. However when they sued Borland over its Quattro Pro spreadsheet in Lotus v. Borland, the courts ruled that it was not a copyright violation to merely have a compatible command menu or language. In 1995, the First Circuit found that command menus are an uncopyrightable "method of operation" under section 102(b) of the Copyright Act. The 1-2-3 menu structure (example, slash File Erase) was itself an advanced version of single letter menus introduced in VisiCalc.
Decline.
Microsoft's early spreadsheet Multiplan eventually gave way to Excel, which debuted on the Macintosh in 1985. It arrived on PCs with the release of Windows 2.x in 1987, but as Windows was not yet popular, it posed no serious threat to Lotus' stranglehold on spreadsheet sales. However, Lotus suffered technical setbacks in this period. Version 3 of Lotus 1-2-3, fully rewritten from its original macro assembler into the more portable C language, was delayed by more than a year as the totally new 1-2-3 had to be made portable across platforms and fully compatible with existing macro sets and file formats. The inability to fit the larger code size of compiled C into lower-powered machines forced the company to split its spreadsheet offerings, with 1-2-3 release 3 only for higher-end machines, and a new version 2.2, based on the 2.01 assembler code base, available for PCs without extended memory. By the time these versions were released in 1989, Microsoft was well on its way to breaking through Lotus' market share.
During the early 1990s, Windows grew in popularity and along with it Excel, which gradually displaced Lotus from its leading position. A planned total revamp of 1-2-3 for Windows fell apart and all that the company could manage was a Windows adaptation of their existing spreadsheet with no changes except using a graphical interface. Additionally, several versions of 1-2-3 had different features and slightly different interfaces.
1-2-3's intended successor, Lotus Symphony, was Lotus' entry into the anticipated "integrated software" market. It intended to expand the rudimentary all-in-one 1-2-3 into a fully-fledged spreadsheet, graph, database and word processor for DOS, but none of the integrated packages ever really succeeded. 1-2-3 migrated to the Windows platform, as part of Lotus SmartSuite.
IBM's continued development and marketing of Lotus SmartSuite and OS/2 during the 1990s placed it in direct competition with Microsoft Office and Microsoft Windows, respectively. As a result, Microsoft "punished the IBM PC Company with higher prices, a late license for Windows 95, and the withholding of technical and marketing support." IBM wasn't granted OEM rights for Windows 95 until 15 minutes prior to the release of Windows 95, August 24, 1995. Because of this uncertainty, IBM machines were sold without Windows 95, while Compaq, HP, and other companies sold machines with Windows 95 from day one.
On June 11, 2013, IBM announced it would withdraw the Lotus brand: IBM Lotus 123 Millennium Edition V9.x, IBM Lotus SmartSuite 9.x V9.8.0, and Organizer V6.1.0. IBM stated, "Customers will no longer be able to receive support for these offerings after September 30, 2014. No service extensions will be offered. There will be no replacement programs." 
User features.
The name "1-2-3" stemmed from the product's integration of three main capabilities. Along with being a spreadsheet, it also offered integral charting/graphing and rudimentary database operations.
Data features included sorting data in any defined rectangle, by order of information in one or two columns in the rectangular area. Justifying text in a range into paragraphs allowed it to be used as a primitive word processor.
It had keyboard-driven pop-up menus as well as one-key commands, making it fast to operate. It was also user-friendly, introducing an early instance of context-sensitive help accessed by the F1 key.
Macros in version one and add-ins (introduced in version 2.0) contributed much to 1-2-3's popularity, allowing dozens of outside vendors to sell macro packages and add-ins ranging from dedicated financial worksheets like F9 to full-fledged word processors. In the single-tasking MS-DOS, 1-2-3 was sometimes used as a complete office suite. All major graphics standards were supported; initially CGA and Hercules, and later EGA, AT&T, and VGA. Early versions used the filename extension "WKS". In version 2.0, the extension changed first to "WK1", then "WK2". This later became "WK3" for version 3.0 and "WK4" for version 4.0.
Version 2 introduced macros with syntax and commands similar in complexity to an advanced BASIC interpreter, as well as string variable expressions. Later versions supported multiple worksheets and were written in C. The charting/graphing routines were written in Forth by Jeremy Sagan (son of Carl Sagan) and the printing routines by Paul Funk (founder of Funk Software).
PC Version History.
DOS.
Real Mode (8088+).
These editions of 1-2-3 for DOS were primarily written in x86 assembly language.
Protected Mode (80286+).
These editions of 1-2-3 for DOS were primarily written in C.
Reception.
After previewing "1-2-3" on the IBM PC in 1982, "BYTE" called it "modestly revolutionary" for elegantly combining spreadsheet, database, and graphing functions. It praised the application's speed and ease of use, stating that with the built-in help screens and tutorial "1-2-3 is one of the few pieces of software that can literally be used by anybody. You can buy 1-2-3 and [an IBM PC] and be running the two together the same day". "PC Magazine" in 1983 called 1-2-3 "a powerful and impressive program ... as a spreadsheet, it's excellent", and attributed its very fast performance to being written in assembly language.

</doc>
<doc id="18274" url="http://en.wikipedia.org/wiki?curid=18274" title="List of facilities named after Lyndon Johnson">
List of facilities named after Lyndon Johnson

Many facilities have been named after Lyndon B. Johnson, thirty-sixth President of the United States, including the following:

</doc>
<doc id="18278" url="http://en.wikipedia.org/wiki?curid=18278" title="Liberation Day (Netherlands)">
Liberation Day (Netherlands)

In the Netherlands, Liberation Day (Dutch: "Bevrijdingsdag") is celebrated each year on May the 5th to mark the end of the occupation by Nazi Germany during World War II.
The nation was liberated largely by the First Canadian Army, which included in addition to Canadian forces the British I Corps, and the 1st Polish Armoured Division, as well as, at various times, American, Belgian, Dutch and Czechoslovak troops. Parts of the country, in particular the south-east, were liberated by the British Second Army, which included American and Polish airborne forces, (see Operation Market Garden) and French airbornes (see Operation Amherst). On 5 May 1945, the Canadian General Charles Foulkes and the German Commander-in-Chief Johannes Blaskowitz reached an agreement on the capitulation of German forces in the Netherlands in Hotel de Wereld in Wageningen. One day later, the capitulation document was signed in the auditorium of Wageningen University, located next door.
After the liberation in 1945, Liberation Day was commemorated every five years. Finally, in 1990, the day was declared to be a national holiday, when the liberation would be commemorated and celebrated every year.
On 4 May, the Dutch hold "Dodenherdenking," Remembrance of the Dead for the people who fought and died during World War II and in wars in general. There are remembrance gatherings all over cities and in the country, the better-known at the National Monument on Dam Square in Amsterdam and at the Waalsdorpervlakte in the dunes near The Hague, one of the infamous Nazi execution places. Throughout the country, two minutes of silence are observed at 8 p.m. On May 5, the liberation is celebrated and festivals are held at most places in the Netherlands, with parades of veterans and a musical festival in Wageningen, the latter with an estimated attendance of 100,000.
External links.
 

</doc>
<doc id="18279" url="http://en.wikipedia.org/wiki?curid=18279" title="Light pollution">
Light pollution

Light pollution, also known as photopollution or luminous pollution, is excessive, misdirected, or obtrusive artificial light. Pollution is the adding-of/added light itself, in analogy to added sound, carbon dioxide, etc. Adverse consequences are multiple; some of them may not be known yet. Scientific definitions thus include the following:
The first three of the above four scientific definitions describe the state of the environment. The fourth (and newest) one describes the process of polluting by light.
Light pollution competes with starlight in the night sky for urban residents, interferes with astronomical observatories, and, like any other form of pollution, disrupts ecosystems and has adverse health effects. Light pollution can be divided into two main types:
Light pollution is a side effect of industrial civilization. Its sources include building exterior and interior lighting, advertising, commercial properties, offices, factories, streetlights, and illuminated sporting venues. It is most severe in highly industrialized, densely populated areas of North America, Europe, and Japan and in major cities in the Middle East and North Africa like Tehran and Cairo, but even relatively small amounts of light can be noticed and create problems. Since the early 1980s, a global dark-sky movement has emerged, with concerned people campaigning to reduce the amount of light pollution. The International Dark-Sky Association (IDA) is one non-profit advocacy group involved in this movement.
Impact on energy usage.
Energy conservation advocates contend that light pollution must be addressed by changing the habits of society, so that lighting is used more efficiently, with less waste and less creation of unwanted or unneeded illumination. Several industry groups also recognize light pollution as an important issue. For example, the Institution of Lighting Engineers in the United Kingdom provides its members with information about light pollution, the problems it causes, and how to reduce its impact.
Since not everyone is irritated by the same lighting sources, it is common for one person's light "pollution" to be light that is desirable for another. One example of this is found in advertising, when an advertiser wishes for particular lights to be bright and visible, even though others find them annoying. Other types of light pollution are more certain. For instance, light that "accidentally" crosses a property boundary and annoys a neighbor is generally wasted and pollutive light.
Disputes are still common when deciding appropriate action, and differences in opinion over what light is considered reasonable, and who should be responsible, mean that negotiation must sometimes take place between parties. Where objective measurement is desired, light levels can be quantified by field measurement or mathematical modeling, with results typically displayed as an isophote map or light contour map. Authorities have also taken a variety of measures for dealing with light pollution, depending on the interests, beliefs and understandings of the society involved. Measures range from doing nothing at all, to implementing strict laws and regulations about how lights may be installed and used.
Types.
Light pollution is a broad term that refers to multiple problems, all of which are caused by inefficient, unappealing, or (arguably) unnecessary use of artificial light. Specific categories of light pollution include light trespass, over-illumination, glare, light clutter, and skyglow. A single offending light source often falls into more than one of these categories.
Light trespass.
Light trespass occurs when unwanted light enters one's property, for instance, by shining over a neighbor's fence. A common light trespass problem occurs when a strong light enters the window of one's home from the outside, causing problems such as sleep deprivation or the blocking of an evening view.
A number of cities in the U.S. have developed standards for outdoor lighting to protect the rights of their citizens against light trespass. To assist them, the International Dark-Sky Association has developed a set of model lighting ordinances.
The Dark-Sky Association was started to reduce the light going up into the sky which reduces visibility of stars (see Skyglow below). This is any light which is emitted more than 90° above nadir. By limiting light at this 90° mark they have also reduced the light output in the 80–90° range which creates most of the light trespass issues.
U.S. federal agencies may also enforce standards and process complaints within their areas of jurisdiction. For instance, in the case of light trespass by white strobe lighting from communication towers in excess of FAA minimum lighting requirements the Federal Communications Commission maintains an Antenna Structure Registration database information which citizens may use to identify offending structures and provides a mechanism for processing citizen inquiries and complaints. The U.S. Green Building Council (USGBC) has also incorporated a credit for reducing the amount of light trespass and sky glow into their environmentally friendly building standard known as LEED.
Light trespass can be reduced by selecting light fixtures which limit the amount of light emitted more than 80° above the nadir. The IESNA definitions include full cutoff (0%), cutoff (10%), and semi-cutoff (20%). (These definitions also include limits on light emitted above 90° to reduce sky glow.)
Over-illumination.
Over-illumination is the excessive use of light. Specifically within the United States, over-illumination is responsible for approximately two million barrels of oil per day in energy wasted. This is based upon U.S. consumption of equivalent of 18.8 Moilbbl/d of petroleum. It is further noted in the same U.S. Department of Energy source that over 30% of all primary energy is consumed by commercial, industrial and residential sectors. Energy audits of existing buildings demonstrate that the lighting component of residential, commercial and industrial uses consumes about 20–40% of those land uses, variable with region and land use. (Residential use lighting consumes only 10–30% of the energy bill while commercial buildings' major use is lighting.) Thus lighting energy accounts for about four or five million barrels of oil (equivalent) per day. Again energy audit data demonstrates that about 30–60% of energy consumed in lighting is unneeded or gratuitous.
An alternative calculation starts with the fact that commercial building lighting consumes in excess of 81.68 terawatts (1999 data) of electricity, according to the U.S. DOE. Thus commercial lighting alone consumes about four to five million barrels per day (equivalent) of petroleum, in line with the alternate rationale above to estimate U.S. lighting energy consumption. Even among developed countries there are large differences in patterns of light use. American cities emit 3-5 times more light to space per capita compared to German cities.
Over-illumination stems from several factors:
Most of these issues can be readily corrected with available, inexpensive technology, and with resolution of landlord/tenant practices that create barriers to rapid correction of these matters. Most importantly, public awareness would need to improve for industrialized countries to realize the large payoff in reducing over-illumination.
In certain cases an over-illumination lighting technique may be needed. For example, indirect lighting is often used to obtain a "softer" look, since hard direct lighting is generally found less desirable for certain surfaces, such as skin. The indirect lighting method is perceived as more cozy and suits bars, restaurants and living quarters. It is also possible to block the direct lighting effect by adding softening filters or other solutions, though intensity will be reduced.
Glare.
Glare can be categorized into different types. One such classification is described in a book by Bob Mizon, coordinator for the British Astronomical Association's Campaign for Dark Skies. According to this classification:
According to Mario Motta, president of the Massachusetts Medical Society, "... glare from bad lighting is a public-health hazard—especially the older you become. Glare light scattering in the eye causes loss of contrast and leads to unsafe driving conditions, much like the glare on a dirty windshield from low-angle sunlight or the high beams from an oncoming car." In essence bright and/or badly shielded lights around roads can partially blind drivers or pedestrians and contribute to accidents.
The blinding effect is caused in large part by reduced contrast due to light scattering in the eye by excessive brightness, or to reflection of light from dark areas in the field of vision, with luminance similar to the background luminance. This kind of glare is a particular instance of disability glare, called veiling glare. (This is not the same as loss of accommodation of night vision which is caused by the direct effect of the light itself on the eye.)
Light clutter.
Light clutter refers to excessive groupings of lights. Groupings of lights may generate confusion, distract from obstacles (including those that they may be intended to illuminate), and potentially cause accidents. Clutter is particularly noticeable on roads where the street lights are badly designed, or where brightly lit advertising surrounds the roadways. Depending on the motives of the person or organization that installed the lights, their placement and design can even be intended to distract drivers, and can contribute to accidents.
Clutter may also present a hazard in the aviation environment if aviation safety lighting must compete for pilot attention with non-relevant lighting. For instance, runway lighting may be confused with an array of suburban commercial lighting and aircraft collision avoidance lights may be confused with ground lights.
Skyglow.
Skyglow refers to the glow effect that can be seen over populated areas. It is the combination of all light reflected from what it has illuminated escaping up into the sky and from "all" of the badly directed light in that area that also escapes into the sky being scattered (redirected) by the atmosphere back toward the ground. This scattering is very strongly related to the wavelength of the light when the air is very clear (with very few aerosols). Rayleigh scattering dominates in such clear air, making the sky appear blue in the daytime. When there is significant aerosol (typical of most modern polluted conditions), the scattered light has less dependence on wavelength, making a whiter daytime sky. Because of this Rayleigh effect, and because of the eye's increased sensitivity to white or blue-rich light sources when adapted to very low light levels (see Purkinje effect), white or blue-rich light contributes significantly more to sky-glow than an equal amount of yellow light. Sky glow is of particular irritation to astronomers, because it reduces contrast in the night sky to the extent where it may even become impossible to see any but the brightest stars.
The Bortle Dark-Sky Scale, originally published in "Sky & Telescope" magazine, is sometimes used (by groups like the U.S. National Park Service) to quantify skyglow and general sky clarity. The nine-class scale rates the darkness of the night sky and the visibility of its phenomena, such as the gegenschein and the zodiacal light (easily masked by skyglow), providing a detailed description of each level on the scale (with Class 1 being the best).
Light is particularly problematic for amateur astronomers, whose ability to observe the night sky from their property is likely to be inhibited by any stray light from nearby. Most major optical astronomical observatories are surrounded by zones of strictly enforced restrictions on light emissions.
Direct skyglow is reduced by selecting lighting fixtures which limit the amount of light emitted more than 90° above the nadir. The IESNA definitions include full cutoff (0%), cutoff (2.5%), and semi-cutoff (5%). Indirect skyglow produced by reflections from vertical and horizontal surfaces is harder to manage; the only effective method for preventing it is by minimizing over-illumination. But it has to be taken into account that, according to late 2010 publications, Italian regions using full cut off lighting only does not increase skyglow. Anyway light reflected upwards by dark surfaces such as roads or building can be considered as minor, so debate about contribution of indirect skyglow will last long.
Skyglow is made considerably worse when clouds are present. While this has no effect on astronomical observations (which are not possible at visible wavelengths under cloud cover), it is very important in the context of ecological light pollution. Since cloudy nights in artificially lit areas can be up to ten thousand times brighter than in natural areas, any organisms that are affected by sky glow (e.g. zooplankton and fish that visually prey on them) are much more likely to have their ordinary behavior disturbed on cloudy nights.
Measurement and global effects.
Measuring the effect of sky glow on a global scale is a complex procedure. The natural atmosphere is not completely dark, even in the absence of terrestrial sources of light and illumination from the Moon. This is caused by two main sources: "airglow" and "scattered light".
At high altitudes, primarily above the mesosphere, there is enough UV radiation from the sun of very short wavelength to cause ionization. When the ions collide with electrically neutral particles they recombine and emit photons in the process, causing airglow. The degree of ionization is sufficiently large to allow a constant emission of radiation even during the night when the upper atmosphere is in the Earth's shadow. Lower in the atmosphere all of the solar photons with energies above the ionization potential of N2 and O2 have already been absorbed by the higher layers and thus no appreciable ionization occurs.
Apart from emitting light, the sky also scatters incoming light, primarily from distant stars and the Milky Way, but also the zodiacal light, sunlight that is reflected and backscattered from interplanetary dust particles.
The amount of airglow and zodiacal light is quite variable (depending, amongst other things on sunspot activity and the Solar cycle) but given optimal conditions the darkest possible sky has a brightness of about 22 magnitude/square arcsecond. If a full moon is present, the sky brightness increases to about 18 magnitude/sq. arcsecond depending on local atmospheric transparency, 40 times brighter than the darkest sky. In densely populated areas a sky brightness of 17 magnitude/sq. arcsecond is not uncommon, or as much as 100 times brighter than is natural.
To precisely measure how bright the sky gets, night time satellite imagery of the earth is used as raw input for the number and intensity of light sources. These are put into a physical model of scattering due to air molecules and aerosoles to calculate cumulative sky brightness. Maps that show the enhanced sky brightness have been prepared for the entire world.
Inspection of the area surrounding Madrid reveals that the effects of light pollution caused by a single large conglomeration can be felt up to 100 km away from the center.
Global effects of light pollution are also made obvious. The entire area consisting of southern England, Netherlands, Belgium, west Germany, and northern France have a sky brightness of at least 2 to 4 times above normal (see above right). The only places in continental Europe where the sky can attain its natural darkness are in northern Scandinavia and in islands far from the continent.
In North America the situation is comparable. There is a significant problem with light pollution ranging from the Canadian Maritime Provinces to the American Southwest.
Light pollution in Hong Kong was declared the 'worst on the planet' in March 2013.
Consequences.
Energy waste.
Lighting is responsible for one-fourth of all electricity consumption worldwide, and case studies have shown that several forms of over-illumination constitute energy wastage, including non-beneficial upward direction of night-time lighting. In 2007, Terna, the company responsible for managing electricity flow in Italy, reported a saving of 645.2 million kWh in electricity consumption during the daylight saving period from April to October. It attributes this saving to the delayed need for artificial lighting during the evenings.
In Australia,
... public lighting is the single largest source of local government's greenhouse gas emissions, typically accounting for 30 to 50% of their emissions. There are 1.94 million public lights — one for every 10 Australians — that annually cost A$210 million, use 1,035 GWh of electricity and are responsible for 1.15 million tonnes of CO2 emissions.<br>
Current public lighting in Australia, particularly for minor roads and streets, uses large amounts of energy and financial resources, while often failing to provide high quality lighting. There are many ways to improve lighting quality while reducing energy use and greenhouse gas emissions as well as lowering costs.
Effects on animal and human health and psychology.
Medical research on the effects of excessive light on the human body suggests that a variety of adverse health effects may be caused by light pollution or excessive light exposure, and some lighting design textbooks use human health as an explicit criterion for proper interior lighting. Health effects of over-illumination or improper spectral composition of light may include: increased headache incidence, worker fatigue, medically defined stress, decrease in sexual function and increase in anxiety. Likewise, animal models have been studied demonstrating unavoidable light to produce adverse effect on mood and anxiety. For those who need to be awake at night, light at night also has an acute effect on alertness and mood.
In 2007, "shift work that involves circadian disruption" was listed as a probable carcinogen by the World Health Organization's International Agency for Research on Cancer. (IARC Press release No. 180). Multiple studies have documented a correlation between night shift work and the increased incidence of breast and prostate cancer.
A more recent discussion (2009), written by Professor Steven Lockley, Harvard Medical School, can be found in the CfDS handbook "Blinded by the Light?". Chapter 4, "Human health implications of light pollution" states that "... light intrusion, even if dim, is likely to have measurable effects on sleep disruption and melatonin suppression. Even if these effects are relatively small from night to night, continuous chronic circadian, sleep and hormonal disruption may have longer-term health risks". The New York Academy of Sciences hosted a meeting in 2009 on Circadian Disruption and Cancer. Red light suppresses melatonin the least.
In June 2009, the American Medical Association developed a policy in support of control of light pollution. News about the decision emphasized glare as a public health hazard leading to unsafe driving conditions. Especially in the elderly, glare produces loss of contrast, obscuring night vision.
Disruption of ecosystems.
When artificial light affects organisms and ecosystems it is called ecological light pollution. While light at night can be beneficial, neutral, or damaging for individual species, its presence invariably disturbs ecosystems. For example, some species of spiders avoid lit areas, while other species are happy to build their spider web directly on a lamp post. Since lamp posts attract many flying insects, the spiders that don't mind light gain an advantage over the spiders that avoid it. This is a simple example of the way in which species frequencies and food webs can be disturbed by the introduction of light at night.
Light pollution poses a serious threat in particular to nocturnal wildlife, having negative impacts on plant and animal physiology. It can confuse animal navigation, alter competitive interactions, change predator-prey relations, and cause physiological harm. The rhythm of life is orchestrated by the natural diurnal patterns of light and dark, so disruption to these patterns impacts the ecological dynamics.
Studies suggest that light pollution around lakes prevents zooplankton, such as Daphnia, from eating surface algae, causing algal blooms that can kill off the lakes' plants and lower water quality. Light pollution may also affect ecosystems in other ways. For example, lepidopterists and entomologists have documented that nighttime light may interfere with the ability of moths and other nocturnal insects to navigate. Night-blooming flowers that depend on moths for pollination may be affected by night lighting, as there is no replacement pollinator that would not be affected by the artificial light. This can lead to species decline of plants that are unable to reproduce, and change an area's longterm ecology.
A 2009 study also suggests deleterious impacts on animals and ecosystems because of perturbation of polarized light or artificial polarisation of light (even during the day, because direction of natural polarization of sun light and its reflexion is a source of information for a lot of animals). This form of pollution is named polarized light pollution (PLP). Unnatural polarized light sources can trigger maladaptive behaviors in polarization-sensitive taxa and alter ecological interactions.
Lights on tall structures can disorient migrating birds. Estimates by the U.S. Fish and Wildlife Service of the number of birds killed after being attracted to tall towers range from 4 to 5 million per year to an order of magnitude higher. The Fatal Light Awareness Program (FLAP) works with building owners in Toronto, Canada and other cities to reduce mortality of birds by turning out lights during migration periods.
Similar disorientation has also been noted for bird species migrating close to offshore production and drilling facilities. Studies carried out by Nederlandse Aardolie Maatschappij b.v. (NAM) and Shell have led to development and trial of new lighting technologies in the North Sea. In early 2007, the lights were installed on the Shell production platform L15. The experiment proved a great success since the number of birds circling the platform declined by 50 to 90%.
Sea turtle hatchlings emerging from nests on beaches are another casualty of light pollution. It is a common misconception that hatchling sea turtles are attracted to the moon. Rather, they find the ocean by moving away from the dark silhouette of dunes and their vegetation, a behavior with which artificial lights interfere. The breeding activity and reproductive phenology of toads, however, are cued by moonlight. Juvenile seabirds may also be disoriented by lights as they leave their nests and fly out to sea. Amphibians and reptiles are also affected by light pollution. Introduced light sources during normally dark periods can disrupt levels of melatonin production. Melatonin is a hormone that regulates photoperiodic physiology and behaviour. Some species of frogs and salamanders utilize a light-dependent "compass" to orient their migratory behaviour to breeding sites. Introduced light can also cause developmental irregularities, such as retinal damage, reduced sperm production, and genetic mutation.
In September 2009, the 9th European Dark-Sky Symposium in Armagh, Northern Ireland had a session on the environmental effects of light at night (LAN). It dealt with bats, turtles, the "hidden" harms of LAN, and many other topics. The environmental effects of LAN were mentioned as early as 1897, in a "Los Angeles Times" article—the text of which can be obtained from Dr. Travis Longcore of the Urban Wildlands Group, California. The following is an excerpt from that article, called "Electricity and English songbirds":
An English journal has become alarmed at the relation of electricity to songbirds, which it maintains is closer than that of cats and fodder crops. How many of us, it asks, foresee that electricity may extirpate the songbird?...With the exception of the finches, all the English songbirds may be said to be insectivorous, and their diet consists chiefly of vast numbers of very small insects which they collect from the grass and herbs before the dew is dry. As the electric light is finding its way for street illumination into the country parts of England, these poor winged atoms are slain by thousands at each light every warm summer evening...The fear is expressed, that when England is lighted from one end to the other with electricity the song birds will die out from the failure of their food supply.
Effect on astronomy.
Astronomy is very sensitive to light pollution. The night sky viewed from a city bears no resemblance to what can be seen from dark skies. Skyglow (the scattering of light in the atmosphere) reduces the contrast between stars and galaxies and the sky itself, making it much harder to see fainter objects. This is one factor that has caused newer telescopes to be built in increasingly remote areas. Some astronomers use narrow-band "nebula filters" which only allow specific wavelengths of light commonly seen in nebulae, or broad-band "light pollution filters" which are designed to reduce (but not eliminate) the effects of light pollution by filtering out spectral lines commonly emitted by sodium- and mercury-vapor lamps, thus enhancing contrast and improving the view of dim objects such as galaxies and nebulae. Unfortunately these light pollution reduction (LPR) filters are not a cure for light pollution. LPR filters reduce the brightness of the object under study and this limits the use of higher magnifications. LPR filters work by blocking light of certain wavelengths, which alters the color of the object, often creating a pronounced green cast. Furthermore, LPR filters only work on certain object types (mainly emission nebulae) and are of little use on galaxies and stars. No filter can match the effectiveness of a dark sky for visual or photographic purposes. Due to their low surface brightness, the visibility of diffuse sky objects such as nebulae and galaxies is affected by light pollution more than are stars. Most such objects are rendered invisible in heavily light polluted skies around major cities. A simple method for estimating the darkness of a location is to look for the Milky Way, which from truly dark skies appears bright enough to cast a shadow.
In addition to skyglow, light trespass can impact observations when artificial light directly enters the tube of the telescope and is reflected from non-optical surfaces until it eventually reaches the eyepiece. This direct form of light pollution causes a glow across the field of view which reduces contrast. Light trespass also makes it hard for a visual observer to become sufficiently dark adapted. The usual measures to reduce this glare, if reducing the light directly is not an option, include flocking the telescope tube and accessories to reduce reflection, and putting a light shield (also usable as a dew shield) on the telescope to reduce light entering from angles other than those near the target. Under these conditions, some astronomers prefer to observe under a black cloth to ensure maximum dark adaptation. In one Italian regional lighting code this effect of stray light is defined as "optical pollution", due to the fact that there is a direct path from the light source to the "optic" – the observer's eye or telescope.
Increase in atmospheric pollution.
A study presented at the American Geophysical Union meeting in San Francisco found that light pollution destroys nitrate radicals thus preventing the normal night time reduction of atmospheric smog produced by fumes emitted from cars and factories. The study was presented by Harald Stark from the National Oceanic and Atmospheric Administration.
Reduction of natural sky polarization.
In the night, the polarization of the moonlit sky is very strongly reduced in the presence of urban light pollution, because scattered urban light is not strongly polarized. Polarized moonlight can't be seen by humans, but is believed to be used by many animals for navigation.
Reduction.
Reducing light pollution implies many things, such as reducing sky glow, reducing glare, reducing light trespass, and reducing clutter. The method for best reducing light pollution, therefore, depends on exactly what the problem is in any given instance. Possible solutions include:
Improving lighting fixtures.
The use of "full cutoff" lighting fixtures, as much as possible, is advocated by most campaigners for the reduction of light pollution. It is also commonly recommended that lights be spaced appropriately for maximum efficiency, and that number of luminaires being used as well as the wattage of each luminaire match the needs of the particular application (based on local lighting design standards).
Full cutoff fixtures first became available in 1959 with the introduction of General Electric's M100 fixture.
A full cutoff fixture, when correctly installed, reduces the chance for light to escape above the plane of the horizontal. Light released above the horizontal may sometimes be lighting an intended target, but often serves no purpose. When it enters into the atmosphere, light contributes to sky glow. Some governments and organizations are now considering, or have already implemented, full cutoff fixtures in street lamps and stadium lighting.
The use of full cutoff fixtures help to reduce sky glow by preventing light from escaping above the horizontal. Full cutoff typically reduces the visibility of the lamp and reflector within a luminaire, so the effects of glare are also reduced. Campaigners also commonly argue that full cutoff fixtures are more efficient than other fixtures, since light that would otherwise have escaped into the atmosphere may instead be directed towards the ground. However, full cutoff fixtures may also trap more light in the fixture than other types of luminaires, corresponding to lower luminaire efficiency, suggesting a re-design of some luminaires may be necessary.
The use of full cutoff fixtures can allow for lower wattage lamps to be used in the fixtures, producing the same or sometimes a better effect, due to being more carefully controlled. In every lighting system, some sky glow also results from light reflected from the ground. This reflection can be reduced, however, by being careful to use only the lowest wattage necessary for the lamp, and setting spacing between lights appropriately. Assuring luminaire setback is greater than 90° from highly reflective surfaces also diminishes reflectance.
A common criticism of full cutoff lighting fixtures is that they are sometimes not as aesthetically pleasing to look at. This is most likely because historically there has not been a large market specifically for full cutoff fixtures, and because people typically like to see the source of illumination. Due to the specificity with their direction of light, full cutoff fixtures sometimes also require expertise to install for maximum effect.
The effectiveness of using full cutoff roadway lights to combat light pollution has also been called into question. According to design investigations, luminaires with full cutoff distributions (as opposed to "cutoff" or "semi cutoff", compared here ) have to be closer together to meet the same light level, uniformity and glare requirements specified by the IESNA. These simulations optimized the height and spacing of the lights while constraining the overall design to meet the IESNA requirements, and then compared total uplight and energy consumption of different luminaire designs and powers. Cutoff designs performed better than full cutoff designs, and semi-cutoff performed better than either cutoff or full cutoff. This indicates that, in roadway installations, over-illumination or poor uniformity produced by full cutoff fixtures may be more detrimental than direct uplight created by fewer cutoff or semi-cutoff fixtures. Therefore, the overall performance of existing systems could be improved more by reducing the number of luminaires than by switching to full cutoff designs.
However, using the definition of "light pollution" from some Italian regional bills (i.e., "every irradiance of artificial light outside competence areas and particularly upward the sky") only full cutoff design prevents light pollution. The Italian Lombardy region, where only full cutoff design is allowed (Lombardy act no. 17/2000, promoted by Cielobuio-coordination for the protection of the night sky), in 2007 had the lowest per capita energy consumption for public lighting in Italy. The same legislation also imposes a minimum distance between street lamps of about four times their height, so full cut off street lamps are the best solution to reduce both light pollution and electrical power usage.
Adjusting types of light sources.
Several different types of light sources exist, each having different properties that affect their appropriateness for certain tasks, particularly efficiency and spectral power distribution. It is often the case that inappropriate light sources have been selected for a task, either due to ignorance or because more sophisticated light sources were unavailable at the time of installation. Therefore, badly chosen light sources often contribute unnecessarily to light pollution and energy waste. By re-assessing and changing the light sources used, it is often possible to reduce energy use and pollutive effects while simultaneously greatly improving efficiency and visibility.
Some types of light sources are listed in order of energy efficiency in the table below.
Many astronomers request that nearby communities use low pressure sodium lights as much as possible, because the principal wavelength emitted is comparably easy to work around or in rare cases filter out. The low cost of operating sodium lights is another feature. In 1980, for example, San Jose, California, replaced all street lamps with low pressure sodium lamps, whose light is easier for nearby Lick Observatory to filter out. Similar programs are now in place in Arizona and Hawaii.
Disadvantages of low pressure sodium lighting are that fixtures must usually be larger than competing fixtures, and that color cannot be distinguished, due to its emitting principally a single wavelength of light (see security lighting). Due to the substantial size of the lamp, particularly in higher wattages such as 135 W and 180 W, control of light emissions from low pressure sodium luminaires is more difficult. For applications requiring more precise direction of light (such as narrow roadways) the native lamp efficacy advantage of this lamp type is decreased and may be entirely lost compared to high pressure sodium lamps. Allegations that this also leads to higher amounts of light pollution from luminaires running these lamps arise principally because of older luminaires with poor shielding, still widely in use in the UK and in some other locations. Modern low-pressure sodium fixtures with better optics and full shielding, and the decreased skyglow impacts of yellow light preserve the luminous efficacy advantage of low-pressure sodium and result in most cases is less energy consumption and less visible light pollution. Unfortunately, due to continued lack of accurate information, many lighting professionals continue to disparage low-pressure sodium, contributing to its decreased acceptance and specification in lighting standards and therefore its use. Another disadvantage of low-pressure sodium lamps is that some people find the characteristic yellow light very displeasing aesthetically.
Because of the scatter of light by the atmosphere, different sources produce dramatically different amounts of skyglow from the same amount of light sent into the atmosphere.
Re-designing lighting plans.
In some cases, evaluation of existing plans has determined that more efficient lighting plans are possible. For instance, light pollution can be reduced by turning off unneeded outdoor lights, and only lighting stadiums when there are people inside. Timers are especially valuable for this purpose. One of the world's first coordinated "legislative" efforts to reduce the adverse effect of this pollution on the environment began in Flagstaff, Arizona, in the U.S. There, over three decades of ordinance development has taken place, with the full support of the population, often with government support, with community advocates, and with the help of major local observatories, including the United States Naval Observatory Flagstaff Station. Each component helps to educate, protect and enforce the imperatives to intelligently reduce detrimental light pollution.
One example of a lighting plan assessment can be seen in a report originally commissioned by the Office of the Deputy Prime Minister in the United Kingdom, and now available through the Department for Communities and Local Government. The report details a plan to be implemented throughout the UK, for designing lighting schemes in the countryside, with a particular focus on preserving the environment.
In another example, the city of Calgary has recently replaced most residential street lights with models that are comparably energy efficient. The motivation is primarily operation cost and environmental conservation. The costs of installation are expected to be regained through energy savings within six to seven years.
The Swiss Agency for Energy Efficiency (SAFE) uses a concept that promises to be of great use in the diagnosis and design of road lighting, ""consommation électrique spécifique" ("CES")", which can be translated into English as "specific electric power consumption (SEC)". Thus, based on observed lighting levels in a wide range of Swiss towns, SAFE has defined target values for electric power consumption per metre for roads of various categories. Thus, SAFE currently recommends an SEC of 2 to 3 watts per meter for roads of less than 10 metre width (4 to 6 watts per metre for wider roads). Such a measure provides an easily applicable environmental protection constraint on conventional "norms", which usually are based on the recommendations of lighting manufacturing interests, who may not take into account environmental criteria. In view of ongoing progress in lighting technology, target SEC values will need to be periodically revised downwards.
A newer method for predicting and measuring various aspects of light pollution was described in the journal Lighting Research Technology (September 2008). Scientists at Rensselaer Polytechnic Institute's Lighting Research Center have developed a comprehensive method called Outdoor Site-Lighting Performance (OSP), which allows users to quantify, and thus optimize, the performance of existing and planned lighting designs and applications to minimize excessive or obtrusive light leaving the boundaries of a property. OSP can be used by lighting engineers immediately, particularly for the investigation of glow and trespass (glare analyses are more complex to perform and current commercial software does not readily allow them), and can help users compare several lighting design alternatives for the same site.
In the effort to reduce light pollution, researchers have developed a "Unified System of Photometry," which is a way to measure how much or what kind of street lighting is needed. The Unified System of Photometry allows light fixtures to be designed to reduce energy use while maintaining or improving perceptions of visibility, safety, and security. There was a need to create a new system of light measurement at night because the biological way in which the eye’s rods and cones process light is different in nighttime conditions versus daytime conditions. Using this new system of photometry, results from recent studies have indicated that replacing traditional, yellowish, high-pressure sodium (HPS) lights with "cool" white light sources, such as induction, fluorescent, ceramic metal halide, or LEDs can actually reduce the amount of electric power used for lighting while maintaining or improving visibility in nighttime conditions.
The International Commission on Illumination, also known as the CIE from its French title, la Commission Internationale de l'Eclairage, will soon be releasing its own form of unified photometry for outdoor lighting.
External links.
 The dictionary definition of Light pollution at Wiktionary
 Media related to at Wikimedia Commons

</doc>
<doc id="18285" url="http://en.wikipedia.org/wiki?curid=18285" title="Lagrangian point">
Lagrangian point

In celestial mechanics, the Lagrangian points (; also Lagrange points, L-points, or libration points) are positions in an orbital configuration of two large bodies where a small object affected only by gravity can maintain a stable position relative to the two large bodies. The Lagrange points mark positions where the combined gravitational pull of the two large masses provides precisely the centripetal force required to orbit with them. There are five such points, labeled L1 to L5, all in the orbital plane of the two large bodies. The first three are on the line connecting the two large bodies and the last two, L4 and L5, form an equilateral triangle with the two large bodies. Objects can orbit these points as well. 
Several planets have minor planets near their L4 and L5 points (trojans) with respect to the Sun, with Jupiter in particular having more than a million of these. Artificial satellites have been placed at L1 and L2 with respect to the Sun and Earth, and Earth and the Moon for various purposes, and the Lagrangian points have been proposed for a variety of future uses in space exploration.
History.
The three collinear Lagrange points (L1, L2, L3) were discovered by Leonhard Euler a few years before Lagrange discovered the remaining two.
In 1772, Joseph-Louis Lagrange published an "Essay on the three-body problem". In the first chapter he considered the general three-body problem. From that, in the second chapter, he demonstrated two special constant-pattern solutions, the collinear and the equilateral, for any three masses, with circular orbits.
Lagrange points.
The five Lagrangian points are labeled and defined as follows:
The L1 point lies on the line defined by the two large masses M1 and M2, and between them. It is the most intuitively understood of the Lagrangian points: the one where the gravitational attraction of M2 partially cancels M1's gravitational attraction.
The L2 point lies on the line through the two large masses, beyond the smaller of the two. Here, the gravitational forces of the two large masses balance the centrifugal effect on a body at L2.
The L3 point lies on the line defined by the two large masses, beyond the larger of the two.
The L4 and L5 points lie at the third corners of the two equilateral triangles in the plane of orbit whose common base is the line between the centers of the two masses, such that the point lies behind (L5) or ahead (L4) of the smaller mass with regard to its orbit around the larger mass.
The triangular points (L4 and L5) are stable equilibria, provided that the ratio of M1/M2 is greater than 24.96. This is the case for the Sun–Earth system, the Sun–Jupiter system, and, by a smaller margin, the Earth–Moon system. When a body at these points is perturbed, it moves away from the point, but the factor opposite of that which is increased or decreased by the perturbation (either gravity or angular momentum-induced speed) will also increase or decrease, bending the object's path into a stable, kidney-bean-shaped orbit around the point (as seen in the corotating frame of reference).
In contrast to L4 and L5, where stable equilibrium exists, the points L1, L2, and L3 are positions of unstable equilibrium. Any object orbiting at one of L1-L3 will tend to fall out of orbit; it is therefore rare to find natural objects there, and spacecraft inhabiting these areas must employ station keeping in order to maintain their position.
Natural objects at Lagrangian points.
It is common to find objects at or orbiting the L4 and L5 points of natural orbital systems. These are commonly called "trojans"; in the 20th century, asteroids discovered orbiting at the Sun–Jupiter L4 and L5 points were named after characters from Homer's "Iliad". Asteroids at the L4 point, which leads Jupiter, are referred to as the "Greek camp", whereas those at the L5 point are referred to as the "Trojan camp".
Other examples of natural objects orbiting at Lagrange points:
Mathematical details.
Lagrangian points are the constant-pattern solutions of the restricted three-body problem. For example, given two massive bodies in orbits around their common barycenter, there are five positions in space where a third body, of comparatively negligible mass, could be placed so as to maintain its position relative to the two massive bodies. As seen in a rotating reference frame that matches the angular velocity of the two co-orbiting bodies, the gravitational fields of two massive bodies combined with the minor body's centrifugal force are in balance at the Lagrangian points, allowing the smaller third body to be relatively stationary with respect to the first two.
L1.
The location of L1 is the solution to the following equation, balancing gravitation and the centrifugal force:
formula_1
where "r" is the distance of the L1 point from the smaller object, "R" is the distance between the two main objects, and M1 and M2 are the masses of the large and small object, respectively. (The quantity in parentheses on the right is the distance of L1 from the center of mass.) Solving this for "r" involves solving a quintic function, but if the mass of the smaller object (M2) is much smaller than the mass of the larger object (M1) then L1 and L2 are at approximately equal distances "r" from the smaller object, equal to the radius of the Hill sphere, given by:
This distance can be described as being such that the orbital period, corresponding to a circular orbit with this distance as radius around M2 in the absence of M1, is that of M2 around M1, divided by formula_3:
L2.
The location of L2 is the solution to the following equation, balancing gravitation and inertia:
formula_5
with parameters defined as for the L1 case. Again, if the mass of the smaller object (M2) is much smaller than the mass of the larger object (M1) then L2 is at approximately the radius of the Hill sphere, given by:
L3.
The location of L3 is the solution to the following equation, balancing gravitation and the centrifugal force:
formula_7
with parameters defined as for the L1 and L2 cases except that "r" now indicates how much closer L3 is to the more massive object than the smaller object. If the mass of the smaller object (M2) is much smaller than the mass of the larger object (M1) then:
L4 and L5.
The reason these points are in balance is that, at L4 and L5, the distances to the two masses are equal. Accordingly, the gravitational forces from the two massive bodies are in the same ratio as the masses of the two bodies, and so the resultant force acts through the barycenter of the system; additionally, the geometry of the triangle ensures that the resultant acceleration is to the distance from the barycenter in the same ratio as for the two massive bodies. The barycenter being both the center of mass and center of rotation of the three-body system, this resultant force is exactly that required to keep the smaller body at the Lagrange point in orbital equilibrium with the other two larger bodies of system. (Indeed, the third body need not have negligible mass.) The general triangular configuration was discovered by Lagrange in work on the three-body problem.
Stability.
Although the L1, L2, and L3 points are nominally unstable, it turns out that it is possible to find (unstable) periodic orbits around these points, at least in the restricted three-body problem. These periodic orbits, referred to as "halo" orbits, do not exist in a full "n"-body dynamical system such as the Solar System. However, quasi-periodic (i.e. bounded but not precisely repeating) orbits following Lissajous-curve trajectories do exist in the "n"-body system. These quasi-periodic Lissajous orbits are what most of Lagrangian-point missions to date have used. Although they are not perfectly stable, a relatively modest effort at station keeping can allow a spacecraft to stay in a desired Lissajous orbit for an extended period of time. It also turns out that, at least in the case of Sun–Earth-L1 missions, it is actually preferable to place the spacecraft in a large-amplitude (100000 –) Lissajous orbit, instead of having it sit at the Lagrangian point, because this keeps the spacecraft off the direct line between Sun and Earth, thereby reducing the impact of solar interference on Earth–spacecraft communications. Similarly, a large-amplitude Lissajous orbit around L2 can keep a probe out of Earth's shadow and therefore ensures a better illumination of its solar panels. 
Spaceflight applications.
Earth–Moon L1 allows comparatively easy access to Lunar and Earth orbits with minimal change in velocity and has this as an advantage to position a half-way manned space station intended to help transport cargo and personnel to the Moon and back.
Earth–Moon L2 would be a good location for a communications satellite covering the Moon's far side and would be "an ideal location" for a propellant depot as part of the proposed depot-based space transportation architecture.
Sun–Earth L1 is suited for making observations of the Sun–Earth system. Objects here are never shadowed by Earth or the Moon. The first mission of this type was the International Sun Earth Explorer 3 (ISEE-3) mission used as an interplanetary early warning storm monitor for solar disturbances.
Sun–Earth L2 is a good spot for space-based observatories. Because an object around L2 will maintain the same relative position with respect to the Sun and Earth, shielding and calibration are much simpler. It is, however, slightly beyond the reach of Earth's umbra, so solar radiation is not completely blocked. From this point, the Sun, Earth and Moon are relatively closely positioned together in the sky, and hence leave a large field of view without interference – this is especially helpful for infrared astronomy.
Sun–Earth L3 was a popular place to put a "Counter-Earth" in pulp science fiction and comic books. Once space-based observation became possible via satellites and probes, it was shown to hold no such object. The Sun–Earth L3 is unstable and could not contain an object, large or small, for very long. This is because the gravitational forces of the other planets are stronger than that of Earth (Venus, for example, comes within 0.3 AU of this L3 every 20 months).
A spacecraft orbiting near Sun–Earth L3 would be able to closely monitor the evolution of active sunspot regions before they rotate into a geoeffective position, so that a 7-day early warning could be issued by the NOAA Space Weather Prediction Center. Moreover, a satellite near Sun–Earth L3 would provide very important observations not only for Earth forecasts, but also for deep space support (Mars predictions and for manned mission to near-Earth asteroids). In 2010, spacecraft transfer trajectories to Sun–Earth L3 were studied and several designs were considered.
Scientists at the B612 Foundation are planning to use Venus's L3 point to position their planned Sentinel telescope, which aims to look back towards Earth's orbit and compile a catalogue of near-Earth asteroids.
Missions to Lagrangian points generally orbit the points rather than occupy them directly.
Another interesting and useful property of the collinear Lagrangian points and their associated Lissajous orbits is that they serve as "gateways" to control the chaotic trajectories of the Interplanetary Transport Network.
Spacecraft at Sun–Earth L1.
International Sun Earth Explorer 3 (ISEE-3) began its mission at the Sun–Earth L1 before leaving to intercept a comet in 1982. The Sun–Earth L1 is also the point to which the Reboot ISEE-3 mission was attempting to return the craft as the first phase of a recovery mission (as of September 25, 2014 all efforts have failed and contact was lost).
Solar and Heliospheric Observatory (SOHO) is stationed in a halo orbit at L1, and the Advanced Composition Explorer (ACE) in a Lissajous orbit, also at the L1 point. WIND is also at L1.
Deep Space Climate Observatory (DSCOVR), launched on 11 February 2015, will orbit L1 to study the solar wind and its effects on Earth.
Spacecraft at Sun–Earth L2.
Spacecraft at the Sun–Earth L2 point are in a Lissajous orbit until decommissioned, when they are sent into a heliocentric graveyard orbit.
List of missions to Lagrangian points.
Color key:
Future and proposed missions.
The Aditya mission for a solar observatory, is officially scheduled for launch in 2016–17 by ISRO. "It will be going to a point 1.5 million kilometers away from Earth, that is L1 Lagrangian point. From this point it will observe the Sun constantly and study the solar corona, the region around the sun's surface.

</doc>
<doc id="18286" url="http://en.wikipedia.org/wiki?curid=18286" title="Lucid dream">
Lucid dream

A lucid dream is any dream in which one is aware that one is dreaming. In relation to this phenomenon, Greek philosopher Aristotle observed: "often when one is asleep, there is something in consciousness which declares that what then presents itself is but a dream". One of the earliest references to personal experiences with lucid dreaming was by Marie-Jean-Léon, Marquis d'Hervey de Saint Denys.
The person most widely acknowledged as having coined the term is Dutch psychiatrist and writer Frederik (Willem) van Eeden (1860–1932). In a lucid dream, the dreamer has greater chances to exert some degree of control over their participation within the dream or be able to manipulate their imaginary experiences in the dream environment. Lucid dreams can be realistic and vivid. It is shown that there are higher amounts of beta-1 frequency band (13–19 Hz) brain wave activity experienced by lucid dreamers, hence there is an increased amount of activity in the parietal lobes making lucid dreaming a conscious process.
Skeptics of the phenomenon suggest that it is not a state of sleep, but of brief wakefulness. Others point out that there is no way to prove the truth of lucid dreaming other than to ask the dreamer. Lucid dreaming has been researched scientifically, with participants performing pre-determined physical responses while experiencing a lucid dream.
Scientific history.
The first book to recognize the scientific potential of lucid dreams was Marquis d'Hervey de Saint-Denys's 1867 "Les Reves et Les Moyens de Les Diriger: Observations Pratiques". This French publication, originally published anonymously, translates as 'Dreams and the ways to direct them: practical observations'. It accounts for Saint-Denys' own experiences, but made also an extensive study of the phenomenon of lucid dreaming. Later, researcher Celia Green's 1968 study "Lucid Dreams". analyzed the main characteristics of such dreams, reviewing previously published literature on the subject and incorporating new data from participants of her own. She concluded that lucid dreams were a category of experience quite distinct from ordinary dreams, and predicted that they would turn out to be associated with rapid eye movement sleep (REM sleep). Green was also the first to link lucid dreams to the phenomenon of false awakenings.
Philosopher Norman Malcolm's 1959 text "Dreaming" had argued against the possibility of checking the accuracy of dream reports. He points out "The only criterion of the truth of a statement that someone has had a certain dream is, essentially, his saying so.". The realization that eye movements performed in dreams may affect the dreamer's physical eyes provided a way to prove that actions agreed upon during waking life could be recalled and performed once lucid in a dream. The first evidence of this type was produced in the late 1970s by British parapsychologist Keith Hearne. A volunteer named Alan Worsley used eye movements to signal the onset of lucidity, which were recorded by a polysomnograph machine.
Hearne's results were not widely distributed. The first peer-reviewed article was published some years later by Stephen LaBerge at Stanford University, who had independently developed a similar technique as part of his doctoral dissertation. During the 1980s, further scientific evidence of lucid dreaming was produced as lucid dreamers were able to demonstrate to researchers that they were consciously aware of being in a dream state (again, primarily using eye movement signals). Additionally, techniques were developed that have been experimentally proven to enhance the likelihood of achieving this state.
Paul Tholey, an oneirologist and Gestalt theorist laid the epistemological basis for the research of lucid dreams. His work laid the foreground for further researchers to categorize what a lucid dream is. Tholey (1980, 1981) defined seven different conditions of clarity that a dream must fulfill in order to be defined as a lucid dream:
For a dream to be lucid as defined by Tholey, it must fulfill all 7 factors together. Tholey replaces the word ‘Klarheit’ (clarity) with the word ‘awareness’, which is a well
known and central term in Gestalt therapy theory and describes the subjective experience
of the conscious dream state quite well (Lucid dreaming – dreams of clarity).
Initiation.
A lucid dream can begin in one of many ways. A dream-initiated lucid dream (D.I.L.D.) starts as a normal dream, and the dreamer eventually concludes it is a dream. A wake-initiated lucid dream (W.I.L.D.) occurs when the dreamer goes from a normal waking state directly into a dream state, with no apparent lapse in consciousness. The wake-initiated lucid dream occurs when the sleeper enters REM sleep with unbroken self-awareness directly from the waking state.
"Induction of self awareness in dreams through frontal low current stimulation of gamma activity", by Ursula Voss et al., was published online 11 May 2014 in "Nature Neuroscience". A popular science report of this article
at the website IFLS, noted that "The authors see potential for lucid dreaming to help people with post traumatic stress disorder (PTSD) or otherwise prone to nightmares to get their fears under control."
Pre-lucid dream.
A pre-lucid dream is one in which the dreamer considers the question, "Am I asleep and dreaming?" The dreamer may or may not come to the correct conclusion. Such experiences are liable to occur to people who are deliberately cultivating lucid dreams, but may also occur spontaneously to those with no prior intention to achieve lucidity in dreams.
Research and clinical applications.
Neurobiological model.
Neuroscientist J. Allan Hobson has hypothesized what might be occurring in the brain while lucid. The first step to lucid dreaming is recognizing one is dreaming. Once this area is activated and the recognition of dreaming occurs, the dreamer must be cautious to let the dream continue but be conscious enough to remember that it is a dream. While maintaining this balance, the amygdala and parahippocampal cortex might be less intensely activated. To continue the intensity of the dream hallucinations, it is expected the pons and the parieto-occipital junction stay active.
In a study performed by Stephen LaBerge, four subjects were compared either singing while dreaming or counting while dreaming. LaBerge found that the right hemisphere was more active during singing and the left hemisphere was more active during counting. These results resemble similar findings in subjects that were awake. This study was a precursor to the question LaBerge has always tried to answer, is there a brain state for lucid dreaming? Physiological recordings like electroencephalograms (EEG) have shown that lucid dreams begin in the Rapid Eye Movement (REM) stage of sleep. In fact, multiple studies led by LaBerge have only shown lucid dreams to occur in the REM stage. Nonetheless it is not impossible that lucid dreams could occur in any of the other sleep stages, which is why physiological tests are highly valuable to this topic. Polysomnography is another beneficial physiological study to conduct because it collects data from a number of sources, inevitably compounding all the information to give the researcher a well-rounded understanding of the human physiology while an individual is experiencing a lucid dream. Polysomnography records your brain waves, the oxygen level in your blood, heart rate and breathing, eye movements, leg movements, and many other variables during the study. Via polysomnographic technology LaBerge, Levitan and Dement (1986) compared eye movements, heart rate, blood pressure and skin potential in both lucid and non-lucid dreams and showed that lucid dreams occurred in the REM stage characterized by increased physiological activation.
Treatment for nightmares.
It has been suggested that sufferers of nightmares could benefit from the ability to be aware they are indeed dreaming. A pilot study was performed in 2006 that showed that lucid dreaming therapy treatment was successful in reducing nightmare frequency. This treatment consisted of exposure to the idea, mastery of the technique, and lucidity exercises. It was not clear what aspects of the treatment were responsible for the success of overcoming nightmares, though the treatment as a whole was successful.
Australian psychologist Milan Colic has explored the application of principles from narrative therapy with clients' lucid dreams, to reduce the impact not only of nightmares during sleep, but also depression, self-mutilation, and other problems in waking life. Colic found that clients preferred direction for their lives, as identified during therapeutic conversations, could lessen the distressing content of dreams, while understandings about life—and even characters—from lucid dreams could be invoked in "real" life with marked therapeutic benefits.
Psychotherapists have applied lucid dreaming as an application for therapy. Studies have shown that by inducing a lucid dream recurrent nightmares can be alleviated. This alleviation is unclear whether it is due to lucidity or the ability to alter the dream itself. A study performed by Victor Spoormaker and van den Bout (2006) evaluated the validity of lucid dreaming treatment (LDT) in chronic nightmare sufferers. LDT is composed of exposure, mastery, and lucidity exercises. Results of lucid dreaming treatment revealed that the nightmare frequency of the treatment groups had decreased. In another study, Spoormaker, van den Bout, and Meijer (2003) investigated lucid dreaming treatment for nightmares by testing 8 subjects who received a one-hour individual session, which consisted of lucid dreaming exercises. The results of the study revealed that the nightmare frequency had decreased and the sleep quality had slightly increased. Holzinger, Klösch and Saletu managed a psychotherapy study under the working name of ‘Cognition during dreaming – a therapeutic intervention in nightmares’, which included 40 subjects, men and women, 18–50 years old, whose life quality was significantly altered by nightmares. The test subjects were administered Gestalt group therapy and 24 of them were also taught to enter the state of lucid dreaming by Holzinger. This was purposefully taught in order to change the course of their nightmares. The subjects then reported the diminishment of their nightmare prevalence from 2-3 times a week to 2-3 times per month.
Perception of time.
In 1985, Stephen LaBerge performed a pilot study that showed that time perception while counting during a lucid dream is about the same as during waking life. Lucid dreamers counted out ten seconds while dreaming, signaling the start and the end of the count with a pre-arranged eye signal measured with electrooculogram recording. LaBerge's results were confirmed by German researchers in 2004. The German study, by D. Erlacher and M. Schredl, also studied motor activity and found deep knee bends took 44% longer to perform while lucid dreaming.
Awareness and reasoning.
While dream control and dream awareness are correlated, neither requires the other—LaBerge found dreams that exhibit one clearly without the capacity for the other; also, in some dreams where the dreamer is lucid and aware they could exercise control, they choose simply to observe. In 1992, a study by Deirdre Barrett examined whether lucid dreams contained four "corollaries" of lucidity: 
Barrett found less than a quarter of lucidity accounts exhibited all four. A related and reciprocal category of dreams that are lucid in terms of some of these four corollaries, but miss the realization that "I'm dreaming," were also reported. Scores on these corollaries and correctly identifying the experience as a dream increased with lucidity experience. In a later study in Barrett's book, "The Committee of Sleep", she describes how some experienced lucid dreamers have learned to remember specific practical goals such as artists looking for inspiration seeking a show of their own work once they become lucid or computer programmers looking for a screen with their desired code. However, most of these dreamers had many experiences of failing to recall waking objectives before gaining this level of control.
In addition, a case study published in 2012 suggests that during a lucid dream the bilateral frontopolar area of the brain (usually attributed to the evaluation of ones own thoughts and feelings etc.) undergoes an increase in activity.
Skepticism.
According to Dr. Patrick McNamara of Boston University, there is no scientific way to know for certain that someone is dreaming other than to wake them up and ask them. Professor Norman Malcolm goes on to say that the only criterion of the truth of a statement that someone has had a certain dream is the dreamer saying so. Malcolm further describes lucid dreaming as absurd and impossible, recalling as an example, "I dreamt that I realised I was dreaming, dreamt that I was affecting the course of my dream, and then dreamt that I woke myself up by telling myself to wake up." 
Other researchers have described the phenomenon of lucid dreaming not as a part of sleep, but as a brief wakeful state, or "micro-awakening". Experiments by Stephen LaBerge used "perception of the outside world" as a criterion for wakefulness while studying lucid dreamers. Although their sleep state was corroborated with physiological measurements, LaBerge admits the criterion is subjective. Physiologically, brain activity during REM sleep is similar to wakefulness. Dr. John Allan Hobson illustrates the ambiguity of these experiments, as LaBerge's subjects always experienced their lucid dream while in a state of REM. Hobson concludes that lucid dreaming is a state of both waking and dreaming. Fellow dream researcher Michael Schredl found Hobson's conclusion to be over-simplifying, saying that the physiological state of lucid dreamers appears to be closer to other states of consciousness, such as meditation, than to wakefulness.
Cultural history.
Even though it has only come to the attention of the general public in the last few decades, lucid dreaming is not a modern discovery. Lucid dreaming is the western term used to denote a practice similar to Yoga nidra. The distinguishing difference is the degree to which one remains cognizant of the actual physical environment as opposed to a dream environment. In lucid dreaming, we are only (or mainly) cognizant of the dream environment, and have little or no awareness of our actual environment. The concept of Yoga nidra is very ancient in Indian traditions such as Hinduism and Buddhism. Krishna is often associated with Yoga nidra in epic "Mahabharata". Similarly, many yogis and rishis are supposed to have experienced Yoga nidra throughout their life.
In modern times, Yoga nidra was experienced by Swami Satyananda Saraswati when he was living with his guru Swami Sivananda in Rishikesh. He began studying the tantric scriptures and, after practice, constructed a system of relaxation, which he began popularizing in the mid 20th century. He explained yoga nidra as a state of mind between wakefulness and sleep that opened deep phases of the mind, suggesting a connection with the ancient tantric practice called nyasa, whereby Sanskrit mantras are mentally placed within specific body parts, while meditating on each part (of the bodymind). The form of practice taught by Swami Satyananda includes eight stages (Internalisation, Sankalpa, Rotation of Consciousness, Breath Awareness, Manifestation of Opposites, Creative Visualization and Externalisation).
Also, in early Buddhism it was a common practice among people in the monastic community. As preserved in the ancient Sarvastivada school's Sutra on Mindfulness of the Body in the Madhayama agama (equivalent of Pali Kayagatasati) it states that monks and nuns under practice should be 'Understanding (having awareness in) the four postures and states of being asleep or awake'. Documented since the 8th century, Tibetan Buddhists and Bonpo were practicing a form of dream yoga held to maintain full waking consciousness while in the dream state. One important message of the book is the distinction between the Dzogchen meditation of "awareness" and "dream yoga". The Dzogchen "awareness" meditation has also been referred to by the terms "rigpa awareness", "contemplation", and "presence". "Awareness" during the sleep and dream states is associated with the Dzogchen "practice of natural light". This practice only achieves lucid dreams as a secondary effect—in contrast to dream yoga, which aims primarily at lucid dreaming. According to Buddhist teachers, the experience of lucidity helps us understand the unreality of phenomena, which would otherwise be overwhelming during dream or the death experience.
In Western culture, the phenomenon had been referred to by Greek philosopher Aristotle who had written: "often when one is asleep, there is something in consciousness which declares that what then presents itself is but a dream". The physician Galen of Pergamon used lucid dreams in his therapy. Also in a letter written by St. Augustine of Hippo in 415 AD about a story of a dreamer, Doctor Gennadius, refers to lucid dreaming. The dreamer reported that he didn't realize he was in the dream world but the man whom he met in his dream reminded him about this and pointed out that his experience was a proof of life after death.
An early recorded lucid dreamer was the philosopher and physician Sir Thomas Browne (1605–1682). Browne was fascinated by the world of dreams and described his own ability to lucid dream in his "Religio Medici": "...yet in one dream I can compose a whole Comedy, behold the action, apprehend the jests and laugh my self awake at the conceits thereof". Similarly, Samuel Pepys in his diary entry for 15 August 1665 records a dream "that I had my Lady Castlemayne in my arms and was admitted to use all the dalliance I desired with her, and then dreamt that this could not be awake, but that it was only a dream". Marquis d'Hervey de Saint-Denys argued that it is possible for anyone to learn to dream consciously. In 1867, he published his book "Les Rêves et les moyens de les diriger; observations pratiques" ("Dreams and How to Guide them; Practical Observations"), in which he documented more than twenty years of his own research into dreams.
The term "lucid dreaming" was coined by Dutch author and psychiatrist Frederik van Eeden in his 1913 article "A Study of Dreams". This paper was highly anecdotal and not embraced by the scientific community. Some consider this a misnomer because it means much more than just "clear or vivid" dreaming. The alternative term "conscious dreaming" avoids this confusion. However, the term "lucid" was used by van Eeden in its sense of "having insight", as in the phrase "a lucid interval" applied to someone in temporary remission from a psychosis, rather than as a reference to the perceptual quality of the experience, which may or may not be clear and vivid.
In the 1950s, the Senoi hunter-gatherers of Malaysia were reported to make extensive use of lucid dreaming to ensure mental health, although later studies refuted these claims.
A 2012 report by the BBC claimed that "interest in lucid dreaming has grown in recent years", and corroborated this with examples of the many smartphone apps that exist to help people experience the phenomenon. One such app was downloaded half a million times in six weeks, the report says.
Other associated phenomena.
Rapid eye movement (REM).
When a person is dreaming, the eyes shift rapidly. Scientific research has found that these eye movements may correspond to the direction the dreamer "looks" at in the dreamscape. This has enabled trained lucid dreamers to communicate with researchers while dreaming by using eye movement signals.
False awakening.
In a false awakening, one dreams of having awoken. The room the dreamer falsely awakens in is often similar to the room he/she fell asleep in. If the person was lucid, they often believe that they are no longer dreaming and begin their morning routine. The dreamer remains naive to the dream either until they realize they haven't actually woken up or until they wake up again (whether false or real).
Sleep paralysis.
During sleep the body paralyzes itself as a protection mechanism to prevent the movements that occur in the dream from causing the physical body to move. However, this mechanism can be triggered before, during, or after normal sleep while the brain awakens. This can lead to a state where the awakened sleeper feels paralyzed. Hypnagogic hallucination may occur in this state, especially auditory ones. Effects of sleep paralysis include heaviness or inability to move the muscles, rushing or pulsating noises, and brief hypnogogic or hypnopompic imagery.
Out-of-body experience.
An out-of-body experience (OBE or sometimes OOBE) is an experience that typically involves a sensation of floating outside of one's body and, in some cases, perceiving one's physical body from a place outside one's body (autoscopy). About one in ten people claim to have had an out-of-body experience at some time in their lives. Scientists are learning about the phenomenon. Some work by neurologists suggests that such experiences are generated by the same brain mechanisms that cause lucid dreams.
Despite some similarities in their phenomenology and induction methods, EEG studies do not suggest an equivalence between OBEs and lucid dreams. Lucidity is strongly associated with stage 1 REM sleep but OBEs are far less consistent, producing EEG traces that can variously resemble stage 3 sleep, a waking, eyes-closed state or other uncategorized states.
 However, while this may suggest that perceived OBEs are a type of lucid dream which takes place in a dream environment that mimics the actual environment of the dreamer, this falls short of supporting the idea that some conscious form of the dreamer actually leaves the body and perceives their external environment while still in a sleeping state.
Rarity.
During most dreams, the person dreaming is not aware that they are dreaming, no matter how absurd or eccentric the dream is. The reason for this has not been determined, and does not have an obvious answer. There have been attempts by various fields of psychology to provide an explanation. For example, some proponents of depth psychology suggest that mental processes inhibit the critical evaluation of reality within dreams.
Certain physiological studies suggest that "seeing is believing" to the brain during any mental state. If the brain perceives something with great clarity or intensity, it will believe that it is real, even when asleep. Dream consciousness is similar to that of a hallucinating awake subject. Dreams or hallucinatory images triggered by the brain stem are considered to be real, even if fantasy. The impulse to accept the experience as real is so strong the dreamer will often invent a memory or a story to cover up an incongruous or unrealistic event in the dream. For example, "That man has two heads!" is not usually followed with "I must be dreaming!" but with something like "Yes, I read in the paper about these famous conjoined twins." Other times there will be an explanation that, in the dream, makes sense and seems very logical. However, when the dreamer awakes, he/she will realize that it is rather far-fetched or even completely absurd.

</doc>
<doc id="18288" url="http://en.wikipedia.org/wiki?curid=18288" title="Lyric">
Lyric

Lyric may refer to:

</doc>
<doc id="18290" url="http://en.wikipedia.org/wiki?curid=18290" title="Light-emitting diode">
Light-emitting diode

A light-emitting diode (LED) is a two-lead semiconductor light source. It is a pn-junction diode, which emits light when activated. When a suitable voltage is applied to the leads, electrons are able to recombine with electron holes within the device, releasing energy in the form of photons. This effect is called electroluminescence, and the color of the light (corresponding to the energy of the photon) is determined by the energy band gap of the semiconductor.
An LED is often small in area (less than 1 mm2) and integrated optical components may be used to shape its radiation pattern.
Appearing as practical electronic components in 1962, the earliest LEDs emitted low-intensity infrared light.
Infrared LEDs are still frequently used as transmitting elements in remote-control circuits, such as those in remote controls for a wide variety of consumer electronics.
The first visible-light LEDs were also of low intensity, and limited to red. Modern LEDs are available across the visible, ultraviolet, and infrared wavelengths, with very high brightness.
Early LEDs were often used as indicator lamps for electronic devices, replacing small incandescent bulbs. They were soon packaged into numeric readouts in the form of seven-segment displays, and were commonly seen in digital clocks.
Recent developments in LEDs permit them to be used in environmental and task lighting. LEDs have many advantages over incandescent light sources including lower energy consumption, longer lifetime, improved physical robustness, smaller size, and faster switching. Light-emitting diodes are now used in applications as diverse as aviation lighting, automotive headlamps, advertising, general lighting, traffic signals, and camera flashes.
However, LEDs powerful enough for room lighting are still relatively expensive, and require more precise current and heat management than compact fluorescent lamp sources of comparable output.
LEDs have allowed new text, video displays, and sensors to be developed, while their high switching rates are also useful in advanced communications technology.
History.
Discoveries and early devices.
Electroluminescence as a phenomenon was discovered in 1907 by the British experimenter H. J. Round of Marconi Labs, using a crystal of silicon carbide and a cat's-whisker detector.
Soviet inventor Oleg Losev reported creation of the first LED in 1927. His research was distributed in Soviet, German and British scientific journals, but no practical use was made of the discovery for several decades. Kurt Lehovec, Carl Accardo and Edward Jamgochian, explained these first light-emitting diodes in 1951 using an apparatus employing SiC crystals with a current source of battery or pulse generator and with a comparison to a variant, pure, crystal in 1953.
Rubin Braunstein of the Radio Corporation of America reported on infrared emission from gallium arsenide (GaAs) and other semiconductor alloys in 1955. Braunstein observed infrared emission generated by simple diode structures using gallium antimonide (GaSb), GaAs, indium phosphide (InP), and silicon-germanium (SiGe) alloys at room temperature and at 77 kelvins.
In 1957, Braunstein further demonstrated that the rudimentary devices could be used for non-radio communication across a short distance. As noted by Kroemer Braunstein".. had set up a simple optical communications link: Music emerging from a record player was used via suitable electronics to modulate the forward current of a GaAs diode. The emitted light was detected by a PbS diode some distance away. This signal was fed into an audio amplifier, and played back by a loudspeaker. Intercepting the beam stopped the music. We had a great deal of fun playing with this setup." This setup presaged the use of LEDs for optical communication applications.
In the fall of 1965, while working at Texas Instruments Inc. in Dallas, TX, James R. Biard and Gary Pittman found that gallium arsenide (GaAs) emitted infrared light when electric current was applied. On August 8, 1962, Biard and Pittman filed a patent titled "Semiconductor Radiant Diode" based on their findings, which described a zinc diffused p–n junction LED with a spaced cathode contact to allow for efficient emission of infrared light under forward bias.
After establishing the priority of their work based on engineering notebooks predating submissions from G.E. Labs, RCA Research Labs, IBM Research Labs, Bell Labs, and Lincoln Lab at MIT, the U.S. patent office issued the two inventors the patent for the GaAs infrared (IR) light-emitting diode (U.S. Patent ), the first practical LED. Immediately after filing the patent, Texas Instruments began a project to manufacture infrared diodes. In October 1962, they announced the first LED commercial product (the SNX-100), which employed a pure GaAs crystal to emit a 900 nm light output.
The first visible-spectrum (red) LED was developed in 1962 by Nick Holonyak, Jr., while working at General Electric Company. Holonyak first reported his LED in the journal "Applied Physics Letters" on the December 1, 1962.
M. George Craford, a former graduate student of Holonyak, invented the first yellow LED and improved the brightness of red and red-orange LEDs by a factor of ten in 1972. In 1976, T. P. Pearsall created the first high-brightness, high-efficiency LEDs for optical fiber telecommunications by inventing new semiconductor materials specifically adapted to optical fiber transmission wavelengths.
Initial commercial development.
The first commercial LEDs were commonly used as replacements for incandescent and neon indicator lamps, and in seven-segment displays, first in expensive equipment such as laboratory and electronics test equipment, then later in such appliances as TVs, radios, telephones, calculators, as well as watches (see list of signal uses).
Until 1968, visible and infrared LEDs were extremely costly, in the order of US$200 per unit, and so had little practical use.
The Monsanto Company was the first organization to mass-produce visible LEDs, using gallium arsenide phosphide (GaAsP) in 1968 to produce red LEDs suitable for indicators. Hewlett Packard (HP) introduced LEDs in 1968, initially using GaAsP supplied by Monsanto. These red LEDs were bright enough only for use as indicators, as the light output was not enough to illuminate an area. Readouts in calculators were so small that plastic lenses were built over each digit to make them legible. Later, other colors became widely available and appeared in appliances and equipment. In the 1970s commercially successful LED devices at less than five cents each were produced by Fairchild Optoelectronics. These devices employed compound semiconductor chips fabricated with the planar process invented by Dr. Jean Hoerni at Fairchild Semiconductor. The combination of planar processing for chip fabrication and innovative packaging methods enabled the team at Fairchild led by optoelectronics pioneer Thomas Brandt to achieve the needed cost reductions. These methods continue to be used by LED producers.
Most LEDs were made in the very common 5 mm T1¾ and 3 mm T1 packages, but with rising power output, it has grown increasingly necessary to shed excess heat to maintain reliability, so more complex packages have been adapted for efficient heat dissipation. Packages for state-of-the-art high-power LEDs bear little resemblance to early LEDs.
The blue and white LED.
The first high-brightness blue LED was demonstrated by Shuji Nakamura of Nichia Corporation in 1994 and was based on InGaN. In parallel, Isamu Akasaki and Hiroshi Amano in Nagoya were working on developing the important GaN nucleation on sapphire substrates and the demonstration of p-type doping of GaN. Nakamura, Akasaki and Amano were awarded the Nobel prize in physics for their work. In 1995, Alberto Barbieri at the Cardiff University Laboratory (GB) investigated the efficiency and reliability of high-brightness LEDs and demonstrated a "transparent contact" LED using indium tin oxide (ITO) on (AlGaInP/GaAs). The existence of blue LEDs and high-efficiency LEDs quickly led to the development of the first white LED, which employed a Y3Al5O12:Ce, or "YAG", phosphor coating to mix down-converted yellow light with blue to produce light that appears white.
In 2001 and 2002, processes for growing gallium nitride (GaN) LEDs on silicon were successfully demonstrated. In January 2012, Osram demonstrated high-power InGaN LEDs grown on silicon substrates commercially. It has been speculated that the use of six-inch silicon wafers instead of two-inch sapphire wafers and epitaxy manufacturing processes could reduce production costs by up to 90%.
Illumination breakthrough.
The invention of the blue LED made possible a simple and effective way to generate white light. By coating a blue LED with a phosphor material a portion of the blue light can be converted to green, yellow and red light. This mixture of colored light will be perceived by humans as white light and can therefore be used for general illumination. The first white LEDs were expensive and inefficient. However the development of LED technology has caused their efficiency and light output to rise exponentially, with a doubling occurring approximately every 36 months since the 1960s, in a way similar to Moore's law. This trend is generally attributed to the parallel development of other semiconductor technologies and advances in optics and material science, and has been called Haitz's law after Dr. Roland Haitz.
As LED materials technology grew more advanced, light output rose, while maintaining efficiency and reliability at acceptable levels. The invention and development of the high-power white-light LED led to use for illumination, and is slowly replacing incandescent and fluorescent lighting (see list of illumination applications).
The blue LED was final piece of the puzzle to create the RGB LED which can produce any color of light. LEDs can now produce over 300 lumens per watt of electricity, while lasting up to 100,000 hours.
Working.
A "P-N junction" can connect the absorbed light energy into its proportional electric current. The same process is reversed here. i.e. the P-N junction emits light when energy is applied on it. This phenomenon is generally called electroluminance, which can be defined as the emission of light from a semi-conductor under the influence of an electric field. The charge carriers recombine in a forward P-N junction as the electrons cross from the N-region and recombine with the holes existing in the P-region. Free electrons are in the conduction band of energy levels, while holes are in the valence energy band. Thus the energy level of the holes will be lesser than the energy levels of the electrons. Some part of the energy must be dissipated in order to recombine the electrons and the holes. This energy is emitted in the form of heat and light.
The electrons dissipate energy in the form of heat for silicon and germanium diodes. But in "Gallium- Arsenide-phosphorus" (GaAsP) and "Gallium-phosphorus" (GaP) semiconductors, the electrons dissipate energy by emitting photons. If the semiconductor is translucent, the junction becomes the source of light as it is emitted, thus becoming a light emitting diode (LED). But when the junction is reverse biased no light will be produced by the LED, and, on the contrary the device may also get damaged.
Technology.
Physics.
The LED consists of a chip of semiconducting material doped with impurities to create a "p-n junction". As in other diodes, current flows easily from the p-side, or anode, to the n-side, or cathode, but not in the reverse direction. Charge-carriers—electrons and holes—flow into the junction from electrodes with different voltages. When an electron meets a hole, it falls into a lower energy level and releases energy in the form of a photon.
The wavelength of the light emitted, and thus its color, depends on the band gap energy of the materials forming the "p-n junction". In silicon or germanium diodes, the electrons and holes usually recombine by a "non-radiative transition", which produces no optical emission, because these are indirect band gap materials. The materials used for the LED have a direct band gap with energies corresponding to near-infrared, visible, or near-ultraviolet light.
LED development began with infrared and red devices made with gallium arsenide. Advances in materials science have enabled making devices with ever-shorter wavelengths, emitting light in a variety of colors.
LEDs are usually built on an n-type substrate, with an electrode attached to the p-type layer deposited on its surface. P-type substrates, while less common, occur as well. Many commercial LEDs, especially GaN/InGaN, also use sapphire substrate.
Most materials used for LED production have very high refractive indices. This means that much light will be reflected back into the material at the material/air surface interface. Thus, light extraction in LEDs is an important aspect of LED production, subject to much research and development.
Refractive index.
Bare uncoated semiconductors such as silicon exhibit a very high refractive index relative to open air, which prevents passage of photons arriving at sharp angles relative to the air-contacting surface of the semiconductor. This property affects both the light-emission efficiency of LEDs as well as the light-absorption efficiency of photovoltaic cells. The refractive index of silicon is 3.96 (590 nm), while air is 1.0002926.
In general, a flat-surface uncoated LED semiconductor chip will emit light only perpendicular to the semiconductor's surface, and a few degrees to the side, in a cone shape referred to as the "light cone", "cone of light", or the "escape cone". The maximum angle of incidence is referred to as the critical angle. When this angle is exceeded, photons no longer escape the semiconductor but are instead reflected internally inside the semiconductor crystal as if it were a mirror.
Internal reflections can escape through other crystalline faces, if the incidence angle is low enough and the crystal is sufficiently transparent to not re-absorb the photon emission. But for a simple square LED with 90-degree angled surfaces on all sides, the faces all act as equal angle mirrors. In this case most of the light can not escape and is lost as waste heat in the crystal.
A convoluted chip surface with angled facets similar to a jewel or fresnel lens can increase light output by allowing light to be emitted perpendicular to the chip surface while far to the sides of the photon emission point.
The ideal shape of a semiconductor with maximum light output would be a microsphere with the photon emission occurring at the exact center, with electrodes penetrating to the center to contact at the emission point. All light rays emanating from the center would be perpendicular to the entire surface of the sphere, resulting in no internal reflections. A hemispherical semiconductor would also work, with the flat back-surface serving as a mirror to back-scattered photons.
Transition coatings.
After the doping of the wafer, it is cut apart into individual dies. Each die is commonly called a chip.
Many LED semiconductor chips are encapsulated or potted in clear or colored molded plastic shells. The plastic shell has three purposes:
The third feature helps to boost the light emission from the semiconductor by acting as a diffusing lens, allowing light to be emitted at a much higher angle of incidence from the light cone than the bare chip is able to emit alone.
Efficiency and operational parameters.
Typical indicator LEDs are designed to operate with no more than 30–60 milliwatts (mW) of electrical power. Around 1999, Philips Lumileds introduced power LEDs capable of continuous use at one watt. These LEDs used much larger semiconductor die sizes to handle the large power inputs. Also, the semiconductor dies were mounted onto metal slugs to allow for heat removal from the LED die.
One of the key advantages of LED-based lighting sources is high luminous efficacy. White LEDs quickly matched and overtook the efficacy of standard incandescent lighting systems. In 2002, Lumileds made five-watt LEDs available with a luminous efficacy of 18–22 lumens per watt (lm/W). For comparison, a conventional incandescent light bulb of 60–100 watts emits around 15 lm/W, and standard fluorescent lights emit up to 100 lm/W.
As of 2012, the Lumiled catalog gives the following as the best efficacy for each color. The watt-per-watt value is derived using the luminosity function.
In September 2003, a new type of blue LED was demonstrated by the company Cree Inc. to provide 24 mW at 20 milliamperes (mA). This produced a commercially packaged white light giving 65 lm/W at 20 mA, becoming the brightest white LED commercially available at the time, and more than four times as efficient as standard incandescents. In 2006, they demonstrated a prototype with a record white LED luminous efficacy of 131 lm/W at 20 mA. Nichia Corporation has developed a white LED with luminous efficacy of 150 lm/W at a forward current of 20 mA. Cree's XLamp XM-L LEDs, commercially available in 2011, produce 100 lm/W at their full power of 10 W, and up to 160 lm/W at around 2 W input power. In 2012, Cree announced a white LED giving 254 lm/W, and 303 lm/W in March 2014
Practical general lighting needs high-power LEDs, of one watt or more. Typical operating currents for such devices begin at 350 mA.
Note that these efficiencies are for the LED chip only, held at low temperature in a lab. Lighting works at higher temperature and with drive circuit losses, so efficiencies are much lower. United States Department of Energy (DOE) testing of commercial LED lamps designed to replace incandescent lamps or CFLs showed that average efficacy was still about 46 lm/W in 2009 (tested performance ranged from 17 lm/W to 79 lm/W).
Efficiency droop.
Efficiency droop is the decrease (up to 20%) in luminous efficacy of LEDs as the electrical current increases above tens of milliamps (mA).
This effect, first reported in 1999, was initially theorized to be related to elevated temperatures. Scientists proved the opposite to be true that, although the life of an LED would be shortened, the efficiency droop is less severe at elevated temperatures. The mechanism causing efficiency droop was identified in 2007 as Auger recombination, which was taken with mixed reaction. In 2013, a study conclusively identified Auger recombination as the cause of efficiency droop.
In addition to being less efficient, operating LEDs at higher electrical currents creates higher heat levels which compromise the lifetime of the LED. Because of this increased heating at higher currents, high-brightness LEDs have an industry standard of operating at only 350 mA, which is a compromise between light output, efficiency, and longevity.
Possible solutions.
Instead of increasing current levels, luminance is usually increased by combining multiple LEDs in one bulb. Solving the problem of efficiency droop would mean that household LED light bulbs would need fewer LEDs, which would significantly reduce costs.
Researchers at the U.S. Naval Research Laboratory have found a way to lessen the efficiency droop. They found that the droop arises from non-radiative Auger recombination of the injected carriers. They created quantum wells with a soft confinement potential to lessen the non-radiative Auger processes.
Researchers at Taiwan National Central University and Epistar Corp are developing a way to lessen the efficiency droop by using ceramic aluminium nitride (AlN) substrates, which are more thermally conductive than the commercially used sapphire. The higher thermal conductivity reduces self-heating effects.
Lifetime and failure.
Solid-state devices such as LEDs are subject to very limited wear and tear if operated at low currents and at low temperatures. Many of the LEDs made in the 1970s and 1980s are still in service in the early 21st century. Typical lifetimes quoted are 25,000 to 100,000 hours, but heat and current settings can extend or shorten this time significantly.
The most common symptom of LED (and diode laser) failure is the gradual lowering of light output and loss of efficiency. Sudden failures, although rare, can also occur. Early red LEDs were notable for their short service life. With the development of high-power LEDs the devices are subjected to higher junction temperatures and higher current densities than traditional devices. This causes stress on the material and may cause early light-output degradation. To quantitatively classify useful lifetime in a standardized manner it has been suggested to use the terms L70 and L50, which is the time it will take a given LED to reach 70% and 50% light output respectively.
LED performance is temperature dependent. Most manufacturers' published ratings of LEDs are for an operating temperature of 25 C. LEDs used outdoors, such as traffic signals or in-pavement signal lights, and that are utilized in climates where the temperature within the light fixture gets very high, could result in low signal intensities or even failure.
LED light output rises at lower temperatures, leveling off, depending on type, at around −30 C. Thus, LED technology may be a good replacement in uses such as supermarket freezer lighting and will last longer than other technologies. Because LEDs emit less heat than incandescent bulbs, they are an energy-efficient technology for uses such as in freezers and refrigerators. However, because they emit little heat, ice and snow may build up on the LED light fixture in colder climates. Similarly, this lack of waste heat generation has been observed to sometimes cause significant problems with street traffic signals and airport runway lighting in snow-prone areas. In response to this problem, some LED lighting systems have been designed with an added heating circuit at the expense of reduced overall electrical efficiency of the system; additionally, research has been done to develop heat sink technologies that will transfer heat produced within the junction to appropriate areas of the light fixture.
Colors and materials.
Conventional LEDs are made from a variety of inorganic semiconductor materials. The following table shows the available colors with wavelength range, voltage drop and material:
RGB.
RGB LEDs consist of three LEDs. Each LED actually has one red, one green and one blue light. These three colored LEDs are capable of producing any color.
Ultraviolet and blue LEDs.
Current bright blue LEDs are based on the wide band gap between semiconductors GaN (gallium nitride) and InGaN (indium gallium nitride). They can be added to existing red and green LEDs to produce the impression of white light. Modules combining the three colors are used in big video screens and in adjustable-color fixtures.
The first blue-violet LED using magnesium-doped gallium nitride was made at Stanford University in 1972 by Herb Maruska and Wally Rhines, doctoral students in materials science and engineering. At the time Maruska was on leave from RCA Laboratories, where he collaborated with Jacques Pankove on related work. In 1971, the year after Maruska left for Stanford, his RCA colleagues Pankove and Ed Miller demonstrated the first blue electroluminescence from zinc-doped gallium nitride, though the subsequent device Pankove and Miller built, the first actual gallium nitride light-emitting diode, emitted green light. In 1974 the U.S. patent office awarded Maruska, Rhines and Stanford professor David Stevenson a patent for their work in 1972 (U.S. Patent ) and today magnesium-doping of gallium nitride continues to be the basis for all commercial blue LEDs and laser diodes. These devices built in the early 1970s had too little light output to be of practical use and research into gallium nitride devices slowed. In August 1989, Cree Inc. introduced the first commercially available blue LED based on the indirect bandgap semiconductor, silicon carbide. SiC LEDs had very low efficiency, no more than about 0.03%, but did emit in the blue portion of the visible light spectrum.
In the late 1980s, key breakthroughs in GaN epitaxial growth and p-type doping ushered in the modern era of GaN-based optoelectronic devices. Building upon this foundation, in 1993 high-brightness blue LEDs were demonstrated.
High-brightness blue LEDs invented by Shuji Nakamura of Nichia Corporation using gallium nitride revolutionized LED lighting, making high-power light sources practical.
Nakamura was awarded the 2006 Millennium Technology Prize for his invention.
Nakamura, Hiroshi Amano and Isamu Akasaki were awarded the Nobel Prize in Physics in 2014 for the invention of the blue LED.
By the late 1990s, blue LEDs became widely available. They have an active region consisting of one or more InGaN quantum wells sandwiched between thicker layers of GaN, called cladding layers. By varying the relative In/Ga fraction in the InGaN quantum wells, the light emission can in theory be varied from violet to amber. Aluminium gallium nitride (AlGaN) of varying Al/Ga fraction can be used to manufacture the cladding and quantum well layers for ultraviolet LEDs, but these devices have not yet reached the level of efficiency and technological maturity of InGaN/GaN blue/green devices. If un-alloyed GaN is used in this case to form the active quantum well layers, the device will emit near-ultraviolet light with a peak wavelength centred around 365 nm. Green LEDs manufactured from the InGaN/GaN system are far more efficient and brighter than green LEDs produced with non-nitride material systems, but practical devices still exhibit efficiency too low for high-brightness applications.
With nitrides containing aluminium, most often AlGaN and AlGaInN, even shorter wavelengths are achievable. Ultraviolet LEDs in a range of wavelengths are becoming available on the market. Near-UV emitters at wavelengths around 375–395 nm are already cheap and often encountered, for example, as black light lamp replacements for inspection of anti-counterfeiting UV watermarks in some documents and paper currencies. Shorter-wavelength diodes, while substantially more expensive, are commercially available for wavelengths down to 240 nm. As the photosensitivity of microorganisms approximately matches the absorption spectrum of DNA, with a peak at about 260 nm, UV LED emitting at 250–270 nm are to be expected in prospective disinfection and sterilization devices. Recent research has shown that commercially available UVA LEDs (365 nm) are already effective disinfection and sterilization devices.
Deep-UV wavelengths were obtained in laboratories using aluminium nitride (210 nm), boron nitride (215 nm) and diamond (235 nm).
White light.
There are two primary ways of producing white light-emitting diodes (WLEDs), LEDs that generate high-intensity white light. One is to use individual LEDs that emit three primary colors—red, green, and blue—and then mix all the colors to form white light. The other is to use a phosphor material to convert monochromatic light from a blue or UV LED to broad-spectrum white light, much in the same way a fluorescent light bulb works.
There are three main methods of mixing colors to produce white light from an LED:
Because of metamerism, it is possible to have quite different spectra that appear white. However, the appearance of objects illuminated by that light may vary as the spectrum varies.
RGB systems.
White light can be formed by mixing differently colored lights; the most common method is to use red, green, and blue (RGB). Hence the method is called multi-color white LEDs (sometimes referred to as RGB LEDs). Because these need electronic circuits to control the blending and diffusion of different colors, and because the individual color LEDs typically have slightly different emission patterns (leading to variation of the color depending on direction) even if they are made as a single unit, these are seldom used to produce white lighting. Nevertheless, this method is particularly interesting in many uses because of the flexibility of mixing different colors, and, in principle, this mechanism also has higher quantum efficiency in producing white light.
There are several types of multi-color white LEDs: di-, tri-, and tetrachromatic white LEDs. Several key factors that play among these different methods, include color stability, color rendering capability, and luminous efficacy. Often, higher efficiency will mean lower color rendering, presenting a trade-off between the luminous efficiency and color rendering. For example, the dichromatic white LEDs have the best luminous efficacy (120 lm/W), but the lowest color rendering capability. However, although tetrachromatic white LEDs have excellent color rendering capability, they often have poor luminous efficiency. Trichromatic white LEDs are in between, having both good luminous efficacy (>70 lm/W) and fair color rendering capability.
One of the challenges is the development of more efficient green LEDs. The theoretical maximum for green LEDs is 683 lumens per watt but as of 2010 few green LEDs exceed even 100 lumens per watt. The blue and red LEDs get closer to their theoretical limits.
Multi-color LEDs offer not merely another means to form white light but a new means to form light of different colors. Most perceivable colors can be formed by mixing different amounts of three primary colors. This allows precise dynamic color control. As more effort is devoted to investigating this method, multi-color LEDs should have profound influence on the fundamental method that we use to produce and control light color. However, before this type of LED can play a role on the market, several technical problems must be solved. These include that this type of LED's emission power decays exponentially with rising temperature,
resulting in a substantial change in color stability. Such problems inhibit and may preclude industrial use. Thus, many new package designs aimed at solving this problem have been proposed and their results are now being reproduced by researchers and scientists.
Correlated color temperature (CCT) dimming for LED technology is regarded as a difficult task, since binning, age and temperature drift effects of LEDs change the actual color value output. Feedback loop systems are used for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.
Phosphor-based LEDs.
This method involves coating LEDs of one color (mostly blue LEDs made of InGaN) with phosphors of different colors to form white light; the resultant LEDs are called phosphor-based or phosphor-converted white LEDs (pcLEDs). A fraction of the blue light undergoes the Stokes shift being transformed from shorter wavelengths to longer. Depending on the color of the original LED, phosphors of different colors can be employed. If several phosphor layers of distinct colors are applied, the emitted spectrum is broadened, effectively raising the color rendering index (CRI) value of a given LED.
Phosphor-based LED efficiency losses are due to the heat loss from the Stokes shift and also other phosphor-related degradation issues. Their luminous efficacies compared to normal LEDs depend on the spectral distribution of the resultant light output and the original wavelength of the LED itself. For example, the luminous efficacy of a typical YAG yellow phosphor based white LED ranges from 3 to 5 times the luminous efficacy of the original blue LED because of the human eye's greater sensitivity to yellow than to blue (as modeled in the luminosity function). Due to the simplicity of manufacturing the phosphor method is still the most popular method for making high-intensity white LEDs. The design and production of a light source or light fixture using a monochrome emitter with phosphor conversion is simpler and cheaper than a complex RGB system, and the majority of high-intensity white LEDs presently on the market are manufactured using phosphor light conversion.
Among the challenges being faced to improve the efficiency of LED-based white light sources is the development of more efficient phosphors. As of 2010, the most efficient yellow phosphor is still the YAG phosphor, with less than 10% Stoke shift loss. Losses attributable to internal optical losses due to re-absorption in the LED chip and in the LED packaging itself account typically for another 10% to 30% of efficiency loss. Currently, in the area of phosphor LED development, much effort is being spent on optimizing these devices to higher light output and higher operation temperatures. For instance, the efficiency can be raised by adapting better package design or by using a more suitable type of phosphor. Conformal coating process is frequently used to address the issue of varying phosphor thickness.
Some phosphor-based white LEDs encapsulate InGaN blue LEDs inside phosphor-coated epoxy. Alternatively, the LED might be paired with a remote phosphor, a preformed polycarbonate piece coated with the phosphor material. Remote phosphors provide more diffuse light, which is desirable for many applications. Remote phosphor designs are also more tolerant of variations in the LED emissions spectrum. A common yellow phosphor material is cerium-doped yttrium aluminium garnet (Ce3+:YAG).
White LEDs can also be made by coating near-ultraviolet (NUV) LEDs with a mixture of high-efficiency europium-based phosphors that emit red and blue, plus copper and aluminium-doped zinc sulfide (ZnS:Cu, Al) that emits green. This is a method analogous to the way fluorescent lamps work. This method is less efficient than blue LEDs with YAG:Ce phosphor, as the Stokes shift is larger, so more energy is converted to heat, but yields light with better spectral characteristics, which render color better. Due to the higher radiative output of the ultraviolet LEDs than of the blue ones, both methods offer comparable brightness. A concern is that UV light may leak from a malfunctioning light source and cause harm to human eyes or skin.
Other white LEDs.
Another method used to produce experimental white light LEDs used no phosphors at all and was based on homoepitaxially grown zinc selenide (ZnSe) on a ZnSe substrate that simultaneously emitted blue light from its active region and yellow light from the substrate.
A new style of wafers composed of gallium-nitride-on-silicon (GaN-on-Si) is being used to produce white LEDs using 200-mm silicon wafers. This avoids the typical costly sapphire substrates in relatively small 100- or 150-mm wafer sizes. It is predicted that by 2020, 40% of all GaN LEDs will be made with GaN-on-Si. Manufacturing large sapphire material is difficult, while large silicon material is cheaper and more abundant. LED companies shifting from using sapphire to silicon should be a minimal investment.
Organic light-emitting diodes (OLEDs).
In an organic light-emitting diode (OLED), the electroluminescent material comprising the emissive layer of the diode is an organic compound. The organic material is electrically conductive due to the delocalization of pi electrons caused by conjugation over all or part of the molecule, and the material therefore functions as an organic semiconductor.
The organic materials can be small organic molecules in a crystalline phase, or polymers.
The potential advantages of OLEDs include thin, low-cost displays with a low driving voltage, wide viewing angle, and high contrast and color gamut. Polymer LEDs have the added benefit of printable and flexible displays. OLEDs have been used to make visual displays for portable electronic devices such as cellphones, digital cameras, and MP3 players while possible future uses include lighting and televisions.
Quantum dot LEDs.
Quantum dots (QD) are semiconductor nanocrystals that possess unique optical properties. Their emission color can be tuned from the visible throughout the infrared spectrum. This allows quantum dot LEDs to create almost any color on the CIE diagram. This provides more color options and better color rendering than white LEDs since the emission spectra is much narrower, characteristic of quantum confined states. There are two types of schemes for QD excitation.
One uses photo excitation with a primary light source LED (typically blue or UV LEDs are used). The other is direct electrical excitation first demonstrated by Alivisatos et al.
One example of the photo-excitation scheme is a method developed by Michael Bowers, at Vanderbilt University in Nashville, involving coating a blue LED with quantum dots that glow white in response to the blue light from the LED. This method emits a warm, yellowish-white light similar to that made by incandescent bulbs. Quantum dots are also being considered for use in white light-emitting diodes in liquid crystal display (LCD) televisions.
In February 2011 scientists at PlasmaChem GmbH could synthesize quantum dots for LED applications and build a light converter on their basis, which could efficiently convert light from blue to any other color for many hundred hours. Such QDs can be used to emit visible or near infrared light of any wavelength being excited by light with a shorter wavelength.
The structure of QD-LEDs used for the electrical-excitation scheme is similar to basic design of OLED. A layer of quantum dots is sandwiched between layers of electron-transporting and hole-transporting materials. An applied electric field causes electrons and holes to move into the quantum dot layer and recombine forming an exciton that excites a QD. This scheme is commonly studied for quantum dot display. The tunability of emission wavelengths and narrow bandwidth is also beneficial as excitation sources for fluorescence imaging. Fluorescence near-field scanning optical microscopy
(NSOM) utilizing an integrated QD-LED has been demonstrated.
In February 2008, a luminous efficacy of 300 lumens of visible light per watt of radiation (not per electrical watt) and warm-light emission was achieved by using nanocrystals.
Types.
The main types of LEDs are miniature, high-power devices and custom designs such as alphanumeric or multi-color.
Miniature.
These are mostly single-die LEDs used as indicators, and they come in various sizes from 2 mm to 8 mm, through-hole and surface mount packages. They usually do not use a separate heat sink. Typical current ratings ranges from around 1 mA to above 20 mA. The small size sets a natural upper boundary on power consumption due to heat caused by the high current density and need for a heat sink.
Common package shapes include round, with a domed or flat top, rectangular with a flat top (as used in bar-graph displays), and triangular or square with a flat top.
The encapsulation may also be clear or tinted to improve contrast and viewing angle.
Researchers at the University of Washington have invented the thinnest LED. It is made of two-dimensional (2-D) flexible materials. It is 3 atoms thick, which is 10 to 20 times thinner than three-dimensional (3-D) LEDs and is also 10,000 times smaller than the thickness of a human hair. These 2-D LEDs are going to make it possible to create smaller, more energy-efficient lighting, optical communication and nano lasers.
There are three main categories of miniature single die LEDs:
5 V and 12 V LEDs are ordinary miniature LEDs that incorporate a suitable series resistor for direct connection to a 5 V or 12 V supply.
Mid-range.
Medium-power LEDs are often through-hole-mounted and mostly utilized when an output of just tens of lumens are needed. They sometimes have the diode mounted to four leads (two cathode leads, two anode leads) for better heat conduction and carry an integrated lens. An example of this is the Superflux package, from Philips Lumileds. These LEDs are most commonly used in light panels, emergency lighting, and automotive tail-lights. Due to the larger amount of metal in the LED, they are able to handle higher currents (around 100 mA). The higher current allows for the higher light output required for tail-lights and emergency lighting.
High-power.
High-power LEDs (HPLEDs) or high-output LEDs (HO-LEDs) can be driven at currents from hundreds of mA to more than an ampere, compared with the tens of mA for other LEDs. Some can emit over a thousand lumens. LED power densities up to 300 W/cm2 have been achieved. Since overheating is destructive, the HPLEDs must be mounted on a heat sink to allow for heat dissipation. If the heat from a HPLED is not removed, the device will fail in seconds. One HPLED can often replace an incandescent bulb in a flashlight, or be set in an array to form a powerful LED lamp.
Some well-known HPLEDs in this category are the Nichia 19 series, Lumileds Rebel Led, Osram Opto Semiconductors Golden Dragon, and Cree X-lamp. As of September 2009, some HPLEDs manufactured by Cree Inc. now exceed 105 lm/W (e.g. the XLamp XP-G LED chip emitting Cool White light) and are being sold in lamps intended to replace incandescent, halogen, and even fluorescent lights, as LEDs grow more cost competitive.
The impact of Haitz's law which describes the exponential rise in light output of LEDs over time can be readily seen in year over year increases in lumen output and efficiency. For example, the CREE XP-G series LED achieved 105 lm/W in 2009, while Nichia released the 19 series with a typical efficacy of 140 lm/W in 2010.
AC driven LED.
LEDs have been developed by Seoul Semiconductor that can operate on AC power without the need for a DC converter. For each half-cycle, part of the LED emits light and part is dark, and this is reversed during the next half-cycle. The efficacy of this type of HPLED is typically 40 lm/W. A large number of LED elements in series may be able to operate directly from line voltage. In 2009, Seoul Semiconductor released a high DC voltage LED, named as 'Acrich MJT', capable of being driven from AC power with a simple controlling circuit. The low-power dissipation of these LEDs affords them more flexibility than the original AC LED design.
Application-specific variations.
Flashing.
Used as attention seeking indicators without requiring external electronics. Flashing LEDs resemble standard LEDs but they contain an integrated multivibrator circuit that causes the LED to flash with a typical period of one second. In diffused lens LEDs this is visible as a small black dot. Most flashing LEDs emit light of one color, but more sophisticated devices can flash between multiple colors and even fade through a color sequence using RGB color mixing.
Bi-color LED.
Two different LED emitters in one case. There are two types of these. One type consists of two dies connected to the same two leads antiparallel to each other. Current flow in one direction emits one color, and current in the opposite direction emits the other color. The other type consists of two dies with separate leads for both dies and another lead for common anode or cathode, so that they can be controlled independently.
Tri-color.
Three different LED emitters in one case. Each emitter is connected to a separate lead so they can be controlled independently. A four-lead arrangement is typical with one common lead (anode or cathode) and an additional lead for each color.
RGB.
Tri-color LEDs with red, green, and blue emitters, in general using a four-wire connection with one common lead (anode or cathode). These LEDs can have either common positive or common negative leads. Others however, have only two leads (positive and negative) and have a built in tiny electronic control unit.
Decorative multicolor.
Incorporates several emitters of different colors supplied by only two lead-out wires. Colors are switched internally simply by varying the supply voltage. (In a cheap 'Melinera' garden lamp supplied by OWIM GmbH & Co KG in 2013 the LEDs are within a clear casting of 5mm diameter, 10mm long which encapsulates 3 LEDs which change between red, green and blue as the DC supply varies between about 2 volts and 3 volts).
Alphanumeric.
Available in seven-segment, starburst and dot-matrix format. Seven-segment displays handle all numbers and a limited set of letters. Starburst displays can display all letters. Dot-matrix displays typically use 5x7 pixels per character. Seven-segment LED displays were in widespread use in the 1970s and 1980s, but rising use of liquid crystal displays, with their lower power needs and greater display flexibility, has reduced the popularity of numeric and alphanumeric LED displays.
Digital RGB.
These are RGB LEDs that contain their own "smart" control electronics. In addition to power and ground, these provide connections for data in, data out, and sometimes a clock or strobe signal. These are connected in a daisy chain, with the data in of the first LED sourced by a microprocessor, which can control the brightness and color of each LED independently of the others. They are used where a combination of maximum control and minimum visible electronics are needed such as strings for Christmas and LED matrices, few even have refresh rates in the kHz range allowing for basic video applications.
Considerations for use.
Power sources.
The current–voltage characteristic of an LED is similar to other diodes, in that the current is dependent exponentially on the voltage (see Shockley diode equation). This means that a small change in voltage can cause a large change in current. If the applied voltage exceeds the LED's forward voltage drop by a small amount, the current rating may be exceeded by a large amount, potentially damaging or destroying the LED. The typical solution is to use constant-current power supplies to keep the current below the LED's maximum current rating. Since most common power sources (batteries, mains) are constant-voltage sources, most LED fixtures must include a power converter, at least a current-limiting resistor.
However, the high resistance of 3 V coin cells combined with the high differential resistance of nitride-based LEDs makes it possible to power such an LED from such a coin cell without an external resistor.
Electrical polarity.
As with all diodes, current flows easily from p-type to n-type material.
However, no current flows and no light is emitted if a small voltage is applied in the reverse direction. If the reverse voltage grows large enough to exceed the breakdown voltage, a large current flows and the LED may be damaged. If the reverse current is sufficiently limited to avoid damage, the reverse-conducting LED is a useful noise diode.
Safety and health.
The vast majority of devices containing LEDs are "safe under all conditions of normal use", and so are classified as "Class 1 LED product"/"LED Klasse 1". At present, only a few LEDs—extremely bright LEDs that also have a tightly focused viewing angle of 8° or less—could, in theory, cause temporary blindness, and so are classified as "Class 2".
The opinion of the French Agency for Food, Environmental and Occupational Health & Safety (ANSES) of 2010, on the health issues concerning LEDs, suggested banning public use of lamps which were in the moderate Risk Group 2, especially those with a high blue component in places frequented by children.
In general, laser safety regulations—and the "Class 1", "Class 2", etc. system—also apply to LEDs.
While LEDs have the advantage over fluorescent lamps that they do not contain mercury, they may contain other hazardous metals such as lead and arsenic. A study published in 2011 states (concerning toxicity of LEDs when treated as waste): "According to federal standards, LEDs are not hazardous except for low-intensity red LEDs, which leached Pb [lead] at levels exceeding regulatory limits (186 mg/L; regulatory limit: 5). However, according to California regulations, excessive levels of copper (up to 3892 mg/kg; limit: 2500), lead (up to 8103 mg/kg; limit: 1000), nickel (up to 4797 mg/kg; limit: 2000), or silver (up to 721 mg/kg; limit: 500) render all except low-intensity yellow LEDs hazardous."
Applications.
LED uses fall into four major categories:
Indicators and signs.
The low energy consumption, low maintenance and small size of LEDs has led to uses as status indicators and displays on a variety of equipment and installations. Large-area LED displays are used as stadium displays and as dynamic decorative displays. Thin, lightweight message displays are used at airports and railway stations, and as destination displays for trains, buses, trams, and ferries.
One-color light is well suited for traffic lights and signals, exit signs, emergency vehicle lighting, ships' navigation lights or lanterns (chromacity and luminance standards being set under the Convention on the International Regulations for Preventing Collisions at Sea 1972, Annex I and the CIE) and LED-based Christmas lights. In cold climates, LED traffic lights may remain snow-covered. Red or yellow LEDs are used in indicator and alphanumeric displays in environments where night vision must be retained: aircraft cockpits, submarine and ship bridges, astronomy observatories, and in the field, e.g. night time animal watching and military field use.
Because of their long life, fast switching times, and their ability to be seen in broad daylight due to their high output and focus, LEDs have been used in brake lights for cars' high-mounted brake lights, trucks, and buses, and in turn signals for some time, but many vehicles now use LEDs for their rear light clusters. The use in brakes improves safety, due to a great reduction in the time needed to light fully, or faster rise time, up to 0.5 second faster than an incandescent bulb. This gives drivers behind more time to react. It is reported that at normal highway speeds, this equals one car length equivalent in increased time to react. In a dual intensity circuit (rear markers and brakes) if the LEDs are not pulsed at a fast enough frequency, they can create a phantom array, where ghost images of the LED will appear if the eyes quickly scan across the array. White LED headlamps are starting to be used. Using LEDs has styling advantages because LEDs can form much thinner lights than incandescent lamps with parabolic reflectors.
Due to the relative cheapness of low output LEDs, they are also used in many temporary uses such as glowsticks, throwies, and the photonic textile Lumalive. Artists have also used LEDs for LED art.
Weather and all-hazards radio receivers with Specific Area Message Encoding (SAME) have three LEDs: red for warnings, orange for watches, and yellow for advisories and statements whenever issued.
Lighting.
With the development of high-efficiency and high-power LEDs, it has become possible to use LEDs in lighting and illumination. Replacement light bulbs have been made, as well as dedicated fixtures and LED lamps. To encourage the shift to very high efficiency lighting, the US Department of Energy has created the L Prize competition. The Philips Lighting North America LED bulb won the first competition on August 3, 2011 after successfully completing 18 months of intensive field, lab, and product testing.
LEDs are used as street lights and in other architectural lighting where color changing is used. The mechanical robustness and long lifetime is used in automotive lighting on cars, motorcycles, and bicycle lights.
LED street lights are employed on poles and in parking garages. In 2007, the Italian village Torraca was the first place to convert its entire illumination system to LEDs.
LEDs are used in aviation lighting. Airbus has used LED lighting in their Airbus A320 Enhanced since 2007, and Boeing uses LED lighting in the 787. LEDs are also being used now in airport and heliport lighting. LED airport fixtures currently include medium-intensity runway lights, runway centerline lights, taxiway centerline and edge lights, guidance signs, and obstruction lighting.
LEDs are also suitable for backlighting for LCD televisions and lightweight laptop displays and light source for DLP projectors (See LED TV). RGB LEDs raise the color gamut by as much as 45%. Screens for TV and computer displays can be made thinner using LEDs for backlighting.
LEDs are used increasingly in aquarium lights. In particular for reef aquariums, LED lights provide an efficient light source with less heat output to help maintain optimal aquarium temperatures. LED-based aquarium fixtures also have the advantage of being manually adjustable to emit a specific color-spectrum for ideal coloration of corals, fish, and invertebrates while optimizing photosynthetically active radiation (PAR), which raises growth and sustainability of photosynthetic life such as corals, anemones, clams, and macroalgae. These fixtures can be electronically programmed to simulate various lighting conditions throughout the day, reflecting phases of the sun and moon for a dynamic reef experience. LED fixtures typically cost up to five times as much as similarly rated fluorescent or high-intensity discharge lighting designed for reef aquariums and are not as high output to date.
The lack of IR or heat radiation makes LEDs ideal for stage lights using banks of RGB LEDs that can easily change color and decrease heating from traditional stage lighting, as well as medical lighting where IR-radiation can be harmful. In energy conservation, the lower heat output of LEDs also means air conditioning (cooling) systems have less heat to dispose of.
LEDs are small, durable and need little power, so they are used in hand held devices such as flashlights. LED strobe lights or camera flashes operate at a safe, low voltage, instead of the 250+ volts commonly found in xenon flashlamp-based lighting. This is especially useful in cameras on mobile phones, where space is at a premium and bulky voltage-raising circuitry is undesirable.
LEDs are used for infrared illumination in night vision uses including security cameras. A ring of LEDs around a video camera, aimed forward into a retroreflective background, allows chroma keying in video productions.
LEDs are used in mining operations, as cap lamps to provide light for miners. Research has been done to improve LEDs for mining, to reduce glare and to increase illumination, reducing risk of injury for the miners.
LEDs are now used commonly in all market areas from commercial to home use: standard lighting, AV, stage, theatrical, architectural, and public installations, and wherever artificial light is used.
LEDs are increasingly finding uses in medical and educational applications, for example as mood enhancement , and new technologies such as AmBX, exploiting LED versatility. NASA has even sponsored research for the use of LEDs to promote health for astronauts.
Data communication and other signaling.
Light can be used to transmit data and analog signals.
Assistive listening devices in many theaters and similar spaces use arrays of infrared LEDs to send sound to listeners' receivers. Light-emitting diodes (as well as semiconductor lasers) are used to send data over many types of fiber optic cable, from digital audio over TOSLINK cables to the very high bandwidth fiber links that form the internet backbone. For some time, computers were commonly equipped with IrDA interfaces, which allowed them to send and receive data to nearby machines via infrared.
Because LEDs can cycle on and off millions of times per second, very high data bandwidth can be achieved.
Sustainable lighting.
Efficient lighting is needed for sustainable architecture. In 2009, a typical 13-watt LED lamp emitted 450 to 650 lumens, which is equivalent to a standard 40-watt incandescent bulb. In 2011, LEDs had become more efficient, so that a 6-watt LED could easily achieve the same results. A standard 40-watt incandescent bulb has an expected lifespan of 1,000 hours, whereas an LED can continue to operate with reduced efficiency for more than 50,000 hours, 50 times longer than the incandescent bulb.
Energy consumption.
In the US, one kilowatt-hour (3.6 MJ) of electricity currently causes an average 1.34 lb of CO2 emission. Assuming the average light bulb is on for 10 hours a day, a 40-watt bulb will cause 196 lb of CO2 emission per year. The 6-watt LED equivalent will only cause 30 lb of CO2 over the same time span. A building’s carbon footprint from lighting can therefore be reduced by 85% by exchanging all incandescent bulbs for new LEDs if a building previously used only incandescent bulbs.
In practice, most buildings that use a lot of lighting use fluorescent lighting, which has 22% luminous efficiency compared with 5% for filaments, so changing to LED lighting would still give a 34% reduction in electrical power use and carbon emissions.
The reduction in carbon emissions depends on the source of electricity. Nuclear power in the United States produced 19.2% of electricity in 2011, so reducing electricity consumption in the U.S. reduces carbon emissions more than in France (75% nuclear electricity) or Norway (almost entirely hydroelectric).
Replacing lights that spend the most time lit results in the most savings, so LED lights in infrequently used locations bring a smaller return on investment.
Economically sustainable.
LED light bulbs could be a cost-effective option for lighting a home or office space because of their very long lifetimes. Consumer use of LEDs as a replacement for conventional lighting system is currently hampered by the high cost and low efficiency of available products. 2009 DOE testing results showed an average efficacy of 35 lm/W, below that of typical CFLs, and as low as 9 lm/W, worse than standard incandescents. However, as of 2011, there are LED bulbs available as efficient as 150 lm/W and even inexpensive low-end models typically exceed 50 lm/W. The high initial cost of commercial LED bulbs is due to the expensive sapphire substrate, which is key to the production process. The sapphire apparatus must be coupled with a mirror-like collector to reflect light that would otherwise be wasted.
Light sources for machine vision systems.
Machine vision systems often require bright and homogeneous illumination, so features of interest are easier to process.
LEDs are often used for this purpose, and this is likely to remain one of their major uses until price drops low enough to make signaling and illumination uses more widespread. Barcode scanners are the most common example of machine vision, and many low cost ones use red LEDs instead of lasers. Optical computer mice are also another example of LEDs in machine vision, as it is used to provide an even light source on the surface for the miniature camera within the mouse. LEDs constitute a nearly ideal light source for machine vision systems for several reasons:
Other applications.
The light from LEDs can be modulated very quickly so they are used extensively in optical fiber and free space optics communications. This includes remote controls, such as for TVs, VCRs, and LED Computers, where infrared LEDs are often used. Opto-isolators use an LED combined with a photodiode or phototransistor to provide a signal path with electrical isolation between two circuits. This is especially useful in medical equipment where the signals from a low-voltage sensor circuit (usually battery-powered) in contact with a living organism must be electrically isolated from any possible electrical failure in a recording or monitoring device operating at potentially dangerous voltages. An optoisolator also allows information to be transferred between circuits not sharing a common ground potential.
Many sensor systems rely on light as the signal source. LEDs are often ideal as a light source due to the requirements of the sensors. LEDs are used as motion sensors, for example in optical computer mice. The Nintendo Wii's sensor bar uses infrared LEDs. Pulse oximeters use them for measuring oxygen saturation. Some flatbed scanners use arrays of RGB LEDs rather than the typical cold-cathode fluorescent lamp as the light source. Having independent control of three illuminated colors allows the scanner to calibrate itself for more accurate color balance, and there is no need for warm-up. Further, its sensors only need be monochromatic, since at any one time the page being scanned is only lit by one color of light. Touch sensing: Since LEDs can also be used as photodiodes, they can be used for both photo emission and detection. This could be used, for example, in a touch-sensing screen that registers reflected light from a finger or stylus.
Many materials and biological systems are sensitive to or dependent on light. Grow lights use LEDs to increase photosynthesis in plants and bacteria and viruses can be removed from water and other substances using UV LEDs for sterilization. Plant growers are interested in LEDs because they are more energy-efficient, emit less heat (can damage plants close to hot lamps), and can provide the optimum light frequency for plant growth and bloom periods compared to currently used grow lights: HPS (high-pressure sodium), metal-halide (MH) or CFL/low-energy. However, LEDs have not replaced these grow lights due to higher price. As mass production and LED kits develop, the LED products will become cheaper.
LEDs have also been used as a medium-quality voltage reference in electronic circuits. The forward voltage drop (e.g., about 1.7 V for a normal red LED) can be used instead of a Zener diode in low-voltage regulators. Red LEDs have the flattest "I"/"V" curve above the knee. Nitride-based LEDs have a fairly steep "I"/"V" curve and are useless for this purpose. Although LED forward voltage is far more current-dependent than a good Zener, Zener diodes are not widely available below voltages of about 3 V.
Further reading.
</dl>

</doc>
<doc id="18291" url="http://en.wikipedia.org/wiki?curid=18291" title="Luxembourgish language">
Luxembourgish language

Luxembourgish, Luxemburgish () or Letzeburgesch ( or ) (Luxembourgish: "Lëtzebuergesch") is a Moselle Franconian variety of West Central German that is spoken mainly in Luxembourg. About 400,000 people worldwide speak Luxembourgish.
Language family.
Luxembourgish belongs to the West Central German group of High German languages and is the primary example of a Moselle Franconian language.
Usage.
Luxembourgish is the national language of Luxembourg and one of three administrative languages, alongside French and German.
Luxembourgish is also spoken in the Arelerland region of Belgium (part of the Province of Luxembourg) and in small parts of Lorraine in France.
In the German Eifel and Hunsrück regions, and in Lorraine, similar local Moselle Franconian dialects of German are spoken. Furthermore, the language is spoken by a few descendants of Luxembourg immigrants in the United States, and another similar Moselle Franconian dialect is spoken by ethnic Germans long settled in Transylvania, Romania (Siebenbürgen).
Moselle Franconian dialects outside the Luxembourg state border tend to have far fewer French loan words, and these mostly remain from the French revolution.
Varieties.
There are several distinct dialect forms of Luxembourgish including Areler (from Arlon), Eechternoacher (Echternach), Kliärrwer (Clervaux), Miseler (Moselle), Stater (Luxembourg), Veiner (Vianden), Minetter (Southern Luxembourg) and Weelzer (Wiltz). Further small vocabulary differences may be seen even between small villages.
Increasing mobility of the population and the dissemination of the language through mass media such as radio and television are leading to a gradual standardisation towards a "Standard Luxembourgish" through the process of koineization.
Surrounding languages.
There is no distinct geographic boundary between the use of Luxembourgish and the use of other closely related High German dialects (for example Lorraine Franconian); it instead forms a dialect continuum of gradual change.
Spoken Luxembourgish is relatively hard to understand for speakers of German who are generally not familiar with Moselle Franconian dialects (or at least other West Central German dialects). However, they can usually read the language to some degree. For those Germans familiar with Moselle Franconian dialects, it is relatively easy to understand and speak Luxembourgish as far as the everyday vocabulary is concerned. However, the large number of French loanwords in Luxembourgish may hamper communication about certain topics, or with certain speakers (who use surpassingly many French loanwords).
There is no intelligibility between Luxembourgish and French or any of the Romance dialects spoken in the adjacent parts of Belgium and France.
Erna Hennicot-Schoepges, President of the Christian Social People's Party of Luxembourg 1995-2003, was active in promoting the language beyond Luxembourg's borders.
Written Luxembourgish.
Standardisation.
A number of proposals for standardising the orthography of Luxembourgish can be documented, going back to the middle of the 19th century. There was no officially recognised system, however, until the adoption of the "OLO" ("ofizjel lezebuurjer ortografi") on 5 June 1946. This orthography provided a system for speakers of all varieties of Luxembourgish to transcribe words the way they pronounced them, rather than imposing a single, standard spelling for the words of the language. The rules explicitly rejected certain elements of German orthography (e.g., the use of "ä" and "ö", the capitalisation of nouns). Similarly, new principles were adopted for the spelling of French loanwords.
This proposed orthography, so different from existing "foreign" standards that people were already familiar with, did not enjoy widespread approval.
A more successful standard eventually emerged from the work of the committee of specialists charged with the task of creating the "Luxemburger Wörterbuch", published in 5 volumes between 1950 and 1977. The orthographic conventions adopted in this decades-long project, set out in Bruch (1955), provided the basis of the standard orthography that became official on 10 October 1975. Modifications to this standard were proposed by the "Conseil permanent de la langue luxembourgeoise" and adopted officially in the spelling reform of 30 July 1999. A detailed explanation of current practice for Luxembourgish can be found in Schanen & Lulling (2003).
Alphabet.
The Luxembourgish alphabet consists of the 26 Latin letters plus three letters with diacritics: "é", "ä", and "ë". In loanwords from French and High German, other diacritics are usually preserved:
Eifeler Regel.
Like many other varieties of Western High German, Luxembourgish has a rule of final "n"-deletion in certain contexts. The effects of this rule (known as the "Eifel Rule") are indicated in writing, and therefore must be taken into account when spelling words and morphemes ending in ⟨n⟩ or ⟨nn⟩. For example:
Grammar.
Nominal syntax.
Luxembourgish has three genders (masculine, feminine, and neuter), and has three cases (nominative, accusative, and dative). These are marked morphologically on determiners and pronouns. As in German, there is no morphological gender distinction in the plural.
The forms of the articles and of some selected determiners are given below:
As seen above, Luxembourgish has plural forms of "en" ("a, an"), namely "eng" in the nominative/accusative and "engen" in the dative. They are not used as indefinite articles, which—as in German and English—do not exist in the plural, but they do occur in the compound pronouns "wéi en" ("what, which") and "sou en" ("such"). For example: "wéi eng Saachen" ("what things"); "sou eng Saachen" ("such things"). Moreover, they are used before numbers to express an estimation: "eng 30.000 Spectateuren" ("some 30,000 spectators").
Distinct nominative forms survive in a few nominal phrases such as "der Däiwel" ("the devil") and "eiser Herrgott" ("our Lord"). Rare examples of the genitive are also found: "Enn des Mounts" ("end of the month"), "Ufanks der Woch" ("at the beginning of the week"). The functions of the genitive are normally expressed using a combination of the dative and a possessive determiner: e.g. "dem Mann säi Buch" (lit. "to the man his book", i.e. "the man's book"). This is known as a periphrastic genitive, and is a phenomenon also commonly seen in dialectal and colloquial German, and in Dutch.
The forms of the personal pronouns are given in the following table (unstressed forms appear in parentheses):
The 2pl form is also used as a polite singular (like French "vous", see T-V distinction); the forms are capitalised in writing:
Like most varieties of colloquial German, but even more invariably, Luxembourgish uses definite articles with personal names. They are obligatory and not to be translated:
A feature Luxembourgish shares with only some western dialects of German is that women and girls are most often referred to with forms of the "neuter" pronoun "hatt":
Adjectives.
Luxembourgish morphology distinguishes two types of adjective: attributive and predicative. Predicative adjectives appear with verbs like "sinn" ("to be"), and receive no extra ending:
Attributive adjectives are placed before the noun they describe, and change their ending according to the grammatical gender, number, and case:
Interesting to note is how the definite article changes with the use of an attributive adjective: feminine "d" goes to "déi" (or "di"), neuter "d'" goes to "dat", and plural "d'" changes to "déi".
The comparative in Luxembourgish is formed analytically, i.e. the adjective itself is not altered (compare the use of -"er" in German and English; "tall" → "taller", "klein" → "kleiner"). Instead it is formed using the adverb "méi": e.g. "schéin" → "méi schéin"
The superlative involves a synthetic form consisting of the adjective and the suffix "-st": e.g. "schéin" → "schéinst " (compare German "schönst", English "prettiest"). Attributive modification requires the emphatic definite article and the inflected superlative adjective:
Predicative modification uses either the same adjectival structure or the adverbial structure "am"+ -"sten": e.g. "schéin" → "am schéinsten":
Some common adjectives have exceptional comparative and superlative forms:
Several other adjectives also have comparative forms. However, these are not commonly used as normal comparatives, but in special senses:
Word-order.
Luxembourgish exhibits "verb second" word order in clauses. More specifically, Luxembourgish is a V2-SOV language, like German and Dutch. In other words, we find the following finite clausal structures:
Non-finite verbs (infinitives and participles) generally appear in final position:
These rules interact so that in subordinate clauses, the finite verb and any non-finite verbs must all cluster at the end. Luxembourgish allows different word orders in these cases:
This is also the case when two non-finite verb forms occur together:
Luxembourgish (like Dutch and German) allows prepositional phrases to appear after the verb cluster in subordinate clauses:
Vocabulary.
Luxembourgish has borrowed many French words. For example, the name for a bus driver is "Buschauffeur" (also Dutch), which would be "Busfahrer" in German and "chauffeur de bus" in French.
Some words are different from Standard German but have equivalents in German dialects. An example is gromperen (potatoes - German: Kartoffeln). Other words are exclusive to Luxembourgish.
Selected common phrases.
  
"Note: Words spoken in sound clip do not reflect all words on this list."
Neologisms.
Neologisms in Luxembourgish include both entirely new words, and the attachment of new meanings to old words in everyday speech. The most recent neologisms come from the English language in the fields of telecommunications, computer science, and the Internet.
Recent neologisms in Luxembourgish include:
Academic projects.
Between 2000 and 2002, the Luxembourgish linguist, Jérôme Lulling, compiled a lexical database of 125,000 word forms as the basis for the very first Luxembourgish spellchecker (Projet C.ORT.IN.A).
The LaF ("Lëtzebuergesch als Friemsprooch" – Luxembourgish as a Foreign Language) is a set of four language proficiency certifications for Luxembourgish and follows the ALTE framework of language examination standards. The tests are administered by the Institut National des Langues Luxembourg.
The "Centre for Luxembourg Studies" at the University of Sheffield was founded in 1995 on the initiative of Professor Gerald Newton. It is supported by the government of Luxembourg which funds an endowed chair in Luxembourg Studies at the university.
The first class of students to study the language outside of the country as undergraduate students began their studies at the 'Centre for Luxembourg Studies' at Sheffield in the academic year 2011-2012.
Further reading.
In English
In French
In Luxembourgish
In German
External links.
Spellcheckers and dictionaries

</doc>
<doc id="18292" url="http://en.wikipedia.org/wiki?curid=18292" title="Lev Kuleshov">
Lev Kuleshov

Lev Vladimirovich Kuleshov (Russian: Лев Влади́мирович Кулешо́в; 13 January [O.S. 1 January] 1899 – 29 March 1970) was a Soviet filmmaker and film theorist who taught at and helped establish the world's first film school, the Moscow Film School.
Career.
Kuleshov may well be the very first film theorist as he was a leader in Soviet montage theory — developing his theories of editing before those of Sergei Eisenstein (briefly a student of Kuleshov) and Vsevolod Pudovkin. For Kuleshov, the essence of the cinema was editing, the juxtaposition of one shot with another. To illustrate this principle, he created what has come to be known as the Kuleshov Effect. In this now-famous editing exercise, shots of an actor were intercut with various meaningful images (a casket, a bowl of soup, and so on) in order to show how editing changes viewers' interpretations of images.
In addition to his theoretical work, Kuleshov was an active director of feature-length films until 1943. Since 1943 Kuleshov was serving as the academic rector of Gerasimov Institute of Cinematography.

</doc>
<doc id="18295" url="http://en.wikipedia.org/wiki?curid=18295" title="Legacy system">
Legacy system

In computing a legacy system is an old method, technology, computer system, or application program,"of, relating to, or being a previous or outdated computer system." Often a pejorative term, referencing a system as "legacy" often implies that the system is out of date or in need of replacement.
Overview.
The first use of the term legacy to describe computer systems probably occurred in the 1970s. By the 1980s it was commonly used to refer to existing computer systems to distinguish them from the design and implementation of new systems. Legacy was often heard during a conversion process, for example, when moving data from the legacy system to a new database.
While this term may indicate that some engineers may feel that a system is out of date, a legacy system may continue to be used for a variety of reasons. It may simply be that the system still provides for the users' needs. In addition, the decision to keep an old system may be influenced by economic reasons such as return on investment challenges or vendor lock-in, the inherent challenges of change management, or a variety of other reasons other than functionality. Backward compatibility (such as the ability of newer systems to handle legacy file formats and character encodings) is a goal that software developers often include in their work.
Even if it is no longer used, a legacy system may continue to impact the organization due to its historical role. Historic data may not have been converted into the new system format and may exist within the new system with the use of a customized schema crosswalk, or may exist only in a data warehouse. In either case, the effect on business intelligence and operational reporting can be significant. A legacy system may include procedures or terminology which are no longer relevant in the current context, and may hinder or confuse understanding of the methods or technologies used.
Organizations can have compelling reasons for keeping a legacy system, such as:
Problems posed by legacy computing.
Legacy systems are considered to be potentially problematic by some software engineers for several reasons (for example, see Bisbal et al., 1999). 
Improvements on legacy software systems.
Where it is impossible to replace legacy systems through the practice of application retirement, it is still possible to enhance (or "re-face") them. Most development often goes into adding new interfaces to a legacy system. The most prominent technique is to provide a Web-based interface to a terminal-based mainframe application. This may reduce staff productivity due to slower response times and slower mouse-based operator actions, yet it is often seen as an "upgrade", because the interface style is familiar to unskilled users and is easy for them to use. John McCormick discusses such strategies that involve middleware.
Printing improvements are problematic because legacy software systems often add no formatting instructions, or they use protocols that are not usable in modern PC/Windows printers. A print server can be used to intercept the data and translate it to a more modern code. Rich Text Format (RTF) or PostScript documents may be created in the legacy application and then interpreted at a PC before being printed.
Biometric security measures are difficult to implement on legacy systems. A workable solution is to use a telnet or http proxy server to sit between users and the mainframe to implement secure access to the legacy application.
The change being undertaken in some organizations is to switch to Automated Business Process (ABP) software which generates complete systems. These systems can then interface to the organizations' legacy systems and use them as data repositories. This approach can provide a number of significant benefits: the users are insulated from the inefficiencies of their legacy systems, and the changes can be incorporated quickly and easily in the ABP software .
Model-driven reverse and forward engineering approaches can be also used for the improvement of legacy software. Model-driven tools and methodologies can support the migration of legacy software to Cloud computing environments and allow for its modernization, in the notion of Software as a service, exploiting the advanced business and technical characteristics of clouds.
NASA example.
Andreas Hein, from the University of Stuttgart, researched the use of legacy systems in space exploration. According to Hein, legacy systems are attractive for reuse if an organization has the capabilities for verification, validation, testing, and operational history. These capabilities must be integrated into various software life cycle phases such as development, implementation, usage, or maintenance. For software systems, the capability to use and maintain the system are crucial. Otherwise the system will become less and less understandable and maintainable.
According to Hein, verification, validation, testing, and operational history increases the confidence in a system's reliability and quality. However, accumulating this history is often expensive. NASA's now retired Space Shuttle program used a large amount of 1970s-era technology. Replacement was cost-prohibitive because of the expensive requirement for flight certification. The original hardware completed the expensive integration and certification requirement for flight, but any new equipment would have had to go through that entire process again. This long and detailed process required extensive tests of the new components in their new configurations before a single unit could be used in the Space Shuttle program. Thus any new system that started the certification process becomes a "de facto" legacy system by the time it is approved for flight.
Additionally, the entire Space Shuttle system, including ground and launch vehicle assets, was designed to work together as a closed system. Since the specifications did not change, all of the certified systems and components performed well in the roles for which they were designed. Even before the Shuttle was scheduled to be retired in 2010, NASA found it advantageous to keep using many pieces of 1970s technology rather than to upgrade those systems and recertify the new components.
Additional uses of the term "Legacy" in computing.
The term "legacy support" is often used in conjunction with legacy systems. The term may refer to a feature of modern software. For example, Operating systems with "legacy support" can detect and use older hardware. The term may also be used to refer to a business function; e.g. A software or hardware vendor that is supporting, or providing software maintenance, for older products.
A "legacy" product may be a product that is no longer sold, has lost substantial market share, or is a version of a product that is not current. A legacy product may have some advantage over a modern product making it appealing for customers to keep it around. A product is only truly "obsolete" if it has an advantage to nobody – if no person making a rational decision would choose to acquire it new.
The term "legacy mode" often refers specifically to backward compatibility. A software product that is capable of performing as though it were a previous version of itself, is said to be "running in legacy mode." This kind of feature is common in operating systems and internet browsers, where many applications depend on these underlying components.
The computer mainframe era saw many applications running in legacy mode. In the modern business computing environment, n-tier, or 3-tier architectures are more difficult to place into legacy mode as they include many components making up a single system.
Virtualization technology is a recent innovation allowing legacy systems to continue to operate on modern hardware by running older operating systems and browsers on a software system that emulates legacy hardware.
Brownfield architecture.
The field of Information Technology has borrowed the term "brownfield" from the building industry, where undeveloped land (and especially unpolluted land) is described as "greenfield" and previously developed land – which is often polluted and abandoned – is described as "brownfield".
Alternative view.
There is an alternate point of view — growing since the "Dot Com" bubble burst in 1999 — that legacy systems are simply computer systems that are both installed and working. In other words, the term is not pejorative, but the opposite. Bjarne Stroustrup, creator of the C++ language, addressed this issue succinctly:
"Legacy code" often differs from its suggested alternative by actually working and scaling.—
IT analysts estimate that the cost of replacing business logic is about five times that of reuse, and that is not counting the risks involved in wholesale replacement. Ideally, businesses would never have to rewrite most core business logic; debits must equal credits — they always have, and they always will. New software may increase the risk of system failures and security breaches.
The IT industry is responding to these concerns. "Legacy modernization" and "legacy transformation" refer to the act of reusing and refactoring existing core business logic by providing new user interfaces (typically Web interfaces), sometimes through the use of techniques such as screen scraping and service-enabled access (e.g. through web services). These techniques allow organizations to understand their existing code assets (using discovery tools), provide new user and application interfaces to existing code, improve workflow, contain costs, minimize risk, and enjoy classic qualities of service (near 100% uptime, security, scalability, etc.).
The re-examination of attitudes toward legacy systems is also inviting more reflection on what makes legacy systems as durable as they are. Technologists are relearning that sound architecture, practiced up front, helps businesses avoid costly and risky rewrites in the first place. The most common legacy systems tend to be those which embraced well-known IT architectural principles, with careful planning and strict methodology during implementation. Poorly designed systems often don't last, both because they wear out and because their reliability or usability are low enough that no one is inclined to make an effort to extend their term of service when replacement is an option. Thus, many organizations are rediscovering the value of both their legacy systems themselves and those systems' philosophical underpinnings.
Further reading.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="18297" url="http://en.wikipedia.org/wiki?curid=18297" title="Lamentations">
Lamentations

Lamentations may refer to:

</doc>
<doc id="18298" url="http://en.wikipedia.org/wiki?curid=18298" title="Lunar eclipse">
Lunar eclipse

A lunar eclipse occurs when the Moon passes directly behind the Earth into its umbra (shadow). This can occur only when the Sun, Earth, and Moon are aligned (in "syzygy") exactly, or very closely so, with the Earth in the middle. Hence, a lunar eclipse can only occur the night of a full moon. The type and length of an eclipse depend upon the Moon's location relative to its orbital nodes.
Unlike a solar eclipse, which can only be viewed from a certain relatively small area of the world, a lunar eclipse may be viewed from anywhere on the night side of the Earth. A lunar eclipse lasts for a few hours, whereas a total solar eclipse lasts for only a few minutes at any given place, due to the smaller size of the Moon's shadow. Also unlike solar eclipses, lunar eclipses are safe to view without any eye protection or special precautions, as they are dimmer than the full Moon.
For the date of the next eclipse see the section "Recent and forthcoming lunar eclipses".
Types of lunar eclipse.
The shadow of the Earth can be divided into two distinctive parts: the umbra and penumbra. Within the umbra, there is no direct solar radiation. However, as a result of the Sun's large angular size, solar illumination is only partially blocked in the outer portion of the Earth's shadow, which is given the name penumbra.
A penumbral eclipse occurs when the Moon passes through the Earth's penumbra. The penumbra causes a subtle darkening of the Moon's surface. A special type of penumbral eclipse is a total penumbral eclipse, during which the Moon lies exclusively within the Earth's penumbra. Total penumbral eclipses are rare, and when these occur, that portion of the Moon which is closest to the umbra can appear somewhat darker than the rest of the Moon.
A partial lunar eclipse occurs when only a portion of the Moon enters the umbra. When the Moon travels completely into the Earth's umbra, one observes a total lunar eclipse. The Moon's speed through the shadow is about one kilometer per second (2,300 mph), and totality may last up to nearly 107 minutes. Nevertheless, the total time between the Moon's first and last contact with the shadow is much longer, and could last up to 4 hours. The relative distance of the Moon from the Earth at the time of an eclipse can affect the eclipse's duration. In particular, when the Moon is near its apogee, the farthest point from the Earth in its orbit, its orbital speed is the slowest. The diameter of the umbra does not decrease appreciably within the changes in the orbital distance of the Moon. Thus, a totally eclipsed Moon occurring near apogee will lengthen the duration of totality.
A central lunar eclipse is a total lunar eclipse during which the moon passes through the centre of the Earth's shadow. These are relatively rare.
Selenelion.
A selenelion or selenehelion occurs when both the Sun and the eclipsed Moon can be observed at the same time. This can only happen just before sunset or just after sunrise, and both bodies will appear just above the horizon at nearly opposite points in the sky. This arrangement has led to the phenomenon being referred to as a horizontal eclipse. There are typically a number of high ridges undergoing sunrise or sunset that can see it. Although the Moon is in the Earth’s umbra, the Sun and the eclipsed Moon can both be seen at the same time because the refraction of light through the Earth’s atmosphere causes each of them to appear higher in the sky than their true geometric position.
Danjon scale.
The following scale (the Danjon scale) was devised by André Danjon for rating the overall darkness of lunar eclipses:
Lunar versus solar eclipse.
There is often confusion between a solar and lunar eclipse. While both involve interactions between the Sun, Earth, and Moon, they are very different in their interactions.
Lunar eclipse appearance.
The Moon does not completely disappear as it passes through the umbra because of the refraction of sunlight by the Earth's atmosphere into the shadow cone; if the Earth had no atmosphere, the Moon would be completely dark during an eclipse. The reddish coloration arises because sunlight reaching the Moon must pass through a long and dense layer of the Earth's atmosphere, where it is scattered. Shorter wavelengths are more likely to be scattered by the air molecules and the small particles, and so by the time the light has passed through the atmosphere, the longer wavelengths dominate. This resulting light we perceive as red. This is the same effect that causes sunsets and sunrises to turn the sky a reddish color; an alternative way of considering the problem is to realize that, as viewed from the Moon, the Sun would appear to be setting (or rising) behind the Earth.
The amount of refracted light depends on the amount of dust or clouds in the atmosphere; this also controls how much light is scattered. In general, the dustier the atmosphere, the more that other wavelengths of light will be removed (compared to red light), leaving the resulting light a deeper red color. This causes the resulting coppery-red hue of the Moon to vary from one eclipse to the next. Volcanoes are notable for expelling large quantities of dust into the atmosphere, and a large eruption shortly before an eclipse can have a large effect on the resulting color.
Solar eclipse appearance.
A solar eclipse occurs when the Moon casts its shadow on Earth whilst passing between the Sun and Earth. Because the Moon's orbit is 5 degrees tilted to the Earth's orbit around the Sun, a solar eclipse is rare. The Moon has an elliptical orbit around the Earth, so the separation of the two varies from about 221,000 to 252,000 miles. When the Moon's distance from the Earth is low, the Moon appears significantly larger and can completely obscure the Sun causing a total solar eclipse. An annular solar eclipse occurs when the Moon is furthest from the Earth. On these occasions the Moon will appear to be smaller and not fully eclipse the sun.
March 1504 lunar eclipse.
When Christopher Columbus came to the New World—specifically, the north coast of Jamaica—he was able to use European scientific understanding to correctly predict a lunar eclipse. The event is known as the March 1504 lunar eclipse, and occurred when Columbus, after he wanted to be seen as god-like, stated that he would make the Moon disappear during the night of February 29, 1504. The reason Columbus wanted to prove he could make the Moon disappear is because he and his crew were eating a great deal of the inhabitants' food, and the inhabitants refused to feed them anymore. Columbus was right in his prediction, for he used astronomical tables and local clocks in order to predict when the lunar eclipse would happen, and was able to convince the inhabitants that he had the power to make the Moon disappear and then reappear. After the inhabitants believed that Columbus was truly able to make the Moon disappear, they begged him to return the Moon to its previous form, and after roughly an allotted amount of time (the amount of time Columbus discerned to be how long the eclipse would last), Columbus agreed to return the Moon, and the Moon began to reappear. The next day, the inhabitants gave Columbus and his crew the food they desired.
Lunar eclipse in culture.
Several cultures have myths related to lunar eclipses or allude to the lunar eclipse as being a good or bad omen. The Egyptians saw the eclipse as a sow swallowing the Moon for a short time; other cultures view the eclipse as the Moon being swallowed by other animals, such as a jaguar in Mayan tradition, or a three legged toad in China. Some societies thought it was a demon swallowing the Moon, and that they could chase it away by throwing stones and curses at it. The Greeks were ahead of their time when they said the Earth was round and used the shadow from the Lunar Eclipse as evidence. Some Hindus believe in the importance of bathing in the Ganges River following an eclipse because it will help you achieve salvation.
Incans.
Similarly to the Mayans, the Incans believed that lunar eclipses were when a jaguar would eat the Moon, which is why a blood moons look red. The Incans also believed that once the jaguar finished eating the Moon, it could come down and devour all the animals on Earth, so they would take spears and shout at the Moon to keep it away.
Mesopotamians.
The ancient Mesopotamians believed that a lunar eclipse was when the Moon was being attacked by seven demons. This attack was more than just one on the Moon, however, for the Mesopotamians linked what happened in the sky with what happens on the land, and because the king of Mesopotamia represented the land, the seven demons were thought to be also attacking the king. In order to prevent this attack on the king, the Mesopotamians made someone pretend to be the king so they would be attacked instead of the true king. After the lunar eclipse was over, the substitute king was made to disappear (possibly by poisoning).
Chinese.
In some Chinese cultures, people would ring bells to prevent a dragon or other wild animals from biting the Moon. In the nineteenth century, during a lunar eclipse, the Chinese navy fired its artillery because of this belief. During the Zhou Dynasty in the Book of Songs, the sight of a red moon engulfed in darkness led them to believe the sign as a foreshadowing of famine or disease.
Blood moon.
Due to its reddish color, a totally eclipsed Moon is sometimes referred to as a "blood moon". In addition, in the 2010s the media started to associate the term "blood moon" with the four full moons of a lunar tetrad, especially the 2014–2015 tetrad coinciding with the feasts of Passover and Tabernacles. A lunar tetrad is a consecutive sequence of four lunar eclipses, spaced six months apart. The most recent blood moon occurred on October 8, 2014 and was visible across much of the Americas and Asia
Occurrence.
Every year, there are at least two lunar eclipses and as many as five, although total lunar eclipses are significantly less common. If one knows the date and time of an eclipse, it is possible to predict the occurrence of other eclipses using an eclipse cycle like the saros.
Recent and forthcoming lunar eclipses.
Eclipses only occur during an eclipse season, when the Sun is close to either the ascending or descending node of the Moon.

</doc>
<doc id="18303" url="http://en.wikipedia.org/wiki?curid=18303" title="Liber Pontificalis">
Liber Pontificalis

The Liber Pontificalis (Latin for "Book of the Popes") is a book of biographies of popes from Saint Peter until the 15th century. The original publication of the "Liber Pontificalis" stopped with Pope Adrian II (867–872) or Pope Stephen V (885–891), but it was later supplemented in a different style until Pope Eugene IV (1431–1447) and then Pope Pius II (1458–1464). Although quoted virtually uncritically from the 8th to 18th century, the "Liber Pontificalis" has undergone intense modern scholarly scrutiny as an "unofficial instrument of pontifical propaganda."
The title "Liber Pontificalis" goes back to the 12th century, although it only became current in the 15th century, and the canonical title of the work since the edition of Duchesne in the 19th century. In the earliest extant manuscripts it is referred to as Liber episcopalis in quo continentur acta beatorum pontificum Urbis Romae, and later the Gesta or Chronica pontificum.
Authorship.
During the Middle Ages, Saint Jerome was considered the author of all the biographies up until those of Pope Damasus I (366–383), based on an apocryphal letter between Saint Jerome and Pope Damasus published as a preface to the Medieval manuscripts. The attribution originated with Rabanus Maurus and is repeated by Martin of Opava, who extended the work into the 13th century. Other sources attribute the early work to Hegesippus and Irenaeus, having been continued by Eusebius of Caesarea.
In the 16th century, Onofrio Panvinio attributed the biographies after Damasus until Pope Nicholas I (858–867) to Anastasius Bibliothecarius; Anastasius continued to be cited as the author into the 17th century, although this attribution was disputed by the scholarship of Caesar Baronius, Ciampini, Schelstrate and others.
The modern interpretation, following that of Louis Duchesne, who compiled the major scholarly edition, is that the "Liber Pontificalis" was gradually and unsystematically compiled, and that the authorship is impossible to determine, with a few exceptions (e.g. the biography of Pope Stephen II (752–757) to papal "Primicerius" Christopher; the biographies of Pope Nicholas I and Pope Adrian II (867–872) to Anastasius). Duchesne and others have viewed the beginning of the "Liber Pontificalis" up until the biographies of Pope Felix III (483–492) as the work of a single author, who was a contemporary of Pope Anastasius II (496-498), relying on "Catalogus Liberianus", which in turn draws from the papal catalogue of Hippolytus of Rome, and the "Leonine Catalogue", which is no longer extant. Most scholars believe the "Liber Pontificalis" was first compiled in the 5th or 6th century.
Because of the use of the "vestiarium", the records of the papal treasury, some have hypothesized that the author of the early "Liber Pontificalis" was a clerk of the papal treasury. Edward Gibbon's "Decline and Fall of the Roman Empire" (1788) summarised the scholarly consensus as being that the "Liber Pontificalis" was composed by "apostolic librarians and notaries of the viiith and ixth centuries" with only the most recent portion being composed by Anastasius. 
Duchesne and others believe that the author of the first addition to the "Liber Pontificalis" was a contemporary of Pope Silverius (536–537), and that the author of another (not necessarily the second) addition was a contemporary of Pope Conon (686–687), with later popes being added individually and during their reigns or shortly after their deaths.
Content.
The "Liber Pontificalis" originally only contained the names of the bishops of Rome and the durations of their pontificates. As enlarged in the 6th century, each biography consists of: the birth name of the pope and that of his father, place of birth, profession before elevation, length of pontificate, historical notes of varying thoroughness, major theological pronouncements and decrees, administrative milestones (including building campaigns, especially of Roman churches), ordinations, date of death, place of burial, and the duration of the ensuing "sede vacante".
Pope Adrian II (867–872) is the last pope for which there are extant manuscripts of the original "Liber Pontificalis": the biographies of Pope John VIII, Pope Marinus I, and Pope Adrian III are missing and the biography of Pope Stephen V (885–891) is incomplete. From Stephen V through the 10th and 11th centuries, the historical notes are extremely abbreviated, usually with only the pope's origin and reign duration.
Extension.
It was only in the 12th century that the "Liber Pontificalis" was systematically continued, although papal biographies exist in the interim period in other sources.
Petrus Guillermi.
Duchesne refers to the 12th century work by Petrus Guillermi in 1142 at the monastery of St. Gilles (Diocese of Reims) as the "Liber Pontificalis of Petrus Guillermi (son of William)". Guillermi's version is mostly copied from other works with small additions or excisions from the papal biographies of Pandulf, nephew of Hugo of Alatri, which in turn was copied almost verbatim from the original "Liber Pontificalis" (with the notable exception of the biography of Pope Leo IX), then from other sources until Pope Honorius II (1124–1130), and with contemporary information from Pope Paschal II (1099–1118) to Pope Urban II (1088–1099). 
Duchesne attributes all biographies from Pope Gregory VII to Urban II to Pandulf, while earlier historians like Giesebrecht and Watterich attributed the biographies of Gregory VII, Victor III, and Urban II to Petrus Pisanus, and the subsequent biographies to Pandulf. These biographies until those of Pope Martin IV (1281–1285) are extant only as revised by Petrus Guillermi in the manuscripts of the monastery of St. Gilles having been taken from the Chronicle of Martin of Opava.
Early in the 14th century, an unknown author built upon the continuation of Petrus Guillermi, adding the biographies of popes Martin IV (d. 1285) through John XXII (1316–1334), with information taken from the "Chronicon Pontificum" of Bernardus Guidonis, stopping abruptly in 1328.
Boso.
Independently, the cardinal-nephew of Pope Adrian IV, Cardinal Boso intended to extend the "Liber Pontificalis" from where it left off with Stephen V, although his work was only published posthumously as the "Gesta Romanorum Pontificum" alongside the "Liber Censuum" of Pope Honorius III. Boso drew on Bonizo of Sutri for popes from John XII to Gregory VII, and wrote from his own experiences about the popes from Gelasius II (1118–1119) to Alexander III (1179–1181).
Western Schism.
An independent continuation appeared in the reign of Pope Eugene IV (1431–1447), appending biographies from Pope Urban V (1362–1370) to Pope Martin V (1417–1431), encompassing the period of the Western Schism. A later recension of this continuation was expanded under Pope Eugene IV.
15th century.
The two collections of papal biographies of the 15th century remain independent, although they may have been intended to be continuations of the "Liber Pontificalis". The first extends from popes Benedict XII (1334–1342) to Martin V (1417–1431), or in one manuscript to Eugene IV (1431–1447). The second extends from Pope Urban VI (1378–1389) to Pope Pius II (1458–1464).
Editions.
The "Liber Pontificalis" was first edited by J. Busæus under the title "Anastasii bibliothecarii Vitæ seu Gesta. Romanorum Pontificum" (Mainz, 1602). A new edition, including the "Historia ecclesiastica" of Anastasius, was edited by Fabrotti (Paris, l647). Another edition, editing the older "Liber Pontificalis" up to Pope Adrian II and adding Pope Stephen VI, was compiled by Fr. Bianchini (4 vols., Rome, 1718–35; a projected fifth volume did not appear). Muratori reprinted Bianchini's edition, adding the remaining popes through John XXII (Scriptores rerum Italicarum, III). Migne also republished Bianchini's edition, adding several appendixes (P. L., CXXVII-VIII).
Modern editions include those of Louis Duchesne ("Liber Pontificalis. Texte, introduction et commentaire", 2 vols., Paris, 1886–92) and Theodor Mommsen ("Gestorum Pontificum Romanorum pars I: Liber Pontificalis", Mon. Germ. hist., Berlin, 1898). Duchesne incorporates the "Annales Romani" (1044–1187) into his edition of the "Liber Pontificalis", which otherwise relies on the two earliest known recensions of the work (530 and 687). Mommsen's edition is incomplete, extending only until 715. Translations and further commentaries appeared throughout the 20th century.

</doc>
<doc id="18306" url="http://en.wikipedia.org/wiki?curid=18306" title="Latin alphabet">
Latin alphabet

The classical Latin alphabet, also known as the Roman alphabet, is a writing system that evolved from the visually similar Cumaean Greek version of the Greek alphabet. The Greek alphabet, including the Cumaean version, descended from the Phoenician abjad. The Etruscans who ruled early Rome adopted and modified the Cumaean Greek alphabet. The Etruscan alphabet was in turn adopted and further modified by the ancient Romans to write the Latin language.
During the Middle Ages scribes adapted the Latin alphabet for writing Romance languages, direct descendants of Latin, as well as Celtic, Germanic, Baltic, and some Slavic languages. With the age of colonialism and Christian evangelism, the Latin script spread beyond Europe, coming into use for writing indigenous American, Australian, Austronesian, Austroasiatic, and African languages. More recently, linguists have also tended to prefer the Latin script or the International Phonetic Alphabet (itself largely based on Latin script) when transcribing or creating written standards for non-European languages, such as the African reference alphabet.
The term "Latin alphabet" may refer to either the alphabet used to write Latin (as described in this article), or other alphabets based on the Latin script, which is the basic set of letters common to the various alphabets descended from the classical Latin one, such as the English alphabet. These Latin alphabets may discard letters, like the Rotokas alphabet, or add new letters, like the Danish and Norwegian alphabets. Letter shapes have evolved over the centuries, including the creation for Medieval Latin of lower-case forms which did not exist in the Classical period.
History.
Origins.
It is generally believed that the Romans adopted the Cumae alphabet, a variant of the Greek alphabet, in the 7th century BC from Cumae, a Greek colony in Southern Italy. (Gaius Julius Hyginus in "Fab. 277" mentions the legend that it was Carmenta, the Cimmerian Sibyl, who altered fifteen letters of the Greek alphabet to become the Latin alphabet, which her son Evander introduced into Latium, supposedly 60 years before the Trojan War, but there is no historically sound basis to this tale.) The Ancient Greek alphabet was in turn based upon the Phoenician abjad. From the Cumae alphabet, the Etruscan alphabet was derived and the Romans eventually adopted 21 of the original 27 Etruscan letters:
Archaic Latin alphabet.
The letter ⟨C⟩ was the western form of the Greek gamma, but it was used for the sounds /ɡ/ and /k/ alike, possibly under the influence of Etruscan, which might have lacked any voiced plosives. Later, probably during the 3rd century BC, the letter ⟨Z⟩ — unneeded to write Latin properly — was replaced with the new letter ⟨G⟩, a ⟨C⟩ modified with a small vertical stroke, which took its place in the alphabet. From then on, ⟨G⟩ represented the voiced plosive /ɡ/, while ⟨C⟩ was generally reserved for the voiceless plosive /k/. The letter ⟨K⟩ was used only rarely, in a small number of words such as "Kalendae", often interchangeably with ⟨C⟩.
Classical Latin alphabet.
After the Roman conquest of Greece in the 1st century BC, Latin adopted the Greek letters ⟨Y⟩ and ⟨Z⟩ (or readopted, in the latter case) to write Greek loanwords, placing them at the end of the alphabet. An attempt by the emperor Claudius to introduce three additional letters did not last. Thus it was during the classical Latin period that the Latin alphabet contained 23 letters:
The Latin names of some of these letters are disputed. In general, however, the Romans did not use the traditional (Semitic-derived) names as in Greek: the names of the plosives were formed by adding /eː/ to their sound (except for ⟨K⟩ and ⟨Q⟩, which needed different vowels to be distinguished from ⟨C⟩) and the names of the continuants consisted either of the bare sound, or the sound preceded by /e/. The letter ⟨Y⟩ when introduced was probably called "hy" /hyː/ as in Greek, the name upsilon not being in use yet, but this was changed to "i Graeca" (Greek i) as Latin speakers had difficulty distinguishing its foreign sound /y/ from /i/. ⟨Z⟩ was given its Greek name, zeta. This scheme has continued to be used by most modern European languages that have adopted the Latin alphabet. For the Latin sounds represented by the various letters see Latin spelling and pronunciation; for the names of the letters in English see English alphabet.
The primary diacritic was the apex used to mark long vowels, which had previously been written double. However, in place of taking an apex, the letter i was written taller: ⟨⟩. For example, what is today transcribed "lūciī a filiī" was written ⟨⟩ in the inscription at right.
The primary mark of punctuation was the interpunct, which was used as a word divider, though it fell out of use after 200 AD.
Old Roman cursive script, also called majuscule cursive and capitalis cursive, was the everyday form of handwriting used for writing letters, by merchants writing business accounts, by schoolchildren learning the Latin alphabet, and even emperors issuing commands. A more formal style of writing was based on Roman square capitals, but cursive was used for quicker, informal writing. It was most commonly used from about the 1st century BC to the 3rd century, but it probably existed earlier than that. It led to Uncial, a majuscule script commonly used from the 3rd to 8th centuries AD by Latin and Greek scribes.
New Roman cursive script, also known as minuscule cursive, was in use from the 3rd century to the 7th century, and uses letter forms that are more recognizable to modern eyes; ⟨a⟩, ⟨b⟩, ⟨d⟩, and ⟨e⟩ had taken a more familiar shape, and the other letters were proportionate to each other. This script evolved into the medieval scripts known as Merovingian and Carolingian minuscule.
Medieval and later developments.
It was not until the Middle Ages that the letter ⟨W⟩ (originally a ligature of two ⟨V⟩s) was added to the Latin alphabet, to represent sounds from the Germanic languages which did not exist in medieval Latin, and only after the Renaissance did the convention of treating ⟨I⟩ and ⟨U⟩ as vowels, and
⟨J⟩ and ⟨V⟩ as consonants, become established. Prior to that, the former had been merely allographs of the latter.
With the fragmentation of political power, the style of writing changed and varied greatly throughout the Middle Ages, even after the invention of the printing press. Early deviations from the classical forms were the uncial script, a development of the Old Roman cursive, and various so-called minuscule scripts that developed from New Roman cursive, of which the Carolingian minuscule was the most influential, introducing the lower case forms of the letters, as well as other writing conventions that have since become standard.
The languages that use the Latin script today generally use capital letters to begin paragraphs and sentences and proper nouns. The rules for capitalization have changed over time, and different languages have varied in their rules for capitalization. Old English, for example, was rarely written with even proper nouns capitalized; whereas Modern English of the 18th century had frequently all nouns capitalized, in the same way that Modern German is written today, e.g. "Alle Schwestern der alten Stadt hatten die Vögel gesehen" ("All of the sisters of the old city had seen the birds").
Spread.
The Latin alphabet spread, along with the Latin language, from the Italian Peninsula to the lands surrounding the Mediterranean Sea with the expansion of the Roman Empire. The eastern half of the Empire, including Greece, Turkey, the Levant, and Egypt, continued to use Greek as a lingua franca, but Latin was widely spoken in the western half, and as the western Romance languages evolved out of Latin, they continued to use and adapt the Latin alphabet.
With the spread of Western Christianity during the Middle Ages, the script was gradually adopted by the peoples of northern Europe who spoke Celtic languages (displacing the Ogham alphabet) or Germanic languages (displacing earlier Runic alphabets), Baltic languages, as well as by the speakers of several Uralic languages, most notably Hungarian, Finnish and Estonian. The alphabet also came into use for writing the West Slavic languages and several South Slavic languages, as the people who spoke them adopted Roman Catholicism. In the 20th century romanization schemes were applied to many languages.

</doc>
<doc id="18307" url="http://en.wikipedia.org/wiki?curid=18307" title="Lugh">
Lugh

Lugh or Lug (/luɣ/; modern Irish: "Lú" /lu:/) is an Irish deity represented in mythological texts as a hero and High King of the distant past. He is known by the epithets "Lámhfhada" (pronounced /'la:wad̪ˠə/, meaning "long arm" or "long hand"), for his skill with a spear or sling, "Ildánach" ("skilled in many arts"), "Samhildánach" ("Equally skilled in many arts"), "Lonnbeimnech" ("fierce striker" or perhaps "sword-shouter") and "Macnia" ("boy hero"), and by the matronymic "mac Ethlenn" or "mac Ethnenn" ("son of Ethliu or Ethniu"). He is a reflex of the pan-Celtic god Lugus, and his Welsh counterpart is Lleu Llaw Gyffes, "The Bright One with the Strong Hand".
Lugh in Irish tradition.
Birth.
Lugh's father is Cian of the Tuatha Dé Danann, and his mother is Ethniu, daughter of Balor, of the Fomorians. In "Cath Maige Tuired" their union is a dynastic marriage following an alliance between the Tuatha Dé and the Fomorians. In the "Lebor Gabála Érenn" Cian gives the boy to Tailtiu, queen of the Fir Bolg, in fosterage.
A folktale told to John O'Donovan by Shane O'Dugan of Tory Island in 1835 recounts the birth of a grandson of Balor who grows up to kill his grandfather. The grandson is unnamed, his father is called Mac Cinnfhaelaidh and the manner of his killing of Balor is different, but it has been taken as a version of the birth of Lugh, and was adapted as such by Lady Gregory. In this tale, Balor hears a druid's prophecy that he will be killed by his own grandson. To prevent this he imprisons his only daughter in the Tór Mór (great tower) of Tory Island, cared for by twelve women, who are to prevent her ever meeting or even learning of the existence of men. On the mainland, Mac Cinnfhaelaidh owns a magic cow who gives such abundant milk that everyone, including Balor, wants to possess her. While the cow is in the care of Mac Cinnfhaelaidh's brother Mac Samthainn, Balor appears in the form of a little red-haired boy and tricks him into giving him the cow. Looking for revenge, Mac Cinnfhaelaidh calls on a "leanan sídhe" (fairy woman) called Biróg, who transports him by magic to the top of Balor's tower, where he seduces Eithne. In time she gives birth to triplets, which Balor gathers up in a sheet and sends to be drowned in a whirlpool. The messenger drowns two of the babies, but unwittingly drops one child into the harbour, where he is rescued by Biróg. She takes him to his father, who gives him to his brother, Gavida the smith, in fosterage.
There may be further triplism associated with his birth. His father in the folktale is one of a triad of brothers, Mac Cinnfhaelaidh, Gavida and Mac Samthainn, and his father in the medieval texts, Cian, is often mentioned together with his brothers Cú and Cethen. Two characters called Lugaid, a popular medieval Irish name thought to derive from Lugh, have three fathers: Lugaid Riab nDerg (Lugaid of the Red Stripes) was the son of the three "Findemna" or fair triplets, and Lugaid mac Con Roí was also known as "mac Trí Con", "son of three hounds". In Ireland's other great "sequestered maiden" story, the tragedy of Deirdre, the king's intended is carried off by three brothers, who are hunters with hounds. The canine imagery continues with Cian's brother Cú ("hound"), another Lugaid, Lugaid Mac Con (son of a hound), and Lugh's son Cúchulainn ("Culann's Hound"). A fourth Lugaid was Lugaid Loígde, a legendary King of Tara and ancestor of (or inspiration for) Lugaid Mac Con.
Lugh joins the Tuatha Dé Danann.
As a young man Lugh travels to Tara to join the court of king Nuada of the Tuatha Dé Danann. The doorkeeper will not let him in unless he has a skill with which to serve the king. He offers his services as a wright, a smith, a champion, a swordsman, a harpist, a hero, a poet and historian, a sorcerer, and a craftsman, but each time is rejected as the Tuatha Dé Danann already have someone with that skill. But when Lugh asks if they have anyone with all those skills simultaneously, the doorkeeper has to admit defeat, and Lugh joins the court and is appointed Chief Ollam of Ireland. He wins a flagstone-throwing contest against Ogma, the champion, and entertains the court with his harp. The Tuatha Dé Danann are at that time oppressed by the Fomorians, and Lugh is amazed how meekly they accept this. Nuada wonders if this young man could lead them to freedom. Lugh is given command over the Tuatha Dé Danann, and he begins making preparations for war.
The sons of Tuireann.
When the sons of Tuireann: Brian, Iuchar and Iucharba kill Lugh's father, Cian (who was in the form of a pig at the time), Lugh sets them a series of seemingly impossible quests as recompense. They achieve them all but are fatally wounded in completing the last one. Despite Tuireann's pleas, Lugh denies them the use of one of the items they have retrieved, a magic pigskin which heals all wounds. They die of their wounds and Tuireann dies of grief over their bodies.
The Battle of Magh Tuireadh.
Using the magic artifacts the sons of Tuireann have gathered, Lugh leads the Tuatha Dé Danann in the Second Battle of Mag Tuireadh against the Fomorians. Nuada is killed in the battle by Balor. Lugh faces Balor, who opens his terrible, poisonous eye that kills all it looks upon, but Lugh shoots a sling-stone that drives his eye out the back of his head, wreaking havoc on the Fomorian army behind. After the victory Lugh finds Bres, the half-Fomorian former king of the Tuatha Dé Danann, alone and unprotected on the battlefield, and Bres begs for his life. If he is spared, he promises, he will ensure that the cows of Ireland always give milk. The Tuatha Dé Danann refuse the offer. He then promises four harvests a year, but the Tuatha Dé Danann say one harvest a year suits them. But Lugh spares his life on the condition that he teach the Tuatha Dé Danann how and when to plough, sow and reap.
Later life and death.
Lugh instituted an event similar to the Olympic games called the Assembly of Talti which finished on Lughnasadh (1 August) in memory of his foster-mother, Tailtiu, at the town that bears her name (now Teltown, County Meath). He likewise instituted Lughnasadh fairs in the areas of Carman and Naas in honour of Carman and Nás, the eponymous tutelary goddess of these two regions. Horse races and displays of martial arts were important activities at all three fairs. However, Lughnasadh itself is a celebration of Lugh's triumph over the spirits of the Other World who had tried to keep the harvest for themselves. It survived long into Christian times and is still celebrated under a variety of names. "Lúnasa" is now the Irish name for the month of August.
According to a poem of the "dindsenchas", Lugh was responsible for the death of Bres. He made 300 wooden cows, and filled them with a bitter, poisonous red liquid which was then "milked" into pails and offered to Bres to drink. Bres, who was under an obligation not to refuse hospitality, drank it down without flinching, and it killed him.
Lugh is said to have invented the board game fidchell. He had a dog called Failinis.
He had several wives, including Buí and Nás, daughters of Ruadri, king of Britain. Buí lived and was buried at Knowth. Nás was buried at Naas, County Kildare, which is named after her. Lugh had a son, Ibic, by Nás. His daughter or sister was Ebliu, who married Fintan. One of his wives, unnamed, had an affair with Cermait, son of the Dagda. Lugh killed him in revenge, but Cermait's sons, Mac Cuill, Mac Cecht and Mac Gréine, killed Lugh in return, drowning him in Loch Lugborta. He had ruled for forty years.
Lugh’s possessions.
Lug possessed a number of magical items, retrieved by the sons of Tuirill Piccreo in Middle Irish redactions of the Lebor Gabála. Not all the items are listed here. The late narrative "Fate of the Children of Tuireann" not only gives a list of items gathered for Lugh, but also endows him with such gifts from the sea god Manannán as the sword Fragarach, the horse Enbarr (Aonbarr), the boat "Scuabtuinne" / "Sguaba Tuinne" (Ir.)(Wave-Sweeper)., his armor and helmet.
Lugh's Spear.
The lore around Lug's Spear is traced as follows:
Four Treasures Spear of Lug.
Lugh's spear ("sleg"), according to the text of The Four Jewels of the Tuatha Dé Danann, was said to be impossible to overcome, taken to Ireland from Gorias (or Findias).
Gae Assail.
Lugh obtained the Spear of Assal (Ir: Gae Assail) as fine ("éric") imposed on the children of Tuirill Piccreo (or Biccreo), according to the short account in "Lebor Gabála Érenn" (Poem LXV, 319) which adds that the incantation "Ibar (Yew)" made the cast always hit its mark, and "Athibar (Re-Yew)" caused the spear to return.
Areadbhar.
In a full narrative version called "[A]oidhe Chloinne Tuireann" (The Fate of the Children of Tuireann), from copies no earlier than the 18th century, Lugh demands the spear named Ar-éadbair or Areadbhair (Early Mod. Irish "Aɼéadḃaiɼ") which belonged to Pisear, king of Persia. Its tip had to be kept immersed in a pot of water to keep it from igniting, a property similar to the Lúin of Celtchar. This spear is also called "Slaughterer" in translation.
Finest Yew of the Wood.
There is yet another name that Lugh's spear goes by: "A [yew] tree, the finest of the wood" (Early Mod. Irish "eó bo háille d'ḟíoḋḃaiḃ"), occurring in an inserted verse within "The Fate of the Children of Tuireann". And "the famous yew of the wood" ("ibar alai fhidbaidha") is also the name that Lugh's spear is called by in a tract which alleges that it, the Lúin of Celtchar and the spear Crimall that blinded Cormac Mac Airt were the one and the same weapon (tract in TCD MS 1336 (olim H 3. 17), col. 723, discussed in the Lúin page).
Sling-stone.
Lugh used the sling-stone ("cloich tabaill") to slay his grandfather, Balor the Strong-Smiter in the Battle of Magh Tuired according to the brief accounts in the Lebor Gabála Érenn. The narrative "Cath Maige Tured", preserved in a unique 16th century copy, words it slightly different saying that Lugh used the sling-stone (here "liic talma" § 133, i.e. "lía" "stone" of the "tailm" "sling") to destroy the evil eye of Balor of the Piercing Eye (Bolur Birugderc).
Tathlum.
A certain poem recorded by O'Curry in English translation says that the missile fired by Lugh was a tathlum ("táthluib" "(slingstone made of) cement").
Nature Myth Items.
Lugh's projectile weapon whether a dart or missile was envisioned by symbolic of lightning-weapon T. F. O'Rahilly. Lugh’s sling rod was the rainbow and the Milky Way which was called "Lugh's Chain". Unllike the rod-sling, Lugh had no need to wield the spear himself. It was alive and thirsted so for blood that only by steeping its head in a sleeping-draught of pounded fresh poppy seeds could it be kept at rest. When battle was near, it was drawn out; then it roared and struggled against its thongs, fire flashed from it, and it tore through the ranks of the enemy once slipped from the leash, never tired of slaying.
Fragarach.
Lugh is also seen girt with the Freagarthach (better known as Fragarach), the sword of Manannán, in the assembly of the Tuatha Dé Danann in the "Fate of the Children of Tuireann".
Lugh’s horse(s) and magic boat.
Lugh had a horse named Aenbharr which could fare over both land and sea. Like much of his equipment, it was furnished to him by the sea god Manannán mac Lir. When the Children of Tuireann asked to borrow this horse, Lugh begrudged them, saying it would not be proper to make a loan of a loan. Consequently Lugh was unable to refuse their request to use Lugh's currach (coracle) or boat, the Wave-Sweeper ("Sguaba Tuinne" (Ir.))
In the Lebor Gabála, Gainne & Rea were the names of the pair of horses belonging to the king of the isle of Sicily [on the (Tyrrhene sea)], which Lug demanded as éric from the sons of Tuirill Briccreo.
Lugh’s hound Failinis.
Failinis was the name of the whelp of the King of Ioruaidhe that Lugh demanded as éiric in the "Oidhead Chloinne Tuireann".
This concurs with the name of the hound mentioned in an "Ossianic Ballad," sometimes referred to by its opening line "Dám Thrír Táncatair Ille (They came here as a band of three)". In the ballad the hound is called Ṡalinnis (Shalinnis) or Failinis (in the Lismore text), and belonged to a threesome from Iruaide whom the Fianna encounter. It is described as "the ancient grayhound.. that had been with Lugh of the Mantles, / Given him by the sons of Tuireann Bicreann;.."
O'Curry's excerpt ends here, but the subsequent verse runs "The three full-fledged heroes are called Sél, Donait and Domhnán. The dog of the fairest figure, Failinis was brought to Finn". These threesome also appear in "Acallamh na Sénorach" though in that work the wonder-dog is called Fer Mac.
Lugh's name and nature.
Lugh's name has been interpreted as deriving from the Proto-Indo-European root *"leuk-", "flashing light", and he is often surrounded by solar imagery, so from Victorian times he has often been considered a sun god, similar to the Greco-Roman Apollo though historically he is only ever equated with Mercury. He appears in folklore as a trickster, and in County Mayo thunderstorms were referred to as battles between Lugh and Balor, so he is sometimes considered a storm god: Alexei Kondratiev notes his epithet "lonnbeimnech" ("fierce striker") and concludes that "if his name has any relation to 'light' it more properly means 'lightning-flash' (as in Breton "luc'h" and Cornish "lughes")". However, Breton and Cornish are Brythonic languages in which Proto-Celtic *"k" did undergo systematic sound changes into "-gh-" and "-ch-".
Words containing Lu, as in the word Lugh itself, or lo or le, have appeared for millennia always meaning light or sun or sun god. Luwian Apaliunas, Hurrian Aplu, Etruscan Apulu, Homeric Greek: Ἀπόλλων, that is λω, Latin Apollo. The form Apaliunas (]x-ap-pa-li-u-na-aš) is attested as a god of Wilusa in a treaty between Alaksandu of Wilusa interpreted as "Alexander of Ilios", and the Hittite great king Muwatalli II ca. 1280 BC.
Luwian is closely related to Hittite, and was among the languages spoken during the second and first millennia BC by population groups in central Anatolia, Anatolia (from Greek Aνατολή Anatolē—"East"; also Asia Minor. When the Illyrians migrated to Italy and founded Luceria in Apulia, a temple to Minerva was built. Minerva is the Etruscan and Roman equivalent of Athena. The arms (armament and weapons) of Diomedes, given to him by Athena in the Trojan War, were said to be were preserved in her temple.
The Lusitanians (or Lusitani in Latin) were an Indo-European people living in the Western Iberian Peninsula. Endovelicus was the most important god. António da Visitação Freire classified the name of "Endovelicus" as a mixed Celtic and Phoenician name, adapted to the Roman language. The "end-" radical would be from Celtic languages, "bel" (or "vel-") would be Phoenician for "lord", and "-cus" is a usual word termination in Latin. The name would suggest Bal, Bel, or Vel, the god Belenus (also Belenos) was a deity worshipped in Gaul, Britain, and Celtic areas of Austria and Spain. In the Roman period Belenus was identified with Apollo. Belisama has been claimed to be the consort of Belenus and she was identified with Minerva/Athena. It would seem that the word Lugh is related to every Indo-European language word meaning light.
Lugh's mastery of all arts has led many to link him with the unnamed Gaulish god Julius Caesar identifies with Mercury, whom he describes as the "inventor of all the arts". Caesar describes the Gaulish Mercury as the most revered deity in Gaul, overseeing journeys and business transactions. Juliette Wood interprets Lugh's name as deriving from the Celtic root *"lugios", "oath", and the Irish word "lugh" connotes ideas of "blasphemy, cussing, lies, bond, joint, binding oath", which strengthens the identification with Mercury, who was, among other attributes, a god of contracts.
It is also worth noting that parallels exist between the Irish Lugh, Gaulish Lugus, German Wotan, the English Woden, and Norse Odin. Odin was worshipped by the Norse as a god of war among other things, including poetry and the arts. Odin may have replaced Tyr as god of war among north Germanic peoples. As such, it may be that Lugh was also worshipped as a god of war by the Irish. On that note it is worth noting that the ultimate Irish warrior hero Cu Chulainn is cited as the son of Lugh.

</doc>
<doc id="18308" url="http://en.wikipedia.org/wiki?curid=18308" title="Lanthanide">
Lanthanide

The lanthanide or lanthanoid series of chemical elements comprises the fifteen metallic chemical elements with atomic numbers 57 through 71, from lanthanum through lutetium. These fifteen lanthanide elements, along with the chemically similar elements scandium and yttrium, are often collectively known as the rare earth elements.
The informal chemical symbol Ln is used in general discussions of lanthanide chemistry to refer to any lanthanide. All but one of the lanthanides are f-block elements, corresponding to the filling of the 4f electron shell; lutetium, a d-block element, is also generally considered to be a lanthanide due to its chemical similarities with the other fourteen. All lanthanide elements form trivalent cations, Ln3+, whose chemistry is largely determined by the ionic radius, which decreases steadily from lanthanum to lutetium.
The lanthanide elements are the group of elements with atomic number increasing from 57 (lanthanum) to 71 (lutetium). They are termed as lanthanides because the lighter elements in the series are chemically similar to lanthanum. Strictly speaking, both lanthanum and lutetium have been labeled as group 3 elements, because they both have a single valence electron in the d shell. However, both elements are often included in any general discussion of the chemistry of the lanthanide elements.
In presentations of the periodic table, the lanthanides and the actinides are customarily shown as two additional rows below the main body of the table, with placeholders or else a selected single element of each series (either lanthanum and actinium, or lutetium and lawrencium) shown in a single cell of the main table, between barium and hafnium, and radium and rutherfordium, respectively. This convention is entirely a matter of aesthetics and formatting practicality; a rarely used wide-formatted periodic table inserts the lanthanide and actinide series in their proper places, as parts of the table's sixth and seventh rows (periods).
Etymology.
Together with scandium and yttrium, the trivial name "rare earths" is sometimes used to describe all the lanthanides. This name arises from the minerals from which they were isolated, which were uncommon oxide-type minerals. However, the use of the name is deprecated by IUPAC, as the elements are neither rare in abundance nor "earths" (an obsolete term for water-insoluble strongly basic oxides of electropositive metals incapable of being smelted into metal using late 18th century technology) . Cerium is the 26th most abundant element in the Earth's crust, neodymium is more abundant than gold and even thulium (the least common naturally occurring lanthanide) is more abundant than iodine, which is itself common enough for biology to have evolved critical usages therefor. Despite their abundance, even the technical term "lanthanides" could be interpreted to reflect a sense of elusiveness on the part of these elements, as it comes from the Greek λανθανειν ("lanthanein"), "to lie hidden". However, if not referring to their natural abundance, but rather to their property of "hiding" behind each other in minerals, this interpretation is in fact appropriate. The etymology of the term must be sought in the first discovery of lanthanum, at that time a so-called new rare earth element "lying hidden" in a cerium mineral, and it is an irony that lanthanum was later identified as the first in an entire series of chemically similar elements and could give name to the whole series. The term "lanthanide" was introduced by Victor Goldschmidt in 1925.
Physical properties of the elements.
Gschneider and Daane (1988) attribute the trend in melting point which increases across the series, (lanthanum (920 °C) – lutetium (1622 °C)) to the extent of hybridisation of the 6s, 5d and 4f orbitals. The hybridisation is believed to be at its greatest for cerium which has the lowest melting point of all, 795 °C.
The lanthanide metals are soft, their hardness increases across the series. Europium stands out as it has the lowest density in the series at 5.24 g/cm3 and the largest metallic radius in the series at 208.4 pm. It can be compared to barium which has a metallic radius of 222 pm. It is believed that the metal contains the larger Eu2+ ion and that there are only two electrons in the conduction band. Ytterbium also has large metallic radius and a similar explanation is suggested.
The resistivities of the lanthanide metals are relatively high, ranging from 29 to 134 μ Ohm·cm. These values can be compared to a good conductor such as aluminium which has a resistivity of 2.655 μ Ohm·cm.
With the exceptions of La, Yb and Lu (which have no unpaired f electrons) the lanthanides are strongly paramagnetic and this is reflected in their magnetic susceptibilities. Gadolinium becomes ferromagnetic at below 16 °C (Curie point). The other heavier lanthanides, terbium, dysprosium, holmium, erbium, thulium and ytterbium become ferromagnetic at much lower temperatures.
Chemistry and compounds.
Effect of 4f orbitals.
Going across the lanthanides, in the periodic table, the 4f orbitals are usually being filled. The effect of the 4f orbitals on the chemistry of the lanthanides is profound and is the factor that distinguishes them from the transition metals. There are seven 4f orbitals and there are two different ways in which they are depicted, firstly as a "cubic set" or as a general set. The cubic set is "f""z3", "f""xz2", "f""yz2", "f""xyz", "f""z(x2−y2)", "f""x(x2−3y2)" and "f""y(3x2−y2)". The 4f orbitals penetrate the [Xe] core and are isolated and do not participate in bonding. This explains why crystal field effects are small and why they do not form π bonds. As there are seven 4f orbitals the number of unpaired electrons can be as high as 7 which gives rise to the large magnetic moments observed for lanthanide compounds. Measuring the magnetic moment can be used to investigate the 4f electron configuration and this is a useful tool in providing an insight into the chemical bonding. The lanthanide contraction, the reduction in size of the Ln3+ ion from La3+(103 pm)- Lu3+(86.1 pm) is often explained by the poor shielding of the 5s and 5p electrons by the 4f electrons.
The electronic structure of the lanthanide elements, with minor exceptions is [Xe]6s24fn. The chemistry of the lanthanides is dominated by the +3 oxidation state and in LnIII compounds the 6s electrons and (usually) one 4f electron are lost and the ions have the configuration [Xe]4fm. All the lanthanide elements exhibit the oxidation state +3. In addition Ce3+ can lose its single f electron to form Ce4+ with the stable electronic configuration of xenon. Also, Eu3+ can gain an electron to form Eu2+ with the f7 configuration which has the extra stability of a half-filled shell. Other than Ce(IV) and Eu(II), none of the lanthanides are stable in oxidation states other than +3 in aqueous solution. Promethium is effectively a man-made element as all its isotopes are radioactive with half-lives shorter than 20 years.
In terms of reduction potentials, the Ln0/3+ couples are nearly the same for all lanthanides, ranging from −1.99 (for Eu) to −2.35 V (for Pr). Thus, these metals are highly reducing, with reducing power similar to alkaline earth metals such as Mg (−2.36 V).
Lanthanide oxidation states.
The ionisation energies for the lanthanides can be compared with aluminium. In aluminium the sum of the first three ionisation energies is 5139 kJ·mol−1, whereas the lanthanides fall in the range 3455 - 4186 kJ·mol−1. This correlates with the highly reactive nature of the lanthanides.
The sum of the first two ionisation energies for europium, 1632 kJ·mol−1 can be compared with that of barium 1468.1 kJ·mol−1 and europium's third ionisation energy is the highest of the lanthanides. The sum of the first two ionisation energies for ytterbium are the second lowest in the series and its third ionisation energy is the second highest. The high third ionisation energy for Eu and Yb correlate with the half filling 4f7 and complete filling 4f14 of the 4f sub shell, and the stability afforded by such configurations due to exchange energy. Europium and ytterbium form salt like compounds with Eu2+ and Yb2+, for example the salt like dihydrides. Both europium and ytterbium dissolve in liquid ammonia forming solutions of Ln2+(NH3)x again demonstrating their similarities to the alkaline earth metals.
The relative ease with which the 4th electron can be removed in cerium and (to a lesser extent praseodymium) indicates why Ce(IV) and Pr(IV) compounds can be formed, for example CeO2 is formed rather than Ce2O3 when cerium reacts with oxygen.
Separation of lanthanides.
The similarity in ionic radius between adjacent lanthanide elements makes it difficult to separate them from each other in naturally occurring ores and other mixtures. Historically, the very laborious processes of cascading and fractional crystallization were used. Because the lanthanide ions have slightly different radii, the lattice energy of their salts and hydration energies of the ions will be slightly different, leading to a small difference in solubility. Salts of the formula Ln(NO3)3·2NH4NO3·4H2O can be used. Industrially, the elements are separated from each other by solvent extraction. Typically an aqueous solution of nitrates is extracted into kerosene containing tri-"n"-butylphosphate. The strength of the complexes formed increases as the ionic radius decreases, so solubility in the organic phase increases. Complete separation can be achieved continuously by use of countercurrent exchange methods. The elements can also be separated by ion-exchange chromatography, making use of the fact that the stability constant for formation of EDTA complexes increases for log K ≈ 15.5 for [La(EDTA)]− to log K ≈ 19.8 for [Lu(EDTA)]−.
Coordination chemistry and catalysis.
When in the form of coordination complexes, lanthanides exist overwhelmingly in their +3 oxidation state, although particularly stable 4f configurations can also give +4 (Ce, Tb) or +2 (Eu, Yb) ions. All of these forms are strongly electropositive and thus lanthanide ions are hard Lewis acids. The oxidation states are also very stable and with the exception of SmI2 and cerium(IV) salts lanthanides are not used for redox chemistry. 4f electrons have a high probability of being found close to the nucleus and are thus strongly affected as the nuclear charge increases across the series; this results in a corresponding decrease in ionic radii referred to as the lanthanide contraction.
The low probability of the 4f electrons existing at the outer region of the atom or ion permits little effective overlap between the orbitals of a lanthanide ion and any binding ligand. Thus lanthanide complexes typically have little or no covalent character and are not influenced by orbital geometries. The lack of orbital interaction also means that varying the metal typically has little effect on the complex (other than size), especially when compared to transition metals. Complexes are held together by weaker electrostatic forces which are omni-directional and thus the ligands alone dictate the symmetry and coordination of complexes. Steric factors therefore dominate, with coordinative saturation of the metal being balanced against inter-ligand repulsion. This results in a diverse range of coordination geometries, many of which are irregular, and also manifests itself in the highly fluxional nature of the complexes. As there is no energetic reason to be locked into a single geometry rapid intramolecular and intermolecular ligand exchange will take place, which typically results in complexes which will rapidly fluctuate between all possible configurations.
Many of these features make lanthanide complexes effective catalysts. Hard Lewis acids are able to polarise bonds upon coordination and thus alter the electrophilicity of compounds, with a classic example being the Luche reduction. The large size of the ions coupled with their labile ionic bonding allows even bulky coordinating species to bind and dissociate rapidly, resulting in very high turnover rates; thus excellent yields can often be achieved with loadings of only a few mol%. The lack of orbital interactions combined with the lanthanide contraction means that the lanthanides change in size across the series but that their chemistry remains much the same. This allows for easy tuning of the steric environments and examples exist where this has been used to improve the catalytic activity of the complex and change the nuclearity of metal clusters.
Despite this, the use of lanthanide coordination complexes as homogeneous catalysts is largely restricted to the laboratory and there are currently few examples them being used on an industrial scale. It should be noted however, that lanthanides exist in many forms other that coordination complexes and many of these are industrially useful. In particular lanthanide metal oxides are used as heterogeneous catalysts in various industrial processes.
Ln(III) compounds.
The trivalent lanthanides mostly form ionic salts. The trivalent ions are hard acceptors and form more stable complexes with oxygen-donor ligands than with nitrogen-donor ligands. The larger ions are 9-coordinate in aqueous solution, [Ln(H2O)9]3+ but the smaller ions are 8-coordinate, [Ln(H2O)8]3+. There is some evidence that the later lanthanides have more water molecules in the second coordination sphere. Complexation with monodentate ligands is generally weak because it is difficult to displace water molecules from the first coordination sphere. Stronger complexes are formed with chelating ligands because of the chelate effect, such as the tetra-anion derived from 1,4,7,10-tetraazacyclododecane-1,4,7,10-tetraacetic acid (DOTA).
Ln(II) and Ln(IV) compounds.
The most common divalent derivatives of the lanthanides are for Eu(II), which achieves a favorable f7 configuration. Divalent halide derivatives are known for all of the lanthanides. They are either conventional salts or are Ln(III) "electride"-like salts. The simple salts include YbI2, EuI2, and SmI2. The electride-like salts, described as Ln3+, 2I−, e−, include LaI2, CeI2 and GdI2. Many of the iodides form soluble complexes with ethers, e.g. TmI2(dimethoxyethane)3. Samarium(II) iodide is a useful reducing agent. Ln(II) complexes can be synthesized by transmetalation reactions.
Ce(IV) in ceric ammonium nitrate is a useful oxidizing agent. Otherwise tetravalent lanthanides are rare. The Ce(IV) is the exception owing to the tendency to form an unfilled f shell.
Hydrides.
Lanthanide metals react exothermically with hydrogen to form LnH2, dihydrides. With the exception of Eu and Yb which resemble the Ba and Ca hydrides (non conducting,transparent salt like compounds) they form black pyrophoric, conducting compounds where the metal sub-lattice is face centred cubic and the H atoms occupy tetrahedral sites. Further hydrogenation produces a trihydride which is non-stoichiometric, non-conducting, more salt like. The formation of trihydride is associated with and increase in 8-10% volume and this is linked to greater localisation of charge on the hydrogen atoms which become more anionic (H− hydride anion) in character.
Halides.
The only tetrahalides known are those of cerium, praseodymium and terbium mirroring the formation of the dioxides.
All of the lanthanides form trihalides with fluorine, chlorine, bromine and iodine. They are all high melting and predominantly ionic in nature. The fluorides are only slightly soluble in water and are not sensitive to air, and this contrasts with the other halides which are air sensitive, readily soluble in water and react at high temperature to form oxohalides.
The trihalides were important as pure metal can be prepared from them. In the gas phase the trihalides are planar or approximately planar, the lighter lanthanides have a lower % of dimers, the heavier lanthanides a higher proportion. The dimers have a similar structure to Al2Cl6
Some of the dihalides are conducting while the rest are insulators. The conducting forms can be considered as LnIII electride compounds where the electron is delocalised into a conduction band, Ln3+ (X−)2(e−). All of the diodides have relatively short metal-metal separations. The CuTi2 structure of the lanthanum, cerium and praseodymium diodides along with HP-NdI2 contain 44 nets of metal and iodine atoms with short metal-metal bonds (393-386 La-Pr). these compounds should be considered to be two-dimensional metals (two-dimensional in the same way that graphite is). The salt like dihalides include those of Eu Dy Tm and Yb. The formation of a relatively stable +2 oxidation state for Eu and Yb is usually explained by the stability (exchange energy) of half filled (f7) and fully filled f14. GdI2 possesses the layered MoS2 structure, is ferromagnetic and exhibits colossal magnetoresistance
The sesquihalides Ln2X3 and the Ln7I12 compounds listed in the table contain metal clusters, discrete Ln6I12 clusters in Ln7I12 and condensed clusters forming chains in the sesquihalides. Scandium forms a similar cluster compound with chlorine, Sc7Cl12 Unlike many transition metal clusters these lanthanide clusters do not have strong metal-metal interactions and this is due to the low number of valence electrons involved, but instead are stabilised by the surrounding halogen atoms.
LaI is the only known monohalide. Prepared from the reaction of LaI3 and La metal, it has a NiAs type structure and can be formulated La3+ (I−)(e−)2.
Oxides and hydroxides.
All of the lanthanides form sesquioxides, Ln2O3. The lighter/larger lanthanides adopt a hexagonal 7-coordinate structure while the heavier/smaller ones adopt a cubic 6-coordinate "C-M2O3" structure. All of the sesquioxides are basic, and absorb water and carbon dioxide from air to form carbonates, hydroxides and hydroxycarbonates. They dissolve in acids to form salts.
Cerium forms a stoichiometric dioxide, CeO2, where cerium has an oxidation state of +4. CeO2 is basic and dissolves with difficulty in acid to form Ce4+ solutions, from which CeIV salts can be isolated, for example the hydrated nitrate Ce(NO3)4.5H2O. CeO2 is used as an oxidation catalyst in catalytic converters. Praseodymium and terbium form non-stoichiometric oxides containing LnIV, although more extreme reaction conditions can produce stoichiometric (or near stoichiometric) PrO2 and TbO2.
Europium and ytterbium form salt-like monoxides, EuO and YbO, which have a rock salt structure. EuO is ferromagnetic at low temperatures, and is a semiconductor with possible applications in spintronics. A mixed EuII/EuIII oxide Eu3O4 can be produced by reducing Eu2O3 in a stream of hydrogen. Neodymium and samarium also form monoxides, but these are shiny conducting solids, although the existence of samarium monoxide is considered dubious.
All of the lanthanides form hydroxides, Ln(OH)3. With the exception of lutetium hydroxide, which has a cubic structure, they have the hexagonal UCl3 structure. The hydroxides can be precipitated from solutions of LnIII. They can also be formed by the reaction of the sesquioxide, Ln2O3, with water, but although this reaction is thermodynamically favourable it is kinetically slow for the heavier members of the series. Fajan's rules indicate that the smaller Ln3+ ions will be more polarizing and their salts correspondingly less ionic. The hydroxides of the heavier lanthanides become less basic, for example Yb(OH)3 and Lu(OH)3 are still basic hydroxides but will dissolve in hot concentrated NaOH.
Chalcogenides (S, Se, Te).
All of the lanthanides form Ln2Q3 (Q= S, Se, Te). The sesquisulfides can be produced by reaction of the elements or (with the exception of Eu2S3) sulfidizing the oxide (Ln2O3) with H2S. The sesquisulfides, Ln2S3 generally lose sulfur when heated and can form a range of compositions between Ln2S3 and Ln3S4. The sesquisulfides are insulators but some of the Ln3S4 are metallic conductors (e.g. Ce3S4) formulated (Ln3+)3 (S2−)4 (e−), while others (e.g. Eu3S4 and Sm3S4) are semiconductors. Structurally the sesquisulfides adopt structures that vary according the size of the Ln metal. The lighter and larger lanthanides favouring 7 coordinate metal atoms, the heaviest and smallest lanthanides (Yb and Lu) favouring 6 coordination and the rest structures with a mixture of 6 and 7 coordination. Polymorphism is common amongst the sesquisulfides. The colors of the sesquisulfides vary metal to metal and depend on the polymorphic form. The colors of the γ-sesquisulfides are La2S3, white/yellow; Ce2S3, dark red; Pr2S3, green; Nd2S3, light green; Gd2S3, sand; Tb2S3, light yellow and Dy2S3, orange. The shade of γ-Ce2S3 can be varied by doping with Na or Ca with hues ranging from dark red to yellow, and Ce2S3 based pigments are used commercially and are seen as low toxicity substitutes for cadmium based pigments.
All of the lanthanides form monochalcogenides, LnQ, (Q= S, Se, Te). The majority of the monochalcogenides are conducting, indicating a formulation LnIIIQ2−(e-) where the electron is in conduction bands. The exceptions are SmQ, EuQ and YbQ which are semiconductors or insulators but exhibit a pressure induced transition to a conducting state.
Compounds LnQ2 are known but these do not contain LnIV but are LnIII compounds containing polychalcogenide anions.
Oxysulfides Ln2O2S are well known, they all have the same structure with 7 coordinate Ln atoms with 3 sulfur atoms and 4 oxygen as near neighbours.
Doping these with other lanthanide elements produces phosphors. As an example, gadolinium oxysulfide, Gd2O2S doped with Tb3+ produces visible photons when irradiated with high energy X-rays and is used as a scintillator in flat panel detectors.
When mischmetal, an alloy of lanthanide metals, is added to molten steel to remove oxygen and sulfur, stable oxysulfides are produced that form an immiscible solid.
Pnictides (group 15).
All of the lanthanides form a mononitride, LnN, with the rock salt structure. The mononitrides have attracted interest because of their unusual physical properties. SmN and EuN are reported as being "half metals". NdN, GdN, TbN and DyN are ferromagnetic, SmN is antiferromagnetic. Applications in the field of spintronics are being investigated.
CeN is unusual as it is a metallic conductor, contrasting with the other nitrides also with the other cerium pnictides. A simple description is Ce4+ N3− (e–) but the interatomic distances are a better match for the trivalent state rather than for the tetravalent state. A number of different explanations have been offered.
The nitrides can be prepared by the reaction of lanthanum metals with nitrogen. Some nitride is produced along with the oxide, when lanthanum metals are ignited in air. Alternative methods of synthesis are a high temperature reaction of lanthanide metals with ammonia or the decomposition of lanthanide amides, Ln(NH2)3. Achieving pure stoichiometric compounds, and crystals with low defect density has proved difficult. The lanthanide nitrides are sensitive to air and hydrolyse producing ammonia.
The other pnictides phosphorus, arsenic, antimony and bismuth also react with the lanthanide metals to form monopnictides, LnQ. Additionally a range of other compounds can be produced with varying stoichiometries, such as LnP2, LnP5, LnP7 Ln3As, Ln5As3 and LnAs2.
Carbides.
Carbides of varying stoichiometries are known for the lanthanides. Non-stoichiometry is common. All of the lanthanides form LnC2 and Ln2C3 which both contain C2 units. The dicarbides with exception of EuC2, are metallic conductors with the calcium carbide structure and can be formulated as Ln3+C22−(e–). The C-C bond length is longer than that in CaC2, which contains the C22− anion, indicating that the antibonding orbitals of the C22− anion are involved in the conduction band. These dicarbides hydrolyse to form hydrogen and a mixture of hydrocarbons. EuC2 and to a lesser extent YbC2 hydrolyse differently producing a higher percentage of acetylene (ethyne). The sesquicarbides, Ln2C3 can be formulated as Ln4(C2)3. These compounds adopt the Pu2C3 structure which has been described as having C22− anions in bisphenoid holes formed by eight near Ln neighbours. The lengthening of the C-C bond is less marked in the sesquicarbides than in the dicarbides, with the exception of Ce2C3. 
Other carbon rich stoichiometries are known for some lanthanides. Ln3C4 (Ho-Lu) containing C, C2 and C3 units; Ln4C7 (Ho- Lu) contain C atoms and C3 units and Ln4C5 (Gd-Ho) containing C and C2 units
Metal rich carbides contain interstitial C atoms and no C2 or C3 units. These are Ln4C3 (Tb and Lu); Ln2C (Dy, Ho, Tm) and Ln3C (Sm-Lu).
Borides.
All of the lanthanides form a number of borides. The "higher" borides (LnBx where x > 12) are insulators/semiconductors whereas the lower borides are typically conducting. The lower borides have stoichiometries of LnB2, LnB4, LnB6 and LnB12. Applications in the field of spintronics are being investigated. The range of borides formed by the lanthanides can be compared to those formed by the transition metals. The boron rich borides are typical of the lanthanides (and groups 1-3) whereas for the transition metals tend to form metal rich, "lower" borides. The lanthanide borides are typically grouped together with the group 3 metals with which they share many similarities of reactivity, stoichiometry and structure. Collectively these are then termed the rare earth borides.
Many methods of producing lanthanide borides have been used, amongst them are direct reaction of the elements; the reduction of Ln2O3 with boron; reduction of boron oxide, B2O3, and Ln2O3 together with carbon; reduction of metal oxide with boron carbide, B4C. Producing high purity samples has proved to be difficult. Single crystals of the higher borides have been grown in a low melting metal (e.g. Sn, Cu, Al).
Diborides, LnB2, have been reported for Sm, Gd, Tb, Dy, Ho, Er, Tm, Yb and Lu. All have the same, AlB2, structure containing a graphitic layer of boron atoms. Low temperature ferromagnetic transitions for Tb, Dy, Ho and Er. TmB2 is ferromagnetic at 7.2 K.
Tetraborides, LnB4 have been reported for all of the lanthanides except EuB4, all have the same UB4 structure. The structure has a boron sub-lattice consists of chains of octahedral B6 clusters linked by boron atoms. The unit cell decreases in size successively from LaB4 to LuB4. The tetraborides of the lighter lanthanides melt with decomposition to LnB6. Attempts to make EuB4 have failed. The LnB4 are good conductors and typically antiferromagnetic.
Hexaborides, LnB6 have been reported for all of the lanthanides. They all have the CaB6 structure, containing B6 clusters. They are non-stoichiometric due to cation defects. The hexaborides of the lighter lanthanides (La - Sm) melt without decomposition, EuB6 decomposes to boron and metal and the heavier lanthanides decompose to LnB4 with exception of YbB6 which decomposes forming YbB12. The stability has in part been correlated to differences in volatility between the lanthanide metals. In EuB6 and YbB6 the metals have an oxidation state of +2 whereas in the rest of the lanthanide hexaborides it is +3. This rationalises the differences in conductivity, the extra electrons in the LnIII hexaborides entering conduction bands. EuB6 is a semiconductor and the rest are good conductors. LaB6 and CeB6 are thermionic emitters, used, for example, in scanning electron microscopes.
Dodecaborides, LnB12, are formed by the heavier smaller lanthanides, but not by the lighter larger metals, La - Eu. With the exception YbB12 (where Yb takes an intermediate valence and is a Kondo insulator), the dodecaborides are all metallic compounds. They all have the UB12 structure containing a 3 dimensional framework of cubooctahedral B12 clusters.
The higher boride LnB66 is known for all lanthanide metals. The composition is approximate as the compounds are non-stoichiometric. They all have similar complex structure with over 1600 atoms in the unit cell. The boron cubic sub lattice contains super icosahedra made up of a central B12 icosahedra surrounded by 12 others, B12(B12)12. Other complex higher borides LnB50 (Tb, Dy, Ho Er Tm Lu) and LnB25 are known (Gd, Tb, Dy, Ho, Er) and these contain boron icosahedra in the boron framework.
Organometallic compounds.
Lanthanide-carbon σ bonds are well known; however as the 4f electrons have a low probability of existing at the outer region of the atom there is little effective orbital overlap, resulting in bonds with significant ionic character. As such organo-lanthanide compounds exhibit carbanion-like behaviour, unlike in transition metal organometallic compounds. Because of their large size, lanthanides tend to form more stable organometallic derivatives with bulky ligands to give compounds such as Ln[CH(SiMe3)3]. Similarly complexes of cyclopentadienyl anion (Cp−), e.g. [Ln(C5H5)3], are far less common than the corresponding pentamethylcyclopentadienyl, e.g. [Ln(C5Me5)3Cl]. Analogues of uranocene are derived from dilithiocyclooctatetraene, Li2C8H8. Organic lanthanide(II) compounds are also known, such as Cp*2Eu.
Physical properties.
Magnetic and spectroscopic.
All the trivalent lanthanide ions, except lanthanum and lutetium, have unpaired f electrons. However, the magnetic moments deviate considerably from the spin-only values because of strong spin-orbit coupling. The maximum number of unpaired electrons is 7, in Gd3+, with a magnetic moment of 7.94 B.M., but the largest magnetic moments, at 10.4–10.7 B.M., are exhibited by Dy3+ and Ho3+. However, in Gd3+ all the electrons have parallel spin and this property is important for the use of gadolinium complexes as contrast reagent in MRI scans.
Crystal field splitting is rather small for the lanthanide ions and is less important than spin-orbit coupling in regard to energy levels. Transitions of electrons between f orbitals are forbidden by the Laporte rule. Furthermore, because of the "buried" nature of the f orbitals, coupling with molecular vibrations is weak. Consequently the spectra of lanthanide ions are rather weak and the absorption bands are similarly narrow. Glass containing holmium oxide and holmium oxide solutions (usually in perchloric acid) have sharp optical absorption peaks in the spectral range 200–900 nm and can be used as a wavelength calibration standard for optical spectrophotometers, and are available commercially.
As f-f transitions are Laporte-forbidden, once an electron has been excited, decay to the ground state will be slow. This makes them suitable for use in lasers as it makes the population inversion easy to achieve. The is one that is widely used. Europium-doped yttrium vanadate was the first red phosphor to enable the development of color television screens. Lanthanide ions have notable luminescent properties due to their unique 4f orbitals. Laporte forbidden f-f transitions can be activated by excitation of a bound "antenna" ligand. This leads to sharp emission bands throughout the visible, NIR, and IR and relatively long luminescence lifetimes.
Occurrence.
The lanthanide contraction is responsible for the great geochemical divide that splits the lanthanides into light and heavy-lanthanide enriched minerals, the latter being almost inevitably associated with and dominated by yttrium. This divide is reflected in the first two "rare earths" that were discovered: yttria (1794) and ceria (1803). The geochemical divide has put more of the light lanthanides in the Earth's crust, but more of the heavy members in the Earth's mantle. The result is that although large rich ore-bodies are found that are enriched in the light lanthanides, correspondingly large ore-bodies for the heavy members are few. The principal ores are monazite and bastnäsite. Monazite sands usually contain all the lanthanide elements, but the heavier elements are lacking in bastnäsite. The lanthanides obey the Oddo-Harkins rule – odd-numbered elements are less abundant than their even-numbered neighbors.
Three of the lanthanide elements have radioactive isotopes with long half-lives (138La, 147Sm and 176Lu) that can be used to date minerals and rocks from Earth, the Moon and meteorites.
Applications.
Industrial.
Lanthanide elements and their compounds have many uses but the quantities consumed are relatively small in comparison to other elements. About 15000 ton/year of the lanthanides are consumed as catalysts and in the production of glasses. This 15000 tons corresponds to about 85% of the lanthanide production. From the perspective of value, however, applications in phosphors and magnets are more important.
The devices lanthanide elements are used in include superconductors, samarium-cobalt and neodymium-iron-boron high-flux rare-earth magnets, magnesium alloys, electronic polishers, refining catalysts and hybrid car components (primarily batteries and magnets). Lanthanide ions are used as the active ions in luminescent materials used in optoelectronics applications, most notably the laser. Erbium-doped fiber amplifiers are significant devices in optical-fiber communication systems. Phosphors with lanthanide dopants are also widely used in cathode ray tube technology such as television sets. The earliest color television CRTs had a poor-quality red; europium as a phosphor dopant made good red phosphors possible. Yttrium iron garnet (YIG) spheres can act as tunable microwave resonators. Lanthanide oxides are mixed with tungsten to improve their high temperature properties for welding, replacing thorium, which was mildly hazardous to work with. Many defense-related products also use lanthanide elements such as night vision goggles and rangefinders. The SPY-1 radar used in some Aegis equipped warships, and the hybrid propulsion system of "Arleigh Burke"-class destroyers all use rare earth magnets in critical capacities.
The price for lanthanum oxide used in fluid catalytic cracking has risen from $5 per kilogram in early 2010 to $140 per kilogram in June 2011.
Most lanthanides are widely used in lasers, and as (co-)dopants in doped-fiber optical amplifiers; for example, in Er-doped fiber amplifiers, which are used as repeaters in the terrestrial and submarine fiber-optic transmission links that carry internet traffic. These elements deflect ultraviolet and infrared radiation and are commonly used in the production of sunglass lenses. Other applications are summarized in the following table:
The complex Gd(DOTA) is used in magnetic resonance imaging.
Life science.
As mentioned in the industrial applications section above, lanthanide metals are particularly useful in technologies that take advantage of their reactivity to specific wavelengths of light. Certain life science applications take advantage of the unique fluorescence properties of lanthanide ion complexes (Ln(III) chelates or cryptates). These are well-suited for this application due to their large Stokes shifts and extremely long emission lifetimes (from microseconds to milliseconds) compared to more traditional fluorophores (e.g., fluorescein, allophycocyanin, phycoerythrin, and rhodamine). The biological fluids or serum commonly used in these research applications contain many compounds and proteins which are naturally fluorescent. Therefore the use of conventional, steady-state fluorescence measurement presents serious limitations in assay sensitivity. Long-lived fluorophores, such as lanthanides, combined with time-resolved detection (a delay between excitation and emission detection) minimizes prompt fluorescence interference.
Time-resolved fluorometry (TRF) combined with fluorescence resonance energy transfer (FRET) offers a powerful tool for drug discovery researchers: Time-Resolved Fluorescence Resonance Energy Transfer or TR-FRET. TR-FRET combines the low background aspect of TRF with the homogeneous assay format of FRET. The resulting assay provides an increase in flexibility, reliability and sensitivity in addition to higher throughput and fewer false positive/false negative results.
This method involves two fluorophores: a donor and an acceptor. Excitation of the donor fluorophore (in this case, the lanthanide ion complex) by an energy source (e.g. flash lamp or laser) produces an energy transfer to the acceptor fluorophore if they are within a given proximity to each other (known as the Förster’s radius). The acceptor fluorophore in turn emits light at its characteristic wavelength.
The two most commonly used lanthanides in life science assays are shown below along with their corresponding acceptor dye as well as their excitation and emission wavelengths and resultant Stokes shift (separation of excitation and emission wavelengths).
Biological effects.
Due to their sparse distribution in the earth's crust and low aqueous solubility, the lanthanides have a low availability in the biosphere, and are not known to naturally form part of any biological molecules. Compared to most other nondietary elements, non-radioactive lanthanides are classified as having low toxicity.

</doc>
<doc id="18309" url="http://en.wikipedia.org/wiki?curid=18309" title="Lucifer">
Lucifer

Lucifer ( ) is the King James Version rendering of the Hebrew word הֵילֵל in Isaiah . This word, transliterated "hêlêl" or "heylel", occurs only once in the Hebrew Bible and according to the KJV-influenced Strong's Concordance means "shining one, morning star". The word "Lucifer" is taken from the Latin Vulgate, which translates הֵילֵל as "lucifer", meaning "the morning star, the planet Venus", or, as an adjective, "light-bringing". The Septuagint renders הֵילֵל in Greek as ἑωσφόρος ("heōsphoros"), a name, literally "bringer of dawn", for the morning star.
Later Christian tradition came to use the Latin word for "morning star", "lucifer", as a proper name ("Lucifer") for the Devil; as he was before his fall. As a result, "'Lucifer' has become a by-word for Satan/the Devil in the Church and in popular literature", as in Dante Alighieri's "Inferno" and John Milton's "Paradise Lost". However, the Latin word never came to be used almost exclusively, as in English, in this way, and was applied to others also, including Christ. The image of a morning star fallen from the sky is generally believed among scholars to have a parallel in Canaanite mythology.
However, according to both Christian and Jewish exegesis, in the Book of Isaiah, chapter 14, the King of Babylon, Nebuchadnezzar II, conqueror of Jerusalem, is condemned in a prophetic vision by the prophet Isaiah and is called the "Morning Ha" (planet Venus). In this chapter the Hebrew text says הֵילֵל בֶּן-שָׁחַר ("Helel ben Shaḥar", "shining one, son of the morning"). "Helel ben Shaḥar" may refer to the Morning Star, but the text in Isaiah 14 gives no indication that Helel was a star or planet.
Etymology, Lucifer or morning star.
Translation of הֵילֵל as "Lucifer", as in the King James Version, has been abandoned in modern English translations of Isaiah 14:12. Present-day translations have "morning star" (New International Version, New Century Version, New American Standard Bible, Good News Translation, Holman Christian Standard Bible, Contemporary English Version, Common English Bible, Complete Jewish Bible), "daystar" (New Jerusalem Bible, English Standard Version, The Message, "Day Star" New Revised Standard Version), "shining one" (New Life Version, New World Translation, JPS Tanakh) or "shining star" (New Living Translation).
The term appears in the context of an oracle against a dead king of Babylon, who is addressed as הילל בן שחר ("hêlêl ben šāḥar"), rendered by the King James Version as "O Lucifer, son of the morning!" and by others as "morning star, son of the dawn".
In a modern translation from the original Hebrew, the passage in which the phrase "Lucifer" or "morning star" occurs begins with the statement: "On the day the Lord gives you relief from your suffering and turmoil and from the harsh labour forced on you, you will take up this taunt against the king of Babylon: How the oppressor has come to an end! How his fury has ended!" After describing the death of the king, the taunt continues:
J. Carl Laney has pointed out that in the final verses here quoted, the king of Babylon is described not as a god or an angel but as a man.
For the unnamed "king of Babylon" a wide range of identifications have been proposed. They include a Babylonian ruler of the prophet Isaiah's own time the later Nebuchadnezzar II, under whom the Babylonian captivity of the Jews began, or Nabonidus, and the Assyrian kings Tiglath-Pileser, Sargon II and Sennacherib. Herbert Wolf held that the "king of Babylon" was not a specific ruler but a generic representation of the whole line of rulers.
Isaiah 14:12.
Mythology behind Isaiah 14:12.
In ancient Canaanite mythology, the morning star is pictured as a god, Attar, who attempted to occupy the throne of Ba'al and, finding he was unable to do so, descended and ruled the underworld. The original myth may have been about a lesser god Helel trying to dethrone the Canaanite high god El who lived on a mountain to the north. Hermann Gunkel's reconstruction of the myth told of a mighty warrior called Hêlal, whose ambition it was to ascend higher than all the other stellar divinities, but who had to descend to the depths; it thus portrayed as a battle the process by which the bright morning star fails to reach the highest point in the sky before being faded out by the rising sun.
Similarities have been noted with the East Semitic story of Ishtar's or Inanna's descent into the underworld, Ishtar and Inanna being associated with the planet Venus. A connection has been seen also with the Babylonian myth of Etana. The "Jewish Encyclopedia" comments:
The Greek myth of Phaethon, whose name, like that of הֵילֵל, means "Shining One", has also been seen as similar.
The Eerdmans Commentary on the Bible points out that no evidence has been found of any Canaanite myth of a god being thrown from heaven, as in Isaiah 14:12. It concludes that the closest parallels with Isaiah's description of the king of Babylon as a fallen morning star cast down from heaven are to be found not in any lost Canaanite and other myths but in traditional ideas of the Jewish people themselves, echoed in the Biblical account of the fall of Adam and Eve, cast out of God's presence for wishing to be as God, and the picture in of the "gods" and "sons of the Most High" destined to die and fall. This Jewish tradition has echoes also in Jewish pseudepigrapha such as 2 Enoch and the "Life of Adam and Eve".
Latin word "lucifer".
As an adjective, the Latin word "lucifer" meant "light-bringing" and was applied to the moon. As a noun, it meant "morning star", or, in Roman mythology, its divine personification as "the fabled son of Aurora and Cephalus, and father of Ceyx", or (in poetry) "day".
The second of the meanings attached to the word when used as a noun corresponds to the image in Greek mythology of "Eos", the goddess of dawn, giving birth to the morning star Phosphorus.
 is not the only place where the Vulgate uses the word "lucifer". It uses the same word four more times, in contexts where it clearly has no reference to a fallen angel: (meaning "morning star"), ("the light of the morning"), ("the signs of the zodiac") and ("the dawn"). To speak of the morning star, "lucifer" is not the only expression that the Vulgate uses: three times it uses "stella matutina": (referring to the actual morning star), and (of uncertain reference) and (referring to Jesus).
Indications that in Christian tradition the Latin word "Lucifer", unlike the English word, did not necessarily call a fallen angel to mind exist also outside the text of the Vulgate. Two bishops bore that name: Saint Lucifer of Cagliari, and Lucifer of Siena.
In Latin, the word is applied to John the Baptist and is used as a title of Christ himself in several early Christian hymns. The morning hymn "Lucis largitor splendide" of Hilary contains the line: "Tu verus mundi lucifer" (you are the true light bringer of the world). Some interpreted the mention of the morning star ("lucifer") in Ambrose's hymn "Aeterne rerum conditor" as referring allegorically to Christ and the mention of the cock, the herald of the day ("praeco") in the same hymn as referring to John the Baptist. Likewise, in the medieval hymn "Christe qui lux es et dies", some manuscripts have the line "Lucifer lucem proferens".
The Latin word "lucifer" is also used of Christ in the Easter Proclamation prayer to God regarding the paschal candle: "Flammas eius lucifer matutinus inveniat: ille, inquam, lucifer, qui nescit occasum. Christus Filius tuus, qui, regressus ab inferis, humano generi serenus illuxit, et vivit et regnat in saecula saeculorum" (May this flame be found still burning by the Morning Star: the one Morning Star who never sets, Christ your Son, who, coming back from death's domain, has shed his peaceful light on humanity, and lives and reigns for ever and ever). In the works of Latin grammarians, Lucifer, like Daniel, was discussed as an example of a personal name.
Literal meaning.
The Hebrew words הֵילֵל בֶּן-שָׁחַר ("Helel ben Shaḥar", "day-star, son of the morning") in Isaiah 14:12 are part of a prophetic vision against an oppressive king of Babylon. Jewish exegesis of Isaiah 14:12–15 took a humanistic approach by identifying the king of Babylon as Nebuchadnezzar II. Verse 20 says that this king of Babylon will not be "joined with them [all the kings of the nations] in burial, because thou hast destroyed thy land, thou hast slain thy people; the seed of evil-doers shall not be named for ever", but rather be cast out of the grave, while "All the kings of the nations, all of them, sleep in glory, every one in his own house".
Intertestamental Period.
In the Second temple period literature the main possible reference is found in 2 Enoch, also known as Slavonic Enoch:
2 Enoch 29:3 Here Satanail was hurled from the height together with his angels
However the editor of the standard modern edition (Charlesworth "OTP" Vol.1) pipelines the verse as a probable later Christian interpolation on the grounds that "Christian explanations of the origin of evil linked Lk 10:18 with Isa 14 and eventually Gen.3 so vs 4 could be a Christian interpolation... Jewish theology concentrated on Gen 6., and this is prominent in the Enoch cycle as in other apocalypses." Further the name used in 2 Enoch, Satanail, is not directly related to the Isaiah 14 text and the surrounding imagery of fire and stones suggests Ezekiel 28.
Other instances of "Lucifer" in the Old Testament Pseudepigrapha are related simply to the "star" Venus, in the Sibylline Oracles battle of the constellations (line 517) "Lucifer fought mounted on the back of Leo", or the entirely rewritten Christian version of the Greek Apocalypse of Ezra 4:32 which has a reference to Lucifer as Antichrist.
An association of Isaiah 14:12-18 with a personification of the evil, called the Devil developed outside of mainstream (rabbinic) Judaism in Pseudepigrapha and Christian writings. Old Testament Pseudepigrapha are works produced after the closing of the Hebrew Bible canon, they flourished toward the end of the Second Temple period under Roman occupation, particularly with the "apocalypses". Old Testament Pseudepigrapha are not accepted as part of Jewish tradition, but are in custodianship of the church. This period before the closing of the Christian canon is also called the Intertestamental Period when the deuterocanonical books were written.
Especially Isaiah 14:12, became a dominant conception of a fallen angel motif in . Rabbis, in Medieval Judaism, made every attempt to protect the Jewish community from their currency, strictly rejecting these Enochic phantasms. Rabbinical Judaism rejected any belief in rebel or fallen angels, having a view that evil is abstract. In the 11th century, the "Pirqe de-Rabbi Eliezer", an aggadic-midrashic work on the Torah containing exegesis and retellings of biblical stories, illustrates the origin of the "fallen angel myth" by giving two accounts, one relates to the angel in the garden in Eden, who seduces Eve, and the other relates to the angels, the "benei elohim", who cohabit with the daughters of man (Genesis 6:1-4).
Allegorical interpretation in Christianity.
Apart from the literal meaning of Isaiah 14:12, which applies to a king of Babylon, Christian writers applied the words allegorically to Satan. Sigve K Tonstad argues that in the New Testament itself the War in Heaven theme of , in which the dragon "who is called the devil and Satan … was thrown down to the earth", derives from the passage in Isaiah 14. Origen (184/185 – 253/254) interpreted such Old Testament passages as being about manifestations of the Devil; but of course, writing in Greek, not Latin, he did not identify the Devil with the name "Lucifer". Tertullian (c. 160 – c. 225), who wrote in Latin, also understood ("I will ascend above the tops of the clouds; I will make myself like the Most High") as spoken by the Devil, but "Lucifer" is not among the numerous names and phrases he used to describe the Devil. Even at the time of the Latin writer Augustine of Hippo (354 – 430), "Lucifer" had not yet become a common name for the Devil.
Some time later, the metaphor of the morning star that Isaiah 14:12 applied to a king of Babylon gave rise to the general use of the Latin word for "morning star", capitalized, as the original name of the Devil before his fall from grace, linking Isaiah 14:12 with ("I saw Satan fall like lightning from heaven") and interpreting the passage in Isaiah as an allegory of Satan's fall from heaven.
However, the understanding of the morning star in Isaiah 14:12 as a metaphor referring to a king of Babylon continued also to exist among Christians. Theodoret of Cyrus (c. 393 – c. 457) wrote that Isaiah calls the king "morning star", not as being the star, but as having had the illusion of being it. The same understanding is shown in Christian translations of the passage, which in English generally use "morning star" rather than treating the word as a proper name, "Lucifer". So too in other languages, such as French, German, Portuguese, and Spanish. Even the Vulgate text in Latin is printed with lower-case "lucifer" (morning star), not upper-case "Lucifer" (proper name).
Calvin said: "The exposition of this passage, which some have given, as if it referred to Satan, has arisen from ignorance: for the context plainly shows these statements must be understood in reference to the king of the Babylonians." Luther also considered it a gross error to refer this verse to the devil.
Christians who identify Lucifer with Satan or the Devil.
Adherents of the King James Only movement and others who hold that Isaiah 14:12 does indeed refer to the devil have decried the modern translations.
Treating "Lucifer" as a name for the devil or Satan, they may use that name when speaking of such accounts of the devil or Satan as the following:
- Satan inciting David to number Israel (1 Chronicles )
- Job tested by Satan (Book of Job)
- Satan ready to accuse the high priest Joshua (Zechariah )
- Sin brought into the world through the devil's envy (Wisdom )
- "The prince of the power of the air, the spirit that is now at work in the sons of disobedience" (Ephesians )
- "The god of this world" (2 Corinthians ).
- The devil disputing with Michael about the body of Moses (Jude )
- The dragon of the Book of Revelation "who is called the devil and Satan" ()
They may also use the name Lucifer when speaking of Satan's motive for rebelling and of the nature of his sin, which Origen, Chrysostom, Jerome, Ambrose, and Augustine attributed to the devil's pride, and Irenaeus, Tertullian, Justin Martyr, Cyprian, and again Augustine attributed to the devil's envy of humanity created in the image of God. Jealousy of humans, created in the divine image and given authority over the world is the motive that a modern writer, who denies that there is any such person as Lucifer, says that Tertullian attributed to the Devil, and, while he cited Tertullian and Augustine as giving envy as the motive for the fall, an 18th-century French Capuchin preacher himself described the Rebel Angel as jealous of Adam's exaltation, which he saw as a diminution of his own status.
Islam.
In Islam the Devil is known as Iblīs (Arabic: إبليس‎, plural: ابالسة "abālisah") or Shayṭān (Arabic: شيطان‎, plural: شياطين "shayāṭīn"). He has no name corresponding in meaning to that of the Latin word "lucifer" to associate him with the Morning Star, but the accounts of him resemble the fallen-angel accounts in Enochic and Christian literature. Iblis is banished from heaven for refusing to prostrate himself before Adam. Thus, he sins "after" the creation of man. He asks God for a respite until the Last Day rather than being consigned to the Fire of Hell immediately. God grants this request, and Iblis then swears revenge by tempting human beings and turning them away from God. God tells him that any humans who follow him will join him in the Fire of Hell at Judgement, but that Iblis will have no power over all mankind except who wants to follow Iblis. This story is cited multiple times in the Qur'an for different reasons.
Islamic literature presents Iblis as God worshipping and very pious until he refused to prostrate to Adam due to his jealousy and pride. Iblis was from a race known as the Jinn, a race that was made out of fire and created before humankind. 
Occultism.
Luciferianism is a belief system that venerates the essential characteristics that are affixed to Lucifer. The tradition, influenced by Gnosticism, usually reveres Lucifer not as the Devil, but as a liberator or guiding spirit or even the true god as opposed to Jehovah.
In Anton LaVey's "The Satanic Bible", Lucifer is one of the Four Crown Princes of Hell, particularly that of the East. Lord of the Air, and is called "Bringer of Light, the Morning Star, Intellectualism, Enlightenment."
Author Michael W. Ford has written on Lucifer as a "mask" of the Adversary, a motivator and illuminating force of the mind and subconscious.
Taxil's hoax.
Léo Taxil (1854–1907) claimed that Freemasonry is associated with worshipping Lucifer. In what is known as the Taxil hoax, he alleged that leading Freemason Albert Pike had addressed "The 23 Supreme Confederated Councils of the world" (an invention of Taxil), instructing them that Lucifer was God, and was in opposition to the evil god Adonai. Supporters of Freemasonry contend that, when Albert Pike and other Masonic scholars spoke about the "Luciferian path," or the "energies of Lucifer," they were referring to the Morning Star, the light bearer, the search for light; the very antithesis of dark, satanic evil. Taxil promoted a book by Diana Vaughan (actually written by himself, as he later confessed publicly) that purported to reveal a highly secret ruling body called the Palladium, which controlled the organization and had a satanic agenda. As described by "Freemasonry Disclosed" in 1897:
With frightening cynicism, the miserable person we shall not name here [Taxil] declared before an assembly especially convened for him that for twelve years he had prepared and carried out to the end the most sacrilegious of hoaxes. We have always been careful to publish special articles concerning Palladism and Diana Vaughan. We are now giving in this issue a complete list of these articles, which can now be considered as not having existed.
Taxil's work and Pike's address continue to be quoted by anti-masonic groups.
In "Devil-Worship in France", Arthur Edward Waite compared Taxil's work to what today we would call a tabloid story, replete with logical and factual inconsistencies.

</doc>
