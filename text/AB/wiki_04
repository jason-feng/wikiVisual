<doc id="13951" url="http://en.wikipedia.org/wiki?curid=13951" title="Hitachi 6309">
Hitachi 6309

The 6309 is Hitachi's CMOS version of the Motorola 6809 microprocessor. While in "Emulation Mode" it is fully compatible with the 6809. To the 6809 specifications it adds higher clock rates, enhanced features, new instructions, and additional registers. Most new instructions were added to support the additional registers, as well as up to 32-bit math, hardware division, bit manipulations, and block transfers. The 6309 is generally 30% faster in native mode than the 6809.
Surprisingly, this information was never published by Hitachi. The April 1988 issue of "Oh! FM", a Japanese magazine for Fujitsu personal computer users, contained the first description of the 6309's additional capabilities. Later, Hirotsugu Kakugawa posted details of the 6309's new features and instructions to comp.sys.m6809. This led to the development of for the Tandy Color Computer 3.
Differences from the Motorola 6809.
The 6309 differs from the 6809 in several key areas.
Process Technology.
The 6309 is fabricated in CMOS technology, while the 6809 is an NMOS device. As a result, the 6309 requires less power to operate than the 6809. It is also a fully static device, which will not lose internal state information. This means it can be used with external DMA without needing refresh every 14 cycles as the 6809 does.
Clock Speed.
The 6309 has B (2 MHz) versions as the 6809 does. However, a "C" speed rating was produced with either a 3.0 or 3.5 MHz maximum clock rate, depending on which datasheet is referenced. (Several Japanese computers had 63C09 CPUs clocked at 3.58 MHz, the NTSC colorburst frequency, so the 3.5 rating seems most likely). Anecdotal and individual reports indicate that the 63C09 variant can be clocked at 5 MHz with no ill effects. Like the 6809, the Hitachi CPU comes in both internal and external clock versions (HD63B/C09 and HD63B/C09E respectively)
Computational Efficiency.
When switched into 6309 Native Mode (as opposed to the default 6809-compatible mode) many key instructions will complete in fewer clock cycles. This often improves execution speeds by up to 30%.
Additional Instructions.
Most of the new instructions are modifications of existing instructions to handle the existence of the additional registers, such as load, store, add, and the like. Genuine 6309 additions include inter-register arithmetic, block transfers, hardware division, and bit-level manipulations.
Despite the user-friendliness of the additional instructions, analysis by 6809 programming gurus indicates that many of the new instructions are actually slower than the equivalent 6809 code, especially in tight loops. Careful analysis should be done to ensure that the programmer uses the most efficient code for the particular application.
Additional Hardware Features.
It is possible to change the mode of operation for the FIRQ interrupt. Instead of stacking the PC and CC registers (normal 6809 behavior) the FIRQ interrupt can be set to stack the entire register set, as the IRQ interrupt does. In addition, the 6309 has two possible trap modes, one for an illegal instruction fetch and one for division by zero. The illegal instruction fetch is not maskable, and many TRS-80 Color Computer users reported that their 6309's were "buggy" when in reality it was an indicator of enhanced and unknown features.
External links.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="13953" url="http://en.wikipedia.org/wiki?curid=13953" title="Hominy">
Hominy

Hominy is a food which consists of dried maize kernels which have been treated with an alkali in a process called nixtamalization.
Production.
To make hominy, field corn (maize) grain is dried, then treated by soaking and cooking the mature (hard) grain in a dilute solution of lye, slaked lime (calcium hydroxide) or wood ash, a process termed nixtamalization. The soaked maize is washed, and then ground into masa. When fresh masa is dried and powdered, it becomes "masa seca" or "masa harina".
Lye, lime and ash are highly alkaline: the alkalinity helps the dissolution of hemicellulose, the major glue-like component of the maize cell walls, and loosens the hulls from the kernels and softens the corn. Some of the corn oil is broken down into emulsifying agents (monoglycerides and diglycerides), while bonding of the corn proteins to each other is also facilitated. The divalent calcium in lime acts as a cross-linking agent for protein and polysaccharide acidic side chains. As a result, while cornmeal made from untreated ground corn is unable by itself to form a dough with the addition of water, the chemical changes in masa allow dough formation, which is essential to the ability to fashion dough into tortillas. Also, soaking the corn in lime kills the seed's germ, which keeps it from sprouting while in storage. Finally, in addition to providing a source of dietary calcium, the lime reacts with the corn so that the nutrient niacin can be assimilated by the digestive tract. While consumption of untreated corn is a risk factor in predisposition to pellagra, the risk is dramatically reduced or eliminated by nixtamalization.
The process of making hominy is also called nixtamalization and the ground product can be called masa nixtamalera.
The earliest known usage of nixtamalization was in what is present-day southern Mexico and Guatemala around 1500–1200 BC.
Recipes.
In Central American and Mexican cuisine, masa nixtamalera is cooked with water and milk to make a thick, gruel-like beverage called "atole". When made with chocolate and sugar, it becomes "atole de chocolate". Adding anise and piloncillo to this mix creates "champurrado", a popular breakfast drink.
The English term "hominy" is derived from the Powhatan language word for prepared maize. Many other Native American cultures also made hominy and integrated it into their diet. Cherokees, for example, made hominy grits by soaking corn in a weak lye solution obtained by leaching hardwood ash with water and beating it with a "kanona" (ᎧᏃᎾ), or corn beater. The grits were used to make a traditional hominy soup ("gvnohenv amagii" ᎬᏃᎮᏅ ᎠᎹᎩᎢ), a hominy soup that was allowed to ferment ("gvwi sida amagii" ᎬᏫ ᏏᏓ ᎠᎹᎩᎢ), cornbread, dumplings ("digunvi" ᏗᎫᏅᎢ), or, in post-contact times, fried with bacon and green onions.
Some recipes using hominy include "pozole" (a Mexican stew of hominy and pork, chicken, or other meat), hominy bread, hominy chili, hog 'n' hominy, casseroles and fried dishes. Hominy can be ground coarsely to make hominy grits, or into a fine mash (dough) to make masa, a dough used regularly in Latin American cuisine. Many islands in the West Indies, most notably Jamaica, also use hominy (known as cornmeal or polenta) to make a sort of porridge with corn starch or flour to thicken the mixture and condensed milk, vanilla and nutmeg to taste. In Italian cuisine a similar usually savoury dish is eaten, known as polenta (cornmeal) often accompanying meat.
Rockihominy, a popular trail food in the 19th and early 20th centuries, is dried corn roasted to a golden brown, then ground to a very coarse meal, almost like hominy grits. Hominy is also used as animal feed.

</doc>
<doc id="13955" url="http://en.wikipedia.org/wiki?curid=13955" title="W. Heath Robinson">
W. Heath Robinson

William Heath Robinson (31 May 1872 – 13 September 1944) was an English cartoonist and illustrator best known for drawings of ridiculously complicated machines for achieving simple objectives.
In the U.K., the term "Heath Robinson" entered the language during the 1914–1918 First World War as a description of any unnecessarily complex and implausible contrivance, much as "Rube Goldberg machines" came to be used in the U.S. from the 1930s onwards as a term for similar efforts. "Heath Robinson contraption" is perhaps more often used in relation to temporary fixes using ingenuity and whatever is to hand, often string and tape, or unlikely cannibalisations. Its continuing popularity was undoubtedly linked to Second World War Britain's shortages and the need to "make do and mend".
Career.
William Heath Robinson was born at 25 Ennis Road on 31 May 1872 into a family of artists in an area of London known as Stroud Green, Finsbury Park, north London. His father Thomas Robinson (1838-1902) and brothers Thomas Heath Robinson (1869-1954) and Charles Robinson (1870–1937) all worked as illustrators.
His early career involved illustrating books – among others: Hans Christian Andersen's "Danish Fairy Tales and Legends" (1897), "The Arabian Nights" (1899), "Tales from Shakespeare" (1902), "Gargantua and Pantagruel" (1904), "Twelfth Night" (1908), "Andersen's Fairy Tales" (1913), "A Midsummer Night's Dream" (1914), Charles Kingsley's "The Water-Babies" (1915) and Walter de la Mare's "Peacock Pie" (1916).
In the course of his work Heath Robinson also wrote and illustrated three children's books, "The Adventures of Uncle Lubin" (1902), "Bill the Minder" (1912) and "Peter Quip in Search of a Friend" (1922). "Uncle Lubin" is regarded as the start of his career in the depiction of unlikely machines.
During the First World War, he drew large numbers of cartoons, depicting ever-more-unlikely secret weapons being used by the combatants. He also depicted the American Expeditionary Force in France.
He also produced a steady stream of humorous drawings for magazines and advertisements. In 1934 he published a collection of his favourites as "Absurdities", such as:
Most of his cartoons have since been reprinted many times in multiple collections.
The machines he drew were frequently powered by steam boilers or kettles, heated by candles or a spirit lamp and usually kept running by balding, bespectacled men in overalls. There would be complex pulley arrangements, threaded by lengths of knotted string. Robinson's cartoons were so popular that in Britain the term "Heath Robinson" is used to refer to an improbable, rickety machine barely kept going by incessant tinkering. (The corresponding term in the U.S. is "Rube Goldberg", after an American cartoonist with an equal devotion to odd machinery. Similar "inventions" have been drawn by cartoonists in many countries, with the Danish Storm Petersen being on par with Robinson and Goldberg.)
One of his most famous series of illustrations was that which accompanied the first Professor Branestawm book written by Norman Hunter. The stories told of the eponymous professor who was brilliant, eccentric and forgetful and provided a perfect backdrop for Robinson's drawings.
One of the automatic analysis machines built for Bletchley Park during the Second World War to assist in the decryption of German message traffic was named "Heath Robinson" in his honour. It was a direct predecessor to the Colossus, the world's first programmable digital electronic computer.
In 1903 he married Josephine Latey, the daughter of newspaper editor John Latey. Heath Robinson moved to Pinner, Middlesex, in 1908. His house in Moss Lane is commemorated by a blue plaque. West House, in Memorial Park, Pinner, has been restored to house a Heath Robinson Collection, with an extension to the house planned.
He died in September 1944 during the Second World War and is buried in East Finchley Cemetery.
In popular culture.
The name "Heath Robinson" became part of common parlance in the UK for complex inventions that achieved absurdly simple results following its use as services slang during the 1914–1918 First World War.
During the Falklands War (1982), British Harrier aircraft lacked their conventional "chaff"-dispensing mechanism.
Therefore Royal Navy engineers designed an impromptu delivery system of welding rods, split pins and string which allowed six packets of chaff to be stored in the airbrake well and deployed in flight. Due to its complexity it was often referred to as the "Heath Robinson chaff modification".

</doc>
<doc id="13956" url="http://en.wikipedia.org/wiki?curid=13956" title="Heraclius">
Heraclius

Heraclius (Latin: "Flavius Heraclius Augustus", Greek: Φλάβιος Ἡράκλειος, Armenian: Հերակլես Փլավիոս, c. 575 – February 11, 641) was Byzantine Emperor from 610 to 641.
He was responsible for introducing Greek as the Eastern Empire's official language. His rise to power began in 608, when he and his father, Heraclius the Elder, the exarch of Africa, successfully led a revolt against the unpopular usurper Phocas.
Heraclius's reign was marked by several military campaigns. The year Heraclius came to power, the empire was threatened on multiple frontiers. Heraclius immediately took charge of the ongoing war against the Sassanids. The first battles of the campaign ended in defeat for the Byzantines; the Persian army fought their way to the Bosphorus; however, because Constantinople was protected by impenetrable walls and a strong navy, Heraclius was able to avoid total defeat. Soon after, he initiated reforms to rebuild and strengthen the military. Heraclius drove the Persians out of Asia Minor and pushed deep into their territory, defeating them decisively in 627 at the Battle of Nineveh. The Persian king Khosrau II was overthrown and executed soon by his son Kavadh II who soon sued for a peace treaty agreeing to withdraw from all occupied territory. This way peaceful relations were restored to the two deeply strained empires.
However, soon after his victory he faced a new threat, the Muslim invasions. Emerging from the Arabian Peninsula, the Muslims quickly conquered the Sassanid empire. In 634 the Muslims invaded Roman Syria, defeating Heraclius' brother Theodore. Within a short period of time the Arabs would also conquer Mesopotamia, Armenia, and Egypt.
In religious matters, Heraclius is remembered as the driving force in converting the peoples migrating to the Balkan Peninsula. At his request, Pope John IV (640–642) sent Christian teachers and missionaries to Dalmatia, a newly Croatian-ruled province settled by Porga and his clan, who practiced Slavic paganism. He tried to repair the schism in the Christian church in regard to the Monophysites by promoting a compromise doctrine called Monothelitism. The Church of the East (commonly called Nestorian) was also involved in the process. Eventually, however, this project of unity was rejected by all sides of the dispute. Heraclius was the first Emperor to engage the Muslims; in the Islamic tradition he is portrayed as an ideal ruler who corresponded with Muhammad, possibly was a true believer of Islam, and viewed Muhammad as the true prophet, the messenger of God. However, other sources suggest that Heraclius may have never read Muhammad's letter or received any messenger sent by Muhammad.
Early life.
Origins.
Heraclius was the eldest son of Heraclius the Elder and Epiphania, of an Armenian family from Cappadocia, probably of Arsacid descent. Beyond that, there is little specific information known about his ancestry. His father was a key general during Emperor Maurice's war with Bahrām Chobin, usurper of the Sassanid Empire, during 590. After the war, Maurice appointed Heraclius the Elder to the position of Exarch of Africa.
Revolt against Phocas and accession.
In 608, Heraclius the Elder renounced his loyalty to the Emperor Phocas, who had overthrown Maurice six years earlier. The rebels issued coins showing both Heraclii dressed as consuls, though neither of them explicitly claimed the imperial title at this time. Heraclius' younger cousin Nicetas launched an overland invasion of Egypt; by 609, he had defeated Phocas' general Bonosus and secured the province. Meanwhile, the younger Heraclius sailed eastward with another force via Sicily and Cyprus.
As he approached Constantinople, he made contact with prominent leaders and planned an attack to overthrow aristocrats in the city, and soon arranged a ceremony where he was crowned and acclaimed as Emperor. When he reached the capital, the Excubitors, an elite Imperial Guard unit led by Phocas' son-in-law Priscus, deserted to Heraclius, and he entered the city without serious resistance. When Heraclius captured Phocas, he asked him, "Is this how you have ruled, wretch?" Phocas said in reply, "And will you rule better?" With that, Heraclius became so enraged that he beheaded Phocas on the spot. He later had the genitalia removed from the body because Phocas had raped the wife of Photius, a powerful politician in the city.
On October 5, 610, Heraclius was crowned for a second time, this time in the Chapel of St. Stephen within the Great Palace; at the same time he married Fabia, who took the name Eudokia. After her death in 612, he married his niece Martina in 613; this second marriage was considered incestuous and was very unpopular. In the reign of Heraclius' two sons, the divisive Martina was to become the center of power and political intrigue. Despite widespread hatred for Martina in Constantinople, Heraclius took her on campaigns with him and refused attempts by Patriarch Sergius to prevent and later dissolve the marriage.
Byzantine-Sassanid War of 602–628.
Initial Persian advantage.
During his Balkan Campaigns, Emperor Maurice and his family were murdered by Phocas in November 602 after a mutiny. Khosrau II (Chosroes) of the Sassanid Empire had been restored to his throne by Maurice, and they had remained allies. Thus, the Persian King Khosrau II seized the pretext to attack the Byzantine Empire and reconquer the Byzantine province of Mesopotamia. Khosrau had at his court a man who claimed to be Maurice's son Theodosius, and Khosrau demanded that the Byzantines accept this Theodosius as Emperor.
The war initially went the Persians' way, partly because of Phocas' brutal repression and the succession crisis that ensued as the general Heraclius sent his nephew Nicetas to attack Egypt, enabling his son Heraclius the younger to claim the throne in 610. Phocas, an unpopular ruler who is invariably described in historical sources as a "tyrant" (in its original meaning of the word, i.e. illegitimate king by the rules of succession), was eventually deposed by Heraclius, who sailed to Constantinople from Carthage with an icon affixed to the prow of his ship.
By this time, the Persians had conquered Mesopotamia and the Caucasus, and in 611 they overran Syria and entered Anatolia. A major counter-attack led by Heraclius two years later was decisively defeated outside Antioch by Shahrbaraz and Shahin, and the Roman position collapsed; the Persians devastated parts of Asia Minor and captured Chalcedon across from Constantinople on the Bosporus. Over the following decade the Persians were able to conquer Palestine and Egypt (by mid-621 the whole province was in their hands) and to devastate Anatolia, while the Avars and Slavs took advantage of the situation to overrun the Balkans, bringing the Empire to the brink of destruction. In 613, the Persian army took Damascus with the help of the Jews, seized Jerusalem in 614, damaging the Church of the Holy Sepulchre and capturing the True Cross, and afterwards capturing Egypt in 617 or 618.
With the Persians at the very gate of Constantinople, Heraclius thought of abandoning the city and moving the capital to Carthage, but the powerful church figure Patriarch Sergius convinced him to stay. Safe behind the walls of Constantinople, Heraclius was able to sue for peace in exchange for an annual tribute of a thousand talents of gold, a thousand talents of silver, a thousand silk robes, a thousand horses, and a thousand virgins to the Persian King. The peace allowed him to rebuild the Empire's army by slashing non-military expenditure, devaluing the currency, and melting down, with the backing of Patriarch Sergius, Church treasures to raise the necessary funds to continue the war.
Byzantine counter-offensive and resurgence.
On April 5, 622, Heraclius left Constantinople, entrusting the city to Sergius and general Bonus as regents of his son. He assembled his forces in Asia Minor, probably in Bithynia, and, after he revived their broken morale, he launched a new counter-offensive, which took on the character of a holy war; an acheiropoietos image of Christ was carried as a military standard.
The Roman army proceeded to Armenia, inflicted a defeat on an army led by a Persian-allied Arab chief, and then won a victory over the Persians under Shahrbaraz. Heraclius would stay on campaign for several years. On March 25, 624 he again left Constantinople with his wife, Martina, and his two children; after he celebrated Easter in Nicomedia on April 15, he campaigned in the Caucasus, winning a series of victories in Armenia against Khosrau and his generals Shahrbaraz, Shahin, and Shahraplakan. However, in the same year the Visigoths succeeded in recapturing Cartagena, capital of the western Byzantine province of Spania, resulting in the loss of one of the few minor provinces that had been conquered by the armies of Justinian I. In 626 the Avars and Slavs besieged Constantinople, supported by a Persian army commanded by Shahrbaraz, but the siege ended in failure (the victory was attributed to the icons of the Virgin which were led in procession by Sergius about the walls of the city), while a second Persian army under Shahin suffered another crushing defeat at the hands of Heraclius' brother Theodore.
With the Persian war effort disintegrating, Heraclius was able to bring the Gokturks of the Western Turkic Khaganate, Ziebel, who invaded Persian Transcaucasia. Heraclius exploited divisions within the Persian Empire, keeping the Persian general Shahrbaraz neutral by convincing him that Khosrau had grown jealous of him and had ordered his execution. Late in 627 he launched a winter offensive into Mesopotamia, where, despite the desertion of his Turkish allies, he defeated the Persians under Rhahzadh at the Battle of Nineveh. Continuing south along the Tigris he sacked Khosrau's great palace at Dastagird and was only prevented from attacking Ctesiphon by the destruction of the bridges on the Nahrawan Canal. Discredited by this series of disasters, Khosrau was overthrown and killed in a coup led by his son Kavadh II, who at once sued for peace, agreeing to withdraw from all occupied territories. In 629 Heraclius restored the True Cross to Jerusalem in a majestic ceremony.
Heraclius took for himself the ancient Persian title of "King of Kings" after his victory over Persia. Later on, starting in 629, he styled himself as "Basileus", the Greek word for "sovereign", and that title was used by the Roman Emperors for the next 800 years. The reason Heraclius chose this title over previous Roman terms such as Augustus has been attributed by some scholars as relating to his Armenian origins.
Heraclius' defeat of the Persians ended a war that had been going on intermittently for almost 400 years and led to instability in the Persian Empire. Kavadh II died only months after assuming the throne, plunging Persia into several years of dynastic turmoil and civil war. Ardashir III, Heraclius' ally Shahrbaraz, and Khosrau's daughters Purandokht and Azarmidokht all succeeded to the throne within months of each other. Only when Yazdgerd III, a grandson of Khosrau II, succeeded to the throne in 632 was there stability. But by then the Sasanid Empire was severely disorganised and had been severely weakened by years of war and civil strife over the succession to the throne.
However, the Byzantine victory was ultimately a pyrrhic one, as the devastating impact of the war left the Byzantines in much weakened state. Within a few years both empires were overwhelmed by the onslaught of the Arabs who had become newly united by Islam, ultimately leading to the Muslim conquest of Persia in 644 and the fall of the Sassanid dynasty in 651.
War against the Arabs.
In 629, the Islamic Prophet Muhammad had recently succeeded in unifying all of the nomadic tribes of the Arabian Peninsula. Those tribes had previously been too divided to pose a serious military threat to the Byzantines or the Persians. Now unified and animated by their new conversion to Islam, they comprised one of the most powerful states in the region. The first conflict between the Byzantines and Muslims was the Battle of Mu'tah in September 629. A small Muslim skirmishing force attacked the province of Arabia but were repulsed. Because the engagement was a Byzantine victory, there was no apparent reason to make changes to the military configuration of the region. Also, once the severity of the Muslim threat was realized, the Byzantines had little preceding battlefield experience with the Arabs, and even less with zealous soldiers united by a prophet. Even the Strategicon, a manual of war praised for the variety of enemies it covers, does not mention warfare against Arabs at any length.
The following year the Muslims launched raids into the Arabah south of Lake Tiberias, taking Al Karak. Other raids penetrated into the Negev reaching as far as Gaza. Islamic sources record that Heraclius dreamt of the coming Arab invasion. Historian Al-Tabari wrote that Heraclius dreamt of a new kingdom of the "circumcised man" that would be victorious against all its enemies. After telling his court his dream, his patricians, who did not know of the rise of Islam in Arabia, "advised him to send orders to behead every Jew in his dominion."
Only when a Bedouin trader speaking of a man uniting the tribes of Arabia under a new religion was brought before the Emperor did Heraclius and his court realize that the kingdom of the "circumcised man" was not the Jews but the new Islamic Empire. When the Muslim Arabs attacked Syria and Palestine in 634, he was unable to oppose them personally in battle. Although he remained strategically in charge of operations, his generals failed him in battle. The Battle of Yarmouk in 636 resulted in a crushing defeat for the larger Byzantine army; within three years, the Levant had been lost again. By the time of Heraclius' death in Constantinople, on February 11, 641, most of Egypt had fallen as well.
Islamic view of the Emperor.
In Islamic and Arab histories Heraclius is the only Roman Emperor who is discussed at any length. Owing to his role as the Roman Emperor at the time Islam emerged, he was remembered in Arabic literature, such as the Islamic hadith and sira. Outside of Islamic sources there is no evidence to suggest Heraclius ever heard of Islam, and it is possible that he and his advisors actually viewed the Muslims as some special sect of Jews.
In Surah 30, the Qur'an refers to the Perso-Roman wars as follows:
30:2 "Certainly, the Romans will be defeated." 3 "In the lowest land. After their defeat, they will rise again and win." 4 "Within several years. Such is GOD's decision, both in the first prophecy, and the second. On that day, the believers shall rejoice" 5 "in GOD's victory. He grants victory to whomever He wills. He is the Almighty, Most Merciful."
The Swahili "Utendi wa Tambuka", an epic poem composed in 1728 at Pate Island (off the shore of present-day Kenya) and depicting the wars between the Muslims and Byzantines from the former's point of view, is also known as "Kyuo kya Hereḳali" ("The book of Heraclius"). In that work, Heraclius is portrayed as declining the Prophet's command to renounce his false belief in Christianity; he is therefore defeated by the Muslim forces.
In Muslim tradition he is seen as a just ruler of great piety, who had direct contact with the emerging Islamic forces. The 14th century scholar Ibn Kathir (d. 1373) went even further stating that "Heraclius was one of the wisest men and among the most resolute, shrewd, deep and opinionated of kings. He ruled the Romans with great leadership and splendor." Historians such as Nadia Maria El-Cheikh and Lawrence Conrad note that Islamic histories even go so far as claiming that Heraclius recognized the Islam as the true faith and Muhammad as its prophet, by comparing Islam to Christianity. Islamic sources even go so far as to say that when he told his people to turn to Islam and they rioted, he claimed this was just to test their faith.
Islamic historians often cite a letter that they claim Heraclius wrote to Muhammad: "I have received your letter with your ambassador and I testify that you are the messenger of God found in our New Testament. Jesus, son of Mary, announced you." According to the Muslim sources reported by El-Cheikh, he tried to convert the ruling class of the Empire, but they resisted so strongly that he reversed his course and claimed that he was just testing their faith in Christianity. El-Cheikh notes that these accounts of Heraclius add "little to our historical knowledge" of the emperor; rather, they are an important part of "Islamic kerygma," attempting to legitimate Muhammad's status as a prophet.
Legacy.
Looking back at the reign of Heraclius, scholars have credited him with many accomplishments. He enlarged the Empire, and his reorganization of the government and military were great successes. His attempts at religious harmony failed, but he succeeded in returning the True Cross, one of the holiest Christian relics, to Jerusalem.
Accomplishments.
Although the territorial gains produced by his defeat of the Persians were lost to the advance of the Muslims, Heraclius still ranks among the great Roman Emperors. His reforms of the government reduced the corruption which had taken hold in Phocas' reign, and he reorganized the military with great success. Ultimately, the reformed Imperial army halted the Muslims in Asia Minor and held on to Carthage for another 60 years, saving a core from which the empire's strength could be rebuilt.
The recovery of the eastern areas of the Roman Empire from the Persians once again raised the problem of religious unity centering around the understanding of the true nature of Christ. Most of the inhabitants of these provinces were Monophysites who rejected the Council of Chalcedon. Heraclius tried to promote a compromise doctrine called Monothelitism; however, this philosophy was rejected as heretical by both sides of the dispute. For this reason, Heraclius was viewed as a heretic and bad ruler by some later religious writers. After the Monophysite provinces were finally lost to the Muslims, Monotheletism rather lost its raison d'être and was eventually abandoned.
One of the most important legacies of Heraclius was changing the official language of the Empire from Latin to Greek in 620. The Croats and Serbs of Byzantine Dalmatia initiated diplomatic relations and dependencies with Heraclius. The Serbs, who briefly lived in Macedonia, became "foederati" and were baptized at the request of Heraclius (before 626). At his request, Pope John IV (640–642) sent Christian teachers and missionaries to Duke Porga and his Croats, who practiced Slavic paganism. He also created the office of sakellarios, a comptroller of the treasury.
Up to the 20th century he was credited with establishing the Thematic system but modern scholarship now points more to the 660s, under Constans II.
Edward Gibbon in his work "The History of the Decline and Fall of the Roman Empire" wrote:
Of the characters conspicuous in history, that of Heraclius is one of the most extraordinary and inconsistent. In the first and last years of a long reign, the emperor appears to be the slave of sloth, of pleasure, or of superstition, the careless and impotent spectator of the public calamities. But the languid mists of the morning and evening are separated by the brightness of the meridian sun; the Arcadius of the palace arose the Caesar of the camp; and the honor of Rome and Heraclius was gloriously retrieved by the exploits and trophies of six adventurous campaigns. [...] Since the days of Scipio and Hannibal, no bolder enterprise has been attempted than that which Heraclius achieved for the deliverance of the empire.
Recovery of the True Cross.
Despite his actual Orthodox theology, Heraclius was long remembered favourably in the Western church for his reputed feat in recovering the True Cross, which had been captured by the Persians. As Heraclius approached the capital, Khosrau fled from his favourite residence, Dastagird (near Baghdad), without offering resistance. Meanwhile, some of the Persian grandees freed his eldest son Kavadh II, whom Khosrau II had imprisoned, and proclaimed him King on the night of 23–24 February, 628. Kavadh however was mortally ill and was anxious that Heraclius should protect his infant son Ardeshir. So, as a goodwill gesture, he sent the True Cross with a peace negotiator to sue for peace in 628.
After a tour of the Empire he returned the cross on March 21, 629. The story was included in the "Golden Legend", the famous 13th century compendium of hagiography, and he is sometimes shown in art, as in "The History of the True Cross" sequence of frescoes painted by Piero della Francesca in Arezzo, and a similar sequence on a small altarpiece by Adam Elsheimer (Städel, Frankfurt). Both of these show scenes of Heraclius and Constantine I's mother Saint Helena, traditionally responsible for the excavation of the cross. The scene usually shown is Heraclius carrying the cross; according to the "Golden Legend" he insisted on doing this as he entered Jerusalem, against the advice of the Patriarch. At first, when he was on horseback (shown above), the burden was too heavy, but after he dismounted and removed his crown it became miraculously light, and the barred city gate opened of its own accord.
Probably because he was one of the few Eastern Roman Emperors widely known in the West, the Late Antique Colossus of Barletta was considered to depict Heraclius.
Family.
Heraclius was married twice: first to Fabia Eudokia, a daughter of Rogatus, and then to his niece Martina. He had two children with Fabia and at least nine with Martina, most of whom were sickly children. Of Martina's children at least two were disabled, which was seen as punishment for the illegality of the marriage: Fabius (Flavius) had a paralyzed neck and Theodosios, who was a deaf-mute, married Nike, daughter of Persian general Shahrbaraz or daughter of Niketas, cousin of Heraclius.
Two of Heraclius' children would become Emperor: Heraclius Constantine (Constantine III), his son from Eudokia, from 613 – 641, and Martina's son Constantine Heraclius (Heraklonas), from 638 – 641.
Heraclius had at least one illegitimate son, John Athalarichos, who conspired a plot against Heraclius with his cousin, the magister Theodorus, and the Armenian noble David Saharuni. When Heraclius discovered the plot, he had Athalarichos' nose and hands cut off, and he was exiled to Prinkipo, one of the Princes' Islands. Theodorus had the same treatment but was sent to Gaudomelete (possibly modern day Gozo Island) with additional instructions to cut off one leg.
During the last years of Heraclius' life, it became evident that a struggle was taking place between Heraclius Constantine and Martina, who was trying to position her son Heraklonas in line for the throne. When Heraclius died, he willed the empire to both Heraclius Constantine and Heraklonas to rule jointly with Martina as Empress.
References.
</dl>
External links.
 

</doc>
<doc id="13957" url="http://en.wikipedia.org/wiki?curid=13957" title="Henry the Fowler">
Henry the Fowler

Henry the Fowler (German: "Heinrich der Finkler" or "Heinrich der Vogler"; Latin: "Henricius Auceps") (876 – 2 July 936) was the Duke of Saxony from 912 and the King of Germany from 919 until his death. First of the Ottonian Dynasty of German kings and emperors, he is generally considered to be the founder and first king of the medieval German state, known until then as East Francia. An avid hunter, he obtained the epithet "the Fowler" because he was allegedly fixing his birding nets when messengers arrived to inform him that he was to be king.
Family.
Born in Memleben, in what is now Saxony-Anhalt, Henry was the son of Otto the Illustrious, Duke of Saxony, and his wife Hedwiga, daughter of Henry of Franconia and Ingeltrude and a great-great-granddaughter of Charlemagne, or Charles I. In 906 he married Hatheburg von Merseburg, daughter of the Saxon count Erwin. She had previously been a nun. The marriage was annulled in 909 because her vows as a nun were deemed by the church to remain valid. She had already given birth to Henry's son Thankmar. The annulment placed a question mark over Thankmar's legitimacy. Later that year he married Matilda, daughter of Dietrich, Count in Westphalia. Matilda bore him three sons, one called Otto, and two daughters, Hedwig and Gerberga, and founded many religious institutions, including the abbey of Quedlinburg where Henry is buried. She was later canonized.
Succession.
Henry became Duke of Saxony upon his father's death in 912. An able ruler, he continued to strengthen the position of his duchy within the developing Kingdom of Germany, frequently in conflict with his neighbors to the South, the dukes of Franconia.
On 23 December 918 Conrad I, King of East Francia and Franconian duke, died. Although they had been at odds with each other from 912–15 over the title to lands in Thuringia, before he died Conrad recommended Henry as his successor. Conrad's choice was conveyed by Duke Eberhard of Franconia, Conrad's brother and heir, at the Imperial Diet of Fritzlar in 919. The assembled Franconian and Saxon nobles duly elected Henry to be king. Archbishop Heriger of Mainz offered to anoint Henry according to the usual ceremony, but he refused to be anointed by a high church official — the only King of his time not to undergo that rite — allegedly because he wished to be king not by the church's but by the people's acclaim. Duke Burchard II of Swabia soon swore fealty to the new King, but Duke Arnulf of Bavaria did not submit until Henry defeated him in two campaigns in 921. Henry besieged his residence at Ratisbon (Regensburg) and forced Arnulf into submission.
In 920, the West Frankish king Charles the Simple invaded Germany and marched as far as Pfeddersheim near Worms, but he retired when he learned Henry was arming against him. On 7 November 921, Henry and Charles met and concluded a treaty of friendship. Henry then saw an opportunity to wrest the Duchy of Lorraine from France when the French civil war began with the coronation of King Robert I. In 923 Henry crossed the Rhine twice. Later in the year he entered Lorraine with an army, capturing a large part of the duchy. The eastern part of Lorraine was left in Henry's possession until October 924.
Reign.
Henry regarded the German kingdom as a confederation of stem duchies rather than as a feudal monarchy and saw himself as "primus inter pares". Instead of seeking to administer the empire through counts, as Charlemagne had done and as his successors had attempted, Henry allowed the dukes of Franconia, Swabia, and Bavaria to maintain complete internal control of their holdings. In 925, Duke Gilbert of Lorraine again rebelled. Henry invaded the duchy and besieged Gilbert at Zülpich (Tolbiac), captured the town, and became master of a large portion of his lands. Thus he brought that realm, which had been lost in 910, back into the German kingdom as the fifth stem duchy. Allowing Gilbert to remain in power as duke, Henry arranged the marriage of his daughter Gerberga to his new vassal in 928.
Henry was an able military leader. In 921 Hungarians (Magyars) invaded Germany and Italy. Although a sizable force was routed near Bleiburg in the Bavarian March of Carinthia by Eberhard and the Count of Meran and another group was routed by Liutfried, count of Elsass (French reading: Alsace), the Magyars repeatedly raided Germany. Nevertheless Henry, having captured a Hungarian prince, managed to arrange a ten-year-truce in 926, though he was forced to pay tributes. By doing so he and the German dukes gained time to fortify towns and train a new elite cavalry force.
During the truce with the Magyars, Henry subdued the Polabian Slavs, settling on the eastern border of his realm. In the winter of 928, he marched against the Slavic Hevelli tribes and seized their capital, Brandenburg. He then invaded the Glomacze lands on the middle Elbe river, conquering the capital Gana (Jahna) after a siege, and had a fortress (the later Albrechtsburg) built at Meissen. In 929, with the help of Arnulf of Bavaria, Henry entered Bohemia and forced Duke Wenceslaus I to resume the yearly payment of tribute to the king. Meanwhile, the Slavic Redarii had driven away their chief, captured the town of Walsleben, and massacred the inhabitants. Counts Bernard and Thietmar marched against the fortress of Lenzen beyond the Elbe, and, after fierce fighting, completely routed the enemy on 4 September 929. The Lusatians and the Ukrani on the lower Oder were subdued and made tributary in 932 and 934, respectively. Henry left no consistent march administration, which was implemented by his successor Otto I.
In 932 Henry finally refused to pay the regular tribute to the Magyars. When they began raiding again, he led a unified army of all German duchies to victory at the Battle of Riade in 933 near the river Unstrut, thus stopping the Magyar advance into Germany. He also pacified territories to the north, where the Danes had been harrying the Frisians by sea. The monk and chronicler Widukind of Corvey in his "Res gestae Saxonicae" reports that the Danes were subjects of Henry the Fowler. Henry incorporated into his kingdom territories held by the Wends, who together with the Danes had attacked Germany, and also conquered Schleswig in 934.
Family and children.
As the first Saxon ruler of Germany, Henry was the founder of the Ottonian dynasty of German rulers. He and his descendants ruled Germany, and later the Holy Roman Empire, from 919 until 1024. In relation to the other members of his dynasty, Henry I was the father of Otto I, grandfather of Otto II, great-grandfather of Otto III, and great-grandfather of Henry II. 
Henry had two wives and at least six children.
Death.
Henry died on 2 July 936 in his palatium in Memleben, one of his favourite places. By then all German peoples were united in a single kingdom. He was buried at Quedlinburg Abbey, established by his wife Matilda in his honor.
His son Otto succeeded him as king, and was crowned Emperor in 962. His second son, Henry, became Duke of Bavaria. A third son, Brun (or Bruno), became archbishop of Cologne. His son from his first marriage, Thankmar, rebelled against his half-brother Otto and was killed in battle in 936. After the death of her husband Duke Giselbert of Lotharingia, Henry's daughter Gerberga of Saxony married King Louis IV of France. His youngest daughter, Hedwige of Saxony, married Duke Hugh the Great of France and was the mother of Hugh Capet, the first Capetian king of France.
Legacy.
Henry returned to public attention as a character in Richard Wagner's opera, "Lohengrin" (1850), trying to gain the support of the Brabantian nobles against the Magyars. After the attempts to achieve German national unity failed with the Revolutions of 1848, Wagner strongly relied on the picture of Henry as the actual ruler of all German tribes as advocated by pan-Germanist activists like Friedrich Ludwig Jahn.
There are indications that Heinrich Himmler saw himself as the reincarnation of the first king of Germany. Nazi ideology referred to Henry as a founding father of the German nation, fighting both the Latin Western Franks and the Slavic tribes of the East, thereby a precursor of the German "Drang nach Osten".
References.
<br>

</doc>
<doc id="13959" url="http://en.wikipedia.org/wiki?curid=13959" title="Hannibal">
Hannibal

Hannibal Barca, son of Hamilcar Barca, (247 – 183/182/181 BC) was a Punic Carthaginian military commander, generally considered one of the greatest military commanders in history. His father, Hamilcar Barca, was the leading Carthaginian commander during the First Punic War, his younger brothers were Mago and Hasdrubal, and he was brother-in-law to Hasdrubal the Fair.
Hannibal lived during a period of great tension in the Mediterranean, when the Roman Republic established its supremacy over other great powers such as Carthage and the Hellenistic kingdoms of Macedon, Syracuse, and the Seleucid Empire. One of his most famous achievements was at the outbreak of the Second Punic War, when he marched an army, which included elephants, from Iberia over the Pyrenees and the Alps into Italy. In his first few years in Italy, he won three dramatic victories—Trebia, Trasimene, and Cannae, in which he distinguished himself for his ability to determine his and his opponent's strengths and weaknesses, and to play the battle to his strengths and the enemy's weaknesses—and won over many allies of Rome. Hannibal occupied much of Italy for 15 years, but a Roman counter-invasion of North Africa forced him to return to Carthage, where he was decisively defeated by Scipio Africanus at the Battle of Zama. Scipio had studied Hannibal's tactics and brilliantly devised some of his own, and finally defeated Rome's nemesis at Zama, having previously driven Hasdrubal, Hannibal's brother, out of the Iberian Peninsula.
After the war, Hannibal successfully ran for the office of suffete. He enacted political and financial reforms to enable the payment of the war indemnity imposed by Rome; however, Hannibal's reforms were unpopular with members of the Carthaginian aristocracy and in Rome, and he fled into voluntary exile. During this time, he lived at the Seleucid court, where he acted as military advisor to Antiochus III in his war against Rome. After Antiochus met defeat at the Battle of Magnesia and was forced to accept Rome's terms, Hannibal fled again, making a stop in Armenia. His flight ended in the court of Bithynia, where he achieved an outstanding naval victory against a fleet from Pergamon. He was afterwards betrayed to the Romans and committed suicide by poisoning himself.
Often regarded as one of the greatest military strategists in history, Hannibal would later be considered one of the greatest generals of antiquity, together with Alexander the Great, Julius Caesar, Scipio, and Pyrrhus of Epirus. Plutarch states that, when questioned by Scipio as to who was the greatest general, Hannibal is said to have replied either Alexander or Pyrrhus, then himself, or, according to another version of the event, Pyrrhus, Scipio, then himself. Military historian Theodore Ayrault Dodge once famously called Hannibal the "father of strategy", because his greatest enemy, Rome, came to adopt elements of his military tactics in its own strategic arsenal. This praise has earned him a strong reputation in the modern world, and he was regarded as a great strategist by men like Napoleon Bonaparte.
Background and early career.
Hannibal was one of the sons of Hamilcar Barca, a Carthaginian leader. He had several sisters and two brothers, Hasdrubal and Mago. His brothers-in-law were Hasdrubal the Fair and the Numidian king Naravas. He was still a child when his sisters married, and his brothers-in-law were close associates during his father's struggles in the Mercenary War and the Punic conquest of Iberia. In light of Hamilcar Barca's cognomen, historians refer to Hamilcar's family as the Barcids. However, there is debate as to whether the cognomen Barca (meaning "thunderbolt") was applied to Hamilcar alone or was hereditary within his family. If the latter, then Hannibal and his brothers also bore the name 'Barca'.
After Carthage's defeat in the First Punic War, Hamilcar set out to improve his family's and Carthage's fortunes. With that in mind and supported by Gades, Hamilcar began the subjugation of the tribes of the Iberian Peninsula. Carthage at the time was in such a poor state that its navy was unable to transport his army to Iberia (Hispania); instead, Hamilcar had to march it towards the Pillars of Hercules and transport it across the Strait of Gibraltar. 
According to Polybius, Hannibal much later said that when he came upon his father and begged to go with him, Hamilcar agreed and demanded that he swear that as long as he lived he would never be a friend of Rome. There is even an account of him at a very young age begging his father to take him to an overseas war. In the story, Hannibal's father took him up and brought him to a sacrificial chamber. Hamilcar held Hannibal over the fire roaring in the chamber and made him swear that he would never be a friend of Rome. Other sources report that Hannibal told his father, "I swear so soon as age will permit...I will use fire and steel to arrest the destiny of Rome." According to the tradition, Hannibal's oath took place in the town of Peñíscola, today part of the community of Valencia, Spain.
Hannibal's father went about the conquest of Hispania. When his father drowned in battle, Hannibal's brother-in-law Hasdrubal succeeded to his command of the army with Hannibal serving as an officer under him. Hasdrubal pursued a policy of consolidation of Carthage's Iberian interests, even signing a treaty with Rome whereby Carthage would not expand north of the Ebro River, so long as Rome did not expand south of it. Hasdrubal also endeavoured to consolidate Carthaginian power through diplomatic relationships with native tribes. 
Upon the assassination of Hasdrubal (221 BC), Hannibal was proclaimed commander-in-chief by the army and confirmed in his appointment by the Carthaginian government. Livy, a Roman scholar, gives a depiction of the young Carthaginian: No sooner had he arrived...the old soldiers fancied they saw Hamilcar in his youth given back to them; the same bright look; the same fire in his eye, the same trick of countenance and features. Never was one and the same spirit more skillful to meet opposition, to obey, or to command...
After he assumed command, Hannibal spent two years consolidating his holdings and completing the conquest of Hispania, south of the Ebro. In his first campaign, Hannibal attacked and stormed the Olcades' strongest centre, Alithia, which promptly led to their surrender, and brought Punic power close to the River Tagus. His following campaign in 220 was against the Vaccaei to the west, where he stormed the Vaccaen strongholds of Helmantice and Arbucala. On his return home, laden with many spoils, a coalition of Spanish tribes, led by the Carpetani, attacked, and Hannibal won his first major battlefield success and showed off his tactical skills at the battle of the River Tagus. However, Rome, fearing the growing strength of Hannibal in Iberia, made an alliance with the city of Saguntum, which lay a considerable distance south of the River Ebro and claimed the city as its protectorate. Hannibal not only perceived this as a breach of the treaty signed with Hasdrubal, but as he was already planning an attack on Rome, this was his way to start the war. So he laid siege to the city, which fell after eight months. Rome reacted to this apparent violation of the treaty and demanded justice from Carthage. In view of Hannibal's great popularity, the Carthaginian government did not repudiate Hannibal's actions, and the war he sought was declared at the end of the year. Hannibal was now determined to carry the war into the heart of Italy by a rapid march through Hispania and southern Gaul.
Second Punic War in Italy (218–203 BC).
Overland journey to Italy.
This journey was originally planned by Hannibal's brother-in-law Hasdrubal the Fair who became a Carthaginian general in Iberia in 229 BC. He would maintain this post for some eight years until 221 BC. Soon the Romans became aware of an alliance between Carthage and the Celts of the Po River valley in Northern Italy. The latter were amassing forces to invade farther south in Italy, presumably with Carthaginian backing. Thus, the Romans preemptively invaded the Po region in 225 BC. By 220 BC, the Romans had annexed the area as Gallia Cisalpina. Hasdrubal was assassinated around the same time (221 BC), bringing Hannibal to the fore. It seems that, having apparently dealt with the threat of a Gallo-Carthaginian invasion (and perhaps knowing that the original Carthaginian commander had been killed), the Romans lulled themselves into a false sense of security. 
Hannibal departed New Carthage in late spring of 218 BC. He fought his way through the northern tribes to the foothills of the Pyrenees, subduing the tribes through clever mountain tactics and stubborn fighting. He left a detachment of 20,000 troops to garrison the newly conquered region. At the Pyrenees, he released 11,000 Iberian troops who showed reluctance to leave their homeland. Hannibal reportedly entered Gaul with 40,000 foot soldiers and 12,000 horsemen.
Hannibal recognized that he still needed to cross the Pyrenees, the Alps, and many significant rivers. Additionally, he would have to contend with opposition from the Gauls, whose territory he passed through. Starting in the spring of 218 BC, he crossed the Pyrenees and, by conciliating the Gaulish chiefs along his passage, reached the River Rhône before the Romans could take any measures to bar his advance. Arriving at the Rhône in September, Hannibal's army numbered 38,000 infantry, 8,000 cavalry, and 38 elephants, almost all of which would not survive the harsh conditions of the Alps.
After outmaneuvering the natives, who had tried to prevent his crossing, Hannibal evaded a Roman force marching from the Mediterranean coast by turning inland up the valley of the Rhône. His exact route over the Alps has been the source of scholarly dispute ever since. (Polybius, the surviving ancient account closest in time to Hannibal's campaign, reports that the route was already debated.) The most influential modern theories favor either a march up the valley of the Drôme and a crossing of the main range to the south of the modern highway over the Col de Montgenèvre or a march farther north up the valleys of the Isère and Arc crossing the main range near the present Col de Mont Cenis or the Little St Bernard Pass. Recent numismatic evidence suggests that Hannibal's army may have passed within sight of the Matterhorn.
By Livy's account the crossing was accomplished in the face of huge difficulties. These Hannibal surmounted with ingenuity, such as when he used vinegar and fire to break through a rockfall. According to Polybius he arrived in Italy accompanied by 20,000 foot soldiers and 4,000 horsemen, and only a few elephants. The fired rockfall event is mentioned only by Livy; Polybius is mute on the subject and there is no evidence of carbonized rock at the only two-tier rockfall in the Western Alps, located below the Col de la Traversette (Mahaney, 2008). If Polybius is correct in his figure for the number of troops he commanded after the crossing of the Rhône, this would suggest that he had lost almost half of his force. Historians like Serge Lancell have questioned the reliability of the figures for the number of troops he had when he left Hispania. From the start, he seems to have calculated that he would have to operate without aid from Hispania.
Hannibal's vision of military affairs, derived partly from the teaching of his Greek tutors and experience gained alongside his father, stretched over most of the Hellenistic World of his time. Indeed, the breadth of his vision gave rise to his grand strategy of conquering Rome by opening a northern front and subduing allied city-states on the peninsula rather than by attacking Rome directly. Historical events, which led to the defeat of Carthage during the First Punic War when his father commanded the Carthaginian Army, led Hannibal to plan the invasion of Italy by land across the Alps.
The task was daunting to say the least. It involved the mobilization of between 60,000 and 100,000 troops (see Proctor, 1971) and the training of a war-elephant corps, all of which had to be provisioned along the way. The alpine invasion of Italy was a military operation that would shake the Mediterranean World of 218 BC with repercussions for more than two decades.
Battle of Trebia.
Hannibal's perilous march brought him into the Roman territory and frustrated the attempts of the enemy to fight out the main issue on foreign ground. His sudden appearance among the Gauls of the Po Valley, moreover, enabled him to detach those tribes from their new allegiance to the Romans before the latter could take steps to check the rebellion.
Publius Cornelius Scipio, the consul who commanded the Roman force sent to intercept Hannibal, and Scipio Africanus' father, had not expected Hannibal to make an attempt to cross the Alps, since the Romans were prepared to fight the war in Iberia. With a small detachment still positioned in Gaul, Scipio made an attempt to intercept Hannibal. Through prompt decision and speedy movement, he succeeded in transporting his army to Italy by sea, in time to meet Hannibal. Hannibal's forces moved through the Po Valley and were engaged in a large scale skirmish at Ticinus. Here, Hannibal forced the Romans, by virtue of his superior cavalry, to evacuate the plain of Lombardy. While the victory was minor, it encouraged the Gauls and Ligurians to join the Carthaginian cause, whose troops bolstered his army back to around 40,000 men. Scipio was severely injured, his life only saved by the bravery of his son who rode back onto the field to rescue his fallen father. Scipio retreated across the river Trebia to camp at Placentia with his army mostly intact.
The other Roman consular army was rushed to the Po Valley. Even before news of the defeat at Ticinus had reached Rome, the Senate had ordered the Consul Sempronius Longus to bring his army back from Sicily to meet Scipio and face Hannibal. Hannibal, by skillful maneuvers, was in position to head him off, for he lay on the direct road between Placentia and Arminum, by which Sempronius would have to march to reinforce Scipio. He then captured Clastidium, from which he drew large amounts of supplies for his men. But this gain was not without its loss, as Sempronius avoided Hannibal's watchfulness, slipped around his flank, and joined his colleague in his camp near the Trebia River near Placentia. There, in December of the same year, Hannibal had an opportunity to show his masterful military skill at Trebia; where after wearing down the superior Roman infantry he then cut it to pieces with a surprise attack and ambush from the flanks.
Battle of Lake Trasimene.
Hannibal quartered his troops for the winter with the Gauls, whose support for him had abated. In the Spring of 217 BC, Hannibal decided to find a more reliable base of operations farther south. Expecting Hannibal to advance on Rome, Gnaeus Servilius and Gaius Flaminius (the new consuls of Rome) took their armies to block the eastern and western routes Hannibal could use.
The only alternative route to central Italy lay at the mouth of the Arno. This area was practically one huge marsh, and happened to be overflowing more than usual during this particular season. Hannibal knew that this route was full of difficulties, but it remained the surest and certainly the quickest way to central Italy. Polybius claims Hannibal's men marched for four days and three nights, "through a land that was under water", suffering terribly from fatigue and enforced want of sleep. He crossed the Apennines (during which he lost his right eye because of conjunctivitis) and the seemingly impassable Arno without opposition, but in the marshy lowlands of the Arno, he lost a large part of his force.
Arriving in Etruria in the spring of 217 BC, Hannibal decided to lure the main Roman army under Flaminius, into a pitched battle, by devastating the region Flaminius had been sent to protect. As Polybius recounts, "he [Hannibal] calculated that, if he passed the camp and made a descent into the district beyond, Flaminius (partly for fear of popular reproach and partly of personal irritation) would be unable to endure watching passively the devastation of the country but would spontaneously follow him... and give him opportunities for attack." At the same time, Hannibal tried to break the allegiance of Rome's allies by proving that Flaminius was powerless to protect them. Despite this, Flaminius remained passively encamped at Arretium. Unable to draw Flaminius into battle by mere devastation, Hannibal marched boldly around his opponent's left flank and effectively cut Flaminius off from Rome (thus executing the first recorded turning movement in military history). Advancing through the uplands of Etruria, Hannibal provoked Flaminius into a hasty pursuit and, catching him in a defile on the shore of Lake Trasimenus, destroyed his army in the waters or on the adjoining slopes, killing Flaminius as well (see Battle of Lake Trasimene). This was the most costly ambush the Romans would ever sustain until the Battle of Carrhae against the Parthians. He had now disposed of the only field force that could check his advance upon Rome, but, realizing that without siege engines, he could not hope to take the capital, he preferred to exploit his victory by entering into central and southern Italy and encouraging a general revolt against the sovereign power.
The Romans appointed Fabius Maximus as their dictator. Departing from Roman military traditions, Fabius adopted the strategy named after him: avoiding open battle, while placing several Roman armies in Hannibal's vicinity in order to watch and limit his movements.
Having ravaged Apulia without bringing Fabius to battle, Hannibal decided to march through Samnium to Campania, one of the richest and most fertile provinces of Italy, hoping that the devastation would draw Fabius into battle. Fabius closely followed Hannibal's path of destruction, yet still refused to let himself be drawn out of the defensive. This strategy was unpopular with many Romans, who believed it was a form of cowardice.
Hannibal decided that it would be unwise to winter in the already devastated lowlands of Campania, but Fabius had ensured that all the passes out of Campania were blocked. To avoid this, Hannibal deceived the Romans into thinking that the Carthaginian army was going to escape through the woods. As the Romans moved off towards the woods, Hannibal's army occupied the pass, and his army made their way through the pass unopposed. Fabius was within striking distance but in this case his caution worked against him. Smelling a stratagem (rightly), he stayed put. For the winter, Hannibal found comfortable quarters in the Apulian plain. What Hannibal achieved in extricating his army was, as Adrian Goldsworthy puts it, "a classic of ancient generalship, finding its way into nearly every historical narrative of the war and being used by later military manuals". This was a severe blow to Fabius' prestige and soon after this his period of dictatorial power ended.
Battle of Cannae.
In the spring of 216 BC, Hannibal took the initiative and seized the large supply depot at Cannae in the Apulian plain. By capturing Cannae, Hannibal had placed himself between the Romans and their crucial sources of supply. Once the Roman Senate resumed their consular elections in 216 BC, they appointed Gaius Terentius Varro and Lucius Aemilius Paullus as consuls. In the meantime, the Romans, hoping to gain success through sheer strength and weight of numbers, raised a new army of unprecedented size, estimated by some to be as large as 100,000 men, but more likely around 50-80,000.
The Romans and allied legions, resolving to confront Hannibal, marched southward to Apulia. They eventually found Hannibal on the left bank of the Aufidus River, and encamped six miles (10 km) away. On this occasion, the two armies were combined into one, the consuls having to alternate their command on a daily basis. Varro, who was in command on the first day, was a man of reckless and hubristic nature (according to Livy) and was determined to defeat Hannibal. Hannibal capitalized on the eagerness of Varro and drew him into a trap by using an envelopment tactic, which eliminated the Roman numerical advantage by shrinking the combat area. Hannibal drew up his least reliable infantry in a semicircle in the center with the wings composed of the Gallic and Numidian horse. The Roman legions forced their way through Hannibal's weak center, but the Libyan mercenaries on the wings, swung around by the movement, menaced their flanks. The onslaught of Hannibal's cavalry was irresistible, and Maharbal, Hannibal's chief cavalry commander, who led the mobile Numidian cavalry on the right, shattered the Roman cavalry opposing them. Hannibal's Iberian and Gallic heavy cavalry, led by Hanno on the left, defeated the Roman heavy cavalry, and then both the Carthaginian heavy cavalry and the Numidians attacked the legions from behind. As a result, the Roman army was hemmed in with no means of escape.
Due to these brilliant tactics, Hannibal, with much inferior numbers, managed to surround and destroy all but a small remnant of his enemy. Depending upon the source, it is estimated that 50,000-70,000 Romans were killed or captured. Among the dead were the Roman Consul Lucius Aemilius Paullus, as well as two consuls for the preceding year, two quaestors, twenty-nine out of the forty-eight military tribunes and an additional eighty senators (at a time when the Roman Senate comprised no more than 300 men, this constituted 25%–30% of the governing body). This makes the battle one of the most catastrophic defeats in the history of Ancient Rome, and one of the bloodiest battles in all of human history (in terms of the number of lives lost within a single day). After Cannae, the Romans were very hesitant to confront Hannibal in pitched battle, preferring instead to weaken him by attrition, relying on their advantages of interior lines, supply, and manpower. As a result, Hannibal fought no more major battles in Italy for the rest of the war. It is believed his refusal to bring the war to Rome itself was due to a lack of commitment from Carthage of men, money and materiel — principally siege equipment. Whatever the reason, the choice prompted Maharbal to say, "Hannibal, you know how to gain a victory, but not how to use one."
As a result of this victory, many parts of Italy joined Hannibal's cause. As Polybius notes, "How much more serious was the defeat of Cannae, than those that preceded it can be seen by the behavior of Rome's allies; before that fateful day, their loyalty remained unshaken, now it began to waver for the simple reason that they despaired of Roman Power." During that same year, the Greek cities in Sicily were induced to revolt against Roman political control, while the Macedonian king, Philip V, pledged his support to Hannibal – thus initiating the First Macedonian War against Rome. Hannibal also secured an alliance with newly appointed Hieronymus of Syracuse. It is often argued that if Hannibal had received proper material reinforcements from Carthage, he might have succeeded with a direct attack upon Rome. Instead, he had to content himself with subduing the fortresses that still held out against him, and the only other notable event of 216 BC was the defection of certain Italian territories, including Capua, the second largest city of Italy, which Hannibal made his new base. However, only a few of the Italian city-states he expected to gain as allies defected to him.
Stalemate.
The war in Italy settled into a strategic stalemate. The Romans used the attritional strategy Fabius had taught them, and which, they finally realized, were the only feasible means of defeating Hannibal. Indeed, Fabius received the surname "Cunctator" ("the Delayer") because of his policy of not meeting Hannibal in open battle but through guerilla, scorched earth tactics.
The Romans deprived Hannibal of a large-scale battle and instead, assaulted his weakening army with multiple smaller armies in an attempt to both weary him and create unrest in his troops. For the next few years, Hannibal was forced to sustain a scorched earth policy and obtain local provisions for protracted and ineffectual operations throughout southern Italy. His immediate objectives were reduced to minor operations centered mainly round the cities of Campania.
As the forces detached to his lieutenants were generally unable to hold their own, and neither his home government nor his new ally Philip V of Macedon helped to make good his losses, his position in southern Italy became increasingly difficult and his chance of ultimately conquering Rome grew ever more remote. Hannibal still won a number of notable victories: completely destroying two Roman armies in 212 BC, and at one point, killing two consuls (including the famed Marcus Claudius Marcellus) in a battle in 208 BC. However, inadequately supported by his Italian allies, abandoned by his government (either because of jealousy or simply because Carthage was overstretched), and unable to match Rome's resources, Hannibal slowly began losing ground, never able to bring about another grand decisive victory that could produce a lasting strategic change.
Carthaginian political will was embodied in the ruling oligarchy. While there was a Carthaginian Senate, the real power was with the inner "Council of 30 Nobles" and the board of judges from ruling families known as the "Hundred and Four". These two bodies came from the wealthy, commercial families of Carthage. Two political factions operated in Carthage: the war party, also known as the "Barcids" (Hannibal's family name) and the peace party led by Hanno II the Great. Hanno had been instrumental in denying Hannibal's requested reinforcements following the battle at Cannae.
Hannibal started the war without the full backing of Carthaginian oligarchy. His attack of Saguntum had presented the oligarchy with a choice of war with Rome or loss of prestige in Iberia. The oligarchy, not Hannibal, controlled the strategic resources of Carthage. Hannibal constantly sought reinforcements from either Iberia or North Africa. Hannibal's troops lost in combat were replaced with less well-trained and motivated mercenaries from Italy or Gaul. The commercial interests of the Carthaginian oligarchy dictated the reinforcement and supply of Iberia rather than Hannibal throughout the campaign.
Hannibal's retreat in Italy.
In 212 BC Hannibal captured Tarentum but he failed to obtain control of its harbour. The tide was slowly turning against him, and in favor of Rome.
The Romans then mounted two sieges of Capua, which fell in 211 BC, and completed their conquest of Syracuse and destruction of the Carthaginian army in Sicily. Shortly thereafter, the Romans pacified Sicily and entered into an alliance with the Aetolian League to counter Phillip V. Philip, who attempted to exploit Rome's preoccupation in Italy to conquer Illyria, now found himself under attack from several sides at once and was quickly subdued by Rome and her Greek allies. Meanwhile, Hannibal had defeated Fulvius at the battle of Herdonia in Apulia, but lost Tarentum the following year.
In 210 BC Hannibal again proved his superiority in tactics by inflicting a severe defeat at Herdonia (modern Ordona) in Apulia upon a proconsular army, and in 208 BC destroyed a Roman force engaged in the siege of Locri Epizephyri. But with the loss of Tarentum in 209 BC and the gradual reconquest by the Romans of Samnium and Lucania, his hold on south Italy was almost lost. In 207 BC he succeeded in making his way again into Apulia, where he waited to concert measures for a combined march upon Rome with his brother Hasdrubal Barca. On hearing, however, of his brother's defeat and death at the Metaurus he retired into Bruttium, where he maintained himself for the ensuing years. His brother's head had been cut off, carried across Italy, and tossed over the palisade of Hannibal's camp as a cold message of the iron-clad will of the Roman Republic. The combination of these events marked the end to Hannibal's success in Italy. With the failure of his brother Mago Barca in Liguria (205–203 BC) and of his own negotiations with Philip V of Macedon, the last hope of recovering his ascendancy in Italy was lost. In 203 BC, after nearly fifteen years of fighting in Italy, and with the military fortunes of Carthage rapidly declining, Hannibal was recalled to Carthage to direct the defense of his native country against a Roman invasion under Scipio Africanus.
Conclusion of Second Punic War (203–201 BC).
Return to Carthage.
In 203 BC, Hannibal was recalled from Italy by the war party in Carthage. After leaving a record of his expedition engraved in Punic and Greek upon bronze tablets in the temple of Juno at Crotona, he sailed back to Africa. His arrival immediately restored the predominance of the war party, which placed him in command of a combined force of African levies and his mercenaries from Italy. In 202 BC, Hannibal met Scipio in a fruitless peace conference. Despite mutual admiration, negotiations floundered due to Roman allegations of "Punic Faith," referring to the breach of protocols that ended the First Punic War by the Carthaginian attack on Saguntum, and a Carthaginan attack on a stranded Roman fleet. Scipio and Carthage had worked out a peace plan, which was approved by Rome. The terms of the treaty were quite modest, but the war had been long for the Romans. Carthage could keep its African territory but would lose its overseas empire, a "fait accompli". Masinissa (Numidia) was to be independent. Also, Carthage was to reduce its fleet and pay a war indemnity. But Carthage then made a terrible blunder. Its long-suffering citizens had captured a stranded Roman fleet in the Gulf of Tunis(Tunisia) and stripped it of supplies, an action that aggravated the faltering negotiations. Meanwhile Hannibal, recalled from Italy by the Carthaginian Senate, had returned with his army. Fortified by both Hannibal and the supplies, the Carthaginians rebuffed the treaty and Roman protests. The decisive battle at Zama soon followed; the defeat removed Hannibal's air of invincibility.
Battle of Zama.
Unlike most battles of the Second Punic War, at Zama, the Romans were superior in cavalry and the Carthaginians had the edge in infantry. This Roman cavalry superiority was due to the betrayal of Masinissa, who had earlier assisted Carthage in Iberia, but changed sides in 206 BC with the promise of land and due to his personal conflicts with Syphax, a Carthaginian ally. Although the aging Hannibal was suffering from mental exhaustion and deteriorating health after years of campaigning in Italy, the Carthaginians still had the advantage in numbers and were boosted by the presence of 80 war elephants.
The Roman cavalry won an early victory by swiftly routing the Carthaginian horse, and standard Roman tactics for limiting the effectiveness of the Carthaginian war elephants were successful, including playing trumpets to frighten the elephants into running into the Carthaginian lines. Some historians say that the elephants routed the Carthaginian cavalry and not the Romans, whilst others suggest that it was actually a tactical retreat planned by Hannibal. Whatever the truth, the battle remained closely fought. At one point, it seemed that Hannibal was on the verge of victory, but Scipio was able to rally his men, and his cavalry, having routed the Carthaginian horse, attacked Hannibal's rear. This two-pronged attack caused the Carthaginian formation to collapse. Classicist T.E.Willis attributes eventual failure in the battle to a temporal lapse in the soundness of Hannibal's tactics -- most likely ascribable to symptoms associated with particularly virulent syphilis. 
With their foremost general defeated, the Carthaginians had no choice but to surrender. Carthage lost approximately 20,000 troops with an additional 15,000 wounded. In contrast, the Romans suffered only 1,500 casualties. The last major battle of the Second Punic War resulted in a loss of respect for Hannibal by his fellow Carthaginians. The conditions of defeat were such that Carthage could no longer battle for Mediterranean supremacy.
Later career.
Peacetime Carthage (200–196 BC).
Hannibal was still only 43 and soon showed that he could be a statesman as well as a soldier. Following the conclusion of a peace that left Carthage stripped of its formerly mighty empire, Hannibal prepared to take a back seat for a time. However, the blatant corruption of the oligarchy gave Hannibal a chance to re-emerge and he was elected as "suffete" or chief magistrate. The office had become rather insignificant, but Hannibal restored its power and authority. The oligarchy, always jealous of him, had even charged him with having betrayed the interests of his country while in Italy, for neglecting to take Rome when he might have done so. So effectively did Hannibal reform abuses that the heavy tribute imposed by Rome could be paid by installments without additional and extraordinary taxation. He also reformed the Hundred and Four, stipulating that its membership be chosen by direct election rather than co-option. He also used citizen support to change the term of office in the Hundred and Four from life to a year, with a term limit of two years.
Exile (195–183/181 BC).
Seven years after the victory of Zama, the Romans, alarmed by Carthage's renewed prosperity, demanded Hannibal's surrender. Hannibal thereupon went into voluntary exile. He journeyed to Tyre, the mother city of Carthage, and then to Ephesus, where he was honorably received by Antiochus III of Syria, who was preparing for war with Rome. Hannibal soon saw that the king's army was no match for the Romans. He advised equipping a fleet and landing a body of troops in the south of Italy, offering to take command himself. But he could not make much impression on Antiochus, who listened to his courtiers and would not entrust Hannibal with any important office. According to Cicero, while at the court of Antiochus, Hannibal attended a lecture by Phormio, a philosopher, that ranged through many topics. When Phormio finished a discourse on the duties of a general, Hannibal was asked his opinion. He replied, "I have seen during my life many old fools; but this one beats them all." Another story according to Aulus Gellius is that when Antiochus III showed off the gigantic and elaborately equipped army he had created to invade Greece to Hannibal, he asked him if they would be enough for the Roman Republic, to which Hannibal replied, "I think all this will be enough, yes, quite enough, for the Romans, even though they are most avaricious." In 191 BC, the Romans under Manius Acilius Glabrio routed Antiochus at Thermopylae and obliged him to withdraw to Asia. The Romans followed up their success by attacking Antiochus in Anatolia, and the Seleucids were decisively defeated at Magnesia ad Sipylum in 190 BC by Scipio Asiaticus.
In 190 BC, he was placed in command of a Seleucid fleet, but was defeated in a battle off the Eurymedon River. According to Strabo and Plutarch, Hannibal also received hospitality at the Armenian court of Artaxias I. The authors add an apocryphal story of how Hannibal planned and supervised the building of the new royal capital Artaxata. When Antiochus seemed prepared to surrender him to the Romans, Hannibal fled to Crete, but he soon went back to Asia Minor and sought refuge with Prusias I of Bithynia, who was engaged in warfare with Rome's ally, King Eumenes II of Pergamon. Hannibal went on to serve Prusias in this war. During one of the naval victories he gained over Eumenes, Hannibal had large pots filled with venomous snakes thrown onto Eumenes' ships. Hannibal also went on to defeat Eumenes in two other battles on land until the Romans interfered and threatened Bithynia into giving up Hannibal. Hannibal also visited Tyre, the home of his forefathers. However the Romans were determined to hunt him down, and they insisted on his surrender .
Death (183/181 BC).
Prusias agreed to give him up, but Hannibal was determined not to fall into his enemies' hands. At Libyssa on the eastern shore of the Sea of Marmara, he took poison, which, it was said, he had long carried about with him in a ring. Before dying, he left behind a letter declaring, "Let us relieve the Romans from the anxiety they have so long experienced, since they think it tries their patience too much to wait for an old man's death."
The precise year of Hannibal's death is unknown. In his "Annales", Titus Pomponius Atticus reports it occurred in 183 BC, and Livy implies the same. Polybius, who wrote nearest the event, gives 182 BC. Sulpicius Blitho records it under 181 BC.
Legacy to the ancient world.
It was written that Hannibal taught the Romans the meaning of fear. It has been said that for generations, Roman housekeepers would tell their children brutal tales of Hannibal when they misbehaved. In fact, Hannibal became such a figure of terror that whenever disaster struck, the Roman Senators would exclaim "" ("Hannibal is at the gates!") to express their fear or anxiety. This famous Latin phrase became a common expression that is often still used when a client arrives through the door or when one is faced with calamity.
The works of Roman writers such as Livy, Frontinus, and Juvenal show a grudging admiration for Hannibal. The Romans even built statues of the Carthaginian in the very streets of Rome to advertise their defeat of such a worthy adversary. It is plausible to suggest that Hannibal engendered the greatest fear Rome had towards an enemy. Nevertheless, they grimly refused to admit the possibility of defeat and rejected all overtures for peace; they even refused to accept the ransom of prisoners after Cannae.
During the war there are no reports of revolutions among the Roman citizens, no factions with the Senate desiring peace, no pro-Carthaginian Roman turncoats, no coups. Indeed, throughout the war Roman aristocrats ferociously competed with each other for positions of command to fight against Rome's most dangerous enemy. Hannibal's military genius was not enough to really disturb the Roman political process and the collective political and military capacity of the Roman people. As Lazenby states,
"It says volumes, too, for their political maturity and respect for constitutional forms that the complicated machinery of government continued to function even amidst disaster—there are few states in the ancient world in which a general who had lost a battle like Cannae would have dared to remain, let alone would have continued to be treated respectfully as head of state."
According to the historian Livy, the Romans feared Hannibal's military genius, and during Hannibal's march against Rome in 211 BC
"a messenger who had travelled from Fregellae for a day and a night without stopping created great alarm in Rome, and the excitement was increased by people running about the City with wildly exaggerated accounts of the news he had brought. The wailing cry of the matrons was heard everywhere, not only in private houses but even in the temples. Here they knelt and swept the temple-floors with their dishevelled hair and lifted up their hands to heaven in piteous entreaty to the gods that they would deliver the City of Rome out of the hands of the enemy and preserve its mothers and children from injury and outrage." In the Senate the news was "received with varying feelings as men's temperaments differed," so it was decided to keep Capua under siege, but to send 15,000 infantry and 1,000 cavalry as reinforcements to Rome.
According to Livy, the land occupied by Hannibal's army outside Rome in 211 BC was sold at the very time of its occupation and for the same price. This may not be true but as Lazenby states, "could well be, exemplifying as it does not only the supreme confidence felt by the Romans in ultimate victory, but also the way in which something like normal life continued. After Cannae the Romans showed a considerable steadfastness in adversity. An undeniable proof of Rome's confidence is demonstrated by the fact that after the Cannae disaster she was left virtually defenseless, but the Senate still chose not to withdraw a single garrison from an overseas province to strengthen the city. In fact, they were reinforced and the campaigns there maintained until victory was secured; beginning first in Sicily under the direction of Claudius Marcellus, and later in Hispania under Scipio Africanus. Although the long-term consequences of Hannibal's war are debatable, this war was undeniably Rome's "finest hour".
Most of the sources available to historians about Hannibal are from Romans. They considered him the greatest enemy Rome had ever faced. Livy gives us the idea that he was extremely cruel. Even Cicero, when he talked of Rome and its two great enemies, spoke of the "honourable" Pyrrhus and the "cruel" Hannibal. Yet a different picture is sometimes revealed. When Hannibal's successes had brought about the death of two Roman consuls, he vainly searched for the body of Gaius Flaminius Nepos on the shores of Lake Trasimene, held ceremonial rituals in recognition of Lucius Aemilius Paullus, and sent Marcellus' ashes back to his family in Rome. Any bias attributed to Polybius, however, is more troublesome, since he was clearly sympathetic towards Hannibal. Nevertheless, Polybius spent a long period as a hostage in Italy and relied heavily on Roman sources, so there remains the possibility that he reproduced elements of Roman propaganda.
Legacy.
Military history.
Hannibal is generally regarded as one of the best military strategists and tacticians of all time, the double envelopment at Cannae an enduring legacy of tactical brilliance. According to Appian, several years after the Second Punic War, Hannibal served as a political advisor in the Seleucid Kingdom and Scipio was sent there on a diplomatic mission from Rome.
It is said that at one of their meetings in the gymnasium Scipio and Hannibal had a conversation on the subject of generalship, in the presence of a number of bystanders, and that Scipio asked Hannibal whom he considered the greatest general, to which the latter replied, "Alexander of Macedonia".
To this Scipio assented since he also yielded the first place to Alexander. Then he asked Hannibal whom he placed next, and he replied, "Pyrrhus of Epirus", because he considered boldness the first qualification of a general; "for it would not be possible", he said, "to find two kings more enterprising than these".
Scipio was rather nettled by this, but nevertheless he asked Hannibal to whom he would give the third place, expecting that at least the third would be assigned to him; but Hannibal replied, "to myself; for when I was a young man I conquered Hispania and crossed the Alps with an army, the first after Hercules."
As Scipio saw that he was likely to prolong his self-laudation he said, laughing, "where would you place yourself, Hannibal, if you had not been defeated by me?" Hannibal, now perceiving his jealousy, replied, "in that case I should have put myself before Alexander". Thus Hannibal continued his self-laudation, but flattered Scipio in a indirect manner by suggesting that he had conquered one who was the superior of Alexander.
At the end of this conversation Hannibal invited Scipio to be his guest, and Scipio replied that he would be so gladly if Hannibal were not living with Antiochus, who was held in suspicion by the Romans. Thus did they, in a manner worthy of great commanders, cast aside their enmity at the end of their wars.
Military academies all over the world continue to study Hannibal's exploits (especially his victory at Cannae).
Maximilian Otto Bismarck Caspari, in his article in the 1911 Encyclopædia Britannica, praises Hannibal in these words:
 As to the transcendent military genius of Hannibal there cannot be two opinions. The man who for fifteen years could hold his ground in a hostile country against several powerful armies and a succession of able generals must have been a commander and a tactician of supreme capacity. In the use of strategies and ambuscades he certainly surpassed all other generals of antiquity. Wonderful as his achievements were, we must marvel the more when we take into account the grudging support he received from Carthage. As his veterans melted away, he had to organize fresh levies on the spot. We never hear of a mutiny in his army, composed though it was of North Africans, Iberians and Gauls. Again, all we know of him comes for the most part from hostile sources. The Romans feared and hated him so much that they could not do him justice. Livy speaks of his great qualities, but he adds that his vices were equally great, among which he singles out his more than Punic perfidy and an inhuman cruelty. For the first there would seem to be no further justification than that he was consummately skillful in the use of ambuscades. For the latter there is, we believe, no more ground than that at certain crises he acted in the general spirit of ancient warfare. Sometimes he contrasts most favorably with his enemy. No such brutality stains his name as that perpetrated by Claudius Nero on the vanquished Hasdrubal. Polybius merely says that he was accused of cruelty by the Romans and of avarice by the Carthaginians. He had indeed bitter enemies, and his life was one continuous struggle against destiny. For steadfastness of purpose, for organizing capacity and a mastery of military science he has perhaps never had an equal.
Even the Roman chroniclers acknowledged Hannibal's supreme military leadership, writing that, "he never required others to do what he could and would not do himself".
According to Polybius 23, 13, p. 423:
"It is a remarkable and very cogent proof of Hannibal's having been by nature a real leader and far superior to anyone else in statesmanship, that though he spent seventeen years in the field, passed through so many barbarous countries, and employed to aid him in desperate and extraordinary enterprises numbers of men of different nations and languages, no one ever dreamt of conspiring against him, nor was he ever deserted by those who had once joined him or submitted to him."
Count Alfred von Schlieffen developed his eponymously titled "Schlieffen Plan" (1905/1906) from his military studies, with a particularly heavy emphasis on the envelopment technique which Hannibal employed to surround and destroy the Roman army at Cannae. George S. Patton believed himself a reincarnation of Hannibal as well as of many other people, including a Roman legionary and a Napoleonic soldier. Norman Schwarzkopf, the commander of the Coalition Forces in the Gulf War of 1990-1991, claimed: "The technology of war may change, the sophistication of weapons certainly changes. But those same principles of war that applied to the days of Hannibal apply today."
According to the military historian Theodore Ayrault Dodge,
Hannibal excelled as a tactician. No battle in history is a finer sample of tactics than Cannae. But he was yet greater in logistics and strategy. No captain ever marched to and fro among so many armies of troops superior to his own numbers and material as fearlessly and skillfully as he. No man ever held his own so long or so ably against such odds. Constantly overmatched by better soldiers, led by generals always respectable, often of great ability, he yet defied all their efforts to drive him from Italy, for half a generation. Excepting in the case of Alexander, and some few isolated instances, all wars up to the Second Punic War, had been decided largely, if not entirely, by battle-tactics. Strategic ability had been comprehended only on a minor scale. Armies had marched towards each other, had fought in parallel order, and the conqueror had imposed terms on his opponent. Any variation from this rule consisted in ambuscades or other stratagems. That war could be waged by avoiding in lieu of seeking battle; that the results of a victory could be earned by attacks upon the enemy's communications, by flank-maneuvers, by seizing positions from which safely to threaten him in case he moved, and by other devices of strategy, was not understood... [However] For the first time in the history of war, we see two contending generals avoiding each other, occupying impregnable camps on heights, marching about each other's flanks to seize cities or supplies in their rear, harassing each other with small-war, and rarely venturing on a battle which might prove a fatal disaster—all with a well-conceived purpose of placing his opponent at a strategic disadvantage... That it did so was due to the teaching of Hannibal.
Hannibal in literature.
Hannibal's name is also commonplace in later art and popular culture, an objective measure of his foreign influence on Western history.
Like other military leaders, Hannibal's victories against superior forces in an ultimately losing cause won him enduring fame that outlasted his native country within North Africa. His crossing of the Alps remains one of the most monumental military feats of ancient warfare and has since captured the imagination of the world (romanticized by several artworks).
Novel unless otherwise noted:
Timeline.
Timeline of Hannibal's life (248 BC–c. 183 BC)
Further reading.
</dl>

</doc>
<doc id="13961" url="http://en.wikipedia.org/wiki?curid=13961" title="Hansie Cronje">
Hansie Cronje

Wessel Johannes "Hansie" Cronje (25 September 1969 – 1 June 2002) was a South African cricketer and captain of the South African national cricket team in the 1990s. He died in a plane crash in 2002. He was voted the 11th greatest South African in 2004 despite having been banned for life from professional cricket for his role in a match-fixing scandal.
Early life.
He was born in Bloemfontein, South Africa to Ewie Cronje, his father and San-Marie Cronje, his mother on the 25th of September 1969. Cronje matriculated in 1987 from Grey College in Bloemfontein where he was the headboy. An excellent all round sportsman, he represented the then Orange Free State Province in cricket and rugby at schools level. He was the captain of his school's Cricket and Rugby team. Cronje earned a Bachelor of Commerce degree from the University of the Free State. He had an older brother, Frans Cronje and a younger sister, Hester Parsons.
His father Ewie had played for Orange Free State in the 1960s, and Hansie's older brother Frans had also played first-class cricket.
First-class career.
Cronje made his first-class debut for Orange Free State against Transvaal at Johannesburg in January 1988 at the age of 18. In the following season, he was a regular, appearing in all eight Currie Cup matches plus being part of the Benson and Hedges Series winning team, scoring 73 as an opener in the final. In 1989–90, despite playing all the Currie Cup matches, he failed to make a century, and averaged only 19.76; however, in one-day games he averaged 60.12. During that season he scored his maiden century for South African Universities against Mike Gatting's rebels.
Despite having just turned 21, Cronje was made captain of Orange Free State for the 1990–91 season. He scored his maiden century for them against Natal in December 1990, and finished the season with another century and a total of 715 runs at 39.72. That season he also scored 159* in a 40-over match against Griqualand West.
In 1992–93, he captained Orange Free State to the Castle Cup/Total Power Series double.
In 1995, Cronje appeared for Leicestershire where he scored 1301 runs at 52.04 finishing the season as the county's leading scorer.
In 1995–96, he finished the season top of the batting averages in the Currie Cup, his top score of 158 helped Free State chase down 389 to beat Northern Transvaal.
In 1997, Cronje played for Ireland as an overseas player in the Benson and Hedges Cup and helped them to a 46-run win over Middlesex by scoring 94 not out and taking three wickets. This was Ireland's first ever win against English county opposition. Later in the same competition, he scored 85 and took one wicket against Glamorgan.
International career.
Debuts.
Cronje's form in 1991/92 was impressive especially in the one-day format where he averaged 61.40. He earned an international call up for the 1992 World Cup, making his One Day International debut against Australia at Sydney. During the tournament he played in eight of the team's nine games, averaging 34.00 with the bat, while his medium pace was used in bowling 20 overs.
After the World Cup Cronje was part of the tour to the West Indies; he featured in the three ODI's, and in the Test match at Bridgetown that followed, he made his Test debut. This was South Africa's first Test since readmission and they came close to beating a strong West Indian side, going into the final day at 122/2 chasing 200 they collapsed to 148.
India toured South Africa in 1992/93. In the first one day international, he hit the famous six when his team needed 6 runs off only 4 balls, and was awarded Man of the match for his bowling. In the one-day series, Cronje managed just one fifty but with the ball he was economical and took his career best figures of 5/32, becoming the second South African to take five wickets in an ODI. In the Test series that followed he scored his maiden test century, 135 off 411 balls, after coming in at 0–1 in the second over he was last man out, after eight and three-quarter hours, in a total of 275. This contributed to South Africa's first Test win since readmission. At the end of the season in a triangular tournament with Pakistan and West Indies he scored 81 off 70 balls against Pakistan.
In South Africa's next Test series against Sri Lanka, Cronje scored his second Test century, 122 in the second Test in Colombo; the victory margin of an innings and 208 runs is a South African record. He finished the series with 237 runs at 59.25 after scoring 73* in the drawn third Test.
Stand-in captain.
In 1993–94, there was another Castle Cup/Total Power Series double for Orange Free State. In international cricket, he was named as vice-captain for the tour of Australia despite being the youngest member of the squad. In the first ODI of the triangular tournament with New Zealand and Australia, he guided South Africa to victory against Australia at the MCG with 91*, which won him the man of the match award. He scored 71 in a rain-affected first test at Melbourne before a tense second test that South Africa won by 5 runs. An injury to captain Kepler Wessels meant Cronje was captain for the final day of the match. Between the second and third tests, the one-day tournament continued, now with Cronje as captain, South Africa made the final series but lost it 2–1 to Australia. He became South Africa's second youngest Test captain, after Murray Bisset in 1898–99, when he led the team for the third test at Adelaide but it was an unsuccessful start to his captaincy career as the series was squared.
In February 1994, there was the return series as Australia toured South Africa. Cronje started the ODI series with scores of 112, 97, 45 and 50* and when Australia played Orange Free State in their final match before the first Test, Cronje hit 251 off 306 balls, 200 of these came on the final day in which 294 runs were added. Despite this, Orange Free State lost the match. In the first test at Johannesburg, he added another century as South Africa won by 197 runs. This innings was the end of a 14 day period in which he'd scored 721 runs against the Aussies. However, he failed to reach fifty in the next two tests and four ODIs as both series were drawn.
There was another drawn series when South Africa toured England in 1994. Cronje scored only one century on the whole tour and scored only 90 runs in the three-test series. In October 1994, South Africa again came up against Australia in a triangular one day series also featuring Pakistan. Cronje scored 354 runs at an average of 88.50. Despite this, South Africa lost all their matches. This series was Bob Woolmer's first as coach and Kepler Wessels' last as captain. Cronje, who'd previously been vice-captain, was named as captain for the test series with New Zealand in 1994–95.
Permanent captain.
South Africa lost the first Test in Johannesburg but before the second test the two teams plus Pakistan and Sri Lanka competed for the Mandela Trophy, New Zealand failed to gain a win in the six match round robin stage while South Africa beat Pakistan in the final. This changed the momentum as South Africa secured wins in Durban and Cape Town, where Cronje scored his fourth test century, he was the first captain since W. G. Grace to win a three-match rubber after being one down.
In early 1995, South Africa won one-off tests against both Pakistan and New Zealand, in Auckland Cronje scored the only century of the match before a final day declaration left his bowlers barely enough time to dismiss the Kiwis.
In October 1995, South Africa won a one-off Test with Zimbabwe. Cronje scored a second innings 54* to guide them to seven wicket win. In two one-dayers that followed, he took five wickets as South Africa comfortably won both. South Africa won the five Test series against England 1–0 despite Cronje struggling, scoring 113 runs at 18.83. However, he top scored in the one-day series that they won 6–1.
In the 1996 World Cup, he scored 78 and 45* against New Zealand and Pakistan respectively as South Africa won their group but in the Quarter final with West Indies a Brian Lara century ended their ten-game winning streak.
The 1996–97 season featured back-to-back series with India. The first away was lost 2–1. The home series was won 2–0. In the six tests combined, Cronje managed one fifty. Cronje produced better form against Australia, averaging over 50 in both test and ODI series although both were lost.
Cronje started 1997–98 by leading South Africa to their first series victory in Pakistan, his batting continued to struggle with his biggest contribution being taking the wickets of Inzamam-ul-Haq and Moin Khan in the Third Test.
Better form.
Cronje once again came up against Australia and once again ended on the losing side. In the triangular one day series they won the group with Australia just scraping through, they also won the first 'final' but South Africa lost the last two finals. During the group matches Cronje had threatened to lead his team off after Pat Symcox had missiles thrown at him, Symcox had the last laugh ending the match with 4/24. Before the Test series started he scored consecutive centuries against Tasmania and Australia A these were his first in two years.
In the first Test, Cronje scored 70 as South Africa saved the match; in the second Test, he lasted 335 minutes for his 88. Despite this, they lost by an innings. In the third Test, they scored 517 and although Mark Taylor carried his bat for 169, Australia needed to bat 109 overs to save the match. Mark Waugh batted 404 minutes, and, despite controversy when Waugh hit one of his bails off (under Law 35 he was adjudged to have finished his stroke and therefore given not out), South Africa fell three wickets short. Cronje put a stump through the umpires` dressing room door after the match and was lucky to avoid a ban.
Cronje missed the first Test of the series with Pakistan because of a knee injury. The second Test at Durban was lost, but he top scored at Port Elizabeth with 85, to help square the three Test series 1–1. There was still time in the season for a two-Test series with Sri Lanka. The first was won with Cronje scoring 49 and 74; in the second Test, he took 3/14, his best bowling in Tests,and smashed 82 off 63 balls, his fifty being brought up with three consecutive sixes off Muttiah Muralitharan, and was reached off just 31 balls; at the time, it was the second fastest in Tests after Kapil Dev's. In the triangular series, which South Africa won, he scored only one fifty at East London where he also took 2/17 off 10 overs.
During the 1998 Test series against England, Cronje scored five consecutive fifties, having failed to score one in the nine previous Tests against them. In his fiftieth Test, at Trent Bridge he scored 126, his sixth and last Test century and his first in 29 matches. During his second innings of 67, he passed 3,000 runs – only the second South African to do so. However, England won the Test, and the one at Headingley, to win the series 2–1, Cronje finished the series as South Africa's top scorer with 401 runs at 66.83.
Whitewash, tie and forfeit.
In the West Indies series of 1998–99, Cronje captained South Africa to their only whitewash in a five Test series. His best batting against West Indies came when playing for Free State; he scored 158* as they chased down 438 and made up a first innings deficit of 249. In the ODI series he was South Africa's top scorer and took 11 wickets at 14.72 as South Africa won 6–1.
In March 1999, they toured New Zealand, beating them 1–0 in the Test series and 3–2 in the one-dayers.
At the 1999 World Cup, Cronje finished with 98 runs at 12.25 as South Africa were eliminated after the famous tied semi-final against Australia at Edgbaston. In the first match of the tournament versus India, Cronje came onto the field with an earpiece wired to coach Bob Woolmer, but at the first drinks break match referee Talat Ali ordered him to remove it.
In October 1999, Cronje became South Africa's highest Test run scorer during the first Test against Zimbabwe. The two Test series was won 2–0 thanks to innings victories. South Africa won the series with England in the fourth Test at Cape Town, Cronje's fiftieth as captain.
The fifth test of the 1999–2000 South Africa versus England series at Centurion was ruined by rain, entering the final day only 45 overs had been possible with South Africa 155/6. On the final morning as they batted on, news filtered through that the captains had met and were going to "make a game of it". A target of 250 from 70 overs was agreed. When South Africa reached 248/8, Cronje declared; both teams then forfeited an innings leaving England a target of 249 to win the Test, which they did with two wickets left and only five balls remaining. It ended South Africa's 14 game unbeaten streak in Test cricket. It was later learnt Cronje accepted money and a gift from a bookmaker in return for making an early declaration in this Test. (See below).
Cronje top scored with 56 after South Africa were left reeling at 21–5 in the Final of the triangular tournament which featured England and Zimbabwe. Cronje struggled against India in his final Test series, scoring 25 runs in two Tests and taking six wickets. South Africa still completed their first series win in India, India's first lost series at home since 1987.
On 31 March 2000, his cricket career finished with a 73-ball 79 against Pakistan in the final of Sharjah Cup 1999/2000.
Career record.
Under Cronje's captaincy, South Africa won 27 Tests and lost 11, completing series victories against every team except Australia.
He captained the One Day International team to 99 wins out of 138 matches with one tied match and three no results. He holds the South African record for matches won as captain, and his record of captaining his side in 138 matches stands bettered only by Graeme Smith's 149 matches as ODI captain. His 99 wins as captain makes him the third most successful captain worldwide in terms of matches won, behind Ricky Ponting and Allan Border, and in terms of percentage of wins (73.70), behind Ponting and Clive Lloyd.
Between September 1993 and March 2000, he played in 162 consecutive ODIs, a South African record.
Match fixing.
On 7 April 2000, Delhi police revealed they had a recording of a conversation between Cronje and Sanjay Chawla, a representative of an Indian betting syndicate, over match-fixing allegations. Three other players, Herschelle Gibbs, Nicky Boje, and Pieter Strydom, were also implicated. After an enquiry by the King Commission, Cronje was banned from playing or coaching cricket for life. He challenged his life ban in September 2001 but on 17 October 2001, his application was dismissed.
After 13 years on July 22, 2013 the Delhi Police registered an First Information Report for match-fixing in 2000, the chargesheet in the case involving a few South African cricketers including its former captain Hansie Cronje, was finally filed. The scandal was one of the biggest ever to have hit international cricket until the Pakistan cricket spot-fixing scandal.
Death.
On 1 June 2002, Cronje's scheduled flight home from Johannesburg to George was grounded so he hitched a ride as the only passenger aboard a Hawker Siddeley HS 748 turboprop aircraft. Near George airport, the pilots lost visibility in clouds and were unable to land, partly due to unusable navigational equipment. While circling, the plane crashed into the Outeniqua Mountains northeast of the airport. Cronje, aged 32, and the two pilots were killed instantly.
In August 2006, an inquest into the plane crash was opened by South Africa's High Court. The inquest concluded that "the death of the deceased Wessel Johannes Cronje was brought about by an act or omission prima facie amounting to an offence on the part of pilots."
Theories that Cronje was murdered on the orders of a cricket betting syndicate flourished after his death and were most recently re-floated by former Nottinghamshire coach Clive Rice in the wake of the death of Pakistan coach Bob Woolmer in March 2007.
Personal life.
Hansie Cronje married Bertha Hans on the 8th of April, 1995. Bertha and Hansie never had any children. Hansie's widow later married Jacques Du Plessis, a financial auditor, in 2003. It was reported that the private ceremony was attended by Hansie Cronje's parents, and his brother and sister.

</doc>
<doc id="13963" url="http://en.wikipedia.org/wiki?curid=13963" title="Hultsfred Municipality">
Hultsfred Municipality

Hultsfred Municipality ("Hultsfreds kommun") is a municipality in Kalmar County, in south-eastern Sweden. The seat is in the town of Hultsfred.
The present municipality was created in 1971 through the amalgamation of the market town ("köping") of Hultsfred (instituted in 1927) with a number of surrounding municipalities. In 1863 there were eight entities in the area.
Hultsfred is known as the site of a major rock festival in Sweden, the Hultsfred Festival.
Geography.
Basically every one of the localities of Hultsfred Municipality are situated on the railway. Besides Hultsfred, in the mid north of the municipality, there are the towns of Virserum in the south-west and other ever smaller settlements such as Lönneberga, Silverdalen and Målilla. The population of the municipality has however been decreasing with some 2,000 people in the last 10 years, as many people move to larger cities, causing a decrease in nativity.
Much of the geography is taken up with forests, a notability for the entire province of Småland, with some few scattered areas suitable for agriculture.
History.
In the age known as the Nordic Bronze Age, the area had some shipping of furs to northern Germany and the Roman army, but not much is known from that time other than the area being inhabited; there has also been older finds from 3000-4000 BC. However, from the medieval age, around 1100 AD, there still remains a few churches.
The area continued to be inhabited mainly by farmers until the 20th century. In the 17th and 18th there was some production of iron in Kalmar County, totalling about 10 mines; of those 2 were located to the municipality of Hultsfred. Hultsfred was a center for some military exercising companies during the 19th century, and some remaining building can be visited in the vicinity of Silverån. When the railroads through Sweden were built late in that century, Hultsfred received a population boost.
There are several folks museums around the area that keeps trace of its history.
Localities.
There are eight urban areas (also called a Tätort or locality) in Hultsfred Municipality.
In the table the localities are listed according to the size of the population as of December 31, 2005. The municipal seat is in bold characters.
International relations.
Twin towns — Sister cities.
Hultsfred Municipality is twinned with:

</doc>
<doc id="13964" url="http://en.wikipedia.org/wiki?curid=13964" title="Parliament of the United Kingdom">
Parliament of the United Kingdom

The Parliament of the United Kingdom of Great Britain and Northern Ireland, commonly known as the UK Parliament or the British Parliament, is the supreme legislative body in the United Kingdom, British Crown dependencies and British overseas territories. It alone possesses legislative supremacy and thereby ultimate power over all other political bodies in the UK and its territories. Its head is the Sovereign of the United Kingdom (currently Queen Elizabeth II) and its seat is the Palace of Westminster in Westminster, London.
The parliament is bicameral, consisting of an upper house (the House of Lords) and a lower house (the House of Commons). The Sovereign forms the third component of the legislature (the Queen-in-Parliament). The House of Lords includes two different types of members: the Lords Spiritual (the senior bishops of the Church of England) and the Lords Temporal (members of the Peerage) whose members are not elected by the population at large, but are appointed by the Sovereign on the advice of the Prime Minister. Prior to the opening of the Supreme Court in October 2009, the House of Lords also performed a judicial role through the Law Lords.
The House of Commons is a democratically elected chamber with elections held at least every five years. The two Houses meet in separate chambers in the Palace of Westminster (commonly known as the Houses of Parliament) in London. By constitutional convention, all government ministers, including the Prime Minister, are members of the House of Commons – or, less commonly, the House of Lords – and are thereby accountable to the respective branches of the legislature.
The Parliament of Great Britain was formed in 1707 following the ratification of the Treaty of Union by Acts of Union passed by the Parliament of England and the Parliament of Scotland. At the start of the nineteenth century, Parliament was further enlarged by Acts of Union ratified by the Parliament of Great Britain and the Parliament of Ireland that abolished the latter and added 100 Irish MPs and 32 Lords to the former to create the Parliament of the United Kingdom of Great Britain and Ireland. The Royal and Parliamentary Titles Act 1927 formally amended the name to the "Parliament of the United Kingdom of Great Britain and Northern Ireland", 5 years after the secession of the Irish Free State.
The UK parliament and its institutions have set the patterns for many democracies throughout the world, and it has been called "the mother of parliaments". However, John Bright – who coined the epithet – used it with reference to a country (England) rather than a parliament.
In theory, the UK's supreme legislative power is vested in the Crown-in-Parliament. As, however, the crown acts on the advice of the Prime Minister and the powers of the House of Lords have been curtailed, "de facto" power is vested in the House of Commons.
History.
Parliament of the United Kingdom of Great Britain and Ireland.
The United Kingdom of Great Britain and Ireland was created in 1801 by the merger of the Kingdoms of Great Britain and Ireland under the Acts of Union.
The principle of ministerial responsibility to the lower House did not develop until the 19th century—the House of Lords was superior to the House of Commons both in theory and in practice. Members of the House of Commons were elected in an antiquated electoral system, under which constituencies of vastly different sizes existed. Thus, the borough of Old Sarum, with seven voters, could elect two members, as could the borough of Dunwich, which had almost completely disappeared into the sea due to land erosion.
In many cases, members of the Upper House also controlled tiny constituencies, known as pocket or rotten boroughs, and could ensure the election of their relatives or supporters. Many seats in the House of Commons were "owned" by the Lords. After the reforms of the 19th century, beginning with the Reform Act 1832, the electoral system in the lower House was much more regularised. No longer dependent on the upper House for their seats, members of the House of Commons began to grow more assertive.
The supremacy of the British House of Commons was established in the early 20th century. In 1909, the Commons passed the so-called "People's Budget", which made numerous changes to the taxation system in a manner detrimental to wealthy landowners. The House of Lords, which consisted mostly of powerful landowners, rejected the Budget. On the basis of the Budget's popularity and the Lords' consequent unpopularity, the Liberal Party narrowly won two general elections in 1910.
Using the result as a mandate, the Liberal Prime Minister, Herbert Henry Asquith, introduced the Parliament bill, which sought to restrict the powers of the House of Lords. (He did not reintroduce the land tax provision of the People's Budget). When the Lords refused to pass the bill, Asquith countered with a promise extracted from the King in secret before the second general election of 1910 and requested the creation of several hundred Liberal peers so as to erase the Conservative majority in the House of Lords. In the face of such a threat, the House of Lords narrowly passed the bill.
The Parliament Act 1911, as it became, prevented the Lords from blocking a money bill (a bill dealing with taxation), and allowed them to delay any other bill for a maximum of three sessions (reduced to two sessions in 1949), after which it could become law over their objections. However, regardless of the Parliament Acts of 1911 and 1949, the House of Lords has always retained the unrestricted power to be able to block and veto any bill outright which attempts to extend the life of a parliament.
Parliament of the United Kingdom of Great Britain and Northern Ireland.
The Government of Ireland Act 1920 created the parliaments of Northern Ireland and Southern Ireland and reduced the representation of both parts at Westminster. (The number of Northern Ireland seats was increased again after the introduction of direct rule in 1973.) The Irish Free State became independent in 1922, and in 1927 parliament was renamed the "Parliament of the United Kingdom of Great Britain and Northern Ireland".
Further reforms to the House of Lords have been made during the 20th century. The Life Peerages Act 1958 authorised the regular creation of life peerage dignities. By the 1960s, the regular creation of hereditary peerage dignities had ceased; thereafter, almost all new peers were life peers only.
More recently, the House of Lords Act 1999 removed the automatic right of hereditary peers to sit in the Upper House (although it made an exception for 92 of them on a temporary basis, to be elected to life-terms by the other hereditary peers with by-elections upon their death). The House of Lords is now a chamber that is subordinate to the House of Commons. Additionally, the Constitutional Reform Act 2005 led to abolition of the judicial functions of the House of Lords with the creation of the new Supreme Court of the United Kingdom in October 2009.
Composition and powers.
The legislative authority, the Crown-in-Parliament, has three separate elements: the Monarch, the House of Lords, and the House of Commons. No individual may be a member of both Houses, and members of the House of Lords are legally barred from voting in elections for members of the House of Commons.
Royal Assent of the Monarch is required for all Bills to become law, and certain Delegated Legislation must be made by the Monarch by Order in Council. The Crown also has executive powers which do not depend on Parliament, through prerogative powers, which include among others the ability to make treaties, declare war, award honours, and appoint officers and civil servants. In practice these are always exercised by the monarch on the advice of the Prime Minister and the other ministers of HM Government. The Prime Minister and government are directly accountable to Parliament, through its control of public finances, and to the public, through election of Members of Parliament.
The Monarch also chooses the Prime Minister, who then forms a government from members of the houses of parliament. This must be someone who could command a majority in a confidence vote in the House of Commons. In the recent past the monarch has had to make a judgment, as in the appointment of Alec Douglas-Home in 1963 when it was thought that the incumbent Prime Minister, Harold Macmillan, had become ill with terminal cancer. However, today the monarch is advised by the outgoing Prime Minister as to whom he or she should offer the position next.
The Upper House is formally styled "The Right Honourable The Lords Spiritual and Temporal in Parliament Assembled", the Lords Spiritual being bishops of the Church of England and the Lords Temporal being Peers of the Realm. The Lords Spiritual and Lords Temporal are considered separate "estates", but they sit, debate and vote together.
Since the Parliament Acts 1911 and 1949, the powers of the House of Lords have been very much less than those of the House of Commons. All bills except money bills are debated and voted upon in House of Lords; however by voting against a bill, the House of Lords can only delay it for a maximum of two parliamentary sessions over a year. After this time, the House of Commons can force the Bill through without the Lords' consent under the Parliament Acts. The House of Lords can also hold the government to account through questions to government ministers and the operation of a small number of select committees. The highest court in England & Wales and Northern Ireland used to be a committee of the House of Lords, but it became an independent supreme court in 2009.
The Lords Spiritual formerly included all of the senior clergymen of the Church of England—archbishops, bishops, abbots and mitred priors. Upon the Dissolution of the Monasteries under Henry VIII the abbots and mitred priors lost their positions in Parliament. All diocesan bishops continued to sit in Parliament, but the Bishopric of Manchester Act 1847, and later acts, provide that only the 26 most senior are Lords Spiritual. These always include the incumbents of the "five great sees", namely the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham and the Bishop of Winchester. The remaining 21 Lords Spiritual are the most senior diocesan bishops, ranked in order of consecration, although the Lords Spiritual (Women) Act 2015 makes time-limited provision for vacancies to be filled by women bishops.
The Lords Temporal are all members of the Peerage. Formerly, they were hereditary peers. The right of some hereditary peers to sit in Parliament was not automatic: after Scotland and England united into Great Britain in 1707, it was provided that all peers whose dignities had been created by English Kings could sit in Parliament, but those whose dignities had been created by Scottish Kings were to elect a limited number of "representative peers". A similar arrangement was made in respect of Ireland when it was united with Great Britain in 1801, but when southern Ireland left the United Kingdom in 1922 the election of Irish representative peers ceased. By the Peerage Act 1963, the election of Scottish representative peers also ended, and all Scottish peers were granted the right to sit in Parliament. Under the House of Lords Act 1999, only life peerages (that is to say, peerage dignities which cannot be inherited) automatically entitle their holders to seats in the House of Lords. Of the hereditary peers, only 92—the Earl Marshal, the Lord Great Chamberlain and the 90 elected by other peers—retain their seats in the House.
The Commons, the last of the "estates" of the Kingdom, are represented in the House of Commons, which is formally styled "The Honourable The Commons in Parliament Assembled" ("commons" coming not from the term "commoner", but from "commune", the old French term for a district). The House currently consists of 650 members. Each "Member of Parliament" or "MP" is chosen by a single constituency according to the First-Past-the-Post electoral system. Universal adult suffrage exists for those 18 and over; citizens of the United Kingdom, and those of the Republic of Ireland and Commonwealth nations resident in the United Kingdom are qualified to vote, unless they are in prison at the time of the elections. The term of members of the House of Commons depends on the term of Parliament, a maximum of five years; a general election, during which all the seats are contested, occurs after each dissolution (see below).
All legislation must be passed by the House of Commons to become law and it controls taxation and the supply of money to the government. Government ministers (including the Prime Minister) must regularly answer questions in the House of Commons and there are a number of select committees that scrutinise particular issues and the workings of the government. There are also mechanisms that allow members of the House of Commons to bring to the attention of the government particular issues affecting their constituents.
State Opening.
The State Opening of Parliament is an annual event that marks the commencement of a session of the Parliament of the United Kingdom. It is held in the House of Lords Chamber, and took place in November or December, or in a general election year, when the new Parliament first assembled. From 2012 onwards, the ceremony takes place in May or June.
Motioned by the Monarch, the Lord Great Chamberlain raises his wand of office to signal to Black Rod, who is charged with summoning the House of Commons and has been waiting in the Commons lobby. Black Rod turns and, under the escort of the Door-keeper of the House of Lords and an inspector of police approaches the doors to the Chamber of the Commons. In 1642, King Charles I stormed into the House of Commons in an unsuccessful attempt to arrest the Five Members, which included the celebrated English patriot and leading Parliamentarian John Hampden, sparking the English Civil War. The wars established the constitutional rights of parliament, a concept legally established as part of the Glorious Revolution in 1688 and the subsequent Bill of Rights 1689. Since that time, no British monarch has entered the House of Commons when it is sitting [meeting]. On Black Rod's approach, the doors are slammed shut against him, symbolising the rights of parliament and it's independence from the monarch. He then strikes with the end of his ceremonial staff (the Black Rod) three times on the closed doors of the Commons Chamber, and is admitted, and announces the command of the monarch for the attendance of the Commons.
The monarch reads a speech, known as the Speech from the Throne, which is prepared by the Prime Minister and his cabinet members, outlining the Government's agenda for the coming year. The speech reflects the legislative agenda for which the Cabinet seeks the agreement of both Houses of Parliament.
After the monarch leaves, each Chamber proceeds to the consideration of an "Address in Reply to Her Majesty's Gracious Speech." But first, each House considers a bill "pro forma" to symbolise their right to deliberate independently of the monarch. In the House of Lords, the bill is called the "Select Vestries Bill", while the Commons equivalent is the "Outlawries Bill". The Bills are considered for the sake of form only, and do not make any actual progress.
Procedure.
Both houses of the British Parliament are presided over by a speaker, the Speaker of the House for the Commons and the Lord Speaker in the House of Lords.
For the Commons, the approval of the Sovereign is theoretically required before the election of the Speaker becomes valid, but it is, by modern convention, always granted. The Speaker's place may be taken by three deputies, known as the Chairman, First Deputy Chairman and Second Deputy Chairman of Ways and Means. (They take their name from the Committee of Ways and Means, of which they were once presiding officers, but which no longer exists.)
Prior to July 2006, the House of Lords was presided over by a Lord Chancellor (a Cabinet member), whose influence as Speaker was very limited (whilst the powers belonging to the Speaker of the House of Commons are vast). However, as part of the Constitutional Reform Act 2005, the position of Speaker of the House of Lords (as it is termed in the Act) was separated from the office of Lord Chancellor (the office which has control over the judiciary as a whole), though the Lords remain largely self-governing. Decisions on points of order and on the disciplining of unruly members are made by the whole body in the Upper House, but by the Speaker alone in the Lower House. Speeches in the House of Lords are addressed to the House as a whole (using the words "My Lords"), but those in the House of Commons are addressed to the Speaker alone (using "Mr Speaker" or "Madam Speaker"). Speeches may be made to both Houses simultaneously.
Both Houses may decide questions by voice vote; members shout out "Aye" and "No" in the Commons—or "Content" and "Not-Content" in the Lords—and the presiding officer declares the result. The pronouncement of either Speaker may be challenged, and a recorded vote (known as a division) demanded. (The Speaker of the House of Commons may choose to overrule a frivolous request for a division, but the Lord Speaker does not have that power). In each House, a division requires members to file into one of the two lobbies alongside the Chamber; their names are recorded by clerks, and their votes are counted as they exit the lobbies to re-enter the Chamber. The Speaker of the House of Commons is expected to be non-partisan, and does not cast a vote except in the case of a tie; the Lord Speaker, however, votes along with the other Lords.
Both Houses normally conduct their business in public, and there are galleries where visitors may sit.
Term.
Currently, Parliament has a fixed term of 5 years.
Originally there was no fixed limit on the length of a Parliament, but the Triennial Act 1694 set the maximum duration at three years. As the frequent elections were deemed inconvenient, the Septennial Act 1715 extended the maximum to seven years, but the Parliament Act 1911 reduced it to five. During the Second World War, the term was temporarily extended to ten years by Acts of Parliament. Since the end of the war the maximum has remained five years. Modern Parliaments, however, rarely continued for the maximum duration; normally, they were dissolved earlier. For instance, the 52nd, which assembled in 1997, was dissolved after four years. The Septennial Act was repealed by the Fixed-term Parliaments Act 2011.
Summary History of Terms of the Parliament of the United Kingdom 
Following a general election, a new Parliamentary session begins. Parliament is formally summoned 40 days in advance by the Sovereign, who is the source of parliamentary authority. On the day indicated by the Sovereign's proclamation, the two Houses assemble in their respective chambers. The Commons are then summoned to the House of Lords, where Lords Commissioners (representatives of the Sovereign) instruct them to elect a Speaker. The Commons perform the election; on the next day, they return to the House of Lords, where the Lords Commissioners confirm the election and grant the new Speaker the royal approval in the Sovereign's name.
The business of Parliament for the next few days of its session involves the taking of the oaths of allegiance. Once a majority of the members have taken the oath in each House, the State Opening of Parliament may occur. The Lords take their seats in the House of Lords Chamber, the Commons appear at the Bar (immediately outside the Chamber), and the Sovereign takes his or her seat on the throne. The Sovereign then reads the Speech from the Throne—the content of which is determined by the Ministers of the Crown—outlining the Government's legislative agenda for the upcoming year. Thereafter, each House proceeds to the transaction of legislative business.
By custom, before considering the Government's legislative agenda, a bill is introduced "pro forma" in each House—the Select Vestries Bill in the House of Lords and the Outlawries Bill in the House of Commons. These bills do not become laws; they are ceremonial indications of the power of each House to debate independently of the Crown. After the "pro forma" bill is introduced, each House debates the content of the Speech from the Throne for several days. Once each House formally sends its reply to the Speech, legislative business may commence, appointing committees, electing officers, passing resolutions and considering legislation.
A session of Parliament is brought to an end by a prorogation. There is a ceremony similar to the State Opening, but much less well-known. Normally, the Sovereign does not personally attend the prorogation ceremony in the House of Lords; he or she is represented by Lords Commissioners. The next session of Parliament begins under the procedures described above, but it is not necessary to conduct another election of a Speaker or take the oaths of allegiance afresh at the beginning of such subsequent sessions. Instead, the State Opening of Parliament proceeds directly. To avoid the delay of opening a new session in the event of an emergency during the long summer recess, Parliament is no longer prorogued beforehand, but only after the Houses have reconvened in the autumn; the State Opening follows a few days later.
Each Parliament comes to an end, after a number of sessions, in anticipation of a general election. Parliament is dissolved by virtue of the Fixed-term Parliaments Act 2011. Prior to it, dissolution was effected by the Sovereign, always on the advice of the Prime Minister. The Prime Minister could seek dissolution because the time was politically advantageous to his or her party. If the Prime Minister loses the support of the House of Commons, Parliament will dissolve and a new election will be held. Parliaments can also be dissolved if two thirds of the House of Commons votes for an early election.
Formerly, the demise of the Sovereign automatically brought a Parliament to an end, the Crown being seen as the "caput, principium, et finis" (beginning, basis and end) of the body, but this is no longer the case. The first change was during the reign of William and Mary, when it was seen to be inconvenient to have no Parliament at a time when succession to the Crown could be disputed, and an act was passed that provided that a Parliament was to continue for six months after the death of a Sovereign, unless dissolved earlier. Under the Representation of the People Act 1867 Parliament can now continue for as long as it would otherwise have done in the event of the death of the Sovereign.
After each Parliament concludes, the Crown issues writs to hold a general election and elect new members of the House of Commons though membership of the House of Lords does not change due to dissolution.
Legislative functions.
Laws can be made by Acts of the United Kingdom Parliament. While Acts can apply to the whole of the United Kingdom including Scotland, due to the continuing separation of Scots law many Acts do not apply to Scotland and are either matched by equivalent Acts that apply to Scotland alone or, since 1999, by legislation set by the Scottish Parliament relating to devolved matters.
This has led to a paradox known as the West Lothian question. The existence of a devolved Scottish Parliament means that while Westminster MPs from Scotland may vote directly on matters that affect English constituencies, they may not have much power over their laws affecting their own constituency. While any Act of the Scottish Parliament may be overturned, amended or ignored by Westminster, in practice this has yet to happen. Furthermore, the existence of the Legislative Consent Motion enables English MPs to vote on issues nominally devolved to Scotland, as part of United Kingdom legislation. Since there is no devolved "English Parliament", the converse is not true.
Laws, in draft form known as bills, may be introduced by any member of either House, but usually a bill is introduced by a Minister of the Crown. A bill introduced by a Minister is known as a "Government Bill"; one introduced by another member is called a "Private Member's Bill". A different way of categorising bills involves the subject. Most bills, involving the general public, are called "Public Bills". A bill that seeks to grant special rights to an individual or small group of individuals, or a body such as a local authority, is called a "Private Bill". A Public Bill which affects private rights (in the way a Private Bill would) is called a "Hybrid Bill".
Private Members' Bills make up the majority of bills, but are far less likely to be passed than government bills. There are three methods for an MP to introduce a Private Member's Bill. The Private Members' Ballot (once per Session) put names into a ballot, and those who win are given time to propose a bill. The Ten Minute Rule is another method, where MPs are granted ten minutes to outline the case for a new piece of legislation. Standing Order 57 is the third method, which allows a bill to be introduced without debate if a day's notice is given to the Table Office. Filibustering is a danger, as an opponent to a bill can waste much of the limited time allotted to it. Private Members' Bills have no chance of success if the current government opposes them, but they are used in moral issues: the bills to decriminalise homosexuality and abortion were Private Members' Bills, for example. Governments can sometimes attempt to use Private Members' Bills to pass things it would rather not be associated with. "Handout bills" are bills which a government hands to MPs who win Private Members' Ballots.
Each Bill goes through several stages in each House. The first stage, called the first reading, is a formality. At the second reading, the general principles of the bill are debated, and the House may vote to reject the bill, by not passing the motion "That the Bill be now read a second time". Defeats of Government Bills are extremely rare, the last being in 2005.
Following the second reading, the bill is sent to a committee. In the House of Lords, the Committee of the Whole House or the Grand Committee are used. Each consists of all members of the House; the latter operates under special procedures, and is used only for uncontroversial bills. In the House of Commons, the bill is usually committed to a Public Bill Committee, consisting of between 16 and 50 members, but the Committee of the Whole House is used for important legislation. Several other types of committees, including Select Committees, may be used, but rarely. A committee considers the bill clause by clause, and reports the bill as amended to the House, where further detailed consideration ("consideration stage" or "report stage") occurs. However, a practice which used to be called the "kangaroo" (Standing Order 32) allows the Speaker to select which amendments are debated. This device is also used under Standing Order 89 by the committee chairman, to restrict debate in committee.
Once the House has considered the bill, the third reading follows. In the House of Commons, no further amendments may be made, and the passage of the motion "That the Bill be now read a third time" is passage of the whole bill. In the House of Lords further amendments to the bill may be moved. After the passage of the third reading motion, the House of Lords must vote on the motion "That the Bill do now pass." Following its passage in one House, the bill is sent to the other House. If passed in identical form by both Houses, it may be presented for the Sovereign's Assent. If one House passes amendments that the other will not agree to, and the two Houses cannot resolve their disagreements, the bill fails.
However, since the passage of the Parliament Act 1911 the power of the House of Lords to reject bills passed by the House of Commons has been restricted, and further restrictions were placed by the Parliament Act 1949. If the House of Commons passes a public bill in two successive sessions, and the House of Lords rejects it both times, the Commons may direct that the bill be presented to the Sovereign for his or her Assent, disregarding the rejection of the Bill in the House of Lords. In each case, the bill must be passed by the House of Commons at least one calendar month before the end of the session. The provision does not apply to bills originated in the House of Lords, to bills seeking to extend the duration of a Parliament beyond five years, or to Private Bills. A special procedure applies in relation to bills classified by the Speaker of the House of Commons as "Money Bills". A Money Bill concerns "solely" national taxation or public funds; the Speaker's certificate is deemed conclusive under all circumstances. If the House of Lords fails to pass a Money Bill within one month of its passage in the House of Commons, the Lower House may direct that the Bill be submitted for the Sovereign's Assent immediately.
Even before the passage of the Parliament Acts, the Commons possessed pre-eminence in cases of financial matters. By ancient custom, the House of Lords may not introduce a bill relating to taxation or Supply, nor amend a bill so as to insert a provision relating to taxation or Supply, nor amend a Supply Bill in any way. The House of Commons is free to waive this privilege, and sometimes does so to allow the House of Lords to pass amendments with financial implications. The House of Lords remains free to reject bills relating to Supply and taxation, but may be overruled easily if the bills are Money Bills. (A bill relating to revenue and Supply may not be a Money Bill if, for example, it includes subjects other than national taxation and public funds).
The last stage of a bill involves the granting of the Royal Assent. Theoretically, the Sovereign may either grant the Royal Assent (that is, make the bill a law) or withhold it (that is, veto the bill). Under modern conventions the Sovereign always grants the Royal Assent, in the Norman French words "La Reyne le veult" (the Queen wishes it; "Le roy" instead in the case of a king). The last refusal to grant the Assent was in 1708, when Queen Anne withheld her Assent from a bill "for the settling of Militia in Scotland", in the words "La reyne s'avisera" (the Queen will think it over).
Thus, every bill obtains the assent of all three components of Parliament before it becomes law (except where the House of Lords is over-ridden under the Parliament Acts 1911 and 1949). The words "BE IT ENACTED by the Queen's [King's] most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows:-", or, where the House of Lords' authority has been overridden by use of the Parliament Acts, the words "BE IT ENACTED by The Queen's [King's] most Excellent Majesty, by and with the advice and consent of the Commons in this present Parliament assembled, in accordance with the provisions of the Parliament Acts 1911 and 1949, and by the authority of the same, as follows:-" appear near the beginning of each Act of Parliament. These words are known as the enacting formula.
Judicial functions.
Prior to the creation of the Supreme Court of the United Kingdom in October 2009, Parliament also used to perform several judicial functions. The Queen-in-Parliament constituted the highest court in the realm for most purposes, but the Privy Council had jurisdiction in some cases (for instance, appeals from ecclesiastical courts). The jurisdiction of Parliament arose from the ancient custom of petitioning the Houses to redress grievances and to do justice. The House of Commons ceased considering petitions to reverse the judgements of lower courts in 1399, effectively leaving the House of Lords as the court of last resort. In modern times, the judicial functions of the House of Lords were performed not by the whole House, but by a group of "Lords of Appeal in Ordinary" (judges granted life peerage dignities under the Appellate Jurisdiction Act 1876 by the Sovereign) and by "Lords of Appeal" (other peers with experience in the judiciary). However, under the Constitutional Reform Act 2005, these judicial functions were transferred to the newly created Supreme Court in 2009, and the Lords of Appeal in Ordinary became the first Justices of the Court. Peers who hold high judicial office are no longer allowed to vote or speak in the Lords until they retire as Justices.
In the late 19th century, Acts allowed for the appointment of "Scottish Lords of Appeal in Ordinary" and ended appeal in Scottish criminal matters to the House of Lords, so that the High Court of Justiciary became the highest criminal court in Scotland. There is an argument that the provisions of Article XIX of the Union with England Act 1707 prevent any Court outside Scotland from hearing any appeal in criminal cases: "And that the said Courts or any other of the like nature after the Unions shall have no power to Cognosce Review or Alter the Acts or Sentences of the Judicatures within Scotland or stop the Execution of the same." Nowadays the House of Lords legislative committee usually has a minimum of two Scottish Judges to ensure that some experience of Scots law is brought to bear on Scottish appeals in civil cases, from the Court of Session.
Certain other judicial functions have historically been performed by the House of Lords. Until 1948, it was the body in which peers had to be tried for felonies or high treason; now, they are tried by normal juries. When the House of Commons impeaches an individual, the trial takes place in the House of Lords. Impeachments are now rare; the last one occurred in 1806. In 2006, a number of MPs attempted to revive the custom, having signed a motion for the impeachment of Tony Blair, but this was unsuccessful.
Relationship with the Government.
The British Government is answerable to the House of Commons. However, neither the Prime Minister nor members of the Government are elected by the House of Commons. Instead, the Queen requests the person most likely to command the support of a majority in the House, normally the leader of the largest party in the House of Commons, to form a government. So that they may be accountable to the Lower House, the Prime Minister and most members of the Cabinet are, by convention, members of the House of Commons. The last Prime Minister to be a member of the House of Lords was Alec Douglas-Home, 14th Earl of Home, who became Prime Minister in 1963. To adhere to the convention under which he was responsible to the Lower House, he disclaimed his peerage and procured election to the House of Commons within days of becoming Prime Minister.
Governments have a tendency to dominate the legislative functions of Parliament, by using their in-built majority in the House of Commons, and sometimes using their patronage power to appoint supportive peers in the Lords. Formerly, no-one could be a member of Parliament while holding an Office of profit under the Crown, thus maintaining the separation of powers, but the principle has been gradually eroded. Until 1919, Members of Parliament who were appointed to ministerial office lost their right to sit in the House of Commons and had to seek re-election. The rule survives in the House of Commons Disqualification Act 1975 which specifies a number of state positions that make an individual ineligible to serve as a Member of Parliament. The only vestige of the principle is the process of resignation from the House of Commons.
In practice, governments can pass any legislation (within reason) in the Commons they wish, unless there is major dissent by MPs in the governing party.
But even in these situations, it is highly unlikely a bill will be defeated, though dissenting MPs may be able to extract concessions from the government. In 1976, Lord Hailsham created a now widely used name for this behaviour, in an academic paper called "elective dictatorship".
Parliament controls the executive by passing or rejecting its Bills and by forcing Ministers of the Crown to answer for their actions, either at "Question Time" or during meetings of the parliamentary committees. In both cases, Ministers are asked questions by members of their Houses, and are obliged to answer.
Although the House of Lords may scrutinise the executive through Question Time and through its committees, it cannot bring down the Government. A ministry must always retain the confidence and support of the House of Commons. The Lower House may indicate its lack of support by rejecting a Motion of Confidence or by passing a Motion of No Confidence. Confidence Motions are generally originated by the Government in order to reinforce its support in the House, whilst No Confidence Motions are introduced by the Opposition. The motions sometimes take the form "That this House has [no] confidence in Her Majesty's Government" but several other varieties, many referring to specific policies supported or opposed by Parliament, are used. For instance, a Confidence Motion of 1992 used the form, "That this House expresses the support for the economic policy of Her Majesty's Government." Such a motion may theoretically be introduced in the House of Lords, but, as the Government need not enjoy the confidence of that House, would not be of the same effect as a similar motion in the House of Commons; the only modern instance of such an occurrence involves the 'No Confidence' motion that was introduced in 1993 and subsequently defeated.
Many votes are considered votes of confidence, although not including the language mentioned above. Important bills that form part of the Government's agenda (as stated in the Speech from the Throne) are generally considered matters of confidence. The defeat of such a bill by the House of Commons indicates that a Government no longer has the confidence of that House. The same effect is achieved if the House of Commons "withdraws Supply", that is, rejects the budget.
Where a Government has lost the confidence of the House of Commons, the Prime Minister is obliged either to resign, or seek the dissolution of Parliament and a new general election. Where a Prime Minister has ceased to retain a majority in that vote and requests a dissolution, the Sovereign can in theory reject his request, forcing his resignation and allowing the Leader of the Opposition to be asked to form a new government. This power is used extremely rarely. The conditions that should be met to allow such a refusal are known as the Lascelles Principles. These conditions and principles are constitutional conventions arising from the Sovereign's reserve powers as well as longstanding tradition and practice, not laid down in law.
In practice, the House of Commons' scrutiny of the Government is very weak. Since the first-past-the-post electoral system is employed in elections, the governing party tends to enjoy a large majority in the Commons; there is often limited need to compromise with other parties. Modern British political parties are so tightly organised that they leave relatively little room for free action by their MPs. In many cases, MPs may be expelled from their parties for voting against the instructions of party leaders. During the 20th century, the Government has lost confidence issues only three times—twice in 1924, and once in 1979.
Parliamentary Questions.
In the United Kingdom, question time in the House of Commons lasts for an hour each day from Monday to Thursday (2:30 to 3:30pm on Mondays, 11:30am to 12:30pm on Tuesdays and Wednesdays, and 9:30 to 10:30am on Thursdays). Each Government department has its place in a rota which repeats every five weeks. The exception to this sequence are the Business Questions (Questions to the Leader of House of Commons), in which questions are answered each Thursday about the business of the House the following week. Also, Questions to the Prime Minister takes place each Wednesday from noon to 12:30pm.
In addition to government departments, there are also questions to the Church commissioners. Additionally, each Member of Parliament is entitled to table questions for written answer. Written questions are addressed to the Ministerial head of a government department, usually a Secretary of State, but they are often answered by a Minister of State or Parliamentary Under Secretary of State. Written Questions are submitted to the Clerks of the Table Office, either on paper or electronically, and answers are recorded in "The Official Report (Hansard)" so as to be widely available and accessible.
In the House of Lords, a half hour is set aside each afternoon at the start of the day's proceedings for Lords' oral questions. A peer submits a question in advance, which then appears on the Order Paper for the day's proceedings. The Lord shall say: "My Lords, I beg leave to ask the Question standing in my name on the Order Paper". The Minister responsible then answers the question. The Lord is then allowed to ask a supplementary question and other peers ask further questions on the theme of the original put down on the order paper. (For instance, if the question regards immigration, Lords can ask the Minister any question related to immigration during the allowed period).
Sovereignty.
Several different views have been taken of Parliament's sovereignty. According to the jurist Sir William Blackstone, "It has sovereign and uncontrollable authority in making, confirming, enlarging, restraining, abrogating, repealing, reviving, and expounding of laws, concerning matters of all possible denominations, ecclesiastical, or temporal, civil, military, maritime, or criminal ... it can, in short, do every thing that is not naturally impossible."
A different view has been taken by the Scottish judge Lord Cooper of Culross. When he decided the 1953 case of "MacCormick v. Lord Advocate" as Lord President of the Court of Session, he stated, "The principle of unlimited sovereignty of Parliament is a distinctively English principle and has no counterpart in Scottish constitutional law." He continued, "Considering that the Union legislation extinguished the Parliaments of Scotland and England and replaced them by a new Parliament, I have difficulty in seeing why the new Parliament of Great Britain must inherit all the peculiar characteristics of the English Parliament but none of the Scottish." Nevertheless, he did not give a conclusive opinion on the subject.
Thus, the question of Parliamentary sovereignty appears to remain unresolved. Parliament has not passed any Act defining its own sovereignty. A related possible limitation on Parliament relates to the Scottish legal system and Presbyterian faith, preservation of which were Scottish preconditions to the creation of the unified Parliament. Since the Parliament of the United Kingdom was set up in reliance on these promises, it may be that it has no power to make laws that break them.
Parliament's power has often been eroded by its own Acts. Acts passed in 1921 and 1925 granted the Church of Scotland complete independence in ecclesiastical matters. More recently, its power has been restricted by membership of the European Union, which has the power to make laws enforceable in each member state. In the Factortame case, the European Court of Justice ruled that British courts could have powers to overturn British legislation contravening European law.
Parliament has also created national devolved parliaments and assemblies with differing degrees of legislative authority in Scotland, Wales and Northern Ireland. Parliament still has the power over areas for which responsibility lies with the devolved institutions, but would gain the agreement of those institutions to act on their behalf. Similarly, it has granted the power to make regulations to Ministers of the Crown, and the power to enact religious legislation to the General Synod of the Church of England. (Measures of the General Synod and, in some cases proposed statutory instruments made by ministers, must be approved by both Houses before they become law.)
In every case aforementioned, authority has been conceded by Act of Parliament and may be taken back in the same manner. It is entirely within the authority of Parliament, for example, to abolish the devolved governments in Scotland, Wales and Northern Ireland or to leave the EU. However, Parliament also revoked its legislative competence over Australia and Canada with the Australia and Canada Acts: although the Parliament of the United Kingdom could pass an Act reversing its action, it would not take effect in Australia or Canada as the competence of the Imperial Parliament is no longer recognised there in law.
One well-recognised exception to Parliament's power involves binding future Parliaments. No Act of Parliament may be made secure from amendment or repeal by a future Parliament. For example, although the Act of Union 1800 states that the Kingdoms of Great Britain and Ireland are to be united "forever", Parliament permitted southern Ireland to leave the United Kingdom in 1922.
Privileges.
Each House of Parliament possesses and guards various ancient privileges. The House of Lords relies on inherent right. In the case of the House of Commons, the Speaker goes to the Lords' Chamber at the beginning of each new Parliament and requests representatives of the Sovereign to confirm the Lower House's "undoubted" privileges and rights. The ceremony observed by the House of Commons dates to the reign of King Henry VIII. Each House is the guardian of its privileges, and may punish breaches thereof. The extent of parliamentary privilege is based on law and custom. Sir William Blackstone states that these privileges are "very large and indefinite", and cannot be defined except by the Houses of Parliament themselves.
The foremost privilege claimed by both Houses is that of freedom of speech in debate; nothing said in either House may be questioned in any court or other institution outside Parliament. Another privilege claimed is that of freedom from arrest; at one time this was held to apply for any arrest except for high treason, felony or breach of the peace but it now excludes any arrest on criminal charges; it applies during a session of Parliament, and 40 days before or after such a session. Members of both Houses are no longer privileged from service on juries.
Both Houses possess the power to punish breaches of their privilege. Contempt of Parliament—for example, disobedience of a subpoena issued by a committee—may also be punished. The House of Lords may imprison an individual for any fixed period of time, but an individual imprisoned by the House of Commons is set free upon prorogation. The punishments imposed by either House may not be challenged in any court, and the Human Rights Act does not apply.
Emblem.
The quasi-official emblem of the Houses of Parliament is a crowned portcullis. The portcullis was originally the badge of various English noble families from the 14th century. It went on to be adopted by the kings of the Tudor dynasty in the 16th century, under whom the Palace of Westminster became the regular meeting place of Parliament. The crown was added to make the badge a specifically royal symbol.
The portcullis probably first came to be associated with the Palace of Westminster through its use as decoration in the rebuilding of the Palace after the fire of 1512. However, at the time it was only one of many symbols. The widespread use of the portcullis throughout the Palace dates from the 19th century, when Charles Barry and Augustus Pugin used it extensively as a decorative feature in their designs for the new Palace built following the disastrous 1834 fire.
The crowned portcullis came to be accepted during the 20th century as the emblem of both houses of parliament. This was simply a result of custom and usage rather than a specific decision. The emblem now appears on official stationery, publications and papers, and is stamped on various items in use in the Palace of Westminster, such as cutlery, silverware and china. Various shades of red and green are used for visual identification of the House of Lords and the House of Commons
References.
Bibliography
External links.
Listen to this article (2 parts) · 
This audio file was created from a revision of the "Parliament of the United Kingdom" article dated 2006-05-20, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="13966" url="http://en.wikipedia.org/wiki?curid=13966" title="Hosea">
Hosea

Hosea ( or ; Hebrew: הוֹשֵׁעַ,  "Hoshea",  "Hôšēăʻ" ; "Salvation"; Greek Ὠσηέ, "Hōsēe") was the son of Beeri, a prophet in Israel in the 8th century BC and author of the book of prophecies bearing his name. He is one of the Twelve Prophets of the Jewish Hebrew Bible, also known as the Minor Prophets of the Christian Old Testament. Hosea is often seen as a "prophet of doom", but underneath his message of destruction is a promise of restoration. The Talmud ("Pesachim" 87a) claims that he was the greatest prophet of his generation. The period of Hosea's ministry extended to some sixty years and he was the only prophet of Israel who left any written prophecy.
Name.
The name "Hosea", meaning "salvation", or "He saves", or "He helps", seems to have been not uncommon, being derived from the auspicious verb from which we have the frequently recurring word "salvation". It may be a contraction of a larger form of which the divine name (Yahweh) or its abbreviation formed a part, so as to signify "Yahweh helps". According to , that was the original name of Joshua son of Nun, until Moses gave him the longer name (compounded with the divine name) which he continued to bear (yehoshua`), "Yahweh is salvation".
Location.
Although it is not expressly stated in the Book of Hosea, it is apparent from the level of detail and familiarity focused on northern geography, that Hosea conducted his prophetic ministries in the Northern Kingdom, of which he was a native.
Family.
Little is known about the life or social status of Hosea. According to the Book of Hosea, he married the prostitute Gomer, the daughter of Diblaim, at God's command. In ff., there is a reference to the wars which led to the capture of the kingdom by the Assyrians (ca. 734–732 BC). It is not certain if he had also experienced the destruction of Samaria, which is foreseen in .
Hosea's family life reflected the "adulterous" relationship which Israel had built with polytheistic gods. The relationship between Hosea and Gomer parallels the relationship between God and Israel. Even though Gomer runs away from Hosea and sleeps with another man, he loves her anyway and forgives her. Likewise, even though the people of Israel worshipped false gods, God continued to love them and did not abandon his covenant with them.
Similarly, his children's names made them like walking prophecies of the fall of the ruling dynasty and the severed covenant with God – much like the prophet Isaiah a generation later. The name of Hosea's daughter, Lo-ruhamah, which translates as "not pitied", is chosen by God as a sign of displeasure with the people of Israel for following false gods. (In Hosea 2:23 she is redeemed, shown mercy with the term "Ruhamah".) The name of Hosea's son, Lo-ammi, which translates as "not my people", is chosen by the Lord as a sign of the Lord's displeasure with the people of Israel for following those false gods (see Hosea 1:8-9).
Christian thought.
One of the early writing prophets, Hosea used his own experience as a symbolic representation of God and Israel: God the husband, Israel the wife. Hosea's wife left him to go with other men; Israel left the Lord to go with false gods. Hosea searched for his wife, found her and brought her back; God would not abandon Israel and brought them back even though they had forsaken him.
The book of Hosea was a severe warning to the northern kingdom against the growing idolatry being practiced there; the book was a dramatic call to repentance. Christians extend the analogy of Hosea to Christ and the church: Christ the husband, his church the bride. Christians see in this book a comparable call to the church not to forsake the Lord Jesus Christ. Christians also take the buying back of Gomer as the redemptive qualities of Jesus Christ's sacrifice on the cross.
Other preachers, like Charles Spurgeon, saw Hosea as a striking presentation of the mercy of God in his sermon on Hosea 1:7 titled The LORD's Own Salvation. “But I will have mercy upon the house of Judah, and will save them by the Lord their God, and will not save them by bow, nor by sword, nor by battle, by horses, nor by horsemen.” – Hosea 1:7 in his sermon NO. 2057, December 16TH, 1888.
Islamic literature.
The Qur'an mentions only some prophets by name, but makes it clear that many were sent who are not mentioned. Therefore, many Muslim scholars, both classical (Ibn Ishaq) and modern (Reza Aslan), speak of Hosea as one of the true Hebrew prophets of Israel. The Book of Hosea has also been used in Qur'anic exegesis by Abdullah Yusuf Ali, especially in reference to Qur'anic verses which speak of the backsliding of Israel.
Observances.
He is commemorated with the other Minor prophets in the Calendar of saints of the Armenian Apostolic Church on July 31. He is commemorated on the Eastern Orthodox liturgical calendar, with a feast day on October 17 (for those churches which follow the Julian Calendar, October 17 currently falls on October 30 of the modern Gregorian Calendar). He is also commemorated on the Sunday of the Holy Fathers (the Sunday before the Nativity of the Lord).
Tomb of Hosea.
The tomb of Hosea is a structure located in the Jewish cemetery of Safed, believed to be the final resting place of Hosea.

</doc>
<doc id="13967" url="http://en.wikipedia.org/wiki?curid=13967" title="Habakkuk">
Habakkuk

Habakkuk ( or ; Hebrew: חֲבַקּוּק‎; also spelled Habacuc), was a prophet in the Hebrew Bible. He is the author of the Book of Habakkuk, the eighth of the collected twelve minor prophets.
Life.
Almost nothing is known about Habakkuk, aside from what few facts are stated within the book of the Bible bearing his name, or those inferences that may be drawn from that book. His name appears in the Bible only in Habakkuk 1:1 and 3:1, with no biographical details provided other than his title "the prophet." Even the origin of his name is uncertain.
For almost every other prophet, more information is given, such as the name of the prophet's hometown, his occupation, or information concerning his parentage or tribe. For Habakkuk, however, there is no reliable account of any of these. Although his home is not identified, scholars conclude that Habakkuk lived in Jerusalem at the time he wrote his prophecy. Further analysis has provided an approximate date for his prophecy and possibilities concerning his activities and background.
Beyond the Bible, considerable conjecture has been put forward over the centuries in the form of Christian and Rabbinic tradition, but such accounts are dismissed by modern scholars as speculative and apocryphal.
Biblical account.
Because the book of Habakkuk consists of five oracles about the Chaldeans (Babylonians), and the Chaldean rise to power is dated circa 612 BC, it is assumed he was active about that time, making him an early contemporary of Jeremiah and Zephaniah. Jewish sources, however, do not group him with those two prophets, who are often placed together, so it is possible that he was slightly earlier than they.
Because the final chapter of his book is a song, it is sometimes assumed that he was a member of the tribe of Levi, which served as musicians in Solomon's Temple.
Name.
The name Habakkuk, or Habacuc, appears in the Hebrew Bible only in Habakkuk 1:1 and 3:1. In the Masoretic Text, it is written in Hebrew: חֲבַקּוּק‎ ( "Ḥavaqquq" "Ḥăḇaqqûq"). This name does not occur elsewhere. The Septuagint transcribes his name into Greek as Ἀμβακοὺμ ("Ambakoum"), and the Vulgate transcribes it into Latin as "Abacuc".
The etymology of the name is not clear, and its form has no parallel in Hebrew. The name is possibly related to the Akkadian "khabbaququ", the name of a fragrant plant, or the Hebrew root #redirect , meaning "embrace".
Tradition.
Habakkuk appears in Bel and the Dragon, which is part of the Additions to Daniel found in the Biblical apocrypha. Verses 33–39 state that Habakkuk is in Judea and after making some stew, he's told by the angel of the Lord to take the stew to Daniel, who is in Babylon in the lion's den. After proclaiming he is unaware of both the den and Babylon, the angel transports Habakkuk to the lion's den. Habakkuk gives Daniel the food to sustain him, and is immediately taken back to "his own place".
Habakkuk is also mentioned in Lives of the Prophets, which also notes his time in Babylon.
According to the Zohar (Volume 1, page 8b) Habakkuk is the boy born to the Shunamite woman through Elisha's blessing:
() And he said, About this season, according to the time of life, thou shalt embrace (חבקת – hoveket, therefore Habakkuk) a son. And she said, Nay, my lord, [thou] man of God, do not lie unto thine handmaid.
Works.
The only work attributed to Habakkuk is the short book of the Bible that bears his name. The book of Habbakuk consists of five oracles about the Chaldeans (Babylonians) and a song of praise to God.
The style of the book has been praised by many scholars, suggesting that its author was a man of great literary talent. The entire book follows the structure of a chiasmus in which parallelism of thought is used to bracket sections of the text.
Habakkuk is unique among the prophets in that he openly questions the working of God (1:3a, 1:13b). In the first part of the second chapter, the Prophet sees the injustice among his people and asks why God does not take action: "1:2 Yahweh, how long will I cry, and you will not hear? I cry out to you 'Violence!' and will you not save?" (World English Bible).
Tombs.
The final resting place of Habakkuk has been claimed at multiple locations. The fifth-century Christian historian Sozomen claimed that the relics of Habakkuk were found at Cela, when God revealed their location to Zebennus, bishop of Eleutheropolis, in a dream. Currently, one location in Israel and one in Iran lay claim to being the burial site of the prophet.
Tomb in Israel.
The burial place of Habakkuk is identified by Jewish tradition as a hillside in the Upper Galilee region of northern Israel, close to the villages Kadarim and Hukok, about six miles southwest of Safed and twelve miles north of Mount Tabor. A small stone building, erected during the 20th century, protects the tomb. Tradition dating as early as the 12th century AD holds that Habakkuk's tomb is at this location, but the tomb may also be of a local sheikh of Yaquq, a name related to the biblical place named "Hukkok" (mentioned in ), whose pronunciation and spelling in Hebrew are close to "Habakkuk". Archaeological findings in this location include several burial places dated to the Second Temple period.
Persian shrine.
A mausoleum southeast of the city of Toyserkan in the west of Iran is also believed to be Habakkuk's burial place. It is protected by Iran's Cultural Heritage Organization. The Organization's guide to the Hamedan Province states that Habakkuk was believed to be a guardian to the Temple of Solomon, and that he was captured by the Babylonians and remained in their prison for some years. After being freed by Cyrus the Great, he went to Ecbatana and remained there until he died, and was buried somewhere nearby, in what is today Toyserkan. Habakkuk is called both Habaghugh and Hayaghugh by the locals.
The surrounding shrine may date to the period of the Great Seljuq Empire (11–12th century); it consists of an octagonal wall and conical dome. Underneath the shrine is a hidden basement with three floors. In the center of the shrine's courtyard is the grave where Habakkuk is said to be buried. A stone upon the grave is inscribed in both Hebrew and Persian stating that the prophet's father was Shioua Lovit, and his mother was Lesho Namit.
Christian commemoration.
On the Eastern Orthodox liturgical calendar, his feast day is December 2. In the Roman Catholic Church, the twelve minor prophets are read in the Roman Breviary during the fourth and fifth weeks of November, which are the last two weeks of the liturgical year, and his feast day is January 15. This day is also celebrated as his feast by the Greek Orthodox Church. In 2011, he was commemorated with the other Minor prophets in the Calendar of saints of the Armenian Apostolic Church on February 8.
Habakkuk has also been commemorated in sculpture. In 1435, the Florentine artist Donatello created a sculpture of the prophet for the bell tower of Florence. This statue, nicknamed "Il Zuccone" ("The Big Pumpkin") because of the shape of the head, now resides in the Museo dell'Opera del Duomo. The church of Santa Maria del Popolo in Rome contains a Baroque sculpture of Habakkuk by the 17th-century artist Bernini. Between 1800 and 1805, the Brazilian sculptor Aleijadinho completed a soapstone sculpture of Habakkuk as part of his "Twelve Prophets". The figures are arranged around the forecourt and monumental stairway in front of the "Santuário do Bom Jesus do Matosinhos" at Congonhas.
References.
</dl>

</doc>
<doc id="13968" url="http://en.wikipedia.org/wiki?curid=13968" title="Haggai">
Haggai

Haggai (; Hebrew: חַגַּי‎, "Ḥaggay" or "Hag-i", Koine Greek: Ἀγγαῖος; Latin: "Aggaeus") was a Hebrew prophet during the building of the Second Temple in Jerusalem, and one of the twelve minor prophets in the Hebrew Bible and the author of the Book of Haggai. His name means "my holiday". He was the first of three post-exile prophets from the Neo-Babylonian Exile of the House of Judah (with Zechariah, his contemporary, and Malachi, who lived about one hundred years later), who belonged to the period of Jewish history which began after the return from captivity in Babylon.
Scarcely anything is known of his personal history. He may have been one of the captives taken to Babylon by Nebuchadnezzar. He began his ministry about sixteen years after the return of the Jews to Judah (ca. 520 BC). The work of rebuilding the temple had been put to a stop through the intrigues of the Samaritans. After having been suspended for eighteen years, the work was resumed through the efforts of Haggai and Zechariah. They exhorted the people, which roused them from their lethargy, and induced them to take advantage of a change in the policy of the Persian government under Darius the Great.
The name Haggai, with various vocalizations, is also found in the Book of Esther, as a eunuch servant of the Queen.
Haggai and officials of his time.
Haggai supported the officials of his time, specifically Zerubbabel, the governor, and Joshua the High Priest. In the Book of Haggai, God refers to Zerubbabel as "my servant" as King David was, and says he will make him as a "signet ring," as King Jehoiachin was (Haggai 2:23; cf. Jer 22:24). The signet ring symbolized a ring worn on the hand of Yahweh, showing that a king held divine favour. Thus, Haggai is implicitly, but not explicitly, saying that Zerubbabel would preside over a restored Davidic kingdom.
Liturgical commemoration.
On the Eastern Orthodox liturgical calendar, Haggai is commemorated as a saint and prophet. His feast day is December 16 (for those churches which follow the traditional Julian Calendar, December 16 currently falls on December 29 of the modern Gregorian Calendar). He is also commemorated, in common with the other righteous persons of the Old Testament, on the Sunday of the Holy Fathers (the Sunday before the Nativity of the Lord).
Haggai is commemorated with the other Minor prophets in the Calendar of saints of the Armenian Apostolic Church on July 31.
Haggai in Freemasonry.
In the Masonic degree of Holy Royal Arch Haggai is one of the Three Principals of the Chapter. Named after Haggai the prophet and is supported by Zerubbabel, Prince of the People, and Joshua, the son of Josedech, the High Priest.

</doc>
<doc id="13969" url="http://en.wikipedia.org/wiki?curid=13969" title="Herman Hollerith">
Herman Hollerith

Herman Hollerith (February 29, 1860 – November 17, 1929) was an American statistician and inventor who developed a mechanical tabulator based on punched cards to rapidly tabulate statistics from millions of pieces of data. He was the founder of the "Tabulating Machine Company" that later merged to become IBM. Hollerith is widely regarded as the father of modern machine data processing. His invention of the punched card evaluating machine marks the beginning of the era of automatic data processing systems, and his concept dominated the computing landscape for nearly a century.
Personal life.
Herman Hollerith was born the son of German immigrant Prof. Georg Hollerith from Großfischlingen (near Neustadt an der Weinstraße) in Buffalo, New York, where he spent his early childhood. He entered the City College of New York in 1875 and graduated from the Columbia University School of Mines with an "Engineer of Mines" degree in 1879, at age 19. In 1880 he listed himself as a mining engineer while living in Manhattan, and completed his Ph.D. in 1890 at Columbia University. In 1882 Hollerith joined the Massachusetts Institute of Technology where he taught mechanical engineering and conducted his first experiments with punched cards. He eventually moved to Washington, D.C., living in Georgetown, with a home on 29th Street and a factory for manufacturing his tabulating machines at 31st Street and the C&O Canal, where today there is a commemorative plaque installed by IBM. He died in Washington D.C. of a heart attack.
Electrical tabulation of data.
At the urging of John Shaw Billings, Hollerith developed a mechanism using electrical connections to trigger a counter, recording information. A key idea was that data could be encoded by the locations of holes in a card. Hollerith determined that data punched in specified locations on a card, in the now-familiar rows and columns, could be counted or sorted mechanically. A description of this system, "An Electric Tabulating System (1889)", was submitted by Hollerith to Columbia University as his doctoral thesis, and is reprinted in Randell's book. On January 8, 1889, Hollerith was issued U.S. Patent 395,782, claim 2 of which reads:
The herein-described method of compiling statistics, which consists in recording separate statistical items pertaining to the individual by holes or combinations of holes punched in sheets of electrically non-conducting material, and bearing a specific relation to each other and to a standard, and then counting or tallying such statistical items separately or in combination by means of mechanical counters operated by electro-magnets the circuits through which are controlled by the perforated sheets, substantially as and for the purpose set forth.
Inventions and businesses.
Hollerith had left teaching and begun working for the United States Census Bureau in the year he filed his first patent application. Titled "Art of Compiling Statistics", it was filed on September 23, 1884; U.S. Patent 395,782 was granted on January 8, 1889.
Hollerith built machines under contract for the Census Office, which used them to tabulate the 1890 census in only one year. The previous 1880 census had taken eight years. In 1896 Hollerith started his own business when he founded the "Tabulating Machine Company". Many major census bureaus around the world leased his equipment and purchased his cards, as did major insurance companies. Hollerith's machines were used for censuses in England, Italy, Germany, Russia, Austria, Canada, France, Norway, Puerto Rico, Cuba, and the Philippines, and again in the 1900 census. To make his system work, he invented the first automatic card-feed mechanism and the first keypunch (that is, a punch operated by a keyboard); a skilled operator could punch 200–300 cards per hour. He also invented a tabulator. The 1890 Tabulator was hardwired to operate only on 1890 Census cards. A plugboard control panel in his 1906 Type I Tabulator allowed it to do different jobs without being rebuilt (the first step towards programming). These inventions were among the foundations of the modern information processing industry and Hollerith's punchcards (though later adapted to encode computer programs) continued in use for almost a century.
In 1911 four corporations, including Hollerith's firm, merged to form the Computing Tabulating Recording Company (CTR). Under the presidency of Thomas J. Watson, it was renamed International Business Machines Corporation (IBM) in 1924.
Death and legacy.
Hollerith is buried at Oak Hill Cemetery in the Georgetown neighborhood of Washington, D.C., as is his son, Herman Hollerith Jr.
Hollerith cards were named after the elder Herman Hollerith, as were Hollerith constants (also sometimes called Hollerith strings), an early type of string constant declaration (in computer programming).
His great-grandson, the Rt. Rev. Herman Hollerith IV is the Episcopal bishop of the Diocese of Southern Virginia, and another great-grandson, Randolph Marshall Hollerith, is an Episcopal priest in Richmond, Virginia.

</doc>
<doc id="13971" url="http://en.wikipedia.org/wiki?curid=13971" title="History of painting">
History of painting

The history of painting reaches back in time to artifacts from pre-historic humans, and spans all cultures. It represents a continuous, though periodically disrupted tradition from Antiquity. Across cultures, and spanning continents and millennia, the history of painting is an ongoing river of creativity, that continues into the 21st century. Until the early 20th century it relied primarily on representational, religious and classical motifs, after which time more purely abstract and conceptual approaches gained favor.
Developments in Eastern painting historically parallel those in Western painting, in general, a few centuries earlier. African art, Jewish art, Islamic art, Indian art, Chinese art, and Japanese art each had significant influence on Western art, and, eventually, vice versa.
Initially serving utilitarian purpose, followed by imperial, private, civic, and religious patronage, Eastern and Western painting later found audiences in the aristocracy and the middle class. From the Modern era, the Middle Ages through the Renaissance painters worked for the church and a wealthy aristocracy. Beginning with the Baroque era artists received private commissions from a more educated and prosperous middle class. Finally in the West the idea of "art for art's sake" began to find expression in the work of the Romantic painters like Francisco de Goya, John Constable, and J.M.W. Turner. During the 19th century the rise of the commercial art gallery provided patronage in the 20th century.
Pre-history.
The oldest known paintings are approximately 40,000 years old. José Luis Sanchidrián at the University of Cordoba, Spain, believes the paintings are more likely to have been painted by Neanderthals than early modern humans. The Grotte Chauvet in France is claimed by some historians to be about 32,000 years old. They are engraved and painted using red ochre and black pigment and show horses, rhinoceros, lions, buffalo, mammoth or humans often hunting. There are examples of cave paintings all over the world—in France, India, Spain, Portugal, China, Australia etc. Various conjectures have been made as to the meaning these paintings had to the people that made them. Prehistoric men may have painted animals to "catch" their soul or spirit in order to hunt them more easily or the paintings may represent an animistic vision and homage to surrounding nature, or they may be the result of a basic need of expression that is innate to human beings, or they could have been for the transmission of practical information.
In Paleolithic times, the representation of humans in cave paintings was rare. Mostly, animals were painted, not only animals that were used as food but also animals that represented strength like the rhinoceros or large Felidae, as in the Chauvet Cave. Signs like dots were sometimes drawn. Rare human representations include handprints and half-human / animal figures. The Chauvet Cave in the Ardèche Departments of France contains the most important preserved cave paintings of the Paleolithic era, painted around 31,000 BC. The Altamira cave paintings in Spain were done 14,000 to 12,000 BC and show, among others, bisons. The hall of bulls in Lascaux, Dordogne, France, is one of the best known cave paintings from about 15,000 to 10,000 BC.
If there is meaning to the paintings, it remains unknown. The caves were not in an inhabited area, so they may have been used for seasonal rituals. The animals are accompanied by signs which suggest a possible magic use. Arrow-like symbols in Lascaux are sometimes interpreted as calendar or almanac use. But the evidence remains inconclusive. The most important work of the Mesolithic era were the "marching warriors", a rock painting at Cingle de la Mola, Castellón, Spain dated to about 7000 to 4000 BC. The technique used was probably spitting or blowing the pigments onto the rock. The paintings are quite naturalistic, though stylized. The figures are not three-dimensional, even though they overlap
The earliest known Indian paintings (see section below) were the rock paintings of prehistoric times, the petroglyphs as found in places like the Rock Shelters of Bhimbetka, (see above) and some of them are older than 5500 BC. Such works continued and after several millennia, in the 7th century, carved pillars of Ajanta, Maharashtra state present a fine example of Indian paintings, and the colors, mostly various shades of red and orange, were derived from minerals.
Eastern painting.
The history of Eastern painting includes a vast range of influences from various cultures and religions. Developments in Eastern painting historically parallel those in Western painting, in general a few centuries earlier. African art, Jewish art, Islamic art, Indian art, Chinese art, Korean Art, and Japanese art each had significant influence on Western art, and, vice versa.
Chinese painting is one of the oldest continuous artistic traditions in the world. The earliest paintings were not representational but ornamental; they consisted of patterns or designs rather than pictures. Early pottery was painted with spirals, zigzags, dots, or animals. It was only during the Warring States period (403–221 B.C.) that artists began to represent the world around them. Japanese painting is one of the oldest and most highly refined of the Japanese arts, encompassing a wide variety of genre and styles. The history of Japanese painting is a long history of synthesis and competition between native Japanese aesthetics and adaptation of imported ideas. The history of Korean painting is dated to approximately 108 C.E., when it first appears as an independent form. Between that time and the paintings and frescoes that appear on the Goryeo dynasty tombs, there has been little research. Suffice to say that until the Joseon dynasty the primary influence was Chinese painting though done with Korean landscapes, facial features, Buddhist topics, and an emphasis on celestial observation in keeping with the rapid development of Korean astronomy.
East Asian painting.
"See also Chinese painting, Japanese painting, Korean painting."
China, Japan and Korea have a strong tradition in painting which is also highly attached to the art of calligraphy and printmaking (so much that it is commonly seen as painting). Far east traditional painting is characterized by water based techniques, less realism, "elegant" and stylized subjects, graphical approach to depiction, the importance of white space (or negative space) and a preference for landscape (instead of human figure) as a subject. Beyond ink and color on silk or paper scrolls, gold on lacquer was also a common medium in painted East Asian artwork. Although silk was a somewhat expensive medium to paint upon in the past, the invention of paper during the 1st century AD by the Han court eunuch Cai Lun provided not only a cheap and widespread medium for writing, but also a cheap and widespread medium for painting (making it more accessible to the public).
The ideologies of Confucianism, Daoism, and Buddhism played important roles in East Asian art. Medieval Song Dynasty painters such as Lin Tinggui and his "Luohan Laundering" (housed in the Smithsonian Freer Gallery of Art) of the 12th century are excellent examples of Buddhist ideas fused into classical Chinese artwork. In the latter painting on silk (image and description provided in the link), bald-headed Buddhist Luohan are depicted in a practical setting of washing clothes by a river. However, the painting itself is visually stunning, with the Luohan portrayed in rich detail and bright, opaque colors in contrast to a hazy, brown, and bland wooded environment. Also, the tree tops are shrouded in swirling fog, providing the common "negative space" mentioned above in East Asian Art.
In Japonisme, late-19th-century artists like the Impressionists, Van Gogh, Henri de Toulouse-Lautrec and Whistler admired traditional Japanese Ukiyo-e artists like Hokusai and Hiroshige and their work was influenced by it.
Panorama of "Along the River During Qing Ming Festival", 18th-century remake of 12th-century Song Dynasty original by Chinese artist Zhang Zeduan. The original painting by Zhang is revered by scholars as "one of Chinese civilization's greatest masterpieces." Note: scroll starts from the right.
Chinese painting.
The earliest (surviving) examples of Chinese painted artwork date to the Warring States Period (481 – 221 BC), with paintings on silk or tomb murals on rock, brick, or stone. They were often in simplistic stylized format and in more-or-less rudimentary geometric patterns. They often depicted mythological creatures, domestic scenes, labor scenes, or palatial scenes filled with officials at court. Artwork during this period and the subsequent Qin Dynasty (221 – 207 BC) and Han Dynasty (202 BC – 220 AD) was made not as a means in and of itself or for higher personal expression. Rather artwork was created to symbolize and honor funerary rites, representations of mythological deities or spirits of ancestors, etc. Paintings on silk of court officials and domestic scenes could be found during the Han Dynasty, along with scenes of men hunting on horseback or partaking in military parade. There was also painting on three dimensional works of art on figurines and statues, such as the original-painted colors covering the soldier and horse statues of the Terracotta Army. During the social and cultural climate of the ancient Eastern Jin Dynasty (316 – 420 AD) based at Nanjing in the south, painting became one of the official pastimes of Confucian-taught bureaucratic officials and aristocrats (along with music played by the guqin zither, writing fanciful calligraphy, and writing and reciting of poetry). Painting became a common form of artistic self-expression, and during this period painters at court or amongst elite social circuits were judged and ranked by their peers.
The establishment of classical Chinese landscape painting is accredited largely to the Eastern Jin Dynasty artist Gu Kaizhi (344 – 406 AD), one of the most famous artists of Chinese history. Like the elongated scroll scenes of Kaizhi, Tang Dynasty (618 – 907 AD) Chinese artists like Wu Daozi painted vivid and highly detailed artwork on long horizontal handscrolls (which were very popular during the Tang), such as his "Eighty Seven Celestial People". Painted artwork during the Tang period pertained the effects of an idealized landscape environment, with sparse amount of objects, persons, or activity, as well as monochromatic in nature (example: the murals of Price Yide's tomb in the Qianling Mausoleum). There were also figures such as early Tang-era painter Zhan Ziqian, who painted superb landscape paintings that were well ahead of his day in portrayal of realism. However, landscape art did not reach greater level of maturity and realism in general until the Five Dynasties and Ten Kingdoms period (907 – 960 AD). During this time, there were exceptional landscape painters like Dong Yuan (refer to this article for an example of his artwork), and those who painted more vivid and realistic depictions of domestic scenes, like Gu Hongzhong and his "Night Revels of Han Xizai".
During the Chinese Song Dynasty (960 – 1279 AD), not only landscape art was improved upon, but portrait painting became more standardized and sophisticated than before (for example, refer to Emperor Huizong of Song), and reached its classical age maturity during the Ming Dynasty (1368 – 1644 AD). During the late 13th century and first half of the 14th century, Chinese under the Mongol-controlled Yuan Dynasty were not allowed to enter higher posts of government (reserved for Mongols or other ethnic groups from Central Asia), and the Imperial examination was ceased for the time being. Many Confucian-educated Chinese who now lacked profession turned to the arts of painting and theatre instead, as the Yuan period became one of the most vibrant and abundant eras for Chinese artwork. An example of such would be Qian Xuan (1235–1305 AD), who was an official of the Song Dynasty, but out of patriotism, refused to serve the Yuan court and dedicated himself to painting. Examples of superb art from this period include the rich and detailed painted murals of the Yongle Palace , or "Dachunyang Longevity Palace", of 1262 AD, a UNESCO World Heritage site. Within the palace, paintings cover an area of more than 1000 square meters, and hold mostly Daoist themes. It was during the Song Dynasty that painters would also gather in social clubs or meetings to discuss their art or others' artwork, the praising of which often led to persuasions to trade and sell precious works of art. However, there were also many harsh critics of others art as well, showing the difference in style and taste amongst different painters. In 1088 AD, the polymath scientist and statesman Shen Kuo once wrote of the artwork of one Li Cheng, who he criticized as follows:
...Then there was Li Cheng, who when he depicted pavilions and lodges amidst mountains, storeyed buildings, pagodas and the like, always used to paint the eaves as seen from below. His idea was that 'one should look upwards from underneath, just as a man standing on level ground and looking up at the eaves of a pagoda can see its rafters and its cantilever eave rafters'. This is all wrong. In general the proper way of painting a landscape is to see the small from the viewpoint of the large...just as one looks at artificial mountains in gardens (as one walks about). If one applies (Li's method) to the painting of real mountains, looking up at them from below, one can only see one profile at a time, and not the wealth of their multitudinous slopes and profiles, to say nothing of all that is going on in the valleys and canyons, and in the lanes and courtyards with their dwellings and houses. If we stand to the east of a mountain its western parts would be on the vanishing boundary of far-off distance, and vice versa. Surely this could not be called a successful painting? Mr. Li did not understand the principle of 'seeing the small from the viewpoint of the large'. He was certainly marvelous at diminishing accurately heights and distances, but should one attach such importance to the angles and corners of buildings?
Although high level of stylization, mystical appeal, and surreal elegance were often preferred over realism (such as in shan shui style), beginning with the medieval Song Dynasty there were many Chinese painters then and afterwards who depicted scenes of nature that were vividly real. Later Ming Dynasty artists would take after this Song Dynasty emphasis for intricate detail and realism on objects in nature, especially in depictions of animals (such as ducks, swans, sparrows, tigers, etc.) amongst patches of brightly colored flowers and thickets of brush and wood (a good example would be the anonymous Ming Dynasty painting "Birds and Plum Blossoms", housed in the Freer Gallery of the Smithsonian Museum in Washington, D.C.). There were many renowned Ming Dynasty artists; Qiu Ying is an excellent example of a paramount Ming era painter (famous even in his own day), utilizing in his artwork domestic scenes, bustling palatial scenes, and nature scenes of river valleys and steeped mountains shrouded in mist and swirling clouds. During the Ming Dynasty there were also different and rivaling schools of art associated with painting, such as the Wu School and the Zhe School.
Classical Chinese painting continued on into the early modern Qing Dynasty, with highly realistic portrait paintings like seen in the late Ming Dynasty of the early 17th century. The portraits of Kangxi Emperor, Yongzheng Emperor, and Qianlong Emperor are excellent examples of realistic Chinese portrait painting. During the Qianlong reign period and the continuing 19th century, European Baroque styles of painting had noticeable influence on Chinese portrait paintings, especially with painted visual effects of lighting and shading. Likewise, East Asian paintings and other works of art (such as porcelain and lacquerware) were highly prized in Europe since initial contact in the 16th century.
Japanese painting.
Japanese painting (絵画) is one of the oldest and most highly refined of the Japanese arts, encompassing a wide variety on genre and styles. As with the history of Japanese arts in general, the history Japanese painting is a long history of synthesis and competition between native Japanese aesthetics and adaptation of imported ideas. Ukiyo-e, "pictures of the floating world", is a genre of Japanese woodblock prints (or woodcuts) and paintings produced between the 17th and the 20th centuries, featuring motifs of landscapes, the theatre and pleasure quarters. It is the main artistic genre of woodblock printing in Japan. Japanese printmaking especially from the Edo period exerted enormous influence on Western painting in France during the 19th century.
Indian painting.
Indian paintings historically revolved around the religious deities and kings. Indian art is a collective term for several different schools of art that existed in the Indian subcontinent. The paintings varied from large frescoes of Ajanta to the intricate Mughal miniature paintings to the metal embellished works from the Tanjore school. The paintings from the Gandhar–Taxila are influenced by the Persian works in the west. The eastern style of painting was mostly developed around the Nalanda school of art. The works are mostly inspired by various scenes from Indian mythology.
History.
The earliest Indian paintings were the rock paintings of prehistoric times, the petroglyphs as found in places like the Rock Shelters of Bhimbetka, and some of them are older than 5500 BC. Such works continued and after several millennia, in the 7th century, carved pillars of Ajanta, Maharashtra state present a fine example of Indian paintings, and the colors, mostly various shades of red and orange, were derived from minerals.
Ajanta Caves in Maharashtra, India are rock-cut cave monuments dating back to the 2nd century BCE and containing paintings and sculpture considered to be masterpieces of both Buddhist religious art and universal pictorial art.
Madhubani painting is a style of Indian painting, practiced in the Mithila region of Bihar state, India. The origins of Madhubani painting are shrouded in antiquity.
Rajput painting.
Rajput painting, a style of Indian painting, evolved and flourished, during the 18th century, in the royal courts of Rajputana, India. Each Rajput kingdom evolved a distinct style, but with certain common features. Rajput paintings depict a number of themes, events of epics like the Ramayana and the Mahabharata, Krishna's life, beautiful landscapes, and humans. Miniatures were the preferred medium of Rajput painting, but several manuscripts also contain Rajput paintings, and paintings were even done on the walls of palaces, inner chambers of the forts, havelies, particularly, the havelis of Shekhawait.
The colors extracted from certain minerals, plant sources, conch shells, and were even derived by processing precious stones, gold and silver were used. The preparation of desired colors was a lengthy process, sometimes taking weeks. Brushes used were very fine.
Mughal painting.
Mughal painting is a particular style of Indian painting, generally confined to illustrations on the book and done in miniatures, and which emerged, developed and took shape during the period of the Mughal Empire 16th −19th centuries.
Tanjore painting.
Tanjore painting is an important form of classical South Indian painting native to the town of Tanjore in Tamil Nadu. The art form dates back to the early 9th century, a period dominated by the Chola rulers, who encouraged art and literature. These paintings are known for their elegance, rich colors, and attention to detail. The themes for most of these paintings are Hindu Gods and Goddesses and scenes from Hindu mythology. In modern times, these paintings have become a much sought after souvenir during festive occasions in South India.
The process of making a Tanjore painting involves many stages. The first stage involves the making of the preliminary sketch of the image on the base. The base consists of a cloth pasted over a wooden base. Then chalk powder or zinc oxide is mixed with water-soluble adhesive and applied on the base. To make the base smoother, a mild abrasive is sometimes used. After the drawing is made, decoration of the jewellery and the apparels in the image is done with semi-precious stones. Laces or threads are also used to decorate the jewellery. On top of this, the gold foils are pasted. Finally, dyes are used to add colors to the figures in the paintings.
The Madras School.
During British rule in India, the crown found that Madras had some of the most talented and intellectual artistic minds in the world. As the British had also established a huge settlement in and around Madras, Georgetown was chosen to establish an institute that would cater to the artistic expectations of the royals in London. This has come to be known as the Madras School. At first traditional artists were employed to produce exquisite varieties of furniture, metal work, and curios and their work was sent to the royal palaces of the Queen.
Unlike the Bengal School where 'copying' is the norm of teaching, the Madras School flourishes on 'creating' new styles, arguments and trends.
The Bengal School.
The Bengal school of art was an influential style of art that flourished in India during the British Raj in the early 20th century. It was associated with Indian nationalism, but was also promoted and supported by many British arts administrators.
The Bengal School arose as an avant garde and nationalist movement reacting against the academic art styles previously promoted in India, both by Indian artists such as Raja Ravi Varma and in British art schools. Following the widespread influence of Indian spiritual ideas in the West, the British art teacher Ernest Binfield Havel attempted to reform the teaching methods at the Calcutta School of Art by encouraging students to imitate Mughal miniatures. This caused immense controversy, leading to a strike by students and complaints from the local press, including from nationalists who considered it to be a retrogressive move. Havel was supported by the artist Abanindranath Tagore, a nephew of the poet Rabindranath Tagore. Tagore painted a number of works influenced by Mughal art, a style that he and Havel believed to be expressive of India's distinct spiritual qualities, as opposed to the "materialism" of the West. Tagore's best-known painting, "Bharat Mata" (Mother India), depicted a young woman, portrayed with four arms in the manner of Hindu deities, holding objects symbolic of India's national aspirations. Tagore later attempted to develop links with Japanese artists as part of an aspiration to construct a pan-Asianist model of art.
The Bengal School's influence in India declined with the spread of modernist ideas in the 1920s. In the post-independence period, Indian artists showed more adaptability as they borrowed freely from european styles and amalgamated them freely with the Indian motifs to new forms of art. While artists like Francis Newton Souza and Tyeb Mehta were more western in their approach, there were others like Ganesh Pyne and Maqbool Fida Husain who developed thoroughly indigenous styles of work. Today after the process of liberalization of market in India, the artists are experiencing more exposure to the international art-scene which is helping them in emerging with newer forms of art which were hitherto not seen in India. Jitish Kallat had shot to fame in the late 1990s with his paintings which were both modern and beyond the scope of generic definition. However, while artists in India in the new century are trying out new styles, themes and metaphors, it would not have been possible to get such quick recognition without the aid of the business houses which are now entering the art field like they had never before.
Modern Indian painting.
Amrita Sher-Gil was an Indian painter, sometimes known as India's Frida Kahlo, and today considered an important woman painter of 20th-century India, whose legacy stands at par with that of the Masters of Bengal Renaissance; she is also the 'most expensive' woman painter of India.
Today, she is amongst "Nine Masters", whose work was declared as "art treasures" by The Archaeological Survey of India, in 1976 and 1979, and over 100 of her paintings are now displayed at National Gallery of Modern Art, New Delhi.
During the colonial era, Western influences started to make an impact on Indian art. Some artists developed a style that used Western ideas of composition, perspective and realism to illustrate Indian themes. Others, like Jamini Roy, consciously drew inspiration from folk art.
By the time of Independence in 1947, several schools of art in India provided access to modern techniques and ideas. Galleries were established to showcase these artists. Modern Indian art typically shows the influence of Western styles, but is often inspired by Indian themes and images. Major artists are beginning to gain international recognition, initially among the Indian diaspora, but also among non-Indian audiences.
The Progressive Artists' Group, established shortly after India became independent in 1947, was intended to establish new ways of expressing India in the post-colonial era. The founders were six eminent artists – K. H. Ara, S. K. Bakre, H. A. Gade, M.F. Husain, S.H. Raza and F. N. Souza, though the group was dissolved in 1956, it was profoundly influential in changing the idiom of Indian art. Almost all India's major artists in the 1950s were associated with the group. Some of those who are well-known today are Bal Chabda, Manishi Dey, Mukul Dey, V. S. Gaitonde, Ram Kumar, Tyeb Mehta, and Akbar Padamsee. Other famous painters like Jahar Dasgupta, Prokash Karmakar, John Wilkins, Narayanan Ramachandran, and Bijon Choudhuri enriched the art culture of India. They have become the icons of modern Indian art. Art historians like Prof. Rai Anand Krishna have also referred to those works of modern artistes that reflect Indian ethos. Geeta Vadhera has had acclaim in translating complex, Indian spiritual themes onto canvas like Sufi thought, the Upanishads and the Bhagwad Geeta.
Indian art got a boost with the economic liberalization of the country since the early 1990s. Artists from various fields now started bringing in varied styles of work. In post-liberalization India, many artists have established themselves in the international art market like the abstract painter Natvar Bhavsar, figurative artist Devajyoti Ray and sculptor Anish Kapoor whose mammoth postminimalist artworks have acquired attention for their sheer size. Many art houses and galleries have also opened in USA and Europe to showcase Indian artworks.
Filipino Painting.
Filipino painting as a whole can be seen as an amalgamation of many cultural influences, though it tends to be more Western in its current form with Eastern roots.
Early Filipino painting can be found in red slip (clay mixed with water) designs embellished on the ritual pottery of the Philippines such as the acclaimed Manunggul Jar. Evidence of Philippine pottery-making dated as early as 6000 BC has been found in Sanga-sanga Cave, Sulu and Laurente Cave, Cagayan. It has been proven that by 5000 BC, the making of pottery was practiced throughout the country. Early Filipinos started making pottery before their Cambodian neighbors and at about the same time as the Thais as part of what appears to be a widespread Ice Age development of pottery technology. Further evidences of painting are manifested in the tattoo tradition of early Filipinos, whom the Portuguese explorer referred to as "Pintados" or the 'Painted People' of the Visayas. Various designs referencing flora and fauna with heavenly bodies decorate their bodies in various colored pigmentation. Perhaps, some of the most elaborate painting done by early Filipinos that survive to the present day can be manifested among the arts and architecture of the Maranao who are well known for the Nāga Dragons and the Sarimanok carved and painted in the beautiful Panolong of their Torogan or King's House.
Filipinos began creating paintings in the European tradition during the 17th-century Spanish period. The earliest of these paintings were Church frescoes, religious imagery from Biblical sources, as well as engravings, sculptures and lithographs featuring Christian icons and European nobility. Most of the paintings and sculptures between the 19th, and 20th century produced a mixture of religious, political, and landscape art works, with qualities of sweetness, dark, and light. Early modernist painters such as Damián Domingo was associated with religious and secular paintings. The art of Juan Luna and Félix Hidalgo showed a trend for political statement. Artist such as Fernando Amorsolo used post-modernism to produce paintings that illustrated Philippine culture, nature, and harmony. While other artists such as Fernando Zóbel used realities and abstract on his work.
Western painting.
Egypt, Greece and Rome.
Ancient Egypt, a civilization with very strong traditions of architecture and sculpture (both originally painted in bright colours) also had many mural paintings in temples and buildings, and painted illustrations on papyrus manuscripts. Egyptian wall painting and decorative painting is often graphic, sometimes more symbolic than realistic. Egyptian painting depicts figures in bold outline and flat silhouette, in which symmetry is a constant characteristic. Egyptian painting has close connection with its written language – called Egyptian hieroglyphs. Painted symbols are found amongst the first forms of written language. The Egyptians also painted on linen, remnants of which survive today. Ancient Egyptian paintings survived due to the extremely dry climate. The ancient Egyptians created paintings to make the afterlife of the deceased a pleasant place. The themes included journey through the afterworld or their protective deities introducing the deceased to the gods of the underworld. Some examples of such paintings are paintings of the gods and goddesses Ra, Horus, Anubis, Nut, Osiris and Isis. Some tomb paintings show activities that the deceased were involved in when they were alive and wished to carry on doing for eternity. In the New Kingdom and later, the Book of the Dead was buried with the entombed person. It was considered important for an introduction to the afterlife.
To the north of Egypt was the Minoan civilization on the island of Crete. The wall paintings found in the palace of Knossos are similar to that of the Egyptians but much more free in style. Around 1100 B.C., tribes from the north of Greece conquered Greece and the Greek art took a new direction.
Ancient Greece had great painters, great sculptors (though both endeavours were regarded as mere manual labour at the time), and great architects. The Parthenon is an example of their architecture that has lasted to modern days. Greek marble sculpture is often described as the highest form of Classical art. Painting on pottery of Ancient Greece and ceramics gives a particularly informative glimpse into the way society in Ancient Greece functioned. Black-figure vase painting and Red-figure vase painting gives many surviving examples of what Greek painting was. Some famous Greek painters on wooden panels who are mentioned in texts are Apelles, Zeuxis and Parrhasius, however no examples of Ancient Greek panel painting survive, only written descriptions by their contemporaries or later Romans. Zeuxis lived in 5–6 BC and was said to be the first to use sfumato. According to Pliny the Elder, the realism of his paintings was such that birds tried to eat the painted grapes. Apelles is described as the greatest painter of Antiquity for perfect technique in drawing, brilliant color and modeling.
Roman art was influenced by Greece and can in part be taken as a descendant of ancient Greek painting. However, Roman painting does have important unique characteristics. The only surviving Roman paintings are wall paintings, many from villas in Campania, in Southern Italy. Such painting can be grouped into 4 main "styles" or periods and may contain the first examples of trompe-l'œil, pseudo-perspective, and pure landscape. Almost the only painted portraits surviving from the Ancient world are a large number of coffin-portraits of bust form found in the Late Antique cemetery of Al-Fayum. Although these were neither of the best period nor the highest quality, they are impressive in themselves, and give an idea of the quality that the finest ancient work must have had. A very small number of miniatures from Late Antique illustrated books also survive, and a rather larger number of copies of them from the Early Medieval period.
Middle Ages.
The rise of Christianity imparted a different spirit and aim to painting styles. Byzantine art, once its style was established by the 6th century, placed great emphasis on retaining traditional iconography and style, and has changed relatively little through the thousand years of the Byzantine Empire and the continuing traditions of Greek and Russian Orthodox icon-painting. Byzantine painting has a particularly hieratic feeling and icons were and still are seen as a reflection of the divine. There were also many wall-paintings in fresco, but fewer of these have survived than Byzantine mosaics. In general Byzantium art borders on abstraction, in its flatness and highly stylised depictions of figures and landscape. However, there are periods, especially in the so-called Macedonian art of around the 10th century, when Byzantine art became more flexible in approach.
In post-Antique Catholic Europe the first distinctive artistic style to emerge that included painting was the Insular art of the British Isles, where the only surviving examples (and quite likely the only medium in which painting was used) are miniatures in Illuminated manuscripts such as the Book of Kells. These are most famous for their abstract decoration, although figures, and sometimes scenes, were also depicted, especially in Evangelist portraits. Carolingian and Ottonian art also survives mostly in manuscripts, although some wall-painting remain, and more are documented. The art of this period combines Insular and "barbarian" influences with a strong Byzantine influence and an aspiration to recover classical monumentality and poise.
Walls of Romanesque and Gothic churches were decorated with frescoes as well as sculpture and many of the few remaining murals have great intensity, and combine the decorative energy of Insular art with a new monumentality in the treatment of figures. Far more miniatures in Illuminated manuscripts survive from the period, showing the same characteristics, which continue into the Gothic period.
Panel painting becomes more common during the Romanesque period, under the heavy influence of Byzantine icons. Towards the middle of the 13th century, Medieval art and Gothic painting became more realistic, with the beginnings of interest in the depiction of volume and perspective in Italy with Cimabue and then his pupil Giotto. From Giotto on, the treatment of composition by the best painters also became much more free and innovative. They are considered to be the two great medieval masters of painting in western culture. Cimabue, within the Byzantine tradition, used a more realistic and dramatic approach to his art. His pupil, Giotto, took these innovations to a higher level which in turn set the foundations for the western painting tradition. Both artists were pioneers in the move towards naturalism.
Churches were built with more and more windows and the use of colorful stained glass become a staple in decoration. One of the most famous examples of this is found in the cathedral of Notre Dame de Paris. By the 14th century Western societies were both richer and more cultivated and painters found new patrons in the nobility and even the bourgeoisie. Illuminated manuscripts took on a new character and slim, fashionably dressed court women were shown in their landscapes. This style soon became known as International Gothic and tempera panel paintings and altarpieces gained importance.
Renaissance and Mannerism.
The Renaissance is said by many to be the golden age of painting. Roughly spanning the 14th through the mid-17th century. In Italy artists like Paolo Uccello, Fra Angelico, Masaccio, Piero della Francesca, Andrea Mantegna, Filippo Lippi, Giorgione, Tintoretto, Sandro Botticelli, Leonardo da Vinci, Michelangelo Buonarroti, Raphael, Giovanni Bellini, and Titian took painting to a higher level through the use of perspective, the study of human anatomy and proportion, and through their development of an unprecedented refinement in drawing and painting techniques.
Flemish, Dutch and German painters of the Renaissance such as Hans Holbein the Younger, Albrecht Dürer, Lucas Cranach, Matthias Grünewald, Hieronymous Bosch, and Pieter Brueghel represent a different approach from their Italian colleagues, one that is more realistic and less idealized. Genre painting became a popular idiom amongst Northern painters such as Pieter Brueghel. A new verisimilitude in depicting reality became possible with the adoption of oil painting, whose invention was traditionally, but erroneously, credited to Jan Van Eyck (an important transitional figure who bridges painting in the Middle Ages with painting of the early Renaissance). Unlike the Italians whose work drew heavily from the art of ancient Greece and Rome, the northerners retained a stylistic residue of the sculpture and illuminated manuscripts of the Middle Ages. These tendencies are also seen in the art of Tudor England, which was heavily influenced by Protestant refugees from the Low Countries.
Renaissance painting reflects the revolution of ideas and science (astronomy, geography) that occur in this period, the Reformation, and the invention of the printing press. Dürer, considered one of the greatest of printmakers, states that painters are not mere artisans but thinkers as well. With the development of easel painting in the Renaissance, painting gained independence from architecture. Following centuries dominated by religious imagery, secular subject matter slowly returned to Western painting. Artists included visions of the world around them, or the products of their own imaginations in their paintings. Those who could afford the expense could become patrons and commission portraits of themselves or their family.
In the 15th and 16th centuries, panel paintings which could be hung on walls and moved around at will, became increasingly popular for both churches and private houses, rather than fresco wall-paintings or paintings incorporated into on permanent structures, such as altarpieces. The High Renaissance gave rise to a stylized art known as Mannerism. In place of the balanced compositions and rational approach to perspective that characterized art at the dawn of the 16th century, the Mannerists sought instability, artifice, and doubt. The unperturbed faces and gestures of Piero della Francesca and the calm Virgins of Raphael are replaced by the troubled expressions of Pontormo and the emotional intensity of El Greco. Some decades later Northern Mannerism dominated Netherlandish and German art until the arrival of the Baroque.
Baroque and Rococo.
Baroque painting is associated with the Baroque cultural movement, a movement often identified with Absolutism and the Counter Reformation or Catholic Revival; the existence of important Baroque painting in non-absolutist and Protestant states also, however, underscores its popularity, as the style spread throughout Western Europe.
Baroque painting is characterized by great drama, rich, deep color, and intense light and dark shadows. Baroque art was meant to evoke emotion and passion instead of the calm rationality that had been prized during the Renaissance. During the period beginning around 1600 and continuing throughout the 17th century, painting is characterized as Baroque. Among the greatest painters of the Baroque are Caravaggio, Rembrandt, Frans Hals, Rubens, Velázquez, Poussin, and Jan Vermeer. Caravaggio is an heir of the humanist painting of the High Renaissance. His realistic approach to the human figure, painted directly from life and dramatically spotlit against a dark background, shocked his contemporaries and opened a new chapter in the history of painting.
Baroque painting often dramatizes scenes using light effects; this can be seen in works by Rembrandt, Vermeer, Le Nain and La Tour.
During the 18th century, Rococo followed as a lighter extension of Baroque, often frivolous and erotic. Rococo developed first in the decorative arts and interior design in France. Louis XV's succession brought a change in the court artists and general artistic fashion. The 1730s represented the height of Rococo development in France exemplified by the works of Antoine Watteau and François Boucher. Rococo still maintained the Baroque taste for complex forms and intricate patterns, but by this point, it had begun to integrate a variety of diverse characteristics, including a taste for Oriental designs and asymmetric compositions.
The Rococo style spread with French artists and engraved publications. It was readily received in the Catholic parts of Germany, Bohemia, and Austria, where it was merged with the lively German Baroque traditions. German Rococo was applied with enthusiasm to churches and palaces, particularly in the south, while Frederician Rococo developed in the Kingdom of Prussia.
The French masters Watteau, Boucher and Fragonard represent the style, as do Giovanni Battista Tiepolo and Jean-Baptiste-Siméon Chardin who was considered by some as the best French painter of the 18th century – the "Anti-Rococo". Portraiture was an important component of painting in all countries, but especially in England, where the leaders were William Hogarth, in a blunt realist style, and Francis Hayman, Angelica Kauffman (who was Swiss), Thomas Gainsborough and Joshua Reynolds in more flattering styles influenced by Anthony van Dyck. While in France during the Rococo era Jean-Baptiste Greuze (the favorite painter of Denis Diderot), Maurice Quentin de La Tour, and Élisabeth Vigée-Lebrun were highly accomplished Portrait painters and History painters.
William Hogarth helped develop a theoretical foundation for Rococo beauty. Though not intentionally referencing the movement, he argued in his "Analysis of Beauty" (1753) that the undulating lines and S-curves prominent in Rococo were the basis for grace and beauty in art or nature (unlike the straight line or the circle in Classicism). The beginning of the end for Rococo came in the early 1760s as figures like Voltaire and Jacques-François Blondel began to voice their criticism of the superficiality and degeneracy of the art. Blondel decried the "ridiculous jumble of shells, dragons, reeds, palm-trees and plants" in contemporary interiors. By 1785, Rococo had passed out of fashion in France, replaced by the order and seriousness of Neoclassical artists like Jacques-Louis David.
18th and 19th centuries: Neo-classicism, History painting, Romanticism, Impressionism, Post Impressionism, Symbolism.
After Rococo there arose in the late 18th century, in architecture, and then in painting severe neo-classicism, best represented by such artists as David and his heir Ingres. Ingres' work already contains much of the sensuality, but none of the spontaneity, that was to characterize Romanticism.
This movement turned its attention toward landscape and nature as well as the human figure and the supremacy of natural order above mankind's will. There is a pantheist philosophy (see Spinoza and Hegel) within this conception that opposes Enlightenment ideals by seeing mankind's destiny in a more tragic or pessimistic light. The idea that human beings are not above the forces of Nature is in contradiction to Ancient Greek and Renaissance ideals where mankind was above all things and owned his fate. This thinking led romantic artists to depict the sublime, ruined churches, shipwrecks, massacres and madness.
By the mid-19th-century painters became liberated from the demands of their patronage to only depict scenes from religion, mythology, portraiture or history. The idea "art for art's sake" began to find expression in the work of painters like Francisco de Goya, John Constable, and J.M.W. Turner. Romantic painters turned landscape painting into a major genre, considered until then as a minor genre or as a decorative background for figure compositions.
Some of the major painters of this period are Eugène Delacroix, Théodore Géricault, J. M. W. Turner, Caspar David Friedrich and John Constable. Francisco de Goya's late work demonstrates the Romantic interest in the irrational, while the work of Arnold Böcklin evokes mystery and the paintings of Aesthetic movement artist James McNeill Whistler evoke both sophistication and decadence. In the United States the Romantic tradition of landscape painting was known as the Hudson River School: exponents include Thomas Cole, Frederic Edwin Church, Albert Bierstadt, Thomas Moran, and John Frederick Kensett. Luminism was a movement in American landscape painting related to the Hudson River School.
The leading Barbizon School painter Camille Corot painted in both a romantic and a realistic vein; his work prefigures Impressionism, as does the paintings of Eugène Boudin who was one of the first French landscape painters to paint outdoors. Boudin was also an important influence on the young Claude Monet, whom in 1857 he introduced to Plein air painting. A major force in the turn towards Realism at mid-century was Gustave Courbet. In the latter third of the century Impressionists like Édouard Manet, Claude Monet, Pierre-Auguste Renoir, Camille Pissarro, Alfred Sisley, Berthe Morisot, Mary Cassatt, and Edgar Degas worked in a more direct approach than had previously been exhibited publicly. They eschewed allegory and narrative in favor of individualized responses to the modern world, sometimes painted with little or no preparatory study, relying on deftness of drawing and a highly chromatic pallette. Manet, Degas, Renoir, Morisot, and Cassatt concentrated primarily on the human subject. Both Manet and Degas reinterpreted classical figurative canons within contemporary situations; in Manet's case the re-imaginings met with hostile public reception. Renoir, Morisot, and Cassatt turned to domestic life for inspiration, with Renoir focusing on the female nude. Monet, Pissarro, and Sisley used the landscape as their primary motif, the transience of light and weather playing a major role in their work. While Sisley most closely adhered to the original principals of the Impressionist perception of the landscape, Monet sought challenges in increasingly chromatic and changeable conditions, culminating in his series of monumental works of Water Lilies painted in Giverny.
Pissarro adopted some of the experiments of Post-Impressionism. Slightly younger Post-Impressionists like Vincent van Gogh, Paul Gauguin, and Georges Seurat, along with Paul Cézanne led art to the edge of modernism; for Gauguin Impressionism gave way to a personal symbolism; Seurat transformed Impressionism's broken color into a scientific optical study, structured on frieze-like
compositions; Van Gogh's turbulent method of paint application, coupled with a sonorous use of color, predicted Expressionism and Fauvism, and Cézanne, desiring to unite classical composition with a revolutionary abstraction of natural forms, would come to be seen as a precursor of 20th-century art.
The spell of Impressionism was felt throughout the world, including in the United States, where it became integral to the painting of American Impressionists such as Childe Hassam, John Twachtman, and Theodore Robinson; and in Australia where painters of the Heidelberg School such as Arthur Streeton, Frederick McCubbin and Charles Conder painted "en plein air" and were particularly interested in the Australian landscape and light. It also exerted influence on painters who were not primarily Impressionistic in theory, like the portrait and landscape painter John Singer Sargent. At the same time in America at the turn of the 20th century there existed a native and nearly insular realism, as richly embodied in the figurative work of Thomas Eakins, the Ashcan School, and the landscapes and seascapes of Winslow Homer, all of whose paintings were deeply invested in the solidity of natural forms. The visionary landscape, a motive largely dependent on the ambiguity of the nocturne, found its advocates in Albert Pinkham Ryder and Ralph Albert Blakelock.
In the late 19th century there also were several, rather dissimilar, groups of Symbolist painters whose works resonated with younger artists of the 20th century, especially with the Fauvists and the Surrealists. Among them were Gustave Moreau, Odilon Redon, Pierre Puvis de Chavannes, Henri Fantin-Latour, Arnold Böcklin, Edvard Munch, Félicien Rops, and Jan Toorop, and Gustave Klimt amongst others including the Russian Symbolists like Mikhail Vrubel.
Symbolist painters mined mythology and dream imagery for a visual language of the soul, seeking evocative paintings that brought to mind a static world of silence. The symbols used in Symbolism are not the familiar emblems of mainstream iconography but intensely personal, private, obscure and ambiguous references. More a philosophy than an actual style of art, the Symbolist painters influenced the contemporary Art Nouveau movement and Les Nabis. In their exploration of dreamlike subjects, symbolist painters are found across centuries and cultures, as they are still today; Bernard Delvaille has described René Magritte's surrealism as "Symbolism plus Freud".
20th-century Modern and Contemporary.
The heritage of painters like Van Gogh, Cézanne, Gauguin, and Seurat was essential for the development of modern art. At the beginning of the 20th century Henri Matisse and several other young artists revolutionized the Paris art world with "wild", multi-colored, expressive, landscapes and figure paintings that the critics called Fauvism. Pablo Picasso made his first cubist paintings based on Cézanne's idea that all depiction of nature can be reduced to three solids: cube, sphere and cone.
Pioneers of the 20th century.
The heritage of painters like Van Gogh, Cézanne, Gauguin, and Seurat was essential for the development of modern art. At the beginning of the 20th century Henri Matisse and several other young artists including the pre-cubist Georges Braque, André Derain, Raoul Dufy and Maurice de Vlaminck revolutionized the Paris art world with "wild", multi-colored, expressive, landscapes and figure paintings that the critics called Fauvism – (as seen in the gallery above). Henri Matisse's second version of "The Dance" signifies a key point in his career and in the development of modern painting. It reflects Matisse's incipient fascination with primitive art: the intense warm colors against the cool blue-green background and the rhythmical succession of dancing nudes convey the feelings of emotional liberation and hedonism. Pablo Picasso made his first cubist paintings based on Cézanne's idea that all depiction of nature can be reduced to three solids: cube, sphere and cone. With the painting Les Demoiselles d'Avignon 1907, (see gallery) Picasso dramatically created a new and radical picture depicting a raw and primitive brothel scene with five prostitutes, violently painted women, reminiscent of African tribal masks and his own new Cubist inventions. analytic Cubism (see gallery) was jointly developed by Pablo Picasso and Georges Braque, exemplified by "Violin and Candlestick, Paris", (seen above) from about 1908 through 1912. Analytic cubism, the first clear manifestation of cubism, was followed by synthetic cubism, practised by Braque, Picasso, Fernand Léger, Juan Gris, Albert Gleizes, Marcel Duchamp and countless other artists into the 1920s. Synthetic cubism is characterized by the introduction of different textures, surfaces, collage elements, papier collé and a large variety of merged subject matter.
Les Fauves (French for "The Wild Beasts") were early-20th-century painters, experimenting with freedom of expression through color. The name was given, humorously and not as a compliment, to the group by art critic Louis Vauxcelles. Fauvism was a short-lived and loose grouping of early-20th-century artists whose works emphasized painterly qualities, and the imaginative use of deep color over the representational values. Fauvists made the subject of the painting easy to read, exaggerated perspectives and an interesting prescient prediction of the Fauves was expressed in 1888 by Paul Gauguin to Paul Sérusier,
"How do you see these trees? They are yellow. So, put in yellow; this shadow, rather blue, paint it with pure ultramarine; these red leaves? Put in vermilion."
The leaders of the movement were Henri Matisse and André Derain — friendly rivals of a sort, each with his own followers. Ultimately Matisse became the "yang" to Picasso's "yin" in the 20th century. Fauvist painters included Albert Marquet, Charles Camoin, Maurice de Vlaminck, Raoul Dufy, Othon Friesz, the Dutch painter Kees van Dongen, and Picasso's partner in Cubism, Georges Braque amongst others.
Fauvism, as a movement, had no concrete theories, and was short lived, beginning in 1905 and ending in 1907, they only had three exhibitions. Matisse was seen as the leader of the movement, due to his seniority in age and prior self-establishment in the academic art world. His 1905 portrait of Mme. Matisse "The Green Line", (above), caused a sensation in Paris when it was first exhibited. He said he wanted to create art to delight; art as a decoration was his purpose and it can be said that his use of bright colors tries to maintain serenity of composition. In 1906 at the suggestion of his dealer Ambroise Vollard, André Derain went to London and produced a series of paintings like "Charing Cross Bridge, London" (above) in the Fauvist style, paraphrasing the famous series by the Impressionist painter Claude Monet. Masters like Henri Matisse and Pierre Bonnard continued developing their narrative styles independent of any movement throughout the 20th century.
By 1907 Fauvism no longer was a shocking new movement, soon it was replaced by Cubism on the critics' radar screen as the latest new development in Contemporary Art of the time.
In 1907 Appolinaire, commenting about Matisse in an article published in La Falange, said, "We are not here in the presence of an extravagant or an extremist undertaking: Matisse's art is eminently reasonable."
Analytic cubism (see gallery) was jointly developed by Pablo Picasso and Georges Braque from about 1908 through 1912. Analytic cubism, the first clear manifestation of cubism, was followed by Synthetic cubism, practised by Braque, Picasso, Fernand Léger, Juan Gris, Albert Gleizes, Marcel Duchamp and countless other artists into the 1920s. Synthetic cubism is characterized by the introduction of different textures, surfaces, collage elements, papier collé and a large variety of merged subject matter.
During the years between 1910 and the end of World War I and after the heyday of cubism, several movements emerged in Paris. Giorgio De Chirico moved to Paris in July 1911, where he joined his brother Andrea (the poet and painter known as Alberto Savinio). Through his brother he met Pierre Laprade a member of the jury at the Salon d'Automne, where he exhibited three of his dreamlike works: "Enigma of the Oracle", "Enigma of an Afternoon" and "Self-Portrait". During 1913 he exhibited his work at the Salon des Indépendants and Salon d'Automne, his work was noticed by Pablo Picasso and Guillaume Apollinaire and several others. His compelling and mysterious paintings are considered instrumental to the early beginnings of Surrealism. (see gallery) During the first half of the 20th century in Europe masters like Georges Braque, André Derain, and Giorgio De Chirico continued painting independent of any movement.
Pioneers of Modern art.
In the first two decades of the 20th century and after Cubism, several other important movements emerged; futurism (Balla), abstract art (Kandinsky), Der Blaue Reiter), Bauhaus, (Kandinsky) and (Klee), Orphism, (Robert Delaunay and František Kupka), Synchromism (Morgan Russell), De Stijl (Mondrian), Suprematism (Malevich), Constructivism (Tatlin), Dadaism (Duchamp, Picabia, Arp) and Surrealism (De Chirico, André Breton, Miró, Magritte, Dalí, Ernst). Modern painting influenced all the visual arts, from Modernist architecture and design, to avant-garde film, theatre and modern dance and became an experimental laboratory for the expression of visual experience, from photography and concrete poetry to advertising art and fashion. Van Gogh's painting exerted great influence upon 20th-century Expressionism, as can be seen in the work of the Fauves, Die Brücke (a group led by German painter Ernst Kirchner), and the Expressionism of Edvard Munch, Egon Schiele, Marc Chagall, Amedeo Modigliani, Chaim Soutine and others..
Wassily Kandinsky a Russian painter, printmaker and art theorist, one of the most famous 20th-century artists is generally considered the first important painter of modern abstract art. As an early Modernist, in search of new modes of visual expression, and spiritual expression, he theorized as did contemporary occultists and theosophists, that pure visual abstraction had corollary vibrations with sound and music. They posited that pure abstraction could express pure spirituality. His earliest abstractions were generally titled as the example in the (above gallery) "Composition VII", making connection to the work of the composers of music. Kandinsky included many of his theories about abstract art in his book "Concerning the Spiritual in Art." Robert Delaunay was a French artist who is associated with Orphism, (reminiscent of a link between pure abstraction and cubism). His later works were more abstract, reminiscent of Paul Klee. His key contributions to abstract painting refer to his bold use of color, and a clear love of experimentation of both depth and tone. At the invitation of Wassily Kandinsky, Delaunay and his wife the artist Sonia Delaunay, joined The Blue Rider (Der Blaue Reiter), a Munich-based group of abstract artists, in 1911, and his art took a turn to the abstract.
Other major pioneers of early abstraction include Russian painter Kasimir Malevich, who after the Russian Revolution in 1917, and after pressure from the Stalinist regime in 1924 returned to painting imagery and "Peasants and Workers in the field", and Swiss painter Paul Klee whose masterful color experiments made him an important pioneer of abstract painting at the Bauhaus. Still other important pioneers of abstract painting include the Swedish artist Hilma af Klint, Czech painter František Kupka as well as American artists Stanton MacDonald-Wright and Morgan Russell who, in 1912, founded Synchromism, an art movement that closely resembles Orphism.
"Expressionism" and "Symbolism" are broad rubrics that involve several important and related movements in 20th-century painting that dominated much of the avant-garde art being made in Western, Eastern and Northern Europe. Expressionist works were painted largely between World War I and World War II, mostly in France, Germany, Norway, Russia, Belgium, and Austria. Expressionist artists are related to both Surrealism and Symbolism and are each uniquely and somewhat eccentrically personal. Fauvism, Die Brücke, and Der Blaue Reiter are three of the best known groups of Expressionist and Symbolist painters.
Artists as interesting and diverse as Marc Chagall, whose painting "I and the Village", (above) tells an autobiographical story that examines the relationship between the artist and his origins, with a lexicon of artistic Symbolism. Gustav Klimt, Egon Schiele, Edvard Munch, Emil Nolde, Chaim Soutine, James Ensor, Oskar Kokoschka, Ernst Ludwig Kirchner, Max Beckmann, Franz Marc, Käthe Schmidt Kollwitz, Georges Rouault, Amedeo Modigliani and some of the Americans abroad like Marsden Hartley, and Stuart Davis, were considered influential expressionist painters. Although Alberto Giacometti is primarily thought of as an intense Surrealist sculptor, he made intense expressionist paintings as well.
Pioneers of abstraction.
Piet Mondrian's art was also related to his spiritual and philosophical studies. In 1908 he became interested in the theosophical movement launched by Helena Petrovna Blavatsky in the late 19th century. Blavatsky believed that it was possible to attain a knowledge of nature more profound than that provided by empirical means, and much of Mondrian's work for the rest of his life was inspired by his search for that spiritual knowledge.
De Stijl also known as neoplasticism, was a Dutch artistic movement founded in 1917. The term "De Stijl" is used to refer to a body of work from 1917 to 1931 founded in the Netherlands.
"De Stijl" is also the name of a journal that was published by the Dutch painter, designer, writer, and critic Theo van Doesburg propagating the group's theories. Next to van Doesburg, the group's principal members were the painters Piet Mondrian, Vilmos Huszár, and Bart van der Leck, and the architects Gerrit Rietveld, Robert van 't Hoff, and J.J.P. Oud. The artistic philosophy that formed a basis for the group's work is known as "neoplasticism" — the new plastic art (or "Nieuwe Beelding" in Dutch).
Proponents of De Stijl sought to express a new utopian ideal of spiritual harmony and order. They advocated pure abstraction and universality by a reduction to the essentials of form and colour; they simplified visual compositions to the vertical and horizontal directions, and used only primary colors along with black and white. Indeed, according to the Tate Gallery's online article on neoplasticism, Mondrian himself sets forth these delimitations in his essay 'Neo-Plasticism in Pictorial Art'. He writes, "... this new plastic idea will ignore the particulars of appearance, that is to say, natural form and colour. On the contrary, it should find its expression in the abstraction of form and colour, that is to say, in the straight line and the clearly defined primary colour." The Tate article further summarizes that this art allows "only primary colours and non-colours, only squares and rectangles, only straight and horizontal or vertical line." The Guggenheim Museum's online article on De Stijl summarizes these traits in similar terms: "It [De Stijl] was posited on the fundamental principle of the geometry of the straight line, the square, and the rectangle, combined with a strong asymmetricality; the predominant use of pure primary colors with black and white; and the relationship between positive and negative elements in an arrangement of non-objective forms and lines."
De Stijl movement was influenced by Cubist painting as well as by the mysticism and the ideas about "ideal" geometric forms (such as the "perfect straight line") in the neoplatonic philosophy of mathematician M.H.J. Schoenmaekers. The works of De Stijl would influence the Bauhaus style and the international style of architecture as well as clothing and interior design. However, it did not follow the general guidelines of an "ism" (Cubism, Futurism, Surrealism), nor did it adhere to the principles of art schools like Bauhaus; it was a collective project, a joint enterprise.
Dada and Surrealism.
Marcel Duchamp, came to international prominence in the wake of his notorious success at the New York City Armory Show in 1913, (soon after he denounced artmaking for chess). After Duchamp's Nude Descending a Staircase became the international cause celebre at the 1913 Armory show in New York he created the "The Bride Stripped Bare by Her Bachelors, Even, Large Glass". The "Large Glass" pushed the art of painting to radical new limits being part painting, part collage, part construction. Duchamp became closely associated with the Dada movement that began in neutral Zurich, Switzerland, during World War I and peaked from 1916 to 1920. The movement primarily involved visual arts, literature (poetry, art manifestoes, art theory), theatre, and graphic design, and concentrated its anti war politic through a rejection of the prevailing standards in art through anti-art cultural works. Francis Picabia (see above), Man Ray, Kurt Schwitters, Tristan Tzara, Hans Richter, Jean Arp, Sophie Taeuber-Arp, along with Duchamp and many others are associated with the Dadaist movement. Duchamp and several Dadaists are also associated with Surrealism, the movement that dominated European painting in the 1920s and 1930s.
In 1924 André Breton published the "Surrealist Manifesto." The Surrealist movement in painting became synonymous with the avant-garde and which featured artists whose works varied from the abstract to the super-realist. With works on paper like "Machine Turn Quickly", (above) Francis Picabia continued his involvement in the Dada movement through 1919 in Zurich and Paris, before breaking away from it after developing an interest in Surrealist art. Yves Tanguy, René Magritte and Salvador Dalí are particularly known for their realistic depictions of dream imagery and fantastic manifestations of the imagination. Joan Miró's "The Tilled Field" of 1923–1924 verges on abstraction, this early painting of a complex of objects and figures, and arrangements of sexually active characters; was Miró's first Surrealist masterpiece. The more abstract Joan Miró, Jean Arp, André Masson, and Max Ernst were very influential, especially in the United States during the 1940s.
Throughout the 1930s, Surrealism continued to become more visible to the public at large. A Surrealist group developed in Britain and, according to Breton, their 1936 London International Surrealist Exhibition was a high water mark of the period and became the model for international exhibitions. Surrealist groups in Japan, and especially in Latin America, the Caribbean and in Mexico produced innovative and original works.
Dalí and Magritte created some of the most widely recognized images of the movement. The 1928/1929 painting "This Is Not A Pipe", by Magritte is the subject of a Michel Foucault 1973 book, "This is not a Pipe" (English edition, 1991), that discusses the painting and its paradox. Dalí joined the group in 1929, and participated in the rapid establishment of the visual style between 1930 and 1935.
Surrealism as a visual movement had found a method: to expose psychological truth by stripping ordinary objects of their normal significance, in order to create a compelling image that was beyond ordinary formal organization, and perception, sometimes evoking empathy from the viewer, sometimes laughter and sometimes outrage and bewilderment.
1931 marked a year when several Surrealist painters produced works which marked turning points in their stylistic evolution: in one example (see gallery above) liquid shapes become the trademark of Dalí, particularly in his "The Persistence of Memory", which features the image of watches that sag as if they are melting. Evocations of time and its compelling mystery and absurdity.
The characteristics of this style – a combination of the depictive, the abstract, and the psychological – came to stand for the alienation which many people felt in the modernist period, combined with the sense of reaching more deeply into the psyche, to be "made whole with one's individuality."
Max Ernst whose 1920 painting "Murdering Airplane", studied philosophy and psychology in Bonn and was interested in the alternative realities experienced by the insane. His paintings may have been inspired by the psychoanalyst Sigmund Freud's study of the delusions of a paranoiac, Daniel Paul Schreber. Freud identified Schreber's fantasy of becoming a woman as a "castration complex." The central image of two pairs of legs refers to Schreber's hermaphroditic desires. Ernst's inscription on the back of the painting reads: "The picture is curious because of its symmetry. The two sexes balance one another."
During the 1920s André Masson's work was enormously influential in helping the newly arrived in Paris and young artist Joan Miró find his roots in the new Surrealist painting. Miró acknowledged in letters to his dealer Pierre Matisse the importance of Masson as an example to him in his early years in Paris.
Long after personal, political and professional tensions have fragmented the Surrealist group into thin air and ether, Magritte, Miró, Dalí and the other Surrealists continue to define a visual program in the arts. Other prominent surrealist artists include Giorgio de Chirico, Méret Oppenheim, Toyen, Grégoire Michonze, Roberto Matta, Kay Sage, Leonora Carrington, Dorothea Tanning, and Leonor Fini among others.
Between the Wars.
Der Blaue Reiter was a German movement lasting from 1911 to 1914, fundamental to Expressionism, along with Die Brücke which was founded the previous decade in 1905 and was a group of German expressionist artists formed in Dresden in 1905. Founding members of Die Brücke were Fritz Bleyl, Erich Heckel, Ernst Ludwig Kirchner and Karl Schmidt-Rottluff. Later members included Max Pechstein, Otto Mueller and others. The group was one of the seminal ones, which in due course had a major impact on the evolution of modern art in the 20th century and created the style of Expressionism.
Wassily Kandinsky, Franz Marc, August Macke, Alexej von Jawlensky, whose psychically expressive painting of the Russian dancer "Portrait of Alexander Sakharoff", 1909 is in the gallery above, Marianne von Werefkin, Lyonel Feininger and others founded the Der Blaue Reiter group in response to the rejection of Kandinsky's painting "Last Judgement" from an exhibition. Der Blaue Reiter lacked a central artistic manifesto, but was centered around Kandinsky and Marc. Artists Gabriele Münter and Paul Klee were also involved.
The name of the movement comes from a painting by Kandinsky created in 1903 (see illustration). It is also claimed that the name could have derived from Marc's enthusiasm for horses and Kandinsky's love of the colour blue. For Kandinsky, "blue" is the colour of spirituality: the darker the blue, the more it awakens human desire for the eternal.
In the USA during the period between World War I and World War II painters tended to go to Europe for recognition. Artists like Marsden Hartley, Patrick Henry Bruce, Gerald Murphy and Stuart Davis, created reputations abroad. In New York City, Albert Pinkham Ryder and Ralph Blakelock were influential and important figures in advanced American painting between 1900 and 1920. During the 1920s photographer Alfred Stieglitz exhibited Georgia O'Keeffe, Arthur Dove, Alfred Henry Maurer, Charles Demuth, John Marin and other artists including European Masters Henri Matisse, Auguste Rodin, Henri Rousseau, Paul Cézanne, and Pablo Picasso, at his gallery "the 291."
Social consciousness.
During the 1920s and the 1930s and the Great Depression, Surrealism, late Cubism, the Bauhaus, De Stijl, Dada, German Expressionism, Expressionism, and modernist and masterful color painters like Henri Matisse and Pierre Bonnard characterized the European art scene. In Germany Max Beckmann, Otto Dix, George Grosz and others politicized their paintings, foreshadowing the coming of World War II. While in America American Scene painting and the social realism and regionalism movements that contained both political and social commentary dominated the art world. Artists like Ben Shahn, Thomas Hart Benton, Grant Wood, George Tooker, John Steuart Curry, Reginald Marsh, and others became prominent. In Latin America besides the Uruguayan painter Joaquín Torres García and Rufino Tamayo from Mexico, the muralist movement with Diego Rivera, David Siqueiros, José Orozco, Pedro Nel Gómez and Santiago Martinez Delgado and the Symbolist paintings by Frida Kahlo began a renaissance of the arts for the region, with a use of color and historic, and political messages. Frida Kahlo's Symbolist works also relate strongly to Surrealism and to the Magic Realism movement in literature. The psychological drama in many of Kahlo's self portraits (above) underscore the vitality and relevance of her paintings to artists in the 21st century.
"American Gothic" is a painting by Grant Wood from 1930. Portraying a pitchfork-holding farmer and a younger woman in front of a house of Carpenter Gothic style, it is one of the most familiar images in 20th-century American art. Art critics had favorable opinions about the painting, like Gertrude Stein and Christopher Morley, they assumed the painting was meant to be a satire of rural small-town life. It was thus seen as part of the trend towards increasingly critical depictions of rural America, along the lines of Sherwood Anderson's "1919 Winesburg, Ohio", Sinclair Lewis' 1920 "Main Street", and Carl Van Vechten's "The Tattooed Countess" in literature. However, with the onset of the Great Depression, the painting came to be seen as a depiction of steadfast American pioneer spirit.
Diego Rivera is perhaps best known by the public world for his 1933 mural, "Man at the Crossroads", in the lobby of the RCA Building at Rockefeller Center. When his patron Nelson Rockefeller discovered that the mural included a portrait of Vladimir Lenin and other communist imagery, he fired Rivera, and the unfinished work was eventually destroyed by Rockefeller's staff. The film "Cradle Will Rock" includes a dramatization of the controversy. Frida Kahlo (Rivera's wife's) works are often characterized by their stark portrayals of pain. Of her 143 paintings 55 are self-portraits, which frequently incorporate symbolic portrayals of her physical and psychological wounds. Kahlo was deeply influenced by indigenous Mexican culture, which is apparent in her paintings' bright colors and dramatic symbolism. Christian and Jewish themes are often depicted in her work as well; she combined elements of the classic religious Mexican tradition—which were often bloody and violent—with surrealist renderings. While her paintings are not overtly Christian – she was, after all, an avowed communist – they certainly contain elements of the macabre Mexican Christian style of religious paintings.
Political activism was an important piece of David Siqueiros' life, and frequently inspired him to set aside his artistic career. His art was deeply rooted in the Mexican Revolution, a violent and chaotic period in Mexican history in which various social and political factions fought for recognition and power. The period from the 1920s to the 1950s is known as the Mexican Renaissance, and Siqueiros was active in the attempt to create an art that was at once Mexican and universal. He briefly gave up painting to focus on organizing miners in Jalisco. He ran a political art workshop in New York City in preparation for the 1936 General Strike for Peace and May Day parade. The young Jackson Pollock attended the workshop and helped build floats for the parade. Between 1937 and 1938 he fought in the Spanish Civil War alongside the Spanish Republican forces, in opposition to Francisco Franco's military coup. He was exiled twice from Mexico, once in 1932 and again in 1940, following his assassination attempt on Leon Trotsky.
World conflict.
During the 1930s radical leftist politics characterized many of the artists connected to Surrealism, including Pablo Picasso. On 26 April 1937, during the Spanish Civil War, the Basque town of Gernika was the scene of the "Bombing of Gernika" by the Condor Legion of Nazi Germany's Luftwaffe. The Germans were attacking to support the efforts of Francisco Franco to overthrow the Basque Government and the Spanish Republican government. The town was devastated, though the Biscayan assembly and the Oak of Gernika survived. Pablo Picasso painted his mural sized "Guernica" to commemorate the horrors of the bombing.
In its final form, "Guernica" is an immense black and white, 3.5 m tall and 7.8 m wide mural painted in oil. The mural presents a scene of death, violence, brutality, suffering, and helplessness without portraying their immediate causes. The choice to paint in black and white contrasts with the intensity of the scene depicted and invokes the immediacy of a newspaper photograph.
Picasso painted the mural sized painting called "Guernica" in protest of the bombing. The painting was first exhibited in Paris in 1937, then Scandinavia, then London in 1938 and finally in 1939 at Picasso's request the painting was sent to the United States in an extended loan (for safekeeping) at MoMA. The painting went on a tour of museums throughout the USA until its final return to the Museum of Modern Art in New York City where it was exhibited for nearly thirty years. Finally in accord with Pablo Picasso's wish to give the painting to the people of Spain as a gift, it was sent to Spain in 1981.
During the Great Depression of the 1930s, through the years of World War II American art was characterized by Social Realism and American Scene Painting (as seen above) in the work of Grant Wood, Edward Hopper, Ben Shahn, Thomas Hart Benton, and several others. "Nighthawks" (1942) is a painting by Edward Hopper that portrays people sitting in a downtown diner late at night. It is not only Hopper's most famous painting, but one of the most recognizable in American art. It is currently in the collection of the Art Institute of Chicago. The scene was inspired by a diner (since demolished) in Greenwich Village, Hopper's home neighborhood in Manhattan. Hopper began painting it immediately after the attack on Pearl Harbor. After this event there was a large feeling of gloominess over the country, a feeling that is portrayed in the painting. The urban street is empty outside the diner, and inside none of the three patrons is apparently looking or talking to the others but instead is lost in their own thoughts. This portrayal of modern urban life as empty or lonely is a common theme throughout Hopper's work.
The Dynamic for artists in Europe during the 1930s deteriorated rapidly as the Nazi's power in Germany and across Eastern Europe increased. The climate became so hostile for artists and art associated with Modernism and abstraction that many left for the Americas. "Degenerate art" was a term adopted by the Nazi regime in Germany for virtually all modern art. Such art was banned on the grounds that it was un-German or Jewish Bolshevist in nature, and those identified as degenerate artists were subjected to sanctions. These included being dismissed from teaching positions, being forbidden to exhibit or to sell their art, and in some cases being forbidden to produce art entirely.
"Degenerate Art" was also the title of an exhibition, mounted by the Nazis in Munich in 1937, consisting of modernist artworks chaotically hung and accompanied by text labels deriding the art. Designed to inflame public opinion against modernism, the exhibition subsequently traveled to several other cities in Germany and Austria. German artist Max Beckmann and scores of others fled Europe for New York. In New York City a new generation of young and exciting Modernist painters led by Arshile Gorky, Willem de Kooning, and others were just beginning to come of age.
Arshile Gorky's portrait of someone who might be Willem de Kooning (above) is an example of the evolution of abstract expressionism from the context of figure painting, cubism and surrealism. Along with his friends de Kooning and John D. Graham Gorky created bio-morphically shaped and abstracted figurative compositions that by the 1940s evolved into totally abstract paintings. Gorky's work seems to be a careful analysis of memory, emotion and shape, using line and color to express feeling and nature.
Towards mid-century.
The 1940s in New York City heralded the triumph of American abstract expressionism, a modernist movement that combined lessons learned from Henri Matisse, Pablo Picasso, Surrealism, Joan Miró, Cubism, Fauvism, and early Modernism via great teachers in America like Hans Hofmann and John D. Graham. American artists benefited from the presence of Piet Mondrian, Fernand Léger, Max Ernst and the André Breton group, Pierre Matisse's gallery, and Peggy Guggenheim's gallery "The Art of This Century", as well as other factors. The figurative work of Francis Bacon, Frida Kahlo, Edward Hopper, Lucian Freud, Andrew Wyeth and others served as a kind of alternative to abstract expressionism.
Post-Second World War American painting called Abstract expressionism included artists like Jackson Pollock, Willem de Kooning, Arshile Gorky, Mark Rothko, Hans Hofmann, Clyfford Still, Franz Kline, Adolph Gottlieb, Mark Tobey, Barnett Newman, James Brooks, Philip Guston, Robert Motherwell, Conrad Marca-Relli, Jack Tworkov, William Baziotes, Richard Pousette-Dart, Ad Reinhardt, Hedda Sterne, Jimmy Ernst, Esteban Vicente, Bradley Walker Tomlin, and Theodoros Stamos, among others. American Abstract expressionism got its name in 1946 from the art critic Robert Coates. It is seen as combining the emotional intensity and self-denial of the German Expressionists with the anti-figurative aesthetic of the European abstract schools such as futurism, the Bauhaus and synthetic cubism. Abstract expressionism, action painting, and Color Field painting are synonymous with the New York School.
Technically Surrealism was an important predecessor for abstract expressionism with its emphasis on spontaneous, automatic or subconscious creation. Jackson Pollock's dripping paint onto a canvas laid on the floor is a technique that has its roots in the work of André Masson. Another important early manifestation of what came to be abstract expressionism is the work of American Northwest artist Mark Tobey, especially his "white writing" canvases, which, though generally not large in scale, anticipate the "all over" look of Pollock's drip paintings.
Abstract expressionism.
Additionally, Abstract expressionism has an image of being rebellious, anarchic, highly idiosyncratic and, some feel, rather nihilistic. In practice, the term is applied to any number of artists working (mostly) in New York who had quite different styles, and even applied to work which is not especially abstract nor expressionist. Pollock's energetic "action paintings", with their "busy" feel, are different both technically and aesthetically, to the violent and grotesque "Women" series of Willem de Kooning. As seen above in the gallery "Woman V" is one of a series of six paintings made by de Kooning between 1950 and 1953 that depict a three-quarter-length female figure. He began the first of these paintings, "Woman I" collection: The Museum of Modern Art, New York City, in June 1950, repeatedly changing and painting out the image until January or February 1952, when the painting was abandoned unfinished. The art historian Meyer Schapiro saw the painting in de Kooning's studio soon afterwards and encouraged the artist to persist. De Kooning's response was to begin three other paintings on the same theme; "Woman II" collection: The Museum of Modern Art, New York City, "Woman III", Tehran Museum of Contemporary Art, "Woman IV", Nelson-Atkins Museum of Art, Kansas City, Missouri. During the summer of 1952, spent at East Hampton, de Kooning further explored the theme through drawings and pastels. He may have finished work on "Woman I" by the end of June, or possibly as late as November 1952, and probably the other three women pictures were concluded at much the same time. The "Woman series" are decidedly figurative paintings. Another important artist is Franz Kline, as demonstrated by his painting "High Street", 1950 (see gallery) as with Jackson Pollock and other Abstract Expressionists, was labelled an "action painter" because of his seemingly spontaneous and intense style, focusing less, or not at all, on figures or imagery, but on the actual brush strokes and use of canvas.
Clyfford Still, Barnett Newman, (see above), Adolph Gottlieb, and the serenely shimmering blocks of color in Mark Rothko's work (which is not what would usually be called expressionist and which Rothko denied was abstract), are classified as abstract expressionists, albeit from what Clement Greenberg termed the Color Field direction of abstract expressionism. Both Hans Hofmann (see gallery) and Robert Motherwell (gallery) can be comfortably described as practitioners of action painting and Color Field painting.
Abstract expressionism has many stylistic similarities to the Russian artists of the early 20th century such as Wassily Kandinsky. Although it is true that spontaneity or of the impression of spontaneity characterized many of the abstract expressionists works, most of these paintings involved careful planning, especially since their large size demanded it. An exception might be the drip paintings of Pollock.
Why this style gained mainstream acceptance in the 1950s is a matter of debate. American Social realism had been the mainstream in the 1930s. It had been influenced not only by the Great Depression but also by the Social Realists of Mexico such as David Alfaro Siqueiros and Diego Rivera. The political climate after World War II did not long tolerate the social protests of those painters. Abstract expressionism arose during World War II and began to be showcased during the early 1940s at galleries in New York like "The Art of This Century Gallery". The late 1940s through the mid-1950s ushered in the McCarthy era. It was after World War II and a time of political conservatism and extreme artistic censorship in the United States. Some people have conjectured that since the subject matter was often totally abstract, Abstract expressionism became a safe strategy for artists to pursue this style. Abstract art could be seen as apolitical. Or if the art was political, the message was largely for the insiders. However, those theorists are in the minority. As the first truly original school of painting in America, Abstract expressionism demonstrated the vitality and creativity of the country in the post-war years, as well as its ability (or need) to develop an aesthetic sense that was not constrained by the European standards of beauty.
Although Abstract expressionism spread quickly throughout the United States, the major centers of this style were New York City and California, especially in the New York School, and the San Francisco Bay area. Abstract expressionist paintings share certain characteristics, including the use of large canvases, an "all-over" approach, in which the whole canvas is treated with equal importance (as opposed to the center being of more interest than the edges). The canvas as the "arena" became a credo of action painting, while the "integrity of the picture plane" became a credo of the Color Field painters. Many other artists began exhibiting their abstract expressionist related paintings during the 1950s including Alfred Leslie, Sam Francis, Joan Mitchell, Helen Frankenthaler, Cy Twombly, Milton Resnick, Michael Goldberg, Norman Bluhm, Ray Parker, Nicolas Carone, Grace Hartigan, Friedel Dzubas, and Robert Goodnough among others. 
During the 1950s Color Field painting initially referred to a particular type of abstract expressionism, especially the work of Mark Rothko, Clyfford Still, Barnett Newman, Robert Motherwell and Adolph Gottlieb. It essentially involved abstract paintings with large, flat expanses of color that expressed the sensual, and visual feelings and properties of large areas of nuanced surface. Art critic Clement Greenberg perceived Color Field painting as related to but different from Action painting. The overall expanse and gestalt of the work of the early color field painters speaks of an almost religious experience, awestruck in the face of an expanding universe of sensuality, color and surface. During the early-to-mid-1960s, "Color Field painting" came to refer to the styles of artists like Jules Olitski, Kenneth Noland, and Helen Frankenthaler, whose works were related to second-generation abstract expressionism, and to younger artists like Larry Zox, and Frank Stella, – all moving in a new direction. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. In "Mountains and Sea", from 1952, (see above) a seminal work of Color Field painting by Helen Frankenthaler the artist used the stain technique for the first time.
In Europe there was the continuation of Surrealism, Cubism, Dada and the works of Matisse. Also in Europe, Tachisme (the European equivalent to Abstract expressionism) took hold of the newest generation. Serge Poliakoff, Nicolas de Staël, Georges Mathieu, Vieira da Silva, Jean Dubuffet, Yves Klein and Pierre Soulages among others are considered important figures in post-war European painting.
Eventually abstract painting in America evolved into movements such as Neo-Dada, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, Lyrical Abstraction, Neo-expressionism and the continuation of Abstract expressionism. As a response to the tendency toward abstraction imagery emerged through various new movements, notably Pop art.
Pop art.
Earlier in England in 1956 the term "Pop Art" was used by Lawrence Alloway for paintings that celebrated consumerism of the post World War II era. This movement rejected abstract expressionism and its focus on the hermeneutic and psychological interior, in favor of art which depicted, and often celebrated material consumer culture, advertising, and iconography of the mass production age. The early works of David Hockney and the works of Richard Hamilton Peter Blake and Eduardo Paolozzi were considered seminal examples in the movement.
Pop art in America was to a large degree initially inspired by the works of Jasper Johns, Larry Rivers, and Robert Rauschenberg. Although the paintings of Gerald Murphy, Stuart Davis and Charles Demuth during the 1920s and 1930s set the table for pop art in America. In New York City during the mid-1950s Robert Rauschenberg and Jasper Johns created works of art that at first seemed to be continuations of Abstract expressionist painting. Actually their works and the work of Larry Rivers, were radical departures from abstract expressionism especially in the use of banal and literal imagery and the inclusion and the combining of mundane materials into their work. The innovations of Johns' specific use of various images and objects like chairs, numbers, targets, beer cans and the American flag; Rivers paintings of subjects drawn from popular culture such as George Washington crossing the Delaware, and his inclusions of images from advertisements like the camel from Camel cigarettes, and Rauschenberg's surprising constructions using inclusions of objects and pictures taken from popular culture, hardware stores, junkyards, the city streets, and taxidermy gave rise to a radical new movement in American art. Eventually by 1963 the movement came to be known worldwide as pop art.
American pop art is exemplified by artists: Andy Warhol, Claes Oldenburg, Wayne Thiebaud, James Rosenquist, Jim Dine, Tom Wesselmann and Roy Lichtenstein among others. Lichtenstein's most important work is arguably "Whaam!" (1963, Tate Modern, London), one of the earliest known examples of pop art, adapted a comic-book panel from a 1962 issue of DC Comics' "All-American Men of War". The painting depicts a fighter aircraft firing a rocket into an enemy plane, with a red-and-yellow explosion. The cartoon style is heightened by the use of the onomatopoeic lettering "Whaam!" and the boxed caption "I pressed the fire control... and ahead of me rockets blazed through the sky..." Pop art merges popular and mass culture with fine art, while injecting humor, irony, and recognizable imagery and content into the mix. In October 1962 the Sidney Janis Gallery mounted "The New Realists" the first major pop art group exhibition in an uptown art gallery in New York City. Sidney Janis mounted the exhibition in a 57th Street storefront near his gallery at 15 E. 57th Street. The show sent shockwaves through the New York School and reverberated worldwide. Earlier in the fall of 1962 an historically important and ground-breaking "New Painting of Common Objects" exhibition of pop art, curated by Walter Hopps at the Pasadena Art Museum sent shock waves across the Western United States.
While in the downtown scene in New York City's East Village 10th Street galleries artists were formulating an American version of Pop Art. Claes Oldenburg had his storefront and made painted objects, and the Green Gallery on 57th Street began to show Tom Wesselmann and James Rosenquist. Later Leo Castelli exhibited other American artists including the bulk of the careers of Andy Warhol and Roy Lichtenstein and his use of Benday dots, a technique used in commercial reproduction. There is a connection between the radical works of Duchamp, and Man Ray, the rebellious Dadaists – with a sense of humor; and pop artists like Alex Katz (who became known for his parody's of portrait photography and suburban life), Claes Oldenburg, Andy Warhol, Roy Lichtenstein and the others.
While throughout the 20th century many painters continued to practice landscape and figurative painting with contemporary subjects and solid technique, like Milton Avery, John D. Graham, Fairfield Porter, Edward Hopper, Balthus, Francis Bacon, Nicolas de Staël, Andrew Wyeth, Lucian Freud, Frank Auerbach, Philip Pearlstein, David Park, Nathan Oliveira, David Hockney, Malcolm Morley, Richard Estes, Ralph Goings, Audrey Flack, Chuck Close, Susan Rothenberg, Eric Fischl, Vija Celmins and Richard Diebenkorn.
Figurative, Landscape, Still-Life, Seascape, and Realism.
During the 1930s through the 1960s abstract painting in America and Europe evolved into movements such as abstract expressionism, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, and Lyrical Abstraction. Other artists reacted as a response to the tendency toward abstraction, allowing figurative imagery to continue through various new contexts like the Bay Area Figurative Movement in the 1950s and new forms of expressionism from the 1940s through the 1960s. In Italy during this time, Giorgio Morandi was the foremost still life painter, exploring a wide variety of approaches to depicting everyday bottles and kitchen implements. Throughout the 20th century many painters practiced Realism and used expressive imagery; practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like still-life painter Giorgio Morandi, Milton Avery, John D. Graham, Fairfield Porter, Edward Hopper, Andrew Wyeth, Balthus, Francis Bacon, Leon Kossoff, Frank Auerbach, Lucian Freud, Philip Pearlstein, Willem de Kooning, Arshile Gorky, Grace Hartigan, Robert De Niro, Sr., Elaine de Kooning and others. Along with Henri Matisse, Pablo Picasso, Pierre Bonnard, Georges Braque, and other 20th-century masters. In particular Milton Avery through his use of color and his interest in seascape and landscape paintings connected with the Color field aspect of Abstract expressionism as manifested by Adolph Gottlieb and Mark Rothko as well as the lessons American painters took from the work of Henri Matisse.
"Head IV", 1949 (see above) is a painting by the Irish born artist Francis Bacon and is an example of Post World War II European Expressionism. The work shows a distorted version of the Portrait of Innocent X painted by the Spanish artist Diego Velázquez in 1650. The work is one of a series of variants of the Velázquez painting which Bacon executed throughout the 1950s and early 1960s, over a total of forty-five works. When asked why he was compelled to revisit the subject so often, Bacon replied that he had nothing against the Popes, that he merely "wanted an excuse to use these colours, and you can't give ordinary clothes that purple colour without getting into a sort of false fauve manner." The Pope in this version seethes with anger and aggression, and the dark colors give the image a grotesque and nightmarish appearance. The pleated curtains of the backdrop are rendered transparent, and seem to fall through the Pope's face.
Italian painter Giorgio Morandi was an important 20th-century early pioneer of Minimalism. Born in Bologna, Italy in 1890, throughout his career, Morandi concentrated almost exclusively on still lives and landscapes, except for a few self-portraits. With great sensitivity to tone, color, and compositional balance, he would depict the same familiar bottles and vases again and again in paintings notable for their simplicity of execution. Morandi executed 133 etchings, a significant body of work in its own right, and his drawings and watercolors often approach abstraction in their economy of means. Through his simple and repetitive motifs and economical use of color, value and surface, Morandi became a prescient and important forerunner of Minimalism. He died in Bologna in 1964.
After World War II the term School of Paris often referred to Tachisme, the European equivalent of American Abstract expressionism and those artists are also related to Cobra. Important proponents being Jean Dubuffet, Pierre Soulages, Nicholas de Staël, Hans Hartung, Serge Poliakoff, and Georges Mathieu, among several others. During the early 1950s Dubuffet (who was always a figurative artist), and de Staël, abandoned abstraction, and returned to imagery via figuration and landscape. De Staël 's work was quickly recognised within the post-war art world, and he became one of the most influential artists of the 1950s. His return to representation (seascapes, footballers, jazz musicians, seagulls) during the early 1950s can be seen as an influential precedent for the American Bay Area Figurative Movement, as many of those abstract painters like Richard Diebenkorn, David Park, Elmer Bischoff, Wayne Thiebaud, Nathan Oliveira, Joan Brown and others made a similar move; returning to imagery during the mid-1950s. Much of de Staël 's late work – in particular his thinned, and diluted oil on canvas abstract landscapes of the mid-1950s predicts Color Field painting and Lyrical Abstraction of the 1960s and 1970s. Nicolas de Staël's bold and intensely vivid color in his last paintings predict the direction of much of contemporary painting that came after him including Pop art of the 1960s.
Art brut, New Realism, Bay Area Figurative Movement, neo-Dada, photorealism.
During the 1950s and 1960s as abstract painting in America and Europe evolved into movements such as Color Field painting, post-painterly abstraction, op art, hard-edge painting, minimal art, shaped canvas painting, Lyrical Abstraction, and the continuation of Abstract expressionism. Other artists reacted as a response to the tendency toward abstraction with art brut, fluxus, neo-Dada, New Realism, allowing imagery to re-emerge through various new contexts like pop art, the Bay Area Figurative Movement and later in the 1970s Neo-expressionism. The Bay Area Figurative Movement of whom David Park, Elmer Bischoff, Nathan Oliveira and Richard Diebenkorn whose painting "Cityscape 1", 1963 is a typical example (see above) were influential members flourished during the 1950s and 1960s in California. Although throughout the 20th century painters continued to practice Realism and use imagery, practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like Milton Avery, Edward Hopper, Jean Dubuffet, Francis Bacon, Frank Auerbach, Lucian Freud, Philip Pearlstein, and others. Younger painters practiced the use of imagery in new and radical ways. Yves Klein, Martial Raysse, Niki de Saint Phalle, Wolf Vostell, David Hockney, Alex Katz, Malcolm Morley, Ralph Goings, Audrey Flack, Richard Estes, Chuck Close, Susan Rothenberg, Eric Fischl, John Baeder and Vija Celmins were a few who became prominent between the 1960s and the 1980s. Fairfield Porter (see above) was largely self-taught, and produced representational work in the midst of the Abstract Expressionist movement. His subjects were primarily landscapes, domestic interiors and portraits of family, friends and fellow artists, many of them affiliated with the New York School of writers, including John Ashbery, Frank O'Hara, and James Schuyler. Many of his paintings were set in or around the family summer house on Great Spruce Head Island, Maine.
Also during the 1960s and 1970s, there was a reaction against painting. Critics like Douglas Crimp viewed the work of artists like Ad Reinhardt, and declared the 'death of painting'. Artists began to practice new ways of making art. New movements gained prominence some of which are: Postminimalism, Earth art, video art, installation art, arte povera, performance art, body art, fluxus, mail art, the situationists and conceptual art among others.
Neo-Dada is also a movement that started 1n the 1950s and 1960s and was related to Abstract expressionism only with imagery. Featuring the emergence of combined manufactured items, with artist materials, moving away from previous conventions of painting. This trend in art is exemplified by the work of Jasper Johns and Robert Rauschenberg, whose "combines" in the 1950s were forerunners of Pop Art and Installation art, and made use of the assemblage of large physical objects, including stuffed animals, birds and commercial photography. Robert Rauschenberg, (see "untitled combine", 1963, above), Jasper Johns, Larry Rivers, John Chamberlain, Claes Oldenburg, George Segal, Jim Dine, and Edward Kienholz among others were important pioneers of both abstraction and Pop Art; creating new conventions of art-making; they made acceptable in serious contemporary art circles the radical inclusion of unlikely materials as parts of their works of art.
New abstraction from the 1950s through the 1980s.
Color Field painting clearly pointed toward a new direction in American painting, away from abstract expressionism. Color Field painting is related to post-painterly abstraction, suprematism, abstract expressionism, hard-edge painting and Lyrical Abstraction.
During the 1960s and 1970s abstract painting continued to develop in America through varied styles. Geometric abstraction, Op art, hard-edge painting, Color Field painting and minimal painting, were some interrelated directions for advanced abstract painting as well as some other new movements. Morris Louis was an important pioneer in advanced Color Field painting, his work can serve as a bridge between abstract expressionism, Color Field painting, and minimal art. Two influential teachers Josef Albers and Hans Hofmann introduced a new generation of American artists to their advanced theories of color and space. Josef Albers is best remembered for his work as an Geometric abstractionist painter and theorist. Most famous of all are the hundreds of paintings and prints that make up the series "Homage to the Square", (see gallery). In this rigorous series, begun in 1949, Albers explored chromatic interactions with flat colored squares arranged concentrically on the canvas. Albers' theories on art and education were formative for the next generation of artists. His own paintings form the foundation of both hard-edge painting and Op art.
Josef Albers, Hans Hofmann, Ilya Bolotowsky, Burgoyne Diller, Victor Vasarely, Bridget Riley, Richard Anuszkiewicz, Frank Stella, Morris Louis, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Larry Poons, Ronald Davis, Larry Zox, Al Held and some others like Mino Argento, are artists closely associated with Geometric abstraction, Op art, Color Field painting, and in the case of Hofmann and Newman Abstract expressionism as well.
In 1965, an exhibition called "The Responsive Eye", curated by William C. Seitz, was held at the Museum of Modern Art, in New York City. The works shown were wide ranging, encompassing the Minimalism of Frank Stella, the Op art of Larry Poons, the work of Alexander Liberman, alongside the masters of the Op Art movement: Victor Vasarely, Richard Anuszkiewicz, Bridget Riley and others. The exhibition focused on the perceptual aspects of art, which result both from the illusion of movement and the interaction of color relationships. Op art, also known as optical art, is a style present in some paintings and other works of art that use optical illusions. Op art is also closely akin to geometric abstraction and hard-edge painting. Although sometimes the term used for it is perceptual abstraction.
Op art is a method of painting concerning the interaction between illusion and picture plane, between understanding and seeing. Op art works are abstract, with many of the better known pieces made in only black and white. When the viewer looks at them, the impression is given of movement, hidden images, flashing and vibration, patterns, or alternatively, of swelling or warping.
Color Field painting sought to rid art of superfluous rhetoric. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, John Hoyland, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. Certain artists quoted references to past or present art, but in general color field painting presents abstraction as an end in itself. In pursuing this direction of modern art, artists wanted to present each painting as one unified, cohesive, monolithic image.
Frank Stella, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Ronald Davis, Neil Williams, Robert Mangold, Charles Hinman, Richard Tuttle, David Novros, and Al Loving are examples of artists associated with the use of the shaped canvas during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and Hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. The Andre Emmerich Gallery, the Leo Castelli Gallery, the Richard Feigen Gallery, and the Park Place Gallery were important showcases for Color Field painting, shaped canvas painting and Lyrical Abstraction in New York City during the 1960s. There is a connection with post-painterly abstraction, which reacted against abstract expressionisms' mysticism, hyper-subjectivity, and emphasis on making the act of painting itself dramatically visible – as well as the solemn acceptance of the flat rectangle as an almost ritual prerequisite for serious painting. During the 1960s Color Field painting and Minimal art were often closely associated with each other. In actuality by the early 1970s both movements became decidedly diverse.
Washington Color School, Shaped Canvas, Abstract Illusionism, Lyrical Abstraction.
Another related movement of the late 1960s, Lyrical Abstraction (the term being coined by Larry Aldrich, the founder of the Aldrich Contemporary Art Museum, Ridgefield Connecticut), encompassed what Aldrich said he saw in the studios of many artists at that time. It is also the name of an exhibition that originated in the Aldrich Museum and traveled to the Whitney Museum of American Art and other museums throughout the United States between 1969 and 1971.
Lyrical Abstraction in the late 1960s is characterized by the paintings of Dan Christensen, Ronnie Landfield, Peter Young and others, and along with the fluxus movement and postminimalism (a term first coined by Robert Pincus-Witten in the pages of "Artforum" in 1969) sought to expand the boundaries of abstract painting and minimalism by focusing on process, new materials and new ways of expression. Postminimalism often incorporating industrial materials, raw materials, fabrications, found objects, installation, serial repetition, and often with references to Dada and Surrealism is best exemplified in the sculptures of Eva Hesse. Lyrical Abstraction, conceptual art, postminimalism, Earth art, video, performance art, installation art, along with the continuation of fluxus, abstract expressionism, Color Field painting, hard-edge painting, minimal art, op art, pop art, photorealism and New Realism extended the boundaries of contemporary art in the mid-1960s through the 1970s. Lyrical Abstraction is a type of freewheeling abstract painting that emerged in the mid-1960s when abstract painters returned to various forms of painterly, pictorial, expressionism with a predominate focus on process, gestalt and repetitive compositional strategies in general.
Lyrical Abstraction shares similarities with color field painting and abstract expressionism, Lyrical Abstraction as exemplified by the 1968 Ronnie Landfield painting "For William Blake", (above) especially in the freewheeling usage of paint – texture and surface. Direct drawing, calligraphic use of line, the effects of brushed, splattered, stained, squeegeed, poured, and splashed paint superficially resemble the effects seen in abstract expressionism and color field painting. However, the styles are markedly different. Setting it apart from abstract expressionism and action painting of the 1940s and 1950s is the approach to composition and drama. As seen in action painting there is an emphasis on brushstrokes, high compositional drama, dynamic compositional tension. While in Lyrical Abstraction there is a sense of compositional randomness, all over composition, low key and relaxed compositional drama and an emphasis on process, repetition, and an all over sensibility.,
Hard-edge painting, minimalism, postminimalism, monochrome painting.
Agnes Martin, Robert Mangold (see above), Brice Marden, Jo Baer, Robert Ryman, Richard Tuttle, Neil Williams, David Novros, Paul Mogenson, Charles Hinman are examples of artists associated with Minimalism and (exceptions of Martin, Baer and Marden) the use of the shaped canvas also during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. The Bykert Gallery, and the Park Place Gallery were important showcases for Minimalism and shaped canvas painting in New York City during the 1960s.
During the 1960s and 1970s artists such as Robert Motherwell, Adolph Gottlieb, Phillip Guston, Lee Krasner, Cy Twombly, Robert Rauschenberg, Jasper Johns, Richard Diebenkorn, Josef Albers, Elmer Bischoff, Agnes Martin, Al Held, Sam Francis, Ellsworth Kelly, Morris Louis, Helen Frankenthaler, Gene Davis, Frank Stella, Kenneth Noland, Joan Mitchell, Friedel Dzubas, and younger artists like Brice Marden, Robert Mangold, Sam Gilliam, John Hoyland, Sean Scully, Pat Steir, Elizabeth Murray, Larry Poons, Walter Darby Bannard, Larry Zox, Ronnie Landfield, Ronald Davis, Dan Christensen, Joan Snyder, Ross Bleckner, Archie Rand, Susan Crile, and dozens of others produced a wide variety of paintings.
During the 1960s and 1970s, there was a reaction against abstract painting. Some critics viewed the work of artists like Ad Reinhardt, and declared the 'death of painting'. Artists began to practice new ways of making art. New movements gained prominence some of which are: postminimalism, Earth art, video art, installation art, arte povera, performance art, body art, fluxus, happening, mail art, the situationists and conceptual art among others.
However still other important innovations in abstract painting took place during the 1960s and the 1970s characterized by monochrome painting and hard-edge painting inspired by Ad Reinhardt, Barnett Newman, Milton Resnick, and Ellsworth Kelly. Artists as diverse as Agnes Martin, Al Held, Larry Zox, Frank Stella, Larry Poons, Brice Marden and others explored the power of simplification. The convergence of Color Field painting, minimal art, hard-edge painting, Lyrical Abstraction, and postminimalism blurred the distinction between movements that became more apparent in the 1980s and 1990s. The neo-expressionism movement is related to earlier developments in abstract expressionism, neo-Dada, Lyrical Abstraction and postminimal painting.
Neo Expressionism.
In the late 1960s the abstract expressionist painter Philip Guston helped to lead a transition from abstract expressionism to Neo-expressionism in painting, abandoning the so-called "pure abstraction" of abstract expressionism in favor of more cartoonish renderings of various personal symbols and objects. These works were inspirational to a new generation of painters interested in a revival of expressive imagery. His painting "Painting, Smoking, Eating" 1973, seen above in the gallery is an example of Guston's final and conclusive return to representation.
In the late 1970s and early 1980s, there was also a return to painting that occurred almost simultaneously in Italy, Germany, France and Britain. These movements were called Transavantguardia, Neue Wilde, Figuration Libre, Neo-expressionism, the school of London, and in the late 1980s the Stuckists respectively. These painting were characterized by large formats, free expressive mark making, figuration, myth and imagination. All work in this genre came to be labeled neo-expressionism. Critical reaction was divided. Some critics regarded it as driven by profit motivations by large commercial galleries. This type of art continues in popularity into the 21st century, even after the art crash of the late 1980s. Anselm Kiefer is a leading figure in European Neo-expressionism by the 1980s, (see "To the Unknown Painter" 1983, in the gallery above) Kiefer's themes widened from a focus on Germany's role in civilization to the fate of art and culture in general. His work became more sculptural and involves not only national identity and collective memory, but also occult symbolism, theology and mysticism. The theme of all the work is the trauma experienced by entire societies, and the continual rebirth and renewal in life.
During the late 1970s in the United States painters who began working with invigorated surfaces and who returned to imagery like Susan Rothenberg gained in popularity, especially as seen above in paintings like "Horse 2", 1979. During the 1980s American artists like Eric Fischl, (see "Bad Boy", 1981, above), David Salle, Jean-Michel Basquiat (who began as a graffiti artist), Julian Schnabel, and Keith Haring, and Italian painters like Mimmo Paladino, Sandro Chia, and Enzo Cucchi, among others defined the idea of Neo-expressionism in America.
Neo-expressionism was a style of modern painting that became popular in the late 1970s and dominated the art market until the mid-1980s. It developed in Europe as a reaction against the conceptual and minimalistic art of the 1960s and 1970s. Neo-expressionists returned to portraying recognizable objects, such as the human body (although sometimes in a virtually abstract manner), in a rough and violently emotional way using vivid colours and banal colour harmonies. The veteran painters Philip Guston, Frank Auerbach, Leon Kossoff, Gerhard Richter, A. R. Penck and Georg Baselitz, along with slightly younger artists like Anselm Kiefer, Eric Fischl, Susan Rothenberg, Francesco Clemente, Jean-Michel Basquiat, Julian Schnabel, Keith Haring, and many others became known for working in this intense expressionist vein of painting.
Painting still holds a respected position in contemporary art. Art is an open field no longer divided by the objective versus non-objective dichotomy. Artists can achieve critical success whether their images are representational or abstract. What has currency is content, exploring the boundaries of the medium, and a refusal to recapitulate the works of the past as an end goal.
Contemporary painting into the 21st century.
At the beginning of the 21st century Contemporary painting and Contemporary art in general continues in several contiguous modes, characterized by the idea of pluralism. The "crisis" in painting and current art and current art criticism today is brought about by pluralism. There is no consensus, nor need there be, as to a representative style of the age. There is an "anything goes" attitude that prevails; an "everything going on", and consequently "nothing going on" syndrome; this creates an aesthetic traffic jam with no firm and clear direction and with every lane on the artistic superhighway filled to capacity. Consequently magnificent and important works of art continue to be made albeit in a wide variety of styles and aesthetic temperaments, the marketplace being left to judge merit.
Hard-edge painting, geometric abstraction, appropriation, hyperrealism, photorealism, expressionism, minimalism, Lyrical Abstraction, pop art, op art, abstract expressionism, Color Field painting, monochrome painting, neo-expressionism, collage, intermedia painting, assemblage painting, digital painting, postmodern painting, neo-Dada painting, shaped canvas painting, environmental mural painting, traditional figure painting, landscape painting, portrait painting, are a few continuing and current directions in painting at the beginning of the 21st century.
Painting in the Americas.
During the period before and after European exploration and settlement of the Americas, including North America, Central America, South America and the Islands of the Caribbean, the Antilles, the Lesser Antilles and other island groups, indigenous native cultures produced creative works including architecture, pottery, ceramics, weaving, , sculpture, painting and murals as well as other religious and utilitarian objects. Each continent of the Americas hosted societies that were unique and individually developed cultures; that produced totems, works of religious symbolism, and decorative and expressive painted works. African influence was especially strong in the art of the Caribbean and South America. The arts of the indigenous people of the Americas had an enormous impact and influence on European art and vice versa during and after the Age of Exploration. Spain, Portugal, France, The Netherlands, and England were all powerful and influential colonial powers in the Americas during and after the 15th century. By the 19th century cultural influence began to flow both ways across the Atlantic
Islamic painting.
The depiction of humans, animals or any other figurative subjects is forbidden within Islam to prevent believers from idolatry so there is no religiously motivated painting (or sculpture) tradition within Muslim culture. Pictorial activity was reduced to Arabesque, mainly abstract, with geometrical configuration or floral and plant-like patterns. Strongly connected to architecture and calligraphy, it can be widely seen as used for the painting of tiles in mosques or in illuminations around the text of the Koran and other books. In fact, abstract art is not an invention of modern art but it is present in pre-classical, barbarian and non-western cultures many centuries before it and is essentially a decorative or applied art. Notable illustrator M. C. Escher was influenced by this geometrical and pattern-based art. Art Nouveau (Aubrey Beardsley and the architect Antonio Gaudí) re-introduced abstract floral patterns into western art.
Note that despite the taboo of figurative visualization, some Muslim countries did cultivate a rich tradition in painting, though not in its own right, but as a companion to the written word. Iranian or Persian art, widely known as Persian miniature, concentrates on the illustration of epic or romantic works of literature. Persian illustrators deliberately avoided the use of shading and perspective, though familiar with it in their pre-Islamic history, in order to abide by the rule of not creating any lifelike illusion of the real world. Their aim was not to depict the world as it is, but to create images of an ideal world of timeless beauty and perfect order.
In present days, painting by art students or professional artists in Arab and non-Arab Muslim countries follows the same tendencies of Western culture art.
Iran.
Oriental historian Basil Gray believes "Iran has offered a particularly unique ["sic"] art to the world which is excellent in its kind". Caves in Iran's Lorestan province exhibit painted imagery of animals and hunting scenes. Some such as those in Fars Province and Sialk are at least 5,000 years old. Painting in Iran is thought to have reached a climax during the Tamerlane era, when outstanding masters such as Kamaleddin Behzad gave birth to a new style of painting.
Paintings of the Qajar period are a combination of European influences and Safavid miniature schools of painting such as those introduced by Reza Abbasi and classical works by Mihr 'Ali. Masters such as Kamal-ol-molk further pushed forward the European influence in Iran. It was during the Qajar era when "Coffee House painting" emerged. Subjects of this style were often religious in nature depicting scenes from Shia epics and the like.
Africa.
African traditional culture and tribes do not seem to have great interest in two-dimensional representations in favour of sculpture and relief. However, decorative painting in African culture is often abstract and geometrical. Another pictorial manifestation is body painting, and face painting present for example in Maasai and Kĩkũyũ culture in their ceremony rituals. Ceremonial cave painting in certain villages can be found to be still in use. Note that Pablo Picasso and other modern artists were influenced by African sculpture and masks in their varied styles.
Contemporary African artists follow western art movements and their paintings have little difference from occidental art works.
Influence on Western art.
At the start of the 20th century, artists like Picasso, Matisse, Vincent van Gogh, Paul Gauguin and Modigliani became aware of, and were inspired by, African art. In a situation where the established avant garde was straining against the constraints imposed by serving the world of appearances, African Art demonstrated the power of supremely well organised forms; produced not only by responding to the faculty of sight, but also and often primarily, the faculty of imagination, emotion and mystical and religious experience. These artists saw in African art a formal perfection and sophistication unified with phenomenal expressive power.

</doc>
<doc id="13972" url="http://en.wikipedia.org/wiki?curid=13972" title="Hungarian language">
Hungarian language

Hungarian is the official language of Hungary and one of the 24 official languages of the European Union. Outside Hungary it is also spoken by communities of Hungarian people in neighboring countries—especially in Romania, Slovakia, Serbia and Ukraine—and by Hungarian diaspora communities worldwide. Like Finnish and Estonian, it belongs to the Uralic language family, with its closest relatives being Mansi and Khanty. It is one of the few languages of Europe that are not part of the Indo-European family.
The Hungarian name for the language is "magyar" ] or "magyar nyelv" (  ). The word "Magyar" is also used as an English word to refer to Hungarian people as an ethnic group, or to the language.
History.
Classification.
Hungarian is a member of the Uralic language family. Linguistic connections between Hungarian and other Uralic languages were noticed in the 1670s, and the family itself (then called Finno-Ugric) was established in 1717, though the classification of Hungarian as a Uralic/Finno-Ugric rather than Turkic language continued to be a matter of impassioned political controversy through the 18th and into the 19th centuries. Hungarian has traditionally been assigned to a Ugric branch within Uralic/Finno-Ugric, along with the Mansi and Khanty languages of western Siberia (Khanty–Mansia region), but it is no longer clear that this is a valid group. When the Samoyed languages were determined to be part of the family, it was thought at first that Finnic and Ugric (Finno-Ugric) were closer to each other than to the Samoyed branch of the family, but this position has been largely abandoned among specialists.
The name of Hungary could be a result of regular sound changes of "Ungrian/Ugrian", and the fact that the Eastern Slavs referred to Hungarians as "Ǫgry/Ǫgrove" (sg. "Ǫgrinŭ") seemed to confirm that. Current literature favors the hypothesis that it comes from the name of the Turkic tribe Onogur (which means "ten arrows" or "ten tribes").
There are numerous regular sound correspondences between Hungarian and the other Ugric languages. For example, Hungarian /aː/ corresponds to Khanty /o/ in certain positions, and Hungarian /h/ corresponds to Khanty /x/, while Hungarian final /z/ corresponds to Khanty final /t/. For example, Hungarian "ház" ] "house" vs. Khanty "xot" [xot] "house", and Hungarian "száz" [saːz] "hundred" vs. Khanty "sot" [sot] "hundred".
The distance between the Ugric and Finnic languages is greater, but the correspondences are also regular.
Prehistory.
It is thought that Hungarian separated from its Ugric relatives in the first half of the 1st millennium b.c., in western Siberia, east of the southern Urals. The Hungarians gradually changed their lifestyle from settled hunters to nomadic pastoralists (cattle, sheep), probably as a result of early contacts with Iranian nomads (Scythians, Sarmatians). In Hungarian, Iranian loans date back to the time immediately following the breakup of Ugric and probably span well over a millennium. Among these include "tehén" ‘cow’ (cf. Avestan "dhaénu"), "tíz" ‘ten’ (cf. Avestan "dasa"), "tej" ‘milk’ (cf. Persian "dáje" ‘wet nurse’, Kashmiri "dái" ‘milk’), and "nád" ‘reed’ (from late Middle Iranian; cf. Middle Persian "nāy").
A small number of anthropologists disputed this theory, such as Hungarian historian and archaeologist Gyula László who claimed that geological data from pollen analysis seems to contradict placing the ancient homeland of the Hungarians near the Urals. However, increasing archaeological evidence from present-day southern Bashkortostan found in the previous decades confirms the existence of Hungarian settlements between the Volga river and Ural Mountains.
The Onogurs (and Bulgars) later had a great influence on the language, especially between the 5th-9th centuries. This layer of Turkic loans is large and varied (e.g. "szó" ‘word’, from Turkic, "daru" ‘crane’, from the related Permic languages), and includes words borrowed from Oghur Turkic, e.g. "borjú" ‘calf’ (cf. Chuvash "pǝ̂ru" vs. Turkish "buzağı"), "dél" ‘noon; south’ (cf. Chuvash "těl" vs. Turkish dial. "düš"). Many words related to agriculture, to state administration or even to family relations have such backgrounds. Hungarian syntax and grammar were not influenced in a similarly dramatic way during these 300 years.
After the arrival of the Hungarians into the Carpathian Basin the language came into contact with different speech communities (mainly Slavic, Turkic, German and Romanian). Turkic loans from this period come mainly from the Pechenegs and Cumanians who settled in Hungary during the 12th-13th centuries; e.g., "koboz" ‘cobza’ (cf. Turkish "kopuz" ‘lute’), "komondor" ‘mop dog’ (< *"kumandur" < "Cuman"). Hungarian borrowed many words from especially the neighboring Slavic languages (e.g., "tégla" ‘brick’, "mák" ‘poppy’,). In exchange, these languages also borrowed words from Hungarian (such as Serbian "ašov" – "spade"). Approximately 1.6% of the Romanian lexicon is of Hungarian origin.
Old Hungarian.
The first written accounts of Hungarian, mostly personal and place names, are dated back to the 10th century. Hungarians also had their own writing system, the Old Hungarian script, but no significant texts remain from that time, as the usual medium of writing, wooden sticks, is perishable.
The Kingdom of Hungary was founded in 1000, by Stephen I of Hungary. The country was a western-styled Christian (Roman Catholic) state, and Latin held an important position, as was usual in the Middle Ages. The Latin script was adopted to write the Hungarian language and Latin influenced the language. The earliest remaining fragments of the language are found in the establishing charter of the abbey of Tihany from 1055, mixed into Latin text. The first extant text fully written in Hungarian is the Funeral Sermon and Prayer, written in the 1190s. The orthography of these early texts was considerably different from the one used today, but when hearing a reconstructed spoken version, contemporary Hungarians can still understand a large part of it, though both vocabulary and grammar has changed to some extent since then. More extensive Hungarian literature arose after 1300. The earliest known example of Hungarian religious poetry is the 14th-century "Lamentations of Mary". The first Bible translation is the Hussite Bible from the 1430s.
The standard language lost its diphthongs, and several postpositions transformed into suffixes, such as "reá" "onto" (the phrase "utu rea "onto the way" found in the 1055 text would later become "útra). There were also changes in the system of vowel harmony. At one time, Hungarian used six verb tenses; today, only two are commonly used (present and past; future is formed with an auxiliary verb and is usually not counted as a separate tense).
Modern Hungarian.
The first printed Hungarian book was published in Kraków in 1533, by Benedek Komjáti. The work's title is "Az Szent Pál levelei magyar nyelven" (In original spelling: "Az zenth Paal leueley magyar nyeluen"), i.e. "The letters of Saint Paul in the Hungarian language". In the 17th century, the language was already very similar to its present-day form, although two of the past tenses were still used. German, Italian and French loans also appeared in the language by these years. Further Turkish words were borrowed during the Ottoman rule of part of Hungary between 1541 and 1699.
In the 18th century a group of writers, most notably Ferenc Kazinczy, began the process of language renewal (Hungarian: "nyelvújítás"). Some words were shortened ("győzedelem" > "győzelem", 'triumph' or 'victory'); a number of dialectal words spread nationally (e.g. "cselleng" 'dawdle'); extinct words were reintroduced ("dísz" 'décor'); a wide range of expressions were coined using the various derivative suffixes; and some other, less frequently used methods of expanding the language were utilized. This movement produced more than ten thousand words, most of which are used actively today.
The 19th and 20th centuries saw further standardization of the language, and differences between the mutually comprehensible dialects gradually lessened. In 1920, by signing the Treaty of Trianon, Hungary lost 71% of its territory, and along with these, 33% of the ethnic Hungarian population. Today, the language is official in Hungary, and regionally also in Romania, in Slovakia, in Serbia, in Austria and in Slovenia.
Geographic distribution.
Hungarian has about 13 million native speakers, of whom more than 9.8 million live in Hungary. According to the 2011 Hungarian census 9,896,333 people (99.6% of the total population) speak Hungarian, of whom 9,827,875 people (98.9%) speak it as a first language, while 68,458 people (0.7%) speak it as a second language. About 2.2 million speakers live in areas that were part of the Kingdom of Hungary before the Treaty of Trianon (1920). Of these, the largest group lives in Transylvania, the western half of present-day Romania, where there are approximately 1.25 million Hungarians. There are large Hungarian communities also in Slovakia, Serbia and Ukraine, and Hungarians can also be found in Austria, Croatia, and Slovenia, as well as about a million additional people scattered in other parts of the world. For example, there are more than one hundred thousand Hungarian speakers in the Hungarian American community and 1.5 million with Hungarian ancestry in the United States.
Official status.
Hungarian is the official language of Hungary, and thus an official language of the European Union. Hungarian is also one of the official languages of Vojvodina and an official language of three municipalities in Slovenia: Hodoš, Dobrovnik and Lendava, along with Slovene. Hungarian is officially recognized as a minority or regional language in Austria, Croatia, Romania, Zakarpattia in Ukraine, and Slovakia. In Romania it is a recognized minority language used at local level in communes, towns and municipalities with an ethnic Hungarian population of over 20%.
Dialects.
The dialects of Hungarian identified by Ethnologue are: Alföld, West Danube, Danube-Tisza, King's Pass Hungarian, Northeast Hungarian, Northwest Hungarian, Székely and West Hungarian. These dialects are, for the most part, mutually intelligible. The Hungarian Csángó dialect, which is mentioned but not listed separately by Ethnologue, is spoken primarily in Bacău County in eastern Romania. The Csángó Hungarian group has been largely isolated from other Hungarian people, and they therefore preserved a dialect closely resembling an earlier form of Hungarian.
Phonetics and phonology.
Hungarian has 14 vowel phonemes and 25 consonant phonemes. The vowel phonemes can be grouped as pairs of short and long vowels, e.g. "o" and "ó". Most of these pairs have a similar pronunciation, only varying significantly in their duration. However, the pairs "a"/"á" and "e"/"é" differ both in closedness and length.
Consonant length is also distinctive in Hungarian. Most of the consonant phonemes can occur as geminates.
The sound voiced palatal plosive /ɟ/, written ⟨gy⟩, sounds similar to 'd' in British English 'duty' (in fact, more similar to 'd' in French 'dieu', or to the Macedonian phoneme 'ѓ' as in 'ѓакон'). It occurs in the name of the country, "Magyarország" (Hungary), pronounced /ˈmɒɟɒrorsaːɡ/.
Single /r/s are tapped (e.g. "akkora" 'of that size'), double /r/s are trilled (e.g. "akkorra" 'by that time'), similar to the Spanish "pero" and "perro".
Prosody.
Primary stress is always on the first syllable of a word, as in the related Finnish languages and in the neighboring languages Slovak and Czech. There is secondary stress on other syllables in compounds, e.g. "viszontlátásra" ("goodbye") pronounced /ˈvisontˌlaːtaːʃrɒ/. Elongated vowels in non-initial syllables may seem to be stressed to the ear of an English speaker, since length and stress correlate in English.
Grammar.
Hungarian is an agglutinative language. It uses various affixes, mainly suffixes, but also some prefixes and a circumfix to change a word's meaning and grammatical function.
Vowel harmony.
Hungarian uses vowel harmony when attaching suffixes to words. This means that most suffixes have two or three different forms and the choice between them depends on the vowels of the head word.
Nouns.
Nouns have a large number of cases (up to 18, depending on definition), but in general, they are formed regularly with suffixes. The nominative case is unmarked ("az alma" ‘the apple'), and for example, the accusative is marked with the suffix "–t" ("az almát" ‘[I eat] the apple'). Half of the 18 cases express a combination of the source-location-target and surface-inside-proximity ternary distinctions (three times three cases), e.g. there is a separate case ending –b"ól"/"–ből" meaning a combination of source and insideness, i.e. 'from inside of'.
Possession is expressed using a possessive suffix on the possessed object and not on the possessor (Peter's apple becomes "Péter almája", literally 'Peter apple-his'). Noun plurals are formed using the suffix "–k" ("az almák" ‘the apples’) - however, following a numeral, the singular is used (e.g. "két alma" ‘two apples’, literally ‘two apple’; not "*két almák").
Unlike English, Hungarian has no prepositions; instead, it uses case suffixes and postpositions.
There are two types of articles in Hungarian, definite and indefinite, roughly corresponding to the English equivalents.
Adjectives.
Adjectives precede nouns ("a piros alma" ‘the red apple’). They have three degrees: positive ("piros" ‘red’), comparative ("pirosabb" ‘redder’), and superlative (" a legpirosabb" ‘the reddest’). If the noun takes the plural or a case, the adjective, used attributively, does not agree with it: "a piros almák" ‘the red apples’. However, when the adjective is used in a predicative sense, it must agree with the noun: "az almák pirosak" ‘the apples are red’. Adjectives in themselves can behave as nouns (e.g. take case suffixes): "Melyik almát kéred? – A pirosat." 'Which apple would you like? – The red one.'
Verbs.
Verbs are conjugated according to two tenses (past and present), to three moods (indicative, conditional and imperative-subjunctive), to two numbers (singular or plural), to three persons (first, second and third) and to whether the object (if any) is definite. This latter feature is the most characteristic: the definite conjugation is used with a transitive verb whose (direct) object is definite ("Péter nézi az almát." "Peter watches the apple.") and the indefinite conjugation either for a verb with an indefinite direct object ("Péter néz egy almát." "Peter watches an apple.") or for a verb without an object. ("Péter néz." "Peter watches.")
Since conjugation expresses the person and number, personal pronouns are usually omitted, unless they are emphasized.
The Present tense is unmarked, while the past is formed using the suffix –t or –tt: "lát" 'sees'; "látott" 'saw', past. Future may be expressed either with the present tense (usually with a word defining the time of the event, such as "holnap" 'tomorrow'), or using the auxiliary verb "fog" (similar to the English ‘will’) together with the verb’s infinitive.
The indicative mood and the conditional mood are used both in the present and the past tenses. Conditional past is expressed using the conjugated past form and the auxiliary word "volna "("látott volna" 'would have seen')"." The imperative mood is used only with the present tense.
Verbs have verbal prefixes, also known as coverbs. Most of them define direction of movement (as "lemegy" "goes down", "felmegy" "goes up"). Some verbal prefixes give an aspect to the verb, such as the prefix meg-, which generally marks telicity.
Word order.
The neutral word order is subject–verb–object (SVO). However, Hungarian is a topic-prominent language, which means that word order does not only depend on syntax, but also on the topic-comment structure of the sentence (e.g. what aspect is assumed to be known and what is emphasized).
A Hungarian sentence generally has the following order: topic, comment (or focus), verb, other parts.
Putting something into the topic means that the proposition is only stated for that particular thing or aspect, and implies that the proposition is not true for some others. For example, in the sentence ""Az almát János látja." "('John sees the apple', more exactly, 'It is John who sees the apple.'), the apple is in the topic, implying that other objects may not be seen by him, but by other people (the pear may be seen by Peter). The topic part may be empty.
Putting something in the focus means that it is the new information for the listener that he may have not known or where his knowledge must be corrected. For example, in the sentence ""Én vagyok az apád." "('I am your father', more exactly, 'It is me who is your father.') from the movie , the pronoun I ("én") is in the focus, implying that this is new information, and the listener thought that another person is his father.
Note that sometimes this is described as Hungarian having free word order, even though different word orders are generally not interchangeable and the neutral order is not always correct to use. Besides word order, intonation is also different with different topic-comment structures. The topic usually has a rising intonation and the focus has a falling intonation. In the following examples the topic is marked with italics, and the focus (comment) with boldface.
Politeness.
Hungarian has a four-tiered system for expressing levels of politeness.
The four-tiered system has somewhat been eroded due to the recent expansion of "tegeződés"".
Some anomalies emerged with the arrival of multinational companies who have addressed their customers in the "te" (least polite) form right from the beginning of their presence in Hungary. A typical example is the Swedish furniture shop IKEA whose web site and other publications address the customers in "te" form. When a news site asked IKEA - using the "te" form - why they address the customers this way, IKEA's PR Manager explained in his answer - using the "ön"(!) form - that their way of communication reflects IKEA's open-mindedness and the Swedish culture. However IKEA in France use the most polite ("vous") form. Another example for this anomaly is the communication of Telenor (a mobile network operator) towards its customers. Telenor chose to communicate towards business customers in the polite "ön" form while all other customers are addressed in the less polite "te" form.
Lexicon.
Giving an accurate estimate for the total word count is difficult, since it is hard to define what to call "a word" in agglutinating languages, due to the existence of affixed words and compound words. To have a meaningful definition of compound words, we have to exclude such compounds whose meaning is the mere sum of its elements. The largest dictionaries from Hungarian to another language contain 120,000 words and phrases (but this may include redundant phrases as well, because of translation issues). The new desk lexicon of the Hungarian language contains 75,000 words and the Comprehensive Dictionary of Hungarian Language (to be published in 18 volumes in the next twenty years) will contain 110,000 words. The default Hungarian lexicon is usually estimated to comprise 60,000 to 100,000 words. (Independently of specific languages, speakers actively use at most 10,000 to 20,000 words, with an average intellectual using 25–30 thousand words.) However, all the Hungarian lexemes collected from technical texts, dialects etc. would all together add up to 1,000,000 words.
Parts of the Lexicon can be organized using word-bushes. (See an example on the right.) The words in these bushes share a common root, are related through inflection, derivation and compounding, and are usually broadly related in meaning.
The basic vocabulary shares a couple of hundred word roots with other Uralic languages like Finnish, Estonian, Mansi and Khanty. Examples of such include the verb "él" 'live' (Finnish "elää"), the numbers "kettő" 'two', "három" 'three', "négy" 'four' (cf. Mansi китыг "kitig", хурум "khurum", нила "nila",
Finnish "kaksi, kolme, neljä", Estonian "kaks, kolm, neli", ), as well as "víz" 'water', "kéz" 'hand', "vér" 'blood', "fej" 'head' (cf. Finnish and Estonian "vesi, käsi, veri", Finnish "pää", Estonian "pea" or 'pää").
Words for elementary kinship and nature are more Ugric, less r-Turkic and less Slavic. Agricultural words are about 50% r-Turkic and 50% Slavic; pastoral terms are more r-Turkic, less Ugric and less Slavic. Finally, Christian and state terminology is more Slavic and less r-Turkic. The Slavic is most probably proto-Slovakian and/or -Slovenian. This is easily understood in the Uralic paradigm, proto-Magyars were first similar to Ob-Ugors who were mainly hunters, fishers & gatherers, but with some horses, too. Then they accultured to Bulgarian r-Turks, so the older layer of agriculture words (wine, beer, wheat, barley &c.) are purely r-Turkic, and also lots of termini of statemanship & religion were, too.
Except for a few Latin and Greek loan-words, these differences are unnoticed even by native speakers; the words have been entirely adopted into the Hungarian lexicon. There are an increasing number of English loan-words, especially in technical fields.
Another source differs in that loanwords in Hungarian are held to constitute about 45% of bases in the language. Although the lexical percentage of native words in Hungarian is 55%, their use accounts for 88.4% of all words used (the percentage of loanwords used being just 11.6%). Therefore the history of Hungarian has come, especially since the 19th century, to favor neologisms from original bases, whilst still having developed as many terms from neighboring languages in the lexicon.
Word formation.
Words can be compounds or derived. Most derivation is with suffixes, but there is a small set of derivational prefixes as well.
Compounds.
Compounds have been present in the language since the Proto-Uralic era. Numerous ancient compounds transformed to base words during the centuries. Today, compounds play an important role in vocabulary.
A good example is the word "arc":
Compounds are made up of two base words: the first is the prefix, the latter is the suffix. A compound can be "subordinative": the prefix is in logical connection with the suffix. If the prefix is the subject of the suffix, the compound is generally classified as a subjective one. There are objective, determinative, and adjunctive compounds as well. Some examples are given below:
According to current orthographic rules, a subordinative compound word has to be written as a single word, without spaces; however, if the length of a compound of three or more words (not counting one-syllable verbal prefixes) is seven or more syllables long (not counting case suffixes), a hyphen must be inserted at the appropriate boundary to ease the determination of word boundaries for the reader.
Other compound words are "coordinatives": there is no concrete relation between the prefix and the suffix. Subcategories include word duplications (to emphasise the meaning; "olykor-olykor"
'really occasionally'), twin words (where a base word and a distorted form of it makes up a compound: "gizgaz", where the suffix 'gaz' means 'weed' and the prefix "giz" is the distorted form; the compound itself means 'inconsiderable weed'), and such compounds which have meanings, but neither their prefixes, nor their suffixes make sense (for example, "hercehurca" 'complex, obsolete procedures').
A compound also can be made up by multiple (i.e., more than two) base words: in this case, at least one word element, or even both the prefix and the suffix is a compound. Some examples:
Noteworthy lexical items.
Points of the compass.
Hungarian words for the points of the compass are directly derived from the position of the Sun during the day in the Northern hemisphere.
Two words for "red".
There are two basic words for "red" in Hungarian: "piros" and "vörös" (variant: "veres"; compare with Estonian "verev" or Finnish "punainen"). (They are basic in the sense that one is not a sub-type of the other, as the English "scarlet" is of "red".) The word "vörös" is related to "vér", meaning "blood" (Finnish & Estonian "veri"). When they refer to an actual difference in color (as on a color chart), "vörös" usually refers to the deeper (darker and/or more red and less orange) hue of red. In English similar differences exist between "scarlet" and "red". While many languages have multiple names for this color, often Hungarian scholars assume this is unique in recognizing two shades of red as separate and distinct "folk colours".
However, the two words are also used independently of the above in collocations. "Piros" is learned by children first, as it is generally used to describe inanimate, artificial things, or things seen as cheerful or neutral, while "vörös" typically refers to animate or natural things (biological, geological, physical and astronomical objects), as well as serious or emotionally charged subjects.
When the rules outlined above are in contradiction, typical collocations usually prevail. In some cases where a typical collocation does not exist, the use of either of the two words may be equally adequate.
Examples:
Kinship terms.
The Hungarian words for brothers and sisters are differentiated based upon relative age. There is also a general word for sibling, testvér, from "test" = body and "vér" = blood—i.e. originating from the same body and blood.
In addition, there are separate prefixes for several ancestors and descendants:
The words for "boy" and "girl" are applied with possessive suffixes. Nevertheless, the terms are differentiated with different declension or lexemes:
"Fia" is only used in this, irregular possessive form; it has no nominative on its own (see inalienable possession). However, the word "fiú" can also take the regular suffix, in which case the resulting word "(fiúja)" will refer to a lover or partner (boyfriend), rather than a male offspring.
The word "fiú" (boy) is also often noted as an extreme example of the ability of the language to add suffixes to a word, by forming "fiaiéi", adding vowel-form suffixes only, where the result is quite a frequently used word:
Extremely long words.
The above word is often considered to be the longest word in Hungarian, although there are longer words like:
Words of such length are not used in practice, but when spoken they are easily understood by natives. They were invented to show, in a somewhat facetious way, the ability of the language to form long words (see agglutinative language). They are not compound words—they are formed by adding a series of one and two-syllable suffixes (and a few prefixes) to a simple root ("szent", saint or holy).
There is virtually no limit for the length of words, but when too many suffixes are added, the meaning of the word becomes less clear, and the word becomes hard to understand, and will work like a riddle even for native speakers.
Hungarian words in English.
The English word best known as being of Hungarian origin is probably "paprika", from Serbo-Croatian "papar" "pepper" and the Hungarian diminutive "-ka". The most common however is "coach", from "kocsi", originally "kocsi szekér" "car from/in the style of Kocs". Others are:
Writing system.
The Hungarian language was originally written in right-to-left Old Hungarian runes, superficially similar in appearance to the better-known futhark runes but unrelated. When Stephen I of Hungary established the Kingdom of Hungary in the year 1000, the old system was gradually discarded in favour of the Latin alphabet and left-to-right order. Although now not used at all in everyday life, the old script is still known and practiced by some enthusiasts.
Modern Hungarian is written using an expanded Latin alphabet, and has a phonemic orthography, i.e. pronunciation can generally be predicted from the written language. In addition to the standard letters of the Latin alphabet, Hungarian uses several modified Latin characters to represent the additional vowel sounds of the language. These include letters with acute accents "(á,é,í,ó,ú)" to represent long vowels, and umlauts ("ö" and "ü") and their long counterparts "ő" and "ű" to represent front vowels. Sometimes (usually as a result of a technical glitch on a computer) ⟨ô⟩ or ⟨õ⟩ is used for ⟨ő⟩, and ⟨û⟩ for ⟨ű⟩. This is often due to the limitations of the Latin-1 / ISO-8859-1 code page. These letters are not part of the Hungarian language, and are considered misprints. Hungarian can be properly represented with the Latin-2 / ISO-8859-2 code page, but this code page is not always available. (Hungarian is the only language using both ⟨ő⟩ and ⟨ű⟩.) Unicode includes them, and so they can be used on the Internet.
Additionally, the letter pairs ⟨ny⟩, ⟨ty⟩, and ⟨gy⟩ represent the palatal consonants /ɲ/, /c/, and /ɟ/ (a little like the "d+y" sounds in British ""du"ke" or American "woul"d y"ou") – a bit like saying "d" with the tongue pointing to the palate.
Hungarian uses ⟨s⟩ for /ʃ/ and ⟨sz⟩ for /s/, which is the reverse of Polish usage. The letter ⟨zs⟩ is /ʒ/ and ⟨cs⟩ is /t͡ʃ/. These digraphs are considered single letters in the alphabet. The letter ⟨ly⟩ is also a "single letter digraph", but is pronounced like /j/ (English ⟨y⟩), and appears mostly in old words. The letters ⟨dz⟩ and ⟨dzs⟩ /d͡ʒ/ are exotic remnants and are hard to find even in longer texts. Some examples still in common use are "madzag" ("string"), "edzeni" ("to train (athletically)") and "dzsungel" ("jungle").
Sometimes additional information is required for partitioning words with digraphs: házszám ("street number") = "ház" ("house") + "szám" ("number"), not an unintelligible "házs" + "zám".
Hungarian distinguishes between long and short vowels, with long vowels written with acutes. It also distinguishes between long and short consonants, with long consonants being doubled. For example, "lenni" ("to be"), "hozzászólás" ("comment"). The digraphs, when doubled, become trigraphs: ⟨sz⟩ + ⟨sz⟩ = ⟨ssz⟩, e.g. "művésszel" ("with an artist"). But when the digraph occurs at the end of a line, all of the letters are written out. For example ("with a bus"):
When the first lexeme of a compound ends in a digraph and the second lexeme starts with the same digraph, both digraphs are written out: "jegy" + "gyűrű" = "jegygyűrű" ("engagement/wedding ring", "jegy" means "sign", "mark". The term "jegyben lenni/járni" means "to be engaged"; "gyűrű" means "ring").
Usually a trigraph is a double digraph, but there are a few exceptions: "tizennyolc" ("eighteen") is a concatenation of "tizen" + "nyolc". There are doubling minimal pairs: "tol" ("push") vs. "toll" ("feather" or "pen").
While to English speakers they may seem unusual at first, once the new orthography and pronunciation are learned, written Hungarian is almost completely phonemic (except for etymological spellings and "ly, j" representing /j/).
Word order.
The word order is basically from general to specific. This is a typical analytical approach and is used generally in Hungarian.
Name order.
The Hungarian language uses the so-called eastern name order, in which the surname (general, deriving from the family) comes first and the given name comes last. If a second given name is used, this follows the first given name.
Hungarian names in foreign languages.
For clarity, in foreign languages Hungarian names are usually represented in the western name order. Sometimes, however, especially in the neighbouring countries of Hungary – where there is a significant Hungarian population – the Hungarian name order is retained, as it causes less confusion there.
For an example of foreign use, the birth name of the Hungarian-born physicist, the "father of the hydrogen bomb" was Teller Ede, but he immigrated to the USA in the 1930s and thus became known as Edward Teller. Prior to the mid-20th century, given names were usually translated along with the name order; this is no longer as common. For example, the pianist uses "András Schiff" when abroad, not "Andrew Schiff" (in Hungarian "Schiff András"). If a second given name is present, it becomes a middle name and is usually written out in full, rather than truncated to an initial.
Foreign names in Hungarian.
In modern usage, foreign names retain their order when used in Hungarian. Therefore:
Before the 20th century, not only was it common to reverse the order of foreign personalities, they were also "Hungarianised": "Goethe János Farkas" (originally Johann Wolfgang Goethe). This usage sounds odd today, when only a few well-known personalities are referred to using their Hungarianised names, including "Verne Gyula" (Jules Verne), "Marx Károly" (Karl Marx), "Kolumbusz Kristóf" (Christopher Columbus, note that it is also translated in English).
Some native speakers disapprove of this usage; the names of certain historical religious personalities (including popes), however, are always Hungarianised by practically all speakers, such as "Luther Márton" (Martin Luther), "Husz János" (Jan Hus), "Kálvin János" (John Calvin); just like the names of monarchs, for example the king of Spain, Juan Carlos I is referred to as "I. János Károly" or the queen of the UK, Elizabeth II is referred to as "II. Erzsébet".
Japanese names, which are usually written in western order in the rest of Europe, retain their original order in Hungarian, e. g. "Kuroszava Akira" instead of Akira Kurosawa.
Date and time.
The Hungarian convention for date and time is to go from the generic to the specific: 1. year, 2. month, 3. day, 4. hour, 5. minute, (6. second)
The year and day are always written in Arabic numerals, followed by a full stop. The month can be written by its full name or can be abbreviated, or even denoted by Roman or Arabic numerals. Except for the first case (month written by its full name), the month is followed by a full stop. Usually, when the month is written in letters, there is no leading zero before the day. On the other hand, when the month is written in Arabic numerals, a leading zero is common, but not obligatory. Except at the beginning of a sentence, the name of the month always begins with a lower-case letter.
Hours, minutes, and seconds are separated by a colon (H:m:s). Fractions of a second are separated by a full stop from the rest of the time. Hungary generally uses the 24-hour clock format, but in verbal (and written) communication 12-hour clock format can also be used. See below for usage examples.
Date and time may be separated by a comma or simply written one after the other.
Date separated by hyphen is also spreading, especially on datestamps. Here – just like the version separated by full stops – leading zeros are in use.
When only hours and minutes are written in a sentence (so not only "displaying" time), these parts can be separated by a full stop (e.g. "Találkozzunk 10.35-kor." – "Let's meet at 10.35."), or it is also regular to write hours in normal size, and minutes put in superscript (and not necessarily) underlined (e.g. "A találkozó 1035-kor kezdődik." "or" "A találkozó 1035-kor kezdődik." – "The meeting begins at 10.35.").
Also, in verbal and written communication it is common to use "délelőtt" (literally "before noon") and "délután" (lit. "after noon") abbreviated as "de." and "du." respectively. Délelőtt and délután is said or written before the time, e.g. "Délután 4 óra van." – "It's 4 p.m.". However e.g. "délelőtt 5 óra" (should mean "5 a.m.") or "délután 10 óra" (should mean "10 p.m.") are never used, because at these times the sun is not up, instead "hajnal" ("dawn"), "reggel" ("morning"), "este" ("evening") and "éjjel" ("night") is used, however there are no exact rules for the use of these, as everybody uses them according to their habits (e.g. somebody may have woken up at 5 a.m. so he/she says "Reggel 6-kor ettem." – "I had food at "*morning" 6.", and somebody woke up at 11 a.m. so he/she says "Hajnali 6-kor még aludtam." – "I was still sleeping at "*dawn" 6."). Roughly, these expressions mean these times:
Addresses.
Although address formatting is increasingly being influenced by standard European conventions, the traditional Hungarian style is:
1052 Budapest, Deák Ferenc tér 1.
So the order is: 1) (HU-)postcode, 2) settlement (most general), 3) street/square/etc. (more specific), 4) house number (most specific). The house number may be followed by the storey and door numbers. The HU- part before the postcode is only for incoming postal traffic from foreign countries. Addresses on envelopes and postal parcels should be formatted and placed on the right side as follows:
Name of the recipient
Settlement
Street address (up to door number if necessary)
(HU-)postcode
Vocabulary examples.
"Note: The stress is always placed on the first syllable of each word. The remaining syllables all receive an equal, lesser stress. All syllables are pronounced clearly and evenly, even at the end of a sentence, unlike in English."
Controversy over origins.
Today the scientific consensus among linguists is that Hungarian is part of the Uralic family of languages. For many years (from 1869), it was a matter of dispute whether Hungarian was a Finno-Ugric/Uralic language, or was more closely related to the Turkic languages, a controversy known as the "Ugric–Turkish war", or whether indeed both the Uralic and the Turkic families formed part of a superfamily of "Ural–Altaic languages". Hungarians did absorb some Turkic influences during several centuries of cohabitation. For example, it appears that the Hungarians learned animal breeding techniques from the Turkic Chuvash, as a high proportion of words specific to agriculture and livestock are of Chuvash origin. There was also a strong Chuvash influence in burial customs. Furthermore, all Ugric languages, not just Hungarian, have Turkic loanwords related to horse riding.
There have been attempts, dismissed by mainstream linguists as pseudoscientific comparisons, to show that Hungarian is related to other languages including Hebrew, Hunnic, Sumerian, Egyptian, Etruscan, Basque, Persian, Pelasgian, Greek, Chinese, Sanskrit, English, Tibetan, Magar, Quechua, Armenian, Japanese and at least 40 other languages.
Comparison of Uralic words.
Wiktionary: Swadesh lists for Uralic languages

</doc>
<doc id="13974" url="http://en.wikipedia.org/wiki?curid=13974" title="Hymenoptera">
Hymenoptera

The Hymenoptera are the third largest orders of insects, comprising the sawflies, wasps, bees and ants. Over 150,000 species are recognized, with many more remaining to be described. The name refers to the wings of the insects, and is derived from the Ancient Greek ὑμήν ("hymen"): membrane and πτερόν ("pteron"): wing. The hind wings are connected to the fore wings by a series of hooks called hamuli.
Females typically have a special ovipositor for inserting eggs into hosts or otherwise inaccessible places. The ovipositor is often modified into a stinger. The young develop through holometabolism, (complete metamorphosis)—that is, they have a worm-like larval stage and an inactive pupal stage before they mature.
Evolution.
Hymenoptera originated in the Triassic, the oldest fossils belonging to the family Xyelidae. Social hymenopterans appeared during the Cretaceous. The evolution of this group has been intensively studied by A. Rasnitsyn, M. S. Engel, G. Dlussky, and others.
This clade has been studied by examining the mitochondrial DNA. Although this study was unable to resolve all the ambiguities in this clade some relationships could be established. "Aculeata", "Ichneumonomorpha" and "Proctotrupomorpha" were monophyletic. The "Megalyroidea" and"Trigonalyoidea" are sister clades as are the "Chalcidoidea"+"Diaprioidea". The "Cynipoidea" was generally recovered as the sister group to "Chalcidoidea" and "Diaprioidea" which are each other closest relations.
Anatomy.
Hymenopterans range in size from very small to large insects, and usually have two pairs of wings. Their mouthparts are adapted for chewing, with well-developed mandibles (ectognathous mouthparts). Many species have further developed the mouthparts into a lengthy proboscis, with which they can drink liquids, such as nectar. They have large compound eyes, and typically three simple eyes, (ocelli).
The forward margin of the hind wing bears a number of hooked bristles, or "hamuli", which lock onto the fore wing, keeping them held together. The smaller species may have only two or three hamuli on each side, but the largest wasps may have a considerable number, keeping the wings gripped together especially tightly. Hymenopteran wings have relatively few veins compared with many other insects, especially in the smaller species.
In the more ancestral hymenopterans, the ovipositor is blade-like, and has evolved for slicing plant tissues. In the majority, however, it is modified for piercing, and, in some cases, is several times the length of the body. In some species, the ovipositor has become modified as a stinger, and the eggs are laid from the base of the structure, rather than from the tip, which is used only to inject venom. The sting is typically used to immobilise prey, but in some wasps and bees may be used in defense.
The larvae of the more ancestral hymenopterans resemble caterpillars in appearance, and like them, typically feed on leaves. They have large chewing mandibles, three pairs of thoracic limbs, and, in most cases, a number of abdominal prolegs. Unlike caterpillars, however, the prolegs have no grasping spines, and the antennae are reduced to mere stubs.
The larvae of other hymenopterans, however, more closely resemble maggots, and are adapted to life in a protected environment. This may be the body of a host organism, or a cell in a nest, where the adults will care for the larva. Such larvae have soft bodies with no limbs. They are also unable to defecate until they reach adulthood due to having an incomplete digestive tract, presumably to avoid contaminating their environment.
Sex determination.
Among most or all hymenopterans, sex is determined by the number of chromosomes an individual possesses. Fertilized eggs get two sets of chromosomes (one from each parent's respective gametes), and so develop into diploid females, while unfertilized eggs only contain one set (from the mother), and so develop into haploid males; the act of fertilization is under the voluntary control of the egg-laying female. This phenomenon is called haplodiploidy.
However, the actual genetic mechanisms of haplodiploid sex determination may be more complex than simple chromosome number. In many Hymenoptera, sex is actually determined by a single gene locus with many alleles. In these species, haploids are male and diploids heterozygous at the sex locus are female, but occasionally a diploid will be homozygous at the sex locus and develop as a male instead. This is especially likely to occur in an individual whose parents were siblings or other close relatives. Diploid males are known to be produced by inbreeding in many ant, bee and wasp species. Diploid biparental males are usually sterile but a few species that have fertile diploid males are known.
One consequence of haplodiploidy is that females on average actually have more genes in common with their sisters than they do with their own daughters. Because of this, cooperation among kindred females may be unusually advantageous, and has been hypothesized to contribute to the multiple origins of eusociality within this order. In many colonies of bees, ants, and wasps, worker females will remove eggs laid by other workers due to increased relatedness to direct siblings, a phenomenon known as worker policing.
Diet.
Different species of Hymenoptera show a wide range of feeding habits. The most primitive forms are typically herbivorous, feeding on leaves or pine needles. Stinging wasps are predators, and will provision their larvae with immobilised prey, while bees feed on nectar and pollen.
A number of species are parasitoid as larvae. The adults inject the eggs into a paralysed host, which they begin to consume after hatching. Some species are even hyperparasitoid, with the host itself being another parasitoid insect. Habits intermediate between those of the herbivorous and parasitoid forms are shown in some hymenopterans, which inhabit the galls or nests of other insects, stealing their food, and eventually killing and eating the occupant.
Classification.
Symphyta.
The suborder Symphyta includes the sawflies, horntails, and parasitic wood wasps. The group may be paraphyletic, as it has been suggested that the family Orussidae may be the group from which the Apocrita arose. They have an unconstricted junction between the thorax and abdomen. The larvae are herbivorous free-living eruciforms, with three pairs of true legs, prolegs (on every segment, unlike Lepidoptera) and ocelli. The prolegs do not have crochet hooks at the ends unlike the larvae of the Lepidoptera.
Apocrita.
The wasps, bees, and ants together make up the suborder Apocrita, characterized by a constriction between the first and second abdominal segments called a wasp-waist (petiole), also involving the fusion of the first abdominal segment to the thorax. Also, the larvae of all Apocrita do not have legs, prolegs, or ocelli. The hindgut of the larvae also remains closed during development, with feces being stored inside the body, with the exception of some bee larvae where the larval anus through developmental reversion has reappeared again. In general, the anus only opens at the completion of larval growth.
References.
</dl>

</doc>
<doc id="13976" url="http://en.wikipedia.org/wiki?curid=13976" title="Hannibal Hamlin">
Hannibal Hamlin

Hannibal Hamlin (August 27, 1809 – July 4, 1891) was the 15th Vice President of the United States (1861–1865), serving under President Abraham Lincoln during the American Civil War. He was the first Vice President from the Republican Party.
Prior to his election in 1860, Hamlin served in the United States Senate, the House of Representatives, and, briefly, as the 26th Governor of Maine.
Early life.
Hamlin was born to Cyrus Hamlin and his wife Anna, née Livermore, in Paris, Maine. He was a descendant in the sixth generation of English colonist James Hamlin, who had settled in the Massachusetts Bay Colony in 1639, and a great nephew of U.S. Senator Samuel Livermore II of New Hampshire, and a grandson of Stephen Emery, Maine's Attorney General in 1839–1840.
Hamlin attended the district schools and Hebron Academy and later managed his father's farm. He studied law and was admitted to the bar in 1833. He began practicing in Hampden, a suburb of Bangor, where he lived until 1848.
Hamlin married Sarah Jane Emery of Paris Hill in 1833. After Sarah died in 1855, he married her half-sister, Ellen Vesta Emery in 1856. He had four children with Sarah: George, Charles, Cyrus and Sarah, and two, Hannibal E. and Frank, with Ellen. Ellen Hamlin died in 1925.
Political beginnings.
Hamlin's political career began in 1835, when he was elected to the Maine House of Representatives.
Appointed a Major on the staff of Governor John Fairfield, he served with the militia in the bloodless Aroostook War of 1839, and the negotiations he facilitated between Fairfield and Lieutenant Governor John Harvey of New Brunswick helped reduce tensions and make possible the Webster-Ashburton Treaty, which ended the war.
Hamlin unsuccessfully ran for the United States House of Representatives in 1840 and left the State House in 1841. He later served two terms in the United States House of Representatives, from 1843–1847. He was elected to fill a U.S. Senate vacancy in 1848, and to a full term in 1851. A Democrat at the beginning of his career, Hamlin supported the candidacy of Franklin Pierce in 1852.
From the very beginning of his service in Congress, he was prominent as an opponent of the extension of slavery. He was a conspicuous supporter of the Wilmot Proviso and spoke against the Compromise Measures of 1850. In 1854, he strongly opposed the passage of the Kansas-Nebraska Act, which repealed the Missouri Compromise. After the Democratic Party endorsed that repeal at the 1856 Democratic National Convention, on June 12, 1856, he withdrew from the Democratic Party and joined the newly organized Republican Party, causing a national sensation.
The Republicans nominated him for Governor of Maine in the same year. He carried the election by a large majority and was inaugurated on January 8, 1857. In the latter part of February 1857, however, he resigned the governorship, and was again a member of the United States Senate from 1857 to January 1861.
Vice presidency.
In 1861, Hamlin became Vice President under Abraham Lincoln, whom he did not meet until after the election. Maine was the first state in the Northeast to embrace the Republican Party, and the Lincoln-Hamlin ticket thus made sense in terms of regional balance. Hamlin was also a strong orator, and a known opponent of slavery. While serving as Vice President, Hamlin had little authority in the Lincoln Administration, although he urged both the Emancipation Proclamation and the arming of Black Americans. He strongly supported Joseph Hooker's appointment as commander of the Army of the Potomac, which was a dismal failure.
Beginning in 1860, Hamlin was a member of Company A of the Maine Coast Guard, a militia unit. When the company was called up in the summer of 1864, Hamlin was told that because of his position as Vice President, he did not have to take part in the muster. He opted to serve, arguing that he could set an example by doing the duty expected of any citizen, and the only concession made because of his office was that he was quartered with the officers. He reported to Fort McClary in July, initially taking part in routine assignments including guard duty, and later taking over as the company cook. He was promoted to corporal during his service, and mustered out with the rest of his unit in mid-September.
In June 1864, the Republicans and War Democrats joined to form the National Union Party. Although Lincoln was renominated, War Democrat Andrew Johnson of Tennessee was named to replace Hamlin as Lincoln's running mate. Lincoln was seeking to broaden his base of support and was also looking ahead to Southern Reconstruction, at which Johnson had proven himself adept as military governor of occupied Tennessee. Hamlin, by contrast, was an ally of Northern radicals (who would later impeach Johnson). Lincoln and Johnson were elected in November 1864, and Hamlin's term expired on March 4, 1865.
Hamlin and Lincoln were not close personally, but had a good working relationship. At the time, White House etiquette did not require the Vice President to regularly attend cabinet meetings; thus, Hamlin did not regularly visit the White House. It was said that Mary Todd Lincoln and Hamlin disliked each other. For his part, Hamlin complained, "I am only a fifth wheel of a coach and can do little for my friends."
Although Hamlin narrowly missed becoming President, his vice presidency would usher in a half-century of sustained national influence for the Maine Republican Party. In the period 1861–1911, Maine Republicans occupied the offices of Vice President, Secretary of the Treasury (twice), Secretary of State, President pro tempore of the United States Senate, Speaker of the United States House of Representatives (twice), and would field a national presidential candidate in James G. Blaine, a level of influence in national politics unmatched by subsequent Maine political delegations.
After leaving the vice presidency Hamlin served briefly as Collector of the Port of Boston. Appointed to the post by Johnson, Hamlin resigned in protest over Johnson's Reconstruction policy and accompanying efforts to built a political following loyal to him after he had been repudiated by the Republicans. Republicans had supported Johnson as part of the National Union ticket during the war, but opposed him after he became President and his position on Reconstruction deviated from theirs.
Later life and death.
Not content with private life, Hamlin returned to the U.S. Senate in 1869 to serve two more 6-year terms before declining to run for re-election in 1880 because of an ailing heart. His last duty as a public servant came in 1881 when then-Secretary of State James G. Blaine convinced President James A. Garfield to name Hamlin as United States Ambassador to Spain. On June 30, 1881, Hamlin was appointed as the United States Envoy Extraordinary and Minister Plenipotentiary to Spain (the ministerial post's official title until 1913 when it became Ambassador Extraordinary and Plenipotentiary). On December 20, 1881, Hamlin was officially presented with ambassadorship credentials and held the post until October 17, 1882.
Upon returning from Spain in the fall of 1882, Hamlin retired from public life to his home in Bangor, Maine, which he had previously purchased in 1851. The Hannibal Hamlin House – as it is known today – is located in central Bangor at 15 5th Street; incorporating Victorian, Italianate, and Mansard-style architecture, the mansion was posted to the National Register of Historic Places in 1979.
On Independence Day, July 4, 1891, Hamlin collapsed and fell unconscious while playing cards at the Tarratine Club he founded in downtown Bangor. He was then placed on one of the club's couches and died in the evening a few hours later. He was 81. Hannibal Hamlin was buried with honors in the Hamlin family plot at Mount Hope Cemetery in Bangor, Maine. He had survived six of his successors as Vice President: Andrew Johnson, Schuyler Colfax, Henry Wilson, William A. Wheeler, Chester A. Arthur and Thomas A. Hendricks. From June 4, 1887 to March 4, 1889, he was the only living Vice President.
Family.
Hamlin had four sons who grew to adulthood: Charles Hamlin, Cyrus Hamlin, Hannibal Emery and Frank Hamlin. Charles and Cyrus served in the Union forces during the Civil War, both becoming generals, Charles by brevet. Cyrus was among the first Union officers to argue for the enlistment of black troops, and himself commanded a brigade of freemen in the Mississippi River campaign. Charles and sister Sarah were present at Ford's Theater the night of Lincoln's assassination. Hannibal Emery Hamlin was Maine Attorney General from 1905 to 1908. Hannibal Hamlin's great-granddaughter Sally Hamlin was a child actor who made many spoken word recordings for the Victor Talking Machine Company in the early years of the 20th century.
Hannibal's older brother, Elijah Livermore Hamlin, was president of the Mutual Fire Insurance Co. of Bangor, and the Bangor Institution for Savings. He was twice an unsuccessful candidate for Governor of Maine in the late 1840s, though he did serve as Mayor of Bangor in 1851–52. The brothers were members of different political parties (Hannibal a Democrat, and Elijah a Whig) before both becoming Republican in the later 1850s.
Hannibal's nephew (Elijah's son) Augustus Choate Hamlin was a physician, artist, mineralogist, author, and historian. He was also Mayor of Bangor in 1877–78, and a founding member of the Bangor Historical Society. Augustus served as surgeon in the 2nd Maine Volunteer Infantry Regiment during the Civil War, eventually becoming a U.S. Army Medical Inspector, and later the Surgeon General of Maine. He wrote books about Andersonville Prison and the Battle of Chancellorsville.
Hannibal's first cousin Cyrus Hamlin, who was a graduate of the Bangor Theological Seminary, became a missionary in Turkey, where he founded Robert College. He later became president of Middlebury College in Vermont. His son, A. D. F. Hamlin, Hannibal's first cousin once removed, became a professor of architecture at Columbia University and a noted architectural historian.
There are biographies of Hamlin by his grandson Charles E. Hamlin (published 1899, reprinted 1971) and by H. Draper Hunt (published 1969).
Honors.
Hamlin County in South Dakota is named in his honor, as are Hamlin, Kansas, Hamlin, New York, Hamlin, West Virginia, and both Hamlin Township and Hamlin Lake in Mason County, Michigan. There are statues in Hamlin's likeness in the United States Capitol and in a public park (Norumbega Mall) in Bangor, Maine. There is also a building on the University of Maine Campus, in Orono, named Hannibal Hamlin Hall. This burned down in 1945, in a fire that killed two students, but was subsequently rebuilt. Hannibal Hamlin Memorial Library is next to his birthplace in Paris, Maine.
Hamlin's house in Bangor subsequently housed the Presidents of the adjacent Bangor Theological Seminary. It is listed on the National Register of Historic Places, as is Hamlin's house in Paris, Maine.

</doc>
<doc id="13978" url="http://en.wikipedia.org/wiki?curid=13978" title="Hopwood Award">
Hopwood Award

The Hopwood Awards are a major scholarship program at the University of Michigan, founded by Avery Hopwood.
Under the terms of the will of Avery Hopwood, a prominent American dramatist and member of the Class of 1905 of The University of Michigan, one-fifth of Mr. Hopwood's estate was given to the Regents of the University for the encouragement of creative work in writing. The first awards were made in 1931, and today the Hopwood Program offers approximately $120,000 in prizes every year to aspiring writers at the University of Michigan. According to Nicholas Delbanco, UM English Professor and Director of the Hopwood Awards Program, "This is the oldest and best known series of writing prizes in the country and it is a very good indicator of future success."
Previous Hopwood winners include Joe Dassin, Brett Ellen Block, Max Apple, Lorna Beers, Sven Birkerts, Anne Stevenson, John Malcolm Brinnin, John Ciardi, Tom Clark, Lyn Coffin, Cid Corman, Christopher Paul Curtis, Mary Gaitskill, Robert Hayden, Garrett Hongo, Lawrence Joseph, Jane Kenyon, Joe Salerno, Laura Kasischke, Elizabeth Kostova, Rita Lakin, Gregory Loselle, Arthur Miller, Howard Moss, Davi Napoleon, Frank O'Hara, Marge Piercy, William Craig Rice, Ari Roth, Davy Rothbart, Betty Smith, Ron Sproat, Cynthia Haven, Keith Waldrop, Rosmarie Waldrop, Edmund White, Nancy Willard, Beth Tanenhaus Winsten, and Maritta Wolff. 
Contests and prizes.
The Graduate and Undergraduate Hopwood Contest.
Awards are offered in the following genres: drama/screenplay, essay, the novel, short fiction, Nonfiction, and poetry. These awards are classified under two categories, Graduate or Undergraduate, except the novel and drama/screenplay, which are combined categories. Award amounts for this contest vary, but usually fall in the range of $1000 to $6000.
Summer Hopwood Contest.
This contest is open only to students who take writing courses during spring and summer terms. Awards are given in the categories of Drama or Screenplay, Nonfiction, Short Fiction, and Poetry. Novels are not eligible for the Summer Hopwood Contest.
Hopwood Underclassmen Contest.
This contest is open only to freshmen and sophomores who are enrolled in writing courses. Awards are given in the categories of Nonfiction, Fiction, and Poetry.

</doc>
<doc id="13979" url="http://en.wikipedia.org/wiki?curid=13979" title="Hopwood Program">
Hopwood Program

The Hopwood Program administers the University of Michigan Hopwood Award in literature, as well as several other awards in writing. It is located in the Hopwood Room at the University of Michigan and serves the needs and interests of Hopwood contestants. The Room was established by Professor Roy W. Cowden, Director of the Hopwood Awards from 1933 to 1952, who generously contributed a part of his library, which has grown through the addition of many volumes of contemporary literature. In addition to housing the winning manuscripts from the past years of the contests, the Hopwood Room has a lending library of twentieth -century literature, a 
generous supply of non-circulating current periodicals, some reference 
books on how to get published, information on graduate and summer writing 
programs, and a collection of screen plays donated by former Hopwood winner 
Lawrence Kasdan.
Prizes Administered by the Hopwood Program.
The Hopwood Program also administers the following writing contests: 
External links.
see literature, University of Michigan, Arthur Miller, Hopwood Award

</doc>
<doc id="13980" url="http://en.wikipedia.org/wiki?curid=13980" title="Homeostasis">
Homeostasis

Homeostasis, also spelled homoeostasis (from Greek: ὅμοιος "homœos", "similar" and στάσις "stasis", "standing still"), is the property of a system in which variables are regulated so that internal conditions remain stable and relatively constant. Examples of homeostasis include the regulation of temperature and the balance between acidity and alkalinity (pH). It is a process that maintains the stability of the human body's internal environment in response to changes in external conditions.
The concept was described by French physiologist Claude Bernard in 1865 and the word was coined by Walter Bradford Cannon in 1926. Although the term was originally used to refer to processes within living organisms, it is frequently applied to automatic control systems such as thermostats. Homeostasis requires a sensor to detect changes in the condition to be regulated, an effector mechanism that can vary that condition; and a negative feedback connection between the two.
Biological.
All living organisms depend on maintaining a complex set of interacting metabolic chemical reactions. From the simplest unicellular organisms to the most complex plants and animals, internal processes operate to keep the conditions within tight limits to allow these reactions to proceed. Homeostatic processes act at the level of the cell, the tissue, and the organ, as well as for the organism as a whole.
Principal Homeostatic processes include the following:
Control mechanisms.
All homeostatic control mechanisms have at least three interdependent components for the variable being regulated: The receptor is the sensing component that monitors and responds to changes in the environment. When the receptor senses a stimulus, it sends information to a "control center", the component that sets the range at which a variable is maintained. The control center determines an appropriate response to the stimulus. The control center then sends signals to an effector, which can be muscles, organs, or other structures that receive signals from the control center. After receiving the signal, a change occurs to correct the deviation by depressing it with negative feedback.
Negative feedback.
Negative feedback mechanisms consist of reducing the output or activity of any organ or system back to its normal range of functioning. A good example of this is regulating blood pressure. Blood vessels can sense resistance of blood flow against the walls when blood pressure increases. The blood vessels act as the receptors and they relay this message to the brain. The brain then sends a message to the heart and blood vessels, both of which are the effectors. The heart rate would decrease as the blood vessels increase in diameter (known as vasodilation). This change would cause the blood pressure to fall back to its normal range. The opposite would happen when blood pressure decreases, and would cause vasoconstriction.
Another important example is seen when the body is deprived of food. The body would then reset the metabolic set point to a lower than normal value. This would allow the body to continue to function, at a slower rate, even though the body is starving. Therefore, people depriving themselves of food while trying to lose weight would find it easy to shed weight initially and much harder to lose more after. This is due to the body's readjusting itself to a lower metabolic set-point to allow the body to survive with its low supply of energy. Exercise can change this effect by increasing the metabolic demand.
Another good example of negative feedback mechanism is temperature control. The hypothalamus, which monitors the body temperature, is capable of determining even the slightest variation of normal body temperature (36.5 degrees Celsius). Response to such variation could be stimulation of glands that produce sweat to reduce the temperature or signaling various muscles to shiver to increase body temperature.
Both feedbacks are equally important for the healthy functioning of one's body. Complications can arise if any of the two feedbacks are affected or altered in any way.
Homeostatic imbalance.
Many diseases involve a disturbance of homeostasis.
As the organism ages, the efficiency in its control systems becomes reduced. The inefficiencies gradually result in an unstable internal environment that increases the risk of illness, and leads to the physical changes associated with aging.
Certain homeostatic imbalances, such as high core temperature, a high concentration of salt in the blood, or low concentration of oxygen, can generate homeostatic emotions (such as warmth, thirst, or breathlessness), which motivate behavior aimed at restoring homeostasis (such as removing a sweater, drinking or slowing down).
Examples from technology.
The following are all examples of familiar homeostatic mechanisms:
Ecological.
The concept of homeostasis is central to the topic of Ecological Stoichiometry. There, it refers to the relationship between the chemical composition of an organism and the chemical composition of the nutrients it consumes. Stoichiometric homeostasis helps explain nutrient recycling and population dynamics.
Throughout history, ecological succession was seen as having a stable end-stage called the climax (see Frederic Clements), sometimes referred to as the 'potential biodiversity' of a site, shaped primarily by the local climate. This idea has been largely abandoned by modern ecologists in favor of nonequilibrium ideas of how ecosystems function, as most natural ecosystems experience disturbance at a rate that makes a "climax" community unattainable.
Only on small, isolated habitats known as ecological islands can the phenomenon be observed. One such case study is the island of Krakatoa after its major eruption in 1883: the established stable homeostasis of the previous forest climax ecosystem was destroyed, and all life was eliminated from the island. In the years after the eruption, Krakatoa went through a sequence of ecological changes in which successive groups of new plant or animal species followed one another, leading to increasing biodiversity and eventually culminating in a re-established climax community. This "ecological succession" on Krakatoa occurred in a number of stages; a "sere" is defined as "a stage in a sequence of events by which succession occurs". The complete chain of seres leading to a climax is called a "prisere". In the case of Krakatoa, the island reached its climax community, with eight hundred different recorded species, in 1983, one hundred years after the eruption that cleared all life off the island. Evidence confirms that this number has been homeostatic for some time, with the introduction of new species rapidly leading to elimination of old ones. The evidence of Krakatoa, and other disturbed island ecosystems, has confirmed many principles of Island Biogeography, mimicking general principles of ecological succession albeit in a virtually closed system comprised almost exclusively of endemic species.
Biosphere.
In the Gaia hypothesis, James Lovelock stated that the entire mass of living matter on Earth (or any planet with life) functions as a vast homeostatic superorganism that actively modifies its planetary environment to produce the environmental conditions necessary for its own survival. In this view, the entire planet maintains homeostasis. Whether this sort of system is present on Earth is still open to debate. However, some relatively simple homeostatic mechanisms are generally accepted. For example, it is sometimes claimed that when atmospheric carbon dioxide levels rise, certain plants are able to grow better and thus act to remove more carbon dioxide from the atmosphere. However, warming has exacerbated droughts, making water the actual limiting factor on land. When sunlight is plentiful and atmospheric temperature climbs, it has been claimed that the phytoplankton of the ocean surface waters may thrive and produce more dimethyl sulfide, DMS. The DMS molecules act as cloud condensation nuclei, which produce more clouds, and thus increase the atmospheric albedo, and this feeds back to lower the temperature of the atmosphere. However, rising sea temperature has stratified the oceans, separating warm, sunlit waters from cool, nutrient-rich waters. Thus, nutrients have become the limiting factor, and plankton levels have actually fallen over the past 50 years, not risen. As scientists discover more about Earth, vast numbers of positive and negative feedback loops are being discovered, that, together, maintain a metastable condition, sometimes within very broad range of environmental conditions.
Environmental pressure, such as competition or change in temperature, can lead to adaptation/extinction of species over time.
Reactive.
Example of use: "Reactive homeostasis is an immediate homeostatic response to a challenge such as predation."
However, "any" homeostasis is impossible without reaction—because homeostasis is and must be a "feedback" phenomenon.
The phrase "reactive homeostasis" is simply short for "reactive compensation reestablishing homeostasis", that is to say, "reestablishing a point of homeostasis"; it should not be confused with a separate "kind" of homeostasis or a distinct phenomenon "from" homeostasis. It is simply the compensation (or compensatory) phase of homeostasis.
Other fields.
The term has come to be used in other fields, for example:
Risk.
An actuary may refer to "risk homeostasis", where (for example) people that have anti-lock brakes have no better safety record than those without anti-lock brakes, because the former unconsciously compensate for the safer vehicle via less-safe driving habits. Previous to the innovation of anti-lock brakes, certain maneuvers involved minor skids, evoking fear and avoidance: Now the anti-lock system moves the boundary for such feedback, and behavior patterns expand into the no-longer punitive area. It has also been suggested that ecological crises are an instance of risk homeostasis in which a particular behavior continues until proven dangerous or dramatic consequences actually occur.
Stress.
Sociologists and psychologists may refer to "stress homeostasis", the tendency of a population or an individual to stay at a certain level of stress, often generating artificial stresses if the "natural" level of stress is not enough. 
Jean-François Lyotard, a postmodern theorist, has applied this term to societal 'power centers' that he describes as being 'governed by a principle of homeostasis,' for example, the scientific hierarchy, which will sometimes ignore a radical new discovery for years because it destabilises previously accepted norms. (See "" by Jean-François Lyotard)
Psychological.
Author George Leonard discusses in his book "Mastery" how homeostasis affects our behavior and who we are. He states that homeostasis will prevent our body from making drastic changes and maintain stability in our lives even if it is detrimental to us. Examples include when an obese person starts exercising, homeostasis in the body resists the activity to maintain stability. Another example Leonard uses is an unstable family where the father has been a raging alcoholic and suddenly stops and the son starts up a drug habit to maintain stability in the family. Homeostasis is the main factor that stops people changing their habits because our bodies view change as dangerous unless it is very slow. Leonard discusses this dilemma, as the media today encourages only fast change and quick results. The opening of his book describes his despair with the current state of the world and how it is at war with homeostasis. "The trouble is that we have few, if any, maps to guide us on the journey or even to show us how to find the path. The modern world, in fact, can be viewed as a prodigious conspiracy against mastery. We're continually bombarded with the promises of immediate gratification, instant success, and fast, temporary relief, all of which lead in exactly the wrong direction."

</doc>
<doc id="13981" url="http://en.wikipedia.org/wiki?curid=13981" title="Hockey">
Hockey

Hockey is a family of sports in which two teams play against each other by trying to maneuver a ball or a puck into the opponent's goal using a hockey stick. In many areas, one sport (typically field hockey or ice hockey) is generally referred to simply as hockey.
Etymology.
The first recorded use of the word "hockey" is from the 1773 book "Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education", by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled "New Improvements on the Game of Hockey". The belief that hockey was mentioned in a 1363 proclamation by King Edward III of England is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games "Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam". The English historian and biographer John Strype did not use the word "hockey" when he translated the proclamation in 1720.
The word "hockey" itself is of unknown origin. One explanation is that it is a derivative of "hoquet", a Middle French word for a shepherd's stave. The curved, or "hooked" ends of the sticks used for hockey would indeed have resembled these staves. Another explanation is that the cork bungs that replaced wooden balls in the 18th century came from barrels containing "Hock" ale, also called "Hocky".
History.
Games played with curved sticks and a ball can be found in the histories of many cultures. In Egypt, 4000-year-old carvings feature teams with sticks and a projectile, hurling dates to before 1272 BC in Ireland, and there is a depiction from approximately 600 BC in Ancient Greece where the game may have been called "kerētízein" or "kerhtízein" (κερητίζειν) because it was played with a horn or horn-like stick("kéras", κέρας) In Inner Mongolia, the Daur people have been playing "beikou", a game similar to modern field hockey, for about 1,000 years.
Most evidence of hockey-like games during the Middle Ages is found in legislation concerning sports and games. The Galway Statute enacted in Ireland in 1527 banned certain types of ball games, including games using "hooked" (written "hockie", similar to "hooky") sticks.
 ...at no tyme to use ne occupye the horlinge of the litill balle with hockie stickes or staves, nor use no hande ball to play withoute walles, but only greate foote balle
By the 19th century, the various forms and divisions of historic games began to differentiate and coalesce into the individual sports defined today. Organizations dedicated to the codification of rules and regulations began to form, and national and international bodies sprung up to manage domestic and international competition. Ice hockey also evolved during this period as a derivative of field hockey adapted to the icy conditions of Canada and the northern United States.
Subtypes.
Field hockey.
Field hockey is played on gravel, natural grass, sand-based or water-based artificial turf, with a small, hard ball approximately 73 mm (2.9 in) in diameter. The game is popular among both males and females in many parts of the world, particularly in Europe, Asia, Australia, New Zealand, South Africa, and Argentina. In most countries, the game is played between single-sex sides, although they can be mixed-sex.
The governing body is the 126-member International Hockey Federation (FIH). Men's field hockey has been played at each summer Olympic Games since 1908 (except 1912 and 1924), while women's field hockey has been played at the Summer Olympic Games since 1980.
Modern field hockey sticks are J-shaped and constructed of a composite of wood, glass fibre or carbon fibre (sometimes both) and have a curved hook at the playing end, a flat surface on the playing side and curved surface on the rear side. All sticks are right-handed – left-handed sticks are not permitted.
While current field hockey appeared in mid-18th century England, primarily in schools, it was not until the first half of the 19th century that it became firmly established. The first club was created in 1849 at Blackheath in south-east London. Field hockey is the national sport of Pakistan. It was the national sport of India until the Ministry of Youth Affairs and Sports declared that India has no national sport in August 2012.
Ice hockey.
Ice hockey is played on a large flat area of ice, using a three-inch-diameter (76.2 mm) vulcanized rubber disc called a puck. This puck is often frozen before high-level games to decrease the amount of bouncing and friction on the ice. The game is contested between two teams of skaters. The game is played all over North America, Europe and in many other countries around the world to varying extent. It is the most popular sport in Canada, Finland, Latvia, the Czech Republic, and Slovakia.
The governing body of international play is the 72-member International Ice Hockey Federation (IIHF). Men's ice hockey has been played at the Winter Olympics since 1924, and was in the 1920 Summer Olympics. Women's ice hockey was added to the Winter Olympics in 1998. North America's National Hockey League (NHL) is the strongest professional ice hockey league, drawing top ice hockey players from around the globe. The NHL rules are slightly different from those used in Olympic ice hockey over many categories.
Ice hockey sticks are long L-shaped sticks made of wood, graphite, or composites with a blade at the bottom that can lie flat on the playing surface when the stick is held upright and can curve either way, legally, as to help a left- or right-handed player gain an advantage.
Various stick and ball games similar to field hockey, bandy and other games where two teams push a ball or object back and forth with sticks were played on ice under the name "hockey" in England throughout the 19th century, and even earlier under various other names. In Canada, there are 24 reports of hockey-like games in the 19th century before 1875 (five of them using the name "hockey"). The first organized indoor game of ice hockey was played in Montreal, Canada on March 3, 1875 and featured several McGill University students. The contemporary sport developed in Canada from these and other influences. International ice hockey rules were adopted from Canadian rules in the early 1900s.
Ice hockey is the national sport of Latvia and the national winter sport of Canada.
Ice hockey is played at a number of levels, by all ages. 
Roller hockey (quad).
Roller hockey, also known as quad hockey, international-style ball hockey, and Hoquei em Patins is an overarching name for a roller sport that has existed since long before inline skates were invented. This sport is played in over sixty countries and has a worldwide following. Roller hockey was a demonstration sport at the 1992 Barcelona Summer Olympics.
Roller hockey (inline).
Inline hockey is a variation of roller hockey very similar to ice hockey, from which it is derived. Inline hockey is played by two teams, consisting of four skaters and one goalie, on a dry rink divided into two halves by a center line, with one net at each end of the rink. The game is played in three 15-minute periods with a variation of the ice hockey off-side rule. Icings are also called, but are usually referred to as illegal clearing. For rink dimensions and an overview of the rules of the game, see IIHF Inline Rules (). Some leagues and competitions do not follow the IIHF regulations, in particular and .
Sledge hockey.
Sledge hockey is a form of ice hockey designed for players with physical disabilities affecting their lower bodies. Players sit on double-bladed sledges and use two sticks; each stick has a blade at one end and small picks at the other. Players use the sticks to pass, stickhandle and shoot the puck, and to propel their sledges. The rules are very similar to IIHF ice hockey rules.
Canada is a recognized international leader in the development of the sport, and of equipment for players. Much of the equipment for the sport was first developed in Canada, such as sledge hockey sticks laminated with fiberglass, as well as aluminum shafts with hand carved insert blades and special aluminum sledges with regulation skate blades.
Based on ice sledge hockey, inline sledge hockey is played to the same rules as inline puck hockey (essentially ice hockey played off ice using inline skates) and has been made possible by the design and manufacture of inline sledges by RGK, Europe’s premier sports wheelchair maker.
There is no classification point system dictating who can play inline sledge hockey, unlike the situation with other team sports such as wheelchair basketball and wheelchair rugby. Inline sledge hockey is being developed to allow everyone, regardless of whether they have a disability or not, to complete up to world championship level based solely on talent and ability. This makes inline sledge hockey truly inclusive.
The first game of inline sledge hockey was played at Bisley, England, on the 19th of December 2009 between the Hull Stingrays and the Grimsby Redwings. Matt Lloyd is credited with inventing inline sledge hockey, and Great Britain is seen as the international leader in the game's development.
Street hockey.
Also known as road hockey, this is a dry-land variant of ice and roller hockey played on a hard surface (usually asphalt). Most of the time, a ball is used instead of a puck, and generally no protective equipment is worn. Street hockey is played year round.
Other forms of hockey.
Other games derived from hockey or its predecessors include the following:

</doc>
<doc id="13983" url="http://en.wikipedia.org/wiki?curid=13983" title="Hawick">
Hawick

Hawick ( ; Scots: "Haaick", Scottish Gaelic: "Hamhaig") is a town in the Scottish Borders council area and historic county of Roxburghshire in the east Southern Uplands of Scotland. It is 10.0 mi south-west of Jedburgh and 8.9 mi south-southeast of Selkirk. It is one of the farthest towns from the sea in Scotland, in the heart of Teviotdale, and the biggest town in the former county of Roxburghshire. Hawick's architecture is distinctive in that it has many sandstone buildings with slate roofs. The town is at the
confluence of the Slitrig Water with the River Teviot. Hawick is known for its yearly Common Riding, for its rugby team Hawick Rugby Football Club and for its knitwear industry.
At the 2001 census Hawick had a resident population of 14,801. By 2011, this had reduced to 14,294.
Monuments.
The west end of the town contains "the Mote", the remains of a Norman motte-and-bailey. In the centre of the High Street is the Scots baronial style Town Hall, built in 1886, and the east end has an equestrian statue, known as "The Horse", erected in 1914. Drumlanrig's Tower, now a museum, dates largely from the mid-16th century.
Economy.
Companies such as Hawick Cashmere, , Johnstons of Elgin, Lyle & Scott, Peter Scott, Pringle of Scotland, and Scott and Charters, all have had and in many cases still have manufacturing plants in Hawick, producing some of the most luxurious cashmere and merino wool knitwear in the world today. The first knitting machine was brought to Hawick in 1771 by John Hardie, building on an existing carpet manufacturing trade. Originally based on linen, this quickly moved to wool and factories multiplied, driving the growth of the town. Engineering firm Turnbull and Scott previously had their headquarters in an Elizabethan-style listed building on Commercial Road before moving to Burnfoot.
Transport.
Hawick lies in the centre of the valley of the Teviot. The A7 Edinburgh to Carlisle road passes through the town, with main roads also leading to Berwick upon Tweed (the A698) and Newcastle upon Tyne (the A6088, which joins the A68 at the Carter Bar, 16 mi south-east of Hawick).
Despite a great deal of local opposition the town lost its rail service in 1969, when as part of the controversial Beeching Axe the 'Waverley Line' from Carlisle to Edinburgh via Hawick was closed. It is now said to be the farthest large town from a railway station in the United Kingdom. Regular buses serve the railway station at Carlisle, 42 mi away. Part of the former Waverley line is currently being rebuilt from Edinburgh to Tweedbank and is due to open in the summer of 2015. Campaigners are lobbying for the line to be extended to Hawick and then on to Carlisle.
The nearest major airports are at Edinburgh, 57 mi away, and Newcastle, 55 mi away.
Culture and traditions.
The town hosts the annual Common Riding, which combines the annual riding of the boundaries of the town's common land with the commemoration of a victory of local youths over an English raiding party in 1514. In March 2007, this was described by the "Rough Guide" publication "World Party" as one of the best parties in the world.
People from Hawick call themselves "Teries", after a traditional song which includes the line "Teribus ye teri odin".
Teri Talk.
Many Hawick residents speak the local dialect of Border Scots which is informally known as "". It is similar (but not identical by any means) to the dialects spoken in surrounding towns, especially Jedburgh, Langholm and Selkirk. The speech of this general area was described in "Dialect of the Southern Counties of Scotland" (1873) by James Murray, considered the first systematic study of any dialect. The Hawick tongue retains many elements of Old English, together with particular vocabulary, grammar and pronunciation. Its distinctiveness arose from the relative isolation of the town.
Sports.
The town is the home of Hawick Rugby Football Club and a senior football team, Hawick Royal Albert, who currently play in the East of Scotland Football League.
Rivalry between the small Border towns is generally played out on the rugby union field. The historical competition continues to this day, as Hawick's main rival is the similarly-sized town of Galashiels.
The Hawick Baw game was once played here by the 'uppies' and the 'doonies' on the first Monday after the new moon in the month of February. The river of the town formed an important part of the pitch. Although no longer played at Hawick, it is still played at nearby Jedburgh.
Tourism.
This Abbey is part of five other abbeys and historic sights though Scotland on Borders Abbeys Way walk.
See also.
Hawick's villages:

</doc>
<doc id="13985" url="http://en.wikipedia.org/wiki?curid=13985" title="Hatfield, Hertfordshire">
Hatfield, Hertfordshire

Hatfield is a town and civil parish in Hertfordshire, England, in the borough of Welwyn Hatfield. It had a population of 29,616 in 2001, and is of Saxon origin. Hatfield House, the home of the Marquess of Salisbury, is the nucleus of the old town. From the 1930s when de Havilland opened a factory until the 1990s when British Aerospace closed, Hatfield was associated with aircraft design and manufacture, which employed more people than any other industry. Hatfield was one of the post-war New Towns built around London and has much modernist architecture from the period. The University of Hertfordshire is based there. Hatfield is 20 miles north of London. A train service runs directly from Hatfield Station to Kings Cross, taking approximately 20 minutes on the fast service.
History.
Early history.
In the Saxon period Hatfield was known as Hetfelle, but by the year 970, when King Edgar gave 5000 acre to the monastery of Ely, it had become known as Haethfeld. Hatfield is mentioned in the Domesday Book as the property of the Abbey of Ely, and unusually, the original census data which compilers of Domesday used still survives, giving us slightly more information than got into the final Domesday record. No other records remain from that time until 1226, when Henry III granted the Bishops of Ely rights to an annual four-day fair and a weekly market. The town was then called Bishop's Hatfield. 
Hatfield House is the seat of the Cecil family, the Marquesses of Salisbury. Elizabeth Tudor was confined there for three years in what is now known as "The Old Palace" in Hatfield Park. Legend has it that it was here in 1558, while sitting under an oak tree in the Park, that she learned that she had become Queen following the death of her half-sister, Queen Mary I. She held her first Council in the Great Hall (The Old Palace) of Hatfield. In 1851, the route of the Great North Road (now the A1000) was altered to avoid cutting through the grounds of Hatfield House.
The town grew up around the gates of Hatfield House. Old Hatfield retains many historic buildings, notably the Old Palace, St Etheldreda's Church and Hatfield House. The Old Palace was built by the Bishop of Ely, Cardinal Morton, in 1497, during the reign of Henry VII, and the only surviving wing is still used today for Elizabethan-style banquets. St Etheldreda's Church was founded by the monks from Ely, and the first wooden church, built in 1285, was probably sited where the existing building stands overlooking the old town.
Aerospace industry.
In 1930 the de Havilland airfield and aircraft factory was opened at Hatfield and by 1949 it had become the largest employer in the town, with almost 4,000 staff. It was taken over by Hawker Siddeley in 1960 and merged into British Aerospace in 1978. In the 1930s it produced a range of small biplanes. During the Second World War it produced the Mosquito fighter bomber and developed the Vampire, the second British production jet aircraft after the Gloster Meteor. After the war, facilities were expanded and it developed the Comet airliner (the world's first production jet liner), the Trident airliner, and an early bizjet, the DH125.
British Aerospace closed the Hatfield site in 1993 having moved the BAe 146 production line to Woodford Aerodrome. The land was used as a film set for Steven Spielberg's movie "Saving Private Ryan" and most of the BBC/HBO television drama "Band of Brothers". It was later developed for housing, higher education, commerce and retail. Part of the former British Aerospace site was intended to be the site of a £500 million new hospital to replace the Queen Elizabeth II Hospital in Welwyn GC and a new campus for Oaklands College, but both projects were cancelled.
Today, Hatfield's aviation history is remembered by the names of certain local streets and pubs (e.g. Comet Way, The Airfield, Dragon Road) as well as The Comet Hotel (now owned by Ramada) built in the 1930s. (The Harrier Pub (formerly The Hilltop) is actually named after the Harrier Bird, not the aircraft, hence the original pub sign of a Harrier Bird.) The de Havilland Aircraft Heritage Centre, at Salisbury Hall in nearby London Colney, preserves and displays many historic de Havilland aeroplanes and related archives.
New Town.
After the Second World War, Hatfield was designated a New Town under the New Towns Act 1946 (and the earlier Abercrombie Plan for London in 1944), forming part of the initial Hertfordshire group with nearby Stevenage, Welwyn Garden City and Letchworth. The Government designated 2340 acre for Hatfield New Town, with a population target of 25,000. (By 2001 the population had reached 27,833.) The Hatfield Development Corporation, tasked with creating the New Town, chose to build a new town centre, rejecting Old Hatfield because it was on the wrong side of the railway, without space for expansion and "with its intimate village character, out of scale with the town it would have to serve." They chose instead St Albans Road on the town's east-west bus route. A road pattern was planned that offered no temptation to through traffic to take short cuts through the town and which enabled local traffic to move rapidly about the town.
Hatfield retains New Town characteristics, including much modernist architecture of the 1950s and the trees and open spaces that were outlined in the original design. The redevelopment of the town centre is being planned, involving the construction of 275 flats and retail units. Planning permission has been granted and compulsory purchase orders have been approved.
Governance.
Hatfield is part of Welwyn Hatfield borough council in the county of Hertfordshire. It is a civil parish and has a . It is twinned with the Dutch port town of Zierikzee. Hatfield is part of the Welwyn Hatfield constituency, which includes Welwyn Garden City. The MP for Welwyn Hatfield is Grant Shapps, (Conservative).
Geography.
Climate.
Hatfield experiences an oceanic climate (Köppen climate classification "Cfb") similar to almost all of the United Kingdom.
Culture and recreation.
Hatfield has a nine-screen Odeon cinema, a stately home (Hatfield House), a museum (Mill Green Museum), a contemporary art gallery (Art and Design Gallery), a theatre (The Weston Auditorium) and a music venue (The Forum Hertfordshire). There are shopping centres in the new town. The Galleria (indoor shopping centre), The Stable Yard (Hatfield House), and at two supermarkets (ASDA and Tesco).
Sport.
Hatfield Town F.C. in Non-League football plays at Gosling Sports Park.
The town also has one public swimming pool, and four sports/leisure centres (two with indoor swimming pools).
Education.
Hatfield contains numerous primary and secondary schools, including The Ryde School, St. Philip Howard Catholic Primary School, Onslow St Audrey's School and Bishops Hatfield Girls School and the independent day and boarding girls' school Queenswood School.
The University of Hertfordshire is based in Hatfield. A large section of the airfield site was purchased by the University and the £120 million de Havilland Campus, incorporating a £15 million Sports Village, was opened in September 2003. The university has closed its sites at Watford and Hertford; faculties situated there have been moved to the de Havilland Campus.
Transport.
Hatfield is 20 mi to the north of London. It is 14 mi from London Luton Airport and also near to Stansted airport The A1(M) runs through the town and it is close to the M25.
The East Coast railway line from London to York runs through the town and separates the old and new parts of Hatfield. A 22-minute commuter service connects Hatfield railway station to London Kings Cross.
There was a fatal rail crash at Hatfield in 2000, which brought track maintenance deficiencies to public attention. A garden beside the East Coast Main Line was built as a memorial for the crash victims.

</doc>
<doc id="13986" url="http://en.wikipedia.org/wiki?curid=13986" title="Hertfordshire">
Hertfordshire

Hertfordshire (; abbreviated Herts) is a county in southern England, bordered by Bedfordshire to the north, Cambridgeshire to the north-east, Essex to the east, Buckinghamshire to the west and Greater London to the south.
Four towns have between 50,000 and 100,000 residents: Hemel Hempstead, Stevenage, Watford and St Albans. The county town, Hertford, once the main market town for the medieval agricultural county ranks 13th in population today deriving its name from a hart (stag) and a ford used as the components of the county's coat of arms and flag. Elevations are high for the region in the north and west. These reach over 240m in the western projection around Tring which is in the Chilterns. The county's borders are approximately the watersheds of the Colne and Lea, which flow southwards each accompanied by a canal. Hertfordshire is at the edge of the London Basin and most of its undeveloped land is agricultural and protected as Metropolitan Green Belt. The volume of intact medieval and Tudor buildings surpasses London, in places in well-preserved conservation areas, especially in St Albans which includes some remains of Verulamium, the town where in the third century AD an early recorded British martyrdom took place. Saint Alban, a Romano-British soldier, took the place of a Christian priest and was beheaded on Holywell Hill. His martyr's cross of a yellow saltire on a blue background is reflected in the flag and coat of arms of Hertfordshire.
The county's landmarks span many centuries, ranging from the six 'Hills', next to the New Town of Stevenage built by inhabitants during the Roman Britain centuries, to Leavesden Film Studios. Leavesden filmed much of the UK-based $7.7 Bn box office "Harry Potter" film series and has the country's studio tour. The largest sector of the economy of the county is services and it has a large proportion of residents who are City of London commuters. Ten railway lines and three motorways pass through or reach into the county.
In 2013, the county had a population of 1,140,700 living in an area of 634 sqmi.
History.
Hertfordshire was the area assigned to a fortress constructed at Hertford under the rule of Edward the Elder in 913. Hertford is derived from the Anglo-Saxon "heort ford," meaning deer crossing (of a watercourse). The name Hertfordshire is first recorded in the "Anglo-Saxon Chronicle" in 1011. Deer feature in many county emblems.
There is evidence of humans living in Hertfordshire from the Middle Stone Age. It was first farmed during the Neolithic period and permanent habitation appeared at the beginning of the Bronze Age. This was followed by tribes settling in the area during the Iron Age.
Following the Roman conquest of Britain in AD 43, the aboriginal Catuvellauni quickly submitted and adapted to the Roman life; resulting in the development of several new towns, including Verulamium (St Albans) where in c. 293 the first recorded British martyrdom is traditionally believed to have taken place. Saint Alban, a Romano-British soldier, took the place of a Christian priest and was beheaded on Holywell Hill. His martyr's cross of a yellow saltire on a blue background is reflected in the flag and coat of arms of Hertfordshire as the yellow background to the stag or Hart representing the county. He is the Patron Saint of Hertfordshire.
With the departure of the Roman Legions in the early 5th century, the now unprotected territory was invaded and colonised by the Anglo-Saxons. By the 6th century the majority of the modern county was part of the East Saxon kingdom. This relatively short lived kingdom collapsed in the 9th century, ceding the territory of Hertfordshire to the control of the West Anglians of Mercia. The region finally became an English shire in the 10th century, on the merger of the West Saxon and Mercian kingdoms.
A century later the victorious William of Normandy received the surrender of the surviving senior English Lords and Clergy, at Berkhamsted, resulting in a new Anglicised title of William the Conqueror. He then embarked on an uncontested entry into London and coronation at Westminster.
After the Norman conquest, Hertfordshire was used for some of the new Norman castles at Bishop's Stortford and at the royal residence of Berkhamsted and at King's Langley, a staging post between London and the royal residence of Berkhamsted.
The Domesday Book recorded the county as having nine hundreds. Tring and Danais became one—Dacorum—from Danis Corum or Danish rule harking back to a Viking not Saxon past. The other seven were Braughing, Broadwater, Cashio, Edwinstree, Hertford, Hitchin and Odsey.
As London grew, Hertfordshire became conveniently close to the English capital; much of the area was owned by the nobility and aristocracy, this patronage helped to boost the local economy. However, the greatest boost to Hertfordshire came during the Industrial Revolution, after which the population rose dramatically. In 1903, Letchworth became the world's first garden city and Stevenage became the first town to redevelop under the New Towns Act 1946.
From the 1920s until the late 1980s, the town of Borehamwood was home to one of the major British film studio complexes, including the MGM-British Studios. Many well-known films were made here including the first three Star Wars movies (, , & ). The studios generally used the name of Elstree (the adjoining village). American director Stanley Kubrick not only used to shoot in those studios but also lived in the area until his death. In more recent times, Elstree has had the likes of Big Brother UK and Who Wants To Be A Millionaire? filmed there, whilst EastEnders is also filmed at the studios. Also Hertfordshire has seen development in other film studio complexes, Leavesden Film Studios were developed on the Leavesden Aerodome site, north of Watford. The Harry Potter series was filmed at the studios, whilst the 1995 James Bond film GoldenEye was also filmed there.
On 17 October 2000, the Hatfield rail crash killed four people with 170 injured. The crash exposed the shortcomings of Railtrack, which consequently saw speed restrictions and major track replacement. On 10 May 2002, the second of the Potters Bar rail accidents occurred killing seven people; the train was at high speed when it derailed and flipped into the air when one of the carriages slid along the platform where it came to rest.
In early December 2005, the 2005 Hemel Hempstead fuel depot explosions occurred at the Hertfordshire Oil Storage Terminal.
In 2012, the canoe and kayak slalom events of the 2012 Summer Olympic Games took place in the town of Waltham Cross, within the borough of Broxbourne.
Following a proposal put forward by The Welwyn Garden Heritage Trust, town-planner Andrés Duany has suggested that designated "Garden Villages" could be built within Hertfordshire to relieve some of the pressure for new homes, with perhaps a third Garden City to follow.
Geography.
Hertfordshire is the county immediately north of London and is part of the East of England region, a mainly statistical unit. A significant minority of the population across all districts are City of London commuters. To the east is Essex, to the west is Buckinghamshire and to the north are Bedfordshire and Cambridgeshire.
The county's boundaries were roughly fixed by the Counties (Detached Parts) Act 1844 which eliminated exclaves; amended when, in 1965 under the London Government Act 1963, East Barnet Urban District and Barnet Urban District were abolished, their area was transferred to form part of the present-day London Borough of Barnet and the Potters Bar Urban District of Middlesex was transferred to Hertfordshire.
The highest point in the county is at 245m (AOD) on the Ridgeway long distance national path, on the border of Hastoe near Tring with Drayton Beauchamp, Buckinghamshire.
As at the 2011 census of the ten Districts, East Hertfordshire had the minimal, 290 people per km², whereas Watford had the maximal 4210 people per km²
An unofficial status, the purple star-shaped flower with yellow stamens, the Pasqueflower is among endemic county flowers.
Geology.
The rocks of Hertfordshire belong to the great shallow syncline known as the London Basin. The beds dip in a south-easterly direction towards the syncline's lowest point roughly under the River Thames. The most important formations are the Cretaceous Chalk, exposed as the high ground in the north and west of the county, forming the Chiltern Hills and the younger Palaeocene, Reading Beds and Eocene, London Clay which occupy the remaining southern part. The eastern half of the county was covered by glaciers during the Ice Age and has a superficial layer of glacial boulder clays.
Natural resources and environment.
Despite the spread of built areas, much of the county is given over to agriculture. One product, now largely defunct, was water-cress, based in Hemel Hempstead and Berkhamsted supported by reliable, clean chalk rivers.
Some quarrying of sand and gravel occurs in the St Albans area. In the past, clay has supplied local brick-making and still does in Bovingdon, just south-west of Hemel Hempstead. The chalk that is the bedrock of much of the county provides an aquifer that feeds streams and is also exploited to provide water supplies for much of the county and beyond. Chalk has also been used as a building material and, once fired, the resultant lime was spread on agricultural land to improve fertility. The mining of chalk since the early 18th century has left unrecorded underground galleries that occasionally collapse unexpectedly and endanger buildings.
Fresh water is supplied to London from Ware, using the New River built by Hugh Myddleton and opened in 1613. Local rivers, although small, supported developing industries such as paper production at Nash Mills.
Hertfordshire affords habitat for a variety of flora and fauna. One bird common in the shire is the Royston crow, which is the eponymous name of the regional newspaper, the "Royston Crow" published in Royston.
Urban areas.
In November 2013, the uSwitch Quality of Life Index listed Hertfordshire as the third-best place to live in the UK.
Economy.
This is a table of trends of regional gross value added of Hertfordshire at current basic prices with figures in millions of British Pounds Sterling.
Hertfordshire has headquarters of many large well-known UK companies. Tesco, the UK's largest employer, is based in Cheshunt. Hemel Hempstead is home to DSG International. Welwyn Garden City hosts Roche UK's headquarters (subsidiary of the Swiss pharmaceutical firm Hoffman-La Roche) and Cereal Partners production facilities, Pure the DAB radio maker is based in Kings Langley. JD Wetherspoon is in Watford. Comet and Skanska are in Rickmansworth, GlaxoSmithKline has plants in Ware and Stevenage. Hatfield used to be connected with the aircraft industry, as it was where de Havilland developed the world's first commercial jet liner, the Comet. Now the site is a business park and new campus for the University of Hertfordshire. This major new employment site is home to, among others, EE, Computacenter and Ocado. A subsidiary of BAE Systems, EADS and Finmeccanica in Stevenage, MBDA, develops missiles. In the same town EADS Astrium produces satellites. The National Pharmacy Association (NPA), the trade association for all of the UK's community pharmacies, is based in St. Albans. Warner Bros. also owns and runs Warner Studios in Leavesden.
Landmarks.
Below is a list of notable visitor attractions in Hertfordshire:
Transport.
Hertfordshire lies across major road and rail routes connecting London to the Midlands, Northern England and Scotland. As one of the home counties, many towns in the county form part of the London commuter belt.
The county has some of the principal roads in England: A1, A1(M), A5, A6, A41, M1, M11, and M25.
Four principal national railway lines pass through the county:
A number of other local rail routes also cross Hertfordshire:
Two commuter lines operated by Transport for London enter the county:
At Elstree is a commercial airfield for light aircraft. Stansted and Luton are within 10 mi of the county's borders.
The Grand Union Canal passes through much of the far west of Hertfordshire: Rickmansworth, Watford, Hemel Hempstead, Berkhamsted and Tring.
Local bus services are run by a number of private operators. Intalink is an organisation run by the county council that manages transport and funds bus services in rural areas.
Education.
Hertfordshire has 26 independent schools and 73 state secondary schools.
The state secondary schools are entirely comprehensive, although 7 schools in the south and southwest of the county are partially selective (see Education in Watford).
All state schools have sixth forms, and there are no sixth form colleges.
The tertiary colleges, each with multiple campuses, are Hertford Regional College, North Hertfordshire College, Oaklands College and West Herts College.
The University of Hertfordshire is a modern university based largely in Hatfield. It has more than 23,000 students.
Literature.
Hertfordshire is the location of Jack Worthing's country house in Oscar Wilde's play "The Importance of Being Earnest".
Jane Austen's novel "Pride and Prejudice" is primarily set in Hertfordshire. Topographical scholars place the town of Meryton either as Hertford or Hemel Hempstead, based on how far Mr Collins travels on the post from Watford, in either an easterly or westerly direction. The former location places the Bennet family home Longbourn as the town of Ware.
The location of Mr Jarndyce's Bleak House in Charles Dickens's Bleak House is near St. Albans in Hertfordshire.
The eponymous residence in E. M. Forster's novel, "Howards End" was based on Rooks Nest House just outside Stevenage. In the novel, Forster describes Hertfordshire as "England at its quietest".
George Orwell based his book "Animal Farm" on the village of Wallington, Hertfordshire where he lived between 1936 and 1940. Manor Farm and The Great Barn both feature in the novel.
Notable residents.
Acclaimed seventeenth-century poet and romance writer Hester Pulter was a resident of Broadfield, Hertfordshire. The town of Berkhamsted was home to the Christian poet and hymn-writer William Cowper and to novelist Graham Greene. Violinist Thomas Bowes was born in Welwyn Garden City, Hertfordshire. American composer Jeff Wayne (his musical version of "The War of the Worlds") resides in Hertfordshire. Hertfordshire was the origin of the only English pope, Pope Adrian IV, and Cecil Rhodes, the founder of the southern African territory of Rhodesia.
Other present day celebrities born in Hertfordshire include Guy Ritchie, Vinnie Jones, Sarah Brightman, Geri Halliwell, Simon Le Bon, and Rou Reynolds.

</doc>
<doc id="13987" url="http://en.wikipedia.org/wiki?curid=13987" title="Helene Kröller-Müller">
Helene Kröller-Müller

Helene Kröller-Müller, (11 February 1869 – 14 December 1939) was one of the first European women to put together a major art collection.
She was born Helene Emma Laura Juliane Müller at Essen-Horst, Essen, Germany, into a wealthy industrialist family. Her father, Wilhelm Müller, owned Wm. H. Müller & Co., a prosperous supplier of raw materials to the mining and steel industries. She married Dutch shipping and mining tycoon Anton Kröller in 1888 and used both surnames in accordance with Dutch tradition.
She studied under Henk Bremmer in 1906-1907. As she was one of the wealthiest women in the Netherlands at the time, Bremmer recommended that she form an art collection. In 1907, she began her collection with the painting "Train in a Landscape" by Paul Gabriël. Subsequently, Helene Kröller-Müller became an avid art collector, and one of the first people to recognise the genius of Vincent van Gogh. She eventually amassed more than 90 van Gogh paintings and 185 drawings, one of the world's largest collections of the artist's work, second only to the Van Gogh Museum in Amsterdam. She also bought more than 400 works by Dutch artist Bart van der Leck, but his popularity did not take off like van Gogh's.
Kröller-Müller also collected works by modern artists, such as Picasso, Georges Braque, Jean Metzinger, Albert Gleizes, Fernand Léger, Diego Rivera, Juan Gris, Piet Mondrian, Gino Severini, Joseph Csaky, Auguste Herbin, Georges Valmier, María Blanchard, Léopold Survage and Tobeen. However, Bremmer advised her not to buy "A Sunday Afternoon on the Island of La Grande Jatte" by Georges Seurat, which turned out to be an important icon of 20th-century art. She did purchase however "Le Chahut" by Seurat, another icon in the history of modern art. Also, she steered away from artists of her native Germany, whose work she found "insufficiently authoritative."
On a trip to Florence in June 1910, she conceived the idea of creating a museum-house. From 1913 onwards parts of her collection were open to the public; until the mid-1930s her exhibition hall in The Hague was one of the very rare places where one could see more than a few works of modern art. In 1928, Anton and Helene created the Kröller-Müller Foundation to protect the collection and the estates. In 1935, they donated to the Dutch people their entire collection totaling approximately 12,000 objects, on condition that a large museum be built in the gardens of her park. Held in the care of the Dutch government, the Kröller-Müller Museum was opened in 1938 near the town of Otterlo in the Netherlands. Due to the threat of war, plans for a lavish museum were never implemented, but once the war was over it was possible to construct the relatively understated but well-lit modern exhibition extension, opened in 1977, that now houses much of the collection.

</doc>
<doc id="13988" url="http://en.wikipedia.org/wiki?curid=13988" title="Hans-Georg Gadamer">
Hans-Georg Gadamer

Hans-Georg Gadamer (]; February 11, 1900 – March 13, 2002) was a German philosopher of the continental tradition, best known for his 1960 magnum opus "Truth and Method" ("Wahrheit und Methode") on hermeneutics.
Life.
Gadamer was born in Marburg, Germany, the son of Johannes Gadamer (1867–1928) a pharmaceutical chemistry professor who later also served as the rector of the university there. He resisted his father's urging to take up the natural sciences and became more and more interested in the humanities. His mother, Emma Karoline Johanna Geiese (1869–1904) died of diabetes while Hans-Georg was four years old, and he later noted that this may have had an effect on his decision to not pursue scientific studies. Jean Grondin describes Gadamer as finding in his mother "a poetic and almost religious counterpart to the iron fist of his father". Gadamer did not serve during World War I for reasons of ill health and similarly was exempted from serving during World War II due to polio.
He grew up and studied philosophy in Breslau under Richard Hönigswald, but soon moved back to Marburg to study with the Neo-Kantian philosophers Paul Natorp and Nicolai Hartmann. He defended his dissertation—"The Essence of Pleasure according to Plato's Dialogues" (Das Wesen der Lust nach den Platonischen Dialogen)—in 1922.
Shortly thereafter, Gadamer moved to Freiburg University and began studying with Martin Heidegger, who was then a promising young scholar who had not yet received a professorship. He and Heidegger became close, and when Heidegger received a position at Marburg, Gadamer followed him there, where he became one of a group of students such as Leo Strauss, Karl Löwith, and Hannah Arendt. It was Heidegger's influence that gave Gadamer's thought its distinctive cast and led him away from the earlier neo-Kantian influences of Natorp and Hartmann. Gadamer studied Aristotle both under Edmund Husserl and under Heidegger.
Gadamer habilitated in 1929 and spent most of the early 1930s lecturing in Marburg. Unlike Heidegger, who joined the Nazi Party in May 1933 and continued as a member until the party was dissolved following World War II, Gadamer was silent on Nazism, and he was not politically active during the Third Reich. Gadamer did not join the Nazis, and he did not serve in the army because of the polio he had contracted in 1922. He joined the National Socialist Teachers League in August 1933. In April 1937 he became a temporary professor at Marburg, then in 1938 he received a professorship at Leipzig. From an SS-point of view Gadamer was classified as neither supportive nor disapproving in the "SD-Dossiers über Philosophie-Professoren" (i.e. SD-files concerning philosophy professors) that were set up by the SS-Security-Service (SD). In 1946, he was found by the American occupation forces to be untainted by Nazism and named rector of the university.
The level of Gadamer's involvement with the Nazis has been disputed in the works of Richard Wolin and Teresa Orozco. Orozco alleges, with reference to Gadamer's published works, that Gadamer had supported the Nazis more than scholars had supposed. Gadamer scholars have rejected these assertions: Jean Grondin has said that Orozco is engaged in a "witch-hunt" while Donatella Di Cesare said that "the archival material on which Orozco bases her argument is actually quite negligible". Cesare and Grondin have argued that there is no trace of antisemitism in Gadamer's work, and that Gadamer maintained friendships with Jews and provided shelter for nearly two years for the philosopher Jacob Klein in 1933 and 1934. Gadamer also reduced his contact with Heidegger during the Nazi era.
Communist East Germany was no more to Gadamer's liking than the Third Reich, and he left for West Germany, accepting first a position in Frankfurt am Main and then the succession of Karl Jaspers in Heidelberg in 1949. He remained in this position, as emeritus, until his death in 2002 at the age of 102. He was also an Editorial Advisor of the journal Dionysius. It was during this time that he completed his "magnum opus", "Truth and Method" (1960), and engaged in his famous debate with Jürgen Habermas over the possibility of transcending history and culture in order to find a truly objective position from which to critique society. The debate was inconclusive, but marked the beginning of warm relations between the two men. It was Gadamer who secured Habermas's first professorship in Heidelberg.
In 1968, Gadamer invited Tomonobu Imamichi for lectures at Heidelberg, but their relationship became very cool after Imamichi alleged that Heidegger had taken his concept of "Dasein" out of Okakura Kakuzo's concept of "das in-der-Welt-sein" (to be in the being in the world) expressed in "The Book of Tea", which Imamichi's teacher had offered to Heidegger in 1919, after having followed lessons with him the year before. Imamichi and Gadamer renewed contact four years later during an international congress.
In 1981, Gadamer attempted to engage with Jacques Derrida at a conference in Paris but it proved less enlightening because the two thinkers had little in common. A last meeting between Gadamer and Derrida was held at the Stift of Heidelberg in July 2001, coordinated by Derrida's students, Joseph Cohen and Raphael Zagury-Orly. This meeting marked, in many ways, a turn in their philosophical encounter. After Gadamer's death, Derrida called their failure to find common ground one of the worst debacles of his life and expressed, in the main obituary for Gadamer, his great personal and philosophical respect. Richard J. Bernstein said that "[a] genuine dialogue between Gadamer and Derrida has never taken place. This is a shame because there are crucial and consequential issues that arise between hermeneutics and deconstruction".
Gadamer received honorary doctorates from the University of Bamberg, the University of Breslau, Boston College, Charles University in Prague, Hamilton College, the University of Leipzig, the University of Marburg (1999) the University of Ottawa, Saint Petersburg State University (2001), the University of Tübingen and University of Washington.
On February 11, 2000, the University of Heidelberg celebrated Gadamer's one hundredth birthday with a ceremony and conference. Gadamer's last academic engagement was in the summer of 2001 at an annual symposium on hermeneutics that two of Gadamer's American students had organised. On March 13, 2002, Gadamer died at Heidelberg's University Clinic. He is buried in the Köpfel cemetery in Ziegelhausen.
Work.
Philosophical Hermeneutics and "Truth and Method",.
Gadamer's philosophical project, as explained in "Truth and Method", was to elaborate on the concept of "philosophical hermeneutics", which Heidegger initiated but never dealt with at length. Gadamer's goal was to uncover the nature of human understanding. In "Truth and Method", Gadamer argued that "truth" and "method" were at odds with one another. He was critical of two approaches to the human sciences ("Geisteswissenschaften"). On the one hand, he was critical of modern approaches to humanities that modeled themselves on the natural sciences, which simply sought to “objectively” observe and analyze texts and art. On the other hand, he took issue with the traditional German approaches to the humanities, represented for instance by Friedrich Schleiermacher and Wilhelm Dilthey, who believed that meaning, as an object, could be found within a text through a particular process that allowed for a connection with the author’s thoughts that led to the creation of a text (Schleiermacher), or the situation that led to an expression of human inner life (Dilthey). Instead, Gadamer argued meaning and understanding are not objects to be found through certain methods, but are inevitable phenomena. Hermeneutics is not a process in which an interpreter finds a particular meaning, but “a philosophical effort to account for understanding as an ontological—the ontological—process of man.” Thus, Gadamer is not giving a prescriptive method on how to understand, but rather he is working to examine how understanding, whether of texts, artwork, or experience, is possible at all. Gadamer intended "Truth and Method" to be a description of what we always do when we interpret things (even if we do not know it): "My real concern was and is philosophic: not what we do or what we ought to do, but what happens to us over and above our wanting and doing".
As a result of Martin Heidegger’s temporal analysis of human existence, Gadamer argued that people have a "historically-effected" consciousness ("wirkungsgeschichtliches Bewußtsein"), and that they are embedded in the particular history and culture that shaped them. However the historical consciousness is not an object over and against our existence, but “a stream in which we move and participate, in every act of understanding.” Therefore, people do not come to any given thing without some form of preunderstanding established by this historical stream. The tradition in which an interpreter stands establishes "prejudices" that affect how he or she will make interpretations. For Gadamer, these prejudices are not something that hinders our ability to make interpretations, but are both integral to the reality of being, and “are the basis of our being able to understand history at all.” Gadamer criticized Enlightenment thinkers for harboring a "prejudice against prejudices".
For Gadamer, interpreting a text involves a fusion of horizons ("Horizontverschmelzung"). Both the text and the interpreter find themselves within a particular historical tradition, or “horizon.” Each horizon is expressed through the medium of language, and both text and interpreter belong to and participate in history and language. This “belongingness” to language is the common ground between interpreter and text that makes understanding possible. As an interpreter seeks to understand a text, a common horizon emerges. This fusion of horizons does not mean the interpreter now fully understands some kind of objective meaning, but is “an event in which a world opens itself to him.” The result is a deeper understanding of the subject matter.
Gadamer further explains the hermeneutical experience as a dialogue. To justify this, he uses Plato’s dialogues as a model for how we are to engage with written texts. To be in conversation, one must take seriously “the truth claim of the person with whom one is conversing.” Further, each participant in the conversation relates to one another insofar as they belong to the common goal of understanding one another. Ultimately, for Gadamer, the most important dynamic of conversation as a model for the interpretation of a text is “the give-and-take of question and answer.” In other words, the interpretation of a
given text will change depending on the questions the interpreter asks of the
text. The "meaning" emerges not as an object that lies in the text or in the interpreter, but rather an event that results from the interaction of the two.
"Truth and Method" was published twice in English, and the revised edition is now considered authoritative. The German-language edition of Gadamer's Collected Works includes a volume in which Gadamer elaborates his argument and discusses the critical response to the book. Finally, Gadamer's essay on Celan (entitled "Who Am I and Who Are You?") has been considered by many—including Heidegger and Gadamer himself—as a "second volume" or continuation of the argument in "Truth and Method".
Contributions to Communication Ethics.
Gadamer's "Truth and Method" has become an authoritative work in the communication ethics field, spawning several prominent ethics theories and guidelines. The most profound of these is the formulation of the dialogic coordinates, a standard set of prerequisite communication elements necessary for inciting dialogue. Adhering to Gadamer's theories regarding bias, communicators can better initiate dialogic transaction, allowing biases to merge and promote mutual understanding and learning.
Other works.
Gadamer also added philosophical substance to the notion of human health. In "The Enigma of Health", Gadamer explored what it means to heal, as a patient and a provider. In this work the practice and art of medicine are thoroughly examined, as is the inevitability of any cure.
In addition to his work in hermeneutics, Gadamer is also well known for a long list of publications on Greek philosophy. Indeed, while "Truth and Method" became central to his later career, much of Gadamer's early life centered around studying Greek thinkers, Plato and Aristotle specifically. In the Italian introduction to "Truth and Method", Gadamer said that his work on Greek philosophy was "the best and most original part" of his career. His book "Plato's Dialectical Ethics" looks at the Philebus dialogue through the lens of phenomenology and the philosophy of Martin Heidegger.

</doc>
<doc id="13991" url="http://en.wikipedia.org/wiki?curid=13991" title="Honeymoon">
Honeymoon

A honeymoon is the traditional holiday taken by newlyweds to celebrate their marriage in intimacy and seclusion. Today, honeymoons by Westerners are often celebrated in destinations considered exotic and/or romantic.
History of honeymoon.
This is the period when newly wed couples take a break to share some private and intimate moments that helps establish love in their relationship. This privacy in turn is believed to ease the comfort zone towards a physical relationship, which is one of the primary means of bonding during the initial days of marriage. The earliest term for this in English was "hony moone", which was recorded as early as 1546.
In Western culture, the custom of a newlywed couple going on a holiday together originated in early 19th century Great Britain. Upper-class couples would take a "bridal tour", sometimes accompanied by friends or family, to visit relatives who had not been able to attend the wedding. The practice soon spread to the European continent and was known as "voyage à la façon anglaise" (English-style voyage) in France from the 1820s on.
Honeymoons in the modern sense (i.e. a pure holiday voyage undertaken by the married couple) became widespread during the Belle Époque, as one of the first instances of modern mass tourism. This came about in spite of initial disapproval by contemporary medical opinion (which worried about women's frail health) and by "savoir vivre" guidebooks (which referred the public attention drawn to what was assumed to be the wife's sexual initiation). The most popular honeymoon destinations at the time were the French Riviera and Italy, particularly its seaside resorts and romantic cities such as Rome, Verona or Venice. Typically honeymoons would start on the night they were married, with the couple leaving midway through the reception to catch a late train or ship. However, in the 21st century, many couples will not leave until 1–3 days after the ceremony and reception in order to tie up loose ends with the reception venue and/or simply enjoy the reception to its fullest and have a relaxing night afterwards to recover, before undertaking a long journey. In Jewish traditions, honeymoons are often put off seven days to allow for the seven nights of feasting if the visits to friends and family can't be incorporated into the trip.
Etymology.
The "Oxford English Dictionary" offers no etymology, but gives examples dating back to the 16th century. The Merriam-Webster dictionary reports the etymology as from "the idea that the first month of marriage is the sweetest" (1546).
A honeymoon can also be the first moments a newly-wed couple spend together, or the first holiday they spend together to celebrate their marriage.
"The first month after marriage, when there is nothing but tenderness and pleasure" (Samuel Johnson); originally having no reference to the period of a month, but comparing the mutual affection of newly married persons to the changing moon which is no sooner full than it begins to wane; now, usually, the holiday spent together by a newly married couple, before settling down at home.
One of the more recent citations in the "Oxford English Dictionary" indicates that, while today "honeymoon" has a positive meaning, the word was originally a reference to the inevitable waning of love like a phase of the moon. This, the first known literary reference to the honeymoon, was penned in 1552, in Richard Huloet's "Abecedarium Anglico Latinum". Huloet writes:
Hony mone, a term proverbially applied to such as be newly married, which will not fall out at the first, but th'one loveth the other at the beginning exceedingly, the likelihood of their exceadinge love appearing to aswage, ye which time the vulgar people call the hony mone.—Abcedarium Anglico-Latinum pro Tyrunculis, 1552
A widely disputed explanation of the term claims that it comes from a tradition in any of a number of cultures (e.g. Welsh, German or Scandinavian or Babylonian) where Mead was drunk in great quantities at weddings, and after the ceremony nuptial couples were given a month’s supply of mead. It was believed that by faithfully drinking mead for that first month, the woman would “bear fruit” and a child would be born within the year.
There are many words of similar meaning in other languages. The Sinhalease from translates as "Madhu Samaya" ("මධු සමය"). The French form translates as "moon of honey" ("lune de miel"), as do the Spanish ("luna de miel"), Romanian ("luna de miere"), Nepali ("Madhumas") Portuguese ("lua de mel") and Italian ("luna di miele") equivalents. The Welsh word for honeymoon is "mis mêl", which means "honey month", and similarly the Ukrainian ("медовий місяць"), Polish ("miesiąc miodowy"), Russian ("медовый месяц"), Arabic ( "shahr el 'assal"), Greek ("μήνας του μέλιτος") and Hebrew (ירח דבש "yerach d'vash") versions. (Interestingly, "Yerach" is used for month, rather than the more common "Chodesh". "Yerach" is related to the word "Yare'ach" for moon and the two words are spelled alike: ירח.) The Persian word is ماه عسل "māh-e asal" which means both "honey moon" and "honey month" ("māh" in Persian means both "moon" and "month"). The same applies to the word "ay" in the Turkish equivalent, "balayı". In Hungarian language it is called "honey weeks" (mézeshetek). Likewise, the Tamil word for honeymoon is "தேனிலவு" (thaen nilavu), with "thaen" 'honey' and "nilavu" 'moon', and the Marathi word for honeymoon is "मधुचंद्र" (madhuchandra) with "Madhu" 'honey' and "chandra" 'moon', whereas in Bangla ('Bengali') language, it is referred to as মধুচন্দ্রিমা (modhuchondrima) with "modhu" 'honey" and "chondrima" 'moon'.

</doc>
<doc id="13992" url="http://en.wikipedia.org/wiki?curid=13992" title="Harold Kushner">
Harold Kushner

Rabbi Harold Samuel Kushner is a prominent American rabbi aligned with the progressive wing of Conservative Judaism, and a popular author.
Education.
Born in Brooklyn, Kushner was educated at Columbia University and later obtained his rabbinical ordination from the Jewish Theological Seminary (JTS) in 1960. The same institution awarded him a doctoral degree in Bible in 1972. Kushner has also studied at the Hebrew University of Jerusalem, taught at Clark University and the Rabbinical School of the JTS, and received six honorary doctorates.
Congregational Rabbi.
He served as the congregational rabbi of Temple Israel of Natick, in Natick, Massachusetts for 24 years and belongs to the Rabbinical Assembly.
Author.
He is the author of a best selling book on the problem of evil, "When Bad Things Happen to Good People." Written following the death of his son, Aaron, from the premature aging disease progeria, the book deals with questions about human suffering, God, omnipotence and theodicy. Aaron was born in 1963 and died in 1977; the book was published in 1981. 
Kushner has written a number of other popular theological books, such as "How Good Do We Have to Be?" (Dedicated to his grandson, Carl), "To Life!" and many others. In collaboration with the late Chaim Potok, Kushner co-edited "Etz Hayim: A Torah Commentary", the new official Torah commentary of the Conservative movement, which was jointly published in 2001 by the Rabbinical Assembly and the Jewish Publication Society. His "Living a Life That Matters" became a best seller in the fall of 2001. Kushner's book, "The Lord Is My Shepherd", was a meditation on the Twenty-Third Psalm released in 2003. Kushner also wrote a response to Simon Wiesenthal's question of forgiveness in the book "."

</doc>
<doc id="13994" url="http://en.wikipedia.org/wiki?curid=13994" title="Hotspot">
Hotspot

Hotspot or Hot spot may refer to:

</doc>
<doc id="13995" url="http://en.wikipedia.org/wiki?curid=13995" title="Heapsort">
Heapsort

In computer programming, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.
Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O("n" log "n") runtime. Heapsort is an in-place algorithm, but it is not a stable sort.
Heapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.
Overview.
The heapsort algorithm can be divided into two parts.
In the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if codice_1 is the index of the current node, then
In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap. Once all objects have been removed from the heap, the result is a sorted array.
Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.
Pseudocode.
The following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and codice_2 is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at codice_3, while at the end of the sort, the largest element is in codice_4.
 procedure heapsort(a, count) is
 input: an unordered array "a" of length "count"
 "(Build the heap in array a so that largest value is at the root)"
 heapify(a, count)
 "(The following loop maintains the invariants that a[0:end] is a heap and every element"
 "beyond end is greater than everything before it (so a[end:count] is in sorted order))"
 end ← count - 1
 while end > 0 do
 "(a[0] is the root and largest value. The swap moves it in front of the sorted elements.)"
 swap(a[end], a[0])
 "(the heap size is reduced by one)"
 end ← end - 1
 "(the swap ruined the heap property, so restore it)"
 siftDown(a, 0, end)
The sorting routine uses two subroutines, codice_5 and codice_6. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing codice_5.
 "(Put elements of 'a' in heap order, in-place)"
 procedure heapify(a, count) is
 "(start is assigned the index in 'a' of the last parent node)"
 "(the last element in a 0-based array is at index count-1; find the parent of that element)"
 start ← floor ((count - 2) / 2)
 while start ≥ 0 do
 "(sift down the node at index 'start' to the proper place such that all nodes below"
 " the start index are in heap order)"
 siftDown(a, start, count - 1)
 "(go to the next parent node)"
 start ← start - 1
 "(after sifting down the root all nodes/elements are in heap order)"
 "(Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)
 procedure siftDown(a, start, end) is
 root ← start
 while root * 2 + 1 ≤ end do "(While the root has at least one child)"
 child ← root * 2 + 1 "(Left child)"
 swap ← root "(Keeps track of child to swap with)"
 if a[swap] < a[child]
 swap ← child
 "(If there is a right child and that child is greater)"
 if child+1 ≤ end and a[swap] < a[child+1]
 swap ← child + 1
 if swap = root
 "(The root holds the largest element. Since we assume the heaps rooted at the"
 " children are valid, this means that we are done.)"
 return
 else
 swap(a[root], a[swap])
 root ← swap "(repeat to continue sifting down the child now)"
The codice_5 procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This codice_9 version can be visualized as starting with an empty heap and successively inserting elements, whereas the codice_6 version given above treats the entire input array as a full but "broken" heap and "repairs" it starting from the last non-trivial sub-heap (that is, the last parent node).
Also, the codice_6 version of heapify has "O"("n") time complexity, while the codice_9 version given below has "O"("n" log "n") time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.
This may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never has an impact on asymptotic analysis.
To grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call "increases" with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more "deep" nodes than there are "shallow" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the "bottom" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call "decreases" as the depth of the node on which the call is made increases. Thus, when the codice_6 codice_5 begins and is calling codice_6 on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the "height" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.
The heapsort algorithm itself has "O"("n" log "n") time complexity using either version of heapify.
 procedure heapify(a,count) is
 "(end is assigned the index of the first (left) child of the root)"
 end := 1
 while end < count
 "(sift up the node at index end to the proper place such that all nodes above"
 " the end index are in heap order)"
 siftUp(a, 0, end)
 end := end + 1
 "(after sifting up the last node all nodes are in heap order)"
 procedure siftUp(a, start, end) is
 input: "start represents the limit of how far up the heap to sift."
 "end is the node to sift up."
 child := end 
 while child > start
 parent := floor((child-1) / 2)
 if a[parent] < a[child] then "(out of max-heap order)"
 swap(a[parent], a[child])
 child := parent "(repeat to continue sifting up the parent now)"
 else
 return
Variations.
Bottom-up heapsort.
Bottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows. Ordinary heapsort extracts the top of the heap, "a"[0], and fills the gap it leaves with "a"["end"], then sifts this latter element down the heap; but this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. Bottom-up heapsort instead finds the element to fill the gap, by tracing a path of maximum children down the heap as before, but then sifts that element "up" the heap, which is likely to take fewer steps.
 function leafSearch(a, end, i) is
 j ← i
 while 2×j ≤ end do
 "(Determine which of j's children is the greater)"
 if 2×j+1 < end and a[2×j+1] > a[2×j] then
 j ← 2×j+1
 else
 j ← 2×j
 return j
The return value of the codice_16 is used in a replacement for the codice_6 routine:
 function siftDown(a, end, i) is
 j ← leafSearch(a, end, i)
 while a[i] > a[j] do
 j ← parent(j)
 x ← a[j]
 a[j] ← a[i]
 while j > i do
 swap x, a[parent(j)]
 j ← parent(j)
Bottom-up heapsort requires only 1.5 "n" log "n" + "O"("n") comparisons in the worst case and "n" log "n" + "O"(1) on average.
A 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort, though, presumably because modern branch prediction nullifies the cost of the comparisons that bottom-up heapsort manages to avoid.
Comparison with other sorts.
Heapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.
Quicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O("n"2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.
Thus, because of the O("n" log "n") upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort.
Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches. On the other hand, merge sort has several advantages over heapsort:
Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.
Example.
Let { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)
1. Build the heap
2. Sorting.

</doc>
<doc id="13996" url="http://en.wikipedia.org/wiki?curid=13996" title="Heap (data structure)">
Heap (data structure)

In computer science, a heap is a specialized tree-based Abstract data type that satisfies the "heap property:" If A is a parent node of B then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap. Heaps can be classified further as either a "max heap" or a "min heap". In a max heap, the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node. Heaps are crucial in several efficient graph algorithms such as Dijkstra's algorithm, and in the sorting algorithm heapsort. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree (see figure).
In a heap, the highest (or lowest) priority element is always stored at the root, hence the name heap. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the Heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes always has log N height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.
Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.
The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact priority queues are often referred to as "heaps", regardless of how they may be implemented. Note that despite the similarity of the name "heap" to "stack" and "queue", the latter two are abstract data types, while a heap is a specific data structure, and "priority queue" is the proper term for the abstract data type.
A "heap" data structure should not be confused with "the heap" which is a common name for the pool of memory from which dynamically allocated memory is allocated. The term was originally used only for the data structure.
Operations.
The common operations involving heaps are:
Implementation.
Heaps are usually implemented in an array (fixed size or dynamic array), and do not require pointers between elements. After an element is inserted into or deleted from a heap, the heap property is violated and the heap must be balanced by internal operations.
Full and almost full binary heaps may be represented in a very space-efficient way (as an implicit data structure) using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position "n" would be at positions 2n and 2n + 1 in a one-based array, or 2n + 1 and 2n + 2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by shift-up or shift-down operations (swapping elements which are out of order). As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.
Different types of heaps implement the operations in different ways, but notably, insertion is often done by adding the new element at the end of the heap in the first available free space. This will generally violate the heap property, and so the elements are then sifted up until the heap property has been reestablished. Similarly, deleting the root is done by removing the root and then putting the last element in the root and sifting down to rebalance. Thus replacing is done by deleting the root and putting the "new" element in the root and sifting down, avoiding a sifting up step compared to pop (sift down of last element) followed by push (sift up of new element).
Construction of a binary (or "d"-ary) heap out of a given array of elements may be performed in linear time using the classic Floyd algorithm, with the worst-case number of comparisons equal to 2"N" − 2"s"2("N") − "e"2("N") (for a binary heap), where "s"2("N") is the sum of all digits of the binary representation of "N" and "e"2("N") is the exponent of 2 in the prime factorization of "N". This is faster than a sequence of consecutive insertions into an originally empty heap, which is log-linear (or linearithmic).
Comparison of theoretic bounds for variants.
In the following time complexities "O"("f") is an asymptotic upper bound and "Θ"("f") is an asymptotically tight bound (see Big O notation). Function names assume a min-heap.
Applications.
The heap data structure has many applications.

</doc>
<doc id="13998" url="http://en.wikipedia.org/wiki?curid=13998" title="Hierarchy">
Hierarchy

A hierarchy (from the Greek ἱεραρχία "hierarchia", "rule of a high priest", from ἱεράρχης "hierarkhes", "leader of sacred rites") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being "above," "below," or "at the same level as" one another.
A hierarchy (sometimes abbreviated HR) can link entities either directly or indirectly, and either vertically or horizontally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior or to one of one's subordinates, although a system that is largely hierarchical can also incorporate alternative hierarchies. Indirect hierarchical links can extend "vertically" upwards or downwards via multiple links in the same direction, following a path. All parts of the hierarchy which are not linked vertically to one another nevertheless can be "horizontally" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy (sometimes abbreviated HT) is one such form.
Nomenclature.
Hierarchies have their own special vocabulary. These terms are easiest to understand when a hierarchy is diagrammed (see below).
In an organizational context, the following terms are often used related to hierarchies:
In a mathematical context (in graph theory), the general terminology used is different.
Most hierarchies use a more specific vocabulary pertaining to their subject, but the idea behind them is the same. For example, with data structures, objects are known as nodes, superiors are called parents and subordinates are called children. In a business setting, a superior is a supervisor/boss and a peer is a colleague.
Degree of branching.
Degree of branching refers to the number of direct subordinates or children an object has (in graph theory, equivalent to the number of other vertices connected to via outgoing arcs, in a directed graph) a node has). Hierarchies can be categorized based on the "maximum degree", the highest degree present in the system as a whole. Categorization in this way yields two broad classes: "linear" and "branching".
In a linear hierarchy, the maximum degree is 1. In other words, all of the objects can be visualized in a lineup, and each object (excluding the top and bottom ones) has exactly one direct subordinate and one direct superior. Note that this is referring to the "objects" and not the "levels"; every hierarchy has this property with respect to levels, but normally each level can have an infinite number of objects. An example of a linear hierarchy is the hierarchy of life.
In a 'branching hierarchy', one or more objects has a degree of 2 or more (and therefore the maximum degree is 2 or higher). For many people, the word "hierarchy" automatically evokes an image of a branching hierarchy. Branching hierarchies are present within numerous systems, including organizations and classification schemes. The broad category of branching hierarchies can be further subdivided based on the degree.
A 'flat hierarchy' is a branching hierarchy in which the maximum degree approaches infinity, i.e., with a wide span. Most often, systems intuitively regarded as hierarchical have at most a moderate span. Therefore, a flat hierarchy is often not viewed as a hierarchy at all at first blush. For example, diamonds and graphite is a flat hierarchy of numerous carbon atoms which can be further decomposed into subatomic particles.
An 'overlapping hierarchy' is a branching hierarchy in which at least one object has two parent objects. For example, a graduate student can have two co-supervisors to whom the student reports directly and equally, and who have the same level of authority within the university hierarchy (i.e., they have the same position or tenure status.
History of the term.
Possibly the first use of the English word "hierarchy" cited by the Oxford English Dictionary was in 1880, when it was used in reference to the three orders of three angels as depicted by Pseudo-Dionysius the Areopagite (5th–6th centuries). Pseudo-Dionysius used the related Greek word ("hierarchia") both in reference to the celestial hierarchy and the ecclesiastical hierarchy. The Greek term "ἱεραρχία" means "rule by priests" (from "ἱεράρχης" – "ierarches", meaning "president of sacred rites, high-priest" and that from "ἱερεύς" – "iereus", "priest" + "ἀρχή" – "arche", amongst others "first place or power, rule"), and Dionysius is credited with first use of it as an abstract noun. Since hierarchical churches, such as the Roman Catholic (see Catholic Church hierarchy) and Eastern Orthodox churches, had tables of organization that were "hierarchical" in the modern sense of the word (traditionally with God as the pinnacle or head of the hierarchy), the term came to refer to similar organizational methods in secular settings.
Visualization.
A hierarchy is typically depicted as a pyramid, where the height of a level represents that level's status and width of a level represents the quantity of items at that level relative to the whole. For example, the few Directors of a company could be at the apex, and the base could be thousands of people who have no subordinates.
These pyramids are typically diagrammed with a tree or triangle diagram (but note that not all triangle/pyramid diagrams are hierarchical), both of which serve to emphasize the size differences between the levels. An example of a triangle diagram appears to the right. An organizational chart is the diagram of a hierarchy within an organization, and is depicted in tree form below.
More recently, as computers have allowed the storage and navigation of ever larger data sets, various methods have been developed to represent hierarchies in a manner that makes more efficient use of the available space on a computer's screen. Examples include fractal maps, TreeMaps and Radial Trees.
Visual hierarchy.
In the design field, mainly graphic design, successful layouts and formatting of the content on documents are heavily dependent on the rules of visual hierarchy. Visual hierarchy is also important for proper organization of files on computers.
An example of visually representing hierarchy is through the Nest structure. The Nest structure represents hierarchical relationships by using layers of information. The child element is within the parent element, such as in a Venn diagram. This structure of representing hierarchy is most effective in representing simple relationships. For example, when directing someone to open a file on a computer desktop, one may first direct them towards the main folder, then the subfolders within the main folder. They will keep opening files within the folders until the designated file is located.
For more complicated hierarchies, the stair structure represents hierarchical relationships through the use of visual stacking. Visually imagine the top of a downward staircase beginning at the left and descending on the right. The child elements are towards the bottom of the stairs and the parent elements are at the top. This structure is effective when representing more complicated hierarchies where steps are not placed in obvious sequences. Further steps are concealed unless all of the steps are revealed in sequence. In the computer desktop example, a file that is being sought after can only be found once another file is opened. The link for the desired file is within another document. All the steps must be completed until the final destination is reached.
Informal representation.
In plain English, a hierarchy can be thought of as a set in which:
The first requirement is also interpreted to mean that a hierarchy can have no circular relationships; the association between two objects is always transitive.
The second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects.
Mathematical representation.
Mathematically, in its most general form, a hierarchy is a partially ordered set or "poset". The system in this case is the entire poset, which is constituted of elements. Within this system, each element shares a particular unambiguous property. Objects with the same property value are grouped together, and each of those resulting levels is referred to as a class.
"Hierarchy" is particularly used to refer to a poset in which the classes are organized in terms of increasing complexity. 
Operations such as addition, subtraction, multiplication and division are often performed in a certain sequence or order. Usually, addition and subtraction are performed after multiplication and division has already been applied to a problem. The use of parenthesis is also a representation of hierarchy, for they show which operation is to be done prior to the following ones. For example:
(2 + 5) × (7 - 4).
In this problem, typically one would multiply 5 by 7 first, based on the rules of mathematical hierarchy. But when the parenthesis is placed, one will know to do the operations within the parenthesis first before continuing on with the problem. These rules are largely dominant in algebraic problems, ones that include several steps in order to solve. The use of hierarchy in mathematics is beneficial in order to quickly and efficiently solve a problem without having to go through the process of slowly dissecting the problem. Most of these rules are now known as the proper way into solving certain equations.
Subtypes.
Nested hierarchy.
A nested hierarchy or "inclusion hierarchy" is a hierarchical ordering of nested sets. The concept of nesting is exemplified in Russian matryoshka dolls. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level. The general concept is both demonstrated and mathematically formulated in the following example:
A square can always also be referred to as a quadrilateral, polygon or shape. In this way, it is a hierarchy. However, consider the set of polygons using this classification. A square can "only" be a quadrilateral; it can never be a triangle, hexagon, etc.
Nested hierarchies are the organizational schemes behind taxonomies and systematic classifications. For example, using the original Linnaean taxonomy (the version he laid out in the 10th edition of "Systema Naturae"), a human can be formulated as:
Taxonomies may change frequently (as seen in biological taxonomy), but the underlying concept of nested hierarchies is always the same.
In many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of Self-similarity and Recursion. Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic.
Containment hierarchy.
A containment hierarchy is a direct extrapolation of the nested hierarchy concept. All of the ordered sets are still nested, but every set must be "strict"—no two sets can be identical. The shapes example above can be modified to demonstrate this:
The notation formula_4 means "x" is a subset of "y" but is not equal to "y".
A general example of a containment hierarchy is demonstrated in class inheritance in object-oriented programming.
Two types of containment hierarchies are the "subsumptive" containment hierarchy and the "compositional" containment hierarchy. A subsumptive hierarchy "subsumes" its children, and a compositional hierarchy is "composed" of its children. A hierarchy can also be both subsumptive "and" compositional.
Subsumptive containment hierarchy.
A "subsumptive" containment hierarchy is a classification of objects from the general to the specific. Other names for this type of hierarchy are "taxonomic hierarchy" and "IS-A hierarchy". The last term describes the relationship between each level—a lower-level object "is a" member of the higher class. The taxonomical structure outlined above is a subsumptive containment hierarchy, as are all systematic naming schemes. Using again the example of Linnaean taxonomy, it can be seen that an object that is part of the level "Mammalia" "is a" member of the level "Animalia"; more specifically, a human "is a" primate, a primate "is a" mammal, and so on. A subsumptive hierarchy can also be defined abstractly as a hierarchy of "concepts". For example, with the Linnaean hierarchy outlined above, an entity name like "Animalia" is a way to group all the species that fit the conceptualization of an animal.
Compositional containment hierarchy.
A "compositional" containment hierarchy is an ordering of the parts that make up a system—the system is "composed" of these parts. Most engineered structures, whether natural or artificial, can be broken down in this manner.
The compositional hierarchy that every person encounters at every moment is the hierarchy of life. Every person can be reduced to organ systems, which are composed of organs, which are composed of tissues, which are composed of cells, which are composed of molecules, which are composed of atoms. In fact, the last two levels apply to all matter, at least at the macroscopic scale. Moreover, each of these levels inherit all the properties of their children.
In this particular example, there are also "emergent properties"—functions that are not seen at the lower level (e.g., cognition is not a property of neurons but is of the brain)—and a scalar quality (molecules are bigger than atoms, cells are bigger than molecules, etc.). Both of these concepts commonly exist in compositional hierarchies, but they are not a required general property. These "level hierarchies" are characterized by bi-directional causation. "Upward causation" involves lower-level entities causing some property of a higher level entity; children entities may interact to yield parent entities, and parents are composed at least partly by their children. "Downward causation" refers to the effect that the incorporation of entity "x" into a higher-level entity can have on "x"'s properties and interactions. Furthermore, the entities found at each level are "autonomous".
Contexts and applications.
Almost every system within the world is arranged hierarchically. By their common definitions, every nation has a government and every government is hierarchical. Socioeconomic systems are stratified into a social hierarchy (the social stratification of societies), and all systematic classification schemes (taxonomies) are hierarchical. Most organized religions, regardless of their internal governance structures, operate as a hierarchy under God. Many Christian denominations have an autocephalous ecclesiastical hierarchy of leadership. Families are viewed as a hierarchical structure in terms of cousinship (e.g., first cousin once removed, second cousin, etc.), ancestry (as depicted in a family tree) and inheritance (succession and heirship). All the requisites of a well-rounded life and lifestyle can be organized using Maslow's hierarchy of human needs. Learning must often follow a hierarchical scheme—to learn differential equations one must first learn calculus; to learn calculus one must first learn elementary algebra; and so on. Even nature itself has its own hierarchies, as demonstrated in numerous schemes such as Linnaean taxonomy, the organization of life, and biomass pyramids. Hierarchies are so infused into daily life that they are viewed as trivial.
While the above examples are often clearly depicted in a hierarchical form and are classic examples, hierarchies exist in numerous systems where this branching structure is not immediately apparent. For example, all postal code systems are necessarily hierarchical. Using the Canadian postal code system, the top level's binding concept is the "postal district", and consists of 18 objects (letters). The next level down is the "zone", where the objects are the digits 0–9. This is an example of an overlapping hierarchy, because each of these 10 objects has 18 parents. The hierarchy continues downward to generate, in theory, 7,200,000 unique codes of the format "A0A 0A0". Most library classification systems are also hierarchical. The Dewey Decimal System is regarded as infinitely hierarchical because there is no finite bound on the number of digits can be used after the decimal point.
Organizations.
Organizations can be structured as a dominance hierarchy. In an organizational hierarchy, there is a single person or group with the most power and authority, and each subsequent level represents a lesser authority. Most organizations are structured in this manner, including governments, companies, militia and organized religions. The units or persons within an organization are depicted hierarchically in an organizational chart.
In a reverse hierarchy, the conceptual pyramid of authority is turned upside-down, so that the apex is at the bottom and the base is at the top. This model represents the idea that members of the higher rankings are responsible for the members of the lower rankings.
Computer graphic imaging.
Within most CGI and computer animation programs is the use of hierarchies. On a 3D model of a human, the chest is a parent of the upper left arm, which is a parent of the lower left arm, which is a parent of the hand. This is used in modeling and animation of almost everything built as a 3D digital model.
Hierarchical verbal alignment.
Languages such as Cree and Mapudungun distinguish subject and object on verbs not by different subject and object markers, but via a hierarchy of persons.
In this system, the three (or four with Algonquian languages) persons are placed in a hierarchy of salience. To distinguish which is subject and which object, "inverse markers" are used if the object outranks the subject.
In music, the structure of a composition is often understood hierarchically (for example by Heinrich Schenker (1768–1835, see Schenkerian analysis), and in the (1985) Generative Theory of Tonal Music, by composer Fred Lerdahl and linguist Ray Jackendoff). The sum of all notes in a piece is understood to be an all-inclusive surface, which can be reduced to successively more sparse and more fundamental types of motion. The levels of structure that operate in Schenker's theory are the foreground, which is seen in all the details of the musical score; the middle ground, which is roughly a summary of an essential contrapuntal progression and voice-leading; and the background or Ursatz, which is one of only a few basic "long-range counterpoint" structures that are shared in the gamut of tonal music literature.
The pitches and form of tonal music are organized hierarchically, all pitches deriving their importance from their relationship to a tonic key, and secondary themes in other keys are brought back to the tonic in a recapitulation of the primary theme. Susan McClary connects this specifically in the sonata-allegro form to the feminist hierarchy of gender (see above) in her book "Feminine Endings", even pointing out that primary themes were often previously called "masculine" and secondary themes "feminine."
Ethics, behavioral psychology, philosophies of identity.
In ethics, various virtues are enumerated and sometimes organized hierarchically according to certain brands of virtue theory.
In all of these random examples, there is an asymmetry of 'compositional' significance between levels of structure, so that small parts of the whole hierarchical array depend, for their meaning, on their membership in larger parts.There is a hierarchy of activities in human life: productive activity serves or is guided by the moral life; the moral life is guided by practical reason; practical reason (used in moral and political life) serves contemplative reason (whereby we contemplate God). Practical reason sets aside time and resources for contemplative reason.
In the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work "Radical Empiricism" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved.
Hierarchy in ethics emerged in Western Europe, West Asia and North Africa around the 1600s. In this aspect, the term hierarchy refers to how distinguishable they are from real to unreal. Feminists, Marxists, anarchists, communists, critical theorists and others, all of whom have multiple interpretations, criticize the hierarchies commonly found within human society, especially in social relationships. Hierarchies are present in all parts of society: in businesses, schools, families, etc. These relationships are often viewed as necessary. Entities that stand in hierarchical arrangements are animals, humans, plants, etc. In some cultures, God can also be an addition to this hierarchy. However, feminists, Marxists, critical theorists and others analyze hierarchy in terms of the values and power that it arbitrarily assigns to one group over another. Hierarchical ethics offers a way of logical reasoning that is compatible with religious commitments. In some cultures, there is hierarchy within humanity. The dominant man in a family is above women, and children are after. In social classes, they are arranged as follows: king, civic officials, craftsmen, unskilled workers.

</doc>
<doc id="14002" url="http://en.wikipedia.org/wiki?curid=14002" title="Outline of healthcare science">
Outline of healthcare science

The following outline is provided as an overview of and topical guide to healthcare science.
Healthcare science is an applied science that addresses the use of science, technology, engineering and mathematics in the delivery of healthcare.

</doc>
<doc id="14004" url="http://en.wikipedia.org/wiki?curid=14004" title="Hour">
Hour

The hour (common symbol: h or hr) is a unit of measurement of time. In modern usage, an hour comprises 60 minutes, or 3,600 seconds. It is approximately 1/24 of a mean solar day.
An hour in the Universal Coordinated Time (UTC) time standard can include a negative or positive leap second, and may therefore have a duration of 3,599 or 3,601 seconds for adjustment purposes.
Although it is not a standard defined by the International System of Units, the hour is a unit accepted for use with SI, represented by the symbol h.
Etymology.
The Middle English word "ure" first appears in the 13th century, as a loanword from Old French "ure", "ore", from Latin "hōra". "Hora", in turn, derives from Greek ὥρα ("season, time of day, hour"). In terms of the Proto-Indo-European language, ὥρα is a cognate of English "year" and is derived from the Proto-Indo-European word "*i̯ēro-" ("year, summer").
The "ure" of Middle English and the Anglo-French "houre" gradually supplanted the Old English nouns "tīd" (which survives in Modern English as "tide") and "stund". "Stund" is the progenitor of "", which remains an archaic synonym for "hour". "Stund" is related to the Old High German "stunta", from Germanic "*stundō" ("time, interval, while").
History.
Ancient Egyptians used sundials that "divided a sunlit day into 10 parts plus two "twilight hours" in the morning and evening." The Greek astronomer, Andronicus of Cyrrhus, oversaw the construction of a horologion called the Tower of the Winds in Athens during the first century BCE. This structure tracked a 24-hour day using both sundials and mechanical hour indicators.
Ancient Sumer and India also divided days into either one twelfth of the time between sunrise and sunset or one twenty-fourth of a full day. In either case the division reflected the widespread use of a duodecimal numbering system. The importance of 12 has been attributed to the number of lunar cycles in a year. In China, the whole day was divided into twelve parts.
Astronomers in Egypt's Middle Kingdom (9th and 10th Dynasties) observed a set of 36 decan stars throughout the year. These star tables have been found on the lids of coffins of the period. The heliacal rising of the next decan star marked the start of a new civil week, which was then ten days. The period from sunset to sunrise was marked by 18 decan stars. Three of these were assigned to each of the two twilight periods, so the period of total darkness was marked by the remaining 12 decan stars, resulting in the 12 divisions of the night. The time between the appearance of each of these decan stars over the horizon during the night would have been about 40 modern minutes. During the New Kingdom, the system was simplified, using a set of 24 stars, 12 of which marked the passage of the night.
Ancient Sinhalese in Sri Lanka divided a solar day into 60 "Peya" (now called Sinhala Peya). One Sinhala Peya was divided into 24 "Vinadi". Since 60 (peya) x 24 (vinadi) = 24 (hours) x 60 (minutes), one "Vinadi" is equal to one present-day standard minute.
Earlier definitions of the hour varied within these parameters:
Counting hours.
Many different ways of counting the hours have been used. Because sunrise, sunset, and, to a lesser extent, noon, are the conspicuous points in the day, starting to count at these times was, for most people in most early societies, much easier than starting at midnight. However, with accurate clocks and modern astronomical equipment (and the telegraph or similar means to transfer a time signal in a split-second), this issue is much less relevant.
Astrolabes, sundials, and astronomical clocks sometimes show the hour length and count using some of these older definitions and counting methods.
Counting from dawn.
In ancient and medieval cultures, the counting of hours generally started with sunrise. Before the widespread use of artificial light, societies were more concerned with the division between night and day, and daily routines often began when light was sufficient.
Unequal hours.
Sunrise marked the beginning of the first hour (the "zero" hour), the middle of the day was at the end of the sixth hour and sunset at the end of the twelfth hour. This meant that the duration of hours varied with the season. In the Northern hemisphere, particularly in the more northerly latitudes, summer daytime hours were longer than winter daytime hours, each being one twelfth of the time between sunrise and sunset. These variable-length hours were variously known as temporal, unequal, or seasonal hours and were in use until the appearance of the mechanical clock, which furthered the adoption of equal length hours.
This is also the system used in Jewish law and frequently called "Talmudic hour" ("Sha'a Zemanit") in a variety of texts. The talmudic hour is one twelfth of time elapsed from sunrise to sunset, day hours therefore being longer than night hours in the summer; in winter they reverse.
The Indic day began at sunrise. The term "Hora" was used to indicate an hour. The time was measured based on the length of the shadow at day time. A "Hora" translated to 2.5 "Pe." There are 60 "Pe" per day, 60 minutes per "Pe" and 60 "Kshana" (snap of a finger or instant) per minute. "Pe" was measured with a bowl with a hole placed in still water. Time taken for this graduated bowl was one "Pe." Kings usually had an officer in charge of this clock.
Babylonian hours.
Babylonian hours divide the day and night into 24 equal hours, reckoned from the time of sunrise.
Counting from sunset.
In so-called "Italian time", "Italian hours", or "Old Czech Time", the first hour started with the sunset Angelus bell (or at the end of dusk, i.e., half an hour after sunset, depending on local custom and geographical latitude). The hours were numbered from 1 to 24. For example, in Lugano, the sun rose in December during the 14th hour and noon was during the 19th hour; in June the Sun rose during the 7th hour and noon was in the 15th hour. Sunset was always at the end of the 24th hour. The clocks in church towers struck only from 1 to 12, thus only during night or early morning hours.
This manner of counting hours had the advantage that everyone could easily know how much time they had to finish their day's work without artificial light. It was already widely used in Italy by the 14th century and lasted until the mid-18th century; it was officially abolished in 1755, or in some regions, customary, until the mid-19th century.
The system of Italian hours can be seen on a number of clocks in Europe, where the dial is numbered from 1 to 24 in either Roman or Arabic numerals. The St Mark's Clock in Venice, and the Orloj in Prague are famous examples. It was also used in Poland and Bohemia until the 17th century.
The Islamic day begins at sunset. The first prayer of the day (maghrib) is to be performed between just after sunset and the end of twilight.
Counting from noon.
For many centuries, up to 1925, astronomers counted the hours and days from noon, because it was the easiest solar event to measure accurately. An advantage of this method (used in the Julian Date system, in which a new Julian Day begins at noon) is that the date doesn't change during a single night's observing.
Counting from midnight.
In the modern 12-hour clock, counting the hours starts at midnight and restarts at noon. Hours are numbered 12, 1, 2, ..., 11. Solar noon is always close to 12 noon, differing according to the equation of time by as much as fifteen minutes either way. At the equinoxes sunrise is around 6 A.M. ("ante meridiem", "before noon"), and sunset around 6 P.M. ("post meridiem", "after noon").
In the modern 24-hour clock, counting the hours starts at midnight and hours are numbered from 0 to 23. Solar noon is always close to 12:00, again differing according to the equation of time. At the equinoxes sunrise is around 06:00 and sunset around 18:00.
Derived measures and applications.
Although the SI unit for speed is metres per second, in everyday usage kilometres per hour or, in the USA and the UK, miles per hour are more practical. Occasionally the metre per hour is used for slow-moving objects like snails.
Worker compensation is commonly based on working time in terms of number of hours worked, referred to as an hourly wage. Worker schedules are categorized by number of work hours per day or number of work hours per week; these are regulated and distinguish part-time from full-time jobs. The man-hour describes the amount of work that a person can complete in one hour. Many professionals such as lawyers and therapists charge a fee per hour.
The credit hour measures the time commitment of an academic course (typically university level) in terms of "contact hours" between students and staff per week. For example, a class that meets for one hour three times a week is said to be a 3-hour class.
The kilowatt hour, the energy expended by a 1000 watt device in one hour, is commonly used as a billing unit for energy delivered to consumers by electric utilities. Conversely, the Btu per hour is a unit of power used in the power industry and heating/cooling applications. In the railroad industry, when sharing locomotives, the horsepower-hour may be used as a measure of energy.
The ampere-hour is a unit of electric charge used in measurements of electrochemical systems such as electroplating and batteries.
In indoor air quality, air changes per hour measures how many times the air within a defined space, such as a room or house, is replaced in an hour.
Passengers per hour per direction is used to describe the capacity of public transport systems.
Pound per hour is a mass flow unit used for fuel flow in engines.

</doc>
<doc id="14005" url="http://en.wikipedia.org/wiki?curid=14005" title="Hezekiah">
Hezekiah

Hezekiah (; Hebrew: חִזְקִיָּ֫הוּ, חִזְקִיָּ֫ה, יְחִזְקִיָּ֫הוּ; Greek: Ἐζεκίας, "Ezekias", in the Septuagint; Latin: "Ezechias"; also transliterated as "Ḥizkiyyahu" or "Ḥizkiyyah") was, according to the Hebrew Bible, the son of Ahaz and the 13th king of Judah. Archaeologist Edwin Thiele has concluded that his reign was between c. 715 and 686 BC. He is also one of the most prominent kings of Judah mentioned in the Hebrew Bible and is one of the kings mentioned in the genealogy of Jesus in the Gospel of Matthew.
According to the Hebrew Bible, Hezekiah witnessed the destruction of the northern Kingdom of Israel by Sargon's Assyrians in c. 720 BC and was king of Judah during the invasion and siege of Jerusalem by Sennacherib in 701 BC. Hezekiah enacted sweeping religious reforms, including a strict mandate for the sole worship of Yahweh and a prohibition on venerating other deities within the Temple in Jerusalem. Isaiah and Micah prophesied during his reign.
Etymology.
Hezekiah, more properly transliterated as Ḥizkiyyahu (and sometimes as Ezekias) (Hebrew: חִזְקִיָּ֫הוּ Ḥizqiyyāhu, Khizkiyahu; or יְחִזְקִיָּ֫הוּ Yəḥizqiyyāhu, Y'khizkiyahu); ; or Ḥizkiyyah (Hebrew: חִזְקִיָּ֫ה Ḥizqiyyāh). The root of the name חִזְקִיָּהוּ Ḥizkiyyahu is חזק, a verb stem that can mean
It also spawns a number of nouns, including
as well as the adjectives
Accordingly, חִזְקִיָּהוּ Ḥizkiyyahu can be said to mean something like "Strengthened by Yahweh".
Biblical sources.
The main account of Hezekiah's reign is found in , , and of the Hebrew Bible. mentions that it is a collection of King Solomon's proverbs that were "copied" "by the officials of King Hezekiah of Judah". His reign is also referred to in the books of the prophets Jeremiah, Hosea, and Micah. The books of Hosea and Micah record that their prophecies were made during Hezekiah’s reign.
Family and life.
Hezekiah was the son of King Ahaz and Abijah. His mother, Abijah (also called Abi), was a daughter of the high priest Zechariah. Based on Thiele's dating, Hezekiah was born in c. 741 BC. He was married to Hephzi-bah. () He died from natural causes at the age of 54 in c. 687 BC, and was succeeded by his son Manasseh ().
Reign over Judah.
According to the Hebrew Bible, Hezekiah assumed the throne of Judah at the age of 25 and reigned for 29 years (). Some writers have proposed that Hezekiah served as coregent with his father Ahaz for about 14 years. His sole reign is dated by William F. Albright as 715–687 BC, and by Edwin R. Thiele as 716–687 BC (the last ten years being a co-regency with his son Manasseh).
Hezekiah purified and repaired the Temple, purged its idols, and reformed the priesthood. In an effort to abolish what he considered idolatry from his kingdom, he destroyed the high places (or bamot) and "bronze serpent" (or "Nehushtan"), recorded as being made by Moses, which became an objects of idolatrous worship. In place of this, he centralized the worship of God at the Jerusalem Temple. Hezekiah also resumed the Passover pilgrimage and the tradition of inviting the scattered tribes of Israel to take part in a Passover festival. He sent messengers to Ephraim and Manasseh inviting them to Jerusalem for the celebration of the Passover. The messengers, however, were not only not listened to, but were even laughed at; only a few men of Asher, Manasseh, and Zebulun came to Jerusalem. Nevertheless the Passover was celebrated with great solemnity and such rejoicing as had not been in Jerusalem since the days of Solomon. Hezekiah is portrayed by the Hebrew Bible as a great and good king. 
Political moves and Assyrian invasion.
When Sargon II, the king of Assyria, died in 705 B.C., states, including Judah, that were subject to Assyria saw an opportunity to throw off their subservience to the Assyrian kings. He ceased to pay the tribute imposed on his father, and entered into a league with Egypt. In 703 B.C. Sennacherib, Sargon's son and successor, began a series of major campaigns to quash opposition to Assyrian rule. After dealing with rebels in the eastern part of the realm, in 701 B.C. he king turned toward those in the west. Though Hezekiah expected the Egyptians to come to his aid, they did not, and Hezekiah had to face the invasion of Judah by Sennacherib.
The Assyrians recorded that Sennacherib lifted his siege of Jerusalem after Hezekiah acknowledged Sennacherib as his overlord and paid him tribute. The Hebrew Bible records that Hezekiah tried to pay off Sennacherib with three hundred talents of silver and thirty of gold as tribute, even despoiling the doors of the Temple to produce the promised amount, but, after the payment was made, Sennacherib renewed his assault on Jerusalem. Sennacherib besieged Jerusalem and sent his Rabshakeh to the walls as a messenger. The Rabshakeh addressed the soldiers manning the city wall in Hebrew ("Yĕhuwdiyth"), asking them to distrust Yahweh and Hezekiah, pointing to Hezekiah's righteous reforms (destroying the High Places) as a sign that the people should not trust their god to be favorably disposed (). records that Hezekiah went to the Temple and there he prayed, the first king of Judah recorded to have done so in about 250 years, since the time of Solomon.
Hezekiah's construction.
Knowing that Jerusalem would eventually be subject to siege, Hezekiah had been preparing for war for some time by fortifying the walls of Jerusalem, building towers, and constructing a tunnel to bring fresh water to the city from a spring outside its walls. He made at least two major preparations that would help Jerusalem to resist conquest: the construction of Hezekiah's Tunnel (also known as the Siloam Tunnel), and construction of the Broad Wall.
"When Sennacherib had come, intent on making war against Jerusalem, Hezekiah consulted with his officers and warriors about stopping the flow of the springs outside the city … for otherwise, they thought, the King of Assyria would come and find water in abundance" (). The narratives of the Hebrew Testament state that Sennacherib besieged Jerusalem (; ; ; ).
Death of Sennacherib.
Sennacherib failed to conquer Judah in full before his death.
 says
"It came about as he was worshiping in the house of Nisroch his god, that Adrammelech and Sharezer killed him [Sennacherib] with the sword; and they escaped into the land of Ararat. And Esarhaddon his son became king in his place."
Assyrian records say that Sennacherib was assassinated in 681 BC (20 years after the 701 BC invasion of Judah). A Neo-Babylonian letter corroborates with the biblical account a sentiment from Sennacherib’s sons to assassinate him, an event Assyriologists have reconstructed as historical. The son Ardi-Mulishi, who is mentioned in the letter as killing anyone who would reveal his conspiracy, successfully murders his father in c. 681 BC, and was most likely the Adrammelech in 2 Kings, though Sharezer is not known elsewhere. Assyryologists posit the murder was motivated because Esarhaddon was chosen as heir to the throne instead of Ardi-Mulishi, the next eldest son. Assyrian and Hebrew biblical history corroborate that Esarhaddon ultimately did succeed the throne.
Hezekiah's illness and death.
The narrative of Hezekiah's sickness and miraculous recovery is found in , , . Various ambassadors came to congratulate him on his recovery, among them from Merodach-baladan, the king of Babylon (; "2 Kings" 20:12). Hezekiah is also remembered for giving too much information to Baladan, king of Babylon (or perhaps for boasting about his wealth), for which he was confronted by Isaiah the prophet ().
According to the Talmud, the disease came about because of a dispute between him and Isaiah over who should pay whom a visit and over Hezekiah's refusal to marry and have children. According to , Hezekiah lived another 15 more years of life after praying to God. Some Talmudists also considered that it might have come about as a way for Hezekiah to purge his sins or due to his arrogance in assuming his righteousness.
Extra-Biblical records.
Extra-Biblical sources do much more for us than give us a pan-Mid Eastern picture into which we contextualize Hezekiah: there are extra-Biblical sources that specify Hezekiah by name, along with his reign and influence. "Historiographically, his reign is noteworthy for the convergence of a variety of biblical sources and diverse extrabiblical evidence often bearing on the same events. Significant data concerning Hezekiah appear in the Deuteronomistic History, the Chronicler, Isaiah, Assyrian annals and reliefs, Israelite epigraphy, and, increasingly, stratigraphy". Archaeologist Amihai Mazar calls the tensions between Assyria and Judah "one of the best-documented events of the Iron Age" (172). Hezekiah's story is one of the best to cross-reference with the rest of the Mid Eastern world's historical documents.
Archaeological record.
A lintel inscription, found over the doorway of a tomb, has been ascribed to his secretary, Shebnah ).
LMLK store jars along the border with Assyria "demonstrate careful preparations to counter Sennacherib's likely route of invasion" and show "a notable degree of royal control of towns and cities which would facilitate Hezekiah's destruction of rural sacrificial sites and his centralization of worship in Jerusalem". Evidence suggests they were used throughout his 29-year reign (Grena, 2004, p. 338). There are some Bullae from sealed documents that may have belonged to Hezekiah himself (Grena, 2004, p. 26, Figs. 9 and 10). There are also some that name his servants ("ah-vah-deem" in Hebrew, ayin-bet-dalet-yod-mem). However, they are all from the antiquities market and subject to authentication disputes (see Biblical archaeology).
Increase in the power of Judah.
According to the work of archaeologists and philologists, the reign of Hezekiah saw a notable increase in the power of the Judean state. At this time Judah was the strongest nation on the Assyrian-Egyptian frontier. There were increases in literacy and in the production of literary works. The massive construction of the Broad Wall was made during his reign, the city was enlarged to accommodate a large influx, and population increased in Jerusalem up to 25,000, "five times the population under Solomon." Archaeologist Amihai Mazar explains, "Jerusalem was a virtual city-state where the majority of the state's population was concentrated," in comparison to the rest of Judah's cities (167). Archaeologist Israel Finkelstein says, "The key phenomenon—which cannot be explained solely against the background of economic prosperity—was the sudden growth of the population of Jerusalem in particular, and of Judah in general" (153). He says the cause of this growth must be a large influx of Israelites fleeing from the Assyrian destruction of the northern state. It is "[t]he only reasonable way to explain this unprecedented demographic development" (154). This, according to Finkelstein, set the stage for motivations to compile and reconcile Hebrew history into a text at that time (157). Mazar questions this explanation, since, he argues, it is "no more than an educated guess" (167).
Siloam inscription.
Hezekiah's Siloam Tunnel was chiseled through 533 meters (1,750 feet) of solid rock in order to provide Jerusalem underground access to the waters of the Gihon Spring or Siloam Pool, which lay outside the city.
The Siloam Inscription from the Siloam Tunnel is now in the Istanbul Archeological Museum. It "commemorates the dramatic moment when the two original teams of tunnelers, digging with picks from opposite ends of the tunnel, met each other" (564). It is "[o]ne of the most important ancient Hebrew inscriptions ever discovered." Finkelstein and Mazar cite this tunnel as an example of Jerusalem's impressive state-level power at the time.
Archeologists like William G. Dever have pointed at archaeological evidence for the iconoclasm during the period of Hezekiah's reign. The central cult room of the temple at Arad (a royal Judean fortress) was deliberately and carefully dismantled, "with the altars and massebot" concealed "beneath a Str. 8 plaster floor". This stratum correlates with the late 8th century; Dever concludes that "the deliberate dismantling of the temple and its replacement by another structure in the days of Hezekiah is an archeological fact. I see no reason for skepticism here."
Lachish relief.
Under Rehoboam, Lachish became the second most important city of the kingdom of Judah. During the revolt of king Hezekiah against Assyria, it was captured by Sennacherib despite determined resistance (see Siege of Lachish).
As the Lachish relief attests, Sennacherib began his siege of the city of Lachish in 701 BC. The Lachish Relief graphically depicts the battle, and the defeat of the city, including Assyrian archers marching up a ramp and Judahites pierced through on mounted stakes. "The reliefs on these slabs" discovered in the Assyrian palace at Nineveh "originally formed a single, continuous work, measuring 8 feet ... tall by 80 feet ... long, which wrapped around the room" (559). Visitors "would have been impressed not only by the magnitude of the artwork itself but also by the magnificent strength of the Assyrian war machine."
Sennacherib's Prism of Nineveh.
Sennacherib's Prism was found buried in the foundations of the Nineveh palace. It was written in cuneiform, the Mesopotamian form of writing of the day. The prism records the conquest of 46 strong towns and "uncountable smaller places," along with the siege of Jerusalem where Sennacherib says he just "shut him up...like a bird in a cage," subsequently enforcing a larger tribute upon him. 
The Hebrew Bible states that during the night, an angel of Yahweh brought death to 185,000 Assyrians troops. There is no account of that in the prism, nor is there any evidence that Sennacherib's army was defeated. Certainly, Sennacherib's campaign continued with further successful attacks on Elam and Babylon, suggesting his army left Jerusalem at full strength. 
Sennacherib also recorded a payment of 800 silver talents to him at the end of the conflict, which suggests a capitulation to end a siege of 500 silver talents, rather than a victory for the Judeans. Furthermore the annals record a list of booty sent from Jerusalem to Nineveh. Hezekiah remained on his throne as a vassal ruler. (The campaign is recorded with differences in the Assyrian records and in the biblical Books of Kings; there is general agreement that the Assyrian sources should be given priority). 
One theory that takes the biblical view as historical fact posits that a defeat was caused by "possibly an outbreak of the bubonic plague" (303). Another that this is a composite text which makes use of a 'legendary motif' analogous to that of the exodus story.
Other records.
The Talmud (Bava Batra 15a) credits Hezekiah with overseeing the compilation of the biblical books of Isaiah, Proverbs, Song of Songs and Ecclesiastes.
According to Jewish tradition, the victory over the Assyrians and Hezekiah's return to health happened at the same time, the first night of Passover.
The Greek historian, Herodotus (c. 484 BC – c. 425 BC), wrote of the invasion and acknowledges many Assyrian deaths, which he claims were the result of a plague of mice. The Jewish historian, Josephus, followed the writings of Herodotus. These historians record Sennacherib's failure to take Jerusalem is "uncontested
Chronological interpretation.
Understanding the biblically recorded sequence of events in Hezekiah's life as chronological or not is critical to the contextual interpretation of his reign. According to scholar Stephen L. Harris, chapter 20 of 2 Kings does not follow the events of chapters 18 and 19 (161). Rather, the Babylonian envoys precede the Assyrian invasion and siege. Chapter 20 would have been added during the exile, and Harris says it "evidently took place before Sennacherib's invasion' when Hezekiah was "trying to recruit Babylon as an ally against Assyria.' Consequently, "Hezekiah ends his long reign impoverished and ruling over only a tiny scrap of his former domain.' Likewise, "The Archaeological Study Bible" says, "The presence of these riches' that Hezekiah shows to the Babylonians "indicates that this event took place before Hezekiah's payment of tribute to Sennacherib in 701 B.C." (564). Again, "Though the king's illness and the subsequent Babylonian mission are described at the end of the accounts of his reign, they must have occurred before the war with Assyria. Thus, Isaiah's chastening of Hezekiah is due to his alliances made with other countries during the Assyrian conflict for insurance, if you will. To a reader who interprets the chapters chronologically, it would appear that Hezekiah ended his reign at a climax, but with a scholarly analysis, his end would contrarily be interpreted as a long fall from where he began.
Other chronological notes.
There has been considerable academic debate about the actual dates of reigns of the Israelite kings. Scholars have endeavored to synchronize the chronology of events referred to in the Hebrew Bible with those derived from other external sources. In the case of Hezekiah, scholars have noted that the apparent inconsistencies are resolved by accepting the evidence that Hezekiah, like his predecessors for four generations in the kings of Judah, had a coregency with his father, and this coregency began in 729 BC.
As an example of the reasoning that finds inconsistencies in calculations when coregencies are "a priori" ruled out, dates the fall of Samaria (the Northern Kingdom) to the 6th year of Hezekiah's reign. William F. Albright has dated the fall of the Kingdom of Israel to 721 BC, while E. R. Thiele calculates the date as 723 BC. If Abright's or Thiele's dating are correct, then Hezekiah's reign would begin in either 729 or 727 BC. On the other hand, states that Sennacherib invaded Judah in the 14th year of Hezekiah's reign. Dating based on Assyrian records date this invasion to 701 BC, and Hezekiah's reign would therefore begin in 716/715 BC. This dating would be confirmed by the account of Hezekiah's illness in chapter 20, which immediately follows Sennacherib's departure (). This would date his illness to Hezekiah's 14th year, which is confirmed by Isaiah's statement () that he will live fifteen more years (29 − 15 = 14). As shown below, these problems are all addressed by scholars who make reference to the ancient Near Eastern practice of coregency.
Following the approach of Wellhausen, another set of calculations shows it is probable that Hezekiah did not ascend the throne before 722 BC. By Albright's calculations, Jehu's initial year is 842 BC; and between it and Samaria's destruction the "Books of Kings" give the total number of the years the kings of Israel ruled as 143 7/12, while for the kings of Judah the number is 165. This discrepancy, amounting in the case of Judah to 45 years (165–120), has been accounted for in various ways; but every one of those theories must allow that Hezekiah's first six years fell before 722 BC. (That Hezekiah began to reign before 722 BC, however, is entirely consistent with the principle that the Ahaz/Hezekiah coregency began in 729 BC.) Nor is it clearly known how old Hezekiah was when called to the throne, although states he was twenty-five years of age. His father died at the age of thirty-six (); it is not likely that Ahaz at the age of eleven should have had a son. Hezekiah's own son Manasseh ascended the throne twenty-nine years later, at the age of twelve. This places his birth in the seventeenth year of his father's reign, or gives Hezekiah's age as forty-two, if he was twenty-five at his ascension. It is more probable that Ahaz was twenty-one or twenty-five when Hezekiah was born (and suggesting an error in the text), and that the latter was thirty-two at the birth of his son and successor, Manasseh.
Since Albright and Friedman, several scholars have explained these dating problems on the basis of a coregency between Hezekiah and his father Ahaz between 729 and 716/715 BC. Assyriologists and Egyptologists recognize that coregency was a practice both in Assyria and Egypt, After noting that coregencies were only used sporadically in the northern kingdom (Israel), Nadav Na'aman writes,
In the kingdom of Judah, on the other hand, the nomination of a co-regent was the common procedure, beginning from David who, before his death, elevated his son Solomon to the throneWhen taking into account the permanent nature of the co-regency in Judah from the time of Joash, one may dare to conclude that dating the co-regencies accurately is indeed the key for solving the problems of biblical chronology in the eighth century B.C."
Among the numerous scholars who have recognized the coregency between Ahaz and Hezekiah are Kenneth Kitchen in his various writings, Leslie McFall, and Jack Finegan. McFall, in his 1991 article, argues that if 729 BC (that is, the Judean regnal year beginning in Tishri of 729) is taken as the start of the Ahaz/Hezekiah coregency, and 716/715 BC as the date of the death of Ahaz, then all the extensive chronological data for Hezekiah and his contemporaries in the late eighth century BC are in harmony. Further, McFall found that no textual emendations are required among the numerous dates, reign lengths, and synchronisms given in the Hebrew Testament for this period. In contrast, those who do not accept the Ancient Near Eastern principle of coregencies require multiple emendations of the Scriptural text, and there is no general agreement on which texts should be emended, nor is there any consensus among these scholars on the resultant chronology for the eighth century BC. This is in contrast with the general consensus among those who accept the Biblical and near Eastern practice of coregencies that Hezekiah was installed as coregent with his father Ahaz in 729 BC, and the synchronisms of 2 Kings 18 must be measured from that date, whereas the synchronisms to Sennacherib are measured from the sole reign starting in 716/715 BC. The two synchronisms to Hoshea of Israel in 2 Kings 18 are then in exact agreement with the dates of Hoshea's reign that can be determined from Assyrian sources, as is the date of Samaria's fall as stated in 2 Kings 18:10. An analogous situation of two ways of measurement, both equally valid, is encountered in the dates given for Jehoram of Israel, whose first year is synchronized to the 18th year of the sole reign of Jehoshaphat of Judah in 2 Kings 3:1 (853/852 BC), but his reign is also reckoned according to another method as starting in the second year of the coregency of Jehoshaphat and his son Jehoram of Judah (2 Kings 1:17); both methods refer to the same calendrical year.
Scholars who accept the principle of coregencies note that abundant evidence for their use is found in the biblical material itself. The agreement of scholarship built on these principles with both biblical and secular texts was such that the Thiele/McFall chronology was accepted as the best chronology for the kingdom period in Jack Finegan's encyclopedic "Handbook of Biblical Chronology".

</doc>
<doc id="14006" url="http://en.wikipedia.org/wiki?curid=14006" title="Haemophilia">
Haemophilia

Haemophilia (; also spelled hemophilia in North America, from the Greek "haima" αἷμα 'blood' and "philia" φιλία 'love') is a group of hereditary genetic disorders that impair the body's ability to control blood clotting, which is used to stop bleeding when a blood vessel is broken. Haemophilia A (clotting factor VIII deficiency) is the most common form of the disorder, present in about 1 in 5,000–10,000 male births. Haemophilia B (factor IX deficiency) occurs in around 1 in about 20,000–34,000 male births.
Like other recessive sex-linked, X chromosome disorders, haemophilia is more likely to occur in males than females. This is because females have two X chromosomes while males have only one, so the defective gene is guaranteed to manifest in any male who carries it. Because females have two X chromosomes and haemophilia is rare, the chance of a female having two defective copies of the gene is very remote, so females are almost exclusively asymptomatic carriers of the disorder. Female carriers can inherit the defective gene from either their mother or father, or it may be a new mutation. Although it is not impossible for a female to have haemophilia, it is unusual: daughters which are the product of both a male with haemophilia A or B and a female carrier will have haemophilia, while the non-sex-linked haemophilia C due to coagulant factor XI deficiency, which can affect either sex, is more common in Jews of Ashkenazi (east European) descent but rare in other population groups.
People with haemophilia have lower clotting factor level of blood plasma or impaired activity of the coagulation factors needed for a normal clotting process. Thus when a blood vessel is injured, a temporary scab does form, but the missing coagulation factors prevent fibrin formation, which is necessary to maintain the blood clot. A haemophiliac does not bleed more intensely than a person without it, but can bleed for a much longer time. In severe haemophiliacs even a minor injury can result in blood loss lasting days or weeks, or even never healing completely. In areas such as the brain or inside joints, this can be fatal or permanently debilitating.
Signs and symptoms.
Characteristic symptoms vary with severity. In general symptoms are internal or external bleeding episodes, which are called "bleeds". People with more severe haemophilia suffer more severe and more frequent bleeds, while people with mild haemophilia usually suffer more minor symptoms except after surgery or serious trauma. Moderate haemophiliacs have variable symptoms which manifest along a spectrum between severe and mild forms.
In both haemophilia A and B, there is spontaneous bleeding but a normal bleeding time, normal prothrombin time, normal thrombin time, but prolonged partial thromboplastin time. Internal bleeding is common in people with severe haemophilia and some individuals with moderate haemophilia. The most characteristic type of internal bleed is a joint bleed where blood enters into the joint spaces. This is most common with severe haemophiliacs and can occur spontaneously (without evident trauma). If not treated promptly, joint bleeds can lead to permanent joint damage and disfigurement. Bleeding into soft tissues such as muscles and subcutaneous tissues is less severe but can lead to damage and requires treatment.
Children with mild to moderate haemophilia may not have any signs or symptoms at birth especially if they do not undergo circumcision. Their first symptoms are often frequent and large bruises and haematomas from frequent bumps and falls as they learn to walk. Swelling and bruising from bleeding in the joints, soft tissue, and muscles may also occur. Children with mild haemophilia may not have noticeable symptoms for many years. Often, the first sign in very mild haemophiliacs is heavy bleeding from a dental procedure, an accident, or surgery. Females who are carriers usually have enough clotting factors from their one normal gene to prevent serious bleeding problems, though some may present as mild haemophiliacs.
Complications.
Severe complications are much more common in severe and moderate haemophiliacs. Complications may be both directly from the disease or from its treatment:
Haemophilic arthropathy is characterized by chronic proliferative synovitis and cartilage destruction. If an intra-articular bleed is not drained early, it may cause apoptosis of chondrocytes and affect the synthesis of proteoglycans. The hypertrophied and fragile synovial lining while attempting to eliminate excessive blood may be more likely to easily rebleed, leading to a vicious cycle of hemarthrosis-synovitis-hemarthrosis. In addition, iron deposition in the synovium may induce an inflammatory response activating the immune system and stimulating angiogenesis, resulting in cartilage and bone destruction.
Life expectancy.
Like most aspects of the disorder, life expectancy varies with severity and adequate treatment. People with severe haemophilia who don't receive adequate, modern treatment have greatly shortened lifespans and often do not reach maturity. Prior to the 1960s when effective treatment became available, average life expectancy was only 11 years. By the 1980s the life span of the average haemophiliac receiving appropriate treatment was 50–60 years. Today with appropriate treatment, males with haemophilia typically have a near normal quality of life with an average lifespan approximately 10 years shorter than an unaffected male.
Since the 1980s the primary leading cause of death of people with severe haemophilia has shifted from haemorrhage to HIV/AIDS acquired through treatment with contaminated blood products. The second leading cause of death related to severe haemophilia complications is intracranial haemorrhage which today accounts for one third of all deaths of people with haemophilia. Two other major causes of death include hepatitis infections causing cirrhosis and obstruction of air or blood flow due to soft tissue haemorrhage.
Causes.
Genetics.
Females possess two X-chromosomes, males have one X and one Y-chromosome. Since the mutations causing the disease are X-linked, a woman carrying the defect on one of her X-chromosomes may not be affected by it, as the equivalent allele on her other chromosome should express itself to produce the necessary clotting factors, due to X inactivation. However, the Y-chromosome in men has no gene for factors VIII or IX. If the genes responsible for production of factor VIII or factor IX present on a male's X-chromosome are deficient there is no equivalent on the Y-chromosome to cancel it out, so the deficient gene is not masked and he will develop the illness.
Since a male receives his single X-chromosome from his mother, the son of a healthy female silently carrying the deficient gene will have a 50% chance of inheriting that gene from her and with it the disease; and if his mother is affected with haemophilia, he will have a 100% chance of being a haemophiliac. In contrast, for a female to inherit the disease, she must receive two deficient X-chromosomes, one from her mother and the other from her father (who must therefore be a haemophiliac himself). Hence haemophilia is far more common among males than females. However, it is possible for female carriers to become mild haemophiliacs due to lyonisation (inactivation) of the X-chromosomes. Haemophiliac daughters are more common than they once were, as improved treatments for the disease have allowed more haemophiliac males to survive to adulthood and become parents. Adult females may experience menorrhagia (heavy periods) due to the bleeding tendency. The pattern of inheritance is criss-cross type. This type of pattern is also seen in colour blindness.
A mother who is a carrier has a 50% chance of passing the faulty X-chromosome to her daughter, while an affected father will always pass on the affected gene to his daughters. A son cannot inherit the defective gene from his father. This is a recessive trait and can be passed on if cases are more severe with carrier.
Genetic testing and genetic counselling is recommended for families with haemophilia. Prenatal testing, such as amniocentesis, is available to pregnant women who may be carriers of the condition.
As with all genetic disorders, it is of course also possible for a human to acquire it spontaneously through mutation, rather than inheriting it, because of a new mutation in one of their parents' gametes. Spontaneous mutations account for about 33% of all cases of haemophilia A. About 30% of cases of haemophilia B are the result of a spontaneous gene mutation.
If a female gives birth to a haemophiliac child, either the female is a carrier for the blood disorder or the haemophilia was the result of a spontaneous mutation. Until modern direct DNA testing, however, it was impossible to determine if a female with only healthy children was a carrier or not. Generally, the more healthy sons she bore, the higher the probability that she was not a carrier.
If a male is afflicted with the disease and has children with a female who is not even a carrier, his daughters will be carriers of haemophilia. His sons, however, will not be affected with the disease. The disease is X-linked and the father cannot pass haemophilia through the Y-chromosome. Males with the disorder are then no more likely to pass on the gene to their children than carrier females, though all daughters they sire will be carriers and all sons they father will not have haemophilia (unless the mother is a carrier).
Severity.
There are numerous different mutations which cause each type of haemophilia. Due to differences in changes to the genes involved, people with haemophilia often have some level of active clotting factor. Individuals with less than 1% active factor are classified as having severe haemophilia, those with 1-5% active factor have moderate haemophilia, and those with mild haemophilia have between 5-40% of normal levels of active clotting factor.
Diagnosis.
Haemophilia A can be mimicked by von Willebrand disease.
Additionally, severe cases of vitamin K deficiency can present similar symptoms to haemophilia. This is because vitamin K is necessary for the human body to produce several protein clotting factors. This vitamin deficiency is rare in adults and older children but is common in newborns. Infants are born with naturally low levels of vitamin K and do not yet have the symbiotic gut flora to properly synthesise their own vitamin K. Bleeding issues due to vitamin K deficiency in infants is known as "haemorrhagic disease of the newborn", to avoid this complication newborns are routinely injected with vitamin K supplements.
Management.
Though there is no cure for haemophilia, it can be controlled with regular infusions of the deficient clotting factor, i.e. factor VIII in haemophilia A or factor IX in haemophilia B. Factor replacement can be either isolated from human blood serum, recombinant, or a combination of the two. Some haemophiliacs develop antibodies (inhibitors) against the replacement factors given to them, so the amount of the factor has to be increased or non-human replacement products must be given, such as porcine factor VIII.
If a person becomes refractory to replacement coagulation factor as a result of circulating inhibitors, this may be partially overcome with recombinant human factor VII (NovoSeven), which is registered for this indication in many countries.
In early 2008, the US Food and Drug Administration (FDA) approved Xyntha (Wyeth) anti-haemophilic factor, genetically engineered from the genes of Chinese hamster ovary cells. Since 1993 (Dr. Mary Nugent) recombinant factor products (which are typically cultured in Chinese hamster ovary (CHO) tissue culture cells and involve little, if any human plasma products) have been available and have been widely used in wealthier western countries. While recombinant clotting factor products offer higher purity and safety, they are, like concentrate, extremely expensive, and not generally available in the developing world. In many cases, factor products of any sort are difficult to obtain in developing countries.
In Western countries, common standards of care fall into one of two categories: prophylaxis or on-demand. Prophylaxis involves the infusion of clotting factor on a regular schedule in order to keep clotting levels sufficiently high to prevent spontaneous bleeding episodes. On-demand treatment involves treating bleeding episodes once they arise. In 2007, a clinical trial was published in the "New England Journal of Medicine" comparing on-demand treatment of boys (< 30 months) with haemophilia A with prophylactic treatment (infusions of 25 IU/kg body weight of Factor VIII every other day) in respect to its effect on the prevention of joint-diseases. When the boys reached 6 years of age, 93% of those in the prophylaxis group and 55% of those in the episodic-therapy group had a normal index joint-structure on MRI. Prophylactic treatment, however, resulted in average costs of $300,000 per year. The author of an editorial published in the same issue of the "NEJM" supports the idea that prophylactic treatment not only is more effective than on demand treatment but also suggests that starting after the first serious joint-related haemorrhage may be more cost effective than waiting until the fixed age to begin. This study resulted in the first (October 2008) FDA approval to label any Factor VIII product to be used prophylactically. As a result, the factor product used in the study (Bayer's Kogenate) is now labelled for use to prevent bleeds, making it more likely that insurance carriers in the US will reimburse consumers who are prescribed and use this product prophylactically. Despite Kogenate only recently being "approved" for this use in the US, it and other factor products have been well studied and are often prescribed to treat Haemophilia prophylactically to prevent bleeds, especially joint bleeds.
Gene therapy.
On 10 December 2011, a team of British and American investigators reported the successful treatment of haemophilia B using gene therapy. The investigators inserted the "F9" gene into an adeno-associated virus-8 vector, which has a propensity for the liver, where factor 9 is produced, and remains outside the chromosomes so as not to disrupt other genes. The transduced virus was infused intravenously. To prevent rejection, the people were primed with steroids to suppress their immune response. In October 2013, the Royal Free London NHS Foundation Trust in London reported that after treating six people with haemophilia in early 2011 with the genetically modified adeno-associated virus, over two years later all were still producing blood plasma clotting factor.
Preventive exercises.
It is recommended that people affected with haemophilia do specific exercises to strengthen the joints, particularly the elbows, knees, and ankles. Exercises include elements which increase flexibility, tone, and strength of muscles, increasing their ability to protect joints from damaging bleeds. These exercises are recommended after an internal bleed occurs and on a daily basis to strengthen the muscles and joints to prevent new bleeding problems. Many recommended exercises include standard sports warm-up and training exercises such as stretching of the calves, ankle circles, elbow flexions, and quadriceps sets.
Alternative medicine.
While not a replacement for traditional treatments, preliminary scientific studies indicate that hypnosis and self-hypnosis may be effective at reducing bleeds and the severity of bleeds and thus the frequency of factor treatment. Herbs which strengthen blood vessels and act as astringents may benefit people with haemophilia, however there are no peer reviewed scientific studies to support these claims.
Contraindications.
Anticoagulants such as heparin and warfarin are contraindicated for people with haemophilia as these can aggravate clotting difficulties. Also contraindicated are those drugs which have "blood thinning" side effects. For instance, medicines which contain aspirin, ibuprofen, or naproxen sodium should not be taken because they are well known to have the side effect of prolonged bleeding.
Also contraindicated are activities with a high likelihood of trauma, such as motorcycling and skateboarding. Popular sports with very high rates of physical contact and injuries such as American football, hockey, boxing, wrestling, and rugby should be avoided by people with haemophilia. Other active sports like soccer, baseball, and basketball also have a high rate of injuries, but have overall less contact and should be undertaken cautiously and only in consultation with a doctor.
Epidemiology.
Haemophilia is rare, with only about 1 instance in every 10,000 births (or 1 in 5,000 male births) for haemophilia A and 1 in 50,000 births for haemophilia B. About 18,000 people in the United States have haemophilia. Each year in the US, about 400 babies are born with the disorder. Haemophilia usually occurs in males and less often in females. It is estimated that about 2500 Canadians have haemophilia A, and about 500 Canadians have haemophilia B.
History.
"About seventy or eighty years ago, a woman by name of Smith, settled in the vicinity of Plymouth, New Hampshire, and transmitted the following idiosyncrasy to her descendants. It is one, she observed, to which her family is unfortunately subject, and had been the source not only of great solicitude, but frequently the cause of death. If the least scratch is made on the skin of some of them, as mortal a hemorrhagy will eventually ensue as if the largest wound is inflicted. (…) So assured are the members of this family of the terrible consequences of the least wound, that they will not suffer themselves to be bled on any consideration, having lost a relation by not being able to stop the discharge occasioned by this operation."
John C. Otto, 1803
Scientific discovery.
The first medical professional to describe a disease was Abulcasis. In the tenth century he described families whose males died of bleeding after only minor traumas. While many other such descriptive and practical references to the disease appear throughout historical writings, scientific analysis did not begin until the start of the nineteenth century.
In 1803, Dr. John Conrad Otto, a Philadelphian physician, wrote an account about "a hemorrhagic disposition existing in certain families" in which he called the affected males "bleeders". He recognised that the disorder was hereditary and that it affected mostly males and was passed down by healthy females. His paper was the second paper to describe important characteristics of an X-linked genetic disorder (the first paper being a description of colour blindness by John Dalton who studied his own family). Otto was able to trace the disease back to a woman who settled near Plymouth, NH in 1720. The idea that affected males could pass the trait onto their unaffected daughters was not described until 1813 when John Hay published an account in The New England Journal of Medicine.
In 1924, a Finnish doctor discovered a hereditary bleeding disorder similar to Haemophilia localised in the "Åland Islands", southwest of Finland. This bleeding disorder is called "Von Willebrand Disease".
The term "haemophilia" is derived from the term "haemorrhaphilia" which was used in a description of the condition written by Friedrich Hopff in 1828, while he was a student at the University of Zurich. In 1937, Patek and Taylor, two doctors from Harvard, discovered anti-haemophilic globulin. In 1947, Pavlosky, a doctor from Buenos Aires, found haemophilia A and haemophilia B to be separate diseases by doing a lab test. This test was done by transferring the blood of one haemophiliac to another haemophiliac. The fact that this corrected the clotting problem showed that there was more than one form of haemophilia.
European royalty.
Haemophilia has featured prominently in European royalty and thus is sometimes known as 'the royal disease'. Queen Victoria passed the mutation for Haemophilia B to her son Leopold and, through some of her daughters, to various royals across the continent, including the royal families of Spain, Germany, and Russia. In Russia, Tsarevich Alexei Nikolaevich, son of Nicholas II, was a descendant of Queen Victoria through his mother Empress Alexandra and suffered from haemophilia.
It was claimed that Rasputin was successful at treating Tsarevich's haemophilia. At the time, a common treatment administered by professional doctors was to use aspirin, which worsened rather than lessened the problem. It is believed that, by simply advising against the medical treatment, Rasputin could bring visible and significant improvement to the condition of Tsarevich.
In Spain, Queen Victoria's youngest daughter, Princess Beatrice, had a daughter Victoria Eugenie of Battenberg, who later became Queen of Spain. Two of her sons were haemophiliacs and both died from minor car accidents. Her eldest son, Prince Alfonso of Spain, Prince of Asturias, died at the age of 31 from internal bleeding after his car hit a telephone booth. Her youngest son, Infante Gonzalo, died at age 19 from abdominal bleeding following a minor car accident where he and his sister hit a wall while avoiding a cyclist. Neither appeared injured or sought immediate medical care and Gonzalo died two days later from internal bleeding.
Blood contamination issues.
Prior to 1985, there were no laws enacted within the U.S. to screen blood. As a result, many people with haemophilia who received untested and unscreened clotting factor prior to 1992 were at an extreme risk for contracting HIV and hepatitis C via these blood products. It is estimated that more than 50% of the haemophilia population, i.e. over 10,000 people, contracted HIV from the tainted blood supply in the United States alone.
As a direct result of the contamination of the blood supply in the late 1970s and early/mid-1980s with viruses such as hepatitis and HIV, new methods were developed in the production of clotting factor products. The initial response was to heat-treat (pasteurise) plasma-derived factor concentrate, followed by the development of monoclonal factor concentrates, which use a combination of heat treatment and affinity chromatography to inactivate any viral agents in the pooled plasma from which the factor concentrate is derived. The Lindsay Tribunal in Ireland investigated, among other things, the slow adoption of the new methods.

</doc>
<doc id="14008" url="http://en.wikipedia.org/wiki?curid=14008" title="Hickory (disambiguation)">
Hickory (disambiguation)

Hickory is a type of tree ("Carya" species) found in North America and East Asia.
Hickory may also refer to:
Place names.
In the United States:

</doc>
<doc id="14009" url="http://en.wikipedia.org/wiki?curid=14009" title="Hemicellulose">
Hemicellulose

A hemicellulose (also known as polyose) is any of several heteropolymers (matrix polysaccharides), such as arabinoxylans, present along with cellulose in almost all plant cell walls. While cellulose is crystalline, strong, and resistant to hydrolysis, hemicellulose has a random, amorphous structure with little strength. It is easily hydrolyzed by dilute acid or base as well as myriad hemicellulase enzymes.
Composition.
Hemicelluloses include xylan, glucuronoxylan, arabinoxylan, glucomannan, and xyloglucan.
These polysaccharides contain many different sugar monomers. In contrast, cellulose contains only anhydrous glucose. For instance, besides glucose, sugar monomers in hemicellulose can include xylose, mannose, galactose, rhamnose, and arabinose. Hemicelluloses contain most of the D-pentose sugars, and occasionally small amounts of L-sugars as well. Xylose is in most cases the sugar monomer present in the largest amount, although in softwoods mannose can be the most abundant sugar. Not only regular sugars can be found in hemicellulose, but also their acidified form, for instance glucuronic acid and galacturonic acid can be present.
Structural comparison to cellulose.
Unlike cellulose, hemicellulose (also a polysaccharide) consists of shorter chains – 500–3,000 sugar units as opposed to 7,000–15,000 glucose molecules per polymer seen in cellulose. In addition, hemicellulose is a branched polymer, while cellulose is unbranched.
Native structure.
Hemicelluloses are embedded in the cell walls of plants, sometimes in chains that form a 'ground' - they bind with pectin to cellulose to form a network of cross-linked fibres.
Biosynthesis.
Hemicelluloses are synthesised from sugar nucleotides in the cell's Golgi apparatus. Two models explain their synthesis: 1) a ‘2 component model' where modification occurs at two transmembrane proteins, and 2) a '1 component model' where modification occurs only at one transmembrane protein. After synthesis, hemicelluloses are transported to the plasma membrane via Golgi vesicles.
Applications.
As percent content of hemicellulose increases in animal feed, the voluntary feed intake decreases.
Hemicellulose is represented by the difference between neutral detergent fiber (NDF) and acid detergent fiber (ADF).
Functions.
Microfibrils are cross-linked together by hemicellulose homopolymers. Lignins assist and strengthen the attachment of hemicelluloses to microfibrils.
Hemicellulose from trees.
Hemicellulose found in hardwood trees is predominantly xylan with some glucomannan, while in softwoods it is mainly rich in galactoglucomannan and contains only a small amount of xylan. The average molecular weight is lower than that of cellulose at less than 30,000, as opposed to the 100,000 average molecular weight reported for cellulose.

</doc>
<doc id="14011" url="http://en.wikipedia.org/wiki?curid=14011" title="Hillbilly">
Hillbilly

Hillbilly is a term (often derogatory) for people who dwell in rural, mountainous areas in the United States, primarily in Appalachia, but also parts of the Ozarks. Due to its strongly stereotypical connotations, the term can be offensive to those Americans of Appalachian or Ozark heritage. "Hillbilly" first appeared in print in a 1900 "New York Journal" article, with the definition: "a Hill-Billie is a free and untrammeled white citizen of Alabama, who lives in the hills, has no means to speak of, dresses as he can, talks as he pleases, drinks whiskey when he gets it, and fires off his revolver as the fancy takes him." The stereotype is two-fold in that it incorporates both positive and negative traits: “Hillbillies” are often considered independent and self-reliant individuals that resist the modernization of society, but at the same time they are also defined as backward, violent, and uncivilized. Scholars argue this duality is reflective of the split ethnic identities in “white America." 
Etymology and history.
The Appalachian Mountains were settled in the eighteenth century by settlers from England, Germany, and primarily from the Province of Ulster in northern Ireland. The settlers from Ulster were mainly Protestants who migrated to Ireland during the Plantation of Ulster in the 17th century from Scotland and northern England. Many further migrated to the American colonies beginning in the 1730s, and in America became known as the Scotch-Irish. Scholars argue that the term "hillbilly" originated from Scottish dialect. The term "hill-folk" referred to people that preferred isolation from the greater society and "billy" meant “comrade” or “companion." It is suggested that “hill-folk” and “billie” were combined when the Cameronians fled to the Highlands. 
Others have suggested the term originated in 17th-century Ireland, during the Williamite War, when Protestant supporters of King William III were often referred to as "Billy's Boys." Some scholars disagree with this theory. Michael Montgomery's "From Ulster to America: The Scotch-Irish Heritage of American English" states, "In Ulster in recent years it has sometimes been supposed that it was coined to refer to followers of King William III and brought to America by early Ulster emigrants…, but this derivation is almost certainly incorrect… In America "hillbilly" was first attested only in 1898, which suggests a later, independent development." 
The term "hillbilly" spread in the years following the American Civil War. At this time, the country was developing both technologically and socially, but the Appalachian region was falling behind. Before the war, Appalachia was not distinctively different from other rural areas of the country. However, post-war, the frontier pushed further west and the region obtained frontier characteristics. Appalachians themselves were perceived as backward, quick to violence, and inbred in their isolation. Fueled by news stories of mountain feuds such as that in the 1880s between the Hatfields and McCoys, the hillbilly stereotype developed in the late 19th to early 20th century.
The "classic" hillbilly stereotype reached its current characterization during the years of the Great Depression when many mountaineers left their homes to find work in other areas of the country. The period of Appalachian out-migration, roughly from the 1930s through the 1950s, saw many mountain residents moving North to the Midwestern industrial cities of Chicago, Cleveland, Akron, and Detroit. This movement North became known as the "Hillbilly Highway." The movement brought these previously isolated communities into mainstream United States culture. Poor white mountaineers became central characters in newspapers, pamphlets and eventually, motion pictures. Authors at this time were inspired by historical figures such as Davy Crockett and Daniel Boone. The mountaineer image transferred over to the 20th century where the “hillbilly” stereotype emerged.
In popular culture.
Pop culture has perpetuated the hillbilly stereotype. The misrepresentation of Appalachian people in the media has led to great cultural distortion of Appalachian beliefs, practices, and lifestyle. Scholarly works suggest that the media has exploited both the Appalachian region and people by classifying them as "hillbillies." These generalizations do not match the cultural experiences of Appalachians. Appalachians, like many other groups, do not subscribe to a single identity. One of the issues associated with stereotyping is that it is profitable. When “hillbilly” became a widely used term, entrepreneurs saw a window for potential revenue. They “recycled” the image and brought it to life through various forms of media.
Television and film have portrayed "hillbillies" in both derogatory and sympathetic terms. Films such as "Sergeant York" or the Ma and Pa Kettle series portrayed the hillbilly as wild but good-natured. Television programs of the 1960s such as "The Real McCoys", "The Andy Griffith Show", and especially "The Beverly Hillbillies", portrayed the hillbilly as backwards but with enough wisdom to outwit more sophisticated city folk. The popular 1970s television variety show "Hee Haw" regularly lampooned the stereotypical hillbilly lifestyle. A darker image of the hillbilly is found in the film "Deliverance" (1972), based on a novel by James Dickey, which depicted the hillbilly as genetically deficient, inbred and murderous.
“Hillbillies” were at the center of reality television in the 21st century. Network television shows such as New Beverly Hillbillies, High Life, and The Simple Life displayed the “hillbilly” lifestyle for viewers in the United States. This sparked protests across the country with rural-minded individuals gathering to fight the stereotype. The Center for Rural Strategies started a nationwide campaign stating the stereotype was “politically incorrect.” The Kentucky-based organization engaged political figures in the movement such as Robert Byrd and Mike Huckabee. Both protestors argued that the discrimination of any other group in United States would not be tolerated, so neither should the discrimination against rural U.S. citizens. A 2003 piece published by The Cincinnati Enquirer read, “In this day of hypersensitivity to diversity and political correctness, Appalachians have been a group that it is still socially acceptable to demean and joke about…But rural folks have spoken up and said “enough” to the Hollywood mockers." 
Music.
"Hillbilly music" was at one time considered an acceptable label for what is now known as country music. The label, coined in 1925 by country pianist Al Hopkins, persisted until the 1950s.
The "hillbilly music" categorization covers a wide variety of musical genres including bluegrass, country western, and gospel. Appalachian folk song existed long before the "hillbilly" label. When the commercial industry was combined with "traditional Appalachian folksong," "hillbilly music" was formed. Some argue this is a "High Culture" issue where sophisticated individuals may see something considered "unsophisticated" as "trash."
In the early 20th century, artists began to utilize the "hillbilly" label. The York Brothers entitled one of their songs "Hillbilly Rose" and the Delmore Brothers followed with their song "Hillbilly Boogie." In 1927, the Gennett studios in Richmond, Indiana, made a recording of black fiddler Jim Booker. The recordings were labeled "made for Hillbilly" in the Gennett files and were marketed to a white audience. Columbia Records had much success with the "Hill Billies" featuring Al Hopkins and Fiddlin' Charlie Bowman.
By the late 1940s, radio stations started to use the "hillbilly music" label. Originally, "hillbilly" was used to describe fiddlers and string bands, but now it was used to describe traditional Appalachian music. Appalachians had never used this term to describe their own music. Popular songs whose style bore characteristics of both hillbilly and African American music were referred to as hillbilly boogie and "rockabilly". Elvis Presley was a prominent player of rockabilly and was known early in his career as the "Hillbilly Cat."
When the Country Music Association was founded in 1958, the term "hillbilly music" gradually fell out of use. The music industry merged hillbilly music, Western swing, and Cowboy music, to form the current category C&W, Country and Western.
Some artists and fans (notably Hank Williams Sr.) were offended by the "hillbilly music" label. While the term is not used as frequently today,"hillbilly music" is still used on occasion to refer to old-time music or bluegrass. For example, WHRB broadcasts a popular weekly radio show entitled "Hillbilly at Harvard." The show is devoted to playing a mix of old-time music, bluegrass, and traditional country and western
Cultural implications.
The hillbilly stereotype has a traumatizing effect on some in the Appalachian region. Feelings of shame, self-hatred, and detachment are cited as a result of "culturally transmitted traumatic stress syndrome." Appalachian scholars say that the large-scale stereotyping has rewritten Appalachian history, making Appalachians feel particularly vulnerable. "Hillbilly" has now become part of Appalachian identity and some Appalachians feel they are constantly defending themselves against this image.
The stereotyping also has political implications for the region. There is a sense of "perceived history" that prevents many political issues from receiving adequate attention. Appalachians are often blamed for economic struggles. "Moonshiners, welfare cheats, and coal miners" are stereotypes stemming from the greater hillbilly stereotype in the region. This prejudice serves as a barrier for addressing some serious issues such as the economy and the environment.
Despite the political and social difficulties associated with stereotyping, Appalachians have organized to enact change. The War on Poverty is an example of one effort that allowed for Appalachian community organization. Grassroots movements, protests, and strikes are common in the area, though not always successful.
Intragroup versus intergroup usage.
The Springfield, Missouri Chamber of Commerce once presented dignitaries visiting the city with an "Ozark Hillbilly Medallion" and a certificate proclaiming the honoree a "hillbilly of the Ozarks." On June 7, 1952, President Harry S. Truman received the medallion after a breakfast speech at the Shrine Mosque for the 35th Division Association. Other recipients included US Army generals Omar Bradley and Matthew Ridgeway, J. C. Penney, Johnny Olsen and Ralph Story.
Hillbilly Days is an annual festival held in mid-April in Pikeville, Kentucky celebrating the best of Appalachian culture. The event began by local Shriners as a fundraiser to support the Shriners Children's Hospital. It has grown since its beginning in 1976 and now is the second largest festival held in the state of Kentucky. Artists and craftspeople showcase their talents and sell their works on display. Nationally renowned musicians as well as the best of the regional mountain musicians share six different stages located throughout the downtown area of Pikeville. Want-to-be hillbillies from across the nation compete to come up with the wildest Hillbilly outfit. The event has earned its name as the Mardi Gras of the Mountains. Fans of "mountain music" come from around the United States to hear this annual concentrated gathering of talent. Some refer to this event as the equivalent of a "Woodstock" for mountain music.

</doc>
<doc id="14012" url="http://en.wikipedia.org/wiki?curid=14012" title="Host">
Host

Host (masculine) and hostess (feminine) most often refer to a person responsible for guests at an event and/or providing hospitality during it, or to an event's presenter or master or mistress of ceremonies. Host or hosts may also refer to:

</doc>
<doc id="14013" url="http://en.wikipedia.org/wiki?curid=14013" title="Hernán Cortés">
Hernán Cortés

Hernán Cortés de Monroy y Pizarro, 1st Marquis of the Valley of Oaxaca (]; 1485 – December 2, 1547) was a Spanish "Conquistador" who led an expedition that caused the fall of the Aztec Empire and brought large portions of mainland Mexico under the rule of the King of Castile in the early 16th century. Cortés was part of the generation of Spanish colonizers who began the first phase of the Spanish colonization of the Americas.
Born in Medellín, Spain, to a family of lesser nobility, Cortés chose to pursue a livelihood in the New World. He went to Hispaniola and later to Cuba, where he received an "encomienda" and, for a short time, became alcalde (magistrate) of the second Spanish town founded on the island. In 1519, he was elected captain of the third expedition to the mainland, an expedition which he partly funded. His enmity with the Governor of Cuba, Diego Velázquez de Cuéllar, resulted in the recall of the expedition at the last moment, an order which Cortés ignored.
Arriving on the continent, Cortés executed a successful strategy of allying with some indigenous people against others. He also used a native woman, Doña Marina, as an interpreter; she would later bear Cortés a son. When the Governor of Cuba sent emissaries to arrest Cortés, he fought them and won, using the extra troops as reinforcements. Cortés wrote letters directly to the king asking to be acknowledged for his successes instead of punished for mutiny. After he overthrew the Aztec Empire, Cortés was awarded the title of "Marqués del Valle de Oaxaca", while the more prestigious title of Viceroy was given to a high-ranking nobleman, Antonio de Mendoza. In 1541 Cortés returned to Spain, where he died peacefully but embittered, six years later.
Because of the controversial undertakings of Cortés and the scarcity of reliable sources of information about him, it has become difficult to assert anything definitive about his personality and motivations. Early lionizing of the conquistadors did not encourage deep examination of Cortés. Later reconsideration of the conquistadors' character in the context of modern anti-colonial sentiment also did little to expand understanding of Cortés as an individual. As a result of these historical trends, descriptions of Cortés tend to be simplistic, and either damning or idealizing.
Name.
While he is often now referred to as Hernán or Hernando Cortés (; ]), in his time, he called himself "Hernando" or "Fernando" "Cortés" (]). The names Hernán, Hernando, and Fernando are all equally correct. The latter two were most commonly used during his lifetime, but the former shortened form has become common in both the Spanish and English languages in modern times, and is the name by which many people know him today.
Early life.
Cortés was born in 1485 in the town of Medellín, in modern-day Extremadura, Spain. His father, Martín Cortés de Monroy, born in 1449 to Rodrigo or Ruy Fernández de Monroy and his wife María Cortés, was an infantry captain of distinguished ancestry but slender means. Hernán's mother was Catalina Pizarro Altamirano.
Through his mother, Hernán was the second cousin once removed of Francisco Pizarro, who later conquered the Inca Empire of modern-day Peru (not to be confused with another Francisco Pizarro who joined Cortés to conquer the Aztecs), through her parents Diego Altamirano and wife and cousin Leonor Sánchez Pizarro Altamirano, first cousin of Pizarro's father. Through his father, Hernán was a twice distant relative of Nicolás de Ovando, the third Governor of Hispaniola. His paternal grandfather was a son of Rodrigo de Monroy y Almaraz, 5th Lord of Monroy, and wife Mencía de Orellana y Carvajal.
Hernán Cortés is described as a pale, sickly child by his biographer, chaplain, and friend Francisco López de Gómara. At the age of 14, Cortés was sent to study Latin under an uncle-in-law in Salamanca.
After two years, Cortés, tired of schooling, returned home to Medellín, much to the irritation of his parents, who had hoped to see him equipped for a profitable legal career. However, those two years at Salamanca, plus his long period of training and experience as a notary, first in Seville and later in Hispaniola, would give him a close acquaintance with the legal codes of Castile that helped him to justify his unauthorized conquest of Mexico.
At this point in his life, Cortés was described by Gómara as restless, haughty and mischievous. This was probably a fair description of a 16-year-old boy who had returned home only to find himself frustrated by life in his small provincial town. By this time, news of the exciting discoveries of Christopher Columbus in the New World was streaming back to Spain.
Early career in the New World.
Plans were made for Cortés to sail to the Americas with a family acquaintance and distant relative, Nicolás de Ovando, the newly appointed governor of Hispaniola (currently Haiti and the Dominican Republic), but an injury he sustained while hurriedly escaping from the bedroom of a married woman from Medellín prevented him from making the journey. Instead, he spent the next year wandering the country, probably spending most of his time in the heady atmosphere of Spain's southern ports of Cadiz, Palos, Sanlucar, and Seville, listening to the tales of those returning from the Indies, who told of discovery and conquest, gold, Indians, and strange unknown lands. He finally left for Hispaniola in 1504 where he became a colonist.
Arrival.
Cortés reached Hispaniola in a ship commanded by Alonso Quintero, who tried to deceive his superiors and reach the New World before them in order to secure personal advantages. Quintero's mutinous conduct may have served as a model for Cortés in his subsequent career. The history of the conquistadores is rife with accounts of rivalry, jockeying for positions, mutiny, and betrayal.
Upon his arrival in 1504 in Santo Domingo, the capital of Hispaniola, the 18-year-old Cortés registered as a citizen, which entitled him to a building plot and land to farm. Soon afterwards, Nicolás de Ovando, still the governor, gave him an "encomienda" and made him a notary of the town of Azua de Compostela. His next five years seemed to help establish him in the colony; in 1506, Cortés took part in the conquest of Hispaniola and Cuba, receiving a large estate of land and Indian slaves for his efforts from the leader of the expedition.
Cuba (1511–1518).
In 1511, Cortés accompanied Diego Velázquez de Cuéllar, an aide of the Governor of Hispaniola, in his expedition to conquer Cuba. Velázquez was appointed as governor. At the age of 26, Cortés was made clerk to the treasurer with the responsibility of ensuring that the Crown received the "quinto", or customary one fifth of the profits from the expedition.
The Governor of Cuba, Diego Velázquez, was so impressed with Cortés that he secured a high political position for him in the colony. He became secretary for Governor Velázquez. Cortés was twice appointed municipal magistrate ("alcalde") of Santiago. In Cuba, Cortés became a man of substance with an "encomienda" to provide Indian labor for his mines and cattle. This new position of power also made him the new source of leadership, which opposing forces in the colony could then turn to. In 1514, Cortés led a group which demanded that more Indians be assigned to the settlers.
As time went on, relations between Cortés and Governor Velázquez became strained. This began once news of Juan de Grijalva, establishing a colony on the mainland where there was a bonanza of silver and gold, reached Velázquez; it was decided to send him help. Cortés was appointed Captain-General of this new expedition in October 1518, but was advised to move fast before Velázquez changed his mind.
With Cortés's experience as an administrator, knowledge gained from many failed expeditions, and his impeccable rhetoric he was able to gather six ships and 300 men, within a month. Predictably, Velázquez's jealousy exploded and decided to place the leadership of the expedition in other hands. However, Cortés quickly gathered more men and ships in other Cuban ports.
Cortés also found time to become romantically involved with Catalina Xuárez (or Juárez), the sister-in-law of Governor Velázquez. Part of Velázquez's displeasure seems to have been based on a belief that Cortés was trifling with Catalina's affections. Cortés was temporarily distracted by one of Catalina's sisters but finally married Catalina, reluctantly, under pressure from Governor Velázquez. However, by doing so, he hoped to secure the good will of both her family and that of Velázquez.
It was not until he had been almost 15 years in the Indies, that Cortés began to look beyond his substantial status as mayor of the capital of Cuba and as a man of affairs in the thriving colony. He missed the first two expeditions, under the orders of Francisco Hernández de Córdoba and then Juan de Grijalva, sent by Diego Velázquez to Mexico in 1518.
Conquest of Mexico (1518–1520).
In 1518 Velázquez put him in command of an expedition to explore and secure the interior of Mexico for colonization. At the last minute, due to the old gripe between Velázquez and Cortés, he changed his mind and revoked his charter. Cortés ignored the orders and went ahead anyway, in February 1519, in an act of open mutiny. He stopped in Trinidad, Cuba, to hire more soldiers and obtain more horses. Accompanied by about 11 ships, 500 men, 13 horses, and a small number of cannons, he landed on the Yucatan Peninsula in Mayan territory.
There, he encountered Geronimo de Aguilar, a Spanish Franciscan priest who had survived a shipwreck, and a period in captivity with the Maya, before escaping. Aguilar had learned the Chontal Maya language and was able to translate for Cortés.
In March 1519, Cortés formally claimed the land for the Spanish crown. Then he proceeded to Tabasco, where he met with resistance and won a battle against the natives. He received twenty young indigenous women from the vanquished natives, and he converted them all to Christianity.
Among these women was La Malinche, his future mistress and mother of his son Martín. Malinche knew both the Nahuatl language and the Chontal Maya, thus enabling Cortés to communicate with the Aztecs through Aguilar.:82,86–87 At San Juan de Ulúa on Easter Sunday 1519, Cortés met with Moctezuma II's Aztec Empire governor's Tendile and Pitalpitoque.:89
In July 1519, his men took over Veracruz. By this act, Cortés dismissed the authority of the Governor of Cuba to place himself directly under the orders of King Charles. In order to eliminate any ideas of retreat, Cortés scuttled his ships.
March on Tenochtitlan.
In Veracruz, he met some of the tributaries of the Aztecs and asked them to arrange a meeting with Moctezuma II, the "tlatoani" (ruler) of the Aztec Empire. Moctezuma repeatedly turned down the meeting, but Cortés was determined. Leaving a hundred men in Veracruz, Cortés marched on Tenochtitlan in mid-August 1519, along with 600 soldiers, 15 horsemen, 15 cannons, and hundreds of indigenous carriers and warriors.
On the way to Tenochtitlan, Cortés made alliances with indigenous peoples such as the Totonacs of Cempoala and the Nahuas of Tlaxcala. The Otomis initially, and then the Tlaxcalans fought the Spanish a series of three battles from 2 Sept. to 5 Sept. 1519, and at one point Diaz remarked, "they surrounded us on every side". After Cortes continued to release prisoners with messages of peace, and realizing the Spanish were enemies of Montezuma, Xicotencatl the Elder, and Maxixcatzin, persuaded the Tlaxcalan warleader, Xicotencatl the Younger, that it would be better to ally with the newcomers than to kill them.:143–155,171
In October 1519, Cortés and his men, accompanied by about 1,000 Tlaxcalteca,:188 marched to Cholula, the second largest city in central Mexico. Cortés, either in a pre-meditated effort to instill fear upon the Aztecs waiting for him at Tenochtitlan or (as he later claimed, when he was being investigated) wishing to make an example when he feared native treachery, massacred thousands of unarmed members of the nobility gathered at the central plaza, then partially burned the city.:199–200
By the time he arrived in Tenochtitlan the Spaniards had a large army. On November 8, 1519, they were peacefully received by Moctezuma II. Moctezuma deliberately let Cortés enter the Aztec capital, the island city of Tenochtitlan, hoping to get to know their weaknesses better and to crush them later.
Moctezuma gave lavish gifts of gold to the Spaniards which, rather than placating them, excited their ambitions for plunder. In his letters to King Charles, Cortés claimed to have learned at this point that he was considered by the Aztecs to be either an emissary of the feathered serpent god Quetzalcoatl or Quetzalcoatl himself – a belief which has been contested by a few modern historians. But quickly Cortés learned that several Spaniards on the coast had been killed by Aztecs while supporting the Totonacs, and decided to take Moctezuma as a hostage in his own palace, indirectly ruling Tenochtitlan through him.
Meanwhile, Velázquez sent another expedition, led by Pánfilo de Narváez, to oppose Cortés, arriving in Mexico in April 1520 with 1,100 men. Cortés left 200 men in Tenochtitlan and took the rest to confront Narváez. He overcame Narváez, despite his numerical inferiority, and convinced the rest of Narváez's men to join him. In Mexico, one of Cortés's lieutenants Pedro de Alvarado, committed the "massacre in the Great Temple", triggering a local rebellion.
Cortés speedily returned to Tenochtitlán. On July 1, 1520 Moctezuma was killed (the Spaniards claimed he was stoned to death by his own people; other claim he was murdered by the Spanish once they realized his inability to placate the locals). Faced with a hostile population, Cortés decided to flee for Tlaxcala. During the "Noche Triste" (June 30 – July 1, 1520), the Spaniards managed a narrow escape from Tenochtitlan across the Tlacopan causeway, while their backguard was being massacred. Much of the treasure looted by Cortés was lost (as well as his artillery) during this panicked escape from Tenochtitlán.
Destruction of Tenochtitlan.
After a battle in Otumba, they managed to reach Tlaxcala, having lost 870 men. With the assistance of their allies, Cortés's men finally prevailed with reinforcements arriving from Cuba. Cortés began a policy of attrition towards Tenochtitlan, cutting off supplies and subduing the Aztecs' allied cities. The siege of Tenochtitlán ended with Spanish victory and the destruction of the city.
In January 1521, Cortés countered a conspiracy against him, headed by Antonio de Villafana, who was hanged for the offense. Finally, with the capture of Cuauhtémoc, the "tlatoani" (ruler) of Tenochtitlán, on August 13, 1521, the Aztec Empire disappeared, and Cortés was able to claim it for Spain, thus renaming the city Mexico City. From 1521 to 1524, Cortés personally governed Mexico.
Appointment to governorship of Mexico and internal dissensions.
Many historical sources have conveyed an impression that Cortés was unjustly treated by the Spanish Crown, and that he received nothing but ingratitude for his role in establishing New Spain. This picture is the one Cortés presents in his letters and in the later biography written by Francisco López de Gómara. However, there may be more to the picture than this. Cortés's own sense of accomplishment, entitlement, and vanity may have played a part in his deteriorating position with the king:
Cortés personally was not ungenerously rewarded, but he speedily complained of insufficient compensation to himself and his comrades. Thinking himself beyond reach of restraint, he disobeyed many of the orders of the Crown, and, what was more imprudent, said so in a letter to the emperor, dated October 15, 1524 (Ycazbalceta, "Documentos para la Historia de México", Mexico, 1858, I). In this letter Cortés, besides recalling in a rather abrupt manner that the conquest of Mexico was due to him alone, deliberately acknowledges his disobedience in terms which could not fail to create a most unfavourable impression.
King Charles appointed Cortés as governor, captain general and chief justice of the newly conquered territory, dubbed "New Spain of the Ocean Sea". But also, much to the dismay of Cortés, four royal officials were appointed at the same time to assist him in his governing – in effect, submitting him to close observation and administration. Cortés initiated the construction of Mexico City, destroying Aztec temples and buildings and then rebuilding on the Aztec ruins what soon became the most important European city in the Americas.
Cortés managed the founding of new cities and appointed men to extend Spanish rule to all of New Spain, imposing the "encomienda" system in 1524. He reserved many encomiendas for himself and for his retinue, which they considered just rewards for their accomplishment in conquering central Mexico. However, later arrivals and members of factions antipathetic to Cortés complained of the favoritism that excluded them.
In 1523, the Crown (possibly influenced by Cortés's enemy, Bishop Fonseca), sent a military force under the command of Francisco de Garay to conquer and settle the northern part of Mexico, the region of Pánuco. This was another setback for Cortés who mentioned this in his fourth letter to the King in which he describes himself as the victim of a conspiracy by his archenemies Diego Velázquez de Cuéllar, Diego Columbus and Bishop Fonseca as well as Francisco Garay. The influence of Garay was effectively stopped by this appeal to the King who sent out a decree forbidding Garay to interfere in the politics of New Spain, causing him to give up without a fight.
Granted Coat of Arms by the King, 1525.
Although Cortés had flouted the authority of Diego Velázquez in sailing to the mainland and then leading an expedition of conquest, Cortés's spectacular success was rewarded by the crown with a coat of arms, a mark of high honor, following the conqueror's request. The document granting the coat of arms summarizes Cortés's accomplishments in the conquest of Mexico. The proclamation of the king says in part We, respecting the many labors, dangers, and adventures which you underwent as stated above, and so that there might remain a perpetual memorial of you and your services and that you and your descendants might be more fully honored...it is our will that besides your coat of arms of your lineage, which you have, you may have and bear as your coat of arms, known and recognized, a shield...:43 The grant specifies the iconography of the coat of arms, the central portion divided into quadrants. In the upper portion, there is a "black eagle with two heads on a white field, which are the arms of the empire.":43 Below that is a "golden lion on a red field, in memory of the fact that you, the said Hernando Cortés, by your industry and effort brought matters to the state described above" (i.e., the conquest).:43 The specificity of the other two quadrants is linked directly to Mexico, with one quadrant showing three crowns representing the three Aztec emperors of the conquest era, Moctezuma, Cuitlahuac, and Cuauhtemoc:43 and the other showing the Aztec capital of Tenochtitlan.:43 Encircling the central shield are symbols of the seven city-states around the lake and their lords that Cortés defeated, with the lords "to be shown as prisoners bound with a chain which shall be closed with a lock beneath the shield.":44–45
Death of his First Wife and Remarriage.
Cortés's wife Catalina Súarez arrived in New Spain from sometime around summer 1522, along with sister and brother. His marriage to Catalina was at this point extremely awkward, since she was a kinswoman of governor of Cuba Diego Velázquez, whose authority Cortés had thrown off and now his enemy. Catalina lacked the noble title of "doña," so at this point his alliance with her no longer raised his status. The marriage had been childless. Since Cortés had sired children with a variety of indigenous women, including a son ca. 1522 by his cultural translator, Doña Marina, Cortés knew he was capable of fathering children. Cortés's only male heir at this point was illegitimate, but nonetheless named after Cortés's father, Martín Cortés. This natural son Martín Cortés was sometimes called "El Mestizo." Cortés's wife, Catalina Suárez, died under mysterious circumstances the night of November 1–2, 1522. There were accusations at the time that Cortés had murdered his wife. There was an investigation into her death, interviewing a variety of household residents and others. The documentation of the investigation published in the nineteenth century in Mexico and archival documents uncovered in the twentieth century. The death of Catalina Suárez had produced a scandal and a major investigation, but weathering that Cortés was now free to marry someone of high status more appropriate to his wealth and power. In 1529 he had been accorded the noble designation of "don", but more importantly was given the noble title of Marquis of the Valley of Oaxaca and married the Spanish noblewoman Doña Juana de Zúñiga. The marriage produced three children, including another son, who was also named Martín. As the first-born legitimate son, Don Martín Cortés y Zúñiga was now Cortés's heir and succeeded his father as holder of the title and estate of the Marquisate of the Valley of Oaxaca. Cortés's legitimate daughters were Doña Maria, Doña Catalina, and Doña Juana.
Cortés and the "Spiritual Conquest" of Mexico.
Since the conversion to Christianity of indigenous peoples was an essential and integral part of the extension of Spanish power, making formal provisions for that conversion once the military conquest was completed was an important task for Cortés. During the Age of Discovery, the Catholic Church had seen early attempts at conversion in the Caribbean islands by Spanish friars, particularly mendicant orders. Cortés made a request to the Spanish monarch to send Franciscan and Dominican friars to Mexico to begin the daunting work of converting vast populations indigenous to Christianity. In his fourth letter to the king, Cortés pleaded for friars rather than diocesan or secular priests because those clerics were in his view a serious danger to the Indians' conversion. If these people [Indians] were now to see the affairs of the Church and the service of God in the hands of canons or other dignitaries, and saw them indulge in the vices and profanities now common in Spain, knowing that such men were the ministers of God, it would bring our Faith into much harm that I believe any further preaching would be of no avail. He wished the mendicants to be the main evangelists. Mendicant friars did not usually have full priestly powers to perform all the sacraments needed for conversion of the Indians and growth of the neophytes in the Christian faith, so Cortés laid out a solution to this to the king. Your Majesty should likewise beseech His Holiness [the pope] to grant these powers to the two principal persons in the religious orders that are to come here, and that they should be his delegates, one from the Order of St. Francis and th other from the Order of St. Dominic. They should bring the most extensive powers Your Majesty is able to obtain, for, because these lands are so far from the Church of Rome, and we, the Christians who now reside here and shall do so in the future, are so far fromthe proper remedies of our consciences and, as we are human, so subject to sin, it is essential that His Holiness should be generous with us and grant to these persons most extensive powers, to be handed down to persons actually in residence here whether it be given to the general of each order or to his provincials.
The Franciscans arrived in May of 1524, a symbolically powerful group of twelve known as the Twelve Apostles of Mexico, led by Fray Martín de Valencia. Franciscan Geronimo de Mendieta claimed that Cortés's most important deed was the way he met this first group of Franciscans. The conqueror himself was said to have met the friars as they approached the capital, kneeling at the feet of the friars who had walked from the coast. This story was used by Franciscans as a demonstration of Cortés's piety and humility was a powerful message to all, including the Indians, that Cortés's earthly power was subordinate to the spiritual power of the friars. However, one of the first twelve Franciscans, Fray Toribio de Benavente Motolinia does not mention it in his history. Cortés and the Franciscans had a particularly strong alliance in Mexico, with Franciscans seeing him as "the new Moses" for conquering Mexico and opening it to Christian evangelization. In Motolinia's 1555 response to Dominican Bartolomé de Las Casas, he praises Cortés. And as to those who murmur against the Marqués del Valle [Cortés], God rest him, and who try to blacken and obscure his deeds, I believe that before God their deeds are not as acceptable as those of the Marqués. Although as a human he was a sinner, he had faith and works of a good Christian, and a great desire to employ his life and property in widening and augmenting the fair of Jesus Christ, and dying for the conversion of these gentiles... Who has loved and defended the Indians of this new world like Cortés?... Through this captain, God opened the door for us to preach his holy gospel and it was he who caused the Indians to revere the holy sacraments and respect the ministers of the church.
In Fray Bernardino de Sahagún's 1585 revision of the conquest narrative first codified as Book XII of the Florentine Codex, there are laudatory references to Cortés that do not appear in the earlier text from the indigenous perspective. Whereas Book XII of the Florentine Codex concludes with an account of Spaniards' search for gold, in Sahagún's 1585 revised account, he ends with praise of Cortés for requesting the Franciscans be sent to Mexico to convert the Indians.
Expedition to Honduras and aftermath.
From 1524 to 1526, Cortés headed an expedition to Honduras where he defeated Cristóbal de Olid, who had claimed Honduras as his own under the influence of the Governor of Cuba Diego Velázquez. Fearing that Cuauhtémoc might head an insurrection in Mexico, he brought him with him in Honduras. In a controversial move, Cuauhtémoc was executed during the journey. Raging over Olid's treason, Cortés issued a decree to arrest Velázquez, whom he was sure was behind Olid's treason. This, however, only served to further estrange the Crown of Castile and the Council of Indies, both of which were already beginning to feel anxious about Cortés's rising power.
Cortés's fifth letter to King Charles attempts to justify his conduct, concludes with a bitter attack on "various and powerful rivals and enemies" who have "obscured the eyes of your Majesty." Charles, who was also Holy Roman Emperor, had little time for distant colonies (much of Charles's reign was taken up with wars with France, the German Protestants and the expanding Ottoman Empire), except insofar as they contributed to finance his wars. In 1521, year of the Conquest, Charles was attending to matters in his German domains and Bishop Adrian of Utrecht functioned as regent in Spain.
Velázquez and Fonseca persuaded the regent to appoint a commissioner with powers, (a "Juez de residencia", Luis Ponce de León), to investigate Cortés's conduct and even arrest him. Cortés was once quoted as saying that it was "more difficult to contend against (his) own countrymen than against the Aztecs." Governor Diego Velázquez continued to be a thorn in his side, teaming up with Bishop Juan Rodríguez de Fonseca, chief of the Spanish colonial department, to undermine him in the Council of the Indies.
A few days after Cortés's return from his expedition, Ponce de León suspended Cortés from his office of governor of New Spain. The Licentiate then fell ill and died shortly after his arrival, appointing Marcos de Aguilar as "alcalde mayor". The aged Aguilar also became sick and appointed Alonso de Estrada governor, who was confirmed in his functions by a royal decree in August 1527. Cortés, suspected of poisoning them, refrained from taking over the government.
Estrada sent Diego de Figueroa to the south. De Figueroa raided graveyards and extorted contributions, meeting his end when the ship carrying these treasures sank. Albornoz persuaded Alonso de Estrada to release Salazar and Chirinos. When Cortés complained angrily after one of his adherents' hands was cut off, Estrada ordered him exiled. Cortés sailed for Spain in 1528 to appeal to King Charles.
First return to Spain (1528) and Marquisate of the Valley of Oaxaca.
In 1528, Cortés returned to Spain to appeal to the justice of his master, Charles V. Juan Altamirano and Alonso Valiente stayed in Mexico and acted as Cortés' representatives during his absence. Cortés presented himself with great splendor before Charles V's court. By this time Charles had returned and Cortés forthrightly responded to his enemy's charges. Denying he had held back on gold due the crown, he showed that he had contributed more than the quinto (one-fifth) required. Indeed, he had spent lavishly to build the new capital of Mexico City on the ruins of the Aztec capital of Tenochtitlán, leveled during the siege that brought down the Aztec empire.
He was received by Charles with every distinction, and decorated with the order of Santiago. In return for his efforts in expanding the still young Spanish Empire, Cortés was rewarded in 1529 by being accorded the noble title of "don" but more importantly named the "Marqués del Valle de Oaxaca" Marquisate of the Valley of Oaxaca and married the Spanish noblewoman, Doña Juana Zúñiga, after the 1522 death of his much less distinguished first wife, Catalina Suárez. The noble title and senorial estate of the Marquesado was passed down to his descendants until 1811. The Oaxaca Valley was one of the wealthiest region of New Spain, and Cortés had 23,000 vassals in 23 named encomiendas in perpetuity.
Although confirmed in his land holdings and vassals, he was not reinstated as governor and was never again given any important office in the administration of New Spain. During his travel to Spain, his property was mismanaged by abusive colonial administrators. He sided with local natives in a lawsuit. The natives documented the abuses in the Huexotzinco Codex.
The entailed estate and title passed to his legitimate son Don Martín Cortés upon Cortés's death in 1547, who became the Second Marquis. Don Martín's association with the so-called Encomenderos' Conspiracy endangered the entailed holdings, but they were restored and remained the continuing reward for Hernán Cortés's family through the generations.
Return to Mexico.
Cortés returned to Mexico in 1530 with new titles and honors, but with diminished power. Although Cortés still retained military authority and permission to continue his conquests, viceroy Don Antonio de Mendoza was appointed in 1535 to administer New Spain's civil affairs. This division of power led to continual dissension, and caused the failure of several enterprises in which Cortés was engaged.
On returning to Mexico, Cortés found the country in a state of anarchy. There was a strong suspicion in court circles of an intended rebellion by Cortés, and a charge was brought against him that cast a fatal blight upon his character and plans. He was accused of murdering his first wife. The proceedings of the investigation were kept secret.
No report, either exonerating or condemning Cortés, was published. Had the Government declared him innocent, it would have greatly increased his popularity. Had it declared him a criminal, a crisis would have been precipitated by the accused and his party. Silence was the only safe policy, but that silence is suggestive that grave danger was feared from his influence.
After reasserting his position and reestablishing some sort of order, Cortés retired to his estates at Cuernavaca, about 30 miles (48 km) south of Mexico City. There he concentrated on the building of his palace and on Pacific exploration. Remaining in Mexico between 1530 and 1541, Cortés quarreled with Nuño Beltrán de Guzmán and disputed the right to explore the territory that is today California with Antonio de Mendoza, the first viceroy.
In 1536, Cortés explored the northwestern part of Mexico and discovered the Baja California peninsula. Cortés also spent time exploring the Pacific coast of Mexico. The Gulf of California was originally named the "Sea of Cortes" by its discoverer Francisco de Ulloa in 1539. This was the last major expedition by Cortés.
Later life and death.
Second return to Spain.
After his exploration of Baja California, Cortés returned to Spain in 1541, hoping to confound his angry civilians, who had brought many lawsuits against him (for debts, abuse of power, etc.).
On his return he was utterly neglected, and could scarcely obtain an audience. On one occasion he forced his way through a crowd that surrounded the emperor's carriage, and mounted on the footstep. The emperor, astounded at such audacity, demanded of him who he was. "I am a man," replied Cortés proudly, "who has given you more provinces than your ancestors left you cities."
Expedition against Algiers.
The emperor finally permitted Cortés to join him and his fleet commanded by Andrea Doria at the great expedition against Algiers in the Barbary Coast in 1541, which was then part of the Ottoman Empire and was used as a base by Hayreddin Barbarossa, a famous Turkish corsair and Admiral-in-Chief of the Ottoman Fleet. During this unfortunate campaign, which was his last, Cortés was almost drowned in a storm that hit his fleet while he was pursuing Barbarossa.
Last years, death, and remains.
Having spent a great deal of his own money to finance expeditions, he was now heavily in debt. In February 1544 he made a claim on the royal treasury, but was given a royal runaround for the next three years. Disgusted, he decided to return to Mexico in 1547. When he reached Seville, he was stricken with dysentery. He died in Castilleja de la Cuesta, Seville province, on December 2, 1547, from a case of pleurisy at the age of 62.
Like Columbus, he died a wealthy but embittered man. He left his many mestizo and white children well cared for in his will, along with every one of their mothers. He requested in his will that his remains eventually be buried in Mexico. Before he died he had the Pope remove the "natural" status of three of his
children (legitimizing them in the eyes of the church), including Martin, the son he had with Doña Marina (also known as La Malinche), said to be his favourite.
After his death his body has been moved more than eight times for several reasons. On December 4, 1547 he was buried in the mausoleum of the Duke of Medina in the church of San Isidoro del Campo, Sevilla. Three years later (1550) due to the space being required by the duke, his body was moved to the altar of Santa Catarina in the same church. In his testament, Cortés asked for his body to be buried in the monastery he had ordered to be built in Coyoacan in México, ten years after his death, but the monastery was never built. So in 1566, his body was sent to New Spain and buried in the church of "San Francisco de Texcoco", where his mother and one of his sisters were buried.
In 1629, "Don Pedro Cortés fourth "Marquez del Valle", his last male descendant, died, so the viceroy decided to move the bones of Cortés along with those of his descendant to the Franciscan church in México. This was delayed for nine years, while his body stayed in the main room of the palace of the viceroy. Eventually it was moved to the Sagrario of Franciscan church, where it stayed for 87 years. In 1716, it was moved to another place in the same church. In 1794, his bones were moved to the "Hospital de Jesus" (founded by Cortés), where a statue by Tolsa and a mausoleum were made. There was a public ceremony and all the churches in the city rang their bells.
In 1823, after the independence of México, it seemed imminent that his body would be desecrated, so the mausoleum was removed, the statue and the coat of arms were sent to Palermo, Sicily, to be protected by the Duke of Terranova. The bones were hidden, and everyone thought that they had been sent out of México. In 1836, his bones were moved to another place in the same building.
It was not until November 24, 1946 that they were rediscovered,:467 thanks to the discovery of a secret document by Lucas Alaman. His bones were put in charge of the Instituto Nacional de Antropología e Historia (INAH). The remains were authenticated by INAH.:468 They were then restored to the same place, this time with a bronze inscription and his coat of arms. When the bones were first rediscovered, the supporters of the Hispanic tradition in Mexico were excited, but one supporter of an indigenist vision of Mexico "proposed that the remains be publicly burned in front of the statue of Cuauhtemoc, and the ashes flung into the air.":468 Following the discovery and authentication of Cortés's remains, there was a discovery of what were described as the bones of Cuauhtemoc occurred, resulting in the so-called "battle of the bones":468 In 1981, when a copy of the bust by Tolsa was put in the church, there was a failed attempt to destroy his bones.
Disputed interpretation of his life.
There are relatively few sources to the early life of Cortés; his fame arose from his participation in the conquest of Mexico and it was only after this that people became interested in reading and writing about him.
Probably the best source is his letters to the king which he wrote during the campaign in Mexico, but they are written with the specific purpose of putting his efforts in a favourable light and so must be read critically. Another main source is the biography written by Cortés's private chaplain Lopez de Gómara, which was written in Spain several years after the conquest. Gómara never set foot in the Americas and knew only what Cortés had told him, and he had an affinity for knightly romantic stories which he incorporated richly in the biography. The third major source is written as a reaction to what its author calls "the lies of Gomara", the eyewitness account written by the Conquistador Bernal Díaz del Castillo does not paint Cortés as a romantic hero but rather tries to emphasize that Cortés's men should also be remembered as important participants in the undertakings in Mexico.
In the years following the conquest more critical accounts of the Spanish arrival in Mexico were written. The Dominican friar Bartolomé de Las Casas wrote his "A Short Account of the Destruction of the Indies" which raises strong accusations of brutality and heinous violence towards the Indians; accusations against both the conquistadors in general and Cortés in particular. The accounts of the conquest given in the Florentine Codex by the Franciscan Bernardino de Sahagún and his native informants are also less than flattering towards Cortés. The scarcity of these sources has led to a sharp division in the description of Cortés's personality and a tendency to describe him as either a vicious and ruthless person or a noble and honorable cavalier.
Representations in México.
In México there are few representations of Cortés. However, many landmarks still bear his name, from the castle in the city of Cuernavaca to some street names throughout the republic.
The only authentic monuments are in Mexico City at the pass between the volcanoes Iztaccíhuatl and Popocatépetl where Cortés took his soldiers on their march to Mexico City. It is known as the Paso de Cortés.
The muralist Diego Rivera painted several representation of him but the most famous, depicts him as a powerful and ominous figure along with Malinche in a mural in the National Palace in Mexico City.
In 1981, President Lopez Portillo tried to bring Cortés to public recognition. First, he made public a copy of the bust of Cortés made by Manuel Tolsá in the Hospital de Jesús Nazareno with an official ceremony, but soon a nationalist group tried to destroy it, so it had to be taken out of the public. Today the copy of bust is in the "Hospital de Jesús Nazareno" while the original is in Nápoles, Italy, in the Villa Pignatelli.
Later, another monument, known as "Monumento al Mestizaje" by Julián Martínez y M. Maldonado (1982) was commissioned by Mexican president José López Portillo to be put in the "Zócalo" (Main square) of Coyoacan, near the place of his country house, but it had to be removed to a little known park, the Jardín Xicoténcatl, Barrio de San Diego Churubusco, to quell protests. The statue depicts Cortés, Malinche and their son Martín.
There is another statue by Sebastián Aparicio, in Cuernavaca, was in a hotel "El casino de la selva". Cortés is barely recognizable, so it sparked little interest. The hotel was closed to make a commercial center, and the statue was put out of public display by Costco the builder of the commercial center.
Writings: the "Cartas de Relación".
Cortés' personal account of the conquest of Mexico is narrated in his five letters addressed to Charles V. These five letters, the "cartas de relación", are Cortés' only surviving writings. See "Letters and Dispatches of Cortés", translated by George Folsom (New York, 1843); Prescott's "Conquest of Mexico" (Boston, 1843); and Sir Arthur Helps's "Life of Hernando Cortes" (London, 1871).
As one specialist describes them:
The "Cartas de relación" have enjoyed an unequaled popularity among students of the Conquest of Mexico. Cortés was a good writer. His letters to the emperor, on the conquest, deserve to be classed among the best Spanish documents of the period. They are, of course, coloured so as to place his own achievements in relief, but, withal, he keeps within bounds and does not exaggerate, except in matters of Indian civilization and the numbers of population as implied by the size of the settlements. Even there he uses comparatives only, judging from outward appearances and from impressions.
Historians, sociologists and political scientists use them to glean information about the Aztec Empire and the clash between the European and Indian cultures. However, as early as the 16th century doubt has been cast on the historicity of these Conquest accounts. It is generally accepted that Cortés does not write a true "history", but rather combines history with fiction. That is to say, in his narrative Cortés manipulates reality in order to achieve his overarching purpose of gaining the favor of the king. Cortés applies the classical rhetorical figure of evidentia as he crafts a powerful narrative full of "vividness" that moves the reader and creates a heightened sense of realism in his letters.
His first letter is lost, and the one from the municipality of Veracruz has to take its place. It was published for the first time in volume IV of "Documentos para la Historia de España", and subsequently reprinted. The first "carta de relación" is available online at the University of Wisconsin.
The "Segunda Carta de Relacion", bearing the date of October 30, 1520, appeared in print at Seville in 1522. The "Carta tercera", May 15, 1522, appeared at Seville in 1523. The fourth, October 20, 1524, was printed at Toledo in 1525. The fifth, on the Honduras expedition, is contained in volume IV of the "Documentos para la Historia de España". The important letter mentioned in the text has been published under the heading of "Carta inédita de Cortés" by Ycazbalceta. A great number of minor documents, either by Cortés or others, for or against him, are dispersed through the voluminous collection above cited and through the "Colección de Documentos de Indias", as well as in the "Documentos para la Historia de México" of Ycazbalceta. There are a number of reprints and translations of Cortés's writings into various languages.
Children.
Natural children of Don Hernán Cortés
He married twice: firstly in Cuba to Catalina Suárez Marcaida, who died at Coyoacán in 1522 without issue, and secondly in 1529 to "doña" Juana Ramírez de Arellano de Zúñiga, daughter of "don" Carlos Ramírez de Arellano, 2nd Count of Aguilar and wife the Countess "doña" Juana de Zúñiga, and had:
Ancestors.
Ancestors of Hernán Cortés de Monroy y Pizarro, 1st Marquess of the Valley of Oaxaca
See also.
General:

</doc>
<doc id="14015" url="http://en.wikipedia.org/wiki?curid=14015" title="Herstory">
Herstory

Herstory is history written from a feminist perspective, emphasizing the role of women, or told from a woman's point of view. It is a neologism coined in the late 1960s as part of a feminist critique of conventional historiography, with the word "history" reinterpreted, using a false etymology, as "his story." (The word "history"—from the Ancient Greek ἱστορία, or historia, meaning "knowledge obtained by inquiry"—is etymologically unrelated to the possessive pronoun "his".)
The herstory movement has spawned women-centered presses, such as Virago Press in 1973, which publishes fiction and non-fiction by noted women authors like Janet Frame and Sarah Dunant.
Usage.
The "Oxford English Dictionary" credits Robin Morgan with coining the term in her 1970 book "Sisterhood is Powerful." Concerning the feminist organization WITCH, Morgan writes:
In 1976, Casey Miller and Kate Swift wrote in "Words & Women,"
During the 1970s and 1980s, second-wave feminists saw the study of history as a male-dominated intellectual enterprise and presented "herstory" as a means of compensation. The term, intended to be both serious and comic, became a rallying cry used on T-shirts and buttons as well as in academia.
In feminist literature and academic discourse, the term has been used occasionally as an "economical way" to describe feminist efforts against a male-centered canon.
Criticism.
Christina Hoff Sommers has been a vocal critic of the concept of herstory, and presented her argument against the movement in her 1994 book, "Who Stole Feminism?". Sommers defined herstory as an attempt to infuse education with ideology, at the expense of knowledge. The "gender feminists", as she termed them, were the band of feminists responsible for the movement, which she felt amounted to negationism. She regarded most attempts to make historical studies more female-inclusive as being artificial in nature, and an impediment to progress.
Professor and author Devoney Looser has criticized the concept of herstory for overlooking the contributions that some women made as historians before the twentieth century.
The Global Language Monitor, a nonprofit group that analyzes and tracks trends in language, named "herstory" the third most "politically incorrect" word of 2006—rivaled only by "macaca" and ""Global Warming Denier"."
Books.
Recent books published on the topic include:

</doc>
<doc id="14017" url="http://en.wikipedia.org/wiki?curid=14017" title="House of Cards (UK TV series)">
House of Cards (UK TV series)

House of Cards is a 1990 British political thriller television drama serial in four episodes, set after the end of Margaret Thatcher's tenure as Prime Minister of the United Kingdom. It was televised by the BBC from 18 November to 9 December 1990, to critical and popular acclaim.
Andrew Davies adapted the story from a novel written by Michael Dobbs, a former Chief of Staff at Conservative Party headquarters. Neville Teller also dramatised Dobbs's novel for BBC World Service in 1996, and it had two television sequels ("To Play the King" and "The Final Cut"). The opening and closing theme music for those TV series is entitled "Francis Urquhart's March".
"House of Cards" was ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes in 2000. In 2013, the serial and the Dobbs novel were the basis for a US adaptation set in Washington, D.C., commissioned and released by Netflix.
Overview.
The antihero of "House of Cards" is Francis Urquhart, a fictional Chief Whip of the Conservative Party, played by Ian Richardson. The plot follows his amoral and manipulative scheme to become leader of the governing party and, thus, Prime Minister of the United Kingdom.
Michael Dobbs did not envisage writing the second and third books, as Urquhart dies at the end of the first novel. The screenplay of the BBC's dramatisation of "House of Cards" differs from the book, and hence allows future series. Dobbs wrote two following books, "To Play the King" and "The Final Cut", which were televised in 1993 and 1995, respectively.
"House of Cards" was said to draw from Shakespeare's plays "Macbeth" and "Richard III", both of which feature main characters who are corrupted by power and ambition. Richardson has a Shakespearean background and said he based his characterization of Urquhart on Shakespeare's portrayal of Richard III. 
Urquhart frequently talks through the camera to the audience, breaking the fourth wall using the aside.
Plot.
After Margaret Thatcher's resignation, the ruling Conservative Party is about to elect a new leader. Francis Urquhart (Ian Richardson), an MP and the Government Chief Whip in the House of Commons, introduces viewers to the contestants, from which Henry "Hal" Collingridge (David Lyon) emerges victorious. Urquhart is secretly contemptuous of the well-meaning but weak Collingridge but expects a promotion to a senior position in the Cabinet. After the general election, which the party wins by a reduced majority, Urquhart makes his suggestions for a cabinet reshuffle that includes his desired promotion. However, Collingridge – citing Harold Macmillan's political demise after Night of the Long Knives (1962) – effects no changes at all. Urquhart resolves to oust Collingridge, with encouragement from his wife, Elizabeth Urquhart (Diane Fletcher).
At the same time, with Elizabeth's blessing, Urquhart begins an affair with Mattie Storin (Susannah Harker), a junior political reporter at a Conservative-leaning tabloid newspaper called "The Chronicle". The affair allows Urquhart to manipulate Mattie and indirectly skew her coverage of the Conservative leadership contest in his favour. Mattie has an apparent Electra complex; in series one episode two, she tells him part of his appeal is his age (after Urquhart notes he is old enough to be her father), and later she refers to Urquhart as "Daddy", a word that later figures prominently in Urquhart's painful flashbacks of her.
Another unwitting pawn in Urquhart's scheme is Roger O'Neill (Miles Anderson), the party's cocaine-addicted public relations consultant. Urquhart blackmails O'Neill into leaking information on budget cuts that humiliates Collingridge during the Prime Minister's Questions. Later, Urquhart blames Party chairman Lord "Teddy" Billsborough (Nicholas Selby) for the leak of an internal poll showing a drop in Tory numbers, leading Collingridge to sack him. Meanwhile, Urquhart encourages ultraconservative Foreign Secretary Patrick Woolton (Malcolm Tierney) and "Chronicle" owner Benjamin Landless to support Collingridge's removal. Finally, Urquhart poses as Collingridge's alcoholic brother, Charles, to trade shares in a chemical company about to benefit from advance information confidential to the government, and Collingridge becomes falsely accused of insider trading. This, combined with his eroding image and his bad showing at the party conference, forces him to resign.
In the ensuing leadership race, Urquhart – in imitation of William Shakespeare's Richard of Gloucester — at first feigns unwillingness to stand before announcing his candidacy. With the help of his underling, Tim Stamper (Colin Jeavons), Urquhart goes about making sure his competitors drop out of the race: Health Secretary Peter MacKenzie (Christopher Owen) accidentally runs his car over and kills a disabled protester at a demonstration staged by Urquhart and is forced by the public outcry to withdraw, while Education Secretary Harold Earle (Kenneth Gilbert) is blackmailed into withdrawing when Urquhart anonymously sends pictures of him in the company of a rent boy whom Earle had paid for sex.
The first ballot leaves Urquhart to face Woolton and Michael Samuels, the moderate Environment Secretary supported by Billsborough. Urquhart eliminates Woolton by a prolonged scheme: at the party conference, he pressures O'Neill into persuading his personal assistant and lover, Penny Guy (Alphonsia Emmanuel), to have a one-night stand with Woolton in his suite, and Urquhart records the encounter via a bugged ministerial red box. When the tape is sent to Woolton, he is led to assume that Samuels is behind the scheme and backs Urquhart in the contest. Urquhart also receives support from Collingridge, who is unaware of Urquhart's role in his own downfall. Samuels is forced out of the running when the tabloids reveal that he backed leftist causes as a student at University of Cambridge.
Stumbling across contradictions in the allegations against Collingridge and his brother, Mattie begins to dig deeper. On Urquhart's orders, O'Neill arranges for her car and flat to be vandalised in a show of intimidation. However, O'Neill becomes increasingly uneasy with what he is being asked to do, and his cocaine addiction adds to his instability. Urquhart mixes O'Neill's cocaine with rat poison, causing him to kill himself when taking the cocaine in a motorway lavatory. Though initially blind to the truth of matters thanks to her relations with Urquhart, Mattie eventually deduces that Urquhart is responsible for O'Neill's death and is behind the unfortunate downfalls of Collingridge and all of Urquhart's rivals.
Mattie looks for Urquhart at the point when it seems his victory is certain. She eventually finds him on the roof garden of the Houses of Parliament, where she confronts him. He admits to O'Neill's murder and everything else he has done. He then asks whether he can trust Mattie, and, though she answers in the affirmative, he does not believe her and throws her off the roof onto a van parked below. An unseen person picks up Mattie's tape recorder, which she had been using to secretly record her conversations with Urquhart. The series ends with Urquhart defeating Samuels in the second leadership ballot and being driven to Buckingham Palace to be invited to form a government by Elizabeth II.
Deviations from the novel in the series.
In the first novel, but not in the television series:
When the series was reissued in 2013, to coincide with the release of the US version of "House of Cards", Dobbs rewrote portions of the novel to bring the series in line with the television mini-series and restore continuity among the three novels. In the 2013 version:
Reception.
The first installment of the TV series coincidentally aired two days before the Conservative Party leadership election. Author Dobbs said that John Major's leadership headquarters "came to a halt" to view the show. During a time of "disillusionment with politics", the series "caught the nation's mood".
Ian Richardson won a Best Actor BAFTA in 1991 for his role as Urquhart, and Andrew Davies won an Emmy for outstanding writing in a miniseries.
The series ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes.
Adaptation.
The Urquhart trilogy has been adapted in the United States as "House of Cards". The show stars Kevin Spacey as Francis "Frank" Underwood, the Majority Whip of the Democratic Party, who schemes and murders his way to becoming President of the United States. It is produced by David Fincher and Spacey's Trigger Street Productions, with the initial episodes directed by Fincher. 
The series, produced and financed by independent studio Media Rights Capital, is one of Netflix's first forays into original programming. Season one was made available online on 1 February 2013. The series is filmed in Baltimore, Maryland. The first season was critically acclaimed and earned four Golden Globe Nominations, including Best Drama, actor, actress and supporting actor, with Robin Wright winning best actress. It also earned nine Primetime Emmy Award nominations, winning three, and was the first show to earn nominations that was broadcast solely via an internet streaming service.
In popular culture.
The drama introduced and popularised the phrase: "You might very well think that; I couldn't possibly comment." It was a non-confirmation confirmative statement, used by Urquhardt whenever he could not be seen to agree with a leading statement, with the emphasis on either the "I" or the "possibly", depending on the situation. The phrase was even used in the House of Commons following the series. 
A variation on the phrase was written into the TV adaptation of Terry Pratchett's "Hogfather" for Death, as an in-joke on the fact that he was voiced by Richardson. 
A further variation was used by Nicola Murray, a fictional government minister, in the third series finale of "The Thick of It". 
In the U.S. adaptation, the phrase is used by Frank Underwood in the first episode during, his initial meeting with Zoe Barnes.

</doc>
<doc id="14018" url="http://en.wikipedia.org/wiki?curid=14018" title="Helen Gandy">
Helen Gandy

Helen W. Gandy (April 8, 1897 – July 7, 1988) was an American civil servant. She was the secretary to Federal Bureau of Investigation director J. Edgar Hoover for 54 years. Hoover called her "indispensable" and she exercised great behind-the-scenes influence on Hoover and the workings of the Bureau. Following Hoover's death in 1972, she spent weeks destroying his "Personal File," thought to be where the most incriminating material he used to manipulate and control the most powerful figures in Washington was kept.
Early life.
Gandy was born in Rockville, New Jersey, one of three children (two daughters and a son) born to Franklin Dallas and Annie (née Williams) Gandy. She grew up in New Jersey in Fairton or the Port Norris section of Commercial Township (sources differ) and graduated from Bridgeton High School in Bridgeton New Jersey. In 1918, aged 21, she moved to Washington, D.C., where she later took classes at Strayer Business College and George Washington University Law School.
Career.
Gandy briefly worked in a department store in Washington before she found a job as a file clerk at the Justice Department in 1918. Within weeks, she went to work as a typist for Hoover, effective March 25, 1918, having told Hoover in her interview she had "no immediate plans to marry." She, like Hoover, would never marry, both being completely devoted to the Bureau.
When Hoover went to the Bureau of Investigation (as it was then known) as its assistant director on August 22, 1921, he specifically requested Gandy return from vacation to help him in the new post. Hoover became director of the Bureau in 1924 and Gandy continued in his service. She was promoted to "office assistant" on August 23, 1937, and "executive assistant" on October 1, 1939. Though she would receive promotions in her civil service grade subsequently, she would retain her title as executive assistant to her retirement on May 2, 1972, the day Hoover died. Hoover said of her "if there is anyone in this Bureau whose services are indispensable I consider Miss Gandy to be that person." Despite this, Curt Gentry wrote:
Theirs was a rigidly formal relationship. He'd always called her 'Miss Gandy' (when angry, barking it out as one word). In all those fifty-four years he had never once called her by her first name.
Hoover biographers Theoharis and Cox would say "her stern face recalled Cerberus at the gate," a view echoed by Anthony Summers in his life of Hoover, who also pictured Gandy as Hoover's first line of defense against the outside world. When Attorney General Robert F. Kennedy, Hoover's superior, had a direct telephone line installed between their offices, Hoover refused to answer the phone. "Put that damn thing on Miss Gandy's desk where it belongs," Hoover would declare.
Gentry would describe her influence:
Her genteel manner and pleasant voice contrasted sharply with this domineering presence. Yet behind the politeness was a resolute firmness not unlike his, and no small amount of influence. Many a career in the Bureau had been quietly manipulated by her. Even those who disliked him, praised her, most often commenting on her remarkable ability to get along with all kinds of people. That she had held her position for fifty-four years was the best evidence of this, for it was a Bureau tradition that the closer you were to him, the more demanding he was.
William C. Sullivan, an agent with the Bureau for three decades, reported in his memoir when he worked in the public relations section answering mail from the public, he gave a correspondent the wrong measurements for Hoover's personal popover recipe, relying on memory rather than the files. Gandy, ever protective of her boss, caught the error and brought it to Hoover's attention. The director then placed an official letter of reprimand in Sullivan's file for the lapse. Mark Felt, deputy associate director of the Bureau, wrote in his memoir that Gandy "was bright and alert and quick-tempered—and completely dedicated to her boss."
The Files.
J. Edgar Hoover died during the night of May 1–2, 1972. According to Curt Gentry, who wrote the book "J Edgar Hoover: The Man and the Secrets", Hoover's body was not discovered by his live-in cook and general housekeeper, Annie Fields. Rather, it was discovered by James Crawford, who had been Hoover's chauffeur for 37 years. Crawford then yelled out to Fields and Tom Moton (Hoover's new chauffeur after Crawford had retired in January, 1972). Ms. Fields first called Hoover's personal physician, Dr. Robert Choisser, then used another phone to call Clyde Tolson's private number. Tolson then called Helen Gandy's private number with the news of Hoover's death along with orders to begin destroying the files. Within an hour, the "D List" ("d" standing for destruction) was being distributed and the destruction of files began. However, "The New York Times" quoted an anonymous F.B.I. source in spring 1975 that "Gandy had begun almost a year before Mr. Hoover's death and was instructed to purge the files that were then in his office."
Anthony Summers reported that G. Gordon Liddy stated his sources in the F.B.I. said "by the time Gray went in to get the files, Miss Gandy had already got rid of them." The day after Hoover died, Gray, who had been named acting director by President Richard Nixon upon Tolson's resignation from that position, went to Hoover's office. Gandy paused from her work to give Gray a tour. He found file cabinets open and packing boxes being filled with papers. She informed him the boxes contained personal papers of Hoover's. Gandy stated Gray flipped through a few files and approved her work, but Gray was to deny he looked at any papers. Gandy also told Gray it would be a week before she could clear Hoover's effects out so he could move into the suite.
Gray reported to Nixon that he had secured Hoover's office and its contents. However, he had sealed only Hoover's personal inner office, where no files were stored, not the entire suite of offices. Since 1957, Hoover's "Official/Confidential" files, containing material too sensitive to include in the Bureau's central files, had been kept in the outer office, where Gandy sat. Gentry reported that Gray would not have known where to look in Gandy's office for the files, as her office was lined floor to ceiling with filing cabinets; moreover, without her index to the files, he would not have been able to locate incriminating material, for files were deliberately mislabeled, e.g. President Nixon's file was labeled "Obscene Matters".
On May 4, she turned over twelve boxes of the "Official/Confidential", containing 167 files and 17,750 pages to Mark Felt. Many of them contained derogatory information. Gray told the press that afternoon that "there are no dossiers or secret files. There are just general files and I took steps to preserve their integrity." Gandy retained the "Personal File".
Gandy worked on going through Hoover's "Personal File" in the office until May 12. She then transferred at least thirty-two file drawers of material to the basement rec room of Hoover's Washington home at 4936 Thirtieth Place, Northwest, where she would continue her work from May 13 to July 17. Gandy later testified nothing official had been removed from the Bureau's offices, "not even his badge." There the destruction was overseen by John P. Mohr, the number three man in the Bureau after Hoover and Tolson. They were aided by James Jesus Angleton, the Central Intelligence Agency's counterintelligence chief, whom Hoover's neighbors saw removing boxes from Hoover's home. Mohr would claim the boxes Angleton removed were cases of spoiled wine.
When the House Committee on Government Oversight investigated the F.B.I.'s spying on and harassment of Martin Luther King, Jr. and others in 1975, Gandy was called to testify. "I tore them up, put them in boxes, and they were taken away to be shredded," she told the congressmen about the papers. The Bureau's Washington field office had F.B.I. drivers transport the material to Hoover's home, then once Gandy had gone through the material, the drivers transported it back to the field office in the Old Post Office Building on Pennsylvania Avenue where it was shredded and burned.
Gandy stated that Hoover had left standing instructions to destroy his personal papers upon his death and that this instruction was confirmed by Tolson and Gray. Gandy stated that she destroyed no official papers, that everything was personal papers of Hoover. The staff of the subcommittee did not believe her, but she told the committee "I have no reason to lie." Representative Andrew Maguire (D-New Jersey), a freshman member of the 94th Congress, said "I find your testimony very difficult to believe." Gandy held her ground: "That is your privilege."
"I can give you my word. I know what there was—letters to and from friends, personal friends, a lot of letters," she testified. Gandy also said the files she took to his home also included his financial papers, such as tax returns and investment statements, the deed to his home, and papers relating to his dogs' pedigrees.
Curt Gentry wrote
In "J. Edgar Hoover: The Man and His Secrets", Gentry describes the nature of the files: "... their contents included blackmail material on the patriarch of an American political dynasty, his sons, their wives, and other women; allegations of two homosexual arrests which Hoover leaked to help defeat a witty, urbane Democratic presidential candidate; the surveillance reports on one of America's best-known first ladies and her alleged lovers, both male and female, white and black; the child molestation documentation the director used to control and manipulate one of the Red-baiting proteges; a list of the Bureau's spies in the White House during the eight administrations when Hoover was FBI director; the forbidden fruit of hundreds of illegal wiretaps and bugs, containing, for example, evidence that an attorney general, Tom C. Clark, who later became Supreme Court justice, had received payoffs from the Chicago syndicate; as well as celebrity files, with all the unsavory gossip Hoover could amass on some of the biggest names in show business."
Later years and death.
While she officially retired the day Hoover died, she spent the next few weeks destroying his papers and Hoover left her $5,000 in his will. In 1961, she and her sister, Lucy G. Rodman, donated a portrait of their mother by Thomas Eakins to the Smithsonian American Art Museum. Gandy lived in Washington, D.C., until 1986, when she moved to DeLand, Florida, in Volusia County where a niece lived.
An avid trout fisherman, she died of a heart attack on July 7, 1988, either in DeLand (says her "New York Times" obituary) or in nearby Orange City, Florida (says her "Post" obituary).
In popular culture.
Gandy was portrayed by actresses Lee Kessler in the 1987 television film "J. Edgar Hoover", and Naomi Watts in the 2011 cinematic release "J. Edgar".
External links.
Listen to this article ()
This audio file was created from a revision of the "Helen Gandy" article dated 2005-08-09, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="14019" url="http://en.wikipedia.org/wiki?curid=14019" title="Horsepower">
Horsepower

Horsepower (hp) is a unit of measurement of power (the rate at which work is done). There are many different standards and types of horsepower. The most common horsepower—especially for electrical power—is 1 hp = 746 watts. The term was adopted in the late 18th century by Scottish engineer James Watt to compare the output of steam engines with the power of draft horses. It was later expanded to include the output power of other types of piston engines, as well as turbines, electric motors and other machinery. The definition of the unit varied between geographical regions. Most countries now use the SI unit "watt" for measurement of power. With the implementation of the EU Directive 80/181/EEC on January 1, 2010, the use of horsepower in the EU is permitted only as a supplementary unit.
Definitions of term.
Units called "horsepower" have differing definitions:
History of the unit.
The development of the steam engine provided a reason to compare the output of horses with that of the engines that could replace them. In 1702, Thomas Savery wrote in The Miner's Friend:
So that an engine which will raise as much water as two horses, working together at one time in such a work, can do, and for which there must be constantly kept ten or twelve horses for doing the same. Then I say, such an engine may be made large enough to do the work required in employing eight, ten, fifteen, or twenty horses to be constantly maintained and kept for doing such a work…
The idea was later used by James Watt to help market his improved steam engine. He had previously agreed to take royalties of one third of the savings in coal from the older Newcomen steam engines. This royalty scheme did not work with customers who did not have existing steam engines but used horses instead. 
Watt determined that a horse could turn a mill wheel 144 times in an hour (or 2.4 times a minute). The wheel was 12 feet in radius; therefore, the horse travelled 2.4·2π·12 feet in one minute. Watt judged that the horse could pull with a force of 180 pounds. So:
Watt defined and calculated the horsepower as 32,572 ft·lbf/min, which was rounded to an even 33,000 ft·lbf/min.
Watt determined that a pony could lift an average 220 lbf 100 ft per minute over a four-hour working shift. Watt then judged a horse was 50% more powerful than a pony and thus arrived at the 33,000 ft·lbf/min figure. "Engineering in History" recounts that John Smeaton initially estimated that a horse could produce 22,916 foot-pounds per minute. John Desaguliers had previously suggested 44,000 foot-pounds per minute and Tredgold 27,500 foot-pounds per minute. "Watt found by experiment in 1782 that a 'brewery horse' could produce 32,400 foot-pounds per minute." James Watt and Matthew Boulton standardized that figure at 33,000 the next year.
Most observers familiar with horses and their capabilities estimate that Watt was either a bit optimistic or intended to underpromise and overdeliver; few horses can maintain that effort for long. Regardless, comparison with a horse proved to be an enduring marketing tool.
In 1993, R. D. Stevenson and R. J. Wassersug published an article calculating the upper limit to an animal's power output. The peak power over a few seconds has been measured to be as high as 14.9 hp. However, Stevenson and Wassersug observe that for sustained activity, a work rate of about 1 hp per horse is consistent with agricultural advice from both 19th and 20th century sources.
When considering human-powered equipment, a healthy human can produce about 1.2 hp briefly (see orders of magnitude) and sustain about 0.1 hp indefinitely; trained athletes can manage up to about 2.5 hp briefly
and 0.3 hp for a period of several hours.
Calculating power.
When torque formula_2 is in pound-foot units, rotational speed formula_3 is in rpm and power is required in horsepower:
The constant 5252 is the rounded value of (33,000 ft·lbf/min)/(2π rad/rev).
When torque formula_2 is in inch pounds:
The constant 63,025 is the approximation of
If torque and rotational speed are expressed in coherent SI units, the power is calculated by ;
where formula_9 is power in watts when formula_10 is torque in newton-metres, and formula_11 is angular speed in radians per second. When using other units or if the speed is in revolutions per unit time rather than radians, a conversion factor has to be included.
Current definitions.
The following definitions have been widely used:
In certain situations it is necessary to distinguish between the various definitions of horsepower and thus a suffix is added: hp(I) for mechanical (or imperial) horsepower, hp(M) for metric horsepower, hp(S) for boiler (or steam) horsepower and hp(E) for electrical horsepower.
Hydraulic horsepower is equivalent to mechanical horsepower. The formula given above is for conversion to mechanical horsepower from the factors acting on a hydraulic system.
Mechanical horsepower.
Assuming the third CGPM (1901, CR 70) definition of standard gravity, "g"n=9.80665 m/s2, is used to define the pound-force as well as the kilogram force, and the international avoirdupois pound (1959), one mechanical horsepower is:
Or given that 1 hp = 550 ft·lbf/s, 1 ft = 0.3048 m, 1 lbf ≈ 4.448 N, 1 J = 1 N·m, 1 W = 1 J/s: 1 hp ≈ 746 W
Metric horsepower (PS, cv, hk, pk, ks, ch).
The various units used to indicate this definition ("PS", "cv", "hk", "pk", "ks" and "ch") all translate to "horse power" in English, so it is common to see these values referred to as "horsepower" or "hp" in the press releases or media coverage of the German, French, Italian, and Japanese automobile companies. British manufacturers often intermix metric horsepower and mechanical horsepower depending on the origin of the engine in question. Sometimes the metric horsepower rating of an engine is conservative enough so that the same figure can be used for both 80/1269/EEC with metric hp and SAE J1349 with imperial hp.
DIN 66036 defines one metric horsepower as the power to raise a mass of 75 kilograms against the earth's gravitational force over a distance of one metre in one second; this is equivalent to 735.49875 W or 98.6% of an imperial mechanical horsepower.
In 1972, the PS was rendered obsolete by EEC directives, when it was replaced by the kilowatt as the official power measuring unit. It is still in use for commercial and advertising purposes, in addition to the kW rating, as many customers are still not familiar with the use of kilowatts for engines.
Other names for the metric horsepower are the Dutch "paardenkracht" (pk), the French "cheval" (ch), the Portuguese "cavalo-vapor" (cv), the Russian "Лошадиная сила" (лс), the Swedish "hästkraft" (hk), the Finnish "hevosvoima" (hv), the Norwegian and Danish "hestekraft" (hk), the Hungarian "lóerő" (LE), the Czech "koňská síla" and Slovak "konská sila" (k or ks), the Bosnian/Croatian/Serbian "konjska snaga" (KS), the Bulgarian "Конска сила", the Macedonian "Коњска сила" (KC), the Polish "koń mechaniczny" (KM), Slovenian "konjska moč" (KM) and the Romanian "cal-putere" (CP), which all equal the German "Pferdestärke" (PS).
In the 19th century, the French had their own unit, which they used instead of the CV or horsepower. It was called the poncelet and was abbreviated "p".
French and Italian tax horsepower (CV).
In addition, the capital form "CV" is used in Italy and France as a unit for tax horsepower, short for, respectively, "cavalli vapore" and "chevaux vapeur" ("steam horses"). CV is a non-linear rating of a motor vehicle for tax purposes. The CV rating, or fiscal power, is formula_12, where "P" is the maximum power in kilowatts and "U" is the amount of carbon dioxide (CO2) emitted in grams per kilometre. The term for CO2 measurements has been included in the definition only since 1998, so older ratings in CV are not directly comparable. The fiscal power has found its way into naming of automobile models, such as the popular Citroën deux-chevaux. The "cheval-vapeur" (ch) unit should not be confused with the French "cheval fiscal" (CV).
Electrical horsepower.
The horsepower used for electrical machines is defined as exactly 746 W. The nameplates on electrical motors show their power output, not their power input. Outside the United States watts or kilowatts are generally used for electric motor ratings and in such usage it is the input power that is stated.
Boiler horsepower.
Boiler horsepower is a boiler's capacity to deliver steam to a steam engine and is not the same unit of power as the 550 ft-lb/s definition. One boiler horsepower is equal to the power required to evaporate 34.5 lb of fresh water at 212°F in one hour. In the early days of steam use, the boiler horsepower was roughly comparable to the horsepower of engines fed by the boiler. 
Boiler horsepower is still used to measure boiler output in industrial boiler engineering in Australia, the US, and New Zealand. Boiler horsepower is abbreviated BHP, not to be confused with brake horsepower, below, which is also called BHP.
Drawbar horsepower.
Drawbar horsepower (dbhp) is the power a railway locomotive has available to haul a train or an agricultural tractor to pull an implement. This is a measured figure rather than a calculated one. A special railway car called a dynamometer car coupled behind the locomotive keeps a continuous record of the drawbar pull exerted, and the speed. From these, the power generated can be calculated. To determine the maximum power available, a controllable load is required; it is normally a second locomotive with its brakes applied, in addition to a static load.
If the drawbar force (formula_13) is measured in pounds-force (lbf) and speed (formula_14) is measured in miles per hour (mph), then the drawbar power (formula_9) in horsepower (hp) is:
Example: How much power is needed to pull a drawbar load of 2,025 pounds-force at 5 miles per hour?
The constant 375 is because 1 hp = 375 lbf·mph. If other units are used, the constant is different. When using coherent SI units (watts, newtons, and metres per second), no constant is needed, and the formula becomes formula_18.
This formula may also be used to calculate the horsepower of a jet engine, using the speed of the jet and the thrust required to maintain that speed.
Example: How much power is generated with a thrust of 4,000 pounds at 400 miles per hour?
RAC horsepower (taxable horsepower).
This measure was instituted by the Royal Automobile Club in Britain and was used to denote the power of early 20th-century British cars. Many cars took their names from this figure (hence the Austin Seven and Riley Nine), while others had names such as "40/50 hp", which indicated the RAC figure followed by the true measured power.
Taxable horsepower does not reflect developed horsepower; rather, it is a calculated figure based on the engine's bore size, number of cylinders, and a (now archaic) presumption of engine efficiency. As new engines were designed with ever-increasing efficiency, it was no longer a useful measure, but was kept in use by UK regulations which used the rating for tax purposes.
This is equal to the engine displacement in cubic inches divided by 10π then divided again by the stroke in inches.
Since taxable horsepower was computed based on bore and number of cylinders, not based on actual displacement, it gave rise to engines with 'undersquare' dimensions (bore smaller than stroke) this tended to impose an artificially low limit on rotational speed (rpm), hampering the potential power output and efficiency of the engine.
The situation persisted for several generations of four- and six-cylinder British engines: for example, Jaguar's 3.4-litre XK engine of the 1950s had six cylinders with a bore of {{convert|83|mm|in|abbr=on|sigfig=3}} and a stroke of {{convert|106|mm|in|abbr=on|sigfig=3}}, where most American automakers had long since moved to oversquare (large bore, short stroke) V-8s (see, for example, the early Chrysler Hemi).
Measurement.
The power of an engine may be measured or estimated at several points in the transmission of the power from its generation to its application. A number of names are used for the power developed at various stages in this process, but none is a clear indicator of either the measurement system or definition used.
In the case of an engine dynamometer, power is measured at the engine's flywheel.{{citation needed|date=June 2014}} Also, with a chassis dynamometer or "rolling road", power output is measured at the driving wheels. This accounts for energy or power loss through the drive train inefficiencies and weight thereof as well as gravitational force placed upon components therein.
In general:
All the above assumes that no power inflation factors have been applied to any of the readings.
Engine designers use expressions other than horsepower to denote objective targets or performance, such as brake mean effective pressure (BMEP). This is a coefficient of theoretical brake horsepower and cylinder pressures during combustion.
Nominal (or rated) horsepower (nhp or rhp).
Nominal horsepower (nhp) is an early 19th-century rule of thumb used to estimate the power of steam engines.
nhp = 7 x area of piston x equivalent piston speed/33,000
For paddle ships the piston speed was estimated as 129.7 x (stroke)1/3.35.
The stroke was the distance moved by the piston.
For the nominal horsepower to equal the actual power it would be necessary for the mean steam pressure in the cylinder during the stroke to be {{convert|48|kPa|psi|0|abbr=on}} and for the piston speed to be of the order of 54–75 m/min.
The French Navy used the same definition of nominal horse power as Britain.
Indicated horsepower (ihp).
Indicated horsepower (ihp) is the theoretical power of a reciprocating engine if it is completely frictionless in converting the expanding gas energy (piston pressure × displacement) in the cylinders. It is calculated from the pressures developed in the cylinders, measured by a device called an "engine indicator" – hence indicated horsepower. As the piston advances throughout its stroke, the pressure against the piston generally decreases, and the indicator device usually generates a graph of pressure vs stroke within the working cylinder. From this graph the amount of work performed during the piston stroke may be calculated.
Indicated horsepower was a better measure of engine power than nominal horsepower (nhp) because it took account of steam pressure. But unlike later measures such as shaft horsepower (shp) and brake horsepower (bhp), it did not take into account power losses due to the machinery internal frictional losses, such as a piston sliding within the cylinder, plus bearing friction, transmission and gear box friction, etc.
Brake horsepower (bhp).
Horsepower at the output shaft of an engine, turbine, or motor is termed brake horsepower or shaft horsepower, depending on what kind of instrument is used to measure it.
It is the measure of an engine's horsepower before the loss in power caused by the gearbox and drive train etc. 
In Europe the DIN standard tested the engine fitted with all ancillaries and exhaust system as used in the car. The American SAE system tests without alternator, water pump, and other auxiliary components such as power steering pump, muffled exhaust system, etc. so the figures are higher than the European figures for the same engine. Brake refers to the device which was used to load an engine and hold it at a desired rotational speed. During testing, the output torque and rotational speed were measured to determine the brake horsepower. Horsepower was originally measured and calculated by use of the "indicator diagram" (a James Watt invention of the late 18th century), and later by means of a de Prony brake connected to the engine's output shaft.
More recently, an electrical brake dynamometer is used instead of a De Prony brake. Although the output delivered to the driving wheels is less than that obtainable at the engine's crankshaft, a chassis dynamometer gives an indication of an engine's "real world" horsepower after losses in the drive train and gearbox.
Shaft horsepower (shp).
Shaft horsepower (shp) is the power delivered to the propeller shafts of a steamship (or one powered by diesel engines or nuclear power), or an aircraft powered by a piston engine or a gas turbine engine, and the rotors of a helicopter. This shaft horsepower can be measured with instruments, or estimated from the indicated horsepower and a standard figure for the losses in the transmission (typical figures are around 10%). This measure is not commonly used in the automobile industry, because in that context drive train losses can become significant.
Wheel horsepower (whp).
Motor vehicle dynamometers are used which measure the actual horsepower delivered to the driving wheel(s), which represents the actual usable power levels available considering all losses in the drive train, and all parasitic losses such as pumps, fans, alternator, etc. The vehicle is generally attached to the dynamometer and accelerates a large roller and Power Absorbing Unit which is driven by the vehicle's drive wheel(s). The actual power is then computer calculated based on the rotational inertia of the roller, its resultant acceleration rates and power applied by the Power Absorbing Unit.
Engine power test codes.
Engine power test codes determine how the power and torque of an automobile engine is measured and corrected. Correction factors are used to adjust power and torque measurements to standard atmospheric conditions to provide a more accurate comparison between engines as they are affected by the pressure, humidity, and temperature of ambient air. There exist several standards for this purpose, some described below.
Society of Automotive Engineers/SAE International.
SAE gross power.
Prior to the 1972 model year, American automakers rated and advertised their engines in brake horsepower (bhp), frequently referred to as SAE gross horsepower, because it was measured in accord with the protocols defined in SAE standards J245 and J1995. As with other brake horsepower test protocols, SAE gross hp was measured using a stock test engine, generally running with few belt-driven accessories and sometimes fitted with long tube test headers in lieu of the OEM exhaust manifolds. The atmospheric correction standards for barometric pressure, humidity and temperature for testing were relatively idealistic.
SAE net power.
In the United States, the term "bhp" fell into disuse in 1971–72, as automakers began to quote power in terms of SAE net horsepower in accord with SAE standard J1349. Like SAE gross and other brake horsepower protocols, SAE Net hp is measured at the engine's crankshaft, and so does not account for transmission losses. However, the SAE net power testing protocol calls for standard production-type belt-driven accessories, air cleaner, emission controls, exhaust system, and other power-consuming accessories. This produces ratings in closer alignment with the power produced by the engine as it is actually configured and sold.
SAE certified power.
In 2005, the SAE introduced "SAE Certified Power" with SAE J2723. This test is voluntary and is in itself not a separate engine test code but a certification of either J1349 or J1995 after which the manufacturer is allowed to advertise "Certified to SAE J1349" or "Certified to SAE J1995" depending on which test standard have been followed. To attain certification the test must follow the SAE standard in question, take place in an ISO9000/9002 certified facility and be witnessed by an SAE approved third party.
A few manufacturers such as Honda and Toyota switched to the new ratings immediately, with multi-directional results; the rated output of Cadillac's supercharged Northstar V8 jumped from {{convert|440|to|469|hp|kW|abbr=on}} under the new tests, while the rating for Toyota's Camry 3.0 L "1MZ-FE" V6 fell from {{convert|210|to|190|hp|kW|abbr=on}}. The company's Lexus ES 330 and Camry SE V6 were previously rated at {{convert|225|hp|kW|abbr=on}} but the ES 330 dropped to {{convert|218|hp|kW|abbr=on}} while the Camry declined to {{convert|210|hp|kW|abbr=on}}. The first engine certified under the new program was the 7.0 L LS7 used in the 2006 Chevrolet Corvette Z06. Certified power rose slightly from {{convert|500|to|505|hp|kW|sigfig=3|abbr=on}}.
While Toyota and Honda are retesting their entire vehicle lineups, other automakers generally are retesting only those with updated powertrains. For example, the 2006 Ford Five Hundred is rated at 203 horsepower, the same as that of 2005 model. However, the 2006 rating does not reflect the new SAE testing procedure as Ford is not going to spend the extra expense of retesting its existing engines. Over time, most automakers are expected to comply with the new guidelines.
SAE tightened its horsepower rules to eliminate the opportunity for engine manufacturers to manipulate factors affecting performance such as how much oil was in the crankcase, engine control system calibration, and whether an engine was tested with premium fuel. In some cases, such can add up to a change in horsepower ratings. A road test editor at Edmunds.com, John Di Pietro, said decreases in horsepower ratings for some '06 models are not that dramatic. For vehicles like a midsize family sedan, it is likely that the reputation of the manufacturer will be more important.
"Deutsches Institut für Normung" 70020.
DIN 70020 is a standard from German DIN regarding road vehicles. DIN testing, unlike SAE, tested the engine as installed in the vehicle, with cooling system, charging system and stock exhaust system all connected. Because the German word for "horsepower" is "Pferdestärke", in Germany it is commonly abbreviated to "PS". DIN hp is measured at the engine's output shaft, and is usually expressed in metric (Pferdestärke) rather than mechanical horsepower.
Economic Commission for Europe R24.
ECE R24 is a UN standard for the approval of compression ignition engine emissions, installation and measurement of engine power. It is similar to DIN 70020 standard, but with different requirements for connecting an engine's fan during testing causing it to absorb less power from the engine.
Economic Commission for Europe R85.
ECE R85 is a UN standard for the approval of internal combustion engines with regard to the measurement of the net power.
80/1269/EEC.
80/1269/EEC of 16 December 1980 is a European Union standard for road vehicle engine power.
Japanese Industrial Standard D 1001.
JIS D 1001 is a Japanese net, and gross, engine power test code for automobiles or trucks having a spark ignition, diesel engine, or fuel injection engine.

</doc>
<doc id="14020" url="http://en.wikipedia.org/wiki?curid=14020" title="History of London">
History of London

London, the capital city of England and the United Kingdom, has a history dating back over 2,000 years. During this time, it has grown to become one of the most significant financial and cultural capitals of the world. It has experienced plague, devastating fire, civil war, aerial bombardment, and terrorist attacks. The City of London is its historic core and today is its primary financial district, though it now represents a tiny part of the wider metropolis of Greater London.
Etymology.
The name of London is derived from Londinium, established in the 1st century as a commercial centre in Roman Britain. The etymology of the name is uncertain. The stems Londin- and Lundin- are the most prevalent in names used from Roman times onward.
Legendary foundations and prehistoric London.
According to the legendary "Historia Regum Britanniae", of Geoffrey of Monmouth, London was founded by Brutus of Troy about 1000–1100 B.C. after he defeated the native giants Gog and Magog; the settlement was known as "Caer Troia", "Troia Nova" (Latin for New Troy), which, according to a pseudo-etymology, was corrupted to "Trinovantum". Trinovantes were the Iron Age tribe who inhabited the area prior to the Romans. Geoffrey provides prehistoric London with a rich array of legendary kings, such as King Lud (see also Lludd, from Welsh Mythology) who, he claims, renamed the town "Caer Ludein", from which London was derived, and was buried at Ludgate.
However, despite intensive excavations, archaeologists have found no evidence of a prehistoric major settlement in the area. There have been scattered prehistoric finds, evidence of farming, burial and traces of habitation, but nothing more substantial. It is now considered unlikely that a pre-Roman city existed, but as some of the Roman city remains unexcavated, it is still just possible that some major settlement may yet be discovered. London was most likely a rural area with scattered settlement. Rich finds such as the Battersea Shield, found in the Thames near Chelsea, suggest the area was important; there may have been important settlements at Egham and Brentford, and there was a hillfort at Uphall Camp, Ilford, but no city in the area of the Roman London, the present day City of London.
Some discoveries indicate probable very early settlements near the Thames in the London area. In 2010 the foundations of a large timber structure, dated to 4000BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. In 1999, the remains of a Bronze Age bridge were found, again on the foreshore south of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500BC. In 2001 a further dig found that the timbers were driven vertically into the ground on the south bank of the Thames west of Vauxhall Bridge. All these structures are on the south bank at a natural crossing point where the River Effra flows into the River Thames.
Numerous finds have been made of spear heads and weaponry from the Bronze and Iron ages near the banks of the Thames in the London area, many of which had clearly been used in battle. This suggests that the Thames was an important tribal boundary. 
Early history.
Roman London (43-410 AD).
"Londinium" was established as a civilian town by the Romans about seven years after the invasion of AD 43. London, like Rome, was founded on the point of the river where it was narrow enough to bridge and the strategic location of the city provided easy access to much of Europe. Early Roman London occupied a relatively small area, roughly equivalent to the size of Hyde Park. In around AD 60, it was destroyed by the Iceni led by their queen Boudica. The city was quickly rebuilt as a planned Roman town and recovered after perhaps 10 years, the city growing rapidly over the following decades.
During the 2nd century "Londinium" was at its height and replaced Colchester as the capital of Roman Britain (Britannia). Its population was around 60,000 inhabitants. It boasted major public buildings, including the largest basilica north of the Alps, temples, bath houses, an amphitheatre and a large fort for the city garrison. Political instability and recession from the 3rd century onwards led to a slow decline.
At some time between 180 and 225 AD the Romans built the defensive London Wall around the landward side of the city. The wall was about 3 km long, 6 m high, and 2.5 m thick. The wall would survive for another 1,600 years and define the City of London's perimeters for centuries to come. The perimeters of the present City are roughly defined by the line of the ancient wall.
In the late 3rd century, Londinium was raided on several occasions by Saxon pirates. This led, from around 255 onwards, to the construction of an additional riverside wall. Six of the traditional seven city gates of London are of Roman origin, namely: Ludgate, Newgate, Aldersgate, Cripplegate, Bishopsgate and Aldgate (Moorgate is the exception, being of medieval origin).
By the 5th century the Roman Empire was in rapid decline, and in 410 AD the Roman occupation of Britain came to an end. Following this, the Roman city also went into rapid decline and by the end of the 5th century was practically abandoned.
Anglo-Saxon London (5th century – 1066 AD).
Until recently it was believed that Anglo-Saxon settlement initially avoided the area immediately around Londinium. However, the discovery in 2008 of an Anglo-Saxon cemetery at Covent Garden indicates that the incomers had begun to settle there at least as early as the 6th century and possibly in the 5th. The main focus of this settlement was outside the Roman walls, clustering a short distance to the west along what is now the Strand, between the Aldwych and Trafalgar Square. It was known as "Lundenwic", the "-wic" suffix here denoting a trading settlement. Recent excavations have also highlighted the population density and relatively sophisticated urban organisation of this earlier Anglo-Saxon London, which was laid out on a grid pattern and grew to house a likely population of 10-12,000.
Early Anglo-Saxon London belonged to a people known as the Middle Saxons, from whom the name of the county of Middlesex is derived, but who probably also occupied the approximate area of modern Hertfordshire and Surrey. However, by the early 7th century the London area had been incorporated into the kingdom of the East Saxons. In 604 King Saebert of Essex converted to Christianity and London received Mellitus, its first post-Roman bishop.
At this time Essex was under the overlordship of King Æthelberht of Kent, and it was under Æthelberht's patronage that Mellitus founded the first St. Paul's Cathedral, traditionally said to be on the site of an old Roman Temple of Diana (although Christopher Wren found no evidence of this). It would have only been a modest church at first and may well have been destroyed after he was expelled from the city by Saeberht's pagan successors.
The permanent establishment of Christianity in the East Saxon kingdom took place in the reign of King Sigeberht II in the 650s. During the 8th century the kingdom of Mercia extended its dominance over south-eastern England, initially through overlordship which at times developed into outright annexation. London seems to have come under direct Mercian control in the 730s.
Viking attacks dominated most of the 9th century, becoming increasingly common from around 830 onwards. London was sacked in 842 and again in 851. The Danish "Great Heathen Army", which had rampaged across England since 865, wintered in London in 871. The city remained in Danish hands until 886, when it was captured by the forces of King Alfred the Great of Wessex and reincorporated into Mercia, then governed under Alfred's sovereignty by his son-in-law Ealdorman Æthelred.
Around this time the focus of settlement moved within the old Roman walls for the sake of defence, and the city became known as "Lundenburgh". The Roman walls were repaired and the defensive ditch re-cut, while the bridge was probably rebuilt at this time. A second fortified Borough was established on the south bank at Southwark, the "Suthringa Geworc" (defensive work of the men of Surrey). The old settlement of "Lundenwic" became known as the "ealdwic" or "old settlement", a name which survives today as Aldwych.
From this point, the City of London began to develop its own unique local government. Following Æthelred's death in 911 it was transferred to Wessex, preceding the absorption of the rest of Mercia in 918. Although it faced competition for political preeminence in the united Kingdom of England from the traditional West Saxon centre of Winchester, London's size and commercial wealth brought it a steadily increasing importance as a focus of governmental activity. King Aethelstan held many meetings of the "witan" in London and issued laws from there, while King Æthelred the Unready issued the Laws of London there in 978.
Following the resumption of Viking attacks in the reign of Æthelred, London was unsuccessfully attacked in 994 by an army under King Sweyn Forkbeard of Denmark. As English resistance to the sustained and escalating Danish onslaught finally collapsed in 1013, London repulsed an attack by the Danes and was the last place to hold out while the rest of the country submitted to Sweyn, but by the end of the year it too capitulated and Æthelred fled abroad. Sweyn died just five weeks after having been proclaimed king and Æthelred was restored to the throne, but Sweyn's son Cnut returned to the attack in 1015.
After Æthelred's death at London in 1016 his son Edmund Ironside was proclaimed king there by the "witangemot" and left to gather forces in Wessex. London was then subjected to a systematic siege by Cnut but was relieved by King Edmund's army; when Edmund again left to recruit reinforcements in Wessex the Danes resumed the siege but were again unsuccessful. However, following his defeat at the Battle of Assandun Edmund ceded to Cnut all of England north of the Thames, including London, and his death a few weeks later left Cnut in control of the whole country.
A Norse saga tells of a battle when King Æthelred returned to attack Danish-occupied London. According to the saga, the Danes lined London Bridge and showered the attackers with spears. Undaunted, the attackers pulled the roofs off nearby houses and held them over their heads in the boats. Thus protected, they were able to get close enough to the bridge to attach ropes to the piers and pull the bridge down, thus ending the Viking occupation of London. This story presumably relates to Æthelred's return to power after Sweyn's death in 1014, but there is no strong evidence of any such struggle for control of London on that occasion.
Following the extinction of Cnut's dynasty in 1042 English rule was restored under Edward the Confessor. He was responsible for the foundation of Westminster Abbey and spent much of his time at Westminster, which from this time steadily supplanted the City itself as the centre of government. Edward's death at Westminster in 1066 without a clear heir led to a succession dispute and the Norman conquest of England. Earl Harold Godwinson was elected king by the "witangemot" and crowned in Westminster Abbey but was defeated and killed by William the Bastard, Duke of Normandy at the Battle of Hastings. The surviving members of the "witan" met in London and elected King Edward's young nephew Edgar the Ætheling as king.
The Normans advanced to the south bank of the Thames opposite London, where they defeated an English attack and burned Southwark but were unable to storm the bridge. They moved upstream and crossed the river at Wallingford before advancing on London from the north-west. The resolve of the English leadership to resist collapsed and the chief citizens of London went out together with the leading members of the Church and aristocracy to submit to William at Berkhamstead, although according to some accounts there was a subsequent violent clash when the Normans reached the city. Having occupied London, William was crowned king in Westminster Abbey.
Norman and Medieval London (1066 – late 15th century).
The new Norman regime established new fortresses within the city to dominate the native population. By far the most important of these was the Tower of London at the eastern end of the city, where the initial wooden fortification was rapidly replaced by the construction of the first stone castle in England. The smaller forts of Baynard's Castle and Montfichet's Castle were also established along the waterfront. King William also granted a charter in 1067 confirming the city's existing rights, privileges and laws. Its growing self-government was consolidated by the election rights granted by King John in 1199 and 1215.
In 1097 William Rufus, the son of William the Conqueror began the construction of 'Westminster Hall', which became the focus of the Palace of Westminster.
In 1176 construction began of the most famous incarnation of London Bridge (completed in 1209) which was built on the site of several earlier wooden bridges. This bridge would last for 600 years, and remained the only bridge across the River Thames until 1739.
In 1216 during the First Barons' War London was occupied by Prince Louis of France, who had been called in by the baronial rebels against King John and was acclaimed as King of England in St Paul's Cathedral. However, following John's death in 1217 Louis's supporters reverted to their Plantagenet allegiance, rallying round John's son Henry III, and Louis was forced to withdraw from England.
Over the following centuries, London would shake off the heavy French cultural and linguistic influence which had been there since the times of the Norman conquest. The city would figure heavily in the development of Early Modern English.
During the Peasants' Revolt of 1381 London was invaded by rebels led by Wat Tyler. A group of peasants stormed the Tower of London and executed the Lord Chancellor, Archbishop Simon Sudbury, and the Lord Treasurer. The peasants looted the city and set fire to numerous buildings. Tyler was stabbed to death by the Lord Mayor William Walworth in a confrontation at Smithfield and the revolt collapsed.
Trade increased steadily during the Middle Ages, and London grew rapidly as a result. In 1100 London's population was somewhat more than 15,000. By 1300 it had grown to roughly 80,000. London lost at least half of its population during the Black Death in the mid-14th century, but its economic and political importance stimulated a rapid recovery despite further epidemics. Trade in London was organised into various guilds, which effectively controlled the city, and elected the Lord Mayor of the City of London.
Medieval London was made up of narrow and twisting streets, and most of the buildings were made from combustible materials such as wood and straw, which made fire a constant threat, while sanitation in cities was poor.
Modern history.
Tudor London (1485–1603).
During the Reformation, London was the principal early centre of Protestantism in England. Its close commercial connections with the Protestant heartlands in northern continental Europe, large foreign mercantile communities, disproportionately large number of literate inhabitants and role as the centre of the English print trade all contributed to the spread of the new ideas of religious reform. Before the Reformation, more than half of the area of London was the property of monasteries, nunneries and other religious houses.
Henry VIII's "Dissolution of the Monasteries" had a profound effect on the city as nearly all of this property changed hands. The process started in the mid 1530s, and by 1538 most of the larger monastic houses had been abolished. Holy Trinity Aldgate went to Lord Audley, and the Marquess of Winchester built himself a house in part of its precincts. The Charterhouse went to Lord North, Blackfriars to , the leper hospital of St Giles to Lord Dudley, while the king took for himself the leper hospital of St James, which was rebuilt as St James's Palace.
The period saw London rapidly rising in importance amongst Europe's commercial centres. Trade expanded beyond Western Europe to Russia, the Levant, and the Americas. This was the period of mercantilism and monopoly trading companies such as the Muscovy Company (1555) and the British East India Company (1600) were established in London by Royal Charter. The latter, which ultimately came to rule India, was one of the key institutions in London, and in Britain as a whole, for two and a half centuries. Immigrants arrived in London not just from all over England and Wales, but from abroad as well, for example Huguenots from France; the population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. The growth of the population and wealth of London was fuelled by a vast expansion in the use of coastal shipping.
The late 16th and early 17th century saw the great flourishing of drama in London whose preeminent figure was William Shakespeare. During the mostly calm later years of Elizabeth's reign, some of her courtiers and some of the wealthier citizens of London built themselves country residences in Middlesex, Essex and Surrey. This was an early stirring of the villa movement, the taste for residences which were neither of the city nor on an agricultural estate, but at the time of Elizabeth's death in 1603, London was still very compact.
Xenophobia was rampant in London, and increased after the 1580s. Many immigrants became disillusioned by routine threats of violence and molestation, attempts at expulsion of foreigners, and the great difficulty in acquiring English citizenship. Dutch cities proved more hospitable, and many left London permanently.
Stuart London (1603–1714).
A panorama of London by Claes Jansz. Visscher, 1616. Old St Paul's had lost its spire by this time. The two theatres on the foreground (Southwark) side of the Thames are The Bear Garden and The Globe. The large church in the foreground is St Mary Overie, now Southwark Cathedral.
London's expansion beyond the boundaries of the City was decisively established in the 17th century. In the opening years of that century the immediate environs of the City, with the principal exception of the aristocratic residences in the direction of Westminster, were still considered not conducive to health. Immediately to the north was Moorfields, which had recently been drained and laid out in walks, but it was frequented by beggars and travellers, who crossed it in order to get into London. Adjoining Moorfields were Finsbury Fields, a favourite practising ground for the archers, Mile End, then a common on the Great Eastern Road and famous as a rendezvous for the troops.
The preparations for King James I becoming king were interrupted by a severe plague epidemic, which may have killed over thirty thousand people. The Lord Mayor's Show, which had been discontinued for some years, was revived by order of the king in 1609. The dissolved monastery of the Charterhouse, which had been bought and sold by the courtiers several times, was purchased by Thomas Sutton for £13,000. The new hospital, chapel, and schoolhouse were begun in 1611. Charterhouse School was to be one of the principal public schools in London until it moved to Surrey in Victorian times, and the site is still used as a medical school.
The general meeting-place of Londoners in the day-time was the nave of Old St. Paul's Cathedral. Merchants conducted business in the aisles, and used the font as a counter upon which to make their payments; lawyers received clients at their particular pillars; and the unemployed looked for work. St Paul's Churchyard was the centre of the book trade and Fleet Street was a centre of public entertainment. Under James I the theatre, which established itself so firmly in the latter years of Elizabeth, grew further in popularity. The performances at the public theatres were complemented by elaborate masques at the royal court and at the inns of court.
Charles I acceded to the throne in 1625. During his reign, aristocrats began to inhabit the West End in large numbers. In addition to those who had specific business at court, increasing numbers of country landowners and their families lived in London for part of the year simply for the social life. This was the beginning of the "London season". Lincoln's Inn Fields was built about 1629. The piazza of Covent Garden, designed by England's first classically trained architect Inigo Jones followed in about 1632. The neighbouring streets were built shortly afterwards, and the names of Henrietta, Charles, James, King and York Streets were given after members of the royal family.
In January 1642 five members of parliament whom the King wished to arrest were granted refuge in the City. In August of the same year the King raised his banner at Nottingham, and during the English Civil War London took the side of the parliament. Initially the king had the upper hand in military terms and in November he won the Battle of Brentford a few miles to the west of London. The City organised a new makeshift army and Charles hesitated and retreated. Subsequently an extensive system of fortifications was built to protect London from a renewed attack by the Royalists. This comprised a strong earthen rampart, enhanced with bastions and redoubts. It was well beyond the City walls and encompassed the whole urban area, including Westminster and Southwark. London was not seriously threatened by the royalists again, and the financial resources of the City made an important contribution to the parliamentarians victory in the war.
The unsanitary and overcrowded City of London has suffered from the numerous outbreaks of the plague many times over the centuries, but in Britain it is the last major outbreak which is remembered as the "Great Plague" It occurred in 1665 and 1666 and killed around 60,000 people, which was one fifth of the population. Samuel Pepys chronicled the epidemic in his diary. On 4 September 1665 he wrote "I have stayed in the city till above 7400 died in one week, and of them about 6000 of the plague, and little noise heard day or night but tolling of bells."
Great Fire of London (1666).
The Great Plague was immediately followed by another catastrophe, albeit one which helped to put an end to the plague. On the Sunday, 2 September 1666 the Great Fire of London broke out at one o'clock in the morning at a bakery in Pudding Lane in the southern part of the City. Fanned by an eastern wind the fire spread, and efforts to arrest it by pulling down houses to make firebreaks were disorganised to begin with. On Tuesday night the wind fell somewhat, and on Wednesday the fire slackened. On Thursday it was extinguished, but on the evening of that day the flames again burst forth at the Temple. Some houses were at once blown up by gunpowder, and thus the fire was finally mastered. The Monument was built to commemorate the fire: for over a century and a half it bore an inscription attributing the conflagration to a "popish frenzy".
The fire destroyed about 60% of the City, including Old St Paul's Cathedral, 87 parish churches, 44 livery company halls and the Royal Exchange. However, the number of lives lost was surprisingly small; it is believed to have been 16 at most. Within a few days of the fire, three plans were presented to the king for the rebuilding of the city, by Christopher Wren, John Evelyn and Robert Hooke.
Wren proposed to build main thoroughfares north and south, and east and west, to insulate all the churches in conspicuous positions, to form the most public places into large piazzas, to unite the halls of the 12 chief livery companies into one regular square annexed to the Guildhall, and to make a fine quay on the bank of the river from Blackfriars to the Tower of London. Wren wished to build the new streets straight and in three standard widths of thirty, sixty and ninety feet. Evelyn's plan differed from Wren's chiefly in proposing a street from the church of St Dunstan's in the East to the St Paul's, and in having no quay or terrace along the river. These plans were not implemented, and the rebuilt city generally followed the streetplan of the old one, and most of it has survived into the 21st century.
Nonetheless, the new City was different from the old one. Many aristocratic residents never returned, preferring to take new houses in the West End, where fashionable new districts such as St. James's were built close to the main royal residence, which was Whitehall Palace until it was destroyed by fire in the 1690s, and thereafter St. James's Palace. The rural lane of Piccadilly sprouted courtiers mansions such as Burlington House. Thus the separation between the middle class mercantile City of London, and the aristocratic world of the court in Westminster became complete.
In the City itself there was a move from wooden buildings to stone and brick construction to reduce the risk of fire. Parliament's Rebuilding of London Act 1666 stated "building with brick [is] not only more comely and durable, but also more safe against future perils of fire". From then on only doorcases, window-frames and shop fronts were allowed to be made of wood.
Christopher Wren's plan for a new model London came to nothing, but he was appointed to rebuild the ruined parish churches and to replace St Paul's Cathedral. His domed baroque cathedral was the primary symbol of London for at least a century and a half. As city surveyor, Robert Hooke oversaw the reconstruction of the City's houses. The East End, that is the area immediately to the east of the city walls, also became heavily populated in the decades after the Great Fire. London's docks began to extend downstream, attracting many working people who worked on the docks themselves and in the processing and distributive trades. These people lived in Whitechapel, Wapping, Stepney and Limehouse, generally in slum conditions.
In the winter of 1683–4 a frost fair was held on the Thames. The frost, which began about seven weeks before Christmas and continued for six weeks after, was the greatest on record. The Revocation of the Edict of Nantes in 1685 led to a large migration on Huguenots to London. They established a silk industry at Spitalfields.
At this time the Bank of England was founded, and the British East India Company was expanding its influence. Lloyd's of London also began to operate in the late 17th century. In 1700 London handled 80% of England's imports, 69% of its exports and 86% of its re-exports. Many of the goods were luxuries from the Americas and Asia such as silk, sugar, tea and tobacco. The last figure emphasises London's role as an entrepot: while it had many craftsmen in the 17th century, and would later acquire some large factories, its economic prominence was never based primarily on industry. Instead it was a great trading and redistribution centre. Goods were brought to London by England's increasingly dominant merchant navy, not only to satisfy domestic demand, but also for re-export throughout Europe and beyond.
William III, a Dutchman, cared little for London, the smoke of which gave him asthma, and after the first fire at Whitehall Palace (1691) he purchased Nottingham House and transformed it into Kensington Palace. Kensington was then an insignificant village, but the arrival of the court soon caused it to grow in importance. The palace was rarely favoured by future monarchs, but its construction was another step in the expansion of the bounds of London. During the same reign Greenwich Hospital, then well outside the boundary of London, but now comfortably inside it, was begun; it was the naval complement to the Chelsea Hospital for former soldiers, which had been founded in 1681. During the reign of Queen Anne an act was passed authorising the building of 50 new churches to serve the greatly increased population living outside the boundaries of the City of London.
18th century.
The 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire.
In 1707 an Act of Union was passed merging the Scottish and the English Parliaments, thus establishing the Kingdom of Great Britain. A year later, in 1708 Christopher Wren's masterpiece, St Paul's Cathedral was completed on his birthday. However, the first service had been held on 2 December 1697; more than 10 years earlier. This Cathedral replaced the original St. Paul's which had been completely destroyed in the Great Fire of London. This building is considered one of the finest in Britain and a fine example of Baroque architecture.
Many tradesmen from different countries came to London to trade goods and merchandise. Also, more immigrants moved to London making the population greater. More people also moved to London for work and for business making London an altogether bigger and busier city. Britain's victory in the Seven Years' War increased the country's international standing and opened large new markets to British trade, further boosting London's prosperity.
During the Georgian period London spread beyond its traditional limits at an accelerating pace. This is shown in a series of detailed maps, particularly John Rocque's 1741–45 map "(see below)" and his 1746 Map of London. New districts such as Mayfair were built for the rich in the West End, new bridges over the Thames encouraged an acceleration of development in South London and in the East End, the Port of London expanded downstream from the City. During this period was also the uprising of the American colonies. In 1780, the Tower of London held its only American prisoner, former President of the Continental Congress, Henry Laurens. In 1779 he was the Congress's representative of Holland, and got the country's support for the Revolution. On his return voyage back to America, the Royal Navy captured him and charged him with treason after finding evidence of a reason of war between Great Britain and the Netherlands. He was released from the Tower on 21 December 1781 in exchange for General Lord Cornwallis.
In 1762 George III acquired Buckingham Palace (then called Buckingham House) from the Duke of Buckingham. It was enlarged over the next 75 years by architects such as John Nash.
A phenomenon of 18th-century London was the coffeehouse, which became a popular place to debate ideas. Growing literacy and the development of the printing press meant that news became widely available. Fleet Street became the centre of the embryonic British press during the century.
18th-century London was dogged by crime, the Bow Street Runners were established in 1750 as a professional police force. Penalties for crime were harsh, with the death penalty being applied for fairly minor crimes. Public hangings were common in London, and were popular public events.
In 1780 London was rocked by the Gordon Riots, an uprising by Protestants against Roman Catholic emancipation led by Lord George Gordon. Severe damage was caused to Catholic churches and homes, and 285 rioters were killed.
In the year 1787, freed slaves from London, America, and many of Britain's colonies founded Freetown in modern-day Sierra Leone.
Up until 1750, London Bridge was the only crossing over the Thames, but in that year Westminster Bridge was opened and, for the first time in history, London Bridge, in a sense, had a rival. In 1798, Frankfurt banker Nathan Mayer Rothschild arrived in London and set up a banking house in the city, with a large sum of money given to him by his father, Amschel Mayer Rothschild. The Rothschilds also had banks in Paris and Vienna. The bank financed numerous large-scale projects, especially regarding railways around the world and the Suez Canal.
The 18th century saw the breakaway of the American colonies and many other unfortunate events in London, but also great change and Enlightenment. This all led into the beginning of modern times, the 19th century.
19th century.
During the 19th century, London was transformed into the world's largest city and capital of the British Empire. Its population expanded from 1 million in 1800 to 6.7 million a century later. During this period, London became a global political, financial, and trading capital. In this position, it was largely unrivalled until the latter part of the century, when Paris and New York began to threaten its dominance.
While the city grew wealthy as Britain's holdings expanded, 19th-century London was also a city of poverty, where millions lived in overcrowded and unsanitary slums. Life for the poor was immortalised by Charles Dickens in such novels as Oliver Twist In 1810, after the death of Sir Francis Baring and Abraham Goldsmid, Rothschild emerges as the major banker in London.
In 1829 the then Home Secretary (and future prime minister) Robert Peel established the Metropolitan Police as a police force covering the entire urban area. The force gained the nickname of "bobbies" or "peelers" named after Robert Peel.
19th-century London was transformed by the coming of the railways. A new network of metropolitan railways allowed for the development of suburbs in neighboring counties from which middle-class and wealthy people could commute to the centre. While this spurred the massive outward growth of the city, the growth of greater London also exacerbated the class divide, as the wealthier classes emigrated to the suburbs, leaving the poor to inhabit the inner city areas.
The first railway to be built in London was a line from London Bridge to Greenwich, which opened in 1836. This was soon followed by the opening of great rail termini which linked London to every corner of Britain. These included Euston station (1837), Paddington station (1838), Fenchurch Street station (1841), Waterloo station (1848), King's Cross station (1850), and St Pancras station (1863). From 1863, the first lines of the London Underground were constructed.
The urbanised area continued to grow rapidly, spreading into Islington, Paddington, Belgravia, Holborn, Finsbury, Shoreditch, Southwark and Lambeth. Towards the middle of the century, London's antiquated local government system, consisting of ancient parishes and vestries, struggled to cope with the rapid growth in population. In 1855 the Metropolitan Board of Works (MBW) was created to provide London with adequate infrastructure to cope with its growth. One of its first tasks was addressing London's sanitation problems. At the time, raw sewage was pumped straight into the River Thames. This culminated in The Great Stink of 1858. Parliament finally gave consent for the MBW to construct a large system of sewers. The engineer put in charge of building the new system was Joseph Bazalgette. In what was one of the largest civil engineering projects of the 19th century, he oversaw construction of over 2100 km of tunnels and pipes under London to take away sewage and provide clean drinking water. When the London sewerage system was completed, the death toll in London dropped dramatically, and epidemics of cholera and other diseases were curtailed. Bazalgette's system is still in use today.
One of the most famous events of 19th-century London was the Great Exhibition of 1851. Held at The Crystal Palace, the fair attracted 6 million visitors from across the world and displayed Britain at the height of its Imperial dominance.
As the capital of a massive empire, London became a magnet for immigrants from the colonies and poorer parts of Europe. A large Irish population settled in the city during the Victorian period, with many of the newcomers refugees from the Great Famine (1845–1849). At one point, Catholic Irish made up about 20% of London's population; they typically lived in overcrowded slums. London also became home to a sizable Jewish community, which was notable for its entrepreneurship in the clothing trade and merchandising.
In 1888, the new County of London was established, administered by the London County Council. This was the first elected London-wide administrative body, replacing the earlier Metropolitan Board of Works, which had been made up of appointees. The County of London covered broadly what was then the full extent of the London conurbation, although the conurbation later outgrew the boundaries of the county. In 1900, the county was sub-divided into 28 metropolitan boroughs, which formed a more local tier of administration than the county council.
Many famous buildings and landmarks of London were constructed during the 19th century including:
20th century.
1900 to World War II.
London entered the 20th century at the height of its influence as the capital of one of the largest empires in history, but the new century was to bring many challenges.
London's population continued to grow rapidly in the early decades of the century, and public transport was greatly expanded. A large tram network was constructed by the London County Council, through the LCC Tramways; the first motorbus service began in the 1900s. Improvements to London's overground and underground rail network, including large scale electrification were progressively carried out.
During World War I, London experienced its first bombing raids carried out by German zeppelin airships; these killed around 700 people and caused great terror, but were merely a foretaste of what was to come. The city of London would experience many more terrors as a result of both World Wars. The largest explosion in London occurred during World War I: the Silvertown explosion, when a munitions factory containing 50 tons of TNT exploded, killing 73 and injuring 400.
The period between the two World Wars saw London's geographical extent growing more quickly than ever before or since. A preference for lower density suburban housing, typically semi-detached, by Londoners seeking a more "rural" lifestyle, superseded Londoners' old predilection for terraced houses. This was facilitated not only by a continuing expansion of the rail network, including trams and the Underground, but also by slowly widening car ownership. London's suburbs expanded outside the boundaries of the County of London, into the neighbouring counties of Essex, Hertfordshire, Kent, Middlesex and Surrey.
Like the rest of the country, London suffered severe unemployment during the Great Depression of the 1930s. In the East End during the 1930s, politically extreme parties of both right and left flourished. The Communist Party of Great Britain and the British Union of Fascists both gained serious support. Clashes between right and left culminated in the Battle of Cable Street in 1936. The population of London reached an all-time peak of 8.6 million in 1939.
Large numbers of Jewish immigrants fleeing from Nazi Germany, settled in London during the 1930s, mostly in the East End.
In World War II.
During World War II, London, as many other British cities, suffered severe damage, being bombed extensively by the "Luftwaffe" as a part of The Blitz. Prior to the bombing, hundreds of thousands of children in London were evacuated to the countryside to avoid the bombing. Civilians took shelter from the air raids in underground stations.
The heaviest bombing took place during The Blitz between 7 September 1940 and 10 May 1941. During this period, London was subjected to 71 separate raids receiving over 18,000 tonnes of high explosive. One raid in December 1940, which became known as the Second Great Fire of London saw a firestorm engulf much of the City of London and destroy many historic buildings. St Paul's Cathedral however remained unscathed; A photograph showing the Cathedral shrouded in smoke became a famous image of the war.
Having failed to defeat Britain, Hitler turned his attention to the Eastern front and regular bombing raids ceased. They began again, but on a smaller scale with the "Little Blitz" in early 1944. Towards the end of the war, during 1944/45 London again came under heavy attack by pilotless V-1 flying bombs and V-2 rockets, which were fired from Nazi occupied Europe. These attacks only came to an end when their launch sites were captured by advancing Allied forces.
London suffered severe damage and heavy casualties, the worst hit part being the Docklands area. By the war's end, just under 30,000 Londoners had been killed by the bombing, and over 50,000 seriously injured, tens of thousands of buildings were destroyed, and hundreds of thousands of people were made homeless.
1945–2000.
Three years after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when the city had barely recovered from the war. London's rebuilding was slow to begin. However, in 1951 the Festival of Britain was held, which marked an increasing mood of optimism and forward looking.
In the immediate postwar years housing was a major issue in London, due to the large amount of housing which had been destroyed in the war. The authorities decided upon high-rise blocks of flats as the answer to housing shortages. During the 1950s and 1960s the skyline of London altered dramatically as tower blocks were erected, although these later proved unpopular. In a bid to reduce the number of people living in overcrowded housing, a policy was introduced of encouraging people to move into newly built new towns surrounding London.
Through the 19th and in the early half of the 20th century, Londoners used coal for heating their homes, which produced large amounts of smoke. In combination with climatic conditions this often caused a characteristic smog, and London became known for its typical "London Fog", also known as "Pea Soupers". London was sometimes referred to as "The Smoke" because of this. In 1952 this culminated in the disastrous Great Smog of 1952 which lasted for five days and killed over 4,000 people. In response to this, the Clean Air Act 1956 was passed, mandating the creating of "smokeless zones" where the use of "smokeless" fuels was required (this was at a time when most households still used open fires); the Act was effective.
Starting in the mid-1960s, and partly as a result of the success of such UK musicians as the Beatles and the Rolling Stones, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture which made Carnaby Street a household name of youth fashion around the world. London's role as a trendsetter for youth fashion was revived strongly in the 1980s during the new wave and punk eras. In the mid-1990s this was revived to some extent with the emergence of the Britpop era.
From the 1950s onwards London became home to a large number of immigrants, largely from Commonwealth countries such as Jamaica, India, Bangladesh, Pakistan, which dramatically changed the face of London, turning it into one of the most diverse cities in Europe. However, the integration of the new immigrants was not always easy. Racial tensions emerged in events such as the Brixton Riots in the early 1980s.
From the beginning of "The Troubles" in Northern Ireland in the early 1970s until the mid-1990s, London was subjected to repeated terrorist attacks by the Provisional IRA.
The outward expansion of London was slowed by the war, and the introduction of the Metropolitan Green Belt. Due to this outward expansion, in 1965 the old County of London (which by now only covered part of the London conurbation) and the London County Council were abolished, and the much larger area of Greater London was established with a new Greater London Council (GLC) to administer it, along with 32 new London boroughs.
Greater London's population declined steadily in the decades after World War II, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. However, it then began to increase again in the late 1980s, encouraged by strong economic performance and an increasingly positive image.
London's traditional status as a major port declined dramatically in the post-war decades as the old Docklands could not accommodate large modern container ships. The principal ports for London moved downstream to the ports of Felixstowe and Tilbury. The docklands area had become largely derelict by the 1980s, but was redeveloped into flats and offices from the mid-1980s onwards. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea.
In the early 1980s political disputes between the GLC run by Ken Livingstone and the Conservative government of Margaret Thatcher led to the GLC's abolition in 1986, with most of its powers relegated to the London boroughs. This left London as the only large metropolis in the world without a central administration.
In 2000, London-wide government was restored, with the creation of the Greater London Authority (GLA) by Tony Blair's government, covering the same area of Greater London. The new authority had similar powers to the old GLC, but was made up of a directly elected Mayor and a London Assembly. The first election took place on 4 May, with Ken Livingstone comfortably regaining his previous post. London was recognised as one of the nine regions of England. In global perspective, it was emerging as a World city widely compared to New York and Tokyo.
21st century.
Around the start of the 21st century, London hosted the much derided Millennium Dome at Greenwich, to mark the new century. Other Millennium projects were more successful. One was the largest observation wheel in the world, the "Millennium Wheel", or the London Eye, which was erected as a temporary structure, but soon became a fixture, and draws four million visitors a year. The National Lottery also released a flood of funds for major enhancements to existing attractions, for example the roofing of the Great Court at the British Museum.
The London Plan, published by the Mayor of London in 2004, estimated that the population would reach 8.1 million by 2016, and continue to rise thereafter. This was reflected in a move towards denser, more urban styles of building, including a greatly increased number of tall buildings, and proposals for major enhancements to the public transport network. However, funding for projects such as Crossrail remained a struggle.
On 6 July 2005 London won the right to host the 2012 Olympics and Paralympics making it the first city to host the modern games three times. However, celebrations were cut short the following day when the city was rocked by a series of terrorist attacks. More than 50 were killed and 750 injured in three bombings on London Underground trains and a fourth on a double decker bus near King's Cross.
In the public there was ambivalence leading-up to the Olympics, though public sentiment changed strongly in their favour following a successful opening ceremony and when the anticipated organisational and transport problems never occurred.

</doc>
<doc id="14021" url="http://en.wikipedia.org/wiki?curid=14021" title="History of astronomy">
History of astronomy

Astronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of pre-history: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy, and not completely disentangled from it until a few centuries ago in the Western World (see astrology and astronomy). In some cultures, astronomical data was used for astrological prognostication.
Ancient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.
Early history.
Early cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first "professional" astronomers were priests, and that their understanding of the "heavens" was seen as "divine", hence astronomy's ancient connection to what is now called astrology. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled both astronomical and religious functions.
Calendars of the world have usually been set by the Sun and Moon (measuring the day, month and year), and were of importance to agricultural societies, in which the harvest depended on planting at the correct time of year. The most common modern calendar is based on the Roman calendar, which divided the year into twelve months of alternating thirty and thirty-one days apiece. In 46 BC, Julius Caesar instigated calendar reform and adopted what's now known as the Julian calendar based upon the 3651⁄4 day year length originally proposed by 4th century BC Greek astronomer Callippus.
Prehistoric Europe.
Since 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy.
Among the discoveries are:
Mesopotamia.
The origins of Western astronomy can be found in Mesopotamia, the "land between the rivers" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, of 60 minutes each, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics.
Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination.
The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the "Enūma Anu Enlil". The oldest significant astronomical text that we possess is Tablet 63 of the "Enūma Anu Enlil", the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.
A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time.
The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the third century BC, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model.
Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.
India.
Some of the earliest forms of ancient Indian astronomy can be dated back to 4,300 BC and to the period of Indus Valley Civilization. Ancient Indian astronomy is based upon sidereal calculation. The sidereal astronomy is based upon the stars and the sidereal period is the time that it takes the object to make one full orbit around the Sun, relative to the stars. Vedanga Jyotisha, attributed to "Lagadha", is considered one of the oldest astronomical texts, dating from 1400–1200 BC (with the extant form possibly from 700–600 BC). Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. After astronomy was influenced by Hellenistic astronomy (adopting the zodiacal signs or "rāśis"). Identical numerical computations for lunar cycles have been found to be used in India and in early Babylonian texts.
Aryabhata (476–550), in his magnum opus "Aryabhatiya" (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II.
Astronomy was advanced during the Sunga Empire and many star catalogues were produced during this time. The Sunga period is known as the "Golden age of astronomy in India".
It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses.
Bhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the "Siddhantasiromani" which consists of two parts: "Goladhyaya" (sphere) and "Grahaganita" (mathematics of the planets). He also calculated the time taken for the Earth to orbit the sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies.
Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his "Aryabhatiyabhasya", a commentary on Aryabhata's "Aryabhatiya", developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more effient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.
Greece and Hellenistic world.
The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis.
A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his "Timaeus", Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century AD.
Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes.
The study of astronomy by the ancient Greeks was not limited to Greece itself but was further developed in the 3rd and 2nd centuries BC, in the Hellenistic states and in particular in Alexandria. However, the work was still done by ethnic Greeks. In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy.
The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century AD, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.
Depending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the "Megale Syntaxis" (Great Synthesis), better known by its Arabic title "Almagest", which had a lasting effect on astronomy up to the Renaissance. In his "Planetary Hypotheses", Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.
Egypt.
The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year.
Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar.
Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites:
And after the Singer advances the Astrologer (ὡροσκόπος), with a "horologium" (ὡρολόγιον) in his hand, and a "palm" (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the sun and moon and five planets; one on the conjunctions and phases of the sun and moon; and one concerns their risings.
The Astrologer's instruments ("horologium" and "palm") are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arms length. The "Hermetic" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.
From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.
China.
The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia.
Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses.
Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose.
Astrological divination was also an important part of astronomy. Astronomers took careful note of "guest stars" which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 A.D. Also, the supernova that created the Crab Nebula in 1054 is an example of a "guest star" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies.
The world's first star catalogue was made by Gan De, a , in the 4th century BC.
Mesoamerica.
Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.
Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.
Medieval Islamic world.
The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories.
In the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his "Book of Fixed Stars". He also gave the first descriptions and pictures of "A Little Cloud" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This "cloud" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star.
In the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian.
Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.
Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century, leading to the development of an astronomical physics.
Medieval Western Europe.
After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.
Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th Century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.
In the 7th Century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the "computus". This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.
The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.
Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.
By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.
In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the earth moves, and "not" the heavens. However, he concluded "everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved." In the 15th century, cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun. He was not, however, describing a scientifically verifiable theory of the universe.
Renaissance Period.
The renaissance came to astronomy with the work of Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His "De revolutionibus" provided a full mathematical discussion of his system, using the geometrical techniques that had been traditional in astronomy since before the time of Ptolemy. His work was later defended, expanded upon and modified by Galileo Galilei and Johannes Kepler.
Galileo was considered the father of observational astronomy. He was among the first to use a telescope to observe the sky and after constructing a 20x refractor telescope he discovered the four largest moons of Jupiter in 1610. This was the first observation of satellites orbiting another planet. He also found that our Moon had craters and observed (and correctly explained) sunspots. Galileo noted that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these observations supported the Copernican system and were, to some extent, incompatible with the favored model of the Earth at the center of the universe. He may have even observed the planet Neptune in 1612 and 1613, over 200 years before it was discovered, but it is unclear if he was aware of what he was looking at.
Uniting physics and astronomy.
Although the motions of celestial bodies had been qualitatively explained in physical terms since Aristotle introduced celestial movers in his Metaphysics and a fifth element in his On the Heavens, Johannes Kepler was the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. Combining his physical insights with the unprecedentedly accurate naked-eye observations made by Tycho Brahe, Kepler discovered the three laws of planetary motion that now carry his name.
Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realising that the same force that attracted objects to the surface of the Earth held the moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiae Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Newton's theoretical developments lay many of the foundations of modern physics.
Completing the solar system.
Outside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibnitz and Cassini accepted only parts of Newton's system, preferring their own philosophies. It wasn't until Voltaire published a popular account in 1738 that the tide changed. In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets towards the end of the century.
Edmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was filled by the discovery of the asteroids Ceres and Pallas in 1801 with many more following.
At first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659.
Modern astronomy.
In the 19th century it was discovered that, when decomposing the light from the Sun, a multitude of spectral lines were observed (regions where there was less or no light). Experiments with hot gases showed that the same lines could be observed in the spectra of gases, specific lines corresponding to unique elements. It was proved that the chemical elements found in the Sun (chiefly hydrogen and helium) were also found on Earth.
During the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, that was necessary to understand the observations.
Although in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human "computers," who performed the tedious calculations while scientists performed research requiring more background knowledge. A number of discoveries in this period were originally noted by the women "computers" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of our solar system. Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, "in only 4 years discovered and catalogued more stars than all the men in history put together."
Most of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.
Cosmology and the expansion of the universe.
Most of our current knowledge was gained during the 20th century. With the help of the use of photography, fainter objects were observed. Our sun was found to be part of a galaxy made up of more than 1010 stars (10 billion stars). The existence of other galaxies, one of the matters of "the great debate", was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy.
Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot big bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.
New windows into the Cosmos open.
In the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to our own sun, but with a range of temperatures, masses and sizes. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us.

</doc>
<doc id="14022" url="http://en.wikipedia.org/wiki?curid=14022" title="Haber process">
Haber process

The Haber process, also called the Haber–Bosch process, is an artificial nitrogen fixation process and is the main industrial procedure for the production of ammonia today. It is named after its inventors, the German chemists Fritz Haber and Carl Bosch, who developed it in the first half of the twentieth century. The process converts atmospheric nitrogen (N2) to ammonia (NH3) by a reaction with hydrogen (H2) using a metal catalyst under high temperatures and pressures:
Before the development of the Haber process, ammonia had been difficult to produce on an industrial scale with early methods such as the Odda process, Birkeland–Eyde process and Frank–Caro process all being highly inefficient.
Although the Haber process is mainly used to produce fertilizer today, during WWI, it provided Germany with a source of ammonia for the production of explosives, compensating for the Allied trade blockade on Chilean saltpeter.
History.
Throughout the nineteenth century the demand for nitrates and ammonia for use as fertilizers and industrial feedstocks had been steadily increasing, however the main source remained the mining of niter deposits. By the start of the twentieth century it was being predicted that these reserves would be unable to satisfy future demand and research into new potential sources of ammonia became ever-more important. The most obvious source was atmospheric nitrogen (N2), which makes up nearly 80% of the air, however N2 is exceptionally stable and will not readily react with other chemicals. Converting N2 into ammonia is therefore exceedingly difficult and posed a chemical challenge which occupied the efforts of chemists across the world.
Haber together with his assistant Robert Le Rossignol developed the high-pressure devices used in the Haber process They demonstrated their process in the summer of 1909 by producing ammonia from air drop by drop, at the rate of about 125 ml per hour. The process was purchased by the German chemical company BASF, which assigned Carl Bosch the task of scaling up Haber's tabletop machine to industrial-level production. He succeeded in this process in 1910. Haber and Bosch were later awarded Nobel prizes, in 1918 and 1931 respectively, for their work in overcoming the chemical and engineering problems posed by the use of large-scale, continuous-flow, high-pressure technology.
Ammonia was first manufactured using the Haber process on an industrial scale in 1913 in BASF's Oppau plant in Germany, production reaching 20 tonnes/day the following year. During World War I, the synthetic ammonia was used for the production of nitric acid, a precursor to munitions. The Allies had access to large amounts of sodium nitrate deposits in Chile (so called "Chile saltpetre") that belonged almost totally to British industries. As Germany lacked access to such readily available natural resources, the Haber process proved important to the German war effort.
The process.
This conversion is typically conducted at 15 – or 20 – and between 400 -, as the gases are passed over four beds of catalyst, with cooling between each pass so as to maintain a reasonable equilibrium constant. On each pass only about 15% conversion occurs, but any unreacted gases are recycled, and eventually an overall conversion of 97% is achieved.
The steam reforming, shift conversion, carbon dioxide removal, and methanation steps each operate at pressures of about 2.5 – or 2.5 –, and the ammonia synthesis loop operates at pressures ranging from 6 – or 6 –, depending upon which proprietary process is used.
Sources of hydrogen.
The major source of hydrogen is methane from natural gas. The conversion, steam reforming, is conducted with air, which is deoxygenated by the combustion of natural gas. Originally Bosch obtained hydrogen by the electrolysis of water.
Reaction rate and equilibrium.
Nitrogen (N2) is very unreactive because the molecules are held together by strong triple bonds. The Haber process relies on catalysts that accelerate the scission of this triple bond.
Two opposing considerations are relevant to this synthesis: the position of the equilibrium and the rate of reaction. At room temperature, the equilibrium is strongly in favor of ammonia, but the reaction doesn't proceed at a detectable rate. The obvious solution is to raise the temperature, but because the reaction is exothermic, the equilibrium constant (using atm units) becomes 1 around 150° or 200 °C. (See Le Chatelier's principle.)
Above this temperature, the equilibrium quickly becomes quite unfavourable at atmospheric pressure, according to the Van 't Hoff equation. Thus one might suppose that a low temperature is to be used and some other means to increase rate. However, the catalyst itself requires a temperature of at least 400 °C to be efficient.
Pressure is the obvious choice to favour the forward reaction because there are 4 moles of reactant for every 2 moles of product (see entropy), and the pressure used (around 200 atm) alters the equilibrium concentrations to give a profitable yield.
Economically, though, pressure is an expensive commodity. Pipes and reaction vessels need to be strengthened, valves more rigorous, and there are safety considerations of working at 200 atm. In addition, running pumps and compressors takes considerable energy. Thus the compromise used gives a single pass yield of around 15%.
Another way to increase the yield of the reaction would be to remove the product (i.e. ammonia gas) from the system. In practice, gaseous ammonia is not removed from the reactor itself, since the temperature is too high; but it is removed from the equilibrium mixture of gases leaving the reaction vessel. The hot gases are cooled enough, whilst maintaining a high pressure, for the ammonia to condense and be removed as liquid. Unreacted hydrogen and nitrogen gases are then returned to the reaction vessel to undergo further reaction.
Catalysts.
The most popular catalysts are based on iron promoted with K2O, CaO, SiO2, and Al2O3. The original Haber–Bosch reaction chambers used osmium as the catalyst, but it was available in extremely small quantities. Haber noted uranium was almost as effective and easier to obtain than osmium. Under Bosch's direction in 1909, the BASF researcher Alwin Mittasch discovered a much less expensive iron-based catalyst, which is still used today. Some ammonia production utilizes ruthenium-based catalysts (the KAAP process). Ruthenium forms more active catalysts that allows milder operating pressures. Such catalysts are prepared by decomposition of triruthenium dodecacarbonyl on graphite.
In industrial practice, the iron catalyst is obtained from finely ground iron powder, which in turn is usually obtained by reduction of high purity magnetite (Fe3O4). The pulverized iron metal is burnt (oxidized) to give magnetite of a defined particle size. The magnetite particles are then partially reduced, removing some of the oxygen in the process. The resulting catalyst particles consist of a core of magnetite, encased in a shell of wüstite (FeO), which in turn is surrounded by an outer shell of iron metal. The catalyst maintains most of its bulk volume during the reduction, resulting in a highly porous high surface area material, which enhances its effectiveness as a catalyst. Other minor components of the catalyst include calcium and aluminium oxides, which support the iron catalyst and help it maintain its surface area. These oxides of Ca, Al, K, and Si are immune to reduction by the hydrogen.
The reaction mechanism, involving the heterogeneous catalyst, is believed to involve the following steps:
Reaction 5 occurs in three steps, forming NH, NH2, and then NH3. Experimental evidence points to reaction 2 as being the slow, rate-determining step. This is not unexpected since the bond broken, the nitrogen triple bond, is the strongest of the bonds that must be broken.
A major contributor to the elucidation of this mechanism is Gerhard Ertl.
Economic and environmental aspects.
The Haber process now produces 450 e6t of nitrogen fertilizer per year, mostly in the form of anhydrous ammonia, ammonium nitrate, and urea. 3–5% of the world's natural gas production is consumed in the Haber process (~1–2% of the world's annual energy supply). In combination with pesticides, these fertilizers have quadrupled the productivity of agricultural land:
Due to its dramatic impact on the human ability to grow food, the Haber process served as the "detonator of the population explosion", enabling the global population to increase from 1.6 billion in 1900 to today's 7 billion. Nearly 80% of the nitrogen found in human tissues originated from the Haber-Bosch process. Since nitrogen use efficiency is typically less than 50%, our heavy use of industrial nitrogen fixation is severely disruptive to our biological habitat.

</doc>
<doc id="14023" url="http://en.wikipedia.org/wiki?curid=14023" title="Hot or Not">
Hot or Not

Hot or Not was a rating site that allowed users to rate the attractiveness of photos submitted voluntarily by others. The site offered a matchmaking engine called 'Meet Me' and an extended profile feature called "Hotlists". The domain hotornot.com is currently owned by Or Not Limited, and was previously owned by Avid Life Media. 'Hot or Not' was a significant influence on the people who went on to create the social media sites Facebook and YouTube.
Description.
Users would submit photographs of themselves to the site for the purpose of other users to rate said person's attractiveness on a scale of 1 - 10, with the cumulative average acting as the overall score for a given photograph.
History.
The site was founded in October 2000 by James Hong and Jim Young, two friends and Silicon Valley-based engineers. Both graduated from the University of California, Berkeley in electrical engineering, with Young pursuing a Ph.D at the time.
The site was a technical solution to a disagreement they made one day over a passing woman's attractiveness. The site was originally called "Am I Hot or Not". Within a week of launching, it had reached almost two million page views per day. Within a few months, the site was immediately behind CNET and NBCi on NetNielsen Rating's Top 25 advertising domains. To keep up with rising costs Hong and Young added a matchmaking component to their website called "Meet Me at Hot or Not", i.e. a system of range voting. The matchmaking service has been especially successful and the site continues to generate most of its revenue through subscriptions. In the December 2006 issue of "Time" magazine, the founders of YouTube stated that they originally set out to make a version of Hot or Not with Video before developing their more inclusive site. Mark Zuckerberg of Facebook similarly got his start by creating a Hot or Not type site called FaceMash, where he posted photos from Harvard's Facebook for the university's community to rate.
Hot or Not was sold for a rumored $20 million on February 8, 2008 to Avid Life Media, owners of Ashley Madison. Annual revenue reached $7.5 million, with net profits of $5.5 million. They initially started off $60,000 in debt due to tuition fees James paid for his MBA. On July 31, 2008, Hot or Not launched Hot or Not Gossip and a Baresi rate box (a "hot meter") – a subdivision to expand their market, run by former radio DJ turned celebrity blogger Zack Taylor.
Predecessors and spin-offs.
Hot or Not was preceded by the rating sites RateMyFace, which was registered a year earlier in the summer of 1999, and AmIHot.com, which was registered in January 2000 by MIT freshman Daniel Roy. Regardless, despite any head starts of its predecessors, Hot or Not quickly became the most popular. Since AmIHotOrNot.com's launch, the concept has spawned many imitators. The concept always remained the same, but the subject matter varied greatly. The concept has also been integrated with a wide variety of dating and matchmaking systems. In 2007 BecauseImHot.com launched and deleted anyone with a rating below 7 after a voting audit or the first 50 votes (whichever is first).
Variations on the Hot or Not concept include voting via a Condorcet method where a candidate is compared with other candidates in a series of pairwise comparisons in order to gauge their popularity. Another variation used a four-way comparison of candidates to gauge their popularity and show a 'type' match for candidates who most closely match the average preferences shown by the user making the choices.
Research.
In 1883, Francis Galton, cousin of Charles Darwin, devised a technique called composite photography, described in detail in "Inquiries in Human Faculty and its Development", which he believed could be used to identify 'types' by appearance, which he hoped would aid medical diagnosis, and even criminology, through the identification of typical criminal faces. In short, he wondered if certain groups of people had certain facial characteristics. To find this answer, he created photographic composite images of the faces of vegetarians and criminals to see if there was a typical facial appearance for each. Galton overlaid multiple images of faces onto a single photographic plate so that each individual face contributed roughly equally to a final composite face. While the resultant “averaged” faces did little to allow the "a priori" identification of either criminals or vegetarians, Galton observed that the composite image was more attractive than the component faces. Similar observations were made in 1886 by J T Stoddard, who created composite faces of members of the National Academy of Sciences and graduating seniors of Smith College. This phenomenon is now known as the averageness-effect, that is that those regarded as highly physically attractive tend to reflect the average traits of the population.
In 2005, as an example of using image morphing methods to study the effects of averageness, imaging researcher Pierre Tourigny created a composite of about 30 faces to find out the current standard of good looks on the Internet (as shown above). On the Hot or Not web site, people rate others' attractiveness on a scale of 1 to 10. An average score based on hundreds or even thousands of individual ratings takes only a few days to emerge. To make this hot or not palette of morphed images, photos from the site were sorted by rank and used SquirlzMorph to create multi-morph composites from them. Unlike projects like Face of Tomorrow, where the subjects are posed for the purpose, the portraits are blurry because the source images are low resolution with differences in posture, hair styles, glasses, etc., so that here images could use only 36 control points for the morphs. A similar study was done with Miss Universe contestants, as shown in the averageness article, as well as one for age, as shown in youthfulness article.
A 2006 "hot" or "not" style study, involving 264 women and 18 men, at the Washington University School of Medicine, as published online in the journal "Brain Research", indicates that a person's brain determines whether an image is erotic long before the viewer is even aware they are seeing the picture. Moreover, according to these researchers, one of the basic functions of the brain is to classify images into a hot or not type categorization. The study's researchers also discovered that sexy shots induce a uniquely powerful reaction in the brain, equal in effect for both men and women, and that erotic images produced a strong reaction in the hypothalamus.
References.
</dl>

</doc>
<doc id="14024" url="http://en.wikipedia.org/wiki?curid=14024" title="H.263">
H.263

H.263 is a video compression standard originally designed as a low-bit-rate compressed format for videoconferencing. It was developed by the ITU-T Video Coding Experts Group (VCEG) in a project ending in 1995/1996 as one member of the H.26x family of video coding standards in the domain of the ITU-T, and it was later extended to add various additional enhanced features in 1998 and 2000. Smaller additions were also made in 1997 and 2001, and a unified edition was produced in 2005.
The H.263 codec was first designed to be utilized in H.324 based systems (PSTN and other circuit-switched network videoconferencing and videotelephony), but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing), RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.
H.263 is a required video codec in ETSI 3GPP technical specifications for IP Multimedia Subsystem (IMS), Multimedia Messaging Service (MMS) and Transparent end-to-end Packet-switched Streaming Service (PSS). In 3GPP specifications, H.263 video is usually used in 3GP container format.
H.263 also found many applications on the internet: much Flash Video content (as used on sites such as YouTube, Google Video, MySpace, etc.) used to be encoded in Sorenson Spark format (an incomplete implementation of H.263). The original version of the RealVideo codec was based on H.263 up until the release of RealVideo 8.
H.263 was developed as an evolutionary improvement based on experience from H.261, the previous ITU-T standard for video compression, and the MPEG-1 and MPEG-2 standards. Its first version was completed in 1995 and provided a suitable replacement for H.261 at all bit rates. It was further enhanced in projects known as H.263v2 (also known as H.263+ or H.263 1998), MPEG-4 Part 2 and H.263v3 (also known as H.263++ or H.263 2000). MPEG-4 Part 2 is H.263 compatible in the sense that basic "baseline" H.263 bitstreams are correctly decoded by an MPEG-4 Video decoder.
The next enhanced codec developed by ITU-T VCEG (in partnership with MPEG) after H.263 was the H.264 standard, also known as AVC and MPEG-4 part 10. As H.264 provides a significant improvement in capability beyond H.263, the H.263 standard is now considered a legacy design. Most new videoconferencing products now include H.264 as well as H.263 and H.261 capabilities. An even-newer standard format, HEVC, has also been developed by VCEG and MPEG, and has begun to emerge in some applications.
Versions.
Since the original ratification of H.263 in March 1996 (approving a document that was produced in November 1995), there have been two subsequent additions which improved on the original codec by additional optional extensions (for example, the H.263v2 project added a deblocking filter in its Annex J).
Version 1 and Annex I.
The original version of H.263 specified the following annexes:
The first version of H.263 supported a limited set of picture sizes:
In March 1997, an informative Appendix I describing Error Tracking – an encoding technique for providing improved robustness to data losses and errors, was approved to provide information for the aid of implementers having an interest in such techniques.
H.263v2 (H.263+).
H.263v2 (also known as "H.263+", or as "the 1998 version of H.263") is the informal name of the second edition of the ITU-T H.263 international video coding standard. It retained the entire technical content of the original version of the standard, but enhanced H.263 capabilities by adding several annexes which can substantially improve encoding efficiency and provide other capabilities (such as enhanced robustness against data loss in the transmission channel). The H.263+ project was ratified by the ITU in February 1998. It added the following Annexes:
H.263v2 also added support for flexible customized picture formats and custom picture clock frequencies. As noted above, the only picture formats previously supported in H.263 had been Sub-QCIF, QCIF, CIF, 4CIF, and 16CIF, and the only picture clock frequency had been 30000/1001 (approximately 29.97) clock ticks per second.
H.263v2 specified a set of recommended modes in an informative appendix (Appendix II, since deprecated):
H.263v3 (H.263++) and Annex X.
The definition of H.263v3 (also known as H.263++ or as the 2000 version of H.263) added three annexes. These annexes and an additional annex that specified profiles (approved the following year) were originally published as separate documents from the main body of the standard itself. The additional annexes specified are:
The prior informative Appendix II (recommended optional enhancement) was obsoleted by the creation of the normative Annex X.
In June 2001, another informative appendix (Appendix III, Examples for H.263 encoder/decoder implementations) was approved. It describes techniques for encoding and for error/loss concealment by decoders.
In January 2005, a unified H.263 specification document was produced (with the exception of Appendix III, which remains as a separately-published document).
In August 2005, an implementors guide was approved to correct a small error in the seldom-used Annex Q reduced-resolution update mode.
Open-source implementation.
In countries without software patents, H.263 video can be legally encoded and decoded with the free LGPL-licensed libavcodec library (part of the FFmpeg project) which is used by programs such as ffdshow, VLC media player and MPlayer.

</doc>
<doc id="14026" url="http://en.wikipedia.org/wiki?curid=14026" title="House of Orange (disambiguation)">
House of Orange (disambiguation)

House of Orange may refer to:

</doc>
<doc id="14029" url="http://en.wikipedia.org/wiki?curid=14029" title="Histone">
Histone

In biology, histones are highly alkaline proteins found in eukaryotic cell nuclei that package and order the DNA into structural units called nucleosomes. They are the chief protein components of chromatin, acting as spools around which DNA winds, and play a role in gene regulation. Without histones, the unwound DNA in chromosomes would be very long (a length to width ratio of more than 10 million to 1 in human DNA). For example, each human cell has about 1.8 meters of DNA, (~6 ft) but wound on the histones it has about 90 micrometers (0.09 mm) of chromatin, which, when duplicated and condensed during mitosis, result in about 120 micrometers of chromosomes.
Classes.
Five major families of histones exist: H1/H5, H2A, H2B, H3 and H4. Histones H2A, H2B, H3 and H4 are known as the core histones, while histones H1 and H5 are known as the linker histones.
Two of each of the core histones assemble to form one octameric nucleosome core, approximately 63 Angstroms in diameter (a solenoid (DNA)-like particle). 147 base pairs of DNA wrap around this core particle 1.65 times in a left-handed super-helical turn to give a particle of around 100 Angstroms across. The linker histone H1 binds the nucleosome at the entry and exit sites of the DNA, thus locking the DNA into place and allowing the formation of higher order structure. The most basic such formation is the 10 nm fiber or beads on a string conformation. This involves the wrapping of DNA around nucleosomes with approximately 50 base pairs of DNA separating each pair of nucleosomes (also referred to as linker DNA). Higher-order structures include the 30 nm fiber (forming an irregular zigzag) and 100 nm fiber, these being the structures found in normal cells. During mitosis and meiosis, the condensed chromosomes are assembled through interactions between nucleosomes and other regulatory proteins.
The following is a list of human histone proteins:
Structure.
The nucleosome core is formed of two H2A-H2B dimers and a H3-H4 tetramer, forming two nearly symmetrical halves by tertiary structure (C2 symmetry; one macromolecule is the mirror image of the other). The H2A-H2B dimers and H3-H4 tetramer also show pseudodyad symmetry. The 4 'core' histones (H2A, H2B, H3 and H4) are relatively similar in structure and are highly conserved through evolution, all featuring a 'helix turn helix turn helix' motif (which allows the easy dimerisation). They also share the feature of long 'tails' on one end of the amino acid structure - this being the location of post-translational modification (see below).
It has been proposed that histone proteins are evolutionarily related to the helical part of the extended AAA+ ATPase domain, the C-domain, and to the N-terminal substrate recognition domain of Clp/Hsp100 proteins. Despite the differences in their topology, these three folds share a homologous helix-strand-helix (HSH) motif.
 
Using an electron paramagnetic resonance spin-labeling technique, British researchers measured the distances between the spools around which eukaryotic cells wind their DNA. They determined the spacings range from 59 to 70 Å.
In all, histones make five types of interactions with DNA:
The highly basic nature of histones, aside from facilitating DNA-histone interactions, contributes to their water solubility.
Histones are subject to post translational modification by enzymes primarily on their N-terminal tails, but also in their globular domains. Such modifications include methylation, citrullination, acetylation, phosphorylation, SUMOylation, ubiquitination, and ADP-ribosylation. This affects their function of gene regulation (see "Function" section).
In general, genes that are active have less bound histone, while inactive genes are highly associated with histones during interphase. It also appears that the structure of histones has been evolutionarily conserved, as any deleterious mutations would be severely maladaptive. All histones have a highly positively charged N-terminus with many lysine and arginine residues.
History.
Histones were discovered in 1884 by Albrecht Kossel. The word "histone" dates from the late 19th century and is from the German word "Histon", a word itself of uncertain origin - perhaps from the Greek "histanai" or "histos". 
Until the early 1990s, histones were dismissed by most as inert packing material for eukaryotic nuclear DNA, a view based in part on the "ball and stick" models of Mark Ptashne and others, who believed that transcription was activated by protein-DNA and protein-protein interactions on largely naked DNA templates, as is the case in bacteria.
During the 1980s, work by Michael Grunstein demonstrated that eukaryotic histones actually repress gene transcription, and that the function of transcriptional activators is to overcome this repression. It is now known that histones play both positive and negative roles in gene expression, forming the basis of the histone code. The work of Vincent Allfrey on histone modification was pioneering and he is regarded as father of epigenetics.
The discovery of the H5 histone appears to date back to the 1970s, and it is now considered an isoform of Histone H1.
Conservation across species.
Histones are found in the nuclei of eukaryotic cells, and in certain Archaea, namely Thermoproteales and Euryarchaea, but not in bacteria. The unicellular algae known as dinoflagellates are the only eukaryotes that are known to completely lack histones.
Archaeal histones may well resemble the evolutionary precursors to eukaryotic histones. Histone proteins are among the most highly conserved proteins in eukaryotes, emphasizing their important role in the biology of the nucleus.:939 In contrast mature sperm cells largely use protamines to package their genomic DNA, most likely because this allows them to achieve an even higher packaging ratio.
Core histones are highly conserved proteins; that is, there are very few differences among the amino acid sequences of the histone proteins of different species. Linker histone usually has more than one form within a species and is also less conserved than the core histones.
There are some "variant" forms in some of the major classes. They share amino acid sequence homology and core structural similarity to a specific class of major histones but also have their own feature that is distinct from the major histones. These "minor histones" usually carry out specific functions of the chromatin metabolism. For example, histone H3-like CenpA is associated with only the centromere region of the chromosome. Histone H2A variant H2A.Z is associated with the promoters of actively transcribed genes and also involved in the prevention of the spread of silent heterochromatin. Furthermore, H2A.Z has roles in chromatin for genome stability. Another H2A variant H2A.X binds to the DNA with double-strand breaks and marks the region undergoing DNA repair. Histone H3.3 is associated with the body of actively transcribed genes.
Function.
Compacting DNA strands.
Histones act as spools around which DNA winds. This enables the compaction necessary to fit the large genomes of eukaryotes inside cell nuclei: the compacted molecule is 40,000 times shorter than an unpacked molecule.
Chromatin regulation.
Histones undergo posttranslational modifications that alter their interaction with DNA and nuclear proteins. The H3 and H4 histones have long tails protruding from the nucleosome, which can be covalently modified at several places. Modifications of the tail include methylation, acetylation, phosphorylation, ubiquitination, SUMOylation, citrullination, and ADP-ribosylation. The core of the histones H2A, H2B, and H3 can also be modified. Combinations of modifications are thought to constitute a code, the so-called "histone code". Histone modifications act in diverse biological processes such as gene regulation, DNA repair, chromosome condensation (mitosis) and spermatogenesis (meiosis).
The common nomenclature of histone modifications is:
So H3K4me1 denotes the monomethylation of the 4th residue (a lysine) from the start (i.e., the N-terminal) of the H3 protein.
Examples of histone modifications in transcription regulation include:
Functions of histone modifications.
A huge catalogue of histone modifications have been described, but a functional understanding of most is still lacking. Collectively, it is thought that histone modifications may underlie a histone code, whereby combinations of histone modifications have specific meanings. However, most functional data concerns individual prominent histone modifications that are biochemically amenable to detailed study.
Chemistry of histone modifications.
Lysine methylation.
The addition of one, two or three methyl groups to lysine has little effect on the chemistry of the histone; methylation leaves the charge of the lysine intact and adds a minimal number of atoms so steric interactions are mostly unaffected. However, proteins containing Tudor, chromo or PHD domains, amongst others, can recognise lysine methylation with exquisite sensitivity and differentiate mono, di and tri-methyl lysine, to the extent that, for some lysines (e.g.: H4K20) mono, di and tri-methylation appear to have different meanings. Because of this, lysine methylation tends to be a very informative mark and dominates the known histone modification functions.
Arginine methylation.
What was said above of the chemistry of lysine methylation also applies to arginine methylation, and some protein domains—e.g., Tudor domains—can be specific for methyl arginine instead of methyl lysine. Arginine is known to be mono- or di-methylated, and methylation can be symmetric or asymmetric, potentially with different meanings.
Lysine acetylation.
Addition of an acetyl group has a major chemical effect on lysine as it neutralises the positive charge. This reduces electrostatic attraction between the histone and the negatively charged DNA backbone, loosening the chromatin structure; highly acetylated histones form more accessible chromatin and tend to be associated with active transcription. Lysine acetylation appears to be less precise in meaning than methylation, in that histone acetyltransferases tend to act on more than one lysine; presumably this reflects the need to alter multiple lysines to have a significant effect on chromatin structure.
Serine/Threonine/Tyrosine phosphorylation.
Addition of a negatively charged phosphate group can lead to major changes in protein structure, leading to the well-characterised role of phosphorylation in controlling protein function. It is not clear what structural implications histone phosphorylation has, but histone phosphorylation has clear functions as a post-translational modification, and binding domains such as BRCT have been characterised.
Functions in transcription.
Most well-studied histone modifications are involved in control of transcription.
Actively transcribed genes.
Two histone modifications are particularly associated with active transcription:
H3K4 trimethylation is performed by the COMPASS complex. Despite the conservation of this complex and histone modification from yeast to mammals, it is not entirely clear what role this modification plays. However, it is an excellent mark of active promoters and the level of this histone modification at a gene’s promoter is broadly correlated with transcriptional activity of the gene. The formation of this mark is tied to transcription in a rather convoluted manner: early in transcription of a gene, RNA polymerase II undergoes a switch from initiating’ to ‘elongating’, marked by a change in the phosphorylation states of the RNA polymerase II C terminal domain (CTD). The same enzyme that phosphorylates the CTD also phosphorylates the Rad6 complex, which in turn adds a ubiquitin mark to H2B K123 (K120 in mammals). H2BK123Ub occurs throughout transcribed regions, but this mark is required for COMPASS to trimethylate H3K4 at promoters.
H3K36 trimethylation is deposited by the methyltransferase Set2. This protein associates with elongating RNA polymerase II, and H3K36Me3 is indicative of actively transcribed genes. H3K36Me3 is recognised by the Rpd3 histone deacetylase complex, which removes acetyl modifications from surrounding histones, increasing chromatin compaction and repressing spurious transcription. Increased chromatin compaction prevents transcription factors from accessing DNA, and reduces the likelihood of new transcription events being initiated within the body of the gene. This process therefore helps ensure that transcription is not interrupted.
Repressed genes.
Three histone modifications are particularly associated with repressed genes:
This histone modification is depositied by the polycomb complex PRC2. It is a clear marker of gene repression, and is likely bound by other proteins to exert a repressive function. Another polycomb complex, PRC1, can bind H3K27Me3 and adds the histone modification H2AK119Ub which aids chromatin compaction. Based on this data it appears that PRC1 is recruited through the action of PRC2, however, recent studies show that PRC1 is recruited to the same sites in the absence of PRC2.
H3K9Me2/3 is a well-characterised marker for heterochromatin, and is therefore strongly associated with gene repression. The formation of heterochromatin has been best studied in the yeast "Schizosaccharomyces pombe", where it is initiated by recruitment of the RNA-induced transcriptional silencing complex to double stranded RNAs produced from centromeric repeats. RITS recruits the Clr4 histone methyltransferase which deposits H3K9Me2/3. This process is called histone methylation. H3K9Me2/3 serves as a binding site for the recruitment of Swi6 (heterochromatin protein 1 or HP1, another classic heterochromatin marker) which in turn recruits further repressive activities including histone modifiers such as histone deacetylases and histone methyltransferases.
This modification is tightly associated with heterochromatin, although its functional importance remains unclear. This mark is placed by the Suv4-20h methyltransferase, which is at least in part recruited by heterochromatin protein 1.
Bivalent promoters.
Analysis of histone modifications in embryonic stem cells (and other stem cells) revealed many gene promoters carrying both H3K4Me3 and H3K27Me3, in other words these promoters display both activating and repressing marks simultaneously. This peculiar combination of modifications marks genes that are poised for transcription; they are not required in stem cells, but are rapidly required after differentiation into some lineages. Once the cell starts to differentiate, these bivalent promoters are resolved to either active or repressive states depending on the chosen lineage.
Other functions.
DNA damage.
Marking sites of DNA damage is an important function for histone modifications.
Phosphorylated H2AX (also known as gamma H2AX) is a marker for DNA double strand breaks, and forms part of the response to DNA damage. H2AX is phosphorylated early after detection of DNA double strand break, and forms a domain extending many kilobases either side of the damage. Gamma H2AX acts as a binding site for the protein MDC1, which in turn recruits key DNA repair proteins (this complex topic is well reviewed in) and as such, gamma H2AX forms a vital part of the machinery that ensures genome stability.
H3K56Acx is required for genome stability. H3K56 is acetylated by the p300/Rtt109 complex, but is rapidly deacetylated around sites of DNA damage. H3K56 acetylation is also required to stabilise stalled replication forks, preventing dangerous replication fork collapses. Although in general mammals make far greater use of histone modifications than microorganisms, a major role of H3K56Ac in DNA replication exists only in fungi, and this has become a target for antibiotic development.
Chromosome condensation
The mitotic kinase aurora B phosphorylates histone H3 at serine 10, triggering a cascade of changes that mediate mitotic chromosome condensation. Condensed chromosomes therefore stain very strongly for this mark, but H3S10 phosphorylation is also present at certain chromosome sites outside mitosis, for example in pericentric heterochromatin of cells during G2. H3S10 phosphorylation has also been linked to DNA damage caused by R loop formation at highly transcribed sites.
Phosphorylation H2B at serine 10 in yeast or serine 14 in mammalian cells (phospho-H2BS10/14)
Phosphorylation of H2B at serine 10 (yeast) or serine 14 (mammals) is also linked to chromatin condensation, but for the very different purpose of mediating chromosome condensation during apoptosis. This mark is not simply a late acting bystander in apoptosis as yeast carrying mutations of this residue are resistant to hydrogen peroxide-induced apoptotic cell death.

</doc>
<doc id="14031" url="http://en.wikipedia.org/wiki?curid=14031" title="Hierarchical organization">
Hierarchical organization

A hierarchical organization is an organizational structure where every entity in the organization, except one, is subordinate to a single other entity. This arrangement is a form of a hierarchy. In an organization, the hierarchy usually consists of a singular/group of power at the top with subsequent levels of power beneath them. This is the dominant mode of organization among large organizations; most corporations, governments, and organized religions are hierarchical organizations with different levels of management, power or authority. For example, the broad, top-level overview of the general organization of the Catholic Church consists of the Pope, then the Cardinals, then the Archbishops, and so on. 
Members of hierarchical organizational structures chiefly communicate with their immediate superior and with their immediate subordinates. Structuring organizations in this way is useful partly because it can reduce the communication overhead by limiting information flow; this is also its major limitation.
Visualization.
A hierarchy is typically visualized as a pyramid, where the height of the ranking or person depicts their power status and the width of that level represents how many people or business divisions are at that level relative to the whole—the highest-ranking people are at the apex, and there are very few of them; the base may include thousands of people who have no subordinates. These hierarchies are typically depicted with a tree or triangle diagram, creating an organizational chart or organigram. Those nearest the top have more power than those nearest the bottom, and there being fewer people at the top than at the bottom. As a result, superiors in a hierarchy generally have higher status and command greater rewards than their subordinates.
Common models.
All governments and most companies have similar structures. Traditionally, the monarch was the pinnacle of the state. In many countries, feudalism and manorialism provided a formal social structure that established hierarchical links at every level of society, with the monarch at the top. 
In modern post-feudal states the nominal top of the hierarchy still remains the head of state, which may be a president or a constitutional monarch, although in many modern states the powers of the head of state are delegated among different bodies. Below the head, there is commonly a senate, parliament or congress, which in turn often delegate the day-to-day running of the country to a prime minister. In many democracies, the people are considered to be the notional top of the hierarchy, over the head of state; in reality, the people's power is restricted to voting in elections.
In business, the business owner traditionally occupied the pinnacle of the organization. In most modern large companies, there is now no longer a single dominant shareholder, and the collective power of the business owners is for most purposes delegated to a board of directors, which in turn delegates the day-to-day running of the company to a managing director or CEO. Again, although the shareholders of the company are the nominal top of the hierarchy, in reality many companies are run at least in part as personal fiefdoms by their management; corporate governance rules are an attempt to mitigate this tendency.
Studies of hierarchical organizations.
The organizational development theorist Elliott Jacques identified a special role for hierarchy in his concept of requisite organization. 
The iron law of oligarchy, introduced by Robert Michels, describes the inevitable tendency of hierarchical organizations to become oligarchic in their decision making.
Hierarchiology is the term coined by Dr. Laurence J. Peter, originator of the Peter Principle described in his humorous book of the same name, to refer to the study of hierarchical organizations and the behavior of their members.
Having formulated the Principle, I discovered that I had inadvertently founded a new science, hierarchiology, the study of hierarchies. The term hierarchy was originally used to describe the system of church government by priests graded into ranks. The contemporary meaning includes any organization whose members or employees are arranged in order of rank, grade or class. Hierarchiology, although a relatively recent discipline, appears to have great applicability to the fields of public and private administration.—Dr. Laurence J. Peter and Raymond Hull, "The Peter Principle: Why Things Always Go Wrong"
The IRG Solution – hierarchical incompetence and how to overcome it argued that hierarchies were inherently incompetent, and were only able to function due to large amounts of informal lateral communication fostered by private informal networks.
Criticism and alternatives.
In the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work "Radical Empiricism" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved. A hesitation to declare success upon the discovery of ambiguities leaves heterarchy at an artificial and subjective disadvantage in the scope of human knowledge. This bias is an artifact of an aesthetic or pedagogical preference for hierarchy, and not necessarily an expression of objective observation.
Hierarchies and hierarchical thinking has been criticized by many people, including Susan McClary and one political philosophy which is vehemently opposed to hierarchical organization: anarchism is generally opposed to hierarchical organization in any form of human relations. Heterarchy is the most commonly proposed alternative to hierarchy and this has been combined with responsible autonomy by Gerard Fairtlough in his work on Triarchy theory.
Amidst constant innovation in information and communication technologies, hierarchical authority structures are giving way to greater decision-making latitude for individuals and more flexible definitions of job activities and this new style of work presents a challenge to existing organizational forms, with some research studies contrasting traditional organizational forms against groups that operate as online communities that are characterized by personal motivation and the satisfaction of making one's own decisions. With all levels of an organization having access to information and communication via digital means, power structures align more as a wirearchy, enabling the flow of power and authority to be based not on hierarchical levels, but on information, trust, credibility, and a focus on results.

</doc>
<doc id="14033" url="http://en.wikipedia.org/wiki?curid=14033" title="Harry Secombe">
Harry Secombe

Sir Harry Donald Secombe, CBE (8 September 1921 – 11 April 2001) was a Welsh comedian and singer. He played Neddie Seagoon, a central character in the BBC radio comedy series "The Goon Show" (1951–60). He also appeared in musicals and films and, in his later years, was a presenter of television shows incorporating hymns and other devotional songs.
Early life.
Secombe was born in rooms in the Danygraig Area of St. Thomas. Later the family moved to a council house in the St Thomas district of Swansea, Secombe being the third of four children of Nellie Jane Gladys (née Davies), a shop manageress, and Frederick Ernest Secombe, a grocer. From the age of 11 he attended Dynevor School, a state secondary school in central Swansea.
His family were regular churchgoers, belonging to the congregation of St Thomas Church. A member of the choir, Secombe would – from the age of 12 – perform a sketch entitled "The Welsh Courtship" at church socials, acting as "feed" to his sister Carol. His elder brother, Fred Secombe, was the author of several books about his experiences as an Anglican priest and rector.
British Army.
After leaving school in 1937, Secombe became a pay clerk at Baldwin's store. With war looming, he decided in 1938 that he would join the Territorial Army. Very short sighted, he got a friend to tell him the sight test, and then learnt it by heart. He served as a Lance Bombardier in No.132 Field Regiment of the Royal Artillery. He would refer to the unit in which he served during World War II in the North African Campaign, Sicily, and Italy, as "The Five-Mile Snipers". While in North Africa Secombe met Spike Milligan for the first time. In Sicily he joined a concert party and developed his own comedy routines to entertain the troops.
When Secombe visited the Falklands to entertain the troops after the 1982 war in the islands, his old regiment promoted him to the rank of sergeant – 37 years after he had been demobbed.
As an entertainer.
He made his first radio broadcast in May 1944 on a variety show aimed at the services. Following the end of fighting in the war but prior to demobilisation Secombe joined a pool of entertainers in Naples and formed a comedy duo with Spike Milligan.
Secombe joined the cast of the Windmill Theatre in 1946, using a routine he had developed in Italy about how people shaved. Secombe always claimed that his ability to sing could always be counted on to save him when he bombed. Both Milligan and Sellers credited him with keeping the act on the bill when club owners had wanted to sack them.
After a regional touring career, his first break came in radio when he was chosen as resident comedian for the Welsh series "Welsh Rarebit," followed by appearances on "Variety Bandbox" and a regular role in "Educating Archie".
Secombe met Michael Bentine at the Windmill Theatre, and was introduced to Peter Sellers by his agent Jimmy Grafton. Together with Spike Milligan, the four wrote a comedy radio script, and "Those Crazy People" was commissioned and first broadcast on 28 May 1951. Produced by Peter Ross, this would soon become "The Goon Show" and the show remained on the air until 1960. Secombe mainly played Neddie Seagoon around whom the show's absurd plots developed.
With the success of "The Goon Show", Secombe developed a dual career as both a comedy actor and a singer. At the beginning of his career as an entertainer, his act would end with a joke version of the duet "Sweethearts," in which he sang both the baritone and falsetto parts. Trained under Italian maestro Manlio di Veroli, he emerged as a "bel canto" tenor (characteristically, he insisted that in his case this meant "can belto") and had a long list of best-selling record albums to his credit.
In 1958 he appeared in the film "Jet Storm," which starred Dame Sybil Thorndike and Richard Attenborough and in the same year Secombe starred in the title role in "Davy", one of Ealing Studios' last films.
The power of his voice allowed Secombe to appear in many stage musicals. This included 1963's "Pickwick," based on Dickens' The Pickwick Papers, which gave him the number eighteen hit single "If I Ruled the World" – his later signature tune. In 1965 the show was produced on tour in the United States, where on Broadway he garnered a nomination for a Tony Award for Best Actor in a Musical. He also appeared in the musical "The Four Musketeers" (1967), as Mr. Bumble in Carol Reed's film of "Oliver!" (1968), and in the Envy segment of "The Magnificent Seven Deadly Sins" (1971).
He would go on to star in his own television show, "The Harry Secombe Show", which debuted on Christmas Day 1968 on BBC 1 and ran for thirty one episodes until 1973. A sketch comedy show featuring Julian Orchard as Secombe's regular sidekick, the series also featured guest appearances by fellow Goon Spike Milligan as well as leading performers such as Ronnie Barker and Arthur Lowe. Secombe later starred in similar vehicles such as "Sing a Song of Secombe" and ITV's "Secombe with Music" during the 1970s.
Later career.
Later in life, Secombe (whose brother Fred Secombe was a priest in the Church in Wales, part of the Anglican Communion) attracted new audiences as a presenter of religious programmes, such as the BBC's "Songs of Praise" and ITV's "Stars on Sunday" and "Highway". He was also a special programming consultant to Harlech Television. and hosted a Thames Television programme in 1979 entitled "Cross on the Donkey's Back". In the latter half of the 1980s, Secombe personally sponsored a football team for boys aged 9–11 in the local West Sutton Little League, 'Secombes Knights'.
In 1990, he was one of a few to be honoured by a second appearance on "This Is Your Life", when he was surprised by Michael Aspel at a book signing in a London branch of WH Smith. Secombe had been a subject of the show previously in March 1958 when Eamonn Andrews surprised him at the BBC Television Theatre.
Honours.
In 1963 he was appointed a Commander of the Order of the British Empire (CBE).
He was knighted in 1981, and jokingly referred to himself as Sir Cumference (in recognition of his rotund figure). The motto he chose for his coat of arms was "GO ON", a reference to goon.
Later life and death.
Secombe suffered from peritonitis in 1980. He had a stroke in 1997, from which he made a slow recovery. He was then diagnosed with prostate cancer in September 1998. After suffering a second stroke in 1999, he was forced to abandon his television career, but made a documentary about his condition in the hope of giving encouragement to other sufferers. Secombe had diabetes in the latter part of his life.
Secombe died on 11 April 2001 at the age of 79, from prostate cancer, in hospital in Guildford, Surrey. His ashes are interred at the parish church of Shamley Green, and a later memorial service to celebrate his life was held at Westminster Abbey on 26 October 2001. As well as family members and friends, the service was also attended by Charles, Prince of Wales and representatives of Prince Philip, Duke of Edinburgh, Anne, Princess Royal, Princess Margaret, Countess of Snowdon and Prince Edward, Duke of Kent. On his tombstone is the inscription: "To know him was to love him."
Upon hearing of his old friend's death, Spike Milligan quipped, "I'm glad he died before me, because I didn't want him to sing at my funeral." But Secombe would have the last laugh: upon Milligan's own death the following year, a recording of Secombe singing was played at Spike's memorial service.
The Secombe Theatre at Sutton, London, bears his name in memory of this former local personality. He is also fondly remembered at the London Welsh Centre, where he opened the bar on St Patrick's Day (17 March) 1971.
Family.
Secombe met Myra Atherton at the Mumbles dance hall. The couple were married from 1948 until his death, and had four children:

</doc>
<doc id="14034" url="http://en.wikipedia.org/wiki?curid=14034" title="Heroin">
Heroin

Heroin (diacetylmorphine or morphine diacetate, also known as diamorphine (BAN, INN)) and commonly known by its street names of H, smack, boy, horse, brown, black, tar, and others is an opioid analgesic originally synthesized by C.R. Alder Wright in 1874 by adding two acetyl groups to the molecule morphine, which is found naturally in the opium poppy. It is the 3,6-diacetyl ester of morphine. Administered intravenously by injection, heroin is two to four times more potent than morphine and is faster in its onset of action.
Illicit heroin is sometimes available in freebase form, dulling the sheen and consistency to a matte-white powder. Because of its lower boiling point, the freebase form of heroin is also smokable. It is prevalent in heroin coming from Afghanistan, which in 2004 produced roughly 87% of the world supply in illicit raw opium. However, the production rate in Mexico has risen sixfold from 2007 to 2011, changing that percentage and placing Mexico as the second largest opium producer in the world.
As with other opioids, diacetylmorphine is used as both a legal, medically prescribed drug (e.g., as an analgesic, cough suppressant and as an anti-diarrhea drug) and a recreational drug, in which case the user is seeking euphoria. Frequent and regular administration is associated with tolerance and physical dependence. Internationally, diacetylmorphine is controlled under Schedules I and IV of the Single Convention on Narcotic Drugs. It is illegal to manufacture, possess, or sell diacetylmorphine without a license.
It is also available for prescription to long-term users as a form of opioid replacement therapy in the United Kingdom, Netherlands, Switzerland, Germany, and Denmark, alongside psycho-social care—in the same manner that methadone or buprenorphine are used in the United States and Canada—and a similar programme is being campaigned for by liberal political parties in Norway.
Usage.
The United Nations estimated in 2005 that there were over 50 million people worldwide who regularly used cocaine, heroin, and synthetic drugs.
Medical use.
Under the chemical name diamorphine, diacetylmorphine is prescribed as a strong analgesic in the United Kingdom, where it is given via subcutaneous, intramuscular, intrathecal or intravenous route. Its use includes treatment for acute pain, such as in severe physical trauma, myocardial infarction, post-surgical pain, and chronic pain, including end-stage cancer and other terminal illnesses. In other countries it is more common to use morphine or other strong opioids in these situations. In 2004, the National Institute for Health and Clinical Excellence, a non-departmental public body of the Department of Health in the United Kingdom, produced guidance on the management of caesarian section, which recommended the use of intrathecal or epidural diacetylmorphine for post-operative pain relief.
In 2005, there was a shortage of diacetylmorphine in the UK, because of a problem at the main UK manufacturers. Because of this, many hospitals changed to using morphine instead of diacetylmorphine. Although there is no longer a problem with the manufacturing of diacetylmorphine in the UK, some hospitals there have continued to use morphine. The majority, however, continue to use diacetylmorphine, and diacetylmorphine tablets are supplied for pain management.
Diacetylmorphine continues to be widely used in palliative care in the UK, where it is commonly given by the subcutaneous route, often via a syringe driver, if patients cannot easily swallow oral morphine solution. The advantage of diacetylmorphine over morphine is that diacetylmorphine is more fat soluble and therefore more potent by injection, so smaller doses of it are needed for the same analgesic effect. Both of these factors are advantageous if giving high doses of opioids via the subcutaneous route, which is often necessary in palliative care.
The medical use of diacetylmorphine, in common with other strong opioids such as morphine, fentanyl and oxycodone, is controlled in the UK by the Misuse of Drugs Act 1971. In the UK, it is a class A controlled drug and as such is subject to guidelines surrounding its storage, administration and destruction. Possession of diamorphine without a prescription is an arrestable offence. When diamorphine is prescribed in a hospital or similar environment, its administration must be supervised by two people who must then complete and sign a controlled drugs register (CD register) detailing the patient's name, amount, time, date and route of administration. In the case of a physician administering diamorphine, then he/she may administer the drug alone, however the rule requiring two registered practitioners, such as a nurse, midwife or another physician to sign the CD register still applies. The use of a witness when administering diamorphine is to avoid the possibility of the drug being diverted onto the black market.
For safety reasons, many UK National Health Service hospitals now only permit the administration of intravenous diamorphine in designated areas. In practice this usually means a critical care unit, an accident and emergency department, operating theatres by an anaesthetist or nurse anaesthetist or other such areas where close monitoring and support from senior staff is immediately available. However, administration by other routes is permitted in other areas of the hospital. This includes subcutaneous, intramuscular, intravenously as part of a patient controlled analgesia setup, and as an already established epidural infusion pump. Subcutaneous infusion, along with subcutaneous and intramuscular injection (bolus administration), is often used in the patient's own home, in order to treat severe pain in terminal illness.
Diacetylmorphine is also used as a maintenance drug to treat certain groups of addicts, normally long-term chronic intravenous (IV) heroin users, and even in these situations it is only prescribed following exhaustive efforts at treatment via other means. It is thought that heroin users can walk into a clinic and walk out with a prescription, but the process takes many weeks before a prescription for diacetylmorphine is issued. Though this is somewhat controversial among proponents of a zero-tolerance drug policy, it has proven superior to methadone in improving the social and health situation of addicts. See: Heroin prescription for addicts
Recreational use.
Diacetylmorphine, almost always still called by its original trade name of heroin in non-medical settings, is used as a recreational drug for the intense euphoria it induces. Anthropologist Michael Agar once described heroin as "the perfect whatever drug." Tolerance develops quickly, and increased doses are needed in order to achieve the same effects. Its popularity with recreational drug users, compared to morphine, reportedly stems from its perceived different effects. In particular, users report an intense rush, an acute transcendent state of euphoria, which occurs while diacetylmorphine is being metabolized into 6-monoacetylmorphine (6-MAM) and morphine in the brain. Some believe that heroin produces more euphoria than other opioids upon injection; one possible explanation is the presence of 6-monoacetylmorphine, a metabolite unique to heroin – although a more likely explanation is the rapidity of onset. While other opioids of recreational use produce only morphine, heroin also leaves 6-MAM, also a psycho-active metabolite. However, this perception is not supported by the results of clinical studies comparing the physiological and subjective effects of injected heroin and morphine in individuals formerly addicted to opioids; these subjects showed no preference for one drug over the other. Equipotent injected doses had comparable action courses, with no difference in subjects' self-rated feelings of euphoria, ambition, nervousness, relaxation, drowsiness, or sleepiness.
Short-term addiction studies by the same researchers demonstrated that tolerance developed at a similar rate to both heroin and morphine. When compared to the opioids hydromorphone, fentanyl, oxycodone, and pethidine/meperidine, former addicts showed a strong preference for heroin and morphine, suggesting that heroin and morphine are particularly susceptible to abuse and addiction. Morphine and heroin were also much more likely to produce euphoria and other positive subjective effects when compared to these other opioids.
Some researchers have attempted to explain heroin use and the culture that surrounds it through the use of sociological theories. In "Righteous Dopefiend", Philippe Bourgois and Jeff Schonberg use anomie theory to explain why people begin using heroin. By analyzing a community in San Francisco, they demonstrated that heroin use was caused in part by internal and external factors such as violent homes and parental neglect. This lack of emotional, social, and financial support causes strain and influences individuals to engage in deviant acts, including heroin usage. They further found that heroin users practiced "retreatism", a behavior first described by Howard Abadinsky, in which those suffering from such strain reject society's goals and institutionalized means of achieving them.
Prescription for addicts.
The UK Department of Health's Rolleston Committee Report in 1926 established the British approach to diacetylmorphine prescription to users, which was maintained for the next 40 years: dealers were prosecuted, but doctors could prescribe diacetylmorphine to users when withdrawing from it would cause harm or severe distress to the patient. This "policing and prescribing" policy effectively controlled the perceived diacetylmorphine problem in the UK until 1959 when the number of diacetylmorphine addicts doubled every 16 months during a period of ten years, 1959–68. In 1964, the Brain Committee recommended that only selected approved doctors working at approved specialised centres be allowed to prescribe diacetylmorphine and benzoylmethylecgonine (cocaine) to users. The law was made more restrictive in 1968. Beginning in the 1970s, the emphasis shifted to abstinence and the use of methadone; until now only a small number of users in the UK are prescribed diacetylmorphine.
In 1994, Switzerland began a trial diamorphine maintenance program for users that had failed multiple withdrawal programs. The aim of this program was to maintain the health of the user by avoiding medical problems stemming from the illicit use of diacetylmorphine. The first trial in 1994 involved 340 users, although enrollment was later expanded to 1000 based on the apparent success of the program.
The trials proved diamorphine maintenance to be superior to other forms of treatment in improving the social and health situation for this group of patients. It has also been shown to save money, despite high treatment expenses, as it significantly reduces costs incurred by trials, incarceration, health interventions and delinquency. Patients appear twice daily at a treatment center, where they inject their dose of diamorphine under the supervision of medical staff. They are required to contribute about 450 Swiss francs per month to the treatment costs. A national referendum in November 2008 showed 68% of voters supported the plan, introducing diacetylmorphine prescription into federal law. The trials before were based on time-limited executive ordinances. The success of the Swiss trials led German, Dutch, and Canadian cities to try out their own diamorphine prescription programs. Some Australian cities (such as Sydney) have instituted legal diacetylmorphine supervised injecting centers, in line with other wider harm minimization programs.
Since January 2009, Denmark has prescribed diamorphine to a few addicts that have tried methadone and subutex without success. Beginning in February 2010, addicts in Copenhagen and Odense will be eligible to receive free diacetylmorphine. Later in 2010 other cities including Århus and Esbjerg will join the scheme. In total, around 230 addicts will be able to receive free diacetylmorphine.
However, Danish addicts will only be able to inject heroin according to the policy set by Danish National Board of Health. Of the estimated 1500 drug users who do not benefit from the current oral substitution treatment, approximately 900 will not be in the target group for treatment with injectable diacetylmorphine, either because of "massive multiple drug abuse of non-opioids" or "not wanting treatment with injectable diacetylmorphine".
In July 2009, the German Bundestag passed a law allowing diacetylmorphine prescription as a standard treatment for addicts; a large-scale trial of diacetylmorphine prescription had been authorized in that country in 2002.
Detection in biological fluids.
The major metabolites of diacetylmorphine, 6-MAM, morphine, morphine-3-glucuronide and morphine-6-glucuronide, may be quantitated in blood, plasma or urine to monitor for abuse, confirm a diagnosis of poisoning or assist in a medicolegal death investigation. Most commercial opiate screening tests cross-react appreciably with these metabolites, as well as with other biotransformation products likely to be present following usage of street-grade diacetylmorphine such as 6-acetylcodeine and codeine. However, chromatographic techniques can easily distinguish and measure each of these substances. When interpreting the results of a test, it is important to consider the diacetylmorphine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate an opiate-naive individual, and the chronic user often has high baseline values of these metabolites in his system. Furthermore, some testing procedures employ a hydrolysis step before quantitation that converts many of the metabolic products to morphine, yielding a result that may be 2 times larger than with a method that examines each product individually.
Adverse effects.
Like most opioids, unadulterated heroin does not cause many long-term complications other than dependence and constipation. The average purity of street heroin in the UK varies between 30% and 50% and heroin that has been seized at the border has purity levels between 40% and 60%; this variation has led to people suffering from overdoses as a result of the heroin missing a stage on its journey from port to end user, as each set of hands that the drug passes through adds further adulterants, the strength of the drug reduces, with the effect that if steps are missed, the purity of the drug reaching the end user is higher than they are used to. Intravenous use of heroin (and any other substance) with non-sterile needles and syringes or other related equipment may lead to:
Many countries and local governments have begun funding programs that supply sterile needles to people who inject illegal drugs in an attempt to reduce these contingent risks, and especially the spread of blood-borne diseases. The Drug Policy Alliance reports that up to 75% of new AIDS cases among women and children are directly or indirectly a consequence of drug use by injection. The United States federal government does not operate needle exchanges, although some state and local governments do support such programs.
Anthropologists Philippe Bourgois and Jeff Schonberg performed a decade of fieldwork among homeless heroin and cocaine addicts in San Francisco, published in 2009. They reported that the African-American addicts they observed were more inclined to "direct deposit" heroin into a vein, while "skin-popping" was a far more widespread practice: "By the midpoint of our fieldwork, most of the whites had given up searching for operable veins and skin-popped. They sank their needles perfunctorily, often through their clothing, into their fatty tissue." Bourgois and Schonberg describes how the cultural difference between the African-Americans and the whites leads to this contrasting behavior, and also points out that the two different ways to inject heroin comes with different health risks. Skin-popping more often results in abscesses, and direct injection more often leads to fatal overdose and also to hepatitis C and HIV infection.
Heroin overdose is usually treated with an opioid antagonist, such as naloxone (Narcan), or naltrexone, which has high affinity for opioid receptors but does not activate them. This reverses the effects of heroin and other opioid agonists and causes an immediate return of consciousness but may precipitate withdrawal symptoms. The half-life of naloxone is much shorter than that of most opioid agonists, so that antagonist typically has to be administered multiple times until the opioid has been metabolized by the body.
Depending on drug interactions and numerous other factors, death from overdose can take anywhere from several minutes to several hours because of anoxia resulting from the breathing reflex being suppressed by agonism of µ-opioid receptors. An overdose is immediately reversible with an opioid antagonist injection. Heroin overdoses can occur because of an unexpected increase in the dose or purity or because of diminished opioid tolerance. However, many fatalities reported as overdoses are probably caused by interactions with other depressant drugs such as alcohol or benzodiazepines. It should also be noted that since heroin can cause nausea and vomiting, a significant number of deaths attributed to heroin overdose are caused by aspiration of vomit by an unconscious victim. Some sources quote the median lethal dose (for an average 75 kg opiate-naive individual) as being between 75 and 600  mg. Experiments on simians have demonstrated therapeutic indexes up to 50. Illicit heroin is of widely varying and unpredictable purity. This means that the user may prepare what they consider to be a moderate dose while actually taking far more than intended. Also, tolerance typically decreases after a period of abstinence. If this occurs and the user takes a dose comparable to their previous use, the user may experience drug effects that are much greater than expected, potentially resulting in a dangerous overdose. It has been speculated that an unknown portion of heroin-related deaths are the result of an overdose or allergic reaction to quinine, which may sometimes be used as a cutting agent.
A final factor contributing to overdoses is environment-conditioned tolerance, in which the body associates the environment in which the drug is used with the drug itself. While the mechanism is not yet clearly understood, long-term heroin users display increased tolerance to the drug in locations where they have repeatedly administered it. When the drug is used in a different location, this environment-conditioned tolerance does not occur, resulting in a greater drug effect. Thus, a dose administered in an unfamiliar environment may be too high and potentially fatal. This tendency toward conditioning is exacerbated by the fact that heroin use is a highly ritualized behavior, which means that there are many cues for conditioning.
A small percentage of heroin smokers, and occasionally IV users, may develop symptoms of toxic leukoencephalopathy. The cause has yet to be identified, but one speculation is that the disorder is caused by an uncommon adulterant that is only active when heated. Symptoms include slurred speech and difficulty walking.
Cocaine is sometimes used in combination with heroin, and is referred to as a speedball when injected or "moonrocks" when smoked together. Cocaine acts as a stimulant, whereas heroin acts as a depressant. Coadministration provides an intense rush of euphoria with a high that combines both effects of the drugs, while excluding the negative effects, such as anxiety and sedation. The effects of cocaine wear off far more quickly than heroin, so if an overdose of heroin was used to compensate for cocaine, the end result is fatal respiratory depression.
Withdrawal.
The withdrawal syndrome from heroin (the so-called "cold turkey") may begin within 6 to 24 hours of discontinuation of the drug; however, this time frame can fluctuate with the degree of tolerance as well as the amount of the last consumed dose. Symptoms may include: sweating, malaise, anxiety, depression, akathisia, priapism, extra sensitivity of the genitals in females, general feeling of heaviness, excessive yawning or sneezing, tears, rhinorrhea, sleep difficulties (insomnia), cold sweats, chills, severe muscle and bone aches, nausea, vomiting, diarrhea, cramps, watery eyes, fever and cramp-like pains and involuntary spasms in the limbs (thought to be an origin of the term "kicking the habit").
Pharmacology.
When taken orally, heroin undergoes extensive first-pass metabolism via deacetylation, making it a prodrug for the systemic delivery of morphine. When the drug is injected, however, it avoids this first-pass effect, very rapidly crossing the blood–brain barrier because of the presence of the acetyl groups, which render it much more fat soluble than morphine itself. Once in the brain, it then is deacetylated variously into the inactive 3-monoacetylmorphine and the active 6-monoacetylmorphine (6-MAM), and then to morphine, which bind to μ-opioid receptors, resulting in the drug's euphoric, analgesic (pain relief), and anxiolytic (anti-anxiety) effects; heroin itself exhibits relatively low affinity for the μ receptor. Unlike hydromorphone and oxymorphone, however, administered intravenously, heroin creates a larger histamine release, similar to morphine, resulting in the feeling of a greater subjective "body high" to some, but also instances of pruritus (itching) when they first start using.
Both morphine and 6-MAM are μ-opioid agonists that bind to receptors present throughout the brain, spinal cord, and gut of all mammals. The μ-opioid receptor also binds endogenous opioid peptides such as β-endorphin, Leu-enkephalin, and Met-enkephalin. Repeated use of heroin results in a number of physiological changes, including an increase in the production of μ-opioid receptors (upregulation). These physiological alterations lead to tolerance and dependence, so that cessation of heroin use results in a set of remarkably uncomfortable symptoms including pain, anxiety, muscle spasms, and insomnia called the opioid withdrawal syndrome. Depending on usage it has an onset four to 24 hours after the last dose of heroin. Morphine also binds to δ- and κ-opioid receptors.
There is also evidence that 6-MAM binds to a subtype of μ-opioid receptors that are also activated by the morphine metabolite morphine-6β-glucuronide but not morphine itself. The third subtype of third opioid type is the mu-3 receptor, which may be a commonality to other six-position monoesters of morphine. The contribution of these receptors to the overall pharmacology of heroin remains unknown.
A subclass of morphine derivatives, namely the 3,6 esters of morphine, with similar effects and uses, includes the clinically used strong analgesics nicomorphine (Vilan), and dipropanoylmorphine; there is also the latter's dihydromorphine analogue, diacetyldihydromorphine (Paralaudin). Two other 3,6 diesters of morphine invented in 1874–75 along with diacetylmorphine, dibenzoylmorphine and acetylpropionylmorphine, were made as substitutes after it was outlawed in 1925 and, therefore, sold as the first "designer drugs" until they were outlawed by the League of Nations in 1930.
Etymology.
In 1895, the German drug company Bayer marketed diacetylmorphine as an over-the-counter drug under the trademark name Heroin. The name was derived from the Greek word "heros" because of its perceived "heroic" effects upon a user. It was developed chiefly as a morphine substitute for cough suppressants that did not have morphine's addictive side-effects. Morphine at the time was a popular recreational drug, and Bayer wished to find a similar but non-addictive substitute to market. However, contrary to Bayer's advertising as a "non-addictive morphine substitute," heroin would soon have one of the highest rates of dependence among its users.
History.
The opium poppy was cultivated in lower Mesopotamia as long ago as 3400 BCE. The chemical analysis of opium in the 19th century revealed that most of its activity could be ascribed to two alkaloids, codeine and morphine.
Diacetylmorphine was first synthesized in 1874 by C. R. Alder Wright, an English chemist working at St. Mary's Hospital Medical School in London. He had been experimenting with combining morphine with various acids. He boiled anhydrous morphine alkaloid with acetic anhydride for several hours and produced a more potent, acetylated form of morphine, now called "diacetylmorphine" or "morphine diacetate". The compound was sent to F. M. Pierce of Owens College in Manchester for analysis. Pierce told Wright:
 Doses ... were subcutaneously injected into young dogs and rabbits ... with the following general results ... great prostration, fear, and sleepiness speedily following the administration, the eyes being sensitive, and pupils constrict, considerable salivation being produced in dogs, and slight tendency to vomiting in some cases, but no actual emesis. Respiration was at first quickened, but subsequently reduced, and the heart's action was diminished, and rendered irregular. Marked want of coordinating power over the muscular movements, and loss of power in the pelvis and hind limbs, together with a diminution of temperature in the rectum of about 4°.
Wright's invention did not lead to any further developments, and diacetylmorphine became popular only after it was independently re-synthesized 23 years later by another chemist, Felix Hoffmann. Hoffmann, working at Bayer pharmaceutical company in Elberfeld, Germany, was instructed by his supervisor Heinrich Dreser to acetylate morphine with the objective of producing codeine, a constituent of the opium poppy, pharmacologically similar to morphine but less potent and less addictive. Instead, the experiment produced an acetylated form of morphine one and a half to two times more potent than morphine itself. The head of Bayer's research department reputedly coined the drug's new name, "heroin," based on the German "heroisch", which means "heroic, strong.'Bayer scientists were not the first to make heroin, but their scientists discovered ways to make it, and Bayer led commercialization of heroin.
From 1898 through to 1910, diacetylmorphine was marketed under the trademark name Heroin as a non-addictive morphine substitute and cough suppressant.
In the U.S.A., the Harrison Narcotics Tax Act was passed in 1914 to control the sale and distribution of diacetylmorphine and other opioids, which allowed the drug to be prescribed and sold for medical purposes. In 1924, the United States Congress banned its sale, importation, or manufacture. It is now a Schedule I substance, which makes it illegal for non-medical use in signatory nations of the Single Convention on Narcotic Drugs treaty, including the United States.
The Health Committee of the League of Nations banned diacetylmorphine in 1925, although it took more than three years for this to be implemented. In the meantime, the first designer drugs, viz. 3,6 diesters and 6 monoesters of morphine and acetylated analogues of closely related drugs like hydromorphone and dihydromorphine, were produced in massive quantities to fill the worldwide demand for diacetylmorphine—this continued until 1930 when the Committee banned diacetylmorphine analogues with no therapeutic advantage over drugs already in use, the first major legislation of this type.
Later, as with Aspirin, Bayer lost some of its trademark rights to heroin under the 1919 Treaty of Versailles following the German defeat in World War I.
Routes of administration.
The onset of heroin's effects depends upon the route of administration. Studies have shown that the subjective pleasure of drug use (the reinforcing component of addiction) is proportional to the rate at which the blood level of the drug increases. Intravenous injection is the fastest route of drug administration, causing blood concentrations to rise the most quickly, followed by smoking, suppository (anal or vaginal insertion), insufflation (snorting), and ingestion (swallowing).
Ingestion does not produce a rush as forerunner to the high experienced with the use of heroin, which is most pronounced with intravenous use. While the onset of the rush induced by injection can occur in as little as a few seconds, the oral route of administration requires approximately half an hour before the high sets in. Thus, with both higher the dosage of heroin used and faster the route of administration used, the higher potential risk for psychological addiction.
Large doses of heroin can cause fatal respiratory depression, and the drug has been used for suicide or as a murder weapon. The serial killer Dr Harold Shipman used diamorphine on his victims, and the subsequent Shipman Inquiry led to a tightening of the regulations surrounding the storage, prescribing and destruction of controlled drugs in the UK. Dr John Bodkin Adams (see his victim Edith Alice Morrell) is also known to have used heroin as a murder weapon.
Because significant tolerance to respiratory depression develops quickly with continued use and is lost just as quickly during withdrawal, it is often difficult to determine whether a heroin lethal overdose was accidental, suicide or homicide. Examples include the overdose deaths of Sid Vicious, Janis Joplin, Tim Buckley, Hillel Slovak, Layne Staley, Bradley Nowell, Ted Binion, and River Phoenix.
Chronic use of heroin and other opioids has been shown to be a potential cause of hyponatremia, resultant because of excess vasopressin secretion.
Oral.
Oral use of heroin is less common than other methods of administration, mainly because there is little to no "rush", and the effects are less potent. Heroin is entirely converted to morphine by means of first-pass metabolism, resulting in deacetylation when ingested. Heroin's oral bioavailability is both dose-dependent (as is morphine's) and significantly higher than oral use of morphine itself, reaching up to 64.2% for high doses and 45.6% for low doses; opiate-naive users showed far less absorption of the drug at low doses, having bioavailabilities of only up to 22.9%. The maximum plasma concentration of morphine following oral administration of heroin was around twice as much as that of oral morphine.
Injection.
Injection, also known as "slamming", "banging", "shooting up", "digging" or "mainlining", is a popular method which carries relatively greater risks than other methods of administration. Heroin base (commonly found in Europe), when prepared for injection, will only dissolve in water when mixed with an acid (most commonly citric acid powder or lemon juice) and heated. Heroin in the east-coast United States is most commonly found in the hydrochloride salt form, requiring just water (and no heat) to dissolve. Users tend to initially inject in the easily accessible arm veins, but as these veins collapse over time, users resort to more dangerous areas of the body, such as the femoral vein in the groin. Users who have used this route of administration often develop a deep vein thrombosis. Intravenous users can use a various single dose range using a hypodermic needle. The dose of heroin used for recreational purposes is dependent on the frequency and level of use: thus a first-time user may use between 5 and 20 mg, while an established addict may require several hundred mg per day. As with the injection of any drug, if a group of users share a common needle without sterilization procedures, blood-borne diseases, such as HIV or hepatitis, can be transmitted.
The use of a common dispenser for water for the use in the preparation of the injection, as well as the sharing of spoons and/or filters can also cause the spread of blood-borne diseases. Many countries now supply small sterile spoons and filters for single use in order to prevent the spread of disease.
Smoking.
Smoking heroin refers to vaporizing it to inhale the resulting fumes, not burning it to inhale the resulting smoke. It is commonly smoked in glass pipes made from glassblown Pyrex tubes and light bulbs. It can also be smoked off aluminium foil, which is heated underneath by a flame and the resulting smoke is inhaled through a tube of rolled up foil, This method is also known as "chasing the dragon" (whereas smoking methamphetamine is known as "chasing the "white" dragon").
Insufflation.
Another popular route to intake heroin is insufflation (snorting), where a user crushes the heroin into a fine powder and then gently inhales it (sometimes with a straw or a rolled-up banknote, as with cocaine) into the nose, where heroin is absorbed through the soft tissue in the mucous membrane of the sinus cavity and straight into the bloodstream. This method of administration redirects first-pass metabolism, with a quicker onset and higher bioavailability than oral administration, though the duration of action is shortened. This method is sometimes preferred by users who do not want to prepare and administer heroin for injection or smoking, but still experience a fast onset. Snorting heroin becomes an often unwanted route, once a user begins to inject the drug. The user may still get high on the drug from snorting, and experience a nod, but will not get a rush. A "rush" is caused by a large amount of heroin entering the body at once. When the drug is taken in through the nose, the user does not get the rush because the drug is absorbed slowly rather than instantly.
Suppository.
Little research has been focused on the suppository (anal insertion) or pessary (vaginal insertion) methods of administration, also known as "plugging". These methods of administration are commonly carried out using an oral syringe. Heroin can be dissolved and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. The rectum or the vaginal canal is where the majority of the drug would likely be taken up, through the membranes lining their walls.
Regulation.
Asia.
In Hong Kong, diacetylmorphine is regulated under Schedule 1 of Hong Kong's Chapter 134 "Dangerous Drugs Ordinance". It is available by prescription. Anyone supplying diacetylmorphine without a valid prescription can be fined $10,000 (HKD). The penalty for trafficking or manufacturing diacetylmorphine is a $50,000 (HKD) fine and life imprisonment. Possession of diacetylmorphine without a license from the Department of Health is illegal with a $10,000 (HKD) fine and/or 7 years of jail time.
Europe.
In the Netherlands, diacetylmorphine is a List I drug of the Opium Law. It is available for prescription under tight regulation exclusively to long-term addicts for whom methadone maintenance treatment has failed. It cannot be used to treat severe pain or other illnesses.
In the United Kingdom, diacetylmorphine is available by prescription, though it is a restricted Class A drug. According to the 50th edition of the British National Formulary (BNF), diamorphine hydrochloride may be used in the treatment of acute pain, myocardial infarction, acute pulmonary oedema, and chronic pain. The treatment of chronic non-malignant pain must be supervised by a specialist. The BNF notes that all opioid analgesics cause dependence and tolerance but that this is "no deterrent in the control of pain in terminal illness". When used in the palliative care of cancer patients, diacetylmorphine is often injected using a syringe driver.
North America.
In Canada, diacetylmorphine is a controlled substance under Schedule I of the Controlled Drugs and Substances Act (CDSA). Any person seeking or obtaining diacetylmorphine without disclosing authorization 30 days before obtaining another prescription from a practitioner is guilty of an indictable offense and subject to imprisonment for a term not exceeding seven years. Possession of diacetylmorphine for the purpose of trafficking is an indictable offense and subject to imprisonment for life.
In the United States, diacetylmorphine is a Schedule I drug according to the Controlled Substances Act of 1970, making it illegal to possess without a DEA license. Possession of more than 100 grams of diacetylmorphine or a mixture containing diacetylmorphine is punishable with a minimum mandatory sentence of 5 years of imprisonment in a federal prison.
Production and trafficking.
Production.
Diacetylmorphine is produced from acetylation of morphine derived from natural opium sources. Numerous mechanical and chemical means are used to purify the final product. The final products have a different appearance depending on purity and have different names.
Heroin grades.
Heroin purity has been classified into four grades. No.4 is the purest form – white powder (salt) to be easily dissolved and injected. No.3 is "brown sugar" for smoking (base). No.1 and No.2 are unprocessed raw heroin (salt or base).
Trafficking.
Traffic is heavy worldwide, with the biggest producer being Afghanistan. According to a U.N. sponsored survey, in 2004, Afghanistan accounted for production of 87 percent of the world's diacetylmorphine. Afghan opium kills around 100,000 people annually.
The cultivation of opium in Afghanistan reached its peak in 1999, when 350 sqmi of poppies were sown. The following year the Taliban banned poppy cultivation, a move which cut production by 94 percent. By 2001 only 30 sqmi of land were in use for growing opium poppies. A year later, after American and British troops had removed the Taliban and installed the interim government, the land under cultivation leapt back to 285 sqmi, with Afghanistan supplanting Burma to become the world's largest opium producer once more.
Opium production in that country has increased rapidly since, reaching an all-time high in 2006. War in Afghanistan once again appeared as a facilitator of the trade. Some 3.3 million Afghans are involved in producing opium.
At present, opium poppies are mostly grown in Afghanistan, and in Southeast Asia, especially in the region known as the Golden Triangle straddling Burma, Thailand, Vietnam, Laos and Yunnan province in China. There is also cultivation of opium poppies in the Sinaloa region of Mexico and in Colombia. The majority of the heroin consumed in the United States comes from Mexico and Colombia. According to the United Nations Office on Drugs and Crime (UNODC), Pakistan has over 1,000 hectare of opium poppies under cultivation concentrated in the areas bordering Afghanistan and is the destination and transit point for 40 percent of the opiates produced in that country.
Conviction for trafficking heroin carries the death penalty in most Southeast Asian, some East Asian and Middle Eastern countries (see Use of death penalty worldwide for details), among which Malaysia, Singapore and Thailand are the most strict. The penalty applies even to citizens of countries where the penalty is not in place, sometimes causing controversy when foreign visitors are arrested for trafficking, for example the arrest of nine Australians in Bali, the death sentence given to Nola Blake in Thailand in 1987, or the hanging of an Australian citizen Van Tuong Nguyen in Singapore.
Trafficking history.
The origins of the present international illegal heroin trade can be traced back to laws passed in many countries in the early 1900s that closely regulated the production and sale of opium and its derivatives including heroin. At first, heroin flowed from countries where it was still legal into countries where it was no longer legal. By the mid-1920s, heroin production had been made illegal in many parts of the world. An illegal trade developed at that time between heroin labs in China (mostly in Shanghai and Tianjin) and other nations. The weakness of government in China and conditions of civil war enabled heroin production to take root there. Chinese triad gangs eventually came to play a major role in the illicit heroin trade. The French Connection route started in the 1930s.
Heroin trafficking was virtually eliminated in the U.S. during World War II because of temporary trade disruptions caused by the war. Japan's war with China had cut the normal distribution routes for heroin and the war had generally disrupted the movement of opium.
After World War II, the Mafia took advantage of the weakness of the postwar Italian government and set up heroin labs in Sicily. The Mafia took advantage of Sicily's location along the historic route opium took westward into Europe and the United States.
Large-scale international heroin production effectively ended in China with the victory of the communists in the civil war in the late 1940s. The elimination of Chinese production happened at the same time that Sicily's role in the trade developed.
Although it remained legal in some countries until after World War II, health risks, addiction, and widespread recreational use led most western countries to declare heroin a controlled substance by the latter half of the 20th century.
In late 1960s and early 1970s, the CIA supported anti-Communist Chinese Nationalists settled near the Sino-Burmese border and Hmong tribesmen in Laos. This helped the development of the Golden Triangle opium production region, which supplied about one-third of heroin consumed in US after the 1973 American withdrawal from Vietnam. In 1999, Burma, the heartland of the Golden Triangle, was the second largest producer of heroin, after Afghanistan.
The Soviet-Afghan war led to increased production in the Pakistani-Afghan border regions, as U.S.-backed mujaheddin militants raised money for arms from selling opium, contributing heavily to the modern Golden Crescent creation. By 1980, 60 percent of heroin sold in the U.S. originated in Afghanistan. It increased international production of heroin at lower prices in the 1980s. The trade shifted away from Sicily in the late 1970s as various criminal organizations violently fought with each other over the trade. The fighting also led to a stepped-up government law enforcement presence in Sicily.
Following the discovery at a Jordanian airport of a toner cartridge that had been modified into an improvised explosive device, the resultant increased level of airfreight scrutiny led to a major shortage (drought) of heroin from October 2010 until April 2011. This was reported in most of mainland Europe and the UK which led to a price increase of approximately 30 percent in the cost of street heroin and an increased demand for diverted methadone. The number of addicts seeking treatment also increased significantly during this period.
Other heroin droughts (shortages) have been attributed to cartels restricting supply in order to force a price increase and also to a fungus that attacked the opium crop of 2009. Many people thought that the American government had introduced pathogens into the Afghanistan atmosphere in order to destroy the opium crop and thus starve insurgents of income.
On 13 March 2012, Haji Bagcho, with ties to the Taliban, was convicted by a U.S. District Court of conspiracy, distribution of heroin for importation into the United States and narco-terrorism. Based on heroin production statistics compiled by the United Nations Office on Drugs and Crime, in 2006, Bagcho's activities accounted for approximately 20 percent of the world's total production for that year.
Street price.
The European Monitoring Centre for Drugs and Drug Addiction reports that the retail price of brown heroin varies from €14.5 per gram in Turkey to €110 per gram in Sweden, with most European countries reporting typical prices of €35–40 per gram. The price of white heroin is reported only by a few European countries and ranged between €27 and €110 per gram.
The United Nations Office on Drugs and Crime claims in its 2008 World Drug Report that typical US retail prices are US$172 per gram.
Harm reduction.
Harm reduction is a public health philosophy that seeks to reduce the harms associated with the use of diacetylmorphine. One aspect of harm reduction initiatives focuses on the behaviour of individual users. This includes promoting safer means of taking the drug, such as smoking, nasal use, oral or rectal insertion. This attempts to avoid the higher risks of overdose, infections and blood-borne viruses associated with injecting the drug. Other measures include using a small amount of the drug first to gauge the strength, and minimize the risks of overdose. For the same reason, poly drug use (the use of two or more drugs at the same time) is discouraged. Injecting diacetylmorphine users are encouraged to use new needles, syringes, spoons/steri-cups and filters every time they inject and not share these with other users. Users are also encouraged to not use it on their own, as others can assist in the event of an overdose.
Governments that support a harm reduction approach usually fund needle and syringe exchange programs, which supply new needles and syringes on a confidential basis, as well as education on proper filtering before injection, safer injection techniques, safe disposal of used injecting gear and other equipment used when preparing diacetylmorphine for injection may also be supplied including citric acid sachets/vitamin C sachets, steri-cups, filters, alcohol pre-injection swabs, sterile water ampules and tourniquets (to stop use of shoe laces or belts).
Another harm reduction measure employed for example in Europe, Canada and Australia are safe injection sites where users can inject diacetylmorphine and cocaine under the supervision of medically trained staff. Safe injection sites are low threshold and allow social services to approach problem users that would otherwise be hard to reach.
In the UK the Criminal Justice System has a protocol in place that requires that any individual that is arrested and is suspected of having a substance misuse problem be offered the chance to enter a treatment program. This has had the effect of drastically reducing an area's crime rate as individuals arrested for theft in order to supply the funds for their drugs are no longer in the position of having to steal to purchase heroin because they have been placed onto a methadone program, quite often more quickly than would have been possible had they not been arrested. This aspect of harm reduction is seen as being beneficial to both the individual and the community at large, who are then protected from the possible theft of their goods.
During the late 1980s and early 1990s, Swiss authorities ran the ZIPP-AIDS (Zurich Intervention Pilot Project), handing out free syringes in the officially tolerated drug scene in Platzspitz park.
Popular culture.
Heroin is mentioned in hundreds of films. Sometimes the use or trafficking of the drug is the central theme of the film but many times it is almost incidental as part of a crime in a police drama, for example.
Use of heroin by jazz musicians in particular was prevalent in the mid-twentieth century, including Billie Holiday, sax legends Charlie Parker and Art Pepper, guitarist Joe Pass and piano player/singer Ray Charles; a "staggering number of jazz musicians were addicts". It was also a problem with many rock musicians, particularly from the late 1960s through the 1990s. Pete Doherty is also a self-confessed user of heroin. Nirvana frontman Kurt Cobain's heroin addiction was well documented. Pantera frontman, Phil Anselmo, turned to heroin while touring during the 1990s to cope with his back pain.

</doc>
<doc id="14035" url="http://en.wikipedia.org/wiki?curid=14035" title="Hellas Verona F.C.">
Hellas Verona F.C.

Hellas Verona Football Club (commonly known simply as Verona, or Hellas within the city of Verona itself) are a professional Italian association football team, based in Verona, Veneto. The team won the Italian Serie A championship in 1984–85, and are playing in Serie A in 2014–15.
History.
Origins and early history.
Founded in 1903 by a group of high school students, the club was named "Hellas" (the Greek word for Greece), at the request of a professor of Classics. At a time in which football was played seriously only in the larger cities of the Northwest of Italy, most of Verona was indifferent to the growing sport. However, when in 1906 two city teams chose the city's Roman amphitheatre as a venue to showcase the game, crowd enthusiasm and media interest began to rise.
During these first few years Hellas was one of three or four area teams playing mainly at a municipal level while fighting against city rivals Bentegodi to become the city's premier football outfit. By the 1907–08 season, Hellas was playing against regional teams and an intense rivalry with Vicenza Calcio that lasts to this day was born.
From 1898 to 1926 Italian football was organised into regional groups. In this period Hellas was one of the founding teams of the early league and often among its top final contenders. In 1911, the city helped Hellas replace the early, gritty football fields with a proper venue. This allowed the team to take part in its first regional tournament, which until 1926, was the qualifying stage for the national title.
In 1919, following a return to activity after a four-year suspension of all football competition in Italy during World War I the team merged with city rival Verona and changed its name to Hellas Verona. Between 1926 and 1929 the elite "Campionato Nazionale" assimilated the top sides from the various regional groups and Hellas Verona joined the privileged teams, yet struggled to remain competitive.
Serie A, as it is structured today, began in 1929, when the "Campionato Nazionale" turned into a professional league. Still an amateur team, Hellas merged with two city rivals, Bentegodi and Scaligera, to form AC Verona. Hoping to build a first class contender for future years the new team debuted in Serie B in 1929. It would take the "gialloblu" 28 years to finally achieve their goal. After first being promoted to Serie A for one season in 1957–58, in 1959 the team merged with another city rival (called Hellas) and commemorated its beginnings by changing its name to Hellas Verona AC.
Success in the 1970s and 1980s.
Coached by Nils Liedholm, the team returned to Serie A in 1968 and remained in the elite league almost without interruption until 1990. Along the way it scored a famous 5–3 win in the 1972–73 season that cost AC Milan the "scudetto" (the Serie A title). The fact that the result came late during the last matchday of the season makes the sudden and unexpected end to the "rossoneri"'s title ambitions all the more memorable.
In 1973–74 Hellas finished the season in 4th last place thus avoiding relegation, but were sent down to Serie B during the summer months as a result of a scandal involving team president Saverio Garonzi. After a year in Serie B Hellas Verona returned to Serie A.
In the 1975–76 season the team had a successful run in the Coppa Italia, eliminating highly rated teams such as Torino, Cagliari, and Internazionale from the tournament. However, in their first ever final in the competition Hellas Verona were trounced 4–0 by Napoli.
Under the leadership of coach Osvaldo Bagnoli, in 1982–83 the team secured 4th place in Serie A (its highest finish at the time) and even lead the Serie A standings for a few weeks. The same season Hellas again reached the Coppa Italia final. After a 2–0 home victory, Hellas Verona travelled to Turin to play Juventus but were defeated 3–0 after extra time.
Further disappointment followed in the 1983–84 season when the team again reached the Coppa Italia final, only to lose the Cup in the final minutes of the return match against defending Serie A champions Roma
The team made its first European appearance in the 1983-84 UEFA Cup and were knocked out in the second round of the tournament by Sturm Graz. Hellas were eliminated from the 1985–86 European Cup in the second round by defending champions and fellow Serie A side Juventus after a contested game, the result of a scandalous arbitrage by the French Wurtz, having beaten PAOK Thessaloniki of Greece in the first round.
In 1988 the team had their best international result when they reached the UEFA Cup quarter-finals with four victories and three draws. The decisive defeat came from German side Werder Bremen.
1984–1985 "Scudetto".
Although the 1984–85 squad was made up of a healthy mix of emerging players and mature stars, at the beginning of the season no one would have regarded the team as having the necessary ingredients to make it to the end. Certainly the additions of Hans-Peter Briegel in midfield and of Danish striker Preben Elkjær to an attack that already featured the wing play of Pietro Fanna, the creative abilities of Antonio Di Gennaro and the scoring touch of Giuseppe Galderisi were to prove crucial.
To mention a few of the memorable milestones on the road to the "scudetto": a decisive win against Juventus (2–0), with a goal scored by Elkjær after having lost a boot in a tackle just outside the box, set the stage early in the championship; an away win over Udinese (5–3) ended any speculation that the team was losing energy at the midway point; three straight wins (including a hard fought 1–0 victory against a strong AS Roma side) served notice that the team had kept its polish and focus intact during their rival's final surge; and a 1–1 draw in Bergamo against Atalanta secured the title with a game in hand.
Hellas Verona finished the year with a 15–13–2 record and 43 points, 4 points ahead of
Torino with Internazionale and Sampdoria rounding out the top four spots. This unusual final table of the Serie A (with the most successful Italian teams of the time, Juventus and AS Roma, ending up much lower than expected) has led to many speculations. The 1984–85 season was the only season when referees were assigned to matches by way of a random draw. Before then each referee had always been assigned to a specific match by a special commission of referees ("designatori arbitrali"). After the betting scandal of the early 1980 (the Calcio Scommesse scandal) it was decided to clean up the image of Italian football by assigning referees randomly instead of picking them, to clear up all the suspicions and accusations always accompanying Italy's football life. This resulted in a quieter championship and in a completely unexpected final table.
In the following season, won again by Juventus, the choice of the referees went back in the hands of the "designatori arbitrali". In 2006 a major scandal in Italian football revealed that certain clubs had been illegally influencing the referee selection process, in an attempt to ensure that certain referees were assigned to their matches.
Between Serie A and Serie B.
These were more than mere modest achievements for a mid-size city with a limited appeal to fans across the nation. But soon enough financial difficulties caught up with team managers. In 1991 the team folded and was reborn as Verona FC, regularly moving to and fro between Serie A and Serie B for several seasons. In 1995 the name was officially changed back to Hellas Verona FC.
After a three-year stay, their last stint in Serie A ended in grief in 2002. That season emerging international talents such as Adrian Mutu, Mauro Camoranesi, Alberto Gilardino, Martin Laursen, Massimo Oddo, Marco Cassetti and coach Alberto Malesani failed to capitalise on an excellent start and eventually dropped into fourth-to-last place for the first time all season on the very last matchday, enforcing relegation into Serie B.
Decline and Serie A comeback (2002–present).
Following the 2002 relegation to Serie B, team fortunes continued to slip throughout the decade. In the 2003–04 season Hellas Verona struggled in Serie B and spent most of the season fighting off an unthinkable relegation to Serie C1. Undeterred, the fans supported their team and a string of late season wins eventually warded off the danger. Over 5000 of them followed Hellas to Como on the final day of the season to celebrate.
In 2004–05 things looked much brighter for the team. After a rocky start Hellas put together a string of results and climbed to third spot. The gialloblù held on to the position until January 2005, when transfers weakened the team, yet they managed to take the battle for Serie A to the last day of the season.
The Serie B 2006-07 seemed to start well, due to the club takeover by Pietro Arvedi D'Emilei, which ended nine years of controversial rule by chairman Gianbattista Pastorello, heavily contested by the supporters in his later years at Verona. However, Verona was immediately involved in the relegation battle, and Massimo Ficcadenti was replaced in December 2006 by Giampiero Ventura. Despite a recovery in the results, Verona ended in an 18th place, thus being forced to play a two-legged playoff against 19th-placed Spezia to avert relegation. A 2–1 away loss in the first leg at La Spezia was followed by a 0–0 home tie, and Verona were relegated to Serie C1 after 64 years of play in the two highest divisions.
Verona appointed experienced coach Franco Colomba for the new season with the aim to return to Serie B as soon as possible. However, despite being widely considered the division favourite, the gialloblù spent almost the entire season in last place. After seven matches club management sacked Colomba in early October and replaced him with youth team coach (and former Verona player) Davide Pellegrini. A new property acquired the club in late 2007, appointing in December Giovanni Galli as new director of football and Maurizio Sarri as new head coach. Halfway through the 2007–08 season the team remained at the bottom of Serie C1, on the brink of relegation to the fourth level (Serie C2).In response, club management sacked Sarri and brought back Pellegrini. Thanks to a late-season surge the "scaligeri" avoided direct relegation by qualifying for the relegation play-off, and narrowly averted dropping to Lega Pro Seconda Divisione in the final game, beating Pro Patria 2–1 on aggregate. However, despite the decline in results, attendance and season ticket sales remained on 15,000 average.
For the 2008–09 season Verona appointed former Sassuolo and Piacenza manager Gian Marco Remondina, with the aim to win promotion to Serie B. However the season did not start impressively, with Verona being out of the playoff zone by mid-season, and club chairman Pietro Arvedi D'Emilei entering into a coma after being involved in a car crash on his way back from a league match in December 2008. Arvedi died in March 2009, two months after the club was bought by new chairman Giovanni Martinelli.
The following season looked promising, as new transfer players were brought aboard, and fans enthusiastically embraced the new campaign. Season ticket figures climbed to over 10,000, placing Verona ahead of several Serie A teams and all but Torino in Serie B attendance. The team led the standings for much of the season, accumulating a seven-point lead by early in the spring. However, the advantage was gradually squandered, and the team dropped to second place on the second last day of the season, with a chance to regain first place in the final regular season match against Portogruaro on home soil. But Verona disappointed a crowd of over 25,000 fans and, with the loss, dropped to third place and headed towards the play-offs. A managerial change for the postseason saw the firing of Remondina and the arrival of Giovanni Vavassori. After eliminating Rimini in the semi-finals (1–0; 0–0) Verona lost the final to Pescara (2–2 on home soil and 0–1 in the return match) and were condemned to a fourth straight year of third division.
Former Italia '90 star Giuseppe Giannini (a famous captain of Roma for many years) signed as manager for the 2010–11 campaign. Once again, the team was almost entirely revamped during the transfer season. The squad struggled in the early months and Giannini was eventually sacked and replaced by former Internazionale defender Andrea Mandorlini, who succeeded in reorganising the team's play and bringing discipline both on and off the pitch. In the second half of the season Verona climbed back from the bottom of the division to clinch a play-off berth (5th place) on the last day of the regular season. The team advanced to the play-off final after eliminating Sorrento in the semi-finals (3–1 agg). Following the play-off final, after four years of Lega Pro, Hellas Verona were promoted back to Serie B after a 2–1 (agg) win over Salernitana on 19 June 2011.
On 18 May 2013 Hellas Verona finished second in Serie B and was promoted to Serie A after 11 years.
Their return to the Serie A started off against Champions League contending teams such as Milan and Roma, where they beat the former 2–1 and lost to the latter 3–0. They lost 3–0, even though their goalie, Rafael, made nine top notch saves. Verona had chances too, including a shot off the crossbar and a shot cleared off the line by Roma. They continued at a steady pace, finishing the first half of the season with 32 points, sitting in 6th place, 11 points behind the closest Champions League spot, and tied with Inter for the last Europa League spot. They eventually finished in 10th place.
Colours and badge.
The team's colours are yellow and blue and "gialloblu" (literally, "yellow-blue" in Italian) is the team's most widely used nickname. The colours represent the city itself and Verona's emblem (a yellow cross on a blue shield) appears on most team apparel. Two more team nicknames are "Mastini" (the mastiffs) and "Scaligeri", both references to Mastino I della Scala of the Della Scala princes that ruled the city during the 13th and 14th centuries.
The Scala family coat of arms is depicted on the team's jersey and on its trademark logo as a stylised image of two large, powerful mastiffs facing opposite directions. In essence, the term "scaligeri" is synonymous with Veronese, and therefore can describe anything or anyone from Verona (e.g., Chievo Verona, a different team that also links itself to the Scala family – specifically to Cangrande I della Scala).
Stadium.
Since 1963, the club have played at the Stadio Marcantonio Bentegodi, which has a capacity of 39,211. The ground is shared with Hellas' rivals, Chievo Verona. It was used as a venue for the 1990 FIFA World Cup.
Derby with Chievo Verona.
The intercity fixtures against Chievo Verona are known as the "Derby della Scala". The name refers to the Scaligeri or della Scala aristocratic family, who were rulers of Verona during the Middle Ages and early Renaissance. In the season 2001–02, both Hellas Verona and the city rivals of Chievo Verona were playing in the Serie A. The first ever derby of Verona in Serie A took place on 18 November 2001,
while both teams were ranked among the top four. The match was won by Hellas, 3–2. Chievo got revenge in the return match in spring 2002, winning 2–1.
The city of Verona became so the 5th city in Italy, after Milan, Rome, Turin and Genoa to host a derby in Serie A.
Results

</doc>
<doc id="14036" url="http://en.wikipedia.org/wiki?curid=14036" title="Hinayana">
Hinayana

Hīnayāna (हीनयान) is a Sanskrit term literally meaning: the "Smaller Vehicle", applied to the "Śrāvakayāna", the Buddhist path followed by a śrāvaka who wishes to become an arhat. The term appeared around the 1st or 2nd century. "Hīnayāna" is often contrasted with "Mahāyāna", which means the "Great Vehicle."
There are a variety of interpretations as to who or what the term "Hīnayāna" refers. Kalu Rinpoche stated the "lesser" or "greater" designation "does not refer to economic or social status, but concerns the spiritual capacities of the practitioner".
The Hinayana or Small Vehicle is defined by Kalu Rinpoche as follows:
"The Small Vehicle is based on becoming aware of the fact that all we experience in samsara is marked by suffering. Being aware of this engenders the will to rid ourselves of this suffering, to liberate ourselves on an individual level, and to attain happiness. We are moved by our own interest.
Renunciation and perseverance allow us to attain our goal."
The Chinese monk Yijing who visited India in the 7th century, distinguishes Mahāyāna from Hīnayāna as follows:
Both adopt one and the same Vinaya, and they have in common the prohibitions of the five offenses, and also the practice of the Four Noble Truths. Those who venerate the bodhisattvas and read the Mahāyāna sūtras are called the Mahāyānists, while those who do not perform these are called the Hīnayānists.
The term was widely used in the past by Western scholars to cover "the earliest system of Buddhist doctrine" as the Monier-Williams Sanskrit-English Dictionary put it. It has been used as a synonym for the Theravada tradition, which continues as the main form of Buddhism in Sri Lanka and South-East Asia, but some scholars deny that the term included Theravada Buddhism. In 1950 the World Fellowship of Buddhists declared that the term Hīnayana should not be used when referring to any form of Buddhism existing today.
Etymology.
The word Hīnayāna is formed of "hīna" (हीन): "little," "poor," "inferior," "abandoned," "deficient," "defective;" and "yāna" (यान): "vehicle", where "vehicle" means "a way of going to enlightenment". The Pali Text Society's Pali-English Dictionary (1921–25) defines "hīna" in even stronger terms, with a semantic field that includes "poor, miserable; vile, base, abject, contemptible," and "despicable."
In the Chinese, Korean, Vietnamese and Japanese languages, the term was translated by Kumārajīva and others as "small vehicle" (小 meaning "small", 乘 meaning "vehicle"), although earlier and more accurate translations of the term also exist. In Mongolian ("Baga Holgon") term for Hinayana also means "small" or "lesser" vehicle, while in Tibetan there are at least two words to designate the term, "theg chung" (Tibetan: ཐེག་ཆུང་
) meaning "small vehicle", and "theg dman" (Tibetan: ཐེག་དམན་
) meaning "inferior vehicle" or "inferior spiritual approach".
Khenchen Thrangu Rinpoche has emphasized that "hinayana" is in no way implying "inferior". In his translation and commentary of Asanga's teaching "Distinguishing Dharma from Dharmata" it is stated "...all three traditions of hinayana, mahayana, and vajrayana were practiced in Tibet and that the hinayana which literally means "lesser vehicle" is in no way inferior to the mahayana."
Origins.
According to Jan Nattier, it is most likely that the term "Hīnayāna" post-dates the term "Mahāyāna", and was only added at a later date due to antagonism and conflict between bodhisattvas and śrāvakas. The sequence of terms then began with "Bodhisattvayāna", which was given the epithet "Mahāyāna" ("Great Vehicle"). It was only later, after attitudes toward the bodhisattvas and their teachings had become more critical, that the term "Hīnayāna" was created as a back-formation, contrasting with the already-established term "Mahāyāna". The earliest Mahāyāna texts often use the term "Mahāyāna" as an epithet and synonym for "Bodhisattvayāna", but the term "Hīnayāna" is comparatively rare in early texts, and is usually not found at all in the earliest translations. Therefore, the often-perceived symmetry between "Mahāyāna" and "Hīnayāna" can be deceptive, as the terms were not actually coined in relation to one another in the same era.
According to Paul Williams, "the deep-rooted misconception concerning an unfailing, ubiquitous fierce criticism of the Lesser Vehicle by the [Mahāyāna] is not supported by our texts." Williams states that while evidence of conflict is present in some cases, there is also substantial evidence demonstrating peaceful coexistence between the two traditions.
Mahāyāna members of the early Buddhist schools.
Although the 18-20 early schools of Buddhism are sometimes loosely classified as Hīnayāna in modern times, this is not necessarily accurate. There is no evidence that Mahāyāna ever referred to a separate formal school of Buddhism, but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas. Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate Vinaya or ordination lineage from the early Buddhist schools, and therefore each bhikṣu or bhikṣuṇī adhering to the Mahāyāna formally belonged to an early school. This continues today with the Dharmaguptaka ordination lineage in East Asia, and the Mūlasarvāstivāda ordination lineage in Tibetan Buddhism. Therefore Mahāyāna was never a separate rival sect of the early schools. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side.
The Chinese Buddhist monk and pilgrim Yijing wrote about relationship between the various "vehicles" and the early Buddhist schools in India. He wrote, "There exist in the West numerous subdivisions of the schools which have different origins, but there are only four principal schools of continuous tradition." These schools are namely the Mahāsāṃghika Nikāya, Sthavira Nikāya, Mūlasarvāstivāda Nikāya, and Saṃmitīya Nikāya. Explaining their doctrinal affiliations, he then writes, "Which of the four schools should be grouped with the Mahāyāna or with the Hīnayāna is not determined." That is to say, there was no simple correspondence between a Buddhist school and whether its members learn "Hīnayāna" or "Mahāyāna" teachings.
To identify entire schools as "Hīnayāna" that contained not only śrāvakas and pratyekabuddhas, but also Mahāyāna bodhisattvas as well, would be attacking the schools of their fellow Mahāyānists as well as their own. Instead, what is demonstrated in the definition of "Hīnayāna" given by Yijing, is that the term referred to individuals based on doctrinal differences with the Mahāyāna tradition.
Hīnayāna as Śrāvakayāna.
Scholar Isabelle Onians asserts that although "the Mahāyāna ... very occasionally referred to earlier Buddhism as the Hinayāna, the Inferior Way," "the preponderance of this name in the secondary literature is far out of proportion to occurrences in the Indian texts." She notes that the term Śrāvakayāna was "the more politically correct and much more usual" term used by Mahāyānists. Jonathan Silk has argued that the term "Hinayana" was used to refer to whomever one wanted to criticize on any given occasion, and did not refer to any definite grouping of Buddhists.
Hīnayāna and Theravāda.
Views of Chinese pilgrims.
In the 7th century, the Chinese Buddhist monk Xuanzang describes the concurrent existence of the Mahāvihara and the Abhayagiri Vihara in Sri Lanka. He refers to the monks of the Mahāvihara as the "Hīnayāna Sthaviras" ("Theras"), and the monks of the Abhayagiri Vihara as the "Mahāyāna Sthaviras".
Xuanzang further writes:
The Mahāvihāravāsins reject the Mahāyāna and practice the Hīnayāna, while the Abhayagirivihāravāsins study both Hīnayāna and Mahāyāna teachings and propagate the "Tripiṭaka".
Philosophical differences.
Mahayanists were primarily in philosophical dialectic with the Vaibhāśika-Sarvāstivāda, which had by far the most "comprehensive edifice of doctrinal systematics" of the nikaya schools. With this in mind it is sometimes argued that the Theravada would not have been considered a "Hinayana" school by Mahayanists because unlike the now-extinct Sarvastivada school, the primary object of Mahayana criticism, the Theravada school does not claim the existence of independent dharmas; in this it maintains the attitude of early Buddhism. Additionally, the concept of the Bodhisattva as one who puts off enlightenment, rather than reaching awakening as soon as possible, has no roots in Theravada textual or cultural contexts, current or historical. Aside from the Theravada schools being geographically distant from the Mahayana, the Hinayana distinction is used in reference to certain views and practices that had become found within the Mahayana tradition itself. Theravada, as well as Mahayana schools stress the urgency of ones' own awakening in order to end suffering. Some contemporary Theravadin figures have thus indicated a sympathetic stance toward the Mahayana philosophy found in the Heart Sutra and the Fundamental Stanzas on the Middle Way.
The Mahayanists were bothered by the substantialist thought of the Sarvastivadins and Sautrantikas, and in emphasizing the doctrine of emptiness, Kalupahana holds that they endeavored to preserve the early teaching. The Theravadins too refuted the Sarvastivadins and Sautrantikas (and other schools) on the grounds that their theories were in conflict with the non-substantialism of the canon. The Theravada arguments are preserved in the Kathavatthu.
Opinions of scholars.
Most western scholars regard the Theravada school to be one of the Hinayana schools referred to in Mahayana literature, or regard Hinayana as a synonym for Theravada. These scholars understand the term to refer to schools of Buddhism that did not accept the teachings of the Mahayana Sutras as authentic teachings of the Buddha. At the same time, scholars have objected to the pejorative connotation of the term Hinayana and some scholars do not use it for any school.
Opinions of Theravadin Buddhists.
Some Theravada Buddhists have opposed the identification of Theravada with Hinayana, as it is seen as more of a derogatory term. As Walpola Rahula noted in his "Gems of Buddhist Wisdom": We must not confuse Hīnayāna with Theravāda because the terms are not synonymous. Theravāda Buddhism went to Sri Lanka during the 3rd Century BC when there was no Mahāyāna at all. Hīnayāna sects developed in India and had an existence independent from the form of Buddhism existing in Sri Lanka. Today there is no Hīnayāna sect in existence anywhere in the world. Therefore, in 1950 the World Fellowship of Buddhists inaugurated in Colombo unanimously decided that the term Hīnayana should be dropped when referring to Buddhism existing today in Sri Lanka, Thailand, Burma, Cambodia, Laos, etc. This is the brief history of Theravāda, Mahayāna and Hīnayāna.
Since the time of Rahula's writing considerable evidence has emerged indicating that Theravadins and Mahayanists interacted extensively in Sri Lanka throughout the first millennium CE, so any suggestion that there was no contact between the two would be incorrect.

</doc>
<doc id="14045" url="http://en.wikipedia.org/wiki?curid=14045" title="Humphrey Bogart">
Humphrey Bogart

Humphrey DeForest Bogart (; December 25, 1899 – January 14, 1957) was an American screen actor whose performances in such iconic 1940s films noir as "The Maltese Falcon", "Casablanca", and "The Big Sleep", earned him the legacy of cultural icon. In 1999, the American Film Institute ranked Bogart as the greatest male star in the history of American cinema. Over his career he received three Academy Award nominations for Best Actor, winning one.
Bogart began acting in 1921 after a hitch in the U.S. Navy in World War I and little success in various jobs in finance and the production side of the theater. Gradually he became a regular in Broadway shows in the 1920s and 1930s. When the stock market crash of 1929 reduced the demand for plays, Bogart turned to film. His first great success was as Duke Mantee in "The Petrified Forest" (1936), and this led to a period of typecasting as a gangster with films such as "Angels with Dirty Faces" (1938) and B-movies like "The Return of Doctor X" (1939).
Bogart's breakthrough as a leading man came in 1941, with "High Sierra" and "The Maltese Falcon." The next year, his performance in "Casablanca" raised him to the peak of his profession and, at the same time, cemented his trademark film persona, that of the hard-boiled cynic who ultimately shows his noble side. Other successes followed, including "To Have and Have Not" (1944); "The Big Sleep" (1946); "Dark Passage" (1947) and "Key Largo" (1948), with his wife Lauren Bacall; and "The Treasure of the Sierra Madre" (1948); "In a Lonely Place" (1950); "The African Queen" (1951), for which he won his only Oscar; "Sabrina" (1954); and "The Caine Mutiny" (1954). His last film was "The Harder They Fall" (1956). During a film career of almost 30 years, he appeared in 75 feature films.
Early life.
Bogart was born on Christmas Day, 1899, in New York City, the eldest child of Dr. Belmont DeForest Bogart (July 1867, Watkins Glen, New York – September 8, 1934, New York City) and Maud Humphrey (1868–1940). Belmont was the only child of the unhappy marriage of Adam Watkins Bogart, a Canandaigua, New York innkeeper, and his wife, Julia, a wealthy heiress. The name "Bogart" comes from the Dutch surname "Bogaert", which derives from the word "bogaard", short for "boomgaard", meaning "orchard". Belmont and Maud married in June 1898, he a Presbyterian of English and Dutch descent, she an Episcopalian of English heritage. Young Humphrey was raised in the Episcopal faith, but was non-practicing for most of his adult life.
The precise date of Bogart's birth was long a matter of dispute, but has been cleared up. Warner Bros listed his birthdate as Christmas Day, 1899, throughout his career; but film historian Clifford McCarty later maintained that the Warner publicity department had altered it from January 23, 1900 "...to foster the view that a man born on Christmas Day couldn't really be as villainous as he appeared to be on screen". The "corrected" January birthdate subsequently appeared—and in some cases, remains—in many otherwise authoritative sources. Biographers A.M. Sperber and Eric Lax documented, however, that Bogart always celebrated his birthday on December 25, and consistently listed it as such on official records, such as his marriage license. Lauren Bacall confirmed in her autobiography that his birthday was always celebrated on Christmas Day, adding that he joked that he was cheated out of a present every year because of it. Sperber and Lax also noted that a birth announcement, printed in the "Ontario County Times" on January 10, 1900, effectively rules out the possibility of a January 23 birthdate; and state and federal census records from 1900 report a Christmas 1899 birthdate as well.
Bogart's father, Belmont, was a cardiopulmonary surgeon. His mother, Maud, was a commercial illustrator who received her art training in New York and France, including study with James McNeill Whistler. Later she became art director of the fashion magazine "The Delineator" and a militant suffragette. She used a drawing of baby Humphrey in a well-known advertising campaign for Mellins Baby Food. In her prime, she made over $50,000 a year, then a vast sum and far more than her husband's $20,000. The Bogarts lived in a fashionable Upper West Side apartment, and had an elegant cottage on a 55-acre estate on Canandaigua Lake in upstate New York. As a youngster, Humphrey's gang of friends at the lake would put on theatricals.
Humphrey had two younger sisters, Frances ("Pat") and Catherine Elizabeth ("Kay"). His parents were busy in their careers and frequently fought. Very formal, they showed little emotion towards their children. Maud told her offspring to call her "Maud" not "Mother", and showed little if any physical affection for them. When pleased she "[c]lapped you on the shoulder, almost the way a man does", Bogart recalled. "I was brought up very unsentimentally but very straightforwardly. A kiss, in our family, was an event. Our mother and father didn't glug over my two sisters and me." As a boy, Bogart was teased for his curls, tidiness, "cute" pictures his mother had him pose for, the Little Lord Fauntleroy clothes she dressed him in—and the name "Humphrey". From his father, Bogart inherited a tendency to needle, fondness for fishing, lifelong love of boating, and an attraction to strong-willed women.
Bogart attended the private Delancey School until fifth grade, then the prestigious Trinity School. He was an indifferent, sullen student who showed no interest in after-school activities. Later he went to the equally elite boarding school Phillips Academy, where he was admitted based on family connections. His parents hoped he would go on to Yale, but in 1918 Bogart was expelled. Several reasons have been given: one claims that it was for throwing the headmaster (or a groundskeeper) into Rabbit Pond on campus. Another cites smoking, drinking, poor academic performance, and possibly some inappropriate comments made to the staff. A third has him withdrawn by his father for failing to improve his grades. Whatever caused his premature departure, his parents were deeply dismayed and rued their failed plans for his future.
Navy.
With no viable career options, Bogart followed his passion for the sea and enlisted in the United States Navy in the spring of 1918. He recalled later, "At eighteen, war was great stuff. Paris! Sexy French girls! Hot damn!" Bogart is recorded as a model sailor who spent most of his sea time after the Armistice ferrying troops back from Europe.
It was during his naval stint that Bogart may have received his trademark scar and developed his characteristic lisp, though the actual circumstances are unclear. In one account his lip was cut by shrapnel when his ship, the "USS "Leviathan"", was shelled, although some claim Bogart did not make it to sea until after the Armistice had been signed. Another version, which Bogart's long-time friend, author Nathaniel Benchley, holds to, is that Bogart was injured while taking a prisoner to Portsmouth Naval Prison in Kittery, Maine. Changing trains in Boston the handcuffed prisoner allegedly asked Bogart for a cigarette, then while Bogart looked for a match smashed him across the mouth with the cuffs, cutting Bogart's lip then fleeing. Recaptured, the prisoner was taken to jail. An alternate has Bogart struck in the mouth by a handcuff loosened while freeing his charge, the other still round the prisoner's wrist.
By the time Bogart was treated by a doctor, a scar had already formed. "Goddamn doctor", Bogart later told David Niven, "instead of stitching it up, he screwed it up." Niven says that when he asked Bogart about his scar, he said it was caused by a childhood accident; Niven claims the stories that Bogart got the scar during wartime were made up by the studios to inject glamor. His post-service physical makes no mention of the lip scar even though it mentions many smaller scars. When actress Louise Brooks met Bogart in 1924, he had some scar-tissue on his upper lip, which Brooks said that Bogart may have had partially repaired before entering films in 1930. She believed his scar had nothing to do with his distinctive speech pattern, his "lip wound gave him no speech impediment, either before or after it was mended. Over the years, Bogart practiced all kinds of lip gymnastics, accompanied by nasal tones, snarls, lisps and slurs. His painful wince, his leer, his fiendish grin were the most accomplished ever seen on film."
World War II.
During World War II, Bogart volunteered to travel to Africa with the USO to entertain the troops. He also joined the Coast Guard Temporary Reserve offering the use of his own yacht, Santana, for Coast Guard use. It was rumored Bogart attempted to enlist but was turned down because of his age.
Early career.
Bogart returned home to find his father suffering from poor health, his medical practice faltering, and much of the family's wealth lost on bad investments in timber. During his naval days, Bogart's character and values developed independently of family influence, and he began to rebel somewhat against their values. He came to be a liberal who hated pretensions, phonies, and snobs, and at times defied conventional behavior and authority, traits he displayed in both life and the movies. He did not, however, forsake good manners, articulateness, punctuality, modesty, and a dislike of being touched. After his naval service, he worked as a shipper and then bond salesman. He joined the Naval Reserve.
Bogart resumed his friendship with boyhood pal Bill Brady, Jr., whose father had show business connections. Eventually Bogart got an office job working for William A. Brady Sr.'s new company, World Films. Bogart was able to try his hand at screenwriting, directing, and production, but excelled at none. For a while he was stage manager for Brady's daughter Alice's play "A Ruined Lady". A few months later he made his stage debut as a Japanese butler in Alice's 1921 play "Drifting", nervously speaking one line of dialog. Several appearances followed in her subsequent plays.
While Bogart had been raised to believe that acting was beneath a gentleman, he liked the late hours actors kept and enjoyed the attention gotten on stage. He stated, "I was born to be indolent and this was the softest of rackets." He spent a lot of his free time in speakeasies and became a heavy drinker. A barroom brawl during this time joins the list of purported causes of Bogart's lip damage, and coincides better with the Brooks account.
Preferring to learn as he went, Bogart never took acting lessons. He was persistent and worked steadily at his craft, appearing in at least seventeen Broadway productions between 1922 and 1935. He played juveniles or romantic second-leads in drawing room comedies, and is said to have been the first actor to ask "Tennis, anyone?" on stage. Critic Alexander Woollcott wrote of Bogart's early work that he "is what is usually and mercifully described as inadequate." Some reviews were kinder. Heywood Broun, reviewing "Nerves" wrote, "Humphrey Bogart gives the most effective performance ... both dry and fresh, if that be possible". He played juvenile lead, reporter Gregory Brown, in the comedy "Meet the Wife", written by Lynn Starling, which had a successful run of 232 performances at the Klaw Theatre from November 1923 through July 1924. Bogart loathed these trivial, effeminate parts he had to play early in his career, calling them "White Pants Willie" roles.
Early in his career, while playing double roles in the play "Drifting" at the Playhouse Theatre in 1922, Bogart met actress Helen Menken. They were married on May 20, 1926, at the Gramercy Park Hotel in New York City. Divorced on November 18, 1927, remained friends. On April 3, 1928, he married Mary Philips, whom he'd met when they appeared in the play "Nerves" during its very brief run at the Comedy Theatre in September 1924, at her mother's apartment in Hartford, Connecticut. She, like Menken, had a fiery temper, and, like every other Bogart spouse, was an actress.
After the stock market crash of 1929, stage production dropped off sharply, and many of the more photogenic actors headed for Hollywood. Bogart's film debut was with Helen Hayes in the 1928 two-reeler "The Dancing Town", of which a complete copy has never been found. He also appeared with Joan Blondell and Ruth Etting in a Vitaphone short, "Broadway's Like That" (1930) which was re-discovered in 1963.
Bogart then signed a contract with Fox Film Corporation for $750 a week. There he met Spencer Tracy, a serious Broadway actor whom Bogart liked and admired, and they became close friends and drinking companions. It was Tracy, in 1930, who first called him "Bogie." Tracy made his film debut in the only film he and Bogart appeared together, John Ford's early sound film "Up the River" (1930). Both had major roles as inmates. Tracy received top billing and Bogart's face was featured on the film's posters instead of Tracy's. Bogart then had a minor supporting role in "The Bad Sister" with Bette Davis in 1931. Decades later, Tracy and Bogart planned to make "The Desperate Hours" together, but both sought top billing, so Tracy dropped out and was replaced by Fredric March.
Bogart shuttled back and forth between Hollywood and the New York stage from 1930 to 1935, suffering long periods without work. His parents had separated, his father dying in 1934 in debt, which Bogart eventually paid off. Bogart inherited his father's gold ring which he always wore, even in many of his films. At his father's deathbed, Bogart finally told him how much he loved him. His second marriage was on the rocks, and he was less than happy with his acting career. He became depressed, irritable, and drank heavily.
"The Petrified Forest".
Bogart starred in the Broadway play "Invitation to a Murder" at the Theatre Masque, now the John Golden Theatre, in 1934. The producer Arthur Hopkins heard the play from off-stage and sent for Bogart to play escaped murderer Duke Mantee in Robert E. Sherwood's new play, "The Petrified Forest". Hopkins recalled:
When I saw the actor I was somewhat taken aback, for he was the one I never much admired. He was an antiquated juvenile who spent most of his stage life in white pants swinging a tennis racquet. He seemed as far from a cold-blooded killer as one could get, but the voice (dry and tired) persisted, and the voice was Mantee's.
The play had 197 performances at the Broadhurst Theatre in New York in 1935. Leslie Howard, though, was the star. "New York Times" critic Brooks Atkinson said of the play, "a peach ... a roaring Western melodrama ... Humphrey Bogart does the best work of his career as an actor." Bogart said the play "marked my deliverance from the ranks of the sleek, sybaritic, stiff-shirted, swallow-tailed 'smoothies' to which I seemed condemned to life." However, he was still feeling insecure.
Warner Bros. bought the screen rights to "The Petrified Forest". The play seemed perfect for the studio, which was famous for its socially realistic, urban, low-budget action pictures, especially for a public entranced by real-life criminals like John Dillinger (whom Bogart resembled) and Dutch Schultz. Bette Davis and Leslie Howard were cast. Howard, who held production rights, made it clear he wanted Bogart to star with him. The studio tested several Hollywood veterans for the Duke Mantee role, and chose Edward G. Robinson, who had first-rank star appeal and was due to make a film to fulfill his expensive contract. Bogart cabled news of this to Howard in Scotland, who replied: "Att: Jack Warner Insist Bogart Play Mantee No Bogart No Deal L.H.". When Warner Bros. saw Howard would not budge, they gave in and cast Bogart. Jack Warner, famous for butting heads with his stars, tried to get Bogart to adopt a stage name, but Bogart stubbornly refused.
The film was highly successful, earning $500,000 at the box office, and making Bogart a star. He never forgot Howard's favor, and in 1952 named his only daughter "Leslie Howard Bogart" after Howard, who had died in World War II under mysterious circumstances. Robert E. Sherwood remained a close friend of Bogart's.
Early film career.
The film version of "The Petrified Forest" was released in 1936. Bogart's performance was called "brilliant", "compelling", and "superb." Despite his success in an "A movie," Bogart received a tepid twenty-six week contract at $550 per week and was typecast as a gangster in a series of "B movie" crime dramas. Bogart was proud of his success, but the fact that it came from playing a gangster weighed on him. He once said: "I can't get in a mild discussion without turning it into an argument. There must be something in my tone of voice, or this arrogant face—something that antagonizes everybody. Nobody likes me on sight. I suppose that's why I'm cast as the heavy." Bogart's roles were not only repetitive, but physically demanding and draining (studios were not yet air-conditioned), and his regimented, tightly scheduled job at Warners was anything but the indolent and "peachy" actor's life he hoped for. However, he was always professional and generally respected by other actors. He used these "B movie" years to start developing his enduring film persona—the wounded, stoical, cynical, charming, vulnerable, self-mocking loner with a code of honor.
In spite of his success, Warner Bros. had no interest in making Bogart a top star. Shooting on a new movie might begin days or only hours after the previous one wrapped. The studio system, then at its most entrenched, restricted actors to their home lot, with only occasional loan-outs. Any actor who refused a role could be suspended without pay. Bogart disliked the roles chosen for him, but he worked steadily. Between 1936 and 1940 he averaged a movie every two months, at times working on two simultaneously. Amenities at Warners were few compared to the prestigious MGM. Bogart thought that the Warners wardrobe department was cheap, and often wore his own suits in his movies. In "High Sierra", Bogart used his own pet dog Zero to play his character's dog, Pard. Bogart's disputes with Warner Bros. over roles and money were similar to those the studio waged with other high-spirited, less-than-obedient stars such as Bette Davis, James Cagney, Errol Flynn, and Olivia de Havilland.
The leading men ahead of Bogart at Warner Bros. included not only such marquee names as James Cagney and Edward G. Robinson, but also journeymen leads such as Victor McLaglen, George Raft and Paul Muni. Most of the studio's better movie scripts went to them, leaving Bogart with what was left. He made films like "Racket Busters", "San Quentin", and "You Can't Get Away with Murder". The only substantial leading role he got during this period was in "Dead End" (1937), while loaned to Samuel Goldwyn, where he portrayed a gangster modeled after Baby Face Nelson. He did play a variety of interesting supporting roles, such as in "Angels with Dirty Faces" (1938) (in which his character got shot by James Cagney's). Bogart was gunned down on film repeatedly by Cagney and Edward G. Robinson, among others. In "Black Legion" (1937), for a change, he played a good man caught up and destroyed by a racist organization, a movie Graham Greene described as "intelligent and exciting, if rather earnest".
In 1938, Warner Bros. put Bogart in a "hillbilly musical" called "Swing Your Lady" as a wrestling promoter; he later apparently considered this his worst film performance. In 1939, Bogart played a mad scientist in "The Return of Doctor X". He cracked, "If it'd been Jack Warner's blood ... I wouldn't have minded so much. The trouble was they were drinking mine and I was making this stinking movie." During this time his wife Mary had a stage hit in "A Touch of Brimstone" (1935), and refused to give up her Broadway career to go to Hollywood. After the play closed she relented, but insisted on continuing her career and the couple divorced in 1937.
On August 21, 1938, Bogart entered into a disastrous third marriage, with actress Mayo Methot, a lively, friendly woman when sober but paranoid and physical when drunk. She became convinced Bogart was cheating on her. The more the two drifted apart, the more she drank, in her fury throwing plants, crockery, anything close at hand, at him. She set their house on fire, stabbed him with a knife, and slashed her wrists on several occasions. Bogart for his part needled her mercilessly and seemed to enjoy confrontation. Sometimes he turned violent. The press accurately dubbed them "the Battling Bogarts." "The Bogart-Methot marriage was the sequel to the Civil War," said their friend Julius Epstein. A wag observed that there was "madness in his Methot." During this time, Bogart bought a motor launch, which he named "Sluggy," his nickname for hot-tempered Methot. Despite his proclamations that, "I like a jealous wife," "We get on so well together (because) we don't have illusions about each other," and, "I wouldn't give you two cents for a dame without a temper," it was a highly destructive relationship.
In California in 1945, Bogart bought a 55 ft sailing yacht, the "Santana", from actor Dick Powell. The sea was his sanctuary, spending about thirty weekends a year on the water, with a particular fondness for sailing around Catalina Island. He once said, "An actor needs something to stabilize his personality, something to nail down what he really is, not what he is currently pretending to be."
Bogart had a lifelong disgust for the pretentious, fake or phoney. Sensitive yet caustic, he was once again disgusted by the inferior movies he was performing in. He rarely saw his own films and avoided premieres. He even issued phony press releases about his private life to satisfy the curiosity of newspapers and the public. When he thought an actor, director, or a movie studio had done something shoddy, he spoke up about it and was willing to be quoted. He advised Robert Mitchum that the only way to stay alive in Hollywood was to be an "againster." As a result, he was not the most popular of actors, and some in the Hollywood community shunned him privately to avoid trouble with the studios. But the Hollywood press, unaccustomed to candor, was delighted. Bogart once said:
All over Hollywood, they are continually advising me, "Oh, you mustn't say that. That will get you in a lot of trouble," when I remark that some picture or writer or director or producer is no good. I don't get it. If he isn't any good, why can't you say so? If more people would mention it, pretty soon it might start having some effect."
Rise to stardom.
"High Sierra".
"High Sierra", a 1941 film directed by Raoul Walsh, had a screenplay written by Bogart's friend and drinking partner, John Huston, adapted from the novel by W. R. Burnett ("Little Caesar", etc.). Both Paul Muni and George Raft turned down the lead role, giving Bogart the opportunity to play a character of some depth, although legendary director Walsh initially fought the casting of supporting player Bogart as a leading man, much preferring Raft for the part. The film was Bogart's last major film playing a gangster (only a supporting role in 1942's "The Big Shot" followed). Bogart worked well with Ida Lupino, and her relationship with him was close, provoking jealousy from Bogart's wife, Mayo.
The film cemented a strong personal and professional connection between Bogart and Huston. Bogart admired and somewhat envied Huston for his skill as a writer. Though a poor student, Bogart was a lifelong reader. He could quote Plato, Pope, Ralph Waldo Emerson and over a thousand lines of Shakespeare. He subscribed to the "Harvard Law Review". He admired writers, and some of his best friends were screenwriters, including Louis Bromfield, Nathaniel Benchley and Nunnally Johnson. Bogart enjoyed intense, provocative conversation and stiff drinks, as did Huston. Both were rebellious and liked to play childish pranks. Huston was reported to be easily bored during production, and admired Bogart (also bored easily off camera) not just for his acting talent but for his intense concentration on the set.
"The Maltese Falcon".
Now regarded as a classic film noir, the "The Maltese Falcon" (1941) was John Huston's directorial debut. Originally a novel written by Dashiell Hammett, it was first published in the pulp magazine "Black Mask" in 1929, and had also served as the basis of another movie version, "Satan Met a Lady" (1936) starring Bette Davis. Producer Hal Wallis initially offered the leading man role to George Raft, a more established box office name than Bogart whose contract stipulated he did not have to appear in remakes. Fearing it would be no more than a cleaned-up version of the pre-Production Code "The Maltese Falcon" (1931), Raft turned it down. Eagerly, Huston accepted Bogart as his Sam Spade.
Complementing Bogart were co-stars Sydney Greenstreet, Peter Lorre, Elisha Cook, Jr., and Mary Astor as the treacherous female foil. Bogart's sharp timing and facial expressions were praised by the cast and director as vital to the quick action and rapid-fire dialogue. The film was a huge hit in theaters and a major triumph for Huston. Bogart was unusually happy with it, remarking, "it is practically a masterpiece. I don't have many things I'm proud of ... but that's one".
"Casablanca".
Bogart gained his first real romantic lead in 1942's "Casablanca", playing Rick Blaine, a hard-pressed expatriate nightclub owner hiding from a shady past while negotiating a fine line among Nazis, the French underground, the Vichy prefect and unresolved feelings for his ex-girlfriend. The film was directed by Michael Curtiz and produced by Hal Wallis, and featured Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre and Dooley Wilson. An avid chess player, Bogart reportedly had the idea that Rick Blaine be portrayed as one, a metaphor for the sparring relationship he maintained with friends, enemies, and tenuous allies. In real life Bogart played tournament level chess one division below master, often enjoying games with crew members and cast, but finding his better in the superior Paul Henreid.
The on-screen magic of Bogart and Bergman was the result of two actors working at their best, not any real-life sparks, though Bogart's perennially jealous wife assumed otherwise. Off the set, the co-stars hardly spoke. Bergman, who had a reputation for affairs with her leading men, later said of Bogart, "I kissed him but I never knew him." Because Bergman was taller, Bogart had 3 in blocks attached to his shoes in certain scenes.
"Casablanca" won the 1943 Academy Award for Best Picture. Bogart was nominated for Best Actor in a Leading Role, but lost to Paul Lukas for his performance in "Watch on the Rhine". The film vaulted Bogart from fourth place to first in the studio's roster, finally overtaking James Cagney. By 1946 he'd more than double his annual salary to over $460,000, making him the highest-paid actor in the world.
Bogart and Bacall.
"To Have and Have Not".
Bogart met Lauren Bacall while filming "To Have and Have Not" (1944), a loose adaptation of the Ernest Hemingway novel. The movie has many similarities with "Casablanca"—the same enemies, the same kind of hero, even a piano player sidekick (played by Hoagy Carmichael). When they met, Bacall was 19 and Bogart 44. He nicknamed her "Baby." She had been a model since 16 and had acted in two failed plays. Bogart was drawn to Bacall's high cheekbones, green eyes, tawny blond hair, and lean body, as well as her poise and earthy, outspoken honesty. Reportedly he said, "I just saw your test. We'll have a lot of fun together". Their physical and emotional rapport was very strong from the start, their age difference and disparity in acting experience allowing the dynamic of a mentor-student relationship to emerge. Quite contrary to Hollywood norm, their affair was Bogart's first with a leading lady. He was still miserably married and his early meetings with Bacall were discreet and brief, their separations bridged by ardent love letters. The relationship made it much easier for the newcomer to make her first film, and Bogart did his best to put her at ease with jokes and quiet coaching. He let her steal scenes and even encouraged it. Howard Hawks, for his part, also did his best to boost her performance and highlight her role, and found Bogart easy to direct.
Hawks at some point began to disapprove of the pair. He considered himself her protector and mentor, and Bogart was usurping that role. Married, and not usually drawn to his starlets, he too fell for Bacall, telling her she meant nothing to Bogart and even threatening to send her to Monogram, the worst studio in Hollywood. Bogart calmed her down and then went after Hawks. Jack Warner settled the dispute and filming resumed. Hawks said of Bacall: "Bogie fell in love with the character she played, so she had to keep playing it the rest of her life."
"The Big Sleep", "Dark Passage" and "Key Largo".
Just months after wrapping the film, Bogart and Bacall were reunited for an encore, the film noir "The Big Sleep", based on the novel by Raymond Chandler, again with script help from William Faulkner. Chandler thoroughly admired Bogart's performance: "Bogart can be tough without a gun. Also, he has a sense of humor that contains that grating undertone of contempt." The film was completed and slated for release in 1945, then withdrawn and substantially re-edited to add new, juiced-up scenes exploiting both the box office chemistry that shone between Bogart and Bacall in "To Have and Have Not" and the notoriety of their personal relationship. At director Howard Hawks' urging production partner Charles K. Feldman agreed to Bacall's scenes being re-written to heighten the 'insolent' quality that had intrigued critics and audiences in that film. By chance, a 35-mm nitrate composite master positive (fine grain) of the 1945 version survived. The UCLA Film Archive, in association with Turner Entertainment and with funding provided by Hugh Hefner, restored and released it in 1996.
Throughout filming Bogart was still torn between his new love and his sense of duty to his marriage. The mood on the set was tense, the actors both emotionally exhausted as Bogart tried to find a way out of his dilemma. The dialogue, especially in the newly shot scenes, was full of sexual innuendo supplied by Hawks, and Bogart proves convincing and enduring as private detective Philip Marlowe. In the end, the film was successful, though some critics found the plot confusing and overly complicated. Reportedly a bemused Chandler himself could not answer baffled screenwriters' question over who killed the limousine driver early in the story.
The suspenseful "Dark Passage" (1947) was Bogart and Bacall's next pairing. Its first third is shot from the Bogart's character's point of view, with the camera seeing what he sees. After his plastic surgery, the rest of the movie is shot normally, with Bogart intent on finding the real murderer in a crime he was blamed for and sentenced to prison.
The couple next starred in the future classic, "Key Largo". Directed by John Huston, the firm is highlighted Edward G. Robinson as gangster "Johnny Rocco," a seething older synthesis of many of his vicious early bad guy roles. The characters are trapped during a spectacular hurricane in a hotel owned by Bacall's screen father-in-law, played by Lionel Barrymore. Claire Trevor won an Academy Award for Best Supporting Actress for her heart-wrenching performance as Rocco's physically abused alcoholic girlfriend. Though Robinson had always had top billing over Bogart in their previous films together, this time Robinson's name appears to the right of Bogart's, but placed a little higher on the posters and in the film's opening credits, to signify Robinson's near-equal status. Robinson's image was also markedly larger and centered on the original poster, with Bogart relegated to the background. In the film's trailer, Bogart is repeatedly mentioned first but Robinson's name is listed above Bogart's in a cast list at the trailer's end. Robinson's role is evocative of Duke Mantee in "The Petrified Forest" (1936), a Bogart leading man breakthrough the studio had originally earmarked for Robinson.
Marriage.
Bogart filed for divorce from Mayo in February 1945. 
He and Bacall married in a small ceremony at the country home of Bogart's close friend, Pulitzer Prize-winning author Louis Bromfield, at Malabar Farm near Lucas, Ohio, on May 21, 1945.
Bogart and Bacall moved into a $160,000 ($ in 2010 dollars) white brick mansion in an exclusive neighborhood in LA's Holmby Hills. The marriage proved a happy one, though there were normal tensions due to differences. He was a homebody and she liked nightlife; he loved the sea, which made her seasick. Bogart's drinking sometimes inflamed tensions.
Bogart became a first-time father at age 49 when Bacall gave birth to Stephen Humphrey Bogart on January 6, 1949, during the filming of "Tokyo Joe". The name was drawn from Bogart's character's nickname in "To Have and Have Not", "Steve". Stephen would go on to become an author and biographer, later hosting a television special about his father on Turner Classic Movies. Three years later the couple's daughter, Leslie Howard Bogart, would draw her name from Bogart's "The Petrified Forest" co-star, British actor Leslie Howard.
Later career.
The enormous success of "Casablanca" redefined Bogart's career. For the first time, Bogart could be cast successfully as both a tough, strong man and vulnerable love interest. Despite his elevated standing, he did not yet have a contractual right of script refusal. When he got weak scripts he simply dug in his heels and locked horns again with the front office, as he did on the film "Conflict" (1945). Though he submitted to Jack Warner on it, he successfully turned down "God is My Co-Pilot" (1945). During part of 1943 and 1944, Bogart went on USO and War Bond tours accompanied by Mayo, enduring arduous travels to Italy and North Africa, including Casablanca.
"The Treasure of the Sierra Madre".
Riding high in 1947 with a new contract which provided limited script refusal and the right to form his own production company, Bogart reunited with John Huston for "The Treasure of the Sierra Madre", a stark tale of greed played out by three gold prospectors in Mexico. Without either a love interest or happy ending it was deemed a risky project. Bogart later said of co-star (and John Huston's father) Walter Huston, "He's probably the only performer in Hollywood to whom I'd gladly lost a scene".
The film was shot in the heat of summer for greater realism and atmosphere, proving grueling to make. James Agee wrote, "Bogart does a wonderful job with this character ... miles ahead of the very good work he has done before". John Huston won the Academy Award for direction and screenplay and his father won Best Supporting Actor, but the film had mediocre box office results. Bogart complained, "An intelligent script, beautifully directed—something different—and the public turned a cold shoulder on it".
House Un-American Activities Committee.
Bogart, a liberal Democrat, organized a delegation to Washington, D.C., called the Committee for the First Amendment, against what he perceived to be the House Un-American Activities Committee's harassment of Hollywood screenwriters and actors. He subsequently wrote an article "I'm No Communist" in the March 1948 edition of "Photoplay" magazine in which he distanced himself from The Hollywood Ten to counter the negative publicity resulting from his appearance. Bogart wrote: "The ten men cited for contempt by the House Un-American Activities Committee were not defended by us."
Santana Productions.
In addition to being offered better, more diverse roles, Bogart started his own production company in 1948, Santana Productions, named after his sailing yacht (which also loaned her name to the cabin cruiser featured in the climax of that year's smash, "Key Largo"). Earning the right to create his own production company had left Warner Bros. head Jack Warner furious, and afraid other stars would do the same and further erode the major studios' power. In addition to the pressure they were bearing from freelancing actors like Bogart, James Stewart, Henry Fonda and others, they were beginning to buckle from the eroding impact of television and enforcement of anti-trust laws breaking up theater chains. Bogart performed in his final films for Warners, "Chain Lightning", released early in 1950, and "The Enforcer", early in 1951.
Bogart's Santana Productions released its films through Columbia Pictures. Without letting up, Bogart starred in "Knock on Any Door" (1949), "Tokyo Joe" (1949), "In a Lonely Place" (1950), "Sirocco" (1951) and "Beat the Devil" (1954). Santana made two other films without him: "And Baby Makes Three" (1949) and "The Family Secret" (1951).
While the majority lost money at the box office, ultimately forcing Santana's sale, at least two are well remembered today: "In a Lonely Place" is considered by many a high point in film noir. Bogart plays embittered writer Dixon Steele, whose history of violence lands him as top suspect in a murder case. At the same time he falls in love with an alluring but failed actress played by Gloria Grahame. It is considered among his best performances, and many Bogart biographers and actress/writer Louise Brooks feel the role is the closest to the real Bogart of any he played. She wrote that the film "gave him a role that he could play with complexity, because the film character's pride in his art, his selfishness, drunkenness, lack of energy stabbed with lightning strokes of violence were shared by the real Bogart". The character even mimics some of Bogart's personal habits, including twice ordering Bogart's favorite meal of ham and eggs.
Something of a parody of "The Maltese Falcon", "Beat the Devil" (1953), was Bogart's last film with his close friend and favorite director John Huston. Co-written by Truman Capote, the eccentrically filmed tale follows an amoral group of rogues chasing an unattainable treasure., and enjoys a cult following
Bogart sold his interest in Santana to Columbia for over $1 million in 1955.
"The African Queen".
Working outside of his own Santana Productions, Bogart starred with Katharine Hepburn in the John Huston directed "The African Queen" in 1951. The novel it was based on was overlooked and left undeveloped for fifteen years until producer Sam Spiegel and Huston bought the rights. Spiegel sent Katharine Hepburn the book and she suggested Bogart for the male lead, firmly believing that "he was the only man who could have played that part". Huston's love of adventure, deep, longstanding friendship–and success–with Bogart, and a chance to work with Hepburn, convinced the actor to leave the comfortable confines of Hollywood for a difficult shoot on location in the Belgian Congo in Africa. Bogart was to get 30 percent of the profits and Hepburn 10 percent, plus a relatively small salary for both. The stars met up in London and announced the happy prospect of working together.
Bacall came for the four-plus month duration, leaving their young child to be cared for in L.A. The Bogarts started the trip with a junket through Europe, including a visit with Pope Pius XII. Later, the glamor would be gone and Bacall would make herself useful as a cook, nurse and clothes washer, earning her husband's praise: "I don't know what we'd have done without her. She Luxed my undies in darkest Africa". Just about everyone in the cast came down with dysentery except Bogart and Huston, who subsisted on canned food and alcohol. Bogart explained: "All I ate was baked beans, canned asparagus and Scotch whisky. Whenever a fly bit Huston or me, it dropped dead." Hepburn, a teetotaler in and out of character, fared worse in the difficult conditions, losing weight and at one point falling very ill. Bogart resisted Huston's insistence on using real leeches in a key scene where Bogart has to drag his tramp steamer through an infested marsh, until reasonable fakes were employed. In the end, the crew overcame illness, soldier ant invasions, leaking boats, poor food, attacking hippos, poor water filters, fierce heat, isolation, and a boat fire to complete a memorable film. Despite the discomfort of jumping from the boat into swamps, rivers and marshes the film apparently rekindled Bogart's early love of boats. On his return to California he bought a classic mahogany Hacker-Craft runabout, which he kept until his death.
The role of cantankerous skipper Charlie Allnutt won Bogart his only Academy Award in three nominations, for Best Actor in a Leading Role in 1951. Bogart considered his performance to be the best of his film career. He had vowed to friends that if he won, his speech would break the convention of thanking everyone in sight. He advised Claire Trevor, when she had been nominated for "Key Largo", to "just say you did it all yourself and don't thank anyone". But when Bogart won the Academy Award, which he truly coveted despite his well-advertised disdain for Hollywood, he said "It's a long way from the Belgian Congo to the stage of this theatre. It's nicer to be here. Thank you very much ... No one does it alone. As in tennis, you need a good opponent or partner to bring out the best in you. John and Katie helped me to be where I am now". Despite the thrilling win and the recognition, Bogart later commented, "The way to survive an Oscar is never to try to win another one ... too many stars ... win it and then figure they have to top themselves ... they become afraid to take chances. The result: A lot of dull performances in dull pictures".
"The African Queen" was the first Technicolor film in which Bogart appeared. He appeared in relatively few color films of any kind during the rest of his career, which continued for another five years.
Final roles.
Just three years after his Best Actor triumph in "African Queen", Bogart dropped his asking price to get the role of Captain Queeg in Edward Dmytryk's 1954 drama "The Caine Mutiny". Though he griped with some of his old bitterness about having to do so, he delivered a strong performance in the lead, earning him his final Oscar nomination. Yet for all his success, Bogart was still his melancholy old self, grumbling and feuding with the studio, while his health was beginning to deteriorate. The character of Queeg mirrored in some ways those Bogart had played in "The Maltese Falcon", "Casablanca" and "The Big Sleep"–the wary loner who trusts no one—but without either the warmth or humor of those roles. Like his portrayal of Fred C. Dobbs in "The Treasure of the Sierra Madre", Bogart played a paranoid, self-pitying character whose small-mindedness eventually destroyed him. Three months before the film's release, Bogart appeared as Queeg on the cover of "TIME" magazine, while on Broadway Henry Fonda was starring in the stage version (in a different role), both of which generated strong publicity for the film.
In "Sabrina", Billy Wilder wished to cast Cary Grant as the older male lead. Unable, he chose Bogart to play the elder, conservative brother who competes with his younger playboy sibling (William Holden) for the affection of the Cinderella-like Sabrina (Audrey Hepburn). Bogart was lukewarm about the part, but agreed to it on a handshake with Wilder, sans finished script but with the director's assurances he'd take good care of Bogart during the filming. Nevertheless, Bogart got on poorly with his director and co-stars. He complained about the script and its last-minute drafting and delivery, and accused Wilder of favoring Hepburn and Holden on and off the set. At the root was Wilder being the opposite of Bogart's ideal director, John Huston, in both style and personality. Bogart groused to the press that Wilder was "overbearing" and "is the kind of Prussian German with a riding crop. He is the type of director I don't like to work with ... the picture is a crock of crap. I got sick and tired of who gets Sabrina." Wilder later claimed, "We parted as enemies but finally made up." Despite the acrimony, the film was successful. "The New York Times" crowed that Bogart was "incredibly adroit ... the skill with which this old rock-ribbed actor blends the gags and such duplicities with a manly manner of melting is one of the incalculable joys of the show."
"The Barefoot Contessa", directed by Joseph Mankiewicz, was filmed in Rome, and released in 1954. In this Hollywood back-story Bogart is again a broken-down man, the cynical director-narrator who saves his career by making a star of a flamenco dancer modeled on real life movie sex goddess Rita Hayworth. Bogart was uneasy with Ava Gardner in the female lead, as she had just split from close "Rat-pack" buddy Frank Sinatra and was carrying on an affair with bullfighter Luis Miguel Dominguín. Bogart told her, "Half the world's female population would throw themselves at Frank's feet and here you are flouncing around with guys who wear capes and little ballerina slippers." He was also annoyed by her inexperienced performance. Later, Gardner credited Bogart with helping her both on and offscreen. Bogart's performance was generally praised as the strongest part of the film. During the filming, while Bacall was home, Bogart resumed his discreet affair with Verita Peterson, his long-time studio assistant, whom he took sailing and enjoyed drinking with. When his wife suddenly arrived on the scene discovering them together, she took it quite well, extracting an expensive shopping spree from her husband, the three traveling together after the shooting.
Bogart could be generous with actors, particularly those who were blacklisted, down on their luck, or having personal problems. During the filming of the Edward Dmytryk directed "The Left Hand of God" (1955), he noticed his co-star Gene Tierney having a hard time remembering her lines and behaving oddly. He coached Tierney, feeding her lines. He was familiar with mental illness from his sister's bouts of depression, and encouraged Tierney to seek treatment. He also stood behind Joan Bennett and insisted on her as his co-star in Michael Curtiz's "We're No Angels" when an ugly public scandal made her persona non grata with Jack Warner.
Bogart rounded out 1955 with "The Desperate Hours", directed by William Wyler. Mark Robson's "The Harder They Fall" (1956) was his last film.
Television and radio.
While Bogart rarely performed on television, he and Bacall appeared on Edward R. Murrow's "Person to Person" in which they disagreed in answering every question. Bogart was also featured on "The Jack Benny Show". The surviving kinescope of the live telecast captures him in his only TV sketch comedy outing.
Bogart and Bacall also worked together on an early color telecast in 1955, an NBC adaptation of "The Petrified Forest" for "Producers' Showcase", with Bogart receiving top billing and Henry Fonda playing Leslie Howard's role; a black and white kinescope of the live telecast has also survived.
Bogart performed radio adaptations of some of his best known films, such as "Casablanca" and "The Maltese Falcon". He also recorded a radio series called "Bold Venture" with Lauren Bacall.
In 1995 newly developed digital technology allowed Bogart's image to be inserted in the Tales from the Crypt television episode "You, Murderer" as one of its many Casablanca references. The "Ingrid Bergman" character was played by her daughter Isabella Rossellini.
Personal life.
The Rat Pack.
Bogart was a founding member and original leader of the so-called Hollywood Rat Pack. In the spring of 1955, after a long party in Las Vegas peopled with Frank Sinatra, Judy Garland, her husband Sid Luft, Mike Romanoff and wife Gloria, David Niven, Angie Dickinson and others, Lauren Bacall surveyed the wreckage and declared, "You look like a goddamn rat pack."
The name stuck and was made official at Romanoff's in Beverly Hills. Sinatra was tabbed Pack Leader, Bacall Den Mother, Bogie Director of Public Relations, and Sid Luft Acting Cage Manager. When asked by columnist Earl Wilson what the group's purpose was, Bacall stated "to drink a lot of bourbon and stay up late."
Death.
Once, after signing a long-term deal with Warner Bros., Bogart had predicted with glee that his teeth and hair would fall out before the contract ended. By the mid-1950s, well established as an independent producer, the sometimes actor's health was failing. In the wake of Santana Productions he'd formed a new company and had anxious plans for a film "Melville Goodwin, U.S.A.", in which he would play a general and Bacall a press magnate. However, his persistent cough and difficulty eating became too serious to ignore and he dropped the project.
Bogart, a heavy smoker and drinker, had developed cancer of the esophagus. He almost never spoke of his failing health and refused to see a doctor until January 1956. A diagnosis was made several weeks later, but by then removal of his esophagus, two lymph nodes, and a rib on March 1, 1956, was too late to halt the raging disease, even with chemotherapy. He underwent corrective surgery in November 1956 after the cancer had spread. With time, he grew too weak to walk up and down stairs, valiantly fighting the pain yet still able to joke: "Put me in the dumbwaiter and I'll ride down to the first floor in style." It was then altered to accommodate his wheelchair. Frank Sinatra was a frequent visitor, as were Katharine Hepburn and Spencer Tracy. In an interview, Hepburn described the last time she and Tracy saw their dear friend, on January 13, 1957:
Spence patted him on the shoulder and said, "Goodnight, Bogie." Bogie turned his eyes to Spence very quietly and with a sweet smile covered Spence's hand with his own and said, "Goodbye, Spence." Spence's heart stood still. He understood.
Bogart fell into a coma and died in his bed the next day. He had just turned 57 twenty days prior and weighed but 80 pounds (36 kg). His simple funeral was held at All Saints Episcopal Church, with musical selections from favorite composers Johann Sebastian Bach and Claude Debussy. The ceremony was attended by some of Hollywood's biggest stars, including Hepburn, Tracy, Judy Garland, David Niven, Ronald Reagan, James Mason, Bette Davis, Danny Kaye, Joan Fontaine, Marlene Dietrich, James Cagney, Errol Flynn, Gregory Peck and Gary Cooper, as well as Billy Wilder and Jack Warner. Bacall had asked Tracy to give the eulogy, but he was too upset, so John Huston spoke instead. He reminded the gathered mourners that while Bogart's life had ended far too soon, it had been a rich one:
Himself, he never took too seriously—his work most seriously. He regarded the somewhat gaudy figure of Bogart, the star, with an amused cynicism; Bogart, the actor, he held in deep respect ... In each of the fountains at Versailles there is a pike which keeps all the carp active; otherwise they would grow overfat and die. Bogie took rare delight in performing a similar duty in the fountains of Hollywood. Yet his victims seldom bore him any malice, and when they did, not for long. His shafts were fashioned only to stick into the outer layer of complacency, and not to penetrate through to the regions of the spirit where real injuries are done ... He is quite irreplaceable. There will never be another like him.
Bogart's cremated remains were interred in Forest Lawn Memorial Park Cemetery, Glendale, California. He was buried with a small, gold whistle once part of a charm bracelet he had given to Lauren Bacall before they married. On it was inscribed an allusion to a line from their first movie together, where Bacall had said to him shortly after their first meeting: "You know how to whistle don't you Steve? You just put your lips together and blow". The inscription read: "If you want anything, just whistle."
The probate value of Bogart's estate was $910,146 gross; $737,668 net.
Legacy and tributes.
After his death, a "Bogie Cult" formed at the Brattle Theatre in Cambridge, Massachusetts, as well as Greenwich Village, Manhattan, New York, and in France, which contributed to his spike in popularity in the late 1950s and 1960s. In 1997, "Entertainment Weekly" magazine named Bogart the number one movie legend of all time. In 1999, the American Film Institute ranked him the Greatest Male Star of All Time.
Jean-Luc Godard's "Breathless" (1960) was the first film to pay tribute to Bogart. Later, in Woody Allen's comic paen to Bogart, "Play It Again, Sam" (1972), Bogart's ghost comes to the aid of Allen's bumbling character, a movie critic with women troubles whose "sex life has turned into the 'Petrified Forest'".
Awards and honors.
On August 21, 1946, Bogart was honored in a ceremony at Grauman's Chinese Theater to record his hand and footprints in cement. On February 8, 1960, he was posthumously given a star on the Hollywood Walk of Fame at 6322 Hollywood Boulevard. During his career, Bogart was nominated for several awards including the BAFTA award for best foreign actor in 1952 for "The African Queen" and three Academy Awards.
In 1997, the United States Postal Service honored Bogart with a stamp bearing his image in its "Legends of Hollywood" series as the third figure to be recognized. At a formal ceremony attended by Lauren Bacall, and the Bogart children, Stephen and Leslie, Tirso del Junco, the chairman of the governing board of the USPS, provided an eloquent tribute:
"Today, we mark another chapter in the Bogart legacy. With an image that is small and yet as powerful as the ones he left in celluloid, we will begin today to bring his artistry, his power, his unique star quality, to the messages that travel the world."
On June 24, 2006, a section of 103rd Street, between Broadway and West End Avenue, in New York City was renamed "Humphrey Bogart Place." Lauren Bacall and her son Stephen Bogart were present at the commemorative event. "Bogie would never have believed it," Lauren Bacall expressed to the assembled group of city officials and onlookers in attendance.
In popular culture.
Humphrey Bogart's life has inspired writers and others:
Quotations.
Bogart is credited with five of the American Film Institute's top 100 quotations in American cinema, the most by any actor:
Bogart is also credited with one of the top movie misquotations, "Play it again, Sam". In "Casablanca", neither his Rick Blaine character nor anyone else says the line, although it is widely credited to him and is the verbatim title of a Woody Allen tribute movie.
When Blaine's former love, Ilsa (Ingrid Bergman), first enters his "Café Americain", she spots Sam, the piano player (Dooley Wilson), and asks him to "Play it once, Sam, for old times' sake." When he feigns ignorance, she persists, "Play it, Sam. Play 'As Time Goes By.'" Later that night, alone with Sam, Rick demands, "You played it for her - you can play it for me." Sam once again resists, prompting Blaine to shout: "If she can stand it, I can! Play it!"
References.
Bibliography.
</dl>

</doc>
<doc id="14051" url="http://en.wikipedia.org/wiki?curid=14051" title="History painting">
History painting

History painting is a genre in painting defined by its subject matter rather than artistic style. History paintings usually depict a moment in a narrative story, rather than a specific and static subject, as in a portrait. The term is derived from the wider senses of the word "historia" in Latin and Italian, and essentially means "story painting." In modern English, historical painting is sometimes used to describe the painting of scenes from history in its narrower sense (excluding religious, mythological and allegorical subjects, which are included in the broader term history painting), especially for 19th-century art. History paintings almost always contain a number of figures, often a large number. The genre includes depictions of moments in religious narratives, above all the "Life of Christ", as well as narrative scenes from mythology, and also allegorical scenes. These groups were for long the most frequently painted; works such as Michelangelo's Sistine Chapel ceiling are therefore history paintings, as are most very large paintings before the 19th century. The term covers large paintings in oil on canvas or fresco produced between the Renaissance and the late 19th century, after which the term is generally not used even for the many works that still meet the basic definition.
History painting may be used interchangeably with historical painting, and was especially so used before the 20th century. Where a distinction is made "historical painting" is the painting of scenes from secular history, whether specific episodes or generalized scenes. In the 19th century historical painting in this sense became a distinct genre. In phrases such as "historical painting materials", "historical" means in use before about 1900, or some earlier date.
History paintings were traditionally regarded as the highest form of Western painting, occupying the most prestigious place in the hierarchy of genres, and considered the equivalent to the epic in literature. In his "De Pictura" of 1436, Leon Battista Alberti had argued that multi-figure history painting was the noblest form of art, as being the most difficult, which required mastery of all the others, because it was a visual form of history, and because it had the greatest potential to move the viewer. He placed emphasis on the ability to depict the interactions between the figures by gesture and expression.
This view remained general until the 19th century, when artistic movements began to struggle against the establishment institutions of academic art, which continued to adhere to it. At the same time there was from the latter part of the 18th century an increased interest in depicting in the form of history painting moments of drama from recent or contemporary history, which had long largely been confined to battle-scenes and scenes of formal surrenders and the like. Scenes from ancient history had been popular in the early Renaissance, and once again became common in the Baroque and Rococo periods, and still more so with the rise of Neoclassicism. In some 19th or 20th century contexts, the term may refer specifically to paintings of scenes from secular history, rather than those from religious narratives, literature or mythology.
Development.
The term is generally not used in art history in speaking of medieval painting, although the Western tradition was developing in large altarpieces, fresco cycles, and other works, as well as miniatures in illuminated manuscripts. It comes to the fore in Italian Renaissance painting, where a series of increasingly ambitious works were produced, many still religious, but several, especially in Florence, which did actually feature near-contemporary historical scenes such as the set of three huge canvases on "The Battle of San Romano" by Paolo Uccello, the abortive "Battle of Cascina" by Michelangelo and the "Battle of Anghiari" by Leonardo da Vinci, neither of which were completed. Scenes from ancient history and mythology were also popular. Writers such as Alberti and the following century Giorgio Vasari in his "Lives of the Artists", followed public and artistic opinion in judging the best painters above all on their production of large works of history painting (though in fact the only modern (post-classical) work described in "De Pictura" is Giotto's huge "Navicella" in mosaic). Artists continued for centuries to strive to make their reputation by producing such works, often neglecting genres to which their talents were better suited.
There was some objection to the term, as many writers preferred terms such as "poetic painting" ("poesia"), or wanted to make a distinction between the "true" "istoria", covering history including biblical and religious scenes, and the "fabula", covering pagan myth, allegory, and scenes from fiction, which could not be regarded as true. The large works of Raphael were long considered, with those of Michelangelo, as the finest models for the genre. 
In the Raphael Rooms in the Vatican Palace, allegories and historical scenes are mixed together, and the Raphael Cartoons show scenes from the Gospels, all in the Grand Manner that from the High Renaissance became associated with, and often expected in, history painting. In the Late Renaissance and Baroque the painting of actual history tended to degenerate into panoramic battle-scenes with the victorious monarch or general perched on a horse accompanied with his retinue, or formal scenes of ceremonies, although some artists managed to make a masterpiece from such unpromising material, as Velázquez did with his "The Surrender of Breda".
An influential formulation of the hierarchy of genres, confirming the history painting at the top, was made in 1667 by André Félibien, a historiographer, architect and theoretician of French classicism became the classic statement of the theory for the 18th century:Celui qui fait parfaitement des païsages est au-dessus d'un autre qui ne fait que des fruits, des fleurs ou des coquilles. Celui qui peint des animaux vivants est plus estimable que ceux qui ne représentent que des choses mortes & sans mouvement ; & comme la figure de l'homme est le plus parfait ouvrage de Dieu sur la Terre, il est certain aussi que celui qui se rend l'imitateur de Dieu en peignant des figures humaines, est beaucoup plus excellent que tous les autres ... un Peintre qui ne fait que des portraits, n'a pas encore cette haute perfection de l'Art, & ne peut prétendre à l'honneur que reçoivent les plus sçavans. Il faut pour cela passer d'une seule figure à la représentation de plusieurs ensemble ; il faut traiter l'histoire & la fable ; il faut représenter de grandes actions comme les historiens, ou des sujets agréables comme les Poëtes ; & montant encore plus haut, il faut par des compositions allégoriques, sçavoir couvrir sous le voile de la fable les vertus des grands hommes, & les mystères les plus relevez.
He who produces perfect landscapes is above another who only produces fruit, flowers or seashells. He who paints living animals is more than those who only represent dead things without movement, and as man is the most perfect work of God on the earth, it is also certain that he who becomes an imitator of God in representing human figures, is much more excellent than all the others ... a painter who only does portraits still does not have the highest perfection of his art, and cannot expect the honour due to the most skilled. For that he must pass from representing a single figure to several together; history and myth must be depicted; great events must be represented as by historians, or like the poets, subjects that will please, and climbing still higher, he must have the skill to cover under the veil of myth the virtues of great men in allegories, and the mysteries they reveal".
By the late 18th century, with both religious and mytholological painting in decline, there was an increased demand for paintings of scenes from history, including contemporary history. This was in part driven by the changing audience for ambitious paintings, which now increasingly made their reputation in public exhibitions rather than by impressing the owners of and visitors to palaces and public buildings. Classical history remained popular, but scenes from national histories were often the best-received. From 1760 onwards, the Society of Artists of Great Britain, the first body to organize regular exhibitions in London, awarded two generous prizes each year to paintings of subjects from British history. 
The unheroic nature of modern dress was regarded as a serious difficulty. When, in 1770, Benjamin West proposed to paint "The Death of General Wolfe" in contemporary dress, he was firmly instructed to use classical costume by many people. He ignored these comments and showed the scene in modern dress. Although George III refused to purchase the work, West succeeded both in overcoming his critics' objections and inaugurating a more historically accurate style in such paintings. Other artists depicted scenes, regardless of when they occurred, in classical dress and for a long time, especially during the French Revolution, history painting often focused on depictions of the heroic male nude.
The large production, using the finest French artists, of propaganda paintings glorifying the exploits of Napoleon, were matched by works, showing both victories and losses, from the anti-Napoleonic alliance by artists such as Goya and J.M.W. Turner. Théodore Géricault's "The Raft of the Medusa" (1818–1819) was a sensation, appearing to update the history painting for the 19th century, and showing anonymous figures famous only for being victims of what was then a famous and controversial disaster at sea. Conveniently their clothes had been worn away to classical-seeming rags by the point the painting depicts. At the same time the demand for traditional large religious history paintings very largely fell away.
In the mid-nineteenth century there arose a style known as historicism, which marked a formal imitation of historical styles and/or artists. Another development in the nineteenth century was the treatment of historical subjects, often on a large scale, with the values of genre painting, the depiction of scenes of everyday life, and anecdote. Grand depictions of events of great public importance were supplemented with scenes depicting more personal incidents in the lives of the great, or of scenes centred on unnamed figures involved in historical events, as in the Troubadour style. At the same time scenes of ordinary life with moral, political or satirical content became often the main vehicle for expressive interplay between figures in painting, whether given a modern or historical setting.
By the later 19th century, history painting was often explicitly rejected by avant-garde movements such as the Impressionists (except for Édouard Manet) and the Symbolists, and according to one recent writer "Modernism was to a considerable extent built upon the rejection of History Painting... All other genres are deemed capable of entering, in one form or another, the 'pantheon' of modernity considered, but History Painting is excluded".
History painting and historical painting.
The terms.
Initially, "history painting" and "historical painting" were used interchangeably in English, as when Sir Joshua Reynolds in his fourth "Discourse" uses both indiscriminately to cover "history painting", while saying "...it ought to be called poetical, as in reality it is", reflecting the French term "peinture historique", one equivalent of "history painting". The terms began to separate in the 19th century, with "historical painting" becoming a sub-group of "history painting" restricted to subjects taken from history in its normal sense. In 1853 John Ruskin asked his audience: "What do you at present "mean" by historical painting? Now-a-days it means the endeavour, by the power of imagination, to portray some historical event of past days." So for example Harold Wethey's three-volume catalogue of the paintings of Titian (Phaidon, 1969–75) is divided between "Religious Paintings", "Portraits", and "Mythological and Historical Paintings", though both volumes I and III cover what is included in the term "History Paintings". This distinction is useful but is by no means generally observed, and the terms are still often used in a confusing manner. Because of the potential for confusion modern academic writing tends to avoid the phrase "historical painting", talking instead of "historical subject matter" in history painting, but where the phrase is still used in contemporary scholarship it will normally mean the painting of subjects from history, very often in the 19th century. "Historical painting" may also be used, especially in discussion of painting techniques in conservation studies, to mean "old", as opposed to modern or recent painting.
In 19th-century British writing on art the terms "subject painting" or "anecdotic" painting were often used for works in a line of development going back to William Hogarth of monoscenic depictions of crucial moments in an implied narrative with unidentified characters, such as William Holman Hunt's 1853 painting "The Awakening Conscience" or Augustus Egg's "Past and Present", a set of three paintings, updating sets by Hogarth such as "Marriage à-la-mode".
19th century.
History painting was the dominant form of academic painting in the various national academies in the 18th century, and for most of the 19th, and increasingly historical subjects dominated. During the Revolutionary and Napoleonic periods the heroic treatment of contemporary history in a frankly propagandistic fashion by Antoine-Jean, Baron Gros, Jacques-Louis David, Carle Vernet and others was supported by the French state, but after the fall of Napoleon in 1815 the French governments were not regarded as suitable for heroic treatment and many artists retreated further into the past to find subjects, though in Britain depicting the victories of the Napoleonic Wars mostly occurred after they were over. Another path was to choose contemporary subjects that were oppositional to government either at home and abroad, and many of what were arguably the last great generation of history paintings were protests at contemporary episodes of repression or outrages at home or abroad: Goya's "The Third of May 1808" (1814), Théodore Géricault's "The Raft of the Medusa" (1818–19), Eugène Delacroix's "The Massacre at Chios" (1824) and "Liberty Leading the People" (1830). These were heroic, but showed heroic suffering by ordinary civilians.
Romantic artists such as Géricault and Delacroix, and those from other movements such as the English Pre-Raphaelite Brotherhood continued to regard history painting as the ideal for their most ambitious works. Others such as Jan Matejko in Poland, Vasily Surikov in Russia, and Paul Delaroche in France became specialized painters of large historical subjects. The "style troubadour" ("troubadour style") was a somewhat derisive French term for earlier paintings of medieval and Renaissance scenes, which were often small and depicting moments of anecdote rather than drama; Ingres, Richard Parkes Bonington and Henri Fradelle painted such works. Sir Roy Strong calls this type of work the "Intimate Romantic", and in French it was known as the "peinture de genre historique" or "peinture anecdotique" ("historical genre painting" or "anecdotal painting").
Church commissions for large group scenes from the Bible had greatly reduced, and historical painting became very significant. Especially in the early 19th century, much historical painting depicted specific moments from historical literature, with the novels of Sir Walter Scott a particular favourite, in France and other European countries as much as Great Britain. By the middle of the century medieval scenes were expected to be very carefully researched, using the work of historians of costume, architecture and all elements of decor that were becoming available. The provision of examples and expertise for artists, as well as revivalist industrial designers, was one of the motivations for the establishment of museums like the Victoria and Albert Museum in London. New techniques of printmaking such as the chromolithograph made good quality monochrome print reproductions both relatively cheap and very widely accessible, and also hugely profitable for artist and publisher, as the sales were so large. Historical painting often had a close relationship with Nationalism, and painters like Matejko in Poland could play an important role in fixing the prevailing historical narrative of national history in the popular mind. In France, "L'art Pompier" ("Fireman art") was a derisory term for official academic historical painting, and in a final phase, "History painting of a debased sort, scenes of brutality and terror, purporting to illustrate episodes from Roman and Moorish history, were Salon sensations. On the overcrowded walls of the exhibition galleries, the paintings that shouted loudest got the attention". Orientalist painting was an alternative genre that offered similar exotic costumes and decor, and at least as much opportunity to depict sex and violence.

</doc>
<doc id="14052" url="http://en.wikipedia.org/wiki?curid=14052" title="Hyperbola">
Hyperbola

In mathematics, a hyperbola (plural "hyperbolas" or "hyperbolae") is a type of smooth curve, lying in a plane, defined by its geometric properties or by equations for which it is the solution set. A hyperbola has two pieces, called connected components or branches, that are mirror images of each other and resemble two infinite bows. The hyperbola is one of the four kinds of conic section, formed by the intersection of a plane and a double cone. (The other conic sections are the parabola, the ellipse, and the circle; the circle is a special case of the ellipse). If the plane intersects both halves of the double cone but does not pass through the apex of the cones, then the conic is a hyperbola.
Hyperbolas arise in many ways: as the curve representing the function formula_1 in the Cartesian plane, as the appearance of a circle viewed from within it, as the path followed by the shadow of the tip of a sundial, as the shape of an open orbit (as distinct from a closed elliptical orbit), such as the orbit of a spacecraft during a gravity assisted swing-by of a planet or more generally any spacecraft exceeding the escape velocity of the nearest planet, as the path of a single-apparition comet (one travelling too fast ever to return to the solar system), as the scattering trajectory of a subatomic particle (acted on by repulsive instead of attractive forces but the principle is the same), and so on.
Each branch of the hyperbola has two arms which become straighter (lower curvature) further out from the center of the hyperbola. Diagonally opposite arms, one from each branch, tend in the limit to a common line, called the asymptote of those two arms. So there are two asymptotes, whose intersection is at the center of symmetry of the hyperbola, which can be thought of as the mirror point about which each branch reflects to form the other branch. In the case of the curve formula_1 the asymptotes are the two coordinate axes.
Hyperbolas share many of the ellipses' analytical properties such as eccentricity, focus, and directrix. Typically the correspondence can be made with nothing more than a change of sign in some term. Many other mathematical objects have their origin in the hyperbola, such as hyperbolic paraboloids (saddle surfaces), hyperboloids ("wastebaskets"), hyperbolic geometry (Lobachevsky's celebrated non-Euclidean geometry), hyperbolic functions (sinh, cosh, tanh, etc.), and gyrovector spaces (a geometry used in both relativity and quantum mechanics which is not Euclidean).
History.
The word "hyperbola" derives from the Greek ὑπερβολή, meaning "over-thrown" or "excessive", from which the English term hyperbole also derives. Hyperbolae were discovered by Menaechmus in his investigations of the problem of doubling the cube, but were then called sections of obtuse cones. The term hyperbola is believed to have been coined by Apollonius of Perga (c. 262–c. 190 BC) in his definitive work on the conic sections, the "Conics". For comparison, the other two general conic sections, the ellipse and the parabola, derive from the corresponding Greek words for "deficient" and "comparable"; these terms may refer to the eccentricity of these curves, which is greater than one (hyperbola), less than one (ellipse) and exactly one (parabola).
Nomenclature and features.
Similar to a parabola, a hyperbola is an open curve, meaning that it continues indefinitely to infinity, rather than closing on itself as an ellipse does. A hyperbola consists of two disconnected curves called its arms or branches.
The points on the two branches that are closest to each other are called the vertices; they are the points where the curve has its smallest radius of curvature. The line segment connecting the vertices is called the "transverse axis" or "major axis", corresponding to the major diameter of an ellipse. The midpoint of the transverse axis is known as the hyperbola's "center". The distance "a" from the center to each vertex is called the semi-major axis. Outside of the transverse axis but on the same line are the two "focal points (foci)" of the hyperbola. The line through these five points is one of the two principal axes of the hyperbola, the other being the perpendicular bisector of the transverse axis. The hyperbola has mirror symmetry about its principal axes, and is also symmetric under a 180° turn about its center.
At large distances from the center, the hyperbola approaches two lines, its asymptotes, which intersect at the hyperbola's center. A hyperbola approaches its asymptotes arbitrarily closely as the distance from its center increases, but it never intersects them; however, a degenerate hyperbola consists only of its asymptotes. Consistent with the symmetry of the hyperbola, if the transverse axis is aligned with the "x"-axis of a Cartesian coordinate system, the slopes of the asymptotes are equal in magnitude but opposite in sign, ±b⁄a, where "b"="a"×tan(θ) and where θ is the angle between the transverse axis and either asymptote. The distance "b" (not shown) is the length of the perpendicular segment from either vertex to the asymptotes.
A "conjugate axis" of length 2"b", corresponding to the "minor axis" of an ellipse, is sometimes drawn on the non-transverse principal axis; its endpoints ±b lie on the minor axis at the height of the asymptotes over/under the hyperbola's vertices. Because of the minus sign in some of the formulas below, it is also called the "imaginary axis" of the hyperbola.
If , the angle 2θ between the asymptotes equals 90° and the hyperbola is said to be "rectangular" or "equilateral". In this special case, the rectangle joining the four points on the asymptotes directly above and below the vertices is a square, since the lengths of its sides "2a" = "2b".
If the transverse axis of any hyperbola is aligned with the "x"-axis of a Cartesian coordinate system and is centered on the origin, the equation of the hyperbola can be written as
A hyperbola aligned in this way is called an "East-West opening hyperbola". Likewise, a hyperbola with its transverse axis aligned with the "y"-axis is called a "North–South opening hyperbola" and has equation
Every hyperbola is congruent to the origin-centered East-West opening hyperbola sharing its same eccentricity ε (its shape, or degree of "spread"), and is also congruent to the origin-centered North–South opening hyperbola with identical eccentricity ε — that is, it can be rotated so that it opens in the desired direction and can be translated (rigidly moved in the plane) so that it is centered at the origin. For convenience, hyperbolas are usually analyzed in terms of their centered East-West opening form.
If formula_5 is the distance from the center to either focus, then formula_6.
The shape of a hyperbola is defined entirely by its eccentricity ε, which is a dimensionless number always greater than one. The distance "c" from the center to the foci equals "a"ε. The eccentricity can also be defined as the ratio of the distances to either focus and to a corresponding line known as the directrix; hence, the distance from the center to the directrices equals "a"/ε. In terms of the parameters "a", "b", "c" and the angle θ, the eccentricity equals
For example, the eccentricity of a rectangular hyperbola , equals the square root of two: ε =  formula_8.
Every hyperbola has a "conjugate hyperbola", in which the transverse and conjugate axes are exchanged without changing the asymptotes. The equation of the conjugate hyperbola of formula_9 is formula_10. If the graph of the conjugate hyperbola is rotated 90° to restore the east-west opening orientation (so that "x" becomes "y" and vice versa), the equation of the resulting rotated conjugate hyperbola is the same as the equation of the original hyperbola except with "a" and "b" exchanged. For example, the angle θ of the conjugate hyperbola equals 90° minus the angle of the original hyperbola. Thus, the angles in the original and conjugate hyperbolas are complementary angles, which implies that they have different eccentricities unless θ = 45° (a rectangular hyperbola). Hence, the conjugate hyperbola does "not" in general correspond to a 90° rotation of the original hyperbola; the two hyperbolas are generally different in shape.
A few other lengths are used to describe hyperbolas. Consider a line perpendicular to the transverse axis (i.e., parallel to the conjugate axis) that passes through one of the hyperbola's foci. The line segment connecting the two intersection points of this line with the hyperbola is known as the "latus rectum" and has a length formula_11. The "semi-latus rectum" "l" is half of this length, i.e., formula_12. The "focal parameter" "p" is the distance from a focus to its corresponding directrix, and equals formula_13.
Mathematical definitions.
A hyperbola can be defined mathematically in several equivalent ways.
Conic section.
A hyperbola may be defined as the curve of intersection between a right circular conical surface and a plane that cuts through both halves of the cone. The other major types of conic sections are the ellipse and the parabola; in these cases, the plane cuts through only one half of the double cone. If the plane passes through the central apex of the double cone a degenerate hyperbola results — two straight lines that cross at the apex point.
Difference of distances to foci.
A hyperbola may be defined equivalently as the locus of points where the absolute value of the "difference" of the distances to the two foci is a constant equal to 2"a", the distance between its two vertices. This definition accounts for many of the hyperbola's applications, such as multilateration; this is the problem of determining position from the "difference" in arrival times of synchronized signals, as in GPS.
This definition may be expressed also in terms of tangent circles. The center of any circles externally tangent to two given circles lies on a hyperbola, whose foci are the centers of the given circles and where the vertex distance 2"a" equals the difference in radii of the two circles. As a special case, one given circle may be a point located at one focus; since a point may be considered as a circle of zero radius, the other given circle—which is centered on the other focus—must have radius 2"a". This provides a simple technique for constructing a hyperbola, as shown below. It follows from this definition that a tangent line to the hyperbola at a point P bisects the angle formed with the two foci, i.e., the angle F1P F2. Consequently, the feet of perpendiculars drawn from each focus to such a tangent line lies on a circle of radius "a" that is centered on the hyperbola's own center.
A proof that this characterization of the hyperbola is equivalent to the conic-section characterization can be done without coordinate geometry by means of Dandelin spheres.
Directrix and focus.
A hyperbola can be defined as the locus of points for which the ratio of the distances to one focus and to a line (called the directrix) is a constant formula_14 that is larger than 1. This constant is the eccentricity of the hyperbola. The eccentricity equals the secant of half the angle between the asymptotes of the hyperbola, so the eccentricity of the hyperbola xy = 1 equals the square root of 2.
By symmetry a hyperbola has two directrices, which are parallel to the conjugate axis and are between it and the tangent to the hyperbola at a vertex. One directrix and its focus is enough to produce both arms of the hyperbola.
Reciprocation of a circle.
The reciprocation of a circle "B" in a circle "C" always yields a conic section such as a hyperbola. The process of "reciprocation in a circle "C"" consists of replacing every line and point in a geometrical figure with their corresponding pole and polar, respectively. The "pole" of a line is the inversion of its closest point to the circle "C", whereas the polar of a point is the converse, namely, a line whose closest point to "C" is the inversion of the point.
The eccentricity of the conic section obtained by reciprocation is the ratio of the distances between the two circles' centers to the radius "r" of reciprocation circle "C". If B and C represent the points at the centers of the corresponding circles, then
Since the eccentricity of a hyperbola is always greater than one, the center B must lie outside of the reciprocating circle "C".
This definition implies that the hyperbola is both the locus of the poles of the tangent lines to the circle "B", as well as the envelope of the polar lines of the points on "B". Conversely, the circle "B" is the envelope of polars of points on the hyperbola, and the locus of poles of tangent lines to the hyperbola. Two tangent lines to "B" have no (finite) poles because they pass through the center C of the reciprocation circle "C"; the polars of the corresponding tangent points on "B" are the asymptotes of the hyperbola. The two branches of the hyperbola correspond to the two parts of the circle "B" that are separated by these tangent points.
Quadratic equation.
A hyperbola can also be defined as a second-degree equation in the Cartesian coordinates ("x", "y") of the plane
provided that the constants "A""xx", "A""xy", "A""yy", "B""x", "B""y", and "C" satisfy the determinant condition
A special case of a hyperbola—the "degenerate hyperbola" consisting of two intersecting lines—occurs when another determinant is zero
This determinant Δ is sometimes called the discriminant of the conic section.
Given the above general parametrization of the hyperbola in Cartesian coordinates, the eccentricity can be found using the formula in Conic section#Eccentricity in terms of parameters of the quadratic form.
The center ("x""c", "y""c") of the hyperbola may be determined from the formulae
In terms of new coordinates, and , the defining equation of the hyperbola can be written
The principal axes of the hyperbola make an angle Φ with the positive "x"-axis that equals
Rotating the coordinate axes so that the "x"-axis is aligned with the transverse axis brings the equation into its canonical form
The major and minor semiaxes "a" and "b" are defined by the equations
where λ1 and λ2 are the roots of the quadratic equation
For comparison, the corresponding equation for a degenerate hyperbola is
The tangent line to a given point ("x"0, "y"0) on the hyperbola is defined by the equation
where "E", "F" and "G" are defined
The normal line to the hyperbola at the same point is given by the equation
The normal line is perpendicular to the tangent line, and both pass through the same point ("x"0, "y"0).
From the equation
the basic property that with formula_34 and formula_35 being the distances from a point formula_36 to the left focus formula_37 and the right focus formula_38 one has for a point on the right branch that
and for a point on the left branch that
can be proved as follows:
If x,y is a point on the hyperbola the distance to the left focal point is
To the right focal point the distance is
If x,y is a point on the right branch of the hyperbola then formula_43 and
Subtracting these equations one gets
If x,y is a point on the left branch of the hyperbola then formula_47 and
Subtracting these equations one gets
True anomaly.
In the section above it is shown that using the coordinate system in which the equation of the hyperbola takes its canonical form
the distance formula_52 from a point formula_53 on the left branch of the hyperbola to the left focal point formula_54 is
Introducing polar coordinates formula_56 with origin at the left focal point the coordinates relative the canonical coordinate system are
and the equation above takes the form
from which follows that
This is the representation of the near branch of a hyperbola in polar coordinates with respect to a focal point.
The polar angle formula_61 of a point on a hyperbola relative the near focal point as described above is called the true anomaly of the point.
Geometrical constructions.
Similar to the ellipse, a hyperbola can be constructed using a taut thread. A straightedge of length "S" is attached to one focus F1 at one of its corners A so that it is free to rotate about that focus. A thread of length "L" = "S" - 2"a" is attached between the other focus F2 and the other corner B of the straightedge. A sharp pencil is held up against the straightedge, sandwiching the thread tautly against the straightedge. Let the position of the pencil be denoted as P. The total length "L" of the thread equals the sum of the distances "L"2 from F2 to P and "L"B from P to B. Similarly, the total length "S" of the straightedge equals the distance "L"1 from F1 to P and "L"B. Therefore, the difference in the distances to the foci, "L"1 − "L"2 equals the constant 2"a"
A second construction uses intersecting circles, but is likewise based on the constant difference of distances to the foci. Consider a hyperbola with two foci F1 and F2, and two vertices P and Q; these four points all lie on the transverse axis. Choose a new point T also on the transverse axis and to the right of the rightmost vertex P; the difference in distances to the two vertices, QT − PT = 2"a", since 2"a" is the distance between the vertices. Hence, the two circles centered on the foci F1 and F2 of radius QT and PT, respectively, will intersect at two points of the hyperbola.
A third construction relies on the definition of the hyperbola as the reciprocation of a circle. Consider the circle centered on the center of the hyperbola and of radius "a"; this circle is tangent to the hyperbola at its vertices. A line "g" drawn from one focus may intersect this circle in two points M and N; perpendiculars to "g" drawn through these two points are tangent to the hyperbola. Drawing a set of such tangent lines reveals the envelope of the hyperbola.
A fourth construction is using the parallelogram method. It is similar to such method for parabola and ellipse construction: certain equally spaced points lying on parallel lines are connected with each other by two straight lines and their intersection point lies on the hyperbola.
Reflections and tangent lines.
The ancient Greek geometers recognized a reflection property of hyperbolas. If a ray of light emerges from one focus and is reflected from either branch of the hyperbola, the light-ray appears to have come from the other focus. Equivalently, by reversing the direction of the light, rays directed at one of the foci are reflected towards the other focus. This property is analogous to the property of ellipses that a ray emerging from one focus is reflected from the ellipse directly "towards" the other focus (rather than "away" as in the hyperbola). Expressed mathematically, lines drawn from each focus to the same point on the hyperbola intersect it at equal angles; the tangent line to a hyperbola at a point P bisects the angle formed with the two foci, F1PF2.
Tangent lines to a hyperbola have another remarkable geometrical property. If a tangent line at a point T intersects the asymptotes at two points K and L, then T bisects the line segment KL, and the product of distances to the hyperbola's center, OK×OL is a constant.
Hyperbolic functions and equations.
Just as the sine and cosine functions give a parametric equation for the ellipse, so the hyperbolic sine and hyperbolic cosine give a parametric equation for the hyperbola.
As
formula_63
one has for any hyperbolic angle formula_64 that the point
satisfies the equation
which is the equation of a hyperbola relative its canonical coordinate system.
When "μ" varies over the interval formula_68 one gets with this formula all points formula_69 on the right branch of the hyperbola.
The left branch for which formula_70 is in the same way obtained as
In the figure the points formula_73 given by
for
on the left branch of a hyperbola with eccentricity 1.2 are marked as dots.
Relation to other conic sections.
There are three major types of conic sections: hyperbolas, ellipses and parabolas. Since the parabola may be seen as a limiting case poised exactly between an ellipse and a hyperbola, there are effectively only two major types, ellipses and hyperbolas. These two types are related in that formulae for one type can often be applied to the other.
The canonical equation for a hyperbola is
Any hyperbola can be rotated so that it is east-west opening and positioned with its center at the origin, so that the equation describing it is this canonical equation.
The canonical equation for the hyperbola may be seen as a version of the corresponding ellipse equation
in which the semi-minor axis length "b" is imaginary. That is, if in the ellipse equation "b" is replaced by "ib" where "b" is real, one obtains the hyperbola equation.
Similarly, the parametric equations for a hyperbola and an ellipse are expressed in terms of hyperbolic and trigonometric functions, respectively, which are again related by an imaginary circular angle, for example,
Hence, many formulae for the ellipse can be extended to hyperbolas by adding the imaginary unit "i" in front of the semi-minor axis "b" and the angle. For example, the arc length of a segment of an ellipse can be determined using an incomplete elliptic integral of the second kind. The corresponding arclength of a hyperbola is given by the same function with imaginary parameters "b" and μ, namely, "ib E(iμ, c)".
Conic section analysis of the hyperbolic appearance of circles.
Besides providing a uniform description of circles, ellipses, parabolas, and hyperbolas, conic sections can also be understood as a natural model of the geometry of perspective in the case where the scene being viewed consists of a circle, or more generally an ellipse. The viewer is typically a camera or the human eye. In the simplest case the viewer's lens is just a pinhole; the role of more complex lenses is merely to gather far more light while retaining as far as possible the simple pinhole geometry in which all rays of light from the scene pass through a single point. Once through the lens, the rays then spread out again, in air in the case of a camera, in the vitreous humor in the case of the eye, eventually distributing themselves over the film, imaging device, or retina, all of which come under the heading of image plane. The lens plane is a plane parallel to the image plane at the lens; all rays pass through a single point on the lens plane, namely the lens itself.
When the circle directly faces the viewer, the viewer's lens is on-axis, meaning on the line normal to the circle through its center (think of the axle of a wheel). The rays of light from the circle through the lens to the image plane then form a cone with circular cross section whose apex is the lens. The image plane concretely realizes the abstract cutting plane in the conic section model.
When in addition the viewer directly faces the circle, the circle is rendered faithfully on the image plane without perspective distortion, namely as a scaled-down circle. When the viewer turns attention or gaze away from the center of the circle the image plane then cuts the cone in an ellipse, parabola, or hyperbola depending on how far the viewer turns, corresponding exactly to what happens when the surface cutting the cone to form a conic section is rotated.
A parabola arises when the lens plane is tangent to (touches) the circle. A viewer with perfect 180-degree wide-angle vision will see the whole parabola; in practice this is impossible and only a finite portion of the parabola is captured on the film or retina.
When the viewer turns further so that the lens plane cuts the circle in two points, the shape on the image plane becomes that of a hyperbola. The viewer still sees only a finite curve, namely a portion of one branch of the hyperbola, and is unable to see the second branch at all, which corresponds to the portion of the circle behind the viewer, more precisely, on the same side of the lens plane as the viewer. In practice the finite extent of the image plane makes it impossible to see any portion of the circle near where it is cut by the lens plane. Further back however one could imagine rays from the portion of the circle well behind the viewer passing through the lens, were the viewer transparent. In this case the rays would pass through the image plane before the lens, yet another impracticality ensuring that no portion of the second branch could possibly be visible.
The tangents to the circle where it is cut by the lens plane constitute the asymptotes of the hyperbola. Were these tangents to be drawn in ink in the plane of the circle, the eye would perceive them as asymptotes to the visible branch. Whether they converge in front of or behind the viewer depends on whether the lens plane is in front of or behind the center of the circle respectively.
If the circle is drawn on the ground and the viewer gradually transfers gaze from straight down at the circle up towards the horizon, the lens plane eventually cuts the circle producing first a parabola then a hyperbola on the image plane as shown in Figure 10. As the gaze continues to rise the asymptotes of the hyperbola, if realized concretely, appear coming in from left and right, swinging towards each other and converging at the horizon when the gaze is horizontal. Further elevation of the gaze into the sky then brings the point of convergence of the asymptotes towards the viewer.
By the same principle with which the back of the circle appears on the image plane were all the physical obstacles to its projection to be overcome, the portion of the two tangents behind the viewer appear on the image plane as an extension of the visible portion of the tangents in front of the viewer. Like the second branch this extension materializes in the sky rather than on the ground, with the horizon marking the boundary between the physically visible (scene in front) and invisible (scene behind), and the visible and invisible parts of the tangents combining in a single X shape. As the gaze is raised and lowered about the horizon, the X shape moves oppositely, lowering as the gaze is raised and vice versa but always with the visible portion being on the ground and stopping at the horizon, with the center of the X being on the horizon when the gaze is horizontal.
All of the above was for the case when the circle faces the viewer, with only the viewer's gaze varying. When the circle starts to face away from the viewer the viewer's lens is no longer on-axis. In this case the cross section of the cone is no longer a circle but an ellipse (never a parabola or hyperbola). However the principle of conic sections does not depend on the cross section of the cone being circular, and applies without modification to the case of eccentric cones.
It is not difficult to see that even in the off-axis case a circle can appear circular, namely when the image plane (and hence lens plane) is parallel to the plane of the circle. That is, to see a circle as a circle when viewing it obliquely, look not at the circle itself but at the plane in which it lies. From this it can be seen that when viewing a plane filled with many circles, all of them will appear circular simultaneously when the plane is looked at directly.
A common misperception about the hyperbola is that it is a mathematical curve rarely if ever encountered in daily life. The reality is that one sees a hyperbola whenever catching sight of portion of a circle cut by one's lens plane (and a parabola when the lens plane is tangent to, i.e. just touches, the circle). The inability to see very much of the arms of the visible branch, combined with the complete absence of the second branch, makes it virtually impossible for the human visual system to recognize the connection with hyperbolas such as "y" = 1/"x" where both branches are on display simultaneously.
Derived curves.
Several other curves can be derived from the hyperbola by inversion, the so-called inverse curves of the hyperbola. If the center of inversion is chosen as the hyperbola's own center, the inverse curve is the lemniscate of Bernoulli; the lemniscate is also the envelope of circles centered on a rectangular hyperbola and passing through the origin. If the center of inversion is chosen at a focus or a vertex of the hyperbola, the resulting inverse curves are a limaçon or a strophoid, respectively.
Coordinate systems.
Cartesian coordinates.
An east-west opening hyperbola centered at ("h","k") has the equation
The major axis runs through the center of the hyperbola and intersects both arms of the hyperbola at the vertices (bend points) of the arms. The foci lie on the extension of the major axis of the hyperbola.
The minor axis runs through the center of the hyperbola and is perpendicular to the major axis.
In both formulas "a" is the semi-major axis (half the distance between the two arms of the hyperbola measured along the major axis), and "b" is the semi-minor axis (half the distance between the asymptotes along a line tangent to the hyperbola at a vertex).
If one forms a rectangle with vertices on the asymptotes and two sides that are tangent to the hyperbola, the sides tangent to the hyperbola are "2b" in length while the sides that run parallel to the line between the foci (the major axis) are "2a" in length. Note that "b" may be larger than "a" despite the names "minor" and "major".
If one calculates the distance from any point on the hyperbola to each focus, the absolute value of the difference of those two distances is always "2a".
The eccentricity is given by
If "c" equals the distance from the center to either focus, then
where
The distance "c" is known as the linear eccentricity of the hyperbola. The distance between the foci is 2"c" or 2"aε".
The foci for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
The directrices for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
Polar coordinates.
The polar coordinates used most commonly for the hyperbola are defined relative to the Cartesian coordinate system that has its origin in a focus and its x-axis pointing towards the origin of the "canonical coordinate system" as illustrated in the figure of the section "True anomaly".
Relative to this coordinate system one has that
and the range of the true anomaly formula_89 is:
With polar coordinate relative to the "canonical coordinate system"
one has that
For the right branch of the hyperbola the range of formula_94 is:
Parametric equations.
"East-west opening hyperbola:"
"North-south opening hyperbola:"
In all formulae ("h","k") are the center coordinates of the hyperbola, "a" is the length of the semi-major axis, and "b" is the length of the semi-minor axis.
Elliptic coordinates.
A family of confocal hyperbolas is the basis of the system of elliptic coordinates in two dimensions. These hyperbolas are described by the equation
where the foci are located at a distance "c" from the origin on the "x"-axis, and where θ is the angle of the asymptotes with the "x"-axis. Every hyperbola in this family is orthogonal to every ellipse that shares the same foci. This orthogonality may be shown by a conformal map of the Cartesian coordinate system "w" = "z" + 1/"z", where "z"= "x" + "iy" are the original Cartesian coordinates, and "w"="u" + "iv" are those after the transformation.
Other orthogonal two-dimensional coordinate systems involving hyperbolas may be obtained by other conformal mappings. For example, the mapping "w" = "z"2 transforms the Cartesian coordinate system into two families of orthogonal hyperbolas.
Rectangular hyperbola.
A rectangular hyperbola, equilateral hyperbola, or right hyperbola is a hyperbola for which the asymptotes are perpendicular.
Rectangular hyperbolas with the coordinate axes parallel to their asymptotes have the equation
Rectangular hyperbolas have eccentricity formula_100 with semi-major axis and semi-minor axis given by formula_101.
The simplest example of rectangular hyperbolas occurs when the center ("h", "k") is at the origin:
describing quantities "x" and "y" that are inversely proportional. By rotating the coordinate axes counterclockwise by 45 degrees, with the new coordinate axes labelled formula_103 the equation of the hyperbola is given by canonical form
If the scale factor "m"=1/2, then this canonical rectangular hyperbola is the unit hyperbola.
A circumconic passing through the orthocenter of a triangle is a rectangular hyperbola.
Applications.
Sundials.
Hyperbolas may be seen in many sundials. On any given day, the sun revolves in a circle on the celestial sphere, and its rays striking the point on a sundial traces out a cone of light. The intersection of this cone with the horizontal plane of the ground forms a conic section. At most populated latitudes and at most times of the year, this conic section is a hyperbola. In practical terms, the shadow of the tip of a pole traces out a hyperbola on the ground over the course of a day (this path is called the "declination line"). The shape of this hyperbola varies with the geographical latitude and with the time of the year, since those factors affect the cone of the sun's rays relative to the horizon. The collection of such hyperbolas for a whole year at a given location was called a "pelekinon" by the Greeks, since it resembles a double-bladed axe.
Multilateration.
A hyperbola is the basis for solving Multilateration problems, the task of locating a point from the differences in its distances to given points — or, equivalently, the difference in arrival times of synchronized signals between the point and the given points. Such problems are important in navigation, particularly on water; a ship can locate its position from the difference in arrival times of signals from a LORAN or GPS transmitters. Conversely, a homing beacon or any transmitter can be located by comparing the arrival times of its signals at two separate receiving stations; such techniques may be used to track objects and people. In particular, the set of possible positions of a point that has a distance difference of 2"a" from two given points is a hyperbola of vertex separation 2"a" whose foci are the two given points.
Path followed by a particle.
The path followed by any particle in the classical Kepler problem is a conic section. In particular, if the total energy "E" of the particle is greater than zero (i.e., if the particle is unbound), the path of such a particle is a hyperbola. This property is useful in studying atomic and sub-atomic forces by scattering high-energy particles; for example, the Rutherford experiment demonstrated the existence of an atomic nucleus by examining the scattering of alpha particles from gold atoms. If the short-range nuclear interactions are ignored, the atomic nucleus and the alpha particle interact only by a repulsive Coulomb force, which satisfies the inverse square law requirement for a Kepler problem.
Korteweg-de Vries equation.
The hyperbolic trig function formula_113 appears as one solution to the Korteweg-de Vries equation which describes the motion of a soliton wave in a canal.
Angle trisection.
As shown first by Apollonius of Perga, a hyperbola can be used to trisect any angle, a well studied problem of geometry. Given an angle, first draw a circle centered at its vertex O, which intersects the sides of the angle at points A and B. Next draw the line through A and B and its perpendicular bisector formula_114. Construct a hyperbola of eccentricity ε=2 with formula_114 as directrix and B as a focus. Let P be the intersection (upper) of the hyperbola with the circle. Angle POB trisects angle AOB. To prove this, reflect the line segment OP about the line formula_114 obtaining the point P' as the image of P. Segment AP' has the same length as segment BP due to the reflection, while segment PP' has the same length as segment BP due to the eccentricity of the hyperbola. As OA, OP', OP and OB are all radii of the same circle (and so, have the same length), the triangles OAP', OPP' and OPB are all congruent. Therefore, the angle has been trisected, since 3×POB = AOB.
Efficient portfolio frontier.
In portfolio theory, the locus of mean-variance efficient portfolios (called the efficient frontier) is the upper half of the east-opening branch of a hyperbola drawn with the portfolio return's standard deviation plotted horizontally and its expected value plotted vertically; according to this theory, all rational investors would choose a portfolio characterized by some point on this locus.
Extensions.
The three-dimensional analog of a hyperbola is a hyperboloid. Hyperboloids come in two varieties, those of one sheet and those of two sheets. A simple way of producing a hyperboloid is to rotate a hyperbola about the axis of its foci or about its symmetry axis perpendicular to the first axis; these rotations produce hyperboloids of two and one sheet, respectively.

</doc>
<doc id="14055" url="http://en.wikipedia.org/wiki?curid=14055" title="Humayun">
Humayun

Humayun (Persian: نصیر الدین محمد همایون‎; OS 7 March 1508 – OS 27 January 1556 ) was the second Mughal Emperor who ruled over territory in what is now Afghanistan, Pakistan, and parts of northern India from 1531–1540 and again from 1555–1556. Like his father, Babur, he lost his kingdom early, but regained it with the aid of the Safavid dynasty of Persia, with additional territory. At the time of his death in 1556, the Mughal empire spanned almost one million square kilometers.
Humayun succeeded his father in 1531, as ruler of the Mughal territories in India. At the age of 23, Humayun was an inexperienced ruler when he came to power. His half-brother Kamran Mirza inherited Kabul and Lahore, the more northern parts of their father's empire. Mirza was to become a bitter rival of Humayun.
Humayun lost Mughal territories to the Pashtun noble, Sher Shah Suri, and, with Persian(Safavid) aid, regained them 15 years later. Humayun's return from Persia was accompanied by a large retinue of Persian noblemen and signaled an important change in Mughal court culture. The Central Asian origins of the dynasty were largely overshadowed by the influences of Persian art, architecture, language and literature. There are many stone carvings and thousands of Persian manuscripts in India dating from the time of Humayun.
Subsequently, in a very short time, Humayun was able to expand the Empire further, leaving a substantial legacy for his son, Akbar. His peaceful personality, patience and non-provocative methods of speech earned him the title "’Insān-i-Kamil" (‘Perfect Man’), among the Mughals.
Background.
Babur's decision to divide the territories of his empire between two of his sons was unusual in India although it had been a common Central Asian practice since the time of Genghis Khan. Unlike most monarchies which practised primogeniture, the Timurids, following Genghis Khan's example, did not leave an entire kingdom to the eldest son. Although under that system only a Chingissid could claim sovereignty and khanal authority, any male Chinggisid within a given sub-branch (such as the Timurids) had an equal right to the throne. While Genghis Khan's Empire had been peacefully divided between his sons upon his death, almost every Chinggisid succession since had resulted in fratricide.
Timur himself had divided his territories between Pir Muhammad, Miran Shah, Khalil Sultan and Shah Rukh, which resulted in inter-family warfare. Upon Babur's death, Humayun's territories were the least secure. He had ruled only four years, and not all "umarah" (nobles) viewed Humayun as the rightful ruler. Indeed earlier, when Babur had become ill, some of the nobles had tried to install his uncle, Mahdi Khwaja, as ruler. Although this attempt failed, it was a sign of problems to come.
Early reign.
When Humayun came to the throne several of his brothers revolted against him. Another brother Kriday Mirza (1508-30) supported Humayun but was assasinated. Humayun had two major rivals for his lands — Sultan Bahadur of Gujarat to the south west and Sher Shah Suri (Sher Khan) settled along the river Ganges in Bihar to the east. Humayun’s first campaign was to confront Sher Khan Suri. Halfway through this offensive Humayun had to abandon it and concentrate on Gujarat, where a threat from Ahmed Shah had to be met. Humayun was victorious annexing Gujarat, Malwa, Champaner and the great fort of Mandu.
During the first five years of Humayun's reign, Bahadur and Sher Khan extended their rule, although Sultan Bahadur faced pressure in the east from sporadic conflicts with the Portuguese. While the Mughals had obtained firearms via the Ottoman Empire, Bahadur's Gujarat had acquired them through a series of contracts drawn up with the Portuguese, allowing the Portuguese to establish a strategic foothold in north western India.
Humayun was made aware that the Sultan of Gujarat was planning an assault on the Mughal territories with Portuguese aid. Humayun gathered an army and marched on Bahadur. Within a month he had captured the forts of Mandu and Champaner. However, instead of pressing his attack, Humayun ceased the campaign and consolidated his newly conquered territory. Sultan Bahadur, meanwhile escaped and took up refuge with the Portuguese.
Sher Shah Suri.
Shortly after Humayun had marched on Gujarat, Sher Shah saw an opportunity to wrest control of Agra from the Mughals. He began to gather his army together hoping for a rapid and decisive siege of the Mughal capital. Upon hearing this alarming news, Humayun quickly marched his troops back to Agra allowing Bahadur to easily regain control of the territories Humayun had recently taken. A few months later, however, Bahadur was dead, killed when a botched plan to kidnap the Portuguese viceroy ended in a fire-fight which the Sultan lost.
Whilst Humayun succeeded in protecting Agra from Sher Shah, the second city of the Empire, Gaur the capital of the "vilayat" of Bengal, was sacked. Humayun's troops had been delayed while trying to take Chunar, a fort occupied by Sher Shah's son, in order to protect his troops from an attack from the rear. The stores of grain at Gauri, the largest in the empire, were emptied and Humayun arrived to see corpses littering the roads. The vast wealth of Bengal was depleted and brought East giving Sher Shah a substantial war chest.
Sher Shah withdrew to the east, but Humayun did not follow: instead he "shut himself up for a considerable time in his Harem, and indulged himself in every kind of luxury." Hindal, Humayun's 19-year-old brother, had agreed to aid him in this battle and protect the rear from attack but abandoned his position and withdrew to Agra where he decreed himself acting emperor. When Humayun sent the grand "Mufti", Sheikh Buhlul, to reason with him, the Sheikh was killed. Further provoking the rebellion, Hindal ordered that the "Khutba" or sermon in the main mosque at Agra be read in his name, a sign of assumption of sovereignty. When Hindal withdrew from protecting the rear of Humayun's troops, Sher Shah's troop quickly reclaimed these positions, leaving Humayun surrounded.
Humayun's other brother, Kamran, marched from his territories in the Punjab, ostensibly to aid Humayun. However, his return home had treacherous motives as he intended to stake a claim for Humayun's apparently collapsing empire. He brokered a deal with Hindal which provided that his brother would cease all acts of disloyalty in return for a share in the new empire which Kamran would create once Humayun was deposed.
Sher Shah met Humayun in battle on the banks of the Ganges, near Benares, in Chausa. This was to become an entrenched battle in which both sides spent a lot of time digging themselves into positions. The major part of the Mughal army, the artillery, was now immobile, and Humayun decided to engage in some diplomacy using Muhammad Aziz as ambassador. Humayun agreed to allow Sher Shah to rule over Bengal and Bihar, but only as provinces granted to him by his Emperor, Humayun, falling short of outright sovereignty. The two rulers also struck a bargain in order to save face: Humayun's troops would charge those of Sher Shah whose forces then retreat in feigned fear. Thus honour would, supposedly, be satisfied.
Once the Army of Humayun had made its charge and Sher Shah's troops made their agreed-upon retreat, the Mughal troops relaxed their defensive preparations and returned to their entrenchments without posting a proper guard. Observing the Mughals' vulnerability, Sher Shah reneged on his earlier agreement. That very night, his army approached the Mughal camp and finding the Mughal troops unprepared with a majority asleep, they advanced and killed most of them. The Emperor survived by swimming the Ganges using an air filled "water skin," and quietly returned to Agra.
In Agra.
When Humayun returned to Agra, he found that all three of his brothers were present. Humayun once again not only pardoned his brothers for plotting against him, but even forgave Hindal for his outright betrayal. With his armies travelling at a leisurely pace, Sher Shah was gradually drawing closer and closer to Agra. This was a serious threat to the entire family, but Humayun and Kamran squabbled over how to proceed. Kamran withdrew after Humayun refused to make a quick attack on the approaching enemy, instead opting to build a larger army under his own name. When Kamran returned to Lahore, his troops followed him shortly afterwards, and Humayun, with his other brothers Askari and Hindal, marched to meet Sher Shah just 240 km east of Agra at the Battle of Kanauj on 17 May 1540. The battle once again saw Humayun make some tactical errors, and his army was soundly defeated. He and his brothers quickly retreated back to Agra, humiliated and mocked along the way by peasants and villagers. They chose not to stay in Agra, and retreated to Lahore, though Sher Shah followed them, founding the short-lived Sur Dynasty of northern India with its capital at Delhi.
In Lahore.
The four brothers were united in Lahore, but every day they were informed that Sher Shah was getting closer and closer. When he reached Sirhind, Humayun sent an ambassador carrying the message "I have left you the whole of Hindustan ("i.e." the lands to the East of Punjab, comprising most of the Ganges Valley). Leave Lahore alone, and let Sirhind be a boundary between you and me." Sher Shah, however, replied "I have left you Kabul. You should go there." Kabul was the capital of the empire of Humayun's brother Kamran Mirza, who was far from willing to hand over any of his territories to his brother. Instead, Kamran approached Sher Shah, and proposed that he actually revolt against his brother and side with Sher Shah in return for most of the Punjab. Sher Shah dismissed his help, believing it not to be required, though word soon spread to Lahore about the treacherous proposal and Humayun was urged to make an example of Kamran and kill him. Humayun refused, citing the last words of his father, Babur "Do nothing against your brothers, even though they may deserve it."
Withdrawing further.
Humayun decided that it would be wise to withdraw still further, Humayun and his army rode out through and across the Thar Desert, when the Hindu ruler Rao Maldeo Rathore allied himself with Sher Shah Suri against the Mughal Empire. In many accounts Humayun mentions how he and his heavily pregnant wife, had to trace their steps through the desert at the hottest time of year. All the wells had been filled with sand by the nearby Hindu inhabitants in order to starve and exhaust the Mughals further, leaving them with nothing but berries to eat. When Hamida's horse died,no one would lend the Queen (who was now eight months pregnant) a horse, so Humayun did so himself, resulting in him riding a camel for six kilometeres (four miles), although Khaled Beg then offered him his mount. Humayun was later to describe this incident as the lowest point in his life.
He asked that his brothers join him as he fell back into Sindh. While the previously rebellious Hindal Mirza remained loyal and was ordered to join his brothers in Kandahar. Kamran Mirza and Askari Mirza instead decided to head to the relative peace of Kabul. This was to be a definitive schism in the family.
Humayun expected aid from the Emir of Sindh, Hussein Umrani, whom he had appointed and who owed him his allegiance. The Emir Hussein Umrani welcomed Humayun's presence and was loyal to Humayun just as he had been loyal to Babur against the renegade Arghuns. Whilst in the oasis garrison of Umerkot in Sindh, Hamida daughter of noble Sindhi, gave birth to Akbar on 25 October 1542, the heir-apparent to the 34-year old Humayun. The date was special because Humayun consulted his Astronomer to utilize the astrolabe and check the location of the planets.
While in Sindh, Humayun alongside Emir Hussein Umrani, gathered horses and weapons and formed new alliances that helped regain lost territories. Until finally Humayun had gathered hundreds of Sindhi and Baloch tribesmen alongside his Mughals and then marched towards Kandahar and later Kabul, thousands more gathered by his side as Humayun continually declared himself the rightful Timurid heir of the first Mughal Emperor Babur.
Retreat to Kabul.
After Humayun set out from his expedition in Sindh, along with 300 camels (mostly wild) and 2000 loads of grain, he set off to join his brothers in Kandahar after crossing the Indus River on 11 July 1543 along with the ambition to regain the Mughal Empire and overthrow the Suri dynasty. Among the tribes that had sworn allegiance to Humayun were the Magsi, Rind and many others.
In Kamran Mirza's territory, Hindal Mirza had been placed under house arrest in Kabul after refusing to have the "Khutba" recited in Kamran Mirza's name. His other brother Askari Mirza was now ordered to gather an army and march on Humayun. When Humayun received word of the approaching hostile army he decided against facing them, and instead sought refuge elsewhere. Akbar was left behind in camp close to Kandahar for, as it was December it would have been too cold and dangerous to include the 14-month-old toddler in the forthcoming march through the dangerous and snowy mountains of the Hindu Kush. Askari Mirza found Akbar in the camp, and embraced him, and allowed his own wife to parent him, she apparently started treating him as her own.
Once again Humayun turned toward Kandahar where his brother Kamran Mirza was in power, but he received no help and had to seek refuge with the Shah of Persia.
Refuge in Persia.
Humayun fled to the refuge of the Safavid Empire in Iran, marching with 40 men and his wife and her companion through mountains and valleys. Amongst other trials the Imperial party were forced to live on horse meat boiled in the soldiers' helmets. These indignities continued during the month it took them to reach Herat, however after their arrival they were reintroduced to the finer things in life. Upon entering the city his army was greeted with an armed escort, and they were treated to lavish food and clothing. They were given fine accommodations and the roads were cleared and cleaned before them. Shah Tahmasp, unlike Humayun's own family, actually welcomed the Mughal, and treated him as a royal visitor. Here Humayun went sightseeing and was amazed at the Persian artwork and architecture he saw: much of this was the work of the Timurid Sultan Husayn Bayqarah and his ancestor, princess Gauhar Shad, thus he was able to admire the work of his relatives and ancestors at first hand. He was introduced to the work of the Persian miniaturists, and Kamaleddin Behzad had two of his pupils join Humayun in his court. Humayun was amazed at their work and asked if they would work for him if he were to regain the sovereignty of Hindustan: they agreed. With so much going on Humayun did not even meet the Shah until July, some six months after his arrival in Persia. After a lengthy journey from Herat the two met in Qazvin where a large feast and parties were held for the event. The meeting of the two monarchs is depicted in a famous wall-painting in the Chehel Sotoun (Forty Columns) palace in Esfahan.
The Shah urged that Humayun convert from Sunni to Shia Islam, and Humayun eventually and reluctantly accepted, in order to keep himself and several hundred followers alive. Although the Mughals initially disagreed to their conversion they knew that with this outward acceptance of Shi'ism, Shah Tahmasp was eventually prepared to offer Humayun more substantial support. When Humayun's brother, Kamran Mirza, offered to cede Kandahar to the Persians in exchange for Humayun, dead or alive, Shah Tahmasp refused. Instead the Shah staged a celebration for Humayun, with 300 tents, an imperial Persian carpet, 12 musical bands and "meat of all kinds". Here the Shah announced that all this, and 12,000 elite cavalry were his to lead an attack on his brother Kamran. All that Shah Tahmasp asked for was that, if Humayun's forces were victorious, Kandahar would be his.
Kandahar and onwards.
With this Persian Safavid aid Humayun took Kandahar from Askari Mirza after a two-week siege. He noted how the nobles who had served Askari Mirza quickly flocked to serve him, "in very truth the greater part of the inhabitants of the world are like a flock of sheep, wherever one goes the others immediately follow". Kandahar was, as agreed, given to the Shah of Persia who sent his infant son, Murad, as the Viceroy. However, the baby soon died and Humayun thought himself strong enough to assume power.
Humayun now prepared to take Kabul, ruled by his brother Kamran Mirza. In the end, there was no actual siege. Kamran Mirza was detested as a leader and as Humayun's Persian army approached the city hundreds of Kamran Mirza's troops changed sides, flocking to join Humayun and swelling his ranks. Kamran Mirza absconded and began building an army outside the city. In November 1545, Hamida and Humayun were reunited with their son Akbar, and held a huge feast. They also held another, larger, feast in the child's honour when he was circumcised.
However, while Humayun had a larger army than his brother and had the upper hand, on two occasions his poor military judgement allowed Kamran Mirza to retake Kabul and Kandahar, forcing Humayun to mount further campaigns for their recapture. He may have been aided in this by his reputation for leniency towards the troops who had defended the cities against him, as opposed to Kamran Mirza, whose brief periods of possession were marked by atrocities against the inhabitants who, he supposed, had helped his brother.
His youngest brother, Hindal Mirza, formerly the most disloyal of his siblings, died fighting on his behalf. His brother Askari Mirza was shackled in chains at the behest of his nobles and aides. He was allowed go on Hajj, and died en route in the desert outside Damascus.
Humayun's other brother, Kamran Mirza, had repeatedly sought to have Humayun killed. In 1552 Kamran Mirza attempted to make a pact with Islam Shah, Sher Shah's successor, but was apprehended by a Gakhar. The Gakhars were one of the minority of tribal groups who had consistently remained loyal to their oath to the Mughals. Sultan Adam of the Gakhars handed Kamran Mirza over to Humayun. Humayun was inclined to forgive his brother. However he was warned that allowing Kamran Mirza's repeated acts of treachery to go unpunished could foment rebellion amongst his own supporters. So, instead of killing his brother, Humayun had Kamran Mirza blinded which would end any claim by the latter to the throne. Humayun sent Kamran Mirza on Hajj, as he hoped to see his brother thereby absolved of his offences. However Kamran Mirza died close to Mecca in the Arabian Peninsula in 1557.
Restoration of the Mughal Empire.
Sher Shah Suri had died in 1545; his son and successor Islam Shah died too, in 1554. These two deaths left the dynasty reeling and disintegrating. Three rivals for the throne all marched on Delhi, while in many cities leaders tried to stake a claim for independence. This was a perfect opportunity for the Mughals to march back to India.
The Mughal Emperor Humayun, gathered a vast army and attempted the challenging task of retaking the throne in Delhi. Humayun placed the army under the able leadership of Bairam Khan. This was a wise move given Humayun's own record of military ineptitude, and turned out to be prescient, as Bairam was to prove himself a great tactician.
"Marriage relations with the Khanzadas".
The "Gazetteer of Ulwur" states:
Soon after Babur's death, his successor, Humayun, was in AD 1540 supplanted by the Pathan Sher Shah, who, in AD 1545, was followed by Islam Shah. During the reign of the latter a battle was fought and lost by the Emperor's troops at Firozpur Jhirka, in Mewat, on which, however, Islam Shah did not loose his hold. Adil Shah, the third of the Pathan interlopers, who succeeded in AD 1552, had to contend for the Empire with the returned Humaiyun.
In these struggles for the restoration of Babar's dynasty Khanzadas apparently do not figure at all. Humaiyun seems to have conciliated them by marrying the elder daughter of Jamal Khan, nephew of Babar's opponent, Hasan Khan, and by causing his great minister, Bairam Khan, to marry a younger daughter of the same Mewatti.
Bairam Khan led the army through the Punjab virtually unopposed. The fort of Rohtas, which was built in 1541–43 by Sher Shah Suri to crush the Gakhars who were loyal to Humayun, was surrendered without a shot by a treacherous commander. The walls of the Rohtas Fort measure up to 12.5 meters in thickness and up to 18.28 meters in height. They extend for 4 km and feature 68 semi-circular bastions. Its sandstone gates, both massive and ornate, are thought to have exerted a profound influence on Mughal military architecture.
The only major battle faced by Humayun's armies was against Sikander Suri in Sirhind, where Bairam Khan employed a tactic whereby he engaged his enemy in open battle, but then retreated quickly in apparent fear. When the enemy followed after them they were surprised by entrenched defensive positions and were easily annihilated.
From here on most towns and villages chose to welcome the invading army as it made its way to the capital. On 23 July 1555, Humayun once again sat on Babur's throne in Delhi.
Ruling Kashmir.
With all of Humayun's brothers now dead, there was no fear of another usurping his throne during his military campaigns. He was also now an established leader, and could trust his generals. With this new-found strength Humayun embarked on a series of military campaigns aimed at extending his reign over areas in eastern and western India. His sojourn in exile seems to have reduced Humayun's reliance on astrology, and his military leadership came to imitate the more effective methods that he had observed in Persia.
In the year 1540, the Mughal Emperor Humayun met the Ottoman Admiral Seydi Ali Reis. During their discussions in the Durbar, Humayun asked which of the two empires was bigger and Seydi Ali Reis, stated that the Ottoman Empire was "ten times bigger", Humayun was very inspired and he turned towards his nobles and remarked without resentment: "Indeed Suleiman the Magnificent, deserves to be called the only Padshah on Earth".
Humayun returned from exile in Persia with thousands of Persians soldiers and nobles. This influx increased the cultural and political influences of the Persians in Mughal Empire. It also applied to the administration of the empire. Persian methods of governance were imported into Kashmir during the remainder of Humayun's reign. The system of revenue collection was improved by following both the Persian model and that of the Delhi Sultanate. The Persian arts became very influential, and Persian-style miniatures were produced at Mughal (and subsequently Rajput) courts. The Chaghatai language, in which Babur had written his memoirs, disappeared almost entirely from use by of the courtly elite, and Akbar could not speak it. Later in life, Humayun himself is said to have frequently used quotations from Persian verse.
Trusted Generals.
After defeating Bahadur Shah's confederacy in Gujarat, Humayun placed the following Generals in Gujarat:
Death and legacy.
On 27 January 1556, Humayun, with his arms full of books, was descending the staircase from his library when the muezzin announced the Adhan (the call to prayer). It was his habit, wherever he heard the summons, to bow his knee in holy reverence. Trying to kneel, he caught his foot in his robe, tumbled down several steps and hit his temple on a rugged stone edge. He died three days later. His body was laid to rest in Purana Quila initially, but because of attack by Hemu on Delhi and capture of Purana Qila, Humayun's body was exhumed by the fleeing army and transferred to Kalanaur in Punjab where Akbar was coronated. His tomb stands in Delhi, where he was later buried in a grand way.
Full title.
His full title as Emperor of the Mughal Empire was:
"Al-Sultan al-'Azam wal Khaqan al-Mukarram, Jam-i-Sultanat-i-haqiqi wa Majazi, Sayyid al-Salatin, Abu'l Muzaffar Nasir ud-din Muhammad Humayun Padshah Ghazi, Zillu'llah"

</doc>
<doc id="14056" url="http://en.wikipedia.org/wiki?curid=14056" title="Prince-elector">
Prince-elector

The prince-electors (or simply electors) of the Holy Roman Empire (German: "Kurfürst" (  ), pl. "Kurfürsten", #REdirect Latin: "Princeps Elector") were the members of the electoral college of the Holy Roman Empire, having since the 13th century the privilege of electing the King of the Romans or, from the middle of the 16th century onwards, directly the Holy Roman Emperor.
The heir apparent to a lay prince-elector was known as an electoral prince (German: "Kurprinz"). The dignity of Elector carried great prestige and was second only to King or Emperor.
Overview.
The Holy Roman Empire was in theory an elective monarchy, but from the 15th century onwards the electors often merely formalised what was a "de facto" dynastic succession within the Austrian House of Habsburg, with the title usually passing to the eldest surviving son of the deceased Emperor. Despite this, the office was not legally hereditary, and the heir could not title himself "Emperor" without having been personally elected.
Formally the Prince-Electors elected a King of the Romans, who was elected in Germany but became Holy Roman Emperor only when crowned by the Pope. Charles V was the last to be a crowned Emperor (elected 1519, crowned 1530); his successors were all Emperors by election only, each titled "Emperor-elect of the Romans" (German: "erwählter Römischer Kaiser"; Latin: "electus Romanorum imperator").
Electors were among the princes of the Empire, but they had exclusive privileges in addition to their electoral ones which were not shared with the other princes. The dignity of Elector was extremely prestigious, and was held in addition to such feudal titles as Duke, Margrave, or Count Palatine.
At least from the 13th century, there were seven electors: three spiritual (the Archbishop of Mainz, the Archbishop of Trier, and the Archbishop of Cologne) and four lay: (the King of Bohemia, the Count Palatine of the Rhine, the Duke of Saxony, and the Margrave of Brandenburg; these last three were also known as the Elector Palatine, the Elector of Saxony, and the Elector of Brandenburg, respectively). Only six of the electors, however, had the right to sit at ordinary meetings: "The King of Bohemia, who was in fact not a prince of the Empire but a neighbouring and independent monarch, might vote at an imperial election, but was allowed on no other occasion to meddle in the affairs of the Empire."
Other electors were added in the 17th century, including the Duke of Bavaria (referred to as the Elector of Bavaria—replacing the Count Palatine of the Rhine, who was of the same family but had lost his title and vote temporarily during the Thirty Years' War), and the Duke of Brunswick-Lüneburg (the Elector of Hanover—an office subsequently held by three Hanoverian kings of Great Britain, George I, George II, and George III).
Several new electors were created during the reorganization of the Empire in 1803, but these never participated in an election. On August 6, 1806, pressed both by Napoleon and by several German princes (including some Electors), the last Holy Roman emperor, Emperor Francis II, by edict dissolved the Empire. After or just before the dissolution, the Electors of Bavaria, Württemberg, Saxony, and Hanover each took the title of king of his former electorate (in the case of Hanover after regaining his lands following Napoleon's defeat in 1814) while the King of Prussia extended his royal title to cover his erstwhile Electorate of Brandenburg as well as the lands he held as king outside the imperial border. The Electors of Regensburg (who had succeeded to the Mainz vote), Würzburg (who had succeeded to the Salzburg vote), and Baden (a new electorate) became grand dukes. The Elector of Hesse and Landgrave of Hesse-Kassel chose to retain the defunct electoral title until the state was annexed by Prussia, 60 years later.
Etymology of "Kurfürst".
The German element "Kur-" is based on the Middle High German irregular verb "kiesen" and is related etymologically to the English word "choose" (cf. Old English "ceosan" ], participle "coren" 'having been chosen' and Gothic "kiusan"). In English, the "s"/"r" mix in the Germanic verb conjugation has been regularized to "s" throughout, while German retains the "r" in "Kur-". There is also a modern German verb "küren" which means 'to choose' in a ceremonial sense. "Fürst" is German for 'prince', but while the German language distinguishes between the head of a principality ("der Fürst") and the son of a monarch ("der Prinz"), English uses "prince" for both concepts. "Fürst" itself is related to English "first" and is thus the 'foremost' person in his realm. Note that 'prince' derives from Latin "princeps", which carried the same meaning.
Composition.
The German practice of electing monarchs began when ancient Germanic tribes formed "ad hoc" coalitions and elected the leaders thereof. Elections were irregularly held by the Franks, whose successor states include France and Germany. The French monarchy eventually became hereditary, but the German monarchy remained elective. While all free men originally exercised the right to vote in such elections, suffrage eventually came to be limited to the leading men of the realm. In the election of Lothar II in 1125, a small number of eminent nobles chose the monarch and then submitted him to the remaining magnates for their approbation. Soon, the right to choose the monarch was settled on an exclusive group of princes, and the procedure of seeking the approval of the remaining nobles was abandoned. The college of electors was mentioned in 1152 and again in 1198. A letter of Pope Urban IV suggests that by "immemorial custom", seven princes had the right to elect the King and future Emperor. These were three ecclesiastic: the Archbishop of Mainz, the Archbishop of Trier, Archbishop of Cologne and four secular: the King of Bohemia ("král český", "König von Böhmen"), the Count Palatine of the Rhine ("Pfalzgraf bei Rhein"), the Duke of Saxony ("Herzog von Sachsen") and the Margrave of Brandenburg ("Markgraf von Brandenburg").
The seven have been mentioned as the vote-casters in the election of 1257 that resulted in two kings becoming elected. The three Archbishops oversaw the most venerable and powerful sees in Germany, while the other four were supposed to represent the dukes of the four nations (the Franks, Swabians, Saxons and Bavarians); they also hold great offices in the imperial household. The dukedoms of Franconia and Swabia had become extinct; their place and power, and the household offices they held, descended to the County Palatine of the Rhine and the Margraviate of Brandenburg. Saxony, even with diminished territory, retained its eminent position. The Palatinate and Bavaria were originally held by the same individual, but in 1253, they were divided between two members of the House of Wittelsbach. The other electors refused to allow two princes from the same dynasty to have electoral rights, so a heated rivalry arose between the Count Palatine and the Duke of Bavaria.
Meanwhile, the King of Bohemia, who held the ancient imperial office of Arch-Cupbearer, asserted his right to participate in elections. Sometimes he was challenged on the grounds that his kingdom was not German, though usually he was recognized, instead of Bavaria which after all was just a younger line of Wittelsbachs.
The Declaration of Rhense issued in 1338 had the effect that election by the majority of the electors automatically conferred the royal title and rule over the empire, without papal confirmation. The Golden Bull of 1356 finally resolved the disputes among the electors. Under it, the Archbishops of Mainz, Trier, and Cologne, as well as the King of Bohemia, the Count Palatine of the Rhine, the Duke of Saxony, and the Margrave of Brandenburg held the right to elect the King.
The college's composition remained unchanged until the 17th century, although in 1547, in the aftermath of the Schmalkaldic War, the Saxon electorship was transferred from the senior to the junior branch of the Wettin family. In 1621, the Elector Palatine, Frederick V, came under the imperial ban after participating in the Bohemian Revolt (a part of the Thirty Years' War). The Elector Palatine's seat was conferred on the Duke of Bavaria, the head of a junior branch of his family. Originally, the Duke held the electorate personally, but it was later made hereditary along with the duchy. When the Thirty Years' War concluded with the Treaty of Münster (also called the Peace of Westphalia) in 1648, a new electorate was created for the Count Palatine of the Rhine. Since the Elector of Bavaria retained his seat, the number of electors increased to eight; the two Wittelsbach lines now sufficiently estranged so as not to pose a combined potential threat.
In 1692, as a result of the inheritance of the Palatinate by a Catholic branch of the Wittelsbach family, which threatened to upset the religious balance of the College of Electors, the number of electors was increased to nine, with a seat being granted to the Duke of Brunswick-Lüneburg, who became known as the Elector of Hanover (the Imperial Diet officially confirmed the creation in 1708). In 1706, the Elector of Bavaria and Archbishop of Cologne were banned during the War of the Spanish Succession, but both were restored in 1714 after the Peace of Baden. In 1777, the number of electors was reduced to eight when the Elector Palatine inherited Bavaria.
Many changes to the composition of the college were necessitated by Napoleon's aggression during the early 19th century. The Treaty of Lunéville (1801), which ceded territory on the Rhine's left bank to France, led to the abolition of the archbishoprics of Trier and Cologne, and the transfer of the remaining spiritual Elector from Mainz to Regensburg. In 1803, electorates were created for the Duke of Württemberg, the Margrave of Baden, the Landgrave of Hesse-Kassel (or Hesse-Cassel), and the Duke of Salzburg, bringing the total number of electors to ten. When Austria annexed Salzburg under the Treaty of Pressburg (1805), the Duke of Salzburg moved to the Grand Duchy of Würzburg and retained his electorate. None of the new electors, however, had an opportunity to cast votes, as the Holy Roman Empire was abolished in 1806, and the new electorates were never confirmed by the Emperor.
States within the Holy Roman Empire granted the electoral dignity throughout the centuries:
Three ecclesiastic/spiritual electors (archbishops):
Four secular electors:
Electors added in 17th century:
During the collapse of the Holy Roman Empire, between 1803 and 1806:
Rights and privileges.
Electors were "reichsstände" (Imperial Estates), enjoying precedence over the other princes. They were, until the 18th century, exclusively entitled to be addressed with the title "Durchlaucht" (Serene Highness). In 1742, the electors became entitled to the superlative "Durchläuchtigste" (Most Serene Highness), while other princes were promoted to "Durchlaucht".
As Imperial Estates, the electors enjoyed all the privileges of the other princes enjoying that status, including the right to enter into alliances, autonomy in relation to dynastic affairs and precedence over other subjects. The Golden Bull had granted them the Privilegium de non appellando, which prevented their subjects from lodging an appeal to a higher Imperial court. However, while this privilege, and some others, were automatically granted to Electors, they were not exclusive to them and many of the larger Imperial Estates were also to be individually granted some or all those rights and privileges.
After the abolition of the Holy Roman Empire in August 1806, the electors continued to reign over their territories, many of them taking higher titles. The Dukes of Bavaria, Württemberg, and Saxony made themselves Kings, as later did the Duke of Brunswick-Lüneburg, who was already King of Great Britain. Meanwhile, the Margrave of Baden elevated himself to the Grand-Ducal dignity. The Landgrave of Hesse-Kassel, however, retained the meaningless title "Elector of Hesse", thus distinguishing himself from other Hessian princes (the Grand Duke of Hesse-Darmstadt and the Landgrave of Hesse-Homburg). Napoleon soon exiled him and Kassel was annexed to the Kingdom of Westphalia, a new creation. The Congress of Vienna accepted that all old secular electorates were entitled to be kingdoms, but generally did not want to accept kingship for new, Napoleon-era electorates. This was one reason why Hanover became a kingdom. Württemberg, already having adopted royal rank in Napoleon era, was ultimately not stripped of it. Baden did not even try. But the restored elector of Hesse (a new electorate) tried to get recognition to title of king (King of Chatti), and was unsuccessful in that pursuit. They being not willing to give up the electoral rank, this led to the situation that this principality, which never cast an electoral vote in any imperial election, was the one which preserved the title of prince-elector. In 1866, however, the last Elector of Hesse was dethroned under Otto von Bismarck's plan for German Unification.
Imperial Diet.
The electors, like the other princes ruling States of the Empire, were members of the Imperial Diet, which was divided into three "collegia": the Council of Electors, the Council of Princes, and the Council of Cities. In addition to being members of the Council of Electors, several lay electors were therefore members of the Council of Princes as well by virtue of other territories they possessed. In many cases, the lay electors ruled numerous States of the Empire, and therefore held several votes in the Council of Princes. In 1792, the King of Bohemia held three votes, the Elector of Bavaria six votes, the Elector of Brandenburg eight votes, and the Elector of Hanover six votes. Thus, of the hundred votes in the Council of Princes in 1792, twenty-three belonged to electors. The lay electors therefore exercised considerable influence, being members of the small Council of Electors and holding a significant number of votes in the Council of Princes. The assent of both bodies was required for important decisions affecting the structure of the Empire, such as the creation of new electorates or States of the Empire.
In addition to voting by colleges or councils, the Imperial Diet also voted on religious lines, as provided for by the Peace of Westphalia. The Archbishop of Mainz presided over the Catholic body, or "corpus catholicorum", while the Elector of Saxony presided over the Protestant body, or "corpus evangelicorum". The division into religious bodies was on the basis of the official religion of the state, and not of its rulers. Thus, even when the Electors of Saxony were Catholics during the eighteenth century, they continued to preside over the "corpus evangelicorum", since the state of Saxony was officially Protestant.
Elections.
The individual chosen by the electors assumed the title "King of the Romans", though he actually reigned in Germany. The King of the Romans became Holy Roman Emperor only when crowned by the Pope. On many occasions, a Pope refused to crown a king with whom he was engaged in a dispute, but a lack of a papal coronation deprived a king of only the title Emperor and not of the power to govern (cf Declaration of Rhens). The Habsburg dynasty stopped the practice of papal coronations. After Charles V, all individuals chosen by the electors were merely "Emperors elect".
The electors were originally summoned by the Archbishop of Mainz within one month of an Emperor's death, and met within three months of being summoned. During the "interregnum", imperial power was exercised by two imperial vicars. Each vicar, in the words of the Golden Bull, was "the administrator of the empire itself, with the power of passing judgments, of presenting to ecclesiastical benefices, of collecting returns and revenues and investing with fiefs, of receiving oaths of fealty for and in the name of the holy empire". The Elector of Saxony was vicar in areas operating under Saxon law (Saxony, Westphalia, Hanover, and northern Germany), while the Elector Palatine was vicar in the remainder of the Empire (Franconia, Swabia, the Rhine, and southern Germany). The Elector of Bavaria replaced the Elector Palatine in 1623, but when the latter was granted a new electorate in 1648, there was a dispute between the two as to which was vicar. In 1659, both purported to act as vicar, but the other vicar recognised the Elector of Bavaria. Later, the two electors made a pact to act as joint vicars, but the Imperial Diet rejected the agreement. In 1711, while the Elector of Bavaria was under the ban of the Empire, the Elector Palatine again acted as vicar, but his cousin was restored to his position upon his restoration three years later. Finally, in 1745, the two agreed to alternate as vicars, with Bavaria starting first. This arrangement was upheld by the Imperial Diet in 1752. In 1777 the question became moot when the Elector Palatine inherited Bavaria. On many occasions, however, there was no interregnum, as a new king had been elected during the lifetime of the previous Emperor.
Frankfurt regularly served as the site of the election from the fifteenth century on, but elections were also held at Cologne (1531), Regensburg (1575 and 1636), and Augsburg (1653 and 1690). An elector could appear in person or could appoint another elector as his proxy. More often, an electoral suite or embassy was sent to cast the vote; the credentials of such representatives were verified by the Archbishop of Mainz, who presided over the ceremony. The deliberations were held at the city hall, but voting occurred in the cathedral. In Frankfurt, a special electoral chapel, or "Wahlkapelle", was used for elections. Under the Golden Bull, a majority of electors sufficed to elect a king, and each elector could cast only one vote. Electors were free to vote for whomsoever they pleased (including themselves), but dynastic considerations played a great part in the choice. Electors drafted a "Wahlkapitulation", or electoral capitulation, which was presented to the king-elect. The capitulation may be described as a contract between the princes and the king, the latter conceding rights and powers to the electors and other princes. Once an individual swore to abide by the electoral capitulation, he assumed the office of King of the Romans.
In the 10th and 11th centuries, princes often acted merely to confirm hereditary succession in the Saxon (Ottonian) and Franconian (Salian) dynasties, whereas beginning from the actual forming of the prince-elector class, elections became less secure (with the election of 1125), though the Staufen dynasty managed to get its sons formally elected in their fathers' lifetimes almost as a formality. After these lines ended in extinction, the electors began to elect kings from different families so that the throne would not once again settle within a single dynasty. For some two centuries, the monarchy was elective both in theory and in practice; the arrangement, however, did not last, since the powerful House of Habsburg managed to secure succession within their dynasty during the fifteenth century. All kings elected from 1438 onwards were from among the Habsburg Archdukes of Austria (and later Kings of Hungary and Bohemia) until 1740, when the archduchy was inherited by a woman, Maria Theresa. A representative of the House of Wittelsbach became elected for a short period of time, but in 1745, Maria Theresa's husband, Francis I of the Habsburg-Lorraine dynasty, became King; all of his successors were also from the same family. Hence, for the greater part of the Empire's history, the role of the electors was largely ceremonial.
High offices.
Each elector held a "High Office of the Empire" ("Reichserzämter") and was a member of the (ceremonial) Imperial Household. The three spiritual electors were all Arch-Chancellors (German: "Erzkanzler", Latin: "Archicancellarius"): the Archbishop of Mainz was Arch-Chancellor of Germany, the Archbishop of Cologne was Arch-Chancellor of Italy, and the Archbishop of Trier was Arch-Chancellor of Burgundy. The other offices were as follows:
When the Duke of Bavaria replaced the Elector Palatine in 1623, he assumed the latter's office of Arch-Steward. When the Count Palatine was granted a new electorate, he assumed the position of Arch-Treasurer of the Empire. When the Duke of Bavaria was banned in 1706, the Elector Palatine returned to the office of Arch-Steward, and in 1710 the Elector of Hanover was promoted to the post of Arch-Treasurer. Matters were complicated by the Duke of Bavaria's restoration in 1714; the Elector of Bavaria resumed the office of Arch-Steward, while the Elector Palatine returned to the post of Arch-Treasurer, and the Elector of Hanover was given the new office of Archbannerbearer. The Electors of Hanover, however, continued to be styled Arch-Treasurers, though the Elector Palatine was the one who actually exercised the office until 1777, when he inherited Bavaria and the Arch-Stewardship. After 1777, no further changes were made to the Imperial Household; new offices were planned for the Electors admitted in 1803, but the Empire was abolished before they could be created. The Duke of Württemberg, however, started to adopt the trappings of the Arch-Bannerbearer.
Many High Officers were entitled to use augmentations on their coats of arms; these augmentations, which were special marks of honour, appeared in the centre of the electors' shields (as shown in the image above) above the other charges (in heraldic terms, the augmentations appeared in the form of inescutcheons). The Arch-Steward used "gules an orb Or" (a gold orb on a red field). The Arch-Marshal utilised the more complicated "per fess sable and argent, two swords in saltire gules" (two red swords arranged in the form of a saltire, on a black and white field). The Arch-Chamberlain's augmentation was "azure a sceptre palewise Or" (a gold sceptre on a blue field), while the Arch-Treasurer's was "gules the crown of Charlemagne Or" (a gold crown on a red field). As noted above, the Elector Palatine and the Elector of Hanover styled themselves Arch-Treasurer from 1714 until 1777; during this time, both electors used the corresponding augmentations. The three Arch-Chancellors and the Arch-Cupbearer did not use any augmentations.
The electors discharged the ceremonial duties associated with their offices only during coronations, where they bore the crown and regalia of the Empire. Otherwise, they were represented by holders of corresponding "Hereditary Offices of the Household". The Arch-Butler was represented by the Butler (Cupbearer) (the Count of Althann), the Arch-Seneschal by the Steward (the Count of Waldburg), the Arch-Chamberlain by the Chamberlain (the Count of Hohenzollern), the Arch-Marshal by the Marshal (the Count of Pappenheim), and the Arch-Treasurer by the Treasurer (the Count of Sinzendorf). The Duke of Württemberg assigned the count of Zeppelin-Aschhausen as hereditary Bannerbearer.

</doc>
