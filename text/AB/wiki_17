<doc id="15533" url="http://en.wikipedia.org/wiki?curid=15533" title="Zionist political violence">
Zionist political violence

Zionist political violence refers to acts of violence committed by Zionists.
Actions have been carried out by individuals and Jewish paramilitary groups such as the Irgun, the Lehi, the Haganah and the Palmach as part of a conflict between Zionists, British authorities, and Palestinian Arabs, regarding land, immigration, and control over Palestine.
British soldiers and officials, United Nations personnel, Palestinian Arab fighters and civilians, and Jewish fighters and civilians have been targets or victims of these actions. Domestic, commercial, and government property, infrastructure, and material have also been attacked.
Main occurrences.
During World War I, Zionist volunteers fought in the Jewish Legion of the British Army against the Ottoman Turks because they expected the British would be less opposed to the Zionist project than the Ottoman authorities.
During the 1920 Nebi Musa riots, the 1921 Jaffa riots and the 1929 Palestine riots, Palestinian Arabs manifested hostility against Jewish immigration and settlement, which provoked the reaction of Jewish militias, sometimes supported by British troops. In 1935, the Irgun, a Zionist underground military organization, split off from the Haganah. The Irgun were the armed expression of the nascent ideology of Revisionist Zionism founded by Ze'ev Jabotinsky. He expressed this ideology as "every Jew had the right to enter Palestine; only active retaliation would deter the Arab and the British; only Jewish armed force would ensure the Jewish state".
During the 1936–39 Arab revolt in Palestine The Irgun began bombing Palestinian Arab civilian targets in 1938. While the Palestinian Arabs were "carefully disarmed" by the British Mandatory authorities by 1939, the Zionists were not.
After the beginning of World War II, the Haganah and Irgun suspended their activity against the British in support of their war against Nazi Germany. During 1944-1945, the most mainstream Jewish paramilitary organization, Haganah, cooperated with the British authorities against the Lehi and Etzel.
At the beginning of the civil war, the Jewish militias organized several bombing attacks against civilians and military Arab targets. On 12 December, Irgun placed a car bomb opposite the Damascus Gate, killing 20 people. On 4 January 1948, the Lehi detonated a lorry bomb against the headquarters of the paramilitary Najjada located in Jaffa's Town Hall, killing 15 Arabs and injuring 80. During the night between 5 and 6 January, the Haganah bombed the Semiramis Hotel in Jerusalem that had been reported to hide Arab militiamen, killing 24 people. The next day, Irgun members in a stolen police van rolled a barrel bomb into a large group of civilians who were waiting for a bus by the Jaffa Gate, killing around 16. Another Irgun bomb went off in the Ramla market on February 18, killing 7 residents and injuring 45. On 28 February, the Palmah organised a bombing attack against a garage at Haifa, killing 30 people.
Condemnation as terrorism.
Irgun was described as a terrorist organization by the United Nations, British, and United States governments, and in media such as "The New York Times" newspaper, and by the Anglo-American Committee of Inquiry. In 1946, The World Zionist Congress strongly condemned terrorist activities in Palestine and "the shedding of innocent blood as a means of political warfare". Irgun was specifically condemned.
Menachem Begin was called a terrorist and a fascist by Albert Einstein and 27 other prominent Jewish intellectuals in a letter to the New York Times which was published on December 4, 1948. Specifically condemned was the participation of the Irgun in the Deir Yassin massacre:
The letter warns American Jews against supporting Begin's request for funding of his political party Herut, and ends with the warning:
Lehi was described as a terrorist organization by the British authorities and United Nations mediator Ralph Bunche.
Jewish public opinion.
During the conflict between Arabs and Jews in Palestine before the war, the criterion of "Purity of arms" was used to distinguish between the respective attitudes of the Irgun and Haganah towards Arabs, with the latter priding itself on its adherence to principle. The Jewish society in the British Mandate Palestine generally disapproved and denounced violent attacks both on grounds moral rejection and political disagreement, stressing that terrorism is counter-productive in the Zionist quest for Jewish self-determination. Generally speaking, this precept requires that "weapons remain pure [and that] they are employed only in self-defence and [never] against innocent civilians and defenceless people". But if it "remained a central value in education" it was "rather vague and intentionally blurred" at the practical level.
In 1946, at a meeting held between the heads of the Haganah, David Ben-Gurion predicted a confrontation between the Arabs of Palestine and the Arab states. Concerning the "principle of purity of arms", he stressed that: "The end does not justify all means. Our war is based on moral grounds" and during the 1948 War, the Mapam, the political party affiliated to Palmach, asked "a strict observance of the Jewish Purity of arms to secure the moral character of [the] war". When he was later criticized by Mapam members for his attitude concerning the Arab refugee problem, Ben-Gurion reminded them the Palestinian exodus from Lydda and Ramle and the fact Palmah officers had been responsible for the "outrage that had encouraged the Arabs' flight made the party uncomfortable."
According to Avi Shlaim, this condemnation of the use of violence is one of the key features of 'the conventional Zionist account or old history' whose 'popular-heroic-moralistic version' is 'taught in Israeli schools and used extensively in the quest for legitimacy abroad'. Benny Morris adds that '[t]he Israelis' collective memory of fighters characterized by "purity of arms" is also undermined by the evidence of [the dozen case] of rapes committed in conquered towns and villages.' According to him, 'after the 1948 war, the Israelis tended to hail the "purity of arms" of its militiamen and soldiers to contrast this with Arab barbarism, which on occasion expressed itself in the mutilation of captured Jewish corpses.' According to him, 'this reinforced the Israelis' positive self-image and helped them "sell" the new state abroad and (...) demonized the enemy'.
Some Israelis justify acts of political violence. Sixty years after participating in the assassination of Count Bernadotte, Geula Cohen had no regrets. As a broadcaster on Lehi's radio, she recalled the threats against Bernadotte in advance of the assassination. "I told him if you are not going to leave Jerusalem and go to your Stockholm, you won't be any more." Asked if it was right to assassinate Bernadotte, she replied, "There is no question about it. We would not have Jerusalem any more." In July 2006, the Menachem Begin Heritage Center organized a conference to mark the 60th anniversary of the bombing. The conference was attended by past and future Prime Minister Benjamin Netanyahu and former members of Irgun. The British Ambassador in Tel Aviv and the Consul-General in Jerusalem protested that a plaque commemorating the bombing stated "For reasons known only to the British, the hotel was not evacuated." Netanyahu, then chairman of Likud and Leader of the Opposition in the Knesset, opined that the bombing was a legitimate act with a military target, distinguishing it from an act of terror intended to harm civilians, since Irgun sent warnings to evacuate the building. He said, "Imagine that Hamas or Hizbullah would call the military headquarters in Tel Aviv and say, 'We have placed a bomb and we are asking you to evacuate the area.' They don't do that. That is the difference." The British Ambassador in Tel Aviv and the Consul-General in Jerusalem protested, saying "We do not think that it is right for an act of terrorism, which led to the loss of many lives, to be commemorated", and wrote to the Mayor of Jerusalem that such an "act of terror" could not be honored. The British government also demanded the removal of the plaque, pointing out that the statement on it accusing the British of failing to evacuate the hotel was untrue and "did not absolve those who planted the bomb." To prevent a diplomatic incident, changes were made in the plaque's text. The final English version says, "Warning phone calls have been made to the hotel, The Palestine Post and the French Consulate, urging the hotel's occupants to leave immediately. The hotel was not evacuated and after 25 minutes the bombs exploded. To the Irgun's regret, 92 persons were killed."
Irgun attacks.
• The Jerusalem Massacre - 1/10/1937
A member of the Irgun Zionist organisation detonated a bomb in the vegetable
market near the Damascus (Nablus) Gate in Jerusalem killing dozens of Arab
civilians and wounding many others.
• The Haifa Massacre - 6/3/1937
Terrorists from the Irgun and Lehi Zionist gangs bombed a market in Haifa killing
18 Arab civilians and wounding 38.
• The Haifa Massacre - 6/7/1938
Terrorists from the Irgun Zionist gang placed two car bombs in a Haifa market
killing 21 Arab civilians and wounding 52.
• The Jerusalem Massacre - 13/7/1938
10 Arabs killed and 31 wounded in a massive explosion in the Arab vegetable
market in the Old City.
• The Jerusalem Massacre - 15/7/1938
A member of the Irgun Zionist gang threw a hand grenade in front of a
mosque in Jerusalem as worshippers were walking out. 10 were killed and 30
were wounded.
• The Haifa Massacre - 25/7/1938
A car bomb was planted by the Irgun Zionist gang in an Arab market in Haifa
which killed 35 Arab civilians and wounded 70.
• The Haifa Massacre - 26/7/1938
A member of Irgun threw a hand grenade in a Haifa market killing 47 Arabs.
• The Jerusalem Massacre - 26/8/1938
A car bomb placed by the Irgun gang exploded in a Jerusalem Arab market
killing 34 Arabs and wounding 35.
• The Haifa Massacre - 27/3/1939
The Irgun Zionist gang detonated two bombs in Haifa killing 27 Arabs and
wounding 39.
• The Balad Al-Shaykh Massacre - 12/6/1939
The Haganah Zionist gang raided the city of Balad Al-Shaykh capturing 35 residents who they then killed. The city of Balad Al-Shaykh is a Palestinian Arab
city located east of Haifa.
• The Haifa Massacre - 19/6/1939
Zionist raiders threw a hand grenade in a Haifa market killing 9 Arabs and
wounding 4.
• The Haifa Massacre - 20/6/1948
78 Arabs were killed and 24 wounded by a bomb placed inside a vegetable
box in a Haifa vegetable market. The Irgun and Lehi gangs were responsible
for this.
• The Al Abbasiyah Massacre - 13/12/1947
A group of Irgun members disguised as British soldiers attacked the village
of Al Abbasiyah and opened fire on its residents sitting outside a village café.
They also bombed a number of their homes and planted several time bombs.
Moreover, British soldiers surrounded the village and allowed the killers to escape
from the northern side of the village. They killed 7 and severely wounded
7 others 2 of whom died later including a 5 year old child.
• The Al-Khasas Massacre - 18/12/1947
3 Zionists from the “Maayan Baruch” kibbutz attacked and shot 5 Arab workers
on their way to work. During the attack, one of the Zionists was stabbed
and killed prompting the commander of the Palmach third battalion, Moshe
Kelman, to ordere a retaliatory operation to burn the homes and kill the men
in Al-Khasas. The commander’s report notes that 12 were killed, all of whom
were women and children.
• The Jerusalem Massacre - 29/12/1947
Irgun members threw a barrel full of explosives near Bab al-Amud in Jerusalem
which resulted in the death of 14 Arabs and the wounding 27 others.
• The Jerusalem Massacre - 30/12/1947
The Irgun gang threw a bomb from a speeding car killing 11 Arabs.
• The Balad Al-Shaykh Massacre - 31/12/1947
A joint force of the first Palmach battalion and a brigade led by Haim Avinoam
attacked the Balad Al-Shaykh village killing 60 civilians, according to Zionist
sources. Those killed included children, women and the elderly, and dozens of
homes were destroyed.
• Al-Sheikh Break Massacre - 31/12/1947
Zionist terrorist groups raided the village of Al-Sheikh Break, killing 40.
• The Jaffa Massacre - 4/1/1948
The Zionist Stern Gang threw a bomb in a crowded plaza in Jaffa, killing 15
people and wounding 98.
• The Al-Saraya Massacre - 4/1/1948
On January 4, 1948 the Irgun Zionist terrorist gang placed a car full of explosives
near Al-Saraya in Jaffa which destroyed all that surrounded it, killed
30 Arabs and wounded several others. A fairly large number of the victims were
Jaffa’s intellectual youth.
• The Semiramis Massacre - 5/1/1948
The Haganah bombed the Semiramis Hotel located in the Katamon neighbourhood
in Jerusalem. The hotel collapsed on its guests, all of whom were Arab,
killing 19 and wounding over 20.
• The Jerusalem Massacre - 7/1/1948
The Irgun gang threw a bomb at the Jaffa Gate in Jerusalem, killing 18 Arab civilians
and wounding 40 others.
• The Al-Saraya Al-Arabeya Massacre - 8/1/1948
Zionist terrorist gangs used a car bomb to kill 70 Arab civilians and wound dozens.
• The Ramla Massacre - 15/1/1948
Palmach soldiers and the Haganah bombed one of the Arab neighbourhoods in
Ramla.
• The Yazur Massacre - 22/1/1948
Yigael Yadin, a Haganah commander, ordered the Palmach commander, Yigal
Allon, to carry out an operation against the village of Yazur. A group from the Palmach
attacked a bus near Yazur, wounding the bus driver a several Arab passengers.
On the same day, another group attacked another bus killing and wounding
several people. These attacks by the Palmach and Givati Brigades on Arab
villages and cars continued for 20 consecutive days while other units detonated
bombs near village homes.
Then the Haganah authorities decided to attack the village and bomb the ice
factory along with two buildings around it. A Haganah group opened fire on the ice
factory in the village, while other groups opened fire and used hand grenades on
the homes in the village. Moreover, an engineering group bombed the Askandroni
building, the ice factory, and killed 15 people.
• The Haifa Massacre - 28/12/1948
Zionist terrorists from the Al-Hadar neighbourhood, located at the top of Al-Abbas
Street in Haifa, rolled down a barrel filled with explosives destroying homes and
killing 20 Arab citizens, as well as wounding 50 others.
• The Tabra Tulkarem Massacre - 10/2/1948
A group of Zionist terrorists stopped Arab citizens going back to the village of
Tabra Tulkarem and opened fire on them, killing 7 and wounding 5 others.
• The Sa’sa’ Massacre - 14/2/1948
A Palmach force raided the village of Sa’sa’ and destroyed 20 inhabited homes,
killing 60 villagers, most of whom were women and children.
• The Jerusalem Massacre - 20/2/1948
The Stern Gang stole a British army vehicle, filled it with explosives, and placed it
in front of the Al Salam building in Jerusalem. The explosion killed 14 Arabs and
5 wounded 26.
• The Haifa Masacre - 20/2/1948
Zionist raiders attacked the Arab neighbourhoods in Haifa with mortar fire killing 6
Arabs and wounding 36 others.
• The Al-Husayniyya Massacre - 13/3/1948
The Haganah raided the village of Al-Husayniyya, destroying homes with explosives
and killing over 30 families.
• The Abu Kabir Massacre - 31/3/1948
Groups from Haganah carried out an armed attack on the Abu Kabir neighbourhood
in Jaffa. They destroyed homes and killed residents fleeing their homes to
seek help.
• The Cairo Train Massacre, Haifa - 31/3/1948
The Stern Gang planted bombs on a Cairo-Haifa train which killed 40 people and
wound 60 others on explosion.
• Ramla Massacre - 1/3/1948
Zionist terrorists planned and carried out this massacre in March 1948 in a market
in the city of Ramla, killing 25 Arab civilians.
• The Deir Yassin Massacre - 9/4/1948
A group of 300 Zionist terrorists attacked the village of Deir Yassin, accompanied
by tanks. The men were lined up and shot; 254 men, women and children were
killed.
• The Qalunya Massacre - 14/4/1948
A force from the Palmach Zionist terrorist group raided Qalunya, bombed several
homes and killed 14 of its residents.
• The Nasir al-Din Massacre - 13/4/1948
A group consisting of forces from the Irgun and Stern Gang in disguise raided the
village of Nasir al-Din opening fired on its inhabitants and killing 50 people. On
the previous day, both Nasir al-Din and Al-Shaykh Qadumi were attacked and 12
were killed.
• The Tiberias Massacre - 19/4/1948
Zionist terrorist gangs bombed a home in Tiberias, killing 14 of its residents.
• The Haifa Massacre - 22/4/1948
Zionist night raiders attacked Haifa from Hadar Alkarmel and occupied homes,
streets and public buildings killing 50 Arabs and wounding 200 others. The Arabs
were surprised by the attack, so they took their women and children to the marina
to move them to the city of Akka during which they were attacked by Zionists who
killing 100 civilians and wounded 200 others.
• The Ayn al-Zaytoun Massacre - 4/5/1948
Ayn al-Zaytoun is a Palestinian village on the outskirts of Safed, the population
of which was 820. The Jewish writer, Netiva Ben-Yehuda writes in her book
FACT SHEET | May 2013 Notorious massacres of Palestinians between 1937 & 1948
“Through the Binding Ropes” about the Ayn al-Zaytoun Massacre saying: “on
May 3rd or 4th, 1948, nearly 39 bound prisoners were shot.”
• The Safed Massacre - 13/5/1948
The Haganah slaughtered about 70 young men from Safed, but there are no
details about this massacre.
• The Abu Shusha Massacre - 14/5/1948
Zionist raisers committed an ugly massacre in the village of Abu Shusha, killing
about 60 of its residents, including men, women, children and the elderly. The
massacre ended with the expulsion of all the residents of the village from their
homes, which were then gradually demolished.
• The Beit Daras Massacre - 21/5/1948
A Zionist force supported by tanks surrounded the village of Beit Daras and
opened fire on it. The people of the village realised the critical situation and
decided to endure the fire and defend their homes at any cost, so they urged the
women, children and the elderly to leave the village to lessen their losses. The
women, children and the elderly headed towards the southern area of the village,
and once they reached the outskirts, were met with Zionist gunfire, despite the
fact that they were defenceless. A large number of them were killed, and the
forces burned down several homes and bombed others.
• The Al-Tantura Massacre - 22/5/1948
This massacre was carried out by the third battalion of the Alexandroni Brigade
and the Zionist plan was to attack the village from two sides; the north and south.
One brigade was to block the road, while a naval boat blocked the withdraw route
by sea. Every attacking unit was provided with a guide from the neighbouring
Zikhron Ya'akov settlement, whose residents knew their way around the village,
and the brigade leadership kept a reserve unit for emergencies. Al-Tantura did
not initiate a battle with the Haganah, but refused their terms, so the attackers
took the men to the village graveyard, lined them up, and killed 200-250 of them.

</doc>
<doc id="15536" url="http://en.wikipedia.org/wiki?curid=15536" title="List of airports">
List of airports

List of airports is an organized list of airports.
Due to the size of the list, it has been broken down into:
List of airports by IATA code: - - - - - - - - - - - - - - - - - - - - - - - - - 

</doc>
<doc id="15538" url="http://en.wikipedia.org/wiki?curid=15538" title="Inclusion body myositis">
Inclusion body myositis

Inclusion body myositis (IBM) is an inflammatory muscle disease, characterized by slowly progressive weakness and wasting of both distal and proximal muscles, most apparent in the muscles of the arms and legs. There are two types: sporadic inclusion body myositis (sIBM) and hereditary inclusion body myopathy (hIBM).
In sporadic inclusion body myositis [MY-oh-sigh-tis] muscle, two processes, one autoimmune and the other degenerative, appear to occur in the muscle cells in parallel. The inflammation aspect is characterized by the cloning of T cells that appear to be driven by specific antigens to invade muscle fibers. The degeneration aspect is characterized by the appearance of holes in the muscle cell vacuoles, deposits of abnormal proteins within the cells and in filamentous inclusions (hence the name inclusion body myositis).
sIBM is a rare yet increasingly prevalent disease, being the most common cause of inflammatory myopathy in the over 50s; the most recent research, done in Australia, indicates that the incidence of IBM varies and is different in different populations and different ethnic groups. The authors found that the current prevalence was 14.9 per million in the overall population, with a prevalence of 51.3 per million population in people over 50 years of age. As seen in these numbers, sIBM is an age-related disease – its incidence increases with age and symptoms usually begin after 50 years of age. It is the most common acquired muscle disorder seen in people over 50, although about 20% of cases display symptoms before the age of 50. Weakness comes on slowly (over months or years) and progresses steadily and usually leads to severe weakness and wasting of arm and leg muscles. It is slightly more common in men than women. Patients may become unable to perform daily living activities and most require assistive devices within 5 to 10 years of symptom onset. sIBM is not considered a fatal disorder – barring complications, all things being equal, sIBM will not kill (but the risk of serious injury due to falls is increased). One common and potentially fatal complication is dysphagia. There is no effective treatment for the disease.
Signs and symptoms.
How sIBM affects individuals is quite variable as is the age of onset (which generally varies from the forties upwards). Because sIBM affects different people in different ways and at different rates, there is no "textbook case."
Eventually, sIBM results in general, progressive muscle weakness. The muscles in the thighs called the quadriceps and the muscles in the arms that control finger flexion—making a fist—are usually affected early on. Common early symptoms include frequent tripping and falling, weakness going up stairs and trouble manipulating the fingers—turning doorknobs, gripping keys, etc. Foot drop in one or both feet has been a symptom of IBM and advanced stages of polymyositis (PM).
During the course of the illness, the patient's mobility is progressively restricted as it becomes hard for him or her to bend down, reach for things, walk quickly and so on. Many patients say they have balance problems and fall easily, as the muscles cannot compensate for an off-balanced posture. Because sIBM makes the leg muscles weak and unstable, patients are very vulnerable to serious injury from tripping or falling down. Although pain has not been traditionally part of the "textbook" description, many patients report severe muscle pain, especially in the thighs.
When present, dysphagia is a progressive condition in patients with inclusion body myositis and often leads to death from aspiration pneumonia. Dysphagia is present in from 40 to 85% of IBM cases.
It is also important to note that IBM can result in diminished aerobic capacity. This decline (in aerobic capacity) is most likely a consequence of the sedentary lifestyle that is often associated with the symptoms of IBM (i.e. progressive muscle weakness, decreased mobility, and increased level of fatigue). Therefore, one focus of treatment should be the improvement of aerobic capacity.
Patients with sIBM usually eventually need to resort to a cane or a walker and in most cases, a wheelchair eventually becomes a necessity.
From a recent article: "The progressive course of s-IBM leads slowly to severe disability. Finger functions can become very impaired, such as for manipulating pens, keys, buttons, and zippers, pulling handles, and firmly grasping handshakes. Arising from a chair becomes difficult. Walking becomes more precarious. Sudden falls, sometimes resulting in major injury to the skull or other bones, can occur, even from walking on minimally-irregular ground or from other minor imbalances outside or in the home, due to weakness of quadriceps and gluteus muscles depriving the patient of automatic posture maintenance. A foot-drop can increase the likelihood of tripping. Dysphagia can occur, usually caused by upper esophageal constriction that often can be symptomatically improved, for several months to years, by bougie dilation per a GI or ENT physician. Respiratory muscle weakness can sometimes eventuate." 
Causes.
The causes of sIBM are currently unknown, though it is likely that it results from the interaction of a number of factors, both genetic and environmental. The understanding of sIBM is slowly maturing and evolving.
Currently, there are two major theories about how sIBM is caused:
1) Some researchers (e.g., Dalakas) advocate the theory that the inflammation-immune reaction, caused by an unknown trigger – likely an undiscovered virus or an autoimmune disorder, is the primary, proximal cause of sIBM and that the degeneration of muscle fibres and protein abnormalities are secondary features.
Despite the arguments "in favor of an adaptive immune response in s-IBM, a purely autoimmune hypothesis for s-IBM is untenable because of the disease's resistance to most immunotherapy." 
2) Some researchers (e.g., Engel and Askanas) advocate the theory that sIBM is a degenerative disorder related to aging of the muscle fibres and that abnormal, potentially pathogenic protein accumulations in myofibers play a key causative role in s-IBM (apparently before the immune system comes into play). This theory emphasizes the abnormal intracellular accumulation of many proteins, protein aggregation and misfolding, proteosome inhibition, and endoplasmic reticulum (ER) stress.
A recent review by Greenberg (2009) discusses the "limitations in the beta-amyloid-mediated theory of IBM myofiber injury," 
Dalakas (2006) said: "we can say that two processes, one autoimmune and the other degenerative, occur in the muscle cells in parallel."
Dalakas (2006) suggested that a chain of events causes IBM—some sort of virus, likely a retrovirus, triggers the cloning of T cells. These T cells appear to be driven by specific antigens to invade muscle fibers. In people with sIBM, the muscle cells display “flags” telling the immune system that they are infected or damaged (the muscles ubiquitously express MHC class I antigens) and this immune process leads to the death of muscle cells. The chronic stimulation of these antigens also causes stress inside the muscle cell in the endoplasmic reticulum (ER) and this ER stress may be enough to cause a self-sustaining T cell response (even after a virus has dissipated). In addition, this ER stress may cause the misfolding of protein. The ER is in charge of processing and folding molecules carrying antigens. In IBM, muscle fibers are overloaded with these major histocompatibility complex (MHC) molecules that carry the antigen protein pieces, leading to more ER stress and more protein misfolding.
A self-sustaining T cell response would make sIBM a type of autoimmune disorder. One confusing aspect is that medications that lower the immune response do not improve sIBM symptoms, as would be expected in the case of an autoimmune disorder.
When studied carefully, it has not been impossible to detect an ongoing viral infection in the muscles. One theory is that a chronic viral infection might be the initial triggering factor setting IBM in motion. There have been a handful of IBM cases—about 15 or so—that have shown clear evidence of a virus called HTLV-1. This is a complex virus that can cause leukemia, but in most cases it lies dormant and people end up being lifelong carriers of the virus. It is too early to say that this is the particular virus directly involved in causing IBM. The Dalakas article says that the best evidence points towards a connection with some type of retrovirus and that a retroviral infection combined with immune recognition of the retrovirus is enough to trigger the inflammation process.
As mentioned above, in the past, some researchers have suggested that it is the protein changes that are primary and that precede or trigger the abnormal immune response. From an article by Askanas and Engel: "Two hypotheses predominate regarding the key pathogenic mechanisms involved in s-IBM: an amyloid-beta-related degenerative process and an immune dysregulation. Ultimately, both may be considered important, and their possible interrelationship may be clarified. An intriguing feature is the accumulation within s-IBM muscle fibers of amyloid-beta (Ab), phosphorylated tau protein, and at least 20 other proteins that are also accumulated in the brain of Alzheimer's disease patients. In the s-IBM muscle fibers, there is evidence of misfolding of proteins, pathologic proteinaceous inclusions including aggresomes, abnormalities of the two protein-disposal systems involving the ubiquitin proteasome pathway and the lysosomes, mitochondrial dysfunctions, and oxidative stress. The pronounced T-cell inflammation can be striking, and it is characterized by activated, antigen-driven, cytotoxic CD8+ T-cells.
Genetic aspects of sIBM.
sIBM is not inherited and is not passed on to the children of IBM patients. There are genetic features that do not directly cause IBM but that appear to predispose a person to getting IBM — having this particular combination of genes increases one's susceptibility to getting IBM. Some 67% of IBM patients have a particular combination of human leukocyte antigen genes in a section of the 8.1 ancestral haplotype in the center of the MHC class II region. sIBM is not passed on from generation to generation, although the susceptibility region of genes may be.
There are also several very rare forms of hereditary inclusion body myopathy (myopathies) that are linked to specific genetic defects and that are passed on from generation to generation. Because these forms do not show inflammation, they are classified as myopathies and not myositis types. Because they do not display inflammation as a primary symptom, they may in fact be similar, but different diseases to sporadic inclusion body myositis. There are several different types, each inherited in different ways. See hereditary inclusion body myopathy.
A 2007 review that summarized current understanding of the contribution of genetic susceptibility factors to the development of sIBM concluded there is no indication that the genes responsible for the familial or hereditary conditions are involved in sIBM.
Differential diagnosis.
IBM is often initially misdiagnosed as polymyositis. A course of prednisone is typically completed with no improvement and eventually sIBM is confirmed. sIBM weakness comes on over months or years and progresses steadily, whereas polymyositis has an onset of weeks or months. Other forms of muscular dystrophy (e.g. limb girdle) must be considered as well.
Diagnosis.
Elevated creatine kinase CK levels (at most ~10 times normal) are typical in sIBM but patients can also present with normal CK levels. Electromyography (EMG) studies usually display abnormalities. Muscle biopsy may display several common findings including; inflammatory cells invading muscle cells, vacuolar degeneration, inclusions or plaques of abnormal proteins. sIBM is a challenge to the pathologist and even with a biopsy, diagnosis can be ambiguous.
Treatment.
There is no standard course of treatment to slow or stop the progression of the disease. sIBM patients do not reliably respond to the anti-inflammatory, immunosuppressant, or immunomodulatory drugs that have been tried. Management is symptomatic. Prevention of falls is an important consideration. Specialized exercise therapy may supplement treatment to enhance quality of life.
Physical therapy is recommended to teach the patient a home exercise program, to teach how to compensate during mobility-gait training with an assistive device, transfers and bed mobility.
There has been a recent report of a novel agent for IBM treatment, Bimagrumab, which is still under study.
Other related disorders.
When sIBM was originally described, the major feature noted was muscle inflammation. Two other disorders were also known to display muscle inflammation, and sIBM was classified along with them. They are dermatomyositis (DM) and polymyositis (PM) and all three illnesses were called idiopathic (of unknown origin) myositis or inflammatory myopathies.
It appears that sIBM and polymyositis share some common features, especially the initial sequence of immune system activation, however, polmyositis comes on over weeks or months, does not display the subsequent muscle degeneration and protein abnormalities as seen in IBM, and as well, polymyositis tends to respond well to treatments, IBM does not. IBM is often confused with (misdiagnosed as) polymyositis. Polymyositis that does not respond to treatment is likely IBM.
Dermatomyositis shares a number of similar physical symptoms and histopathological traits as polymyositis, but exhibits a skin rash not seen in polymyositis or sIBM. It may have different root causes unrelated to either polymyositis or sIBM.

</doc>
<doc id="15539" url="http://en.wikipedia.org/wiki?curid=15539" title="Ion implantation">
Ion implantation

Ion implantation is a materials engineering process by which ions of a material are accelerated in an electrical field and impacted into a solid. This process is used to change the physical, chemical, or electrical properties of the solid. Ion implantation is used in semiconductor device fabrication and in metal finishing, as well as various applications in materials science research. The ions alter the elemental composition of the target, if the ions differ in composition from the target, stop in the target and stay there. They also cause many chemical and physical changes in the target by transferring their energy and momentum to the electrons and atomic nuclei of the target material. This causes a structural change, in that the crystal structure of the target can be damaged or even destroyed by the energetic collision cascades. Because the ions have masses comparable to those of the target atoms, they knock the target atoms out of place more than electron beams do. If the ion energy is sufficiently high (usually tens of MeV) to overcome the coulomb barrier, there can even be a small amount of nuclear transmutation.
General principle.
Ion implantation equipment typically consists of an ion source, where ions of the desired element are produced, an
accelerator, where the ions are electrostatically accelerated to a high energy, and a target chamber, where the ions impinge on a target, which is the material to be implanted. Thus ion implantation is a special case of
particle radiation.
Each ion is typically a single atom or molecule, and thus the actual amount of material implanted in the target is the integral over time of the ion current. This amount is called the dose. The currents supplied by implanters are typically small (microamperes), and thus the dose which can be implanted in a reasonable amount of time is small. Therefore, ion implantation finds application in cases where the amount of chemical change required is small.
Typical ion energies are in the range of 10 to 500 keV (1,600 to 80,000 aJ). Energies in the range 1 to 10 keV (160 to 1,600 aJ) can be used, but result in a penetration of only a few nanometers or less. Energies lower than this result in very little damage to the target, and fall under the designation ion beam deposition. Higher energies can also be used: accelerators capable of 5 MeV (800,000 aJ) are common. However, there is often great structural damage to the target, and because the depth distribution is broad (Bragg peak), the net composition change at any point in the target will be small.
The energy of the ions, as well as the ion species and the composition of the target determine the depth of penetration of the ions in the solid: A monoenergetic ion beam will generally have a broad depth distribution. The average penetration depth is called the range of the ions. Under typical circumstances ion ranges will be between 10 nanometers and 1 micrometer. Thus, ion implantation is especially useful in cases where the chemical or structural change is desired to be near the surface of the target. Ions gradually lose their energy as they travel through the solid, both from occasional collisions with target atoms (which cause abrupt energy transfers) and from a mild drag from overlap of electron orbitals, which is a continuous process. The loss of ion energy in the target is called stopping and can be simulated with the binary collision approximation method.
Accelerator systems for ion implantation are generally classified into medium current (ion beam currents between 10 μA and ~2 mA), high current (ion beam currents up to ~30 mA), high energy (ion energies above 200 keV and up to 10 MeV), and very high dose (efficient implant of dose greater than 1016 ions/cm2).
All varieties of ion implantation beamline designs contain certain general groups of functional components (see image). The first major segment of an ion beamline includes a device known as an ion source to generate the ion species. The source is closely coupled to biased electrodes for extraction of the ions into the beamline and most often to some means of selecting a particular ion species for transport into the main accelerator section. The "mass" selection is often accompanied by passage of the extracted ion beam through a magnetic field region with an exit path restricted by blocking apertures, or "slits", that allow only ions with a specific value of the product of mass and velocity/charge to continue down the beamline. If the target surface is larger than the ion beam diameter and a uniform distribution of implanted dose is desired over the target surface, then some combination of beam scanning and wafer motion is used. Finally, the implanted surface is coupled with some method for collecting the accumulated charge of the implanted ions so that the delivered dose can be measured in a continuous fashion and the implant process stopped at the desired dose level.
Application in semiconductor device fabrication.
Doping.
The introduction of dopants in a semiconductor is the most common application of ion implantation. Dopant ions such as boron, phosphorus or arsenic are generally created from a gas source, so that the purity of the source can be very high. These gases tend to be very hazardous. When implanted in a semiconductor, each dopant atom can create a charge carrier in the semiconductor after annealing. A hole can be created for a p-type dopant, and an electron for an n-type dopant. This modifies the conductivity of the semiconductor in its vicinity. The technique is used, for example, for adjusting the threshold of a MOSFET.
Ion implantation was developed as a method of producing the p-n junction of photovoltaic devices in the late 1970s and early 1980s, along with the use of pulsed-electron beam for rapid annealing, although it has not to date been used for commercial production.
Silicon on insulator.
One prominent method for preparing silicon on insulator (SOI) substrates from conventional silicon substrates is the "SIMOX" (separation by implantation of oxygen) process, wherein a buried high dose oxygen implant is converted to silicon oxide by a high temperature annealing process.
Mesotaxy.
Mesotaxy is the term for the growth of a crystallographically matching phase underneath the surface of the host crystal (compare to epitaxy, which is the growth of the matching phase on the surface of a substrate). In this process, ions are implanted at a high enough energy and dose into a material to create a layer of a second phase, and the temperature is controlled so that the crystal structure of the target is not destroyed. The crystal orientation of the layer can be engineered to match that of the target, even though the exact crystal structure and lattice constant may be very different. For example, after the implantation of nickel ions into a silicon wafer, a layer of nickel silicide can be grown in which the crystal orientation of the silicide matches that of the silicon.
Application in metal finishing.
Tool steel toughening.
Nitrogen or other ions can be implanted into a tool steel target (drill bits, for example). The structural change caused by the implantation produces a surface compression in the steel, which prevents crack propagation and thus makes the material more resistant to fracture. The chemical change can also make the tool more resistant to corrosion.
Surface finishing.
In some applications, for example prosthetic devices such as artificial joints, it is desired to have surfaces very resistant to both chemical corrosion and wear due to friction. Ion implantation is used in such cases to engineer the surfaces of such devices for more reliable performance. As in the case of tool steels, the surface modification caused by ion implantation includes both a surface compression which prevents crack propagation and an alloying of the surface to make it more chemically resistant to corrosion.
Other applications.
Ion beam mixing.
Ion implantation can be used to achieve ion beam mixing, i.e. mixing up atoms of different elements at an interface. This may be useful for achieving graded interfaces or strengthening adhesion between layers of immiscible materials.
Problems with ion implantation.
Crystallographic damage.
Each individual ion produces many point defects in the target crystal on impact such as vacancies and interstitials. Vacancies are crystal lattice points unoccupied by an atom: in this case the ion collides with a target atom, resulting in transfer of a significant amount of energy to the target atom such that it leaves its crystal site. This target atom then itself becomes a projectile in the solid, and can cause successive collision events.
Interstitials result when such atoms (or the original ion itself) come to rest in the solid, but find no vacant space in the lattice to reside. These point defects can migrate and cluster with each other, resulting in dislocation loops and other defects.
Damage recovery.
Because ion implantation causes damage to the crystal structure of the target which is often unwanted, ion implantation processing is often followed by a thermal annealing. This can be referred to as damage recovery.
Amorphization.
The amount of crystallographic damage can be enough to completely amorphize the surface of the target: i.e. it can become an amorphous solid (such a solid produced from a melt is called a glass). In some cases, complete amorphization of a target is preferable to a highly defective crystal: An amorphized film can be regrown at a lower temperature than required to anneal a highly damaged crystal.
Sputtering.
Some of the collision events result in atoms being ejected (sputtered) from the surface, and thus ion implantation will slowly etch away a surface. The effect is only appreciable for very large doses.
Ion channelling.
If there is a crystallographic structure to the target, and especially in semiconductor substrates where the crystal structure is more open, particular crystallographic directions offer much lower stopping than other directions. The result is that the range of an ion can be much longer if the ion travels exactly along a particular direction, for example the <110> direction in silicon and other diamond cubic materials. This effect is called "ion channelling", and, like all the channelling effects, is highly nonlinear, with small variations from perfect orientation resulting in extreme differences in implantation depth. For this reason, most implantation is carried out a few degrees off-axis, where tiny alignment errors will have more predictable effects.
Ion channelling can be used directly in Rutherford backscattering and related techniques as an analytical method to determine the amount and depth profile of damage in crystalline thin film materials.
Hazardous materials.
In the ion implantation semiconductor fabrication process of wafers, it is important for the workers to minimize their exposure to the toxic materials used in the ion implanter process. Such hazardous elements, solid source and gasses are used, such as arsine and phosphine. For this reason, the semiconductor fabrication facilities are highly automated, and may feature negative pressure gas bottles safe delivery system (SDS). Other elements may include antimony, arsenic, phosphorus, and boron. Residue of these elements show up when the machine is opened to atmosphere, and can also be accumulated and found concentrated in the vacuum pumps hardware. It is important not to expose yourself to these carcinogenic, corrosive, flammable, and toxic elements. Many overlapping safety protocols must be used when handling these deadly compounds. Use safety, and read MSDSs.
High voltage safety.
High voltage power supplies in ion implantation equipment can pose a risk of electrocution. In addition, high-energy atomic collisions can generate X-rays and, in some cases, other ionizing radiation and radionuclides. Operators and maintenance personnel should learn and follow the safety advice of the manufacturer and/or the institution responsible for the equipment. Prior to entry to high voltage area, terminal components must be grounded using a grounding stick. Next, power supplies should be locked and tagged in the off state and tagged to prevent unauthorized energizing.
Other types of particle accelerator, such as radio frequency linear particle accelerators and laser wakefield plasma accelerators have their own hazards.

</doc>
<doc id="15570" url="http://en.wikipedia.org/wiki?curid=15570" title="John Ford (disambiguation)">
John Ford (disambiguation)

John Ford (1894–1973) was an American film director who won four Academy Awards.
John Ford or Johnny Ford may also refer to:

</doc>
<doc id="15571" url="http://en.wikipedia.org/wiki?curid=15571" title="John Woo">
John Woo

John Woo SBS (Ng Yu-Sum; born 1 May 1946) is a Hong Kong film director, writer, and producer. He is considered a major influence on the action genre, known for his highly chaotic action sequences, Mexican standoffs, and frequent use of slow-motion. Woo has directed several notable Hong Kong action films, among them, "A Better Tomorrow", "The Killer", "Hard Boiled", and "Red Cliff". His Hollywood films include the action films "Hard Target" and "Broken Arrow", the sci-fi action thriller "Face/Off" and the action spy film "". He also created the comic series "Seven Brothers", published by Virgin Comics. Woo cites his three favorite films as David Lean's "Lawrence of Arabia", Akira Kurosawa's "Seven Samurai" and Jean-Pierre Melville's "Le Samouraï".
Early life.
Woo was born Wu Yu-Seng (Ng Yu-Sum in Cantonese) in Guangzhou, China, amidst the chaos of the Chinese Civil War at the end of October, 1946. Because of school age restrictions, his mother changed his birth date to 22 September 1948, which is what remains on his passport. The Christian Woo family, faced with persecution during Mao Zedong's early anti-bourgeois purges after the communist revolution in China, fled to Hong Kong when he was five.:xv, 3
Impoverished, the Woo family lived in the slums at Shek Kip Mei. His father was a teacher, though rendered unable to work by tuberculosis, and his mother was a manual laborer on construction sites. The family was rendered homeless by the big Shek Kip Mei fire of 1953. Charitable donations from disaster relief efforts enabled the family to relocate; however, violent crime had by then become commonplace in Hong Kong housing projects.
At age three he was diagnosed with a serious medical condition. Following surgery on his spine, he was unable to walk correctly until eight years old, and as a result his right leg is shorter than his left leg. His Christian background shows influences in his films). As a young boy, Woo had wanted to be a Christian minister. He later found a passion for movies influenced by the French New Wave especially Jean-Pierre Melville. Woo has said he was shy and had difficulty speaking, but found making movies a way to explore his feelings and thinking and would "use movies as a language".
The local cinema would prove a haven of retreat. Woo found respite in musical films, such as "The Wizard of Oz" and in American Westerns. He has stated the final scene of "Butch Cassidy and the Sundance Kid" made a particular impression on him in his youth: the device of two comrades, each of whom fire pistols from each hand, is a recurrent spectacle later found in his own work.
Woo married Annie Woo Ngau Chun-lung in 1976 and has three children. He has lived in the United States since 1993.
Hong Kong career.
In 1969, Woo was hired as a script supervisor at Cathay Studios. In 1971, he became an assistant director at Shaw Studios. His directorial debut in 1974 was the feature film "The Young Dragons" (鐵漢柔情, "Tiě hàn róu qíng"). In the Kung fu action genre, it was choreographed by Jackie Chan and featured dynamic camera-work and elaborate action scenes. The film was picked up by Golden Harvest Studio where he went on to direct more martial arts films. He later had success as a comedy director with "Money Crazy" (發錢寒, "Fā qián hàn") (1977), starring Hong Kong comedian Ricky Hui.
By the mid-1980s, Woo was experiencing professional burnout. Several of his films were commercial disappointments, and he felt a distinct lack of creative control. It was during this period of self-imposed exile that director/producer Tsui Hark provided the funding for Woo to film a longtime pet project, "A Better Tomorrow" (1986).
The story of two brothers—one a law enforcement officer, the other a criminal—the film was a financial blockbuster. "A Better Tomorrow" became a defining achievement in Hong Kong action cinema for its combination of emotional drama, slow-motion gunplay, and gritty atmospherics. Its signature visual device of two-handed, two-gunned shootouts within confined quarters—often referred to as "gun fu" was novel, and its diametrical inversion of the "good-guys-bad guys" formula in its characterization would influence later American films.
Woo would make several more Heroic Bloodshed films in the late 1980s and early 1990s, nearly all starring Chow Yun-Fat. These violent gangster thrillers typically focus on men bound by honor and loyalty, at odds with contemporary values of impermanence and expediency. The protagonists of these films, therefore, may be said to present a common lineage with the Chinese literary tradition of loyalty among generals depicted in classics such as "Romance of the Three Kingdoms".
Woo gained international recognition with the release of "The Killer", which became the most successful Hong Kong film in American release since Bruce Lee's "Enter the Dragon" (1973) and garnered Woo an American cult following. "Bullet in the Head" followed a year later failed to find an audience that accepted its political undertones, and failed to recoup its massive budget.
His last Hong Kong film before emigrating to the United States was "Hard Boiled" (1992), a police thriller that served as the antithesis of his previous glorification of gangsters. Most notable of its numerous action scenes is a 30-minute climax set within a hospital. One particular long take follows two characters for exactly 2 minutes and 42 seconds as they fight their way between hospital floors. On the Criterion DVD and laserdisc, this chapter is referenced as "2 minutes, 42 seconds." The film was considerably darker than most of Woo's previous films, depicting a police force nearly helpless to stop the influx of gangsters in the city, and the senseless slaughter of innocents. As a result, it did not match the success of his other films.
"John Woo: Interviews" (ISBN 9781578067763) includes a new 36-page interview with Woo by editor Robert K. Elder, which documents the years 1968 to 1990, from Woo’s early career in working on comedies and kung fu films (in which he gave Jackie Chan one of his first major film roles), to his gunpowder morality plays in Hong Kong.
American career.
An émigré in 1993, the director experienced difficulty in cultural adjustment while contracted with Universal Studios to direct Jean-Claude Van Damme in "Hard Target". As characteristics of other foreign national film directors confronted the Hollywood environment, Woo was unaccustomed to pervasive management concerns, such as limitations on violence and completion schedules. When initial cuts failed to yield an "R" rated film, the studio assumed control of the project and edited footage to produce a cut "suitable for American audiences". A "rough cut" of the film, supposedly the original unrated version, is still circulated among his admirers.
A three-year hiatus saw Woo next direct John Travolta and Christian Slater in "Broken Arrow." A frenetic chase-themed film, the director once again found himself hampered by studio management and editorial concerns. Despite a larger budget than his previous "Hard Target," the final feature lacked the trademark Woo style. Public reception saw modest financial success.
Reluctant to pursue projects which would necessarily entail front-office controls, the director cautiously rejected the script for "Face/Off" several times until it was rewritten to suit him. (The futuristic setting was changed to a contemporary one.) Paramount Pictures also offered the director significantly more freedom to exercise his speciality: emotional characterisation and elaborate action. A complex story of adversaries—each of whom surgically alters their identity—law enforcement agent John Travolta and terrorist Nicolas Cage play a cat-and-mouse game, trapped in each other's outward appearance. "Face/Off" opened in 1997 to critical acclaim and strong attendance. Grosses in the United States exceeded $100 million. "Face/Off" was also nominated for an Academy Award in the category Sound Effects Editing (Mark Stoeckinger) at the 70th Academy Awards.
In 2003, Mr. Woo directed a television pilot entitled "The Robinsons: Lost in Space" for The WB Television Network, based on the 1960s television series "Lost in Space". The pilot was not purchased, although bootleg copies have been made available by fans.
John Woo has made three additional films in Hollywood: "", "Windtalkers" and "Paycheck". "Mission: Impossible II" was the third highest-grossing film in America in 2000, but received mixed reviews. "Windtalkers" and "Paycheck" fared poorly at the box office and were summarily dismissed by critics.
John Woo directed and produced a videogame called "Stranglehold" for games consoles and PC. It is a sequel to his 1992 film, "Hard Boiled". He also produced the 2007 anime movie, "", the sequel to Shinji Aramaki's 2004 film "Appleseed".
Return to Hong Kong.
In 2008, Woo returned to Asian cinema with the completion of the epic war film "Red Cliff", based on an historical battle from "Records of the Three Kingdoms". Produced on a grand scale, it is his first film in China since he emigrated from Hong Kong to the United States in 1993. Part 1 of the film was released throughout Asia in July, 2008, to generally favourable reviews and strong attendance. Part 2 was released in China in January, 2009.
John Woo was presented with a Golden Lion award for Lifetime Achievement at the Venice Film Festival in 2010.
Future film projects.
A CGI, possibly 3D, Mighty Mouse film was announced in 2003 although, as of 2012[ [update]], nothing has yet been produced. There have been rumours that Woo will direct a film version of the videogame "Metroid", however the rights he optioned have since expired.
Woo's next projects are "The Divide", a western concerning the friendship between two workers, one Chinese, the other Irish, on the transcontinental rail-road, while "The Devil's Soldier" is a biopic on Frederick Townsend Ward, an American brought to China in the mid 19th century by the Emperor to suppress rebellion. "Rendezvous in Black" will be an adaptation of the drama/thriller novel of the same name, and "Psi-Ops" is a science fiction thriller about a telepathic agent, and a remake of "Blind Spot".
In May 2008, Woo announced in Cannes that his next movie would be "1949", an epic love story set between the end of World War II and Chinese Civil War to the founding of the People's Republic of China, the shooting of which would take place in China and Taiwan. Its production was due to begin by the end of 2008, with a theatrical release planned in December 2009. However, in early April 2009, the film was cancelled due to script right issues. Reports indicated that Woo might be working on another World War II film, this time about the American Volunteer Group, or the Flying Tigers. The movie was tentatively titled "Flying Tiger Heroes" and Woo is reported as saying it will feature "The most spectacular aerial battle scenes ever seen in Chinese cinema." It was not clear whether Woo would not be directing the earlier war film, or whether it was put on the back burner. Woo has stated that Flying Tiger Heroes would be an "extremely important production" and will "emphasise US-Chinese friendship and the contributions of the Flying Tigers and the Yunnan people during the war of resistance." Woo has announced he will be using IMAX cameras to film the "Flying Tigers" project. “It has always been a dream of mine to explore shooting with IMAX cameras and to work in the IMAX format, and the strong visual element of this film is incredibly well-suited to the tastes of cinemagoers today [...] Using IMAX for Flying Tigers would create a new experience for the audience, and I think it would be another breakthrough for Chinese movies.”
In 2014, after the death of Japanese actor Ken Takakura, Woo announced his next film will "Manhunt", a film based on the novel by Juko Nishimura. The novel has previously been adapted as "Across the River of Wrath" (1976) starring Takakura. Andy Lau, Takeshi Kaneshiro and Shu Qi were in discussion to star in the film.

</doc>
<doc id="15573" url="http://en.wikipedia.org/wiki?curid=15573" title="Japan">
Japan

Japan (Japanese: 日本 Nippon or Nihon; formally 日本国   or Nihon-koku, "State of Japan") is an island nation in East Asia. Located in the Pacific Ocean, it lies to the east of the Sea of Japan, China, North Korea, South Korea and Russia, stretching from the Sea of Okhotsk in the north to the East China Sea and Taiwan in the south. The Kanji that make up Japan's name mean "sun origin", and Japan is often called the "Land of the Rising Sun".
Japan is a stratovolcanic archipelago of 6,852 islands. The four largest are Honshu, Hokkaido, Kyushu, and Shikoku, which make up about ninety-seven percent of Japan's land area. Japan's population of 126 million is the world's tenth largest. Approximately 9.1 million people live in Tokyo, the capital city of Japan, which is the second largest city proper in the OECD. The Greater Tokyo Area, which includes the "de facto" capital of Tokyo and several surrounding prefectures, is the world's largest metropolitan area, with over 30 million residents.
Archaeological research indicates that Japan was inhabited as early as the Upper Paleolithic period. The first written mention of Japan is in Chinese history texts from the 1st century AD. Influence from other regions, mainly Imperial China, followed by periods of isolation, later from Western European influence, has characterized Japan's history. From the 12th century until 1868, Japan was ruled by successive feudal military shoguns in the name of the Emperor. Japan entered into a long period of isolation in the early 17th century, which was only ended in 1853 when a United States fleet pressured Japan to open to the West. Nearly two decades of internal conflict and insurrection followed before the Meiji Emperor was restored as head of state in 1868 and the Empire of Japan was proclaimed, with the Emperor as a divine symbol of the nation. In the late 19th and early 20th centuries, victories in the First Sino-Japanese War, the Russo-Japanese War and World War I allowed Japan to expand its empire during a period of increasing militarism. The Second Sino-Japanese War of 1937 expanded into part of World War II in 1941, which came to an end in 1945 following the atomic bombings of Hiroshima and Nagasaki. Since adopting its revised constitution in 1947, Japan has maintained a unitary constitutional monarchy with an Emperor and an elected legislature called the National Diet.
Japan is a member of the UN, the G7, the G8, and the G20. A major economic and political power, Japan is a developed country and has the world's third-largest economy by nominal GDP and the world's fourth-largest economy by purchasing power parity. It is also the world's fifth-largest exporter and fifth-largest importer. Although Japan has officially renounced its right to declare war, it maintains a modern military with the world's eighth largest military budget, used for self-defense and peacekeeping roles. Japan ranks high in metrics of prosperity such as the Human Development Index, with the Japanese population enjoying the highest life expectancy of any country in the world and the third lowest infant mortality rate.
Etymology.
The English word "Japan" derives from the Chinese pronunciation of the Japanese name, 日本 , which in Japanese is pronounced "Nippon"    or "Nihon"   .
From the Meiji Restoration until the end of World War II, the full title of Japan was "Dai Nippon Teikoku" (大日本帝國), meaning "the Empire of Great Japan". Today the name "Nippon-koku" or "Nihon-koku" (日本国) is used as a formal modern-day equivalent; countries like Japan whose long form does not contain a descriptive designation are generally given a name appended by the character "koku" (国), meaning "country", "nation" or "state".
Japanese people refer to themselves as "Nihonjin" (日本人) and to their language as "Nihongo" (日本語). Both "Nippon" and "Nihon" mean "sun-origin" and are often translated as "Land of the Rising Sun". The term comes from Japanese missions to Imperial China and refers to Japan's eastward position relative to China. Before "Nihon" came into official use, Japan was known as Wa (倭) or Wakoku (倭国).
The English word for Japan came to the West via early trade routes. The Old Mandarin or possibly early Wu Chinese (吳語) pronunciation of Japan was recorded by Marco Polo as "Cipangu". In modern Shanghainese, a Wu dialect, the pronunciation of characters 日本 'Japan' is "Zeppen" ]. The old Malay word for Japan, "Jepang", was borrowed from a southern coastal Chinese dialect, probably Fukienese or Ningpo, and this Malay word was encountered by Portuguese traders in Malacca in the 16th century. Portuguese traders were the first to bring the word to Europe. An early record of the word in English is in a 1565 letter, spelled "Giapan".
History.
Prehistory and ancient history.
A Paleolithic culture around 30,000 BC constitutes the first known habitation of the Japanese archipelago. This was followed from around 14,000 BC (the start of the Jōmon period) by a Mesolithic to Neolithic semi-sedentary hunter-gatherer culture, who include ancestors of both the contemporary Ainu people and Yamato people, characterized by pit dwelling and rudimentary agriculture. Decorated clay vessels from this period are some of the oldest surviving examples of pottery in the world. Around 300 BC, the Yayoi people began to enter the Japanese islands, intermingling with the Jōmon. The Yayoi period, starting around 500 BC, saw the introduction of practices like wet-rice farming, a new style of pottery, and metallurgy, introduced from China and Korea.
Japan first appears in written history in the Chinese "Book of Han". According to the "Records of the Three Kingdoms", the most powerful kingdom on the archipelago during the 3rd century was called Yamataikoku. Buddhism was first introduced to Japan from Baekje of Korea, but the subsequent development of Japanese Buddhism was primarily influenced by China. Despite early resistance, Buddhism was promoted by the ruling class and gained widespread acceptance beginning in the Asuka period (592–710).
The Nara period (710–784) of the 8th century marked the emergence of a strong Japanese state, centered on an imperial court in Heijō-kyō (modern Nara). The Nara period is characterized by the appearance of a nascent literature as well as the development of Buddhist-inspired art and architecture. The smallpox epidemic of 735–737 is believed to have killed as much as one-third of Japan's population. In 784, Emperor Kammu moved the capital from Nara to Nagaoka-kyō before relocating it to Heian-kyō (modern Kyoto) in 794.
This marked the beginning of the Heian period (794–1185), during which a distinctly indigenous Japanese culture emerged, noted for its art, poetry and prose. Murasaki Shikibu's "The Tale of Genji" and the lyrics of Japan's national anthem "Kimigayo" were written during this time.
Buddhism began to spread during the Heian era chiefly through two major sects, Tendai by Saichō, and Shingon by Kūkai. Pure Land Buddhism (Jōdo-shū, Jōdo Shinshū) greatly becomes popular in the latter half of the 11th century.
Feudal era.
Japan's feudal era was characterized by the emergence and dominance of a ruling class of warriors, the samurai. In 1185, following the defeat of the Taira clan in the Genpei War, sung in the epic Tale of Heike, samurai Minamoto no Yoritomo was appointed shogun and established a base of power in Kamakura. After his death, the Hōjō clan came to power as regents for the shoguns. The Zen school of Buddhism was introduced from China in the Kamakura period (1185–1333) and became popular among the samurai class. The Kamakura shogunate repelled Mongol invasions in 1274 and 1281, but was eventually overthrown by Emperor Go-Daigo. Go-Daigo was himself defeated by Ashikaga Takauji in 1336.
Ashikaga Takauji established the shogunate in Muromachi, Kyoto. This was the start of the Muromachi Period (1336–1573). The Ashikaga shogunate achieved glory in the age of Ashikaga Yoshimitsu, and the culture based on Zen Buddhism (art of "Miyabi") prospered. This evolved to Higashiyama Culture, and prospered until the 16th century. On the other hand, the succeeding Ashikaga shogunate failed to control the feudal warlords ("daimyo"), and a civil war (the Ōnin War) began in 1467, opening the century-long Sengoku period ("Warring States").
During the 16th century, traders and Jesuit missionaries from Portugal reached Japan for the first time, initiating direct commercial and cultural exchange between Japan and the West. Oda Nobunaga conquered many other daimyo using European technology and firearms; after he was assassinated in 1582, his successor Toyotomi Hideyoshi unified the nation in 1590. Hideyoshi invaded Korea twice, but following defeats by Korean and Ming Chinese forces and Hideyoshi's death, Japanese troops were withdrawn in 1598. This age is called Azuchi–Momoyama period (1573–1603).
Tokugawa Ieyasu served as regent for Hideyoshi's son and used his position to gain political and military support. When open war broke out, he defeated rival clans in the Battle of Sekigahara in 1600. Ieyasu was appointed shogun in 1603 and established the Tokugawa shogunate at Edo (modern Tokyo). The Tokugawa shogunate enacted measures including "buke shohatto", as a code of conduct to control the autonomous daimyo; and in 1639, the isolationist "sakoku" ("closed country") policy that spanned the two and a half centuries of tenuous political unity known as the Edo period (1603–1868). The study of Western sciences, known as "rangaku", continued through contact with the Dutch enclave at Dejima in Nagasaki. The Edo period also gave rise to "kokugaku" ("national studies"), the study of Japan by the Japanese.
Modern era.
On March 31, 1854, Commodore Matthew Perry and the "Black Ships" of the United States Navy forced the opening of Japan to the outside world with the Convention of Kanagawa. Subsequent similar treaties with Western countries in the Bakumatsu period brought economic and political crises. The resignation of the shogun led to the Boshin War and the establishment of a centralized state nominally unified under the Emperor (the Meiji Restoration).
Adopting Western political, judicial and military institutions, the Cabinet organized the Privy Council, introduced the Meiji Constitution, and assembled the Imperial Diet. The Meiji Restoration transformed the Empire of Japan into an industrialized world power that pursued military conflict to expand its sphere of influence. After victories in the First Sino-Japanese War (1894–1895) and the Russo-Japanese War (1904–1905), Japan gained control of Taiwan, Korea, and the southern half of Sakhalin. Japan's population grew from 35 million in 1873 to 70 million in 1935.
The early 20th century saw a brief period of "Taishō democracy" overshadowed by increasing expansionism and militarization. World War I enabled Japan, on the side of the victorious Allies, to widen its influence and territorial holdings. It continued its expansionist policy by occupying Manchuria in 1931; as a result of international condemnation of this occupation, Japan resigned from the League of Nations two years later. In 1936, Japan signed the Anti-Comintern Pact with Nazi Germany, and the 1940 Tripartite Pact made it one of the Axis Powers. In 1941, Japan negotiated the Soviet–Japanese Neutrality Pact.
The Empire of Japan invaded other parts of China in 1937, precipitating the Second Sino-Japanese War (1937–1945). The Imperial Japanese Army swiftly captured the capital Nanjing and conducted the Nanking Massacre. In 1940, the Empire then invaded French Indochina, after which the United States placed an oil embargo on Japan. On December 7–8, 1941, Japanese forces carried out surprise attacks on Pearl Harbor, attacks on British forces in Malaya, Singapore, and Hong Kong and declared war, bringing the US and the UK into World War II in the Pacific. After the Soviet invasion of Manchuria and the atomic bombings of Hiroshima and Nagasaki in 1945, Japan agreed to an unconditional surrender on August 15. The war cost Japan and the rest of the Greater East Asia Co-Prosperity Sphere millions of lives and left much of the nation's industry and infrastructure destroyed. The Allies (led by the US) repatriated millions of ethnic Japanese from colonies and military camps throughout Asia, largely eliminating the Japanese empire and restoring the independence of its conquered territories. The Allies also convened the International Military Tribunal for the Far East on May 3, 1946 to prosecute some Japanese leaders for war crimes. However, the bacteriological research units and members of the imperial family involved in the war were exonerated from criminal prosecutions by the Supreme Commander for the Allied Powers despite calls for trials for both groups.
In 1947, Japan adopted a new constitution emphasizing liberal democratic practices. The Allied occupation ended with the Treaty of San Francisco in 1952 and Japan was granted membership in the United Nations in 1956. Japan later achieved rapid growth to become the second-largest economy in the world, until surpassed by China in 2010. This ended in the mid-1990s when Japan suffered a major recession. In the beginning of the 21st century, positive growth has signaled a gradual economic recovery. On March 11, 2011, Japan suffered the strongest earthquake in its recorded history; this triggered the Fukushima Daiichi nuclear disaster, one of the worst disasters in the history of nuclear power.
Government and politics.
Japan is a constitutional monarchy whereby the power of the Emperor is very limited. As a ceremonial figurehead, he is defined by the constitution as "the symbol of the State and of the unity of the people." Power is held chiefly by the Prime Minister and other elected members of the Diet, while sovereignty is vested in the Japanese people. Akihito is the current Emperor of Japan; Naruhito, Crown Prince of Japan, stands as next in line to the Chrysanthemum Throne.
Japan's legislative organ is the National Diet, a bicameral parliament. The Diet consists of a House of Representatives with 480 seats, elected by popular vote every four years or when dissolved, and a House of Councillors of 242 seats, whose popularly elected members serve six-year terms. There is universal suffrage for adults over 20 years of age, with a secret ballot for all elected offices. The Diet is dominated by the social liberal Democratic Party of Japan and the conservative Liberal Democratic Party (LDP). The LDP has enjoyed near continuous electoral success since 1955, except for a brief 11-month period between 1993 and 1994, and from 2009 to 2012. It holds 294 seats in the lower house and 83 seats in the upper house.
The Prime Minister of Japan is the head of government and is appointed by the Emperor after being designated by the Diet from among its members. The Prime Minister is the head of the Cabinet, and he appoints and dismisses the Ministers of State. Following the LDP's landslide victory in the 2012 general election, Shinzō Abe replaced Yoshihiko Noda as the Prime Minister on December 26, 2012 and became the country's sixth prime minister to be sworn in 6 years. Although the Prime Minister is formally appointed by the Emperor, the Constitution of Japan explicitly requires the Emperor to appoint whoever is designated by the Diet.
Historically influenced by Chinese law, the Japanese legal system developed independently during the Edo period through texts such as "Kujikata Osadamegaki". However, since the late 19th century the judicial system has been largely based on the civil law of Europe, notably Germany. For example, in 1896, the Japanese government established a civil code based on a draft of the German Bürgerliches Gesetzbuch; with the code remaining in effect with post–World War II modifications. Statutory law originates in Japan's legislature and has the rubber stamp of the Emperor. The Constitution requires that the Emperor promulgate legislation passed by the Diet, without specifically giving him the power to oppose legislation. Japan's court system is divided into four basic tiers: the Supreme Court and three levels of lower courts. The main body of Japanese statutory law is called the Six Codes.
Foreign relations and military.
Japan is a member of the G8, APEC, and "ASEAN Plus Three", and is a participant in the East Asia Summit. Japan signed a security pact with Australia in March 2007 and with India in October 2008. It is the world's third largest donor of official development assistance after the United States and France, donating US$9.48 billion in 2009.
Japan has close economic and military relations with the United States; the US-Japan security alliance acts as the cornerstone of the nation's foreign policy. A member state of the United Nations since 1956, Japan has served as a non-permanent Security Council member for a total of 20 years, most recently for 2009 and 2010. It is one of the G4 nations seeking permanent membership in the Security Council.
Japan is engaged in several territorial disputes with its neighbors: with Russia over the South Kuril Islands, with South Korea over the Liancourt Rocks, with China and Taiwan over the Senkaku Islands, and with China over the EEZ around Okinotorishima. Japan also faces an ongoing dispute with North Korea over the latter's abduction of Japanese citizens and its nuclear weapons and missile program (see also Six-party talks).
Japan maintains one of the largest military budgets of any country in the world. Japan contributed non-combatant troops to the Iraq War but subsequently withdrew its forces. The Japan Maritime Self-Defense Force (JMSDF) is a regular participant in RIMPAC maritime exercises.
Japan's military (the Japan Self-Defense Forces) is restricted by Article 9 of the Japanese Constitution, which renounces Japan's right to declare war or use military force in international disputes. Accordingly Japan's Self-Defence force is a usual military that has never fired shots outside Japan. It is governed by the Ministry of Defense, and primarily consists of the Japan Ground Self-Defense Force (JGSDF), the Japan Maritime Self-Defense Force (JMSDF) and the Japan Air Self-Defense Force (JASDF). The forces have been recently used in peacekeeping operations; the deployment of troops to Iraq marked the first overseas use of Japan's military since World War II. Japan Business Federation has called on the government to lift the ban on arms exports so that Japan can join multinational projects such as the Joint Strike Fighter.
In May 2014 Prime Minister Shinzō Abe said Japan wanted to shed the passiveness it has maintained since the end of World War II and take more responsibility for regional security. He said Japan wanted to play a key role and offered neighboring countries Japan's support.
Administrative divisions.
Japan consists of forty-seven prefectures, each overseen by an elected governor, legislature and administrative bureaucracy. Each prefecture is further divided into cities, towns and villages. The nation is currently undergoing administrative reorganization by merging many of the cities, towns and villages with each other. This process will reduce the number of sub-prefecture administrative regions and is expected to cut administrative costs.
Geography.
Japan has a total of 6,852 islands extending along the Pacific coast of East Asia. The country, including all of the islands it controls, lies between latitudes 24° and 46°N, and longitudes 122° and 146°E. The main islands, from north to south, are Hokkaido, Honshu, Shikoku and Kyushu. The Ryukyu Islands, which includes Okinawa, are a chain to the south of Kyushu. Together they are often known as the Japanese Archipelago.
About 73 percent of Japan is forested, mountainous, and unsuitable for agricultural, industrial, or residential use. As a result, the habitable zones, mainly located in coastal areas, have extremely high population densities. Japan is one of the most densely populated countries in the world.
The islands of Japan are located in a volcanic zone on the Pacific Ring of Fire. They are primarily the result of large oceanic movements occurring over hundreds of millions of years from the mid-Silurian to the Pleistocene as a result of the subduction of the Philippine Sea Plate beneath the continental Amurian Plate and Okinawa Plate to the south, and subduction of the Pacific Plate under the Okhotsk Plate to the north. Japan was originally attached to the eastern coast of the Eurasian continent. The subducting plates pulled Japan eastward, opening the Sea of Japan around 15 million years ago.
Japan has 108 active volcanoes. During the twentieth century several new volcanoes emerged, including Shōwa-shinzan on Hokkaido and Myōjin-shō off the Bayonnaise Rocks in the Pacific. Destructive earthquakes, often resulting in tsunami, occur several times each century. The 1923 Tokyo earthquake killed over 140,000 people. More recent major quakes are the 1995 Great Hanshin earthquake and the 2011 Tōhoku earthquake, a 9.0-magnitude quake which hit Japan on March 11, 2011, and triggered a large tsunami.
Due to its location in the Pacific Ring of Fire, Japan is substantially prone to earthquakes and tsunami, having the highest natural disaster risk in the developed world.
Climate.
The climate of Japan is predominantly temperate, but varies greatly from north to south. Japan's geographical features divide it into six principal climatic zones: Hokkaido, Sea of Japan, Central Highland, Seto Inland Sea, Pacific Ocean, and Ryūkyū Islands. The northernmost zone, Hokkaido, has a humid continental climate with long, cold winters and very warm to cool summers. Precipitation is not heavy, but the islands usually develop deep snowbanks in the winter.
In the Sea of Japan zone on Honshu's west coast, northwest winter winds bring heavy snowfall. In the summer, the region is cooler than the Pacific area, though it sometimes experiences extremely hot temperatures because of the foehn wind. The Central Highland has a typical inland humid continental climate, with large temperature differences between summer and winter, and between day and night; precipitation is light, though winters are usually snowy. The mountains of the Chūgoku and Shikoku regions shelter the Seto Inland Sea from seasonal winds, bringing mild weather year-round.
The Pacific coast features a humid subtropical climate that experiences milder winters with occasional snowfall and hot, humid summers because of the southeast seasonal wind. The Ryukyu Islands have a subtropical climate, with warm winters and hot summers. Precipitation is very heavy, especially during the rainy season.
The average winter temperature in Japan is 5.1 C and the average summer temperature is 25.2 C. The highest temperature ever measured in Japan—40.9 °C—was recorded on August 16, 2007. The main rainy season begins in early May in Okinawa, and the rain front gradually moves north until reaching Hokkaido in late July. In most of Honshu, the rainy season begins before the middle of June and lasts about six weeks. In late summer and early autumn, typhoons often bring heavy rain.
Biodiversity.
Japan has nine forest ecoregions which reflect the climate and geography of the islands. They range from subtropical moist broadleaf forests in the Ryūkyū and Bonin Islands, to temperate broadleaf and mixed forests in the mild climate regions of the main islands, to temperate coniferous forests in the cold, winter portions of the northern islands. Japan has over 90,000 species of wildlife, including the brown bear, the Japanese macaque, the Japanese raccoon dog, and the Japanese giant salamander. A large network of national parks has been established to protect important areas of flora and fauna as well as thirty-seven Ramsar wetland sites. Four sites have been inscribed on the UNESCO World Heritage List for their outstanding natural value.
Environment.
In the period of rapid economic growth after World War II, environmental policies were downplayed by the government and industrial corporations; as a result, environmental pollution was widespread in the 1950s and 1960s. Responding to rising concern about the problem, the government introduced several environmental protection laws in 1970. The oil crisis in 1973 also encouraged the efficient use of energy because of Japan's lack of natural resources. Current environmental issues include urban air pollution (NOx, suspended particulate matter, and toxics), waste management, water eutrophication, nature conservation, climate change, chemical management and international co-operation for conservation.
Japan is a world leader in developing and implementing new environmentally-friendly technologies, subsequently ranking 26th in the 2014 Environmental Performance Index, which measures a nation's commitment to environmental sustainability. As a signatory of the Kyoto Protocol, and host of the 1997 conference that created it, Japan is under treaty obligation to reduce its carbon dioxide emissions and to take other steps to curb climate change.
Economy.
Economic history.
Some of the structural features of Japan's economic growth developed in the Edo period, such as the network of transport routes, by road and water, and the futures contracts, banking and insurance of the Osaka rice brokers. During the Meiji period from 1868, Japan expanded economically with the embrace of the market economy. Many of today's enterprises were founded at the time, and Japan emerged as the most developed nation in Asia. The period of overall real economic growth from the 1960s to the 1980s has been called the Japanese post-war economic miracle: it averaged 7.5 percent in the 1960s and 1970s, and 3.2 percent in the 1980s and early 1990s.
Growth slowed markedly in the 1990s during what the Japanese call the Lost Decade, largely because of the after-effects of the Japanese asset price bubble and domestic policies intended to wring speculative excesses from the stock and real estate markets. Government efforts to revive economic growth met with little success and were further hampered by the global slowdown in 2000. The economy showed strong signs of recovery after 2005; GDP growth for that year was 2.8 percent, surpassing the growth rates of the US and European Union during the same period.
s of 2012[ [update]], Japan is the third largest national economy in the world, after the United States and China, in terms of nominal GDP, and the fourth largest national economy in the world, after the United States, China and India, in terms of purchasing power parity. s of 2013[ [update]], Japan's public debt was more than 200 percent of its annual gross domestic product, the second largest of any nation in the world. In August 2011, Moody's rating has cut Japan's long-term sovereign debt rating one notch from Aa3 to Aa2 inline with the size of the country's deficit and borrowing level. The large budget deficits and government debt since the 2009 global recession and followed by earthquake and tsunami in March 2011 made the rating downgrade. The service sector accounts for three quarters of the gross domestic product.
Exports.
Japan has a large industrial capacity, and is home to some of the largest and most technologically advanced producers of motor vehicles, electronics, machine tools, steel and nonferrous metals, ships, chemical substances, textiles, and processed foods. Agricultural businesses in Japan cultivate 13 percent of Japan's land, and Japan accounts for nearly 15 percent of the global fish catch, second only to China. s of 2010[ [update]], Japan's labor force consisted of some 65.9 million workers. Japan has a low unemployment rate of around four percent. Some 20 million people, around 17 per cent of the population, were below the poverty line in 2007. Housing in Japan is characterized by limited land supply in urban areas.
Japan's exports amounted to US$4,210 per capita in 2005. s of 2012[ [update]], Japan's main export markets were China (18.1 percent), the United States (17.8 percent), South Korea (7.7 percent), Thailand (5.5 percent) and Hong Kong (5.1 percent). Its main exports are transportation equipment, motor vehicles, electronics, electrical machinery and chemicals. Japan's main import markets as of 2012[ [update]] were China (21.3 percent), the US (8.8 percent), Australia (6.4 percent), Saudi Arabia (6.2 percent), United Arab Emirates (5.0 percent), South Korea (4.6 percent) and Qatar (4.0 percent).
Imports.
Japan's main imports are machinery and equipment, fossil fuels, foodstuffs (in particular beef), chemicals, textiles and raw materials for its industries. By market share measures, domestic markets are the least open of any OECD country. Junichiro Koizumi's administration began some pro-competition reforms, and foreign investment in Japan has soared.
Japan ranks 27th of 189 countries in the 2014 Ease of doing business index and has one of the smallest tax revenues of the developed world. The Japanese variant of capitalism has many distinct features: keiretsu enterprises are influential, and lifetime employment and seniority-based career advancement are relatively common in the Japanese work environment. Japanese companies are known for management methods like "The Toyota Way", and shareholder activism is rare.
Some of the largest enterprises in Japan include Toyota, Nintendo, NTT DoCoMo, Canon, Honda, Takeda Pharmaceutical, Sony, Panasonic, Toshiba, Sharp, Nippon Steel, Nippon Oil, and Seven & I Holdings Co.. It has some of the world's largest banks, and the Tokyo Stock Exchange (known for its Nikkei 225 and TOPIX indices) stands as the second largest in the world by market capitalization. s of 2006[ [update]], Japan was home to 326 companies from the Forbes Global 2000 or 16.3 percent. In 2013, it was announced that Japan would be importing shale natural gas.
Science and technology.
Japan is a leading nation in scientific research, particularly technology, machinery and biomedical research. Nearly 700,000 researchers share a US$130 billion research and development budget, the third largest in the world. Japan is a world leader in fundamental scientific research, having produced nineteen Nobel laureates in either physics, chemistry or medicine, three Fields medalists, and one Gauss Prize laureate. Some of Japan's more prominent technological contributions are in the fields of electronics, automobiles, machinery, earthquake engineering, industrial robotics, optics, chemicals, semiconductors and metals. Japan leads the world in robotics production and use, possessing more than 20% (300,000 of 1.3 million) of the world's industrial robots as of 2013[ [update]]—though their share was historically even higher, representing one-half of all industrial robots worldwide in 2000.
The Japan Aerospace Exploration Agency (JAXA) is Japan's space agency; it conducts space, planetary, and aviation research, and leads development of rockets and satellites. It is a participant in the International Space Station: the Japanese Experiment Module (Kibo) was added to the station during Space Shuttle assembly flights in 2008. Japan's plans in space exploration include: launching a space probe to Venus, "Akatsuki"; developing the "Mercury Magnetospheric Orbiter" to be launched in 2016; and building a moon base by 2030.
On September 14, 2007, it launched lunar explorer "SELENE" (Selenological and Engineering Explorer) on an H-IIA (Model H2A2022) carrier rocket from Tanegashima Space Center. "SELENE" is also known as "Kaguya", after the lunar princess of "The Tale of the Bamboo Cutter". "Kaguya" is the largest lunar mission since the Apollo program. Its purpose is to gather data on the moon's origin and evolution. It entered a lunar orbit on October 4, flying at an altitude of about 100 km. The probe's mission was ended when it was deliberately crashed by JAXA into the Moon on June 11, 2009.
Infrastructure.
s of 2011[ [update]], 46.1 percent of energy in Japan was produced from petroleum, 21.3 percent from coal, 21.4 percent from natural gas, 4.0 percent from nuclear power, and 3.3 percent from hydropower. Nuclear power produced 9.2 percent of Japan's electricity, as of 2011, down from 24.9 percent the previous year. However, by May 2012 all of the country's nuclear power plants had been taken offline because of ongoing public opposition following the Fukushima Daiichi nuclear disaster in March 2011, though government officials continued to try to sway public opinion in favor of returning at least some of Japan's 50 nuclear reactors to service. s of November 2014[ [update]], two reactors at Sendai are likely to restart in early 2015. Given its heavy dependence on imported energy, Japan has aimed to diversify its sources and maintain high levels of energy efficiency.
Japan's road spending has been extensive. Its 1.2 million kilometers of paved road are the main means of transportation. A single network of high-speed, divided, limited-access toll roads connects major cities and is operated by toll-collecting enterprises. New and used cars are inexpensive; car ownership fees and fuel levies are used to promote energy efficiency. However, at just 50 percent of all distance traveled, car usage is the lowest of all G8 countries.
Dozens of Japanese railway companies compete in regional and local passenger transportation markets; major companies include seven JR enterprises, Kintetsu Corporation, Seibu Railway and Keio Corporation. Some 250 high-speed Shinkansen trains connect major cities and Japanese trains are known for their safety and punctuality. Proposals for a new Maglev route between Tokyo and Osaka are at an advanced stage. There are 175 airports in Japan; the largest domestic airport, Haneda Airport, is Asia's second-busiest airport. The largest international gateways are Narita International Airport, Kansai International Airport and Chūbu Centrair International Airport. Nagoya Port is the country's largest and busiest port, accounting for 10 percent of Japan's trade value.
Demographics.
Japan's population is estimated at around 127.1 million, with 80% of the population living on Honshū. Japanese society is linguistically and culturally homogeneous, composed of 98.5% ethnic Japanese, with small populations of foreign workers. Zainichi Koreans, Zainichi Chinese, Filipinos, Brazilians mostly of Japanese descent, and Peruvians mostly of Japanese descent are among the small minority groups in Japan. In 2003, there were about 134,700 non-Latin American Western and 345,500 Latin American expatriates, 274,700 of whom were Brazilians (said to be primarily Japanese descendants, or "nikkeijin", along with their spouses), the largest community of Westerners.
The most dominant native ethnic group is the Yamato people; primary minority groups include the indigenous Ainu and Ryukyuan peoples, as well as social minority groups like the "burakumin". There are persons of mixed ancestry incorporated among the Yamato, such as those from Ogasawara Archipelago. In spite of the widespread belief that Japan is ethnically homogeneous (in 2009, foreign-born non-naturalized workers made up only 1.7% of the total population), also because of the absence of ethnicity and/or race statistics for Japanese nationals, at least one analysis describes Japan as a multiethnic society, for example, John Lie. However, this statement is refused by many sectors of Japanese society, who still tend to preserve the idea of Japan being a monocultural society and with this ideology of homogeneity, has traditionally rejected any need to recognize ethnic differences in Japan, even as such claims have been rejected by such ethnic minorities as the Ainu and Ryukyuan people. Former Japanese Prime Minister Tarō Asō has once described Japan as being a nation of "one race, one civilization, one language and one culture".
Japan has the second longest overall life expectancy at birth of any country in the world: 83.5 years for persons born in the period 2010–2015. The Japanese population is rapidly aging as a result of a post–World War II baby boom followed by a decrease in birth rates. In 2012, about 24.1 percent of the population was over 65, and the proportion is projected to rise to almost 40 percent by 2050.
The changes in demographic structure have created a number of social issues, particularly a potential decline in workforce population and increase in the cost of social security benefits like the public pension plan. A growing number of younger Japanese are not marrying or remain childless. In 2011, Japan's population dropped for a fifth year, falling by 204,000 people to 126.24 million people. This was the greatest decline since at least 1947, when comparable figures were first compiled. This decline was made worse by the March 11 earthquake and tsunami, which killed nearly 16,000 people with approximately another 2,600 still listed as missing as of 2014.
Japan's population is expected to drop to 95 million by 2050; demographers and government planners are currently in a heated debate over how to cope with this problem. Immigration and birth incentives are sometimes suggested as a solution to provide younger workers to support the nation's aging population. Japan accepts a steady flow of 15,000 "new Japanese citizens" by "naturalization" (帰化) per year. According to the UNHCR, in 2012 Japan accepted just 18 refugees for resettlement, while the US took in 76,000.
Japan suffers from a high suicide rate. In 2009, the number of suicides exceeded 30,000 for the twelfth straight year. Suicide is the leading cause of death for people under 30.
Religion.
Japan has full religious freedom based on Article 20 of its Constitution. Upper estimates suggest that 84–96 percent of the Japanese population subscribe to Buddhism or Shinto, including a large number of followers of a syncretism of both religions. However, these estimates are based on people affiliated with a temple, rather than the number of true believers. Other studies have suggested that only 30 percent of the population identify themselves as belonging to a religion. According to Edwin Reischauer and Marius Jansen, some 70–80% of the Japanese do not consider themselves believers in any religion.
Nevertheless, the level of participation remains high, especially during festivals and occasions such as the first shrine visit of the New Year. Taoism and Confucianism from China have also influenced Japanese beliefs and customs. Japanese streets are decorated on Tanabata, Obon and Christmas. Fewer than one percent of Japanese are Christian. Other minority religions include Islam, Hinduism, Sikhism, and Judaism, and since the mid-19th century numerous new religious movements have emerged in Japan.
Languages.
More than 99 percent of the population speaks Japanese as their first language. Japanese is an agglutinative language distinguished by a system of honorifics reflecting the hierarchical nature of Japanese society, with verb forms and particular vocabulary indicating the relative status of speaker and listener. Japanese writing uses kanji (Chinese characters) and two sets of kana (syllabaries based on cursive script and radical of kanji), as well as the Latin alphabet and Arabic numerals.
Besides Japanese, the Ryukyuan languages (Amami, Kunigami, Okinawan, Miyako, Yaeyama, Yonaguni), also part of the Japonic language family, are spoken in the Ryukyu Islands chain. Few children learn these languages, but in recent years the local governments have sought to increase awareness of the traditional languages. The Okinawan Japanese dialect is also spoken in the region. The Ainu language, which has no proven relationship to Japanese or any other language, is moribund, with only a few elderly native speakers remaining in Hokkaido. Most public and private schools require students to take courses in both Japanese and English.
Education.
Primary schools, secondary schools and universities were introduced in 1872 as a result of the Meiji Restoration. Since 1947, compulsory education in Japan comprises elementary and middle school, which together last for nine years (from age 6 to age 15). Almost all children continue their education at a three-year senior high school, and, according to the MEXT, as of 2005[ [update]] about 75.9 percent of high school graduates attended a university, junior college, trade school, or other higher education institution.
The two top-ranking universities in Japan are the University of Tokyo and Kyoto University. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of Japanese 15-year-olds as sixth best in the world.
Health.
In Japan, health care is provided by national and local governments. Payment for personal medical services is offered through a universal health insurance system that provides relative equality of access, with fees set by a government committee. People without insurance through employers can participate in a national health insurance program administered by local governments. Since 1973, all elderly persons have been covered by government-sponsored insurance. Patients are free to select the physicians or facilities of their choice.
Culture.
Japanese culture has evolved greatly from its origins. Contemporary culture combines influences from Asia, Europe and North America. Traditional Japanese arts include crafts such as ceramics, textiles, lacquerware, swords and dolls; performances of bunraku, kabuki, noh, dance, and rakugo; and other practices, the tea ceremony, ikebana, martial arts, calligraphy, origami, onsen, Geisha and games. Japan has a developed system for the protection and promotion of both tangible and intangible Cultural Properties and National Treasures. Eighteen sites have been inscribed on the UNESCO World Heritage List, fourteen of which are of cultural significance.
Art.
The Shrines of Ise have been celebrated as the prototype of Japanese architecture. Largely of wood, traditional housing and many temple buildings see the use of tatami mats and sliding doors that break down the distinction between rooms and indoor and outdoor space. Japanese sculpture, largely of wood, and Japanese painting are among the oldest of the Japanese arts, with early figurative paintings dating back to at least 300 BC. The history of Japanese painting exhibits synthesis and competition between native Japanese aesthetics and adaptation of imported ideas.
The interaction between Japanese and European art has been significant: for example ukiyo-e prints, which began to be exported in the 19th century in the movement known as Japonism, had a significant influence on the development of modern art in the West, most notably on post-Impressionism. Famous ukiyo-e artists include Hokusai and Hiroshige. The fusion of traditional woodblock printing and Western art led to the creation of manga, a comic book format that is now popular within and outside Japan. Manga-influenced animation for television and film is called anime. Japanese-made video game consoles have been popular since the 1980s.
Music.
Japanese music is eclectic and diverse. Many instruments, such as the koto, were introduced in the 9th and 10th centuries. The accompanied recitative of the Noh drama dates from the 14th century and the popular folk music, with the guitar-like shamisen, from the sixteenth. Western classical music, introduced in the late 19th century, now forms an integral part of Japanese culture. The imperial court ensemble Gagaku has influenced the work of some modern Western composers.
Notable classical composers from Japan include Toru Takemitsu and Rentarō Taki. Popular music in post-war Japan has been heavily influenced by American and European trends, which has led to the evolution of J-pop, or Japanese popular music. Karaoke is the most widely practiced cultural activity in Japan. A 1993 survey by the Cultural Affairs Agency found that more Japanese had sung karaoke that year than had participated in traditional pursuits such as flower arranging (ikebana) or tea ceremonies.
Literature.
The earliest works of Japanese literature include the "Kojiki" and "Nihon Shoki" chronicles and the "Man'yōshū" poetry anthology, all from the 8th century and written in Chinese characters. In the early Heian period, the system of phonograms known as "kana" (Hiragana and Katakana) was developed. "The Tale of the Bamboo Cutter" is considered the oldest Japanese narrative. An account of Heian court life is given in "The Pillow Book" by Sei Shōnagon, while "The Tale of Genji" by Murasaki Shikibu is often described as the world's first novel.
During the Edo period, the chōnin ("townspeople") overtook the samurai aristocracy as producers and consumers of literature. The popularity of the works of Saikaku, for example, reveals this change in readership and authorship, while Bashō revivified the poetic tradition of the Kokinshū with his haikai (haiku) and wrote the poetic travelogue "Oku no Hosomichi". The Meiji era saw the decline of traditional literary forms as Japanese literature integrated Western influences. Natsume Sōseki and Mori Ōgai were the first "modern" novelists of Japan, followed by Ryūnosuke Akutagawa, Jun'ichirō Tanizaki, Yukio Mishima and, more recently, Haruki Murakami. Japan has two Nobel Prize-winning authors—Yasunari Kawabata (1968) and Kenzaburō Ōe (1994).
Cuisine.
Japanese cuisine is based on combining staple foods, typically Japanese rice or noodles, with a soup and "okazu" — dishes made from fish, vegetable, tofu and the like – to add flavor to the staple food. In the early modern era ingredients such as red meats that had previously not been widely used in Japan were introduced. Japanese cuisine is known for its emphasis on seasonality of food, quality of ingredients and presentation. Japanese cuisine offers a vast array of regional specialties that use traditional recipes and local ingredients. The Michelin Guide has awarded restaurants in Japan more Michelin stars than the rest of the world combined.
Sports.
Traditionally, sumo is considered Japan's national sport. Japanese martial arts such as judo, karate and kendo are also widely practiced and enjoyed by spectators in the country. After the Meiji Restoration, many Western sports were introduced in Japan and began to spread through the education system. Japan hosted the Summer Olympics in Tokyo in 1964. Japan has hosted the Winter Olympics twice: Sapporo in 1972 and Nagano in 1998. Tokyo will host the 2020 Summer Olympics, making Tokyo the first Asian city to host the Olympics twice. Japan is the most successful Asian Rugby Union country, winning the Asian Five Nations a record 6 times and winning the newly formed IRB Pacific Nations Cup in 2011. Japan will host the 2019 IRB Rugby World Cup.
Baseball is currently the most popular spectator sport in the country. Japan's top professional league, now known as Nippon Professional Baseball, was established in 1936. Since the establishment of the Japan Professional Football League in 1992, association football has also gained a wide following. Japan was a venue of the Intercontinental Cup from 1981 to 2004 and co-hosted the 2002 FIFA World Cup with South Korea. Japan has one of the most successful football teams in Asia, winning the Asian Cup four times. Also, Japan recently won the FIFA Women's World Cup in 2011. Golf is also popular in Japan, as are forms of auto racing like the Super GT series and Formula Nippon. The country has produced one NBA player, Yuta Tabuse.
Further reading.
</dl>
External links.
<span id="Related information" />

</doc>
<doc id="15575" url="http://en.wikipedia.org/wiki?curid=15575" title="Geography of Japan">
Geography of Japan

Japan is an island nation in East Asia comprising a stratovolcanic archipelago extending along the Pacific coast of Asia. It lies between 24° to 46° north latitude and from 123° to 146° east longitude. The country is southeast of the Russian Far East, separated by the Sea of Okhotsk; slightly east of Korea, separated by the Sea of Japan; and east-northeast of China and Taiwan, separated by the East China Sea. The closest neighboring country to Japan is the Russian Federation.
The major islands, sometimes called the "Home Islands", are (from north to south) Hokkaidō, Honshū (the "mainland"), Shikoku and Kyūshū. There are also 2,456 islands, including Okinawa, and islets, some inhabited and others uninhabited. In total, as of 2006, Japan's territory is 377923.1 km², of which 374834 km² is land and 3091 km² water. This makes Japan's total area slightly smaller than the U.S. state of Montana, and slightly larger than Norway.
Location: Eastern Asia, island chain between the North Pacific Ocean and the Sea of Japan, east of the Korean Peninsula.
Map references: Asia, Oceania
Area:
Area comparative: 11% smaller than California; slightly larger than Newfoundland and Labrador
Land boundaries: none
Coastline: 29751 km
Maritime claims: 
Climate: varies from tropical in south to cool temperate in north
Terrain: mostly rugged and mountainous, can easily be compared to Norway, both having about 70% of their land in the mountains.
Natural resources: small deposits of coal, oil, iron, and minerals. Major fishing industry.
Land use: 
Irrigated land: 25,000 km² (2010)
Total renewable water resources: 430 km3 (2011)
Composition and topography.
About 72% of Japan is mountainous, with a mountain range running through each of the main islands. Japan's highest mountain is Mount Fuji, with an elevation of 3776 m. Since so very little flat area exists, many hills and mountainsides are cultivated all the way to the top. As Japan is situated in a volcanic zone along the Pacific deeps, frequent low-intensity earth tremors and occasional volcanic activity are felt throughout the islands. Destructive earthquakes occur several times a century. Hot springs are numerous and have been exploited as an economic capital by the leisure industry.
The mountainous islands of the Japanese archipelago form a crescent off the eastern coast of Asia. They are separated from the mainland by the Sea of Japan, which historically served as a protective barrier. The country consists of four principal islands: Hokkaidō, Honshū, Shikoku, and Kyūshū; more than 3,000 adjacent islands and islets, including Izu Ōshima in the Nanpō Islands; and more than 200 other smaller islands, including those of the Amami, Okinawa, and Sakishima chains of the Ryukyu Islands. The national territory also includes the small Bonin or Ogasawara Islands, which include Iwo Jima and the Volcano Islands (Kazan Retto), stretching some 1,100 kilometers from the main islands. A territorial dispute with Russia, dating from the end of World War II, over the two southernmost of the Kuril Islands, Etorofu and Kunashiri, and the smaller Shikotan Island and Habomai Islands northeast of Hokkaidō remains a sensitive spot in Japanese–Russian relations as of 2005. Excluding disputed territory, the archipelago covers about 377,000 square kilometers. No point in Japan is more than 150 kilometers from the sea.
The four major islands are separated by narrow straits and form a natural entity. The Ryukyu Islands curve 970 kilometers southward from Kyūshū.
The distance between Japan and the Korean Peninsula, the nearest point on the Asian continent, is about 200 kilometers at the Korea Strait. Japan has always been linked with the continent through trade routes, stretching in the north toward Siberia, in the west through the Tsushima Islands to the Korean Peninsula, and in the south to the ports on the south China coast.
The Japanese islands are the summits of mountain ridges uplifted near the outer edge of the continental shelf. About 73 percent of Japan's area is mountainous, and scattered plains and intermontane basins (in which the population is concentrated) cover only about 27 percent. A long chain of mountains runs down the middle of the archipelago, dividing it into two halves, the "face," fronting on the Pacific Ocean, and the "back," toward the Sea of Japan. On the Pacific side are steep mountains 1,500 to 3,000 meters high, with deep valleys and gorges. Central Japan is marked by the convergence of the three mountain chains—the Hida, Kiso, and Akaishi mountains—that form the Japanese Alps (Nihon Arupusu), several of whose peaks are higher than 3,000 meters. The highest point in the Japanese Alps is Mount Kita at 3,193 meters. The highest point in the country is Mount Fuji (Fujisan, also erroneously called Fujiyama), a volcano dormant since 1707 that rises to 3,776 meters above sea level in Shizuoka Prefecture. On the Sea of Japan side are plateaus and low mountain districts, with altitudes of 500 to 1,500 meters.
None of the populated plains or mountain basins are extensive in area. The largest, the Kantō Plain, where Tokyo is situated, covers only 13,000 square kilometers. Other important plains are the Nōbi Plain surrounding Nagoya, the Kinai Plain in the Osaka–Kyoto area, the Sendai Plain around the city of Sendai in northeastern Honshū, and the Ishikari Plain on Hokkaidō. Many of these plains are along the coast, and their areas have been increased by reclamation throughout recorded history.
The small amount of habitable land has prompted significant human modification of the terrain over many centuries. Land was reclaimed from the sea and from river deltas by building dikes and drainage, and rice paddies were built on terraces carved into mountainsides. The process continued in the modern period with extension of shorelines and building of artificial islands for industrial and port development, such as Port Island in Kobe and the new Kansai International Airport in Osaka Bay. Hills and even mountains have been razed to provide flat areas for housing.
Rivers are generally steep and swift, and few are suitable for navigation except in their lower reaches. Most rivers are less than 300 kilometers in length, but their rapid flow from the mountains provides a valuable, renewable resource: hydroelectric power generation. Japan's hydroelectric power potential has been exploited almost to capacity. Seasonal variations in flow have led to extensive development of flood control measures. Most of the rivers are very short. The longest, the Shinano River, which winds through Nagano Prefecture to Niigata Prefecture and flows into the Sea of Japan, is only 367 kilometers long. The largest freshwater lake is Lake Biwa, northeast of Kyoto. 
Extensive coastal shipping, especially around the Seto Inland Sea (Seto Naikai), compensates for the lack of navigable rivers. The Pacific coastline south of Tokyo is characterized by long, narrow, gradually shallowing inlets produced by sedimentation, which has created many natural harbors. The Pacific coastline north of Tokyo, the coast of Hokkaidō, and the Sea of Japan coast are generally unindented, with few natural harbors.
In November 2008 Japan filed a request to expand its claimed continental shelf. In April, 2012 the U.N. Commission on the Limits of the Continental Shelf recognized around 310,000 km2 of seabed around Okinotorishima, giving Japan priority over access to seabed resources in nearby areas. According to U.N. Commission on the Limits of the Continental Shelf, the approved expansion is equal to about 82% of Japan's total land area. The People's Republic of China and South Korea have opposed Japan's claim because they view Okinotorishima not as an island, but a group of rocks.
Climate.
Japan belongs to the temperate zone with four distinct seasons, but its climate varies from cool temperate in the north to subtropical in the south. Two primary factors influence Japan's climate: a location near the Asian continent and the existence of major oceanic currents. Two major ocean currents affect Japan: the warm Kuroshio Current (Black Current; also known as the Japan Current); and the cold Oyashio Current (Parent Current; also known as the Okhotsk Current). The Kuroshio Current flows northward on the Pacific side of Japan and warms areas as far north as Tokyo; a small branch, the Tsushima Current, flows up the Sea of Japan side. The Oyashio Current, which abounds in plankton beneficial to coldwater fish, flows southward along the northern Pacific, cooling adjacent coastal areas. The intersection of these currents at 36 north latitude is a bountiful fishing ground.
Its varied geographical features divide Japan into six principal climatic zones.
Japan is generally a rainy country with high humidity. Because of its wide range of latitude and seasonal winds, Japan has a variety of climates, with a latitude range often compared to that of the east coast of North America, from Nova Scotia to the U.S. state of Georgia. Tokyo is at about 35 degrees north latitude, comparable to that of Tehran, Athens, or Las Vegas. Regional climatic variations range from humid continental in the northern island of Hokkaido extending down through northern Japan to the Central Highland, then blending with and eventually changing to a humid subtropical climate on the Pacific Coast and ultimately bordering very closely on a tropical climate on the Ryukyu Islands. Climate also varies dramatically with altitude and with location on the Pacific Ocean or on the Sea of Japan. Northern Japan has warm summers but long, cold winters with heavy snow. Central Japan in its elevated position, has hot, humid summers and moderate to short winters with some areas having very heavy snow, and southwestern Japan has long, hot, humid summers and mild winters. The generally humid, temperate climate exhibits marked seasonal variation such as the blooming of the spring cherry blossoms, the calls of the summer cicada and fall foliage colors that are celebrated in art and literature
The climate from June to September is marked by hot, wet weather brought by tropical airflows from the Pacific Ocean and Southeast Asia. These airflows are full of moisture and deposit substantial amounts of rain when they reach land. There is a marked rainy season, beginning in early June and continuing for about a month. It is followed by hot, sticky weather. Five or six typhoons pass over or near Japan every year from early August to early September, sometimes resulting in significant damage. Annual precipitation averages between 1000 and except in the hyperhumid Kii Peninsula where it can reach 4000 mm, which is the highest rainfall in subtropical latitudes in the world. Maximum precipitation, like the rest of East Asia, occurs in the summer months except on the Sea of Japan coast where strong northerly winds produce a maximum in late autumn and early winter. Except for a few sheltered inland valleys during December and January, precipitation in Japan is above 25 mm of rainfall equivalent in all months of the year, and in the wettest coastal areas it is above 100 mm per month throughout the year.
In winter, the Siberian High develops over the Eurasian land mass and the Aleutian Low develops over the northern Pacific Ocean. The result is a flow of cold air southeastward across Japan that brings freezing temperatures and heavy snowfalls to the central mountain ranges facing the Sea of Japan, but clear skies to areas fronting on the Pacific.
Late June and early July are a rainy season—except in Hokkaidō—as a seasonal rain front or "baiu zensen" (梅雨前線) stays above Japan. In summer and early autumn, typhoons, grown from tropical depressions generated near the equator, attack Japan with furious rainstorms.
The warmest winter temperatures are found in the Nanpō and Bonin Islands, which enjoy a tropical climate due to the combination of latitude, distance from the Asian mainland, and warming effect of winds from the Kuroshio, as well as the Volcano Islands (at the latitude of the southernmost of the Ryukyu Islands, 24° N). The coolest summer temperatures are found on the northeastern coast of Hokkaidō in Kushiro and Nemuro Subprefectures.
Sunshine, in accordance with Japan’s uniformly heavy rainfall, is generally modest in quantity, though no part of Japan receives the consistently gloomy fogs that envelope the Sichuan Basin or Taipei. Amounts range from about six hours per day in the Inland Sea coast and sheltered parts of the Pacific Coast and Kantō Plain to four hours per day on the Sea of Japan coast of Hokkaidō. In December there is a very pronounced sunshine gradient between the Sea of Japan and Pacific coasts, as the former side can receive less than 30 hours and the Pacific side as much as 180 hours. In summer, however, sunshine hours are lowest on exposed parts of the Pacific coast where fogs from the Oyashio current create persistent cloud cover similar to that found on the Kuril Islands and Sakhalin.
As an island nation, Japan has a long coastline. A few prefectures are landlocked: Gunma, Tochigi, Saitama, Nagano, Yamanashi, Gifu, Shiga, and Nara. As Mt. Fuji and the coastal Japanese Alps provide a rain shadow, Nagano and Yamashi Prefectures receive the least precipitation in Honshū, though this still exceeds 900 mm annually. A similar effect is found in Hokkaidō, where Okhotsk Subprefecture receives as little as 750 mm per year. All other prefectures have coasts on the Pacific Ocean, Sea of Japan, Seto Inland Sea or have a body of salt water connected to them. Two prefectures—Hokkaidō and Okinawa—are composed entirely of islands.
The hottest temperature ever measured in Japan, 41.0 °C, occurred in Shimanto, Kochi on August 12, 2013.
Environmental protection.
Environment - current issues: In the 2006 environment annual report, the Ministry of Environment reported that current major issues are: global warming and preservation of the ozone layer, conservation of the atmospheric environment, water and soil, waste management and recycling, measures for chemical substances, conservation of the natural environment and the participation in the international cooperation.
Environment - international agreements: <br>
"party to": Antarctic-Environmental Protocol, Antarctic Treaty, Biodiversity, Climate Change, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes (Basel Convention), Law of the Sea, Marine Dumping, Ozone Layer Protection (Montreal Protocol), Ship Pollution (MARPOL 73/78), Tropical Timber 83, Tropical Timber 94, Wetlands (Ramsar Convention), Whaling <br>
"signed and ratified": Climate Change-Kyoto Protocol
Natural hazards.
Ten percent of the world's active volcanoes—forty in the early 1990s (another 148 were dormant)—are found in Japan, which lies in a zone of extreme crustal instability. As many as 1,500 earthquakes are recorded yearly, and magnitudes of 4 to 7 in magnitude are common. Minor tremors occur almost daily in one part of the country or another, causing slight shaking of buildings. Major earthquakes occur infrequently; the most famous in the twentieth century was the great Kantō earthquake of 1923, in which 130,000 people died. Undersea earthquakes also expose the Japanese coastline to danger from tsunamis (津波) and tidal waves. On March 11, 2011 the country was subject to a devastating magnitude 9.0 earthquake and a massive tsunami as a result. The March 11 quake was the largest ever recorded in Japan and is the world's fourth largest earthquake to strike since 1900, according to the U.S. Geological Service. It struck offshore about 371 km northeast of Tokyo and 130 km east of the city of Sendai, and created a massive tsunami that devastated Japan's northeastern coastal areas. At least 100 aftershocks registering a 6.0 magnitude or higher have followed the main temblor. At least 15,000 people died as a result.
Japan has become a world leader in research on causes and prediction of earthquakes. The development of advanced technology has permitted the construction of skyscrapers even in earthquake-prone areas. Extensive civil defence efforts focus on training in protection against earthquakes, in particular against accompanying fire, which represents the greatest danger.
Another common hazard are several typhoons that reach Japan from the Pacific every year and heavy snowfall during winter in the snow country regions, causing landslides, flooding, and avalanches.
Regions.
Japan is informally divided into eight regions. Each contains several prefectures, except the Hokkaidō region, which covers only Hokkaidō Prefecture.
The region is not an official administrative unit, but has been traditionally used as the regional division of Japan in a number of contexts: for example, maps and geography textbooks divide Japan into the eight regions, weather reports usually give the weather by region, and many businesses and institutions use their home region as part of their name (Kinki Nippon Railway, Chūgoku Bank, Tohoku University, etc.). While Japan has eight High Courts, their jurisdictions do not correspond to the eight regions.
Extreme points.
This is a list of the extreme points of Japan, the points that are farther north, south, east or west than any other location.
Antipodes.
The only part of Japan with antipodes over land are the Ryukyu Islands, though the islands off the western coast of Kyūshū are close. 
The northernmost antipodal island in the Ryukyu Island chain, Nakanoshima, is opposite the Brazilian coast near Capão da Canoa. The other islands south to the Straits of Okinawa correspond to southern Brazil, with Gaja Island being opposite the outskirts of Santo Antônio da Patrulha, Takarajima with Jua, Amami Ōshima covering the villages of Carasinho and Fazenda Pae João, Ginoza, Okinawa with Palmas, Paraná, the Kerama Islands with Pato Branco, Tonaki Island with São Lourenço do Oeste, and Kume Island corresponding to Palma Sola. The main Daitō Islands correspond to near Guaratuba, with Oki Daitō Island near Apiaí.
The Sakishima Islands beyond the straits are antipodal to Paraguay, from the Brazilian border almost to Asunción, with Ishigaki overlapping San Isidro de Curuguaty, and the uninhabited Senkaku Islands surrounding Villarrica.

</doc>
<doc id="15576" url="http://en.wikipedia.org/wiki?curid=15576" title="Demographics of Japan">
Demographics of Japan

The demographic features of the population of Japan include population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects regarding the population.
Based on the census from October 2010, Japan's population was at one of its peaks – 128,057,352. As of March 2012 the population estimate was50,000 making it the world's tenth-most populous country. Current statistics do not showcase much difference in population numbers. Japan's population size can be attributed to high growth rates experienced during the late 19th and early 20th centuries.
In recent years, Japan has experienced net population loss due to falling birth rates and almost no net immigration, despite having one of the highest life expectancies in the world at 81.25 years of age as of 2006. Using the annual estimate for October of each year, the population peaked in 2008 at 128,083,960 and had fallen 285,256 by October 2011. Japan's population density was 336 people per square kilometer.
Based on the latest data from the National Institute of Population and Social Security Research, Japan's population will keep declining by about one million people every year in the coming decades, which will leave Japan with a population of 86 million in 2060. By that time, more than 40% of the population is expected to be over age 65. In 2012, the population had for six consecutive years declined by 212,000, the largest drop on record since 1947 and also a record low of 1.03 million births. In 2014, a new record of population drop happened with 268,000 people. In 2013, more than 20 percent of the population are age 65 and over.
The population ranking of Japan dropped from 7th to 8th in 1990, to 9th in 1998, and to 10th since.
Population.
Census.
Japan collects census information every five years. The exercise is conducted by the Statistics Bureau of the Ministry of Internal Affairs and Communications.
Population density.
Japan's population density is 336 people per square kilometer according to the UN World Populations Prospects as of July 2005. It ranks 37th in a list of countries by population density, ranking directly above India (336 per km2) and directly below Belgium (341 per km2). Between 1955 and 1989, land prices in the six largest cities increased 15,000% (+12% a year). Urban land prices generally increased 40% from 1980 to 1987; in the six largest cities, the price of land doubled over that period. For many families, this trend put housing in central cities out of reach.
The result was lengthy commutes for many workers; daily commutes of two hours each way are not uncommon in the Tokyo area. After a decade of declining land prices, residents have been moving back into central city areas (especially Tokyo's 23 wards), as evidenced by 2005 census figures. Despite the large amount of forested land in Japan, parks in cities are smaller and scarcer than in major West European or North American cities, which average 10 times the amount of parkland per inhabitant.
National and regional governments devote resources to making regional cities and rural areas more attractive by developing transportation networks, social services, industry, and educational institutions in attempts to decentralize settlement and improve the quality of life. Nevertheless, major cities, especially Tokyo, Yokohama, and Chiba and, to a lesser extent, Kyoto, Osaka and Kobe, remain attractive to young people seeking education and jobs.
Urban distribution.
Japan is an urban society with about only 5% of the labor force working in agriculture. Many farmers supplement their income with part-time jobs in nearby towns and cities. About 80 million of the urban population is heavily concentrated on the Pacific shore of Honshū.
Metropolitan Tokyo-Yokohama, with its population of 35 million residents, is the world's most populous city. Japan faces the same problems that confront urban industrialized societies throughout the world: overcrowded cities and congested highways.
Aging of Japan.
Like other postindustrial countries, Japan faces the benefits as well as potential drawbacks associated with an aging population. While countries with young populations may wrestle with problems of crime, poverty, and social unrest, countries with older populations often enjoy higher standards of living. However, the demographic shift in Japan's age profile has triggered concerns about the nation's economic future and the viability of its welfare state.
In 1989, only 11.6% of the population was 65 years or older, but by 2007, that figure had risen to 21.2%, making Japan one of the "greyest" countries.
Demographic statistics from the CIA World Factbook.
Population.
Population in 5 households, 78.7% in urban areas (July 2000). High population density; 329.5 persons per square kilometer for total area; 1,523 persons per square kilometer for habitable land. More than 50% of population lives on 2% of land. (July 1993)
HIV/AIDS.
adult prevalence rate
people living with HIV/AIDS
deaths
Ethnic groups.
98.5% Japanese citizens and 1.5% foreign citizens. The Japanese Census asks respondents their nationality rather than identify people by ethnic groups as do other countries. For example, the United Kingdom Census asks ethnic or racial background which composites the population of the United Kingdom, regardless of their nationalities. Naturalized Japanese citizens and native-born Japanese nationals with multi-ethnic background are considered to be ethnically Japanese in the population census of Japan.
Thus, in spite of the widespread belief that Japan is ethnically homogeneous, at least one academic recommends description of it as a multiethnic society. Internal to Japan, a distinction between "Polynesian-type" (i.e., with darker skin and round eyes) Jomon and "continental-type" (i.e., lighter skin and narrow eyes) Yayoi is sometimes observed, although the popular shorthand does not actually reflect the observed 90% Yayoi / 10% Jomon haploid-group frequency of modern Japanese DNA.
Foreign citizens.
More than 2.5 million (potentially higher because of undocumented migrants) foreigners live in Japan; the number has grown by 14.9% in five years. The two largest sources of foreign citizens in Japan are 0.53 million North and South Koreans and 0.67 million Chinese followed by smaller numbers of Filipinos and Brazilians. Other nationalities include Americans, Canadians, Australians, British, Indonesians, Thais, South Africans, Nigerians, Iranians, Russians, Turks, Indians and European Union nationals.
Historically, the largest number of foreign citizens in Japan were Japanese-born people of Korean ancestry. In recent years Korean-born Koreans have come to outnumber Japanese-born Koreans and Koreans whether foreign or Japanese-born are now substantially outnumbered by Chinese. Indeed, if only foreign-born foreign nationals are considered, the long-term foreign resident population of Japan can justifiably be described as "predominantly Chinese".
Family and sex.
According to a government survey, more than a quarter of unmarried men and women between the ages of 30 and 34 are virgins. 50% of men and women in Japan said they were not “going out with anyone”.
Vital statistics.
Live births, birth and death rates and overall fertility rate in Japan from 1899 to present.
2012 (and 2011) update:
Total fertility rate.
Japan's total fertility rate (TFR) in 2012 was estimated at 1.41 children per woman, increasing slightly from 1.32 in the 2001–05 period. In 2012, the highest TFR was 1.90, in Okinawa, and the lowest was 1.09, in Tokyo. TFR by prefecture for 2000–05, as well as future estimates, have been released.:page 30
Migration.
Internal migration.
Between 6 million and 7 million people moved their residences each year during the 1980s. About 50% of these moves were within the same prefecture; the others were relocations from one prefecture to another. During Japan's economic development in the twentieth century, and especially during the 1950s and 1960s, migration was characterized by urbanization as people from rural areas in increasing numbers moved to the larger metropolitan areas in search of better jobs and education. Out-migration from rural prefectures continued in the late 1980s, but more slowly than in previous decades.
In the 1980s, government policy provided support for new urban development away from the large cities, particularly Tokyo, and assisted regional cities to attract young people to live and work there. Regional cities offered familiarity to those from nearby areas, lower costs of living, shorter commutes, and, in general, a more relaxed lifestyle than could be had in larger cities. Young people continued to move to large cities, however, to attend universities and find work, but some returned to regional cities (a pattern known as U-turn) or to their prefecture of origin (a pattern referred to as "J-turn").
Government statistics show that in the 1980s significant numbers of people left the largest central cities (Tokyo and Osaka) to move to suburbs within their metropolitan areas. In 1988 more than 500,000 people left Tokyo, which experienced a net loss through migration of nearly 73,000 for the year. Osaka had a net loss of nearly 36,000 in the same year.
The prefectures showing the highest net growth are located near the major urban centers, such as Saitama, Chiba, Ibaraki, and Kanagawa around Tokyo, and Hyogo, Nara, and Shiga near Osaka and Kyoto. This pattern suggests a process of suburbanization, people moving away from the cities for affordable housing but still commuting there for work and recreation, rather than a true decentralization. More people in Japan like to live near coastal areas because they are easier to travel around in than the mountainous interior.
Emigration.
About 663,100 Japanese were living abroad, approximately 75,000 of whom had permanent foreign residency, more than six times the number who had that status in 1975. More than 200,000 Japanese went abroad in 1990 for extended periods of study, research, or business assignments. As the government and private corporations have stressed internationalization, greater numbers of individuals have been directly affected, decreasing Japan's historical insularity. By the late 1980s, these problems, particularly the bullying of returnee children in schools, had become a major public issue both in Japan and in Japanese communities abroad.
Immigration.
According to the Japanese immigration centre, the number of foreign residents in Japan has steadily increased, and the number of foreign residents (excluding a small number of illegal immigrants and short-term visitors, such as foreign nationals staying less than 90 days in Japan), exceeded 2.2 million people in 2008.
In 2010, the number of foreigners in Japan was 2,134,151. This includes 209,373 Filipinos, many of whom are married to Japanese nationals, 210,032 Brazilians, the marjority possessing some degree of japanese ancestry, 687,156 Chinese and 565,989 Koreans. Chinese, Filipinos, Koreans, and Brazilians account for about 69.5% of foreign residents in Japan.
The number of naturalizations peaked in 2008 at 16,000, declining to over 9,000 in the most recent year for which data are available. Most of the decline is accounted for by a steep reduction in the number of Japan-born Koreans taking Japanese citizenship. Historically the bulk of those taking Japanese citizenship have not been foreign-born immigrants but rather Japanese-born descendants of Koreans and Taiwanese who lost their citizenship in the Japanese Empire in 1947 as part of the American Occupation policy for Japan.
The concept of "ethnic group" as used by the Japanese statistical authorities differs from that used in ethnicity surveys in North America and certain Western European countries. For example, the UK Census asks for "ethnic or racial background", regardless of each person’s nationality. The Japanese Statistics Bureau, however, does not ask this question. Since the Japanese population census asks about people’s nationality rather than their ethnic background, naturalized Japanese citizens and Japanese nationals with multi-ethnic backgrounds are considered to be ethnically Japanese in the population census of Japan. Thus, although any casual inspection of the population reveals near ethnic homogeneity, it is in one sense possible to describe the population as “multi-ethnic”, although any percentage of ethnic minorities is vanishingly small compared with the numbers in the UK, the United States, Canada, and most other developed countries.
Languages.
The Japanese society of Yamato people is linguistically homogeneous with small populations of Koreans (0.9 million), Chinese/Taiwanese (0.65 million), Filipino (306,000 some being Japanese Filipino; children of Japanese and Filipino parentage). Brazilians (300,000, many of whom are ethnically Japanese) as well as Peruvians and Argentineans of both Latin American and Japanese descent. Japan has indigenous minority groups such as the Ainu and Ryukyuans, who generally speak Japanese.
Japanese citizenship is conferred "jus sanguinis", and monolingual Japanese-speaking minorities often reside in Japan for generations under permanent residency status without acquiring citizenship in their country of birth, although legally they are allowed to do so. This is because Japanese law does not recognise dual citizenship, and so people becoming naturalised Japanese citizens must relinquish citizenship of other countries. Some ethnic Koreans and Chinese and their descendants (who may speak only Japanese and may never have even visited the country whose nationality they hold) do not wish to abandon this other citizenship.
In addition, people taking Japanese citizenship must take a name using the Japanese character sets hiragana, katakana, and/or kanji. Names using Western alphabet, Korean characters, Arabic characters, etc. are not acceptable as legal names. Chinese characters are usually legally acceptable as nearly all Chinese characters are recognized as valid by the Japanese government. Transliterations of non-Japanese names using katakana (e.g. スミス　"Sumisu" for "Smith") are also legally acceptable.
However, some naturalizing foreigners feel that becoming a Japanese citizen should mean that they have a Japanese name and that they should abandon their foreign name, and some foreign residents do not wish to do this—although most "special permanent resident" Koreans and Chinese already use Japanese names, so this is not such an important factor. Nonetheless, some 10,000 Zainichi Koreans naturalize every year. Approximately 98.6% of the population is pure Japanese (though technically this figure includes all naturalized people regardless of race) and 99% of the population speak Japanese as their first language. Non-ethnic Japanese in the past, and to an extent in the present, also live in small numbers in the Japanese archipelago.
Society.
Lifestyle.
Japanese people enjoy a high standard of living, and nearly 90% of the population consider themselves part of the middle class. However, many studies on happiness and satisfaction with life tend to find that Japanese people average relatively low levels of life satisfaction and happiness when compared with most of the highly developed world; the levels have remained consistent if not declining slightly over the last half century. Japanese have been surveyed to be relatively lacking in financial satisfaction.
The suicide rates per 100,000 in Japan in 2009 were 29.2 for men and 10.5 for women, the third-highest in the OECD. In 2010, 32,000 Japanese committed suicide, which translates to an average of 88 Japanese suicides a day in 2010.
Minorities.
Hisabetsu Buraku.
Three native Japanese minority groups can be identified. The largest are the "hisabetsu buraku" or "discriminated communities", also known as the "burakumin". These descendants of premodern outcast hereditary occupational groups, such as butchers, leatherworkers, funeral directors, and certain entertainers, may be considered a Japanese analog of India's Dalits. Discrimination against these occupational groups arose historically because of Buddhist prohibitions against killing and Shinto notions of pollution, as well as governmental attempts at social control.
During the Tokugawa period, such people were required to live in special "buraku" and, like the rest of the population, were bound by sumptuary laws based on the inheritance of social class. The Meiji government abolished most derogatory names applied to these discriminated communities in 1871, but the new laws had little effect on the social discrimination faced by the former outcasts and their descendants. The laws, however, did eliminate the economic monopoly they had over certain occupations. The "buraku" continue to be treated as social outcasts and some casual interactions with the majority caste was perceived taboo until the era after World War II.
Although members of these discriminated communities are physically indistinguishable from other Japanese, they often live in urban ghettoes or in the traditional special hamlets in rural areas. Some attempt to pass as ordinary Japanese, but the checks on family background that are often part of marriage arrangements and employment applications make this difficult. Estimates of their number range from 2 to 4 million (about 2% to 3% of the national population).
Non-Burakumin Japanese claimed that membership in these discriminated communities can be surmised from the location of the family home, occupation, dialect, or mannerisms and, despite legal equality, continued to discriminate against people they surmised to be members of this group. Past and current discrimination has resulted in lower educational attainment and socioeconomic status among "hisabetsu buraku" than among the majority of Japanese. Movements with objectives ranging from "liberation" to encouraging integration have tried to change this situation.
Ryukyuans.
The second largest minority group among Japanese citizens is the Ryukyuan people. They are primarily distinguished from their use of several distinct Ryukyuan languages though use of Ryukyuan is dying out. The Ryukyuan people and language originated in the Ryukyu Islands, which are in Okinawa prefecture.
Ainu.
The third largest minority group among Japanese citizens is the Ainu, whose language is an isolate. Historically, the Ainu were an indigenous hunting and gathering population who occupied most of northern Honshū as late as the Nara period (A.D. 710–94). As Japanese settlement expanded, the Ainu were pushed northward, by the Tokugawa shogunate, the Ainu were pushed into the island of Hokkaido.
Characterized as remnants of a primitive circumpolar culture, the fewer than 20,000 Ainu in 1990 were considered racially distinct and thus not fully Japanese. Disease and a low birth rate had severely diminished their numbers over the past two centuries, and intermarriage had brought about an almost completely mixed population.
Although no longer in daily use, the Ainu language is preserved in epics, songs, and stories transmitted orally over succeeding generations. Distinctive rhythmic music and dances and some Ainu festivals and crafts are preserved, but mainly in order to take advantage of tourism.
Foreign residents.
In 2005, there were 1,555,505 foreign residents "permanently residing" in Japan, representing 1.22% of the Japanese population. Foreign Army personnel, of which there have been up 430,000 from the US and 40,000 BCOF in the immediate post-war years, are not included in the Japanese statistics of foreigners, nor is such personnel subject to local immigration controls. Particularly the US bases and the culture transmitted through them had a significant influence on Japanese fashions.
Most Koreans in Japan today have never been to the Korean Peninsula and do not speak Korean. A significant portion of these foreign residents are the descendants of Illegal immigration of Koreans, a limited number of whom hold a special residence status, granted under the terms of the Normalisation Treaty (22. June 1965) between South Korea and Japan. In many cases special residents, despite being born in Japan and only speaking Japanese, have chosen not to take advantage of Japan's mostly automatic granting of citizenship to special resident applicants.
Beginning in 1947 the Japanese government started a deport those illegal Korean aliens, who were Japanese subjects. In particular, refugees from the massacres conducted by the Korean forces in what is termed the Jeju Uprising, were treated as "smugglers" and frequently forcibly returned to Korea. When the Treaty of San Francisco came into force all ethnic Koreans lost their Japanese citizenship and with it the right to welfare grants, to hold a government job of any kind or to attend Japanese schools. In the following year the government contrived, with the help of the Red Cross, a scheme to "repatriate" Korean residents, who mainly were from the Southern Provinces, to their "home" of North Korea. Between 1959 and 1984 93,430 people used this route. 6,737 were Japanese or Chinese dependents. Most of these departures – 78,276 – occurred before 1962.
All non-Japanese without special residential status (people whose residential roots go back to before WWII) are required by law to register with the government and carry alien registration cards. From the early 1980s, a civil disobedience movement encouraged refusal of the fingerprinting that accompanied registration every five years.
Opponents of fingerprinting argued that it was discriminatory because the only Japanese who were fingerprinted were criminals. The courts upheld fingerprinting, but the law was changed so that fingerprinting was done once rather than with each renewal of the registration, which until a law reform in 1989 was usually required every six months for anybody from the age of 16. Those refusing fingerprinting were denied re-entry permits, thus depriving them from freedom of movement.
Koseki.
Japanese citizens are recorded in "koseki" (family registry) and "jūminhyō" (resident registry) systems, while foreign residents are only recorded in a separate alien registration system. From July 2012 a new registration system will be enacted: all residents (both Japanese and resident foreigners) will be recorded by municipal offices in the jūminhyō system. The Japanese family register system will continue for Japanese citizens, whilst foreigners will be recorded in a separate residency management system administered by immigration offices which will combine previous immigration status and local alien registration systems.
Foreigner-reporting website and hotline.
The Japanese Ministry of Justice maintains a and for "receiving report on ["sic"] illegal stay foreigner." The criteria for reporting include "feeling anxious about a foreigner", and anonymous submissions are permitted. Japanese immigration authorities work in unison with police to investigate those reported, and human rights groups such as Amnesty International have argued that those reported do not receive proper legal protection.
The Daiyo Kangoku system allows police to detain suspects without charges, access to legal counsel or telephone calls for up to 23 days. In October 2006, the foreigner reporting hotline's operating hours were extended to include Saturday, Sunday and national holidays.
Fingerprinting foreigners when entering Japan.
As of November 20, 2007, all foreigners entering Japan must be biometrically registered (photograph and fingerprints) on arrival; this includes people living in Japan on visas as well as permanent residents, but excludes people with special permanent resident permission, diplomats, and those under 16.
Religion.
Shintō and Buddhism are Japan's two major religions. They have co-existed for more than a thousand years. Most Japanese people generally do not exclusively identify themselves as adherents of only one religion, but rather incorporate various elements in a syncretic fashion. There are small Christian and Muslim minorities as well, with the Christian population dating to as early 
as the 1500s, as a result of European missionary work before Shikoku was implemented from 1635-1853.
Businesses.
Businesses for adults are growing inline with old population, such as diapers for adults. In 2012, the yearly sales of Unicharm adult diapers slightly surpassed those for babies.
See also.
Historical:

</doc>
<doc id="15577" url="http://en.wikipedia.org/wiki?curid=15577" title="Politics of Japan">
Politics of Japan

The politics of Japan is conducted in a framework of a multi-party parliamentary representative democratic Constitutional monarchy where the Emperor acts as the ceremonial Head of state, and the Prime Minister acts as the Head of government and the head of the Cabinet which directs the executive branch. Legislative power is vested in the Diet, which consists of the House of Representatives and the House of Councilors. Judicial power is vested in the Supreme Court and lower courts, and Sovereignty is vested in the Japanese people by the Constitution. Japan is generally considered a constitutional monarchy in academic studies, with a system of civil law.
The Constitution of Japan defines the emperor to be "the symbol of the state and of the unity of the people." He performs ceremonial duties and holds no real power, not even emergency reserve powers. Political power is held mainly by the Prime Minister and other elected members of the Diet. The Imperial Throne is succeeded by a member of the Imperial House of Japan as designated by the law.
The chief of the executive branch, the Prime Minister, is appointed by the Emperor as directed by the Diet. He must be a member of either house of the Diet and a civilian. The Cabinet members are nominated by the Prime Minister, and they must also be civilian. Since the Liberal Democratic Party (the LDP) was in power, it has been convention that the President of the party serves as the prime minister.
The Cabinet is composed of Prime Minister and ministers of state, and is responsible to the Diet. The Prime Minister has the power to appoint and remove the ministers, a majority of whom must be the Diet members. The liberal conservative LDP was in power from 1955 to 2009, except for a very short-lived coalition government formed from the likeminded opposition parties in 1993; the largest opposition party was the social liberal Democratic Party of Japan in the late 1990s and late 2000s.
Emperor.
The Emperor of Japan is the constitutional monarch and the ceremonial head of state of Japan. According to the Japan's 1947 constitution, which dissolved the Empire of Japan, he is "the symbol of the state and of the unity of the people." The current emperor is Emperor Akihito.
The emperor appoints the Prime Minister as designated by the Diet and appoints the Chief Justice of the Supreme Court as designated by the Cabinet. Besides that he has, with the advice and approval of the Cabinet, the following nominal powers according to the Japanese Constitution’s Article 7:
Legislative Branch.
In the Japanese Political System, the House of Councillors is the upper house of the Japanese Diet, and is composed of 242 members. The term of office for the elected members of the upper house is six years. The House of Representatives is the lower house, with 480 members. The term of office for the lower house is four years. Citizens of Japan, age twenty and older, may take part in the election process on the basis of universal adult suffrage. The minimum age for members of the House of Representatives is twenty five; thirty years for the House of Councillors.
Executive Branch.
Prime Ministers lead the executive branch, heading the Executive Cabinet. He is appointed by the Emperor of Japan after being elected by the Diet members. He must retain the confidence of the House of Representatives to remain in the office. The Prime Minister has the power to appoint and dismiss the Ministers of State. The literal translation of the Japanese name for the prime minister office is the Minister for the Comprehensive Administration of the Cabinet or Minister that Presides over the Cabinet. The current prime minister of Japan is Shinzō Abe, who has been in office since 26 December 2012.
Judicial Branch.
The judiciary is independent in Japan. The higher judicial members are appointed by the Emperor with the consensus of prime minister and cabinet. Japan's judicial system - drawn from customary law, civil law, and Anglo-American common law - consists of several levels of courts, with the Supreme Court as the final judicial authority. The Japanese constitution, which went into effect on 3 May 1947 includes a bill of rights similar to the United States Bill of Rights, and the Supreme Court has the right of judicial review. Japanese courts use a modified jury system, and there are no administrative courts or claims courts. Because of the judicial system's basis, court decisions are made in accordance with legal statutes. Only Supreme Court decisions have any direct effect on later interpretation of the law. In Japan, the five types of Courts are present–Supreme Court, High Court, District Court, Family Court and Summary Court. 
"See also:" Japanese law, Judicial system of Japan
Political Parties and Elections.
Several political parties exist in Japan, however, mainly LDP dominated the politics of Japan since 1955 and the DPJ played an important role being in opposition several times. 
Policy Making.
Despite an increasingly unpredictable domestic and international environment, policy making conforms to well establish postwar patterns. The close collaboration of the ruling party, the elite bureaucracy and important interest groups often make it difficult to tell who exactly is responsible for specific policy decisions.
Policy Development in Japan.
After a largely informal process within elite circles in which ideas were discussed and developed, steps might be taken to institute more formal policy development. This process often took place in deliberation councils ("shingikai"). There were about 200 "shingikai", each attached to a ministry; their members were both officials and prominent private individuals in business, education, and other fields. The "shingikai" played a large role in facilitating communication among those who ordinarily might not meet. Given the tendency for real negotiations in Japan to be conducted privately (in the "nemawashi", or root binding, process of consensus building), the "shingikai" often represented a fairly advanced stage in policy formulation in which relatively minor differences could be thrashed out and the resulting decisions couched in language acceptable to all. These bodies were legally established but had no authority to oblige governments to adopt their recommendations.
The most important deliberation council during the 1980s was the Provisional Commission for Administrative Reform, established in March 1981 by Prime Minister Suzuki Zenko. The commission had nine members, assisted in their deliberations by six advisers, twenty-one "expert members," and around fifty "councillors" representing a wide range of groups. Its head, Keidanren president Doko Toshio, insisted that government agree to take its recommendations seriously and commit itself to reforming the administrative structure and the tax system. In 1982, the commission had arrived at several recommendations that by the end of the decade had been actualized. These implementations included tax reform; a policy to limit government growth; the establishment, in 1984, of the Management and Coordination Agency to replace the Administrative Management Agency in the Office of the Prime Minister; and privatization of the state-owned railroad and telephone systems. In April 1990, another deliberation council, the Election Systems Research Council, submitted proposals that included the establishment of single-seat constituencies in place of the multiple-seat system.
Another significant policy-making institution in the early 1990s were the LDP's Policy Research Council. It consisted of a number of committees, composed of LDP Diet members, with the committees corresponding to the different executive agencies. Committee members worked closely with their official counterparts, advancing the requests of their constituents, in one of the most effective means through which interest groups could state their case to the bureaucracy through the channel of the ruling party. 
"See also:" Industrial policy of Japan; Monetary and fiscal policy of Japan; Mass media and politics in Japan
Post-war Political Developments in Japan.
Political parties had begun to revive almost immediately after the occupation began. Left-wing organizations, such as the Japan Socialist Party and the Japanese Communist Party, quickly reestablished themselves, as did various conservative parties. The old Rikken Seiyūkai and Rikken Minseito came back as, respectively, the Liberal Party (Nihon Jiyuto) and the Japan Progressive Party (Nihon Shimpoto). The first postwar elections were held in 1948 (women were given the franchise for the first time in 1947), and the Liberal Party's vice president, Yoshida Shigeru (1878–1967), became prime minister. For the 1947 elections, anti-Yoshida forces left the Liberal Party and joined forces with the Progressive Party to establish the new Democratic Party (Minshuto). This divisiveness in conservative ranks gave a plurality to the Japan Socialist Party, which was allowed to form a cabinet, which lasted less than a year. Thereafter, the socialist party steadily declined in its electoral successes. After a short period of Democratic Party administration, Yoshida returned in late 1948 and continued to serve as prime minister until 1954.
Even before Japan regained full sovereignty, the government had rehabilitated nearly 80,000 people who had been purged, many of whom returned to their former political and government positions. A debate over limitations on military spending and the sovereignty of the emperor ensued, contributing to the great reduction in the Liberal Party's majority in the first post-occupation elections (October 1952). After several reorganizations of the armed forces, in 1954 the Japan Self-Defense Forces were established under a civilian director. Cold War realities and the hot war in nearby Korea also contributed significantly to the United States-influenced economic redevelopment, the suppression of communism, and the discouragement of organized labor in Japan during this period.
Continual fragmentation of parties and a succession of minority governments led conservative forces to merge the Liberal Party (Jiyuto) with the Japan Democratic Party (Nihon Minshuto), an offshoot of the earlier Democratic Party, to form the Liberal Democratic Party (Jiyu-Minshuto; LDP) in November 1955, called 1955 System. This party continuously held power from 1955 through 1993, except for short when it was replaced by a new minority government. LDP leadership was drawn from the elite who had seen Japan through the defeat and occupation; it attracted former bureaucrats, local politicians, businessmen, journalists, other professionals, farmers, and university graduates. In October 1955, socialist groups reunited under the Japan Socialist Party, which emerged as the second most powerful political force. It was followed closely in popularity by the Kōmeitō, founded in 1964 as the political arm of the Soka Gakkai (Value Creation Society), until 1991, a lay organization affiliated with the Nichiren Shoshu Buddhist sect. The Komeito emphasized the traditional Japanese beliefs and attracted urban laborers, former rural residents, and women. Like the Japan Socialist Party, it favored the gradual modification and dissolution of the Japan-United States Mutual Security Assistance Pact.
Political Developments since 1990.
The LDP domination lasted until the Diet Lower House elections on 18 July 1993, in which LDP failed to win a majority. A coalition of new parties and existing opposition parties formed a governing majority and elected a new prime minister, Morihiro Hosokawa, in August 1993. His government's major legislative objective was political reform, consisting of a package of new political financing restrictions and major changes in the electoral system. The coalition succeeded in passing landmark political reform legislation in January 1994.
In April 1994, Prime Minister Hosokawa resigned. Prime Minister Tsutomu Hata formed the successor coalition government, Japan's first minority government in almost 40 years. Prime Minister Hata resigned less than two months later. Prime Minister Tomiichi Murayama formed the next government in June 1994 with the coalition of Japan Socialist Party (JSP), the LDP, and the small New Party Sakigake. The advent of a coalition containing the JSP and LDP shocked many observers because of their previously fierce rivalry.
Prime Minister Murayama served from June 1994 to January 1996. He was succeeded by Prime Minister Ryutaro Hashimoto, who served from January 1996 to July 1998. Prime Minister Hashimoto headed a loose coalition of three parties until the July 1998 Upper House election, when the two smaller parties cut ties with the LDP. Hashimoto resigned due to a poor electoral performance by the LDP in the Upper House elections. He was succeeded as party president of the LDP and prime minister by Keizo Obuchi, who took office on 30 July 1998. The LDP formed a governing coalition with the Liberal Party in January 1999, and Keizo Obuchi remained prime minister. The LDP-Liberal coalition expanded to include the New Komeito Party in October 1999. 
Political Developments since 2000.
Prime Minister Obuchi suffered a stroke in April 2000 and was replaced by Yoshiro Mori. After the Liberal Party left the coalition in April 2000, Prime Minister Mori welcomed a Liberal Party splinter group, the New Conservative Party, into the ruling coalition. The three-party coalition made up of the LDP, New Komeito, and the New Conservative Party maintained its majority in the Diet following the June 2000 Lower House elections.
After a turbulent year in office in which he saw his approval ratings plummet to the single digits, Prime Minister Mori agreed to hold early elections for the LDP presidency in order to improve his party's chances in crucial July 2001 Upper House elections. On 24 April 2001, riding a wave of grassroots desire for change, maverick politician Junichiro Koizumi defeated former Prime Minister Hashimoto and other party stalwarts on a platform of economic and political reform. Koizumi was elected as Japan's 87th Prime Minister on 26 April 2001. On 11 October 2003, Prime Minister Koizumi dissolved the lower house and he was re-elected as the president of the LDP. Likewise, that year, the LDP won the election, even though it suffered setbacks from the new opposition party, the liberal and social-democratic Democratic Party (DPJ). A similar event occurred during the 2004 Upper House elections as well.
In a strong move, on 8 August 2005, Prime Minister Junichiro Koizumi called for a snap election to the lower house, as threatened, after LDP stalwarts and opposition DPJ parliamentarians defeated his proposal for a large-scale reform and privatization of Japan Post, which besides being Japan's state-owned postal monopoly is arguably the world's largest financial institution, with nearly 331 trillion yen of assets. The election was scheduled for 11 September 2005, LDP managed landslide victory by under the leadership of Junichiro Koizumi's. The ruling LDP started losing hold since 2006. No prime minister except Koizumi had good public support. On 26 September 2006, new LDP President Shinzo Abe was elected by a special session of the Diet to succeed Junichiro Koizumi as Prime Minister. He was the Japan's youngest post-World War II prime minister and the first born after the war. On 12 September 2007, Prime Minister Shinzo Abe surprised Japan by announcing his resignation from office. He was eventually replaced by Yasuo Fukuda, a veteran of LDP. 
In the meantime, on 4 November 2007, leader of the main opposition party, Ichiro Ozawa announced his resignation from the post of party president, after controversy over an offer to the DPJ to join the ruling coalition in a grand coalition, but has since, with some embarrassment, rescinded his resignation. 
On 11 January 2008, Prime Minister Yasuo Fukuda forced a bill allowing ships to continue a refueling mission in the Indian Ocean in support of US-led operations in Afghanistan. To do so, PM Fukuda used the LDP's overwhelming majority in the Lower House to ignore a previous 'no-vote' of the opposition-controlled Upper House. This was the first time in 50 years that the Lower House voted to ignore the opinion of the Upper House. Fukuda resigned suddenly on 1 September 2008, just a few weeks after reshuffling his cabinet. And, on 1 September 2008, Fukuda's resignation was designed so that the LDP did not suffer a “power vacuum.” It thus caused a leadership election within the LDP, and the winner, Taro Aso was chosen as the new party president and on 24 September 2008, he was appointed as 92nd Prime Minister after the House of Representatives voted in his favor in the extraordinary session of Diet. Later, on 21 July 2009, Prime Minister Aso dissolved the House of Representatives and elections were held on 30 August.
The election results for the House of Representatives were announced on 30 and 31 August 2009. The opposition party DPJ led by Yukio Hatoyama, won a majority by gaining 308 seats (10 seats were won by its allies the Social Democratic Party and the People's New Party). On 16 September 2009, president of DPJ, Hatoyama was elected by the House of Representatives as the 93rd Prime Minister of Japan.
Political Developments since 2010.
On 2 June 2010, Hatoyama resigned due to lack of fulfillments of his policies, both domestically and internationally and soon after, on 8 June, Akihito, Emperor of Japan ceremonially sworn in the newly elected DPJ's president, Naoto Kan as prime minister. Kan suffered an early setback in the Japanese House of Councillors election, 2010. In a routine political change in Japan, DPJ’s new president and former finance minister of Naoto Kan’s cabinet, Yoshihiko Noda was cleared and elected by the Diet as 95th prime minister on 30 August 2011. He was officially appointed as prime minister in the attestation ceremony at imperial palace on 2 September 2011. In an undesired move, Noda dissolved the lower house on 16 November 2012 (as he fails to get support outside the Diet on various domestic issues i.e. tax, nuclear energy) and elections were held on 16 December. The results were in the favor of LDP, which won absolute majority in the leadership of former Prime Minister Shinzō Abe. He was appointed as the 96th Prime Minister of Japan on 26 December 2012. With the changing political situation, earlier in November 2014, Prime Minister Abe called for fresh mandate for the Lower House. In an opinion poll the government failed to win the public trust due to bad economic achievements in the two consecutive quarters and on the tax reforms. The election held on 14 December 2014, and the results were in the favor of LDP and its ally New Komeito, both together manage to secure huge majority by winning 325 seats for the Lower House. The opposition, DPJ, could not manage to provide the alternatives to the voters with its policies and programs. ‘Abenomics’, the ambitious policy of Abe, managed to attract more voters in this election, many Japanese voters supported the policies. Shinzo Abe was sworn as the 97th prime minister on 24 December 2014 and would like go ahead with his agenda of economic revitalization and structural reforms in Japan.
Foreign Relations.
Japan is a member state of the United Nations and pursues a permanent membership of the Security Council; it is one of the "G4 nations" seeking permanent membership. Japan plays an important role in East Asia. The Japanese Constitution prohibits the use of military forces to wage war against other countries. However, the government maintains "Self-Defense Forces" which include air, land and sea components. Japan's deployment of non-combat troops to Iraq marked the first overseas use of its military since World War II. As an economic power, Japan is a member of the G8 and Asia-Pacific Economic Cooperation (APEC), and has developed relations with ASEAN as a member of "ASEAN plus three" and the East Asia Summit. It is a major donor in international aid and development efforts, donating 0.19% of its Gross National Income in 2004.
Japan has territorial disputes with Russia over the Kuril Islands (Northern Territories), with South Korea over Liancourt Rocks (known as "Dokdo" in Korea, "Takeshima" in Japan), with China and Taiwan over the Senkaku Islands and with China over the status of Okinotorishima. These disputes are in part about the control of marine and natural resources, such as possible reserves of crude oil and natural gas. Japan has an ongoing dispute with North Korea over its abduction of Japanese citizens and nuclear weapons program.

</doc>
<doc id="15578" url="http://en.wikipedia.org/wiki?curid=15578" title="Economy of Japan">
Economy of Japan

The economy of Japan is the third largest in the world by nominal GDP, the fourth largest by purchasing power parity and is the world's second largest developed economy. According to the International Monetary Fund, the country's per capita GDP (PPP) was at $36,899, the 22nd-highest in 2013. Japan is a member of Group of Eight. The Japanese economy is forecasted by the Quarterly Tankan survey of business sentiment conducted by the Bank of Japan.
Due to a volatile currency exchange rate, Japan's GDP as measured in dollars fluctuates widely. Accounting for these fluctuations through use of the Atlas method, Japan is estimated to have a GDP per capita of around $38,490.
Japan is the world's third largest automobile manufacturing country, has the largest electronics goods industry, and is often ranked among the world's most innovative countries leading several measures of global patent filings. Facing increasing competition from China and South Korea, manufacturing in Japan today now focuses primarily on high-tech and precision goods, such as optical instruments, hybrid vehicles, and robotics. Besides the Kantō region, the Kansai region is one of the leading industrial clusters and manufacturing centers for the Japanese economy.<ref name="Profile of Osaka/Kansai"></ref>
Japan is the world's largest creditor nation, generally running an annual trade surplus and having a considerable net international investment surplus. As of 2010, Japan possesses 13.7% of the world's private financial assets (the second largest in the world) at an estimated $14.6 trillion. As of 2013, 62 of the Fortune Global 500 companies are based in Japan.
Overview of economy.
In the three decades of economic development following 1960, Japan ignored defense spending in favor of economic growth, thus allowing for a rapid economic growth referred to as the Japanese post-war economic miracle. By the guidance of Ministry of Economy, Trade and Industry, with average growth rates of 10% in the 1960s, 5% in the 1970s, and 4% in the 1980s, Japan was able to establish and maintain itself as the world's second largest economy from 1978 until 2010, when it was supplanted by the People's Republic of China. By 1990, income per capita in Japan equalled or surpassed that in most countries in the West.
However, in the second half of the 1980s, rising stock and real estate prices caused the economic bubble to the Japanese economy by Bank of Japan. The economic bubble came to an abrupt end as the Tokyo Stock Exchange crashed in 1990–92 and real estate prices peaked in 1991. Growth in Japan throughout the 1990s at 1.5% was slower than growth in other major developed economies, giving rise to the term Lost Decade. Nonetheless, GDP per capita growth from 2001-2010 has still managed to outpace Europe and the United States. But Japan public debt remains a daunting task for the Japanese government due to excessive borrowing, social welfare spending with an aging society and lack of economic/industrial growth in recent years to contribute to the tax revenue. Japan had recently embraced the new strategy of economic growth with such goals to be achieved in 2020 as expected. The ICT industry has generated the major outputs to the Japanese economy. Japan is the second largest music market in the world (for more, see Japan Hot 100). With fewer children in the aging Japan, Japanese Anime industry is facing growing Chinese competition in the targeted Chinese market. Japanese Manga industry (from the Japanese Manga (and anime) profession ) enjoys popularity in most of the Asian markets. The issue of export-oriented economy from the Japanese currency intervention causes the effect of improving export but reduces import due to weaker Yen by the Japanese government.
A mountainous, volcanic island country, Japan has inadequate natural resources to support its growing economy and large population, and therefore exports goods in which it has a comparative advantage such as engineering-oriented, Research and Development-led industrial products in exchange for the import of raw materials and petroleum. Japan is among the top-three importers for agricultural products in the world next to the European Union and United States in total volume for covering of its own domestic agricultural consumption. Japan is the world’s largest single national importer of fish and fishery products. Tokyo Metropolitan Central Wholesale Market is the largest wholesale market for primary products in Japan, including the renowned Tsukiji fish market. Japanese whaling, ostensibly for research purposes, has been challenged as illegal under international law.
Although many kinds of minerals were extracted throughout the country, most mineral resources had to be imported in the postwar era. Local deposits of metal-bearing ores were difficult to process because they were low grade. The nation's large and varied forest resources, which covered 70 percent of the country in the late 1980s, were not utilized extensively. Because of political decisions on local, prefectural, and national levels, Japan decided not to exploit its forest resources for economic gain. Domestic sources only supplied between 25 and 30 percent of the nation's timber needs. Agriculture and fishing were the best developed resources, but only through years of painstaking investment and toil. The nation therefore built up the manufacturing and processing industries to convert raw materials imported from abroad. This strategy of economic development necessitated the establishment of a strong economic infrastructure to provide the needed energy, transportation, communications, and technological know-how.
Deposits of gold, magnesium, and silver meet current industrial demands, but Japan is dependent on foreign sources for many of the minerals essential to modern industry. Iron ore, copper, bauxite, and alumina must be imported, as well as many forest products.
Economic history.
The economic history of Japan is one of the most studied economies for its spectacular growth in three different periods. First was the foundation of Edo (in 1603) to whole inland economical developments, second was the Meiji Restoration (in 1868) to be the first non-European power, third was after the defeat of World War II (in 1945) when the island nation rose to become the world's second largest economy.
First contacts with Europe (16th century).
Japan was considered as a country rich in precious metals, mainly owing to Marco Polo's accounts of gilded temples and palaces, but also due to the relative abundance of surface ores characteristic of a massive huge volcanic country, before large-scale deep-mining became possible in Industrial times. Japan was to become a major exporter of silver, copper, and gold during the period until exports for those minerals were banned.
Renaissance Japan was also perceived as a sophisticated feudal society with a high culture and a strong pre-industrial technology. It was densely populated and urbanized. Prominent European observers of the time seemed to agree that the Japanese "excel not only all the other Oriental peoples, they surpass the Europeans as well" (Alessandro Valignano, 1584, "Historia del Principo y Progresso de la Compania de Jesus en las Indias Orientales).
Early European visitors were amazed by the quality of Japanese craftsmanship and metalsmithing. This stems from the fact that Japan itself is rather poor in natural resources found commonly in Europe, especially iron. Thus, the Japanese were famously frugal with their consumable resources; what little they had they used with expert skill.
The cargo of the first Portuguese ships (usually about 4 smaller-sized ships every year) arriving in Japan almost entirely consisted of Chinese goods (silk, porcelain). The Japanese were very much looking forward to acquiring such goods, but had been prohibited from any contacts with the Emperor of China, as a punishment for Wakō pirate raids. The Portuguese (who were called "Nanban", lit. Southern Barbarians) therefore found the opportunity to act as intermediaries in Asian trade.
Edo period (1603–1868).
The beginning of the Edo period coincides with the last decades of the Nanban trade period, during which intense interaction with European powers, on the economic and religious plane, took place. It is at the beginning of the Edo period that Japan built her first ocean-going Western-style warships, such as the "San Juan Bautista", a 500-ton galleon-type ship that transported a Japanese embassy headed by Hasekura Tsunenaga to the Americas, which then continued to Europe. Also during that period, the "bakufu" commissioned around 350 Red Seal Ships, three-masted and armed trade ships, for intra-Asian commerce. Japanese adventurers, such as Yamada Nagamasa, were active throughout Asia.
In order to eradicate the influence of Christianization, Japan entered in a period of isolation called sakoku, during which its economy enjoyed stability and mild progress.
Economic development during the Edo period included urbanization, increased shipping of commodities, a significant expansion of domestic and, initially, foreign commerce, and a diffusion of trade and handicraft industries. The construction trades flourished, along with banking facilities and merchant associations. Increasingly, "han" authorities oversaw the rising agricultural production and the spread of rural handicrafts.
By the mid-eighteenth century, Edo had a population of more than 1 million and Osaka and Kyoto each had more than 400,000 inhabitants. Many other castle towns grew as well. Osaka and Kyoto became busy trading and handicraft production centers, while Edo was the center for the supply of food and essential urban consumer goods.
Rice was the base of the economy, as the daimyo collected the taxes from the peasants in the form of rice. Taxes were high, about 40% of the harvest. The rice was sold at the "fudasashi" market in Edo. To raise money, the daimyo used forward contracts to sell rice that was not even harvested yet. These contracts were similar to modern futures trading.
During the period, Japan progressively studied Western sciences and techniques (called "rangaku", literally "Dutch studies") through the information and books received through the Dutch traders in Dejima. The main areas that were studied included geography, medicine, natural sciences, astronomy, art, languages, physical sciences such as the study of electrical phenomena, and mechanical sciences as exemplified by the development of Japanese clockwatches, or wadokei, inspired from Western techniques.
Prewar period (1868–1945).
Since the mid-19th century, after the Meiji restoration, the country was opened up to Western commerce and influence and Japan has gone through two periods of economic development. The first began in earnest in 1868 and extended through to World War II; the second began in 1945 and continued into the mid-1980s.
Economic developments of the prewar period began with the “Rich State and Strong Army Policy” by the Meiji government. During the Meiji period (1868–1912), leaders inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Oyatoi gaikokujin). The government also built railroads, improved road, and inaugurated a land reform program to prepare the country for further development.
To promote industrialization, the government decided that, while it should help private business to allocate resources and to plan, the public sector was best equipped to stimulate economic growth. The greatest role of government was to help provide good economic conditions for business. In short, government was to be the guide and business the producer. In the early Meiji period, the government built factories and shipyards that were sold to entrepreneurs at a fraction of their value. Many of these businesses grew rapidly into the larger conglomerates. Government emerged as chief promoter of private enterprise, enacting a series of probusiness policies.
In the mid-1930s, the Japanese nominal wage rates were 10 times less than the one of the U.S (based on mid-1930s exchange rates), while the price level is estimated to have been about 44% the one of the U.S.
Postwar period (1945–present).
From the 1960s to the 1980s, overall real economic growth was extremely large: a 10% average in the 1960s, a 5% average in the 1970s and a 4% average in the 1980s. By the end of said period, Japan had moved into being a high-wage economy.
Growth slowed markedly in the late 1990s also termed the Lost Decade after the collapse of the Japanese asset price bubble. As a consequence Japan ran massive budget deficits (added trillions in Yen to Japanese financial system) to finance large public works programs.
By 1998, Japan's public works projects still could not stimulate demand enough to end the economy's stagnation. In desperation, the Japanese government undertook "structural reform" policies intended to wring speculative excesses from the stock and real estate markets. Unfortunately, these policies led Japan into deflation on numerous occasions between 1999 and 2004. In his 1998 paper, Japan's Trap, Princeton economics professor Paul Krugman argued that based on a number of models, Japan had a new option. Krugman's plan called for a rise in inflation expectations to, in effect, cut long-term interest rates and promote spending.
Japan used another technique, somewhat based on Krugman's, called Quantitative easing. As opposed to flooding the money supply with newly printed money, the Bank of Japan expanded the money supply internally to raise expectations of inflation. Initially, the policy failed to induce any growth, but it eventually began to affect inflationary expectations. By late 2005, the economy finally began what seems to be a sustained recovery. GDP growth for that year was 2.8%, with an annualized fourth quarter expansion of 5.5%, surpassing the growth rates of the US and European Union during the same period. Unlike previous recovery trends, domestic consumption has been the dominant factor of growth.
Despite having interest rates down near zero for a long period of time, the Quantitative easing strategy did not succeed in stopping price deflation. This led some economists, such as Paul Krugman, and some Japanese politicians, to advocate the generation of higher inflation expectations. In July 2006, the zero-rate policy was ended. In 2008, the Japanese Central Bank still has the lowest interest rates in the developed world, deflation has still not been eliminated and the Nikkei 225 has fallen over approximately 50% (between June 2007 and December 2008). However, on April 5, 2013, the Bank of Japan announced that it would be purchasing 60-70 trillion yen in bonds and securities in an attempt to eliminate deflation by doubling the money supply in Japan over the course of two years. Markets around the world have responded positively to the government's current proactive policies, with the Nikkei 225 adding more than 42% since November 2012. The Economist has suggested that improvements to bankruptcy law, land transfer law, and tax laws will aid Japan's economy. In recent years, Japan has been the top export market for almost 15 trading nations worldwide.
Infrastructure.
In 2005, one half of Japan's energy was produced from petroleum, a fifth from coal, and 14% from natural gas. Nuclear power in Japan made a quarter of electricity production but due to the Fukushima Daiichi nuclear disaster there has been a large desire to end Japan's nuclear power program. In September 2013, Japan closed its last 50 nuclear power plants nationwide, causing the nation to be nuclear free.
Japan's spendings on roads has been considered large. The 1.2 million kilometers of paved road are one of the major means of transportation. Japan has left-hand traffic. A single network of speed, divided, limited-access toll roads connects major cities and are operated by toll-collecting enterprises. New and used cars are inexpensive, and the Japanese government has encouraged people to buy hybrid vehicles. Car ownership fees and fuel levies are used to promote energy-efficiency.
Rail transport is a major means of transport in Japan. Dozens of Japanese railway companies compete in regional and local passenger transportation markets; for instance, 6 passenger JR enterprises, Kintetsu Corporation, Seibu Railway, and Keio Corporation. Often, strategies of these enterprises contain real estate or department stores next to stations, and many major stations have major department stores near them. The Japanese cities of Fukuoka, Kobe, Kyoto, Nagoya, Osaka, Sapporo, Sendai, Tokyo and Yokohama all have subway systems. Some 250 high-speed Shinkansen trains connect major cities. All trains are known for punctuality, and a delay of 90 seconds can be considered late for some train services.
There are 98 passenger and 175 total airports in Japan, and flying is a popular way to travel. The largest domestic airport, Tokyo International Airport, is Asia's second busiest airport. The largest international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The largest ports in Japan include Nagoya Port, the Port of Yokohama, the Port of Tokyo and the Port of Kobe.
About 84% of Japan's energy is imported from other countries. Japan is the world's largest liquefied natural gas importer, second largest coal importer, and third largest net oil importer. Given its heavy dependence on imported energy, Japan has aimed to diversify its sources. Since the oil shocks of the 1970s, Japan has reduced dependence on petroleum as a source of energy from 77.4% in 1973 to about 43.7% in 2010 and increased dependence on natural gas and nuclear power. Other important energy source includes coal, and hydroelectricity is Japan's biggest renewable energy source. Japan's solar market is also currently booming. Kerosene is also used extensively for home heating in portable heaters, especially farther north. Many taxi companies run their fleets on liquefied natural gas. A recent success towards greater fuel economy was the introduction of mass-produced Hybrid vehicles. Prime Minister Shinzo Abe, who was working on Japan's economic revival, signed a treaty with Saudi Arabia and UAE about the rising prices of oil, ensuring Japan's stable deliveries from that region.
Macro-economic trend.
This is a chart of trend of gross domestic product of Japan at market prices estimated by the International Monetary Fund with figures in millions of Japanese Yen. See also
For purchasing power parity comparisons, the US dollar was exchanged at ￥109 in 2010.
Industries.
Industries by GDP value-added 2012. Values are converted using the exchange rate on April 13, 2013.
Sectors of the economy.
Agriculture.
The Japanese agricultural sector accounts for about 1.4% of the total country's GDP. Only 12% of Japan's land is suitable for cultivation. Due to this lack of arable land, a system of terraces is used to farm in small areas. This results in one of the world's highest levels of crop yields per unit area, with an overall agricultural self-sufficiency rate of about 50% on fewer than 56,000 km² (14 million acres) cultivated.
Japan's small agricultural sector, however, is also highly subsidized and protected, with government regulations that favor small-scale cultivation instead of large-scale agriculture as practiced in North America. There has been a growing concern about farming as the current farmers are aging with a difficult time finding successors.
Rice accounts for almost all of Japan's cereal production. Japan is the second-largest agricultural product importer in the world. Rice, the most protected crop, is subject to tariffs of 777.7%.
Although Japan is usually self-sufficient in rice (except for its use in making rice crackers and processed foods) and wheat, the country must import about 50% of its requirements of other grain and fodder crops and relies on imports for half of its supply of meat. Japan imports large quantities of wheat and soybeans. Japan is the 5th largest market for EU agricultural exports. Over 90% of mandarin oranges in Japan are grown in Japan. Apples are also grown due to restrictions on apple imports.
Fishery.
Japan ranked fourth in the world in 1996 in tonnage of fish caught. Japan captured 4,074,580 metric tons of fish in 2005, down from 4,987,703 tons in 2000, 9,558,615 tons in 1990, 9,864,422 tons in 1980, 8,520,397 tons in 1970, 5,583,796 tons in 1960 and 2,881,855 tons in 1950. In 2003, the total aquaculture production was predicted at 1,301,437 tonnes. In 2010, Japan's total fisheries production was 4,762,469 fish. Offshore fisheries accounted for an average of 50% of the nation's total fish catches in the late 1980s although they experienced repeated ups and downs during that period.
Coastal fishing by small boats, set nets, or breeding techniques accounts for about one third of the industry's total production, while offshore fishing by medium-sized boats makes up for more than half the total production. Deep-sea fishing from larger vessels makes up the rest. Among the many species of seafood caught are sardines, skipjack tuna, crab, shrimp, salmon, pollock, squid, clams, mackerel, sea bream, sauries, tuna and Japanese amberjack. Freshwater fishing, including salmon, trout and eel hatcheries and fish farms, takes up about 30% of Japan's fishing industry. Among the nearly 300 fish species in the rivers of Japan are native varieties of catfish, chub, herring and goby, as well as such freshwater crustaceans as crabs and crayfish. Marine and freshwater aquaculture is conducted in all 47 prefectures in Japan.
Japan maintains one of the world's largest fishing fleets and accounts for nearly 15% of the global catch, prompting some claims that Japan's fishing is leading to depletion in fish stocks such as tuna. Japan has also sparked controversy by supporting quasi-commercial whaling.
Industry.
Japanese manufacturing and industry is very diversified, with a variety of advanced industries that are highly successful. Industry accounts for 24% of the nation's GDP.
Industry is concentrated in several regions, with the Kantō region surrounding Tokyo, (the Keihin industrial region) as well as the Kansai region surrounding Osaka (the Hanshin industrial region) and the Tōkai region surrounding Nagoya (the Chukyo-Tokai industrial region) the main industrial centers. Other industrial centers include the southwestern part of Honshū and northern Shikoku around the Seto Inland Sea (the Setouchi industrial region); and the northern part of Kyūshū (Kitakyūshū). In addition, a long narrow belt of industrial centers called the Taiheiyō Belt is found between Tokyo and Fukuoka, established by particular industries, that have developed as mill towns.
Japan enjoys high technological development in many fields, including consumer electronics, automobile manufacturing, semiconductor manufacturing, optical fibers, optoelectronics, optical media, facsimile and copy machines, and fermentation processes in food and biochemistry. However, many Japanese companies are facing emerging rivals from the United States of America, South Korea, and China.
Automobile manufacturing.
Japan is the third biggest producer of automobiles in the world. Toyota is currently the world largest car maker, and the Japanese car makers Nissan, Honda, Suzuki, and Mazda also count for some of the largest car makers in the world.
Mining and petroleum exploration.
Japan's mining production has been minimal, and Japan has very little mining deposits. However, massive deposits of rare earths have been found off the coast of Japan. In the 2011 fiscal year, the domestic yield of crude oil was 820 thousand kiloliters, which was 0.4% of Japan's total crude processing volume.
Services.
Japan's service sector accounts for about three-quarters of its total economic output. Banking, insurance, real estate, retailing, transportation, and telecommunications are all major industries such as Mitsubishi UFJ, Mizuho, NTT, TEPCO, Nomura, Mitsubishi Estate, ÆON, Mitsui Sumitomo, Softbank, JR East, Seven & I, KDDI and Japan Airlines counting as one of the largest companies in the world. Four of the five most circulated newspapers in the world are Japanese newspapers. The Koizumi government set Japan Post, one of the country's largest providers of savings and insurance services for privatization by 2015. The six major keiretsus are the Mitsubishi, Sumitomo, Fuyo, Mitsui, Dai-Ichi Kangyo and Sanwa Groups. Japan is home to 251 companies from the Forbes Global 2000 or 12.55% (as of 2013).
Tourism.
In 2012, Japan was the fifth most visited country in Asia and the Pacific, with over 8.3 million tourists. In 2013, due to the weaker yen and easier visa requirements for southwest Asian countries, Japan received a record 11.25 million visitors, which was higher than the government's projected goal of 10 million visitors. The government hopes to attract 20 million visitors a year by the 2020 Summer Olympics in Tokyo. Some of the most popular visited places include the Shinjuku, Ginza, Shibuya and Asakusa areas in Tokyo, and the cities of Osaka, Kobe and Kyoto, as well as Himeji Castle. Hokkaido is also a popular winter destination for visitors with several ski resorts and luxury hotels being built there.
Finance.
The Tokyo Stock Exchange is the fourth largest stock exchange in the world by market capitalization, as well as the 2nd largest stock market in Asia, with 2,292 listed companies. The Nikkei 225 and the TOPIX are the two important stock market indexes of the Tokyo Stock Exchange. The Tokyo Stock Exchange and the Osaka Stock Exchange, another major stock exchange in Japan, merged on January 1, 2013, creating one of the world's largest stock exchanges. Other stock exchanges in Japan include the Nagoya Stock Exchange, Fukuoka Stock Exchange and Sapporo Securities Exchange.
Labor force.
The unemployment rate in December 2013 was 3.7%, down 1.5 percentage points from the claimed unemployment rate of 5.2% in June 2009 due to the strong economic recovery. This is regarded as an underestimate. Even part-time workers with extremely low hours are classified as employed.
In July 2006, the unemployment rate in Japan was 4.1%, according to the OECD. At the end of February 2009, it stood at 4.4% This seemingly modest rate however understates the situation. According to The Economist, the ratio of job offers to number of applicants has declined to just 0.59, from almost 1 at the start of 2008, while average work hours also declined. Average wages also went down by 2.9% over the 12 months ending in February. In 2008, Japan's labor force consisted of some 66 million workers—40% of whom were women—and was rapidly shrinking.
One major long-term concern for the Japanese labor force is a low birthrate. In the first half of 2005, the number of deaths in Japan exceeded the number of births, indicating that the decline in population, initially predicted to start in 2007, had already started. While one countermeasure for a declining birthrate would be to remove barriers to immigration, despite taking new steps towards it, the Japanese government has been reluctant to do so, and foreign immigration to Japan has been unpopular among citizens.
In 1989, the predominantly public sector union confederation, SOHYO (General Council of Trade Unions of Japan), merged with RENGO (Japanese Private Sector Trade Union Confederation) to form the Japanese Trade Union Confederation. Labor union membership is about 12 million.
Law and government.
Japan ranks 27th of 185 countries in the Ease of Doing Business Index 2013.
Japan has one of the smallest tax rates in the developed world. After deductions, the majority of workers are free from personal income taxes. Consumption tax rate is only 8%, while corporate tax rates are high, second highest corporate tax rate in the world, at 36.8%. However, the House of Representatives has passed a bill which will increase the consumption tax to 10% in October 2015. The government has also decided to reduce corporate tax and to phase out automobile tax.
Shareholder activism is rare despite the fact that the corporate law gives shareholders strong powers over managers. Recently, more shareholders have stood up against managers.
The government's liabilities include the second largest public debt of any nation with debt of over one quadrillion yen. Former Prime Minister Naoto Kan has called the situation 'urgent'.
Japan's central bank has the second largest foreign-exchange reserves after the People's Republic of China, with over one trillion US Dollars in foreign reserves.
Culture.
Overview.
Nemawashi (根回し), or "consensus building", in Japanese culture is an informal process of quietly laying the foundation for some proposed change or project, by talking to the people concerned, gathering support and feedback, and so forth. It is considered an important element in any major change, before any formal steps are taken, and successful "nemawashi" enables changes to be carried out with the consent of all sides.
Japanese companies are known for management methods such as "The Toyota Way". Kaizen (改善, Japanese for "improvement") is a Japanese philosophy that focuses on continuous improvement throughout all aspects of life. When applied to the workplace, Kaizen activities continually improve all functions of a business, from manufacturing to management and from the CEO to the assembly line workers. By improving standardized activities and processes, Kaizen aims to eliminate waste (see Lean manufacturing). Kaizen was first implemented in several Japanese businesses during the country's recovery after World War II, including Toyota, and has since spread to businesses throughout the world. Within certain value systems, it is ironic that Japanese workers labor amongst the most hours per day, even though kaizen is supposed to improve all aspects of life.
Some companies have powerful enterprise unions and "shuntō". The Nenko System or Nenko Joretsu as it is called in Japan, is the Japanese system of promoting an employee in order of his or her proximity to retirement. The advantage of the system is that it allows older employees to achieve a higher salary level before retirement and that it usually brings more experience to the executive ranks. The disadvantage of the system is that it does not allow new talent to be merged with the experience and those with specialized skills cannot be promoted to the already crowded executive ranks. It also does not guarantee or even attempt to bring the "right person for the right job". Relationships between government bureaucrats and companies are often close. Amakudari (天下り, amakudari, "descent from heaven") is the institutionalised practice where Japanese senior bureaucrats retire to high-profile positions in the private and public sectors. The practice is increasingly viewed as corrupt and a drag on unfastening ties between private sector and state that prevent economic and political reforms. Lifetime employment ("shushin koyo") and seniority-based career advancement have been common in the Japanese work environment. Japan has begun to gradually move away from some of these norms.
Salaryman (サラリーマン, "Sararīman", salaried man) refers to someone whose income is salary based; particularly those working for corporations. Its frequent use by Japanese corporations, and its prevalence in Japanese manga and anime has gradually led to its acceptance in English-speaking countries as a noun for a Japanese white-collar businessman. The word can be found in many books and articles pertaining to Japanese culture. Immediately following World War II, becoming a salaryman was viewed as a gateway to a stable, middle-class lifestyle. In modern use, the term carries associations of long working hours, low prestige in the corporate hierarchy, absence of significant sources of income other than salary, wage slavery, and karōshi. The term salaryman refers almost exclusively to males.
An office lady, often abbreviated OL (Japanese: オーエル "Ōeru"), is a female office worker in Japan who performs generally pink collar tasks such as serving tea and secretarial or clerical work. Like many unmarried Japanese, OLs often live with their parents well into early adulthood. Office ladies are usually full-time permanent staff, although the jobs they do usually have little opportunity for promotion, and there is usually the tacit expectation that they leave their jobs once they get married.
Freeter (フリーター, furītā) (other spellings below) is a Japanese expression for people between the age of 15 and 34 who lack full-time employment or are unemployed, excluding homemakers and students. They may also be described as "underemployed" or freelance workers. These people do not start a career after high school or university but instead usually live as parasite singles with their parents and earn some money with low skilled and low paid jobs. The low income makes it difficult for freeters to start a family, and the lack of qualifications makes it difficult to start a career at a later point in life.
Karōshi (過労死, "karōshi"), which can be translated quite literally from Japanese as "death from overwork", is occupational sudden death. The major medical causes of karōshi deaths are heart attack and stroke due to stress.
Sōkaiya (総会屋, sōkaiya), (sometimes also translated as "corporate bouncers", "meeting-men", or "corporate blackmailers") are a form of specialized racketeer unique to Japan, and often associated with the yakuza that extort money from or blackmail companies by threatening to publicly humiliate companies and their management, usually in their annual meeting (総会, sōkai). Sarakin (サラ金) is a Japanese term for moneylender, or loan shark. It is a contraction of the Japanese words for salaryman and cash. Around 14 million people, or 10% of the Japanese population, have borrowed from a "sarakin". In total, there are about 10,000 firms (down from 30,000 a decade ago); however, the top seven firms make up 70% of the market. The value of outstanding loans totals $100 billion. The biggest "sarakin" are publicly traded and often allied with big banks.
The first "Western-style" department store in Japan was Mitsukoshi, founded in 1904, which has its root as a kimono store called Echigoya from 1673. When the roots are considered, however, Matsuzakaya has an even longer history, dated from 1611. The kimono store changed to a department store in 1910. In 1924, Matsuzakaya store in Ginza allowed street shoes to be worn indoors, something innovative at the time. These former kimono shop department stores dominated the market in its earlier history. They sold, or rather displayed, luxurious products, which contributed for their sophisticated atmospheres. Another origin of Japanese department store is that from railway company. There have been many private railway operators in the nation, and from the 1920s, they started to build department stores directly linked to their lines' termini. Seibu and Hankyu are the typical examples of this type. From the 1980s onwards, Japanese department stores face fierce competition from supermarkets and convenience stores, gradually losing their presences. Still, "depāto" are bastions of several aspects of cultural conservatism in the country. Gift certificates for prestigious department stores are frequently given as formal presents in Japan. Department stores in Japan generally offer a wide range of services and can include foreign exchange, travel reservations, ticket sales for local concerts and other events.
Keiretsu.
A keiretsu (系列, lit. "system" or "series") is a set of companies with interlocking business relationships and shareholdings. It is a type of business group. The prototypical "keiretsu" appeared in Japan during the "economic miracle" following World War II. Before Japan's surrender, Japanese industry was controlled by large family-controlled vertical monopolies called "zaibatsu". The Allies dismantled the "zaibatsu" in the late 1940s, but the companies formed from the dismantling of the "zaibatsu" were reintegrated. The dispersed corporations were re-interlinked through share purchases to form horizontally integrated alliances across many industries. Where possible, "keiretsu" companies would also supply one another, making the alliances vertically integrated as well. In this period, official government policy promoted the creation of robust trade corporations that could withstand pressures from intensified world trade competition.
The major "keiretsu" were each centered on one bank, which lent money to the "keiretsu's" member companies and held equity positions in the companies. Each central bank had great control over the companies in the "keiretsu" and acted as a monitoring entity and as an emergency bail-out entity. One effect of this structure was to minimize the presence of hostile takeovers in Japan, because no entities could challenge the power of the banks.
There are two types of "keiretsu": vertical and horizontal. Vertical "keiretsu" illustrates the organization and relationships within a company (for example all factors of production of a certain product are connected), while a horizontal "keiretsu" shows relationships between entities and industries, normally centered on a bank and trading company. Both are complexly woven together and self-sustain each other.
The Japanese recession in the 1990s had profound effects on the keiretsu. Many of the largest banks were hit hard by bad loan portfolios and forced to merge or go out of business. This had the effect of blurring the lines between the keiretsu: Sumitomo Bank and Mitsui Bank, for instance, became Sumitomo Mitsui Banking Corporation in 2001, while Sanwa Bank (the banker for the Hankyu-Toho Group) became part of Bank of Tokyo-Mitsubishi UFJ. Additionally, many companies from outside the keiretsu system, such as Sony, began outperforming their counterparts within the system.
Generally, these causes gave rise to a strong notion in the business community that the old keiretsu system was not an effective business model, and led to an overall loosening of keiretsu alliances. While the keiretsu still exist, they are not as centralized or integrated as they were before the 1990s. This, in turn, has led to a growing corporate acquisition industry in Japan, as companies are no longer able to be easily "bailed out" by their banks, as well as rising derivative litigation by more independent shareholders.
Other economic indicators.
Net international investment position: 266,223 \ billion (1st)
Industrial Production Growth Rate: 7.5% (2010 est.)
Investment (gross fixed): 20.3% of GDP (2010 est.)
Household income or consumption by percentage share:
Agriculture – Products: rice, sugar beets, vegetables, fruit, pork, poultry, dairy products, eggs, fish
Exports – Commodities: machinery and equipment, motor vehicles, semiconductors, chemicals
Imports – Commodities: machinery and equipment, fuels, foodstuffs, chemicals, textiles, raw materials (2001)
Exchange rates:<br>
"Japanese Yen per US$1" – 88.67 (2010), 93.57 (2009), 103.58 (2008), 117.99 (2007), 116.18 (2006), 109.69 (2005), 115.93 (2003), 125.39 (2002), 121.53 (2001), 105.16 (January 2000), 113.91 (1999), 130.91 (1998), 120.99 (1997), 108.78 (1996), 94.06 (1995)
Electricity:
Electricity – Production by source:
Electricity – Standards:
Oil:

</doc>
<doc id="15579" url="http://en.wikipedia.org/wiki?curid=15579" title="Communications in Japan">
Communications in Japan

Japan currently possesses one of the most advanced communication networks in the world.
Telephone.
Telephones and ISDN - main lines in use: 52.3981 million (2007)
IP phone lines in use: 16.766 million (2007)
Mobile and PHS lines in use: 105.297 million (2007)
Mobile phone.
There are five nationwide mobile phone service providers: NTT DoCoMo, KDDI, SoftBank Mobile, EMOBILE, and Willcom.
Radio and television.
Radio broadcast stations: AM 190, FM 88, shortwave 24 (1999) 
Radios: 120.5 million (1997) 
Television broadcast stations: 7,108 (plus 441 repeaters; note - in addition, US Forces are served by 3 TV stations and 2 TV cable services) (1999) 
Televisions: 86.5 million (1997)
Amateur radio: 446,602 licensed stations as of October 2011. See Amateur radio call signs of Japan.
Internet Service.
Number of Broadband Users by Access (April 2005)
Number of Broadband Users by Access (June 2004)
Number of Broadband Users by Access (June 2002)
Country code (Top-level domain): JP
History.
The first milestones in the Japanese media history were newspapers in the Meiji period, the first being the Nagasaki Shipping List & Advertiser, founded 1861 in Nagasaki, with the telegraph and telephone following suit. 
The broadcast industry has been dominated by the Japan Broadcasting Corporation (Nippon Hoso Kyokai—NHK) since its founding in 1925. 
In the postwar period, NHK's budget and operations were under the purview of the Ministry of Posts and Telecommunications, the Broadcasting Law of 1950 provides for independent management and programming by NHK. Television broadcasting began in 1953, and color television was introduced in 1960. Cable television was introduced in 1969. In 1978 an experimental broadcast satellite with two color television channels was launched. Operational satellites for television use were launched between 1984 and 1990. Television viewing spread so rapidly that, by 1987, 99 percent of Japan's households had color television sets and the average family had its set on at least five hours a day. Starting in 1987, NHK began full-scale experimental broadcasting on two channels using satellite-to-audience signals, thus bringing service to remote and mountainous parts of the country that earlier had experienced poor reception. The new system also provided twenty-four hours a day, nonstop service. 
In the late 1980s, NHK operated two public television and three radio networks nationally, producing about 1,700 programs per week. Its general and education programs were broadcast through more than 6,900 television stations and nearly 330 AM and more than 500 FM radio transmitting stations. Comprehensive service in twenty-one languages is available throughout the world.
Rapid improvements, innovations, and diversification in communications technology, including optical fiber cables, communications satellites, and fax machines, led to rapid growth of the communications industry in the 1980s. Nippon Telegraph and Telephone Corporation, owned by the government until 1985, had dominated the communications industry until April 1985, when new common carriers, including Daini Denden, were permitted to enter the field. NTT Worldwide Telecommunications Corp (Kokusai Denshin Denwa Company, commonly known as KDD, now part of KDDI Inc.) lost its monopoly hold on international communications activities in 1989, when Nihon Kokusai Tsushin and other private overseas communications firms began operations yesturday. 
In 1992 Japan also had more than 12,000 televisions stations, and the country had more than 350 radio stations, 300 AM radio stations and 58 FM. Broadcasting innovations in the 1980s included sound multiplex (two-language or stereo) broadcasting, satellite broadcasting, and in 1985 the University of the Air and teletext services were inaugurated. 
Japan has been the world leader in telecommunications in the 1980s, but this position that has been challenged by the United States' dot-com industry in the 1990s and the emerging tiger states in Asia. While the United States is leading in digital content, South Korea is leading in broadband access, India is leading in software, and Taiwan is leading in research and development.

</doc>
<doc id="15580" url="http://en.wikipedia.org/wiki?curid=15580" title="Transport in Japan">
Transport in Japan

Transportation in Japan is modern and highly developed. Japan's transportation sector stands out for its energy efficiency: it uses less energy per person compared to other countries, thanks to a high share of rail transportation and low overall travel distances. Transportation in Japan is also very expensive in international comparison, reflecting high tolls and taxes, particularly on automobile transport.
Japan's spending on roads has been large. The 1.2 million kilometers of paved road are the main means of transportation. Japan has left-hand traffic. A single network of high-speed, divided, limited-access toll roads connects major cities, which are operated by toll-collecting enterprises.
Dozens of Japanese railway companies compete in regional and local passenger transportation markets; for instance, seven JR Group companies, Kintetsu Corporation, Seibu Railway, and Keio Corporation. Often, strategies of these enterprises contain real estate or department stores next to stations. Some 250 high-speed Shinkansen trains connect major cities. All trains are known for punctuality.
There are 176 airports, and the largest domestic airport, Haneda Airport, is Asia's busiest airport. The largest international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The largest ports include Nagoya Port.
Rail.
In Japan, railways are a major means of passenger transportation, especially for mass and high-speed transport between major cities and for commuter transport in metropolitan areas. Seven Japan Railways Group companies, state-owned until 1987, cover most parts of Japan. There also are railway services operated by private rail companies, regional governments, and companies funded by both regional governments and private companies.
Total railways of 27,182 km include several track gauges, the most common of which is narrow gauge, with 22,301 km of track of which 15,222 km is electrified.
Fukuoka, Kobe, Kyoto, Nagoya, Osaka, Sapporo, Sendai, Tokyo, and Yokohama have subway systems.
Most Japanese people traveled on foot until the later part of the 19th century. The first railway was built between Tokyo and Yokohama in 1872 and many more developed. Japan now has one of the world's most developed transportation networks. Mass transportation is well developed in Japan, but the road system lags behind and is inadequate for the number of cars. Road construction is difficult because of the high areas of population and the limited amount of usable land. Shinkansen are the high speed trains in Japan and they are known as bullet trains. About 250 Shinkansen trains operate daily. The fastest shinkansen trains are the JR East E5 and E6 series trains, which operate at a maximum speed of 320 km/h. Shinkansen trains are known to be very punctual. A train is recorded as late if it does not arrive at the specified time. In 2003, the average delay per train on the Tokaido Shinkansen was 6 seconds.
Road.
According to , Japan has approximately 1,203,600 km of roads made up of 1,012,000 km of city, town and village roads, 129,000 km of prefectural roads, 55,000 km of general national highways and 7,600 km of national expressways. The cites a total length of expressways at 7,641 km (fiscal 2008). A single network of high-speed, divided, limited-access toll roads connects major cities on Honshu, Shikoku and Kyushu. Hokkaido has a separate network, and Okinawa Island has a highway of this type. In the year 2005, the toll collecting companies, formerly Japan Highway Public Corporation, have been transformed into private companies in public ownership, and there are plans to sell parts of them. The aim of this policy is to encourage competition and decrease tolls.
Road passenger and freight transport expanded considerably during the 1980s as private ownership of motor vehicles greatly increased along with the quality and extent of the nation's roads. Bus companies including the JR Bus companies operate long-distance bus service on the nation's expanding expressway network. In addition to relatively low fares and deluxe seating, the buses are well utilized because they continue service during the night, when air and train service is limited.
The cargo sector grew rapidly in the 1980s, recording 274.2 billion tonne-kilometres in 1990. The freight handled by motor vehicles, mainly trucks, in 1990, was over 6 billion tonnes, accounting for 90 percent of domestic freight tonnage and about 50 percent of tonne-kilometres.
Recent large infrastructure projects were the construction of the Great Seto Bridge and the Tokyo Bay Aqua-Line (opened 1997).
Although road fatalities have been decreasing due in part to stricter enforcement of drink driving laws, 2004 still saw 7,358 deaths on Japanese roads.
Air.
In 2013 Japan had the fourth largest passenger air market in the world with 105,913,000 passengers. In 2012 Japan has 98 airports. The main international gateways are Narita International Airport (Tokyo area), Kansai International Airport (Osaka/Kobe/Kyoto area), and Chūbu Centrair International Airport (Nagoya area). The main domestic hub is Tokyo International Airport (Haneda Airport), Asia's busiest airport and the world's 4th busiest airport; other major traffic hubs include Osaka International Airport, New Chitose Airport outside Sapporo, and Fukuoka Airport. 14 heliports are estimated to exist (1999).
The two main airlines are Japan Airlines and All Nippon Airways. Other passenger carriers include Skymark Airlines, Skynet Asia Airways, Air Do, Star Flyer and Fuji Dream Airlines. United Airlines and Delta Air Lines, formerly Northwest Airlines, are major international operators from Narita Airport.
Domestic air travel in Japan has historically been highly regulated. From 1972, the three major domestic airlines (JAL, ANA, and JAS) were allocated certain routes, with JAL and ANA sharing trunk routes, and ANA and JAS sharing local feeder routes. JAL and JAS have since been merged to help compete with ANA. JAL also had a flag-carrier monopoly on international routes until 1986. Airfares were set by the government until 2000, although carriers had freedom to adjust the standard fares starting in 1995 (when discounts of up to 50% were permitted). Today, fares can be set by carriers, but the government retains the ability to veto fares that are impermissibly high.
Waterways.
There are 1770 km of waterways in Japan; seagoing craft ply all coastal inland seas.
There are some 994 ports in Japan as of April 2014. There are overlapping classifications of these ports, some of which are intermodal e.g. cargo, passenger, naval, and fishery. The 5 designated "super" container ports are: Yokkaichi, Yokohama, Nagoya, Kobe and Osaka. 23 are designated major/international, 125 designated as important, while there are also purely fisherman ports.
The twenty-three major seaports designated as special important ports by Ministry of Land, Infrastructure, Transport and Tourism : Chiba, Fushiki/Toyama, Himeji, Hiroshima, Kawasaki, Kitakyūshū, Kobe, Kudamatsu, Muroran, Nagoya, Niigata, Osaka, Sakai/Senpoku, Sendai/Shiogama, Shimizu, Shimonoseki, Tokyo, Tomakomai, Wakayama, Yokkaichi, and Yokohama.
Japan has 662 ships with a volume of  gross register tons (GRT) or over, totaling  gross register tons (GRT) or  tonnes deadweight (DWT). There are 146 bulk ships, 49 cargo ships, 13 chemical tankers, 16 combination bulk, 4 with combination of ore and oil, 25 container, 45 liquefied gas, 9 passenger, 2 passenger and cargo combination ships, 214 petroleum tankers, 22 refrigerated cargo, 48 roll-on/roll-off ships, 9 short-sea passenger, and 60 vehicle carriers (1999 est.).
Ferries connect Hokkaido to Honshu, and Okinawa Island to Kyushu and Honshu. They also connect other smaller islands and the main islands. The scheduled international passenger routes are to China, Russia, South Korea and Taiwan. Coastal and cross-channel ferries on the main islands decreased in routes and frequencies following the development of bridges and expressways but some are still operating (as of 2007).
Pipelines.
Japan has 84 km of pipelines for crude oil, 322 km for petroleum products, and 1,800 km for natural gas.

</doc>
<doc id="15582" url="http://en.wikipedia.org/wiki?curid=15582" title="Foreign relations of Japan">
Foreign relations of Japan

Foreign relations of Japan is handled by the Ministry of Foreign Affairs of Japan.
Since the surrender after World War II and the Treaty of San Francisco, Japanese diplomatic policy has been based on close partnership with the United States and the emphasis on the international cooperation such as the United Nations. In the Cold War, Japan took a part in the Western world's confrontation of the Soviet Union in East Asia. In the rapid economic developments in the 1960s and 1970s, Japan recovered its influences and became regarded as one of the major powers in the world. However, Japanese influences are regarded as negative by two particular countries: China and South Korea.
During the Cold War, Japanese foreign policy was not self-assertive, relatively focused on their economic growth. However, the end of the Cold War and bitter lessons from the Gulf War changed the policy slowly. Japanese government decided to participate in the Peacekeeping operations by the UN, and sent their troops to Cambodia, Mozambique, Golan Heights and the East Timor in the 1990s and 2000s. After the September 11 attacks, Japanese naval vessels have been assigned to resupply duties in the Indian Ocean to the present date. The Ground Self-Defense Force also dispatched their troops to Southern Iraq for the restoration of basic infrastructures.
Beyond its immediate neighbors, Japan has pursued a more active foreign policy in recent years, recognizing the responsibility which accompanies its economic strength. Prime Minister Yasuo Fukuda stressed a changing direction in a policy speech to the National Diet: "Japan aspires to become a hub of human resource development as well as for research and intellectual contribution to further promote cooperation in the field of peace-building." This follows the modest success of a Japanese-conceived peace plan which became the foundation for nationwide elections in Cambodia in 1998.
East Asia.
Korea.
Japan strongly supports the U.S. in its efforts to encourage Pyongyang to abide by the nuclear Non-Proliferation Treaty and its agreements with the International Atomic Energy Agency (IAEA). Despite the 31 August 1998 North Korean missile test which overflew the Home Islands, Japan has maintained its support for the Korean Energy Development Organization (KEDO) and the Agreed Framework, which seeks to freeze the North Korean nuclear program. The U.S., Japan, and South Korea closely coordinate and consult trilaterally on policy toward North Korea, at least on a government level. Japan has limited economic and commercial ties with North Korea. Japanese normalization talks halted when North Korea refused to discuss a number of issues with Japan.
Japan and South Korea have had many disputes. Former South Korean President Roh Moo-hyun rejected a conference with the Japanese Prime Minister Junichiro Koizumi following his visits to the controversial Yasukuni Shrine. Other long-running issues between the two countries include territorial disputes over the Takeshima and disagreement about whether or not the matter of World War II-era prostitute has been resolved.
People's Republic of China.
During the Meiji Era, China was one of the first countries to feel Japanese Imperialism. After the establishment of the People's Republic of China (PRC) in 1949, relations with Japan changed from hostility and an absence of contact to cordiality and extremely close cooperation in many fields. During the 1960s the two countries resumed trade for the first time since World War II under the Liao–Takasaki Agreement. On 29 September 1972, Japan and China signed a treaty establishing diplomatic relations between the states. The 1990s led to an enormous growth in China’s economic welfare. Trade between Japan and China was one of the many reasons China was able to grow in the double-digit rates during the 1980s and 1990s. Japan was in the forefront among leading industrialized nations in restoring closer economic and political relations with China. Resumption of Japan's multi-billion dollar investments to China and increased visits to China by Japanese officials, culminating in the October 1992 visit of Emperor Akihito, gave a clear indication that Japan considered closer ties with China in its economic and strategic interest. Despite a 1995 apology regarding World War II by Japanese Prime Minister Tomiichi Murayama, tensions still remain, mostly because many Chinese feel there is a lack of true remorse for wartime crimes committed by Imperial Japanese forces. This has been reinforced by numerous visits to the Yasukuni Shrine by Japanese Prime Ministers, attempts to revise textbooks by Japanese nationalists, the continued dispute over Japan's atrocities in the Nanking Massacre, and the resurgence of nationalism and militarism in Japan.
Republic of China (Taiwan).
Taiwan was ceded to Japan in 1895 and was a major Japanese prefecture in World War II. Following the unconditional surrender of Japan to Allied Powers after World War II, Taiwan was relinquished by Japan as a stolen territory from China (like Manchukuo) by the San Francisco Peace Treaty in 1951. Current relations are guided by the 1972 Japan–PRC Joint Communique. Since the joint Communique, Japan has maintained non-governmental, working-level relations with Taiwan. Japan refers to the Republic of China on Taiwan with the neutral name "Taiwan."
Southeast Asia.
By 1990 Japan's interaction with the vast majority of Asia-Pacific countries, especially its burgeoning economic exchanges, was multifaceted and increasingly important to the recipient countries. The developing countries of the Association of Southeast Asian Nations (ASEAN) regarded Japan as critical to their development. Japan's aid to the ASEAN countries totaled US $1.9 billion in Japanese fiscal year (FY) 1988 versus about US $333 million for the United States during U.S. FY 1988. Japan was the number one foreign investor in the ASEAN countries, with cumulative investment as of March 1989 of about US $14.5 billion, more than twice that of the United States. Japan's share of total foreign investment in ASEAN countries in the same period ranged from 70 to 80 percent in Thailand to 20 percent in Indonesia.
In the late 1980s, the Japanese government was making a concerted effort to enhance its diplomatic stature, especially in Asia. Toshiki Kaifu's much publicized spring 1991 tour of five Southeast Asian nations—Malaysia, Brunei, Thailand, Singapore, and the Philippines—culminated in a 3 May major foreign policy address in Singapore, in which he called for a new partnership with the ASEAN and pledged that Japan would go beyond the purely economic sphere to seek an "appropriate role in the political sphere as a nation of peace." As evidence of this new role, Japan took an active part in promoting negotiations to resolve the Cambodian conflict.
In 1997, the ASEAN member nations and the People's Republic of China, South Korea and Japan agreed to hold yearly talks to further strengthen regional cooperation, the ASEAN Plus Three meetings. In 2005 the ASEAN plus Three countries together with India, Australia and New Zealand held the inaugural East Asia Summit (EAS).
Brunei.
Brunei has an embassy in Tokyo, and Japan has an embassy in Bandar Seri Begawan. Relations has been established since 2 April 1984.
Cambodia.
Japan has an embassy in Phnom Penh. Trade is sizable between the two countries:
Japanese investment in Cambodia includes Phnom Penh Commercial Bank, a joint venture of Hyundai Switzerland and Japanese SBI Group, opened in 2008. Japan remains Cambodia’s top donor country providing some US$1.2 billion in total overseas development assistance (ODA) during the period since 1992.
In 2006, Japanese and Cambodian governments signed an agreement outlining a new Japanese aid program worth US$59 million.
The Japanese Government has provided significant assistance for demining and education.
Malaysia.
Japan has an embassy in Kuala Lumpur, which was established in 1957. Malaysia has an embassy in Tokyo. The Japanese and Malaysian Government had visited each other on multiple occasions. Notable visits include the King of Malaysia visiting Japan in 2005 while in 2006, the Emperor and Empress of Japan visited Malaysia.
Philippines.
The Philippines gained independence from the United States in 1945. Diplomatic relations were re-established in 1956, when a war reparations agreement was concluded. By the end of the 1950s, Japanese companies and individual investors had begun to return to the Philippines.
Thailand.
Japan–Thailand relations span a period from the 17th century to the present. Contacts had an early start with Japanese trade on Red seal ships and the installation of Japanese communities on Siamese soil, only to be broken off with Japan's period of seclusion. Contacts resumed in the 19th century and developed to the point where Japan is today one of Thailand's foremost economic partners. Thailand and Japan share the distinction of never having lost sovereignty during the Colonial period.
Vietnam.
Vietnamese–Japanese relations stretch back to the at least the 16th century, when the two countries engaged in friendly trade. Modern relations between the two countries are based on Vietnam's developing economy and Japan's role as an investor and foreign aid donor.
South Asia.
In South Asia, Japan's role is mainly that of an aid donor. Japan's aid to seven South Asian countries totaled US$1.1 billion in 1988 and 1989, dropping to just under US$900 million in 1990. Except for Pakistan, which received heavy inputs of aid from the United States, all other South Asian countries receive most of their aid from Japan. Four South Asian nations—India, Pakistan, Bangladesh, and Sri Lanka—are in the top ten list of Tokyo's aid recipients worldwide. A point to note is that Indian Government has a no receive aid policy since the tsunami that struck India but Indian registerred NGOs look to Japan for much investment in their projects
Prime Minister Toshiki Kaifu signaled a broadening of Japan's interest in South Asia with his swing through the region in April 1990. In an address to the Indian parliament, Kaifu stressed the role of free markets and democracy in bringing about "a new international order," and he emphasized the need for a settlement of the Kashmir territorial dispute between India and Pakistan and for economic liberalization to attract foreign investment and promote dynamic growth. To India, which was very short of hard currency, Kaifu pledged a new concessional loan of ¥100 billion (about US$650 million) for the coming year.
Sri Lanka and Japan are two close friends since the early stages of post World War (II) since Sri Lanka extended a great support for Japanese development plans at the UN secretarial discussions.
Afghanistan.
Afghan–Japanese relations have existed as far back as World War II, and have been mainly positive. The Japanese government in 1974 started feasibility study under grant aid to develop and built Television in Afghanistan.
Bangladesh.
Bangladeshi–Japanese relations were established in February 1972. Japan is Bangladesh's 11th-largest export market; imports from Bangladesh make up 26% of all Japanese imports from the least developed countries, second only to those from Cambodia. Common imports from Bangladesh to Japan include leather goods, ready-made garments, and shrimp. By 2004, Japan had become Bangladesh's fourth-largest source of foreign direct investment, behind the United States, United Kingdom, and Malaysia. Japan's political goals in its relationship with Bangladesh include gaining support for their bid to join the United Nations Security Council, and securing markets for their finished goods. Japan is a significant source of development aid to Bangladesh.
India.
Throughout history, bilateral foreign relations between Japan and India have generally been friendly and strong. In December 2006, Prime Minister Singh's visit to Japan culminated in the signing of the "Joint Statement Towards Japan–India Strategic and Global Partnership."
According to Prime Minister Shinzo Abe's "arc of freedom" theory, it is in Japan's interests to develop closer ties with India, world's most populous democracy, while its relations with China remain chilly. To this end, Japan has funded many infrastructure projects in India, most notably in New Delhi's metro subway system and Maruti.
Indian applicants have been welcomed in 2006/7 to the JET Programme, starting with just one slot available in 2006 and 41 in 2007.
India and Japan signed a security cooperation agreement in which both will hold military exercises, police the Indian Ocean and conduct military-to-military exchanges on fighting terrorism, making India one of only three countries, the other two being the United States and Australia, with which Japan has such a security pact. There are 25,000 Indians in Japan as of 2008.
North America.
Barbados.
Japan is accredited to Barbados from its Embassy in Port of Spain (Trinidad and Tobago) and an honorary consulate in Bridgetown. Barbados is represented in Japan through a non-resident ambassador in Bridgetown.
Canada.
Diplomatic relations between both countries officially began in 1950 with the opening of the Japanese consulate in Ottawa. In 1929, Canada opened its Tokyo legation, the first in Asia; and in that same year, Japan its Ottawa consulate to legation form.
Some Canadian–Japanese contacts predate the mutual establishment of permanent legations. The first known Japanese immigrant to Canada, Manzo Nagano, landed in New Westminster, British Columbia in 1877. Japan's consulate in Vancouver was established in 1889, 40 years before its embassy was opened in Ottawa in 1929.
Canadians G. G. Cochran helped in founding Doshisha University in Kyoto, and Davidson McDonald helped in establishing Aoyama Gakuin University in Tokyo.
In the Great Kanto Earthquake of 1923, a Canadian steamship, the RMS "Empress of Australia" and her captain, Samuel Robinson achieved international acclaim for stalwart rescue efforts during the immediate aftermath of that disaster.
Canadian military attaché Herbert Cyril Thacker served in the field with Japanese forces in the Russo–Japanese War (1904–05), for which the Japanese government awarded him the Order of the Sacred Treasure, Third Class and the Japanese War medal for service during that campaign.
Canada and Japan have had diplomatic relations since 1928. Both countries are characterized by their active role in the Asia-Pacific community, as well as a relationship consisting of important economic, political, and socio-cultural ties. As major international donors, both Canada and Japan are strongly committed to promoting human rights, sustainable development and peace initiatives.
Canada–Japan relations are underpinned by their partnership in multilateral institutions: the G-7/8; the United Nations; the Organisation for Economic Co-operation and Development, the Quad (Canada, the European Union, Japan and the United States), and by their common interest in the Pacific community, including participation in the Asia-Pacific Economic Cooperation forum (APEC) and the ASEAN Regional Forum (ARF).
Emperor Akihito and Empress Michiko visited Canada in 2009.
Mexico.
The Treaty of Amity, Commerce, and Navigation concluded in 1888 between Japan and Mexico was the nation's first "equal" treaty with any country; which overshadows Tokugawa Ieyasu's pre-Edo period initiatives which sought to establish official relations with the New Spain in Mexico.
In 1897, the 35 members of the so-called "Enomoto" Colonization Party settle in the Mexican state of Chiapas. This was the first organized emigration from Japan to Latin America.
President Álvaro Obregón was awarded Japan's Order of the Chrysanthemum at a special ceremony in Mexico City. On 27 November 1924, Baron Shigetsuma Furuya, Special Ambassador from Japan to Mexico, conferred the honor on Obregón. It was reported that this had been the first time that the Order had been conferred outside the Imperial family.
In 1952, Mexico becomes the second country to ratify the San Francisco Peace Treaty, preceded only by the United Kingdom.
Mexico and Japan on 17 September 2004, signed the "Agreement Between Japan and The United Mexican States For The Strengthening of The Economic Partnership." This was the among many historic steps led by Prime Minister Junichiro Koizumi to strengthen global economic stability.
United States.
The United States is Japan's closest ally, and Japan relies on the U.S. for its national security to a high degree. As two of the world's top three economic powers, both countries also rely on close economic ties for their wealth, despite ongoing and occasionally acrimonious trade frictions.
Although its constitution and government policy preclude an offensive military role for Japan in international affairs, Japanese cooperation with the United States through the 1960 U.S.–Japan Security Treaty has been important to the peace and stability of East Asia. Currently, there are domestic discussions about possible reinterpretation of Article 9 of the Japanese Constitution. All postwar Japanese governments have relied on a close relationship with the United States as the foundation of their foreign policy and have depended on the mutual security treaty for strategic protection.
The relationship probably hit a post-war nadir around the early 1990s, when Japan's "economic rise" was seen as a threat to American power. Japan was the primary financier of the Gulf War, yet received major criticism in some US circles for its refusal to commit actual military support. Following the collapse of the so-called Bubble economy and the 1990s boom in the US, the Japanese economy was perceived as less of a threat to US interests. Some observers still feel that Japan's willingness to deploy troops in support of current US operations in Iraq, as spearheaded by Koizumi and the conservative LDP, reflects a vow not to be excluded from the group of countries the US considers friends. This decision may reflect a realpolitik understanding of the threat Japan faces from a rapidly modernizing China, which from its continued and indeed growing pattern of anti-Japanese demonstrations reveals the belief that old historical scores remain unsettled.
Oceania.
Australia.
Australia–Japan relations have elements of tension as well as acknowledged mutuality of strong interests, beliefs and friendship. Memories of World War II linger among the older members of the Australian public, as does a contemporary fear of Japanese economic domination over countries, particularly Australia, although such fears have fallen off in response to Japan's economic stagnation in the 1990s. At the same time, government and business leaders see Japan as a vital export market and an essential element in Australia's strong future growth and prosperity in the Asia-Pacific region.
Australia is also a major source of food and raw materials for Japan. In 1990 Australia accounted for 5.3 percent of total Japanese imports, a share that held relatively steady in the late 1980s. Due to its ability to export raw materials, Australia had a trade surplus with Japan. Australia was the largest single supplier of coal, iron ore, wool, and sugar to Japan in 1990. Australia is also a supplier of uranium. Japanese investment by 1988 made Australia the single largest source of Japanese regional imports. Resource development projects in Australia attracted Japanese capital, as did trade protectionism by necessitating local production for the Australian market. Investments in Australia totaled US$8.1 billion in 1988, accounting for 4.4 percent of Japanese direct investment abroad. Australia and Japanese relations have been growing for some time and will most likely continue to do so in the future.
There is some tension regarding the issue of whaling.
New Zealand.
Japan–New Zealand relations have had generally cordial relations since the post-World War II period, with Japan being a major trading partner with New Zealand. These relations have held together despite policy disputes over whaling and the International Whaling Commission.
New Zealand sent an urban search and rescue team which had spent the previous three weeks searching buildings following the February 2011 Christchurch earthquake, and 15 tonnes of rescue equipment with the 2011 Tōhoku earthquake and tsunami. The government donated $2m to the Japanese Red Cross to support relief efforts.
Tonga.
Japan and the Kingdom of Tonga have maintained official diplomatic relations since July 1970. Japan is Tonga's leading donor in the field of technical aid. The Japanese government describes its relations with Tonga as "excellent", and states that "the Imperial family of Japan and the Royal family of Tonga have developed a cordial and personal relationship over the years".
Europe.
In what became known as the Tenshō embassy, the first ambassadors from Japan to European powers reached Lisbon, Portugal in August 1584. From Lisbon, the ambassadors left for the Vatican in Rome, which was the main goal of their journey. The embassy returned to Japan in 1590, after which time the four nobleman ambassadors were ordained by Alessandro Valignano as the first Japanese Jesuit fathers.
A second embassy, headed by Hasekura Tsunenaga and sponsored by Date Masamune, was also a diplomatic mission to the Vatican. The embassy left 28 October 1613 from Ishinomaki, Miyagi Prefecture, in the northern Tōhoku region of Japan, where Date was daimyo. It traveled to Europe by way of New Spain, arriving in Acapulco on 25 January 1614, Mexico City in March, Havana in July, and finally Seville on 23 October 1614. After a short stop-over in France, the embassy reached Rome in November 1615, where it was received by Pope Paul V. After return travel by way of New Spain and the Philippines, the embassy reached the harbor of Nagasaki in August 1620. While the embassy was gone, Japan had undergone significant change, starting with the 1614 Osaka Rebellion, leading to a 1616 decree from the Tokugawa shogunate that all interaction with non-Chinese foreigners was confined to Hirado and Nagasaki. This was the beginning of "sakoku", where Japan was essentially closed to the western world until 1854.
Although cultural and non-economic ties with Western Europe grew significantly during the 1980s, the economic nexus remained by far the most important element of Japanese – West European relations throughout the decade. Events in West European relations, as well as political, economic, or even military matters, were topics of concern to most Japanese commentators because of the immediate implications for Japan. The major issues centred on the effect of the coming West European economic unification on Japan's trade, investment, and other opportunities in Western Europe. Some West European leaders were anxious to restrict Japanese access to the newly integrated European Union (until November 1993, the European Community), but others appeared open to Japanese trade and investment. In partial response to the strengthening economic ties among nations in Western Europe and to the United States-Canada-Mexico North American Free Trade Agreement, Japan and other countries along the Asia-Pacific rim began moving in the late 1980s toward greater economic cooperation.
On 18 July 1991, after several months of difficult negotiations, Prime Minister Toshiki Kaifu signed a joint statement with the Dutch prime minister and head of the European Community Council, Ruud Lubbers, and with the European Commission president, Jacques Delors, pledging closer Japanese – European Community consultations on foreign relations, scientific and technological cooperation, assistance to developing countries, and efforts to reduce trade conflicts. Japanese Ministry of Foreign Affairs officials hoped that this agreement would help to broaden Japanese – European Community political links and raise them above the narrow confines of trade disputes.
Africa.
Japan is increasingly active in Africa. In May 2008, the first Hideyo Noguchi Africa Prize will be awarded at Fourth Tokyo International Conference on African Development (TICAD IV), which signals a changing emphasis in bilateral relations.
Angola.
Angola–Japan relations were established in September 1976, shortly after Angola received formal sovereignty. As of 2007, economic relations played "a fundamental role in the bilateral relations between the two governments". Susumu Shibata is the ambassador of Japan to Angola.
Ghana.
Japan and Ghana maintain a special relationship and Ghana has an embassy in Tokyo, and Japan has an embassy in Accra.
Egypt.
Japan considers Egypt to be a key player in the Middle East and, as such, sees Egypt as a vital part of its diplomacy in the region. The two heads of government have been known to support each other on issues pertaining to the peace process in the Middle East.
Additionally, the two countries claim to share a common vision for world peace. The two countries maintain a "Joint Committee" dedicated to exploring developments in areas of mutual interest to the two countries.
Nigeria.
Japan and Nigeria engage in strong economic and political cooperation. Both countries established diplomatic relations on 1 October 1960.
South America.
Japan has continued to extend significant support to development and technical assistance projects in Latin America.
Western Asia.
Japan has expanded ties with the Middle East, including controversial water supply activities in Iraq. Japan's contribution to peacekeeping troops in Sudan remains steady.
Debates and frictions.
Japan has formally issued apologies for its military occupations before and during World War II, but that has done little in helping to improve its relationships with neighboring countries, especially the People's Republic of China, North Korea and South Korea. These countries still insist that Japan has yet to formally express remorse for its wrongdoings in the 20th century, despite some formal statements of regret from Prime Ministers Hosokawa Morihiro and Murayama Tomiichi. Japan’s official stance is that all war-related reparation claims have been resolved (except with North Korea). Unofficial visits to the controversial Yasukuni Jinja by past Prime Ministers belonging to the Liberal Democratic Party and the exclusion or generalization of some elements of Japan’s military history in a number school textbooks have also clouded the issue.
In 2004 the People’s Republic of China, North Korea, and South Korea also criticized Japan for sending its Ground Self Defence Forces to Iraq, which was seen as signalling a return to militarism. The government of Japan insisted that its forces would only participate in reconstruction and humanitarian aid missions.
There is a strong anti-Japanese sentiment in the People’s Republic of China, North Korea and South Korea. Antagonism is not inevitable however. South Korea and Japan successfully dual-hosted the 2002 FIFA World Cup, bridging a physical and political gap between the two countries. The great popularity in Japan of Bae Yong-joon, a South Korean actor, has also been seen as a sign that the two countries have moved closer together.
Disputed territories.
Japan has several territorial disputes with its neighbors concerning the control of certain outlying islands.
Japan contests Russia's control of the Southern Kuril Islands (including Etorofu, Kunashiri, Shikotan, and the Habomai group) which were occupied by the Soviet Union in 1945. South Korea's assertions concerning Liancourt Rocks (Japanese: "Takeshima", Korean: "Dokdo") are acknowledged, but not accepted by Japan. Japan has strained relations with the People's Republic of China (PRC) and the Republic of China (Taiwan) over the Senkaku Islands; and with the People's Republic of China over the status of Okinotorishima.
These disputes are in part about irredentism; and they are also about the control of marine and natural resources, such as possible reserves of crude oil and natural gas.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="15587" url="http://en.wikipedia.org/wiki?curid=15587" title="Joshua Jackson">
Joshua Jackson

Joshua Browning Jackson Carter (born June 11, 1978) is a Canadian-American actor. He has appeared in primetime television and in over 32 film roles. His well known roles include Pacey Witter in "Dawson's Creek", Charlie Conway in "The Mighty Ducks" film series and Peter Bishop in "Fringe". Jackson won the Genie Award for Best Performance by an Actor in a Leading Role for his performance in the Canadian independent film "One Week".
Early life.
Jackson was born in Vancouver, British Columbia, to John Carter Jackson and his wife, Fiona. His mother is a casting director. Jackson's father is from Texas; and his mother is a native of Ballyfermot, Dublin, Ireland, having immigrated to North America in the late 1960s. He has a younger sister, Aisleagh (born 1983). He was raised Catholic.
Jackson grew up in California until the age of 8. He moved to Vancouver with his mother and younger sister. He attended Ideal Mini School and later switched to Kitsilano Secondary School. In an interview with "The New York Times", Jackson said he was kicked out of high school once because of "The Jon Stewart Show": "[The show] played, at least where I grew up, at 1:30 in the morning, so I would stay up at night to watch Jon Stewart, but then I'd be too tired—or too lazy—to go to school in the morning. So I'd just take the first couple of classes off, 'cause I wanted to be fresh when I got there." He claims that the first time was because of "attitude" problems and that he "wasn't in the school spirit".
Career.
Jackson started acting in a small role in the film "Crooked Hearts" in 1991. The next year, he played the role of Charlie in a musical version of "Willie Wonka and the Chocolate Factory". At this point, with the help of the play's casting director Laura Kennedy, he joined the William Morris Agency. Soon after, he landed the role of Charlie (#96) in "The Mighty Ducks" series, playing a young and aspiring hockey player.
Joshua Jackson went on to appear as Pacey Witter on "Dawson's Creek", which ran on the WB network from 1998–2003, and also starred James Van Der Beek, Michelle Williams and Katie Holmes. While the show was on hiatus, he appeared in several movies including "Cruel Intentions" (a New York yuppie adaptation of "Les Liaisons dangereuses" that also starred Sarah Michelle Gellar and Ryan Phillippe), "The Skulls", "The Safety of Objects", "The Laramie Project" and a short cameo in the remake of "Ocean's Eleven" where he appears as himself in a poker scene with Brad Pitt, George Clooney and Holly Marie Combs, among others. In 2000, he also guest-starred in Season 12 of "The Simpsons", voicing the character of Jesse Grass, a "hunky environmentalist" and love interest for Lisa Simpson in the episode "Lisa the Tree Hugger".
Shortly after "Dawson's Creek" ended in 2003, Jackson played the lead role in films alongside Dennis Hopper ("Americano"), Harvey Keitel ("Shadows in the Sun"), and Donald Sutherland ("Aurora Borealis"). In 2005, Jackson moved to the UK and made his stage debut on the London West End with Patrick Stewart in David Mamet's two-man play, "A Life in the Theatre". The play was a critical and popular success, and ran from February to April of that year. Jackson said that he would consider returning to the stage, to try his hand on Broadway. His next film role was in "Bobby", directed by Emilio Estevez, Jackson's co-star from "The Mighty Ducks". He played a lead role in "Shutter", a US remake of a Thai horror film of the same name. He starred and acted as executive producer in the Canadian independent film "One Week", which opened on March 6, 2009.
Jackson played Peter Bishop in the science-fiction series "Fringe", created by JJ Abrams, Roberto Orci and Alex Kurtzman. The series appears on the Fox TV network and was the second-highest rated new show of the 2008–09 season after CBS's "The Mentalist". BuddyTV ranked him #9 on its "TV's 100 Sexiest Men of 2010" list, #19 in 2011 and #14 in 2012.
Jackson was nominated for Genie Award for Best Performance by an Actor in a Leading Role for the film "One Week". He won the award on April 12, 2010.
He held and hosted Pacey-Con in 2010, directly across the street from the San Diego Comic-Con, sporting a bowling shirt and giving out fan fiction he wrote himself to those waiting in the Comic-Con entrance line. Footage of the event was recorded for a video, entitled 'Pacey-Con', which he was filming for Will Ferrell's Funny or Die celebrity humor website.
In 2013 Jackson appeared in the IFC film "Inescapable" with Marisa Tomei and Alexander Siddig. Jackson wrote the first story from the comic book trilogy "Beyond the Fringe", titled "Peter and the Machine".
Personal life.
Jackson was in a relationship with Dawson's Creek co-star Katie Holmes during the first two seasons of the shows run. Holmes claims Jackson as her first love.
Jackson has been in a relationship with German actress Diane Kruger since 2006; the couple shares residences in Paris, Los Angeles and Vancouver. He owns his childhood home in Topanga, California. He previously lived in Wilmington, North Carolina, where "Dawson's Creek" was filmed; and in New York, where "Fringe" filmed its first season. In 2009, he moved back to Vancouver for the shooting of the four following seasons before the show aired its last episode on January 18, 2013.
Jackson is a fan of the hockey team Vancouver Canucks. He was arrested on November 9, 2002 at a Carolina Hurricanes ice hockey game in Raleigh, North Carolina after a quarrel with a security guard. He was charged with assault, affray and being intoxicated and disruptive, having 0.14 blood alcohol content. Prosecutors agreed to dismiss the assault charge, and Jackson agreed to attend an alcohol education program and perform 24 hours of community service in order to have the remaining charge dropped.

</doc>
<doc id="15588" url="http://en.wikipedia.org/wiki?curid=15588" title="Jung (disambiguation)">
Jung (disambiguation)

Carl Jung (1875–1961) was the founder of analytical psychology.
Jung may also refer to:

</doc>
<doc id="15593" url="http://en.wikipedia.org/wiki?curid=15593" title="JFK (disambiguation)">
JFK (disambiguation)

JFK are the initials of John F. Kennedy, 35th President of the United States.
JFK may also refer to:

</doc>
<doc id="15596" url="http://en.wikipedia.org/wiki?curid=15596" title="John Ray">
John Ray

John Ray (29 November 1627 – 17 January 1705) was an English naturalist, widely regarded as one of the earliest of the English parson-naturalists. Until 1670, he wrote his name as John Wray. From then on, he used 'Ray', after "having ascertained that such had been the practice of his family before him".
He published important works on botany, zoology, and natural theology. His classification of plants in his "Historia Plantarum", was an important step towards modern taxonomy. Ray rejected the system of dichotomous division by which species were classified according to a pre-conceived, either/or type system, and instead classified plants according to similarities and differences that emerged from observation. Thus he advanced scientific empiricism against the deductive rationalism of the scholastics. He was the first to give a biological definition of the term "species".
Early life.
John Ray was born in the village of Black Notley. He is said to have been born in the smithy, his father having been the village blacksmith. He was sent at the age of sixteen to Cambridge University: studying at Trinity College and Catharine Hall. His tutor at Trinity was James Duport, and his intimate friend and fellow-pupil the celebrated Isaac Barrow. Ray was chosen minor fellow of Trinity in 1649, and later major fellow. He held many college offices, becoming successively lecturer in Greek (1651), mathematics (1653),and humanity (1655), "praelector" (1657), junior dean (1657), and college steward (1659 and 1660); and according to the habit of the time, he was accustomed to preach in his college chapel and also at Great St Mary's, long before he took holy orders on 23 December 1660. Among these sermons were his discourses on "The wisdom of God manifested in the works of the creation", and "Deluge and Dissolution of the World". Ray's reputation was high also as a tutor; and he communicated his own passion for natural history to several pupils, of whom Francis Willughby is by far the most famous.
Career.
When Ray found himself unable to subscribe as required by the ‘Bartholomew Act’ of 1662 he, along with 13 other college fellows, resigned his fellowship on 24 August 1662 rather than swear to the declaration that the Solemn League and Covenant was not binding on those who had taken it. Tobias Smollett quoted the reasoning given in the biography of Ray by William Derham:
"The reason of his refusal was not (says his biographer) as some have imagined, his having taken the solemn league and covenant; for that he never did, and often declared that he ever thought it an unlawful oath: but he said he could not say, for those that had taken the oath, that no obligation lay upon them, but feared there might."
His religious views were generally in accord with those imposed under the restoration of Charles II of England, and (though technically a nonconformist) he continued as a layman in the Established Church of England.
From this time onwards he seems to have depended chiefly on the bounty of his pupil Francis Willughby, who made Ray his constant companion while he lived, and at his death left him 6 shillings a year, with the charge of educating his two sons.
In the spring of 1663 Ray started together with Willughby and two other pupils (Philip Skippon and Nathaniel Bacon) on a tour through Europe, from which he returned in March 1666, parting from Willughby at Montpellier, whence the latter continued his journey into Spain. He had previously in three different journeys (1658, 1661, 1662) travelled through the greater part of Great Britain, and selections from his private notes of these journeys were edited by George Scott in 1760, under the title of "Mr Ray's Itineraries". Ray himself published an account of his foreign travel in 1673, entitled "Observations topographical, moral, and physiological, made on a Journey through part of the Low Countries, Germany, Italy, and France". From this tour Ray and Willughby returned laden with collections, on which they meant to base complete systematic descriptions of the animal and vegetable kingdoms. Willughby undertook the former part, but, dying in 1672, left only an ornithology and ichthyology for Ray to edit; while Ray used the botanical collections for the groundwork of his "Methodus plantarum nova" (1682), and his great "Historia generalis plantarum" (3 vols., 1686, 1688, 1704). The plants gathered on his British tours had already been described in his "Catalogus plantarum Angliae" (1670), which formed the basis for later English floras.
In 1667 Ray was elected Fellow of the Royal Society, and in 1669 he and Willughby published a paper on "Experiments concerning the Motion of Sap in Trees". In 1671, he presented the research of Francis Jessop on formic acid to the Royal Society.
In the 1690s, he published three volumes on religion—the most popular being "The Wisdom of God Manifested in the Works of the Creation" (1691), "an essay in natural religion that called on the full range of his biological learning". In this volume, he moved on from the naming and cataloguing of species like his successor Carl Linnaeus. Instead, Ray considered species' lives and how nature worked as a whole. This work largely epitomized Natural Theology during his time.
Ray gave an early description of dendrochronology, explaining for the ash tree how to find its age from its tree-rings.
Later life and family.
In 1673 Ray married Margaret Oakley of Launton; in 1676 he went to Middleton Hall near Tamworth, and in 1677 to Falborne (or Faulkbourne) Hall in Essex. Finally, in 1679, he removed to Black Notley, where he afterwards remained. His life there was quiet and uneventful, although he had poor health, including chronic sores. Ray kept writing books and corresponded widely on scientific matters. He lived, in spite of his infirmities, to the age of seventy-seven, dying at Black Notley.
Ray's definition of species.
Ray was the first person to produce a biological definition of species, in his 1686 "History of plants":
Works.
Ray published about 23 works, depending on how one counts them. The biological works were usually in Latin, the rest in English. For ease of reading, the short titles below are in English.
Libraries holding Ray's works.
Including the various editions, there are 172 works of Ray, of which most are rare. The only libraries with substantial holdings are all in England.p153 The list in order of holdings is:
Legacy.
His biographer Charles Raven commented that "Ray sweeps away the litter of mythology and fable... and always insists upon accuracy of observation and description and the testing of every new discovery".p10 Ray's works were directly influential on the development of taxonomy by Carl Linnaeus. In 1844, the Ray Society was founded, named after John Ray. By 2013, the registered charity, with its home at the Natural History Museum, London, had published over 172 books on natural history. A different organisation, named the John Ray Society, is the Natural Sciences Society at St Catharine's College, Cambridge; it organises a programme of events of interest to science students in the college. In 1986, to mark the 300th anniversary of the publication of Ray's "Historia Plantarum", there was a celebration of Ray's legacy in Braintree. A "John Ray Gallery" was opened in the Braintree Museum.
The John Ray Initiative (JRI) is an educational charity with a vision to bring together scientific and Christian understandings of the environment in a way that can be widely communicated and lead to effective action. It was formed in 1997 in recognition of the urgent need to respond to the global environmental crisis and the challenges of sustainable development and environmental stewardship. John Ray's writings proclaimed God as creator whose wisdom is ‘manifest in the works of creation’, and as redeemer of all things. Inspired by Ray, JRI seeks to teach appreciation of nature, increase awareness of the state of the global environment and communicate a Christian understanding of environmental issues.
Other sources.
 1950: "John Ray, naturalist: his life and works". Cambridge University Press.
 1686: "Historia plantarum species", etc. [In Latin]. (London:Clark). - - 
 1713a: "", etc. (vol. 1: "Avium") [in Latin]. William Innys, London. 
 1713b: "", etc. (vol. 2: "Piscium") [in Latin]. William Innys, London.

</doc>
<doc id="15600" url="http://en.wikipedia.org/wiki?curid=15600" title="James Joyce">
James Joyce

James Augustine Aloysius Joyce (2 February 1882 – 13 January 1941) was an Irish novelist and poet, considered to be one of the most influential writers in the modernist avant-garde of the early 20th century.
Joyce is best known for "Ulysses" (1922), a landmark work in which the episodes of Homer's "Odyssey" are paralleled in an array of contrasting literary styles, perhaps most prominent among these the stream of consciousness technique he utilized. Other well-known works are the short-story collection "Dubliners" (1914), and the novels "A Portrait of the Artist as a Young Man" (1916) and "Finnegans Wake" (1939). His other writings include three books of poetry, a play, occasional journalism, and his published letters.
Joyce was born in 41 Brighton Square, Rathgar, Dublin—a kilometre from his mother's birthplace in Terenure—into a middle-class family on the way down. A brilliant student, he excelled at the Jesuit schools Clongowes and Belvedere, despite the chaotic family life imposed by his father's alcoholism and unpredictable finances. He went on to attend University College Dublin.
In 1904, in his early twenties he emigrated permanently to continental Europe with his partner Nora Barnacle. They lived in Trieste, Paris, and Zurich. Though most of his adult life was spent abroad, Joyce's fictional universe centres on Dublin, and is populated largely by characters who closely resemble family members, enemies and friends from his time there; "Ulysses" in particular is set with precision in the streets and alleyways of the city. Shortly after the publication of "Ulysses" he elucidated this preoccupation somewhat, saying, "For myself, I always write about Dublin, because if I can get to the heart of Dublin I can get to the heart of all the cities of the world. In the particular is contained the universal."
Biography.
1882–1904: Dublin.
James Augustine Aloysius Joyce was born on 2 February 1882 to John Stanislaus Joyce and Mary Jane "May" Murray, in the Dublin suburb of Rathgar. He was baptized according to the Rites of the Catholic Church in the nearby St Joseph's Church in Terenure on 5 February by Rev. John O'Mulloy. His godparents were Philip and Ellen McCann. He was the eldest of ten surviving children; two of his siblings died of typhoid. His father's family, originally from Fermoy in Cork, had once owned a small salt and lime works. Joyce's father and paternal grandfather both married into wealthy families, though the family's purported ancestor, Seán Mór Seoighe (fl. 1680) was a stonemason from Connemara. In 1887, his father was appointed rate collector (i.e., a collector of local property taxes) by Dublin Corporation; the family subsequently moved to the fashionable adjacent small town of Bray 12 mi from Dublin. Around this time Joyce was attacked by a dog, which engendered in him a lifelong cynophobia. He also suffered from astraphobia, as a superstitious aunt had described thunderstorms to him as a sign of God's wrath.
In 1891 Joyce wrote a poem on the death of Charles Stewart Parnell. His father was angry at the treatment of Parnell by the Catholic church, the Irish Home Rule Party and the English Liberal Party and the resulting collaborative failure to secure Home Rule for Ireland. The Irish Party had dropped Parnell from leadership. But the Vatican's role in allying with the English Conservative Party to prevent Home Rule left a lasting impression on the young Joyce. The elder Joyce had the poem printed and even sent a part to the Vatican Library. In November of that same year, John Joyce was entered in "Stubbs Gazette" (a publisher of bankruptcies) and suspended from work. In 1893, John Joyce was dismissed with a pension, beginning the family's slide into poverty caused mainly by John's drinking and general financial mismanagement.
Joyce had begun his education at Clongowes Wood College, a Jesuit boarding school near Clane, County Kildare, in 1888 but had to leave in 1892 when his father could no longer pay the fees. Joyce then studied at home and briefly at the Christian Brothers O'Connell School on North Richmond Street, Dublin, before he was offered a place in the Jesuits' Dublin school, Belvedere College, in 1893. This came about because of a chance meeting his father had with a Jesuit priest who knew the family and Joyce was given a reduction in fees to attend Belvedere. In 1895, Joyce, now aged 13, was elected to join the Sodality of Our Lady by his peers at Belvedere. The philosophy of Thomas Aquinas continued to have a strong influence on him for most of his life.
Joyce enrolled at the recently established University College Dublin (UCD) in 1898, studying English, French and Italian. He also became active in theatrical and literary circles in the city. In 1900 his laudatory review of Henrik Ibsen's "When We Dead Awaken" was published in "Fortnightly Review"; it was his first publication and, after learning basic Norwegian to send a fan letter to Ibsen, he received a letter of thanks from the dramatist. Joyce wrote a number of other articles and at least two plays (since lost) during this period. Many of the friends he made at University College Dublin appeared as characters in Joyce's works. His closest colleagues included leading figures of the generation, most notably, Thomas Kettle, Francis Sheehy-Skeffington and Oliver St. John Gogarty. Joyce was first introduced to the Irish public by Arthur Griffith in his newspaper, "The United Irishman", in November 1901. Joyce had written an article on the Irish Literary Theatre and his college magazine refused to print it. Joyce had it printed and distributed locally. Griffith himself wrote a piece decrying the censorship of the student James Joyce. In 1901, the National Census of Ireland lists James Joyce (19) as an English- and Irish-speaking scholar living with his mother and father, six sisters and three brothers at Royal Terrace (now Inverness Road), Clontarf, Dublin.
After graduating from UCD in 1902, Joyce left for Paris to study medicine, but he soon abandoned this after a time. Richard Ellmann suggests that this may have been because he found the technical lectures in French too difficult. Joyce had already failed to pass chemistry in English in Dublin. But Joyce claimed ill health as the problem and wrote home that he was unwell and complained about the cold weather . He stayed on for a few months, appealing for finance his family could ill afford and reading late in the Bibliothèque Sainte-Geneviève. When his mother was diagnosed with cancer, his father sent a telegram which read, "NOTHER ["sic"] DYING COME HOME FATHER". Joyce returned to Ireland. Fearing for her son's impiety, his mother tried unsuccessfully to get Joyce to make his confession and to take communion. She finally passed into a coma and died on 13 August, James and Stanislaus having refused to kneel with other members of the family praying at her bedside. After her death he continued to drink heavily, and conditions at home grew quite appalling. He scraped a living reviewing books, teaching, and singing—he was an accomplished tenor, and won the bronze medal in the 1904 Feis Ceoil.
On 7 January 1904 he attempted to publish "A Portrait of the Artist", an essay-story dealing with aesthetics, only to have it rejected from the free-thinking magazine "Dana". He decided, on his twenty-second birthday, to revise the story into a novel he called "Stephen Hero". It was a fictional rendering of Joyce's youth, but he eventually grew frustrated with its direction and abandoned this work. It was never published in this form, but years later, in Trieste, Joyce completely rewrote it as "A Portrait of the Artist as a Young Man". The unfinished "Stephen Hero" was published after his death.
The same year he met Nora Barnacle, a young woman from Galway City who was working as a chambermaid. On 16 June 1904, they first stepped out together, an event which would be commemorated by providing the date for the action of "Ulysses".
Joyce remained in Dublin for some time longer, drinking heavily. After one of these drinking binges, he got into a fight over a misunderstanding with a man in St Stephen's Green; he was picked up and dusted off by a minor acquaintance of his father, Alfred H. Hunter, who brought him into his home to tend to his injuries. Hunter was rumoured to be a Jew and to have an unfaithful wife, and would serve as one of the models for Leopold Bloom, the protagonist of "Ulysses". He took up with medical student Oliver St John Gogarty, who formed the basis for the character Buck Mulligan in "Ulysses". After staying for six nights in the Martello Tower that Gogarty was renting in Sandycove, he left in the middle of the night following an altercation which involved another student he lived with, the unstable Dermot Chenevix Trench (Haines in "Ulysses"), firing a pistol at some pans hanging directly over Joyce's bed. He walked the 13 kilometres back to Dublin to stay with relatives for the night, and sent a friend to the tower the next day to pack his trunk. Shortly thereafter he left Ireland with Nora to live on the Continent.
1904–20: Trieste and Zurich.
Joyce and Nora went into self-imposed exile, moving first to Zurich in Switzerland, where he had supposedly acquired a post to teach English at the Berlitz Language School through an agent in England. It turned out that the agent had been swindled; the director of the school sent Joyce on to Trieste, which was then part of Austria-Hungary (until World War I), and is today part of Italy. Once again, he found there was no position for him, but with the help of Almidano Artifoni, director of the Trieste Berlitz school, he finally secured a teaching position in Pola, then also part of Austria-Hungary (today part of Croatia). He stayed there, teaching English mainly to Austro-Hungarian naval officers stationed at the Pola base, from October 1904 until March 1905, when the Austrians—having discovered an espionage ring in the city—expelled all aliens. With Artifoni's help, he moved back to Trieste and began teaching English there. He remained in Trieste for most of the next ten years.
Later that year Nora gave birth to their first child, Giorgio. Joyce then managed to talk his brother, Stanislaus, into joining him in Trieste, and secured him a position teaching at the school. Joyce's ostensible reasons were desire for Stanislaus's company and the hope of offering him a more interesting life than that of his simple clerking job in Dublin. Joyce also hoped to augment his family's meagre income with his brother's earnings. Stanislaus and Joyce had strained relations throughout the time they lived together in Trieste, with most arguments centring on Joyce's drinking habits and frivolity with money.
Joyce became frustrated with life in Trieste and moved to Rome in late 1906, having secured employment as a letter-writing clerk in a bank. He intensely disliked Rome, and moved back to Trieste in early 1907. His daughter Lucia was born later that year.
Joyce returned to Dublin in mid-1909 with George, to visit his father and work on getting "Dubliners" published. He visited Nora's family in Galway and liked Nora's mother very much. While preparing to return to Trieste he decided to take one of his sisters, Eva, back with him to help Nora run the home. He spent only a month in Trieste before returning to Dublin, this time as a representative of some cinema owners and businessmen from Trieste. With their backing he launched Ireland's first cinema, the Volta Cinematograph, which was well-received, but fell apart after Joyce left. He returned to Trieste in January 1910 with another sister, Eileen, in tow. Eva became homesick for Dublin and returned there a few years later, but Eileen spent the rest of her life on the continent, eventually marrying Czech bank cashier Frantisek Schaurek.
Joyce returned to Dublin again briefly in mid-1912 during his years-long fight with Dublin publisher George Roberts over the publication of "Dubliners". His trip was once again fruitless, and on his return he wrote the poem "Gas from a Burner", an invective against Roberts. After this trip, he never again came closer to Dublin than London, despite many pleas from his father and invitations from fellow Irish writer William Butler Yeats.
One of his students in Trieste was Ettore Schmitz, better known by the pseudonym Italo Svevo. They met in 1907 and became lasting friends and mutual critics. Schmitz was a Catholic of Jewish origin and became a primary model for Leopold Bloom; most of the details about the Jewish faith in "Ulysses" came from Schmitz's responses to queries from Joyce. While living in Trieste, Joyce was first beset with eye problems that ultimately required over a dozen surgical operations.
Joyce concocted a number of money-making schemes during this period, including an attempt to become a cinema magnate in Dublin. He also frequently discussed but ultimately abandoned a plan to import Irish tweed to Trieste. Correspondence relating to that venture with the Irish Woollen Mills were for a long time displayed in the windows of their premises in Dublin. Joyce's skill at borrowing money saved him from indigence. What income he had came partially from his position at the Berlitz school and partially from teaching private students.
In 1915, after most of his students in Trieste were conscripted to fight in World War I, Joyce moved to Zurich. Two influential private students, Baron Ambrogio Ralli and Count Francesco Sordina, petitioned officials for an exit permit for the Joyces, who in turn agreed not to take any action against the emperor of Austria-Hungary during the war. In Zurich, Joyce met one of his most enduring and important friends, the English socialist painter Frank Budgen, whose opinion Joyce constantly sought through the writing of "Ulysses" and "Finnegans Wake". It was also here that Ezra Pound brought him to the attention of English feminist and publisher Harriet Shaw Weaver, who would become Joyce's patron, providing him with thousands of pounds over the next 25 years and relieving him of the burden of teaching to focus on his writing. While in Zurich he wrote "Exiles", published "A Portrait...", and began serious work on "Ulysses". Zurich during the war was home to exiles and artists from across Europe, and its bohemian, multilingual atmosphere suited him. Nevertheless, after four years he was restless, and after the war he returned to Trieste as he had originally planned. He found the city had changed, and some of his old friends noted his maturing from teacher to artist. His relations with his brother Stanislaus (who had been interned in an Austrian prison camp for most of the war due to his pro-Italian politics) were more strained than ever. Joyce went to Paris in 1920 at an invitation from Ezra Pound, supposedly for a week, but the family ended up living there for the next twenty years.
1920–41: Paris and Zurich.
Joyce set himself to finishing "Ulysses" in Paris, delighted to find that he was gradually gaining fame as an avant-garde writer. A further grant from Miss Shaw Weaver meant he could devote himself full-time to writing again, as well as consort with other literary figures in the city.
During this era, Joyce's eyes began to give him more and more problems. He was treated by Dr Louis Borsch in Paris, undergoing nine operations before Borsch's death in 1929. Throughout the 1930s he travelled frequently to Switzerland for eye surgeries and for treatments for his daughter Lucia, who, according to the Joyces, suffered from schizophrenia. Lucia was analysed by Carl Jung at the time, who after reading "Ulysses", is said to have concluded that her father had schizophrenia. Jung said she and her father were two people heading to the bottom of a river, except that Joyce was diving and Lucia was sinking.
In Paris, Maria and Eugene Jolas nursed Joyce during his long years of writing "Finnegans Wake." Were it not for their support (along with Harriet Shaw Weaver's constant financial support), there is a good possibility that his books might never have been finished or published. In their literary magazine ""transition"," the Jolases published serially various sections of "Finnegans Wake" under the title "Work in Progress." Joyce returned to Zurich in late 1940, fleeing the Nazi occupation of France.
On 11 January 1941, he underwent surgery in Zurich for a perforated ulcer. While he at first improved, he relapsed the following day, and despite several transfusions, fell into a coma. He awoke at 2 a.m. on 13 January 1941, and asked for a nurse to call his wife and son, before losing consciousness again. They were still on their way when he died 15 minutes later.
Joyce's body was interred in the Fluntern Cemetery near Zurich Zoo. Swiss tenor Max Meili sang "Addio terra, addio cielo" from Monteverdi's "L'Orfeo" at the burial service. Although two senior Irish diplomats were in Switzerland at the time, neither attended Joyce's funeral, and the Irish government later declined Nora's offer to permit the repatriation of Joyce's remains. Nora, who had married Joyce in London in 1931, survived him by 10 years. She is buried by his side, as is their son Giorgio, who died in 1976.
Joyce and religion.
The issue of Joyce's relationship with religion is somewhat controversial. Early in life, he lapsed from Catholicism, according to first-hand testimonies coming from himself, his brother Stanislaus Joyce, and his wife:
My mind rejects the whole present social order and Christianity—home, the recognised virtues, classes of life, and religious doctrines. [...] Six years ago I left the Catholic church, hating it most fervently. I found it impossible for me to remain in it on account of the impulses of my nature. I made secret war upon it when I was a student and declined to accept the positions it offered me. By doing this I made myself a beggar but I retained my pride. Now I make open war upon it by what I write and say and do.
My brother’s breakaway from Catholicism was due to other motives. He felt it was imperative that he should save his real spiritual life from being overlaid and crushed by a false one that he had outgrown. He believed that poets in the measure of their gifts and personality were the repositories of the genuine spiritual life of their race and the priests were usurpers. He detested falsity and believed in individual freedom more thoroughly than any man I have ever known. [...] The interest that my brother always retained in the philosophy of the Catholic Church sprang from the fact that he considered Catholic philosophy to be the most coherent attempt to establish such an intellectual and material stability.
When the arrangements for Joyce's burial were being made, a Catholic priest offered a religious service, which Joyce's wife Nora declined, saying: "I couldn't do that to him."
However, L. A. G. Strong, William T. Noon, Robert Boyle and others have argued that Joyce, later in life, reconciled with the faith he rejected earlier in life and that his parting with the faith was succeeded by a not so obvious reunion, and that "Ulysses" and "Finnegans Wake" are essentially Catholic expressions. Likewise, Hugh Kenner and T.S. Eliot saw between the lines of Joyce's work the outlook of a serious Christian and that beneath the veneer of the work lies a remnant of Catholic belief and attitude. Kevin Sullivan maintains that, rather than reconciling with the faith, Joyce never left it. Critics holding this view insist that Stephen, the protagonist of the semi-autobiographical "A Portrait of the Artist as a Young Man" as well as "Ulysses", is not Joyce. Somewhat cryptically, in an interview after completing Ulysses, in response to the question "When did you leave the Catholic Church", Joyce answered, "That's for the Church to say." Eamonn Hughes maintains that Joyce takes a dialectic approach, both affirming and denying, saying that Stephen's much noted "non-serviam" is qualified—"I will not serve "that which I no longer believe"...", and that the "non-serviam" will always be balanced by Stephen's "I am a servant..." and Molly's "yes". It is also known from first hand testimonies and his own writing that Joyce attended Catholic Mass and Orthodox Sacred Liturgy, especially during Holy Week, purportedly for aesthetic reasons. His sisters also noted his Holy Week attendance and that he did not seek to dissuade them. One friend witnessed him cry "secret tears" upon hearing Jesus' words on the cross and another accused him of being a "believer at heart" because of his frequency in church.
Umberto Eco compares Joyce to the ancient "episcopi vagantes" (stray bishops) in the Middle Ages. They left a discipline, not a cultural heritage or a way of thinking. Like them, the writer retains the sense of blasphemy held as a liturgical ritual.
Some critics and biographers have opined along the lines of Andrew Gibson: "The modern James Joyce may have vigorously resisted the oppressive power of Catholic tradition. But there was another Joyce who asserted his allegiance to that tradition, and never left it, or wanted to leave it, behind him." Gibson argues that Joyce "remained a Catholic intellectual if not a believer" since his thinking remained influenced by his cultural background, even though he dissented from that culture. His relationship with religion was complex and not easily understood, even perhaps by himself. He acknowledged the debt he owed to his early Jesuit training. Joyce told the sculptor August Suter, that from his Jesuit education, he had 'learnt to arrange things in such a way that they become easy to survey and to judge.'
Joyce and music.
Music is central to Joyce's biography and to the understanding of his writings. In turn, Joyce's poetry and prose became an inspiration for composers and musicians. There are at least five aspects to consider:
1. Joyce's musicality: Joyce had considerable musical talent, which expressed itself in his singing, piano and guitar playing, as well as in a melody that he composed. His own musicality (which once made him consider music as a profession) is the root of his strong adoption of music as a major driving force in his fiction, in addition to his own experience of music in Ireland before he left in 1904. Joyce had a light tenor voice; he was taught by Vincent O'Brien and Benedetto Palmieri; in 1904 won a bronze medal at the competitive music festival Feis Ceoil. His only composition is a melody to his poem "Bid adieu", to which a piano accompaniment was added in the 1920s in Paris by the American composer Edmund Pendleton (1899–1987).
2. The music Joyce knew: Music frequently found its way into Joyce's poetry and prose. Often this happens in the form of allusions to (or partial quotations from) texts of Irish traditional songs, popular ballads, Roman Catholic chant and opera arias. His operatic references include works by Balfe, Wallace and Arthur Sullivan, in addition to Meyerbeer, Mozart, and Wagner (among many others). Joyce also makes frequent use of the "Irish Melodies" of Thomas Moore and ballads such as George Barker's "Dublin Bay" and J.L. Molloy's "Love's Old Sweet Song".
3. Opera as a genre: Joyce had a lifelong preoccupation with opera as a generic precedent for his own fiction. Although Joyce scholarship has long identified an explicit recourse to musical structures in "Ulysses" (in particular the 'Sirens' episode) and "Finnegans Wake", more recent criticism has established a decisive reliance on Wagner's "Ring" in "Finnegans Wake" and an attempt to adapt the structures of opera and oratorio to the medium of fiction, notably in the 'Cyclops' episode of "Ulysses". George Antheil's unfinished setting of 'Cyclops' as an opera attests this attempt.
4. Music to Joyce's words: Music that uses Joyce's texts most frequently appear as settings of his poems in songs, and occasionally as excerpts from prose works. Irish composers were among the first to set Joyce's poetry, including Geoffrey Molyneux Palmer (1882–1957), Herbert Hughes (1882–1937) and Brian Boydell (1917–2000), but the musical qualities of Joyce's verse also attracted European and North American composers, with early settings by Karol Szymanowski ("Songs to Words by James Joyce" op. 54, 1926) and Samuel Barber ("Three Songs" op. 10, 1936) in addition to settings by major exponents of the 1950s and '60s avant-garde such as Elliot Carter (String Quartet No. 1, 1951) and Luciano Berio ("Chamber Music", 1953; "Thema (Ommagio a Joyce)", 1958; etc.). In 2015 "Waywords and Meansigns: Recreating Finnegans Wake [in its whole wholume]" set "Finnegans Wake" to music, unabridged. 
5. Music inspired by Joyce: Often, instrumental music was also inspired by Joyce's writings, including works by Pierre Boulez, Klaus Huber, Rebecca Saunders, Toru Takemitsu and Gerard Victory. With Berio's "Thema (Ommagio a Joyce)" (1958) there is also a key work in the development of electro-acoustic music. In 2014 the English composer Stephen Crowe set Joyce's explicit letters to Nora as a song-cycle for tenor and ensemble.
Joyce himself took a keen interest in musical settings of his work, performed some of them himself, and corresponded with many of the composers in question. He was particularly fond of the early settings by Palmer.
Major works.
"Dubliners".
Joyce's Irish experiences constitute an essential element of his writings, and provide all of the settings for his fiction and much of its subject matter. His early volume of short stories, "Dubliners", is a penetrating analysis of the stagnation and paralysis of Dublin society. The stories incorporate epiphanies, a word used particularly by Joyce, by which he meant a sudden consciousness of the "soul" of a thing.
"A Portrait of the Artist as a Young Man".
"A Portrait of the Artist as a Young Man" is a nearly complete rewrite of the abandoned novel "Stephen Hero". Joyce attempted to burn the original manuscript in a fit of rage during an argument with Nora, though to his subsequent relief it was rescued by his sister. A "Künstlerroman", "Portrait" is a heavily autobiographical coming-of-age novel depicting the childhood and adolescence of protagonist Stephen Dedalus and his gradual growth into artistic self-consciousness. Some hints of the techniques Joyce frequently employed in later works, such as stream of consciousness, interior monologue, and references to a character's psychic reality rather than to his external surroundings, are evident throughout this novel. Joseph Strick directed a film of the book in 1977 starring Luke Johnston, Bosco Hogan, T. P. McKenna and John Gielgud.
"Exiles" and poetry.
Despite early interest in the theatre, Joyce published only one play, "Exiles", begun shortly after the outbreak of World War I in 1914 and published in 1918. A study of a husband and wife relationship, the play looks back to "The Dead" (the final story in "Dubliners") and forward to "Ulysses", which Joyce began around the time of the play's composition.
Joyce also published a number of books of poetry. His first mature published work was the satirical broadside "The Holy Office" (1904), in which he proclaimed himself to be the superior of many prominent members of the Celtic revival. His first full-length poetry collection "Chamber Music" (1907) (referring, Joyce joked, to the sound of urine hitting the side of a chamber pot) consisted of 36 short lyrics. This publication led to his inclusion in the "Imagist Anthology", edited by Ezra Pound, who was a champion of Joyce's work. Other poetry Joyce published in his lifetime includes "Gas From A Burner" (1912), "Pomes Penyeach" (1927) and "Ecce Puer" (written in 1932 to mark the birth of his grandson and the recent death of his father). It was published by the Black Sun Press in "Collected Poems" (1936).
"Ulysses".
As he was completing work on "Dubliners" in 1906, Joyce considered adding another story featuring a Jewish advertising canvasser called Leopold Bloom under the title "Ulysses". Although he did not pursue the idea further at the time, he eventually commenced work on a novel using both the title and basic premise in 1914. The writing was completed in October 1921. Three more months were devoted to working on the proofs of the book before Joyce halted work shortly before his self-imposed deadline, his 40th birthday (2 February 1922).
Thanks to Ezra Pound, serial publication of the novel in the magazine "The Little Review" began in 1918. This magazine was edited by Margaret Anderson and Jane Heap, with the backing of John Quinn, a New York attorney with an interest in contemporary experimental art and literature. Unfortunately, this publication encountered censorship problems in the United States; serialisation was halted in 1920 when the editors were convicted of publishing obscenity. Although the conviction was based on the "Nausicaä" episode of "Ulysses", "The Little Review" had fuelled the fires of controversy with dada poet Elsa von Freytag-Loringhoven's defence of "Ulysses" in an essay "The Modest Woman." Joyce's novel was not published in the United States until 1933.
Partly because of this controversy, Joyce found it difficult to get a publisher to accept the book, but it was published in 1922 by Sylvia Beach from her well-known Rive Gauche bookshop, "Shakespeare and Company". An English edition published the same year by Joyce's patron, Harriet Shaw Weaver, ran into further difficulties with the United States authorities, and 500 copies that were shipped to the States were seized and possibly destroyed. The following year, John Rodker produced a print run of 500 more intended to replace the missing copies, but these were burned by English customs at Folkestone. A further consequence of the novel's ambiguous legal status as a banned book was that a number of "bootleg" versions appeared, most notably a number of pirate versions from the publisher Samuel Roth. In 1928, a court injunction against Roth was obtained and he ceased publication.
With the appearance of both "Ulysses" and T. S. Eliot's poem, "The Waste Land", 1922 was a key year in the history of English-language literary modernism. In "Ulysses", Joyce employs stream of consciousness, parody, jokes, and virtually every other established literary technique to present his characters. The action of the novel, which takes place in a single day, 16 June 1904, sets the characters and incidents of the Odyssey of Homer in modern Dublin and represents Odysseus (Ulysses), Penelope and Telemachus in the characters of Leopold Bloom, his wife Molly Bloom and Stephen Dedalus, parodically contrasted with their lofty models. Both Bloom and Dedalus represent Joyce in difference ages: youth and middle age. And both relate to each other symbolically in the novel as father and son. The key to this father/son relationship is revealed by Stephen on the Sandymount strand when he contemplates the Nicene Creed and the 'consubstantial' relationship of God the Father to Son. The book explores various areas of Dublin life, dwelling on its squalor and monotony. Nevertheless, the book is also an affectionately detailed study of the city, and Joyce claimed that if Dublin were to be destroyed in some catastrophe it could be rebuilt, brick by brick, using his work as a model. To achieve this level of accuracy, Joyce used the 1904 edition of Thom's Directory—a work that listed the owners and/or tenants of every residential and commercial property in the city. He also bombarded friends still living there with requests for information and clarification.
The book consists of 18 chapters, each covering roughly one hour of the day, beginning around 8 a.m. and ending sometime after 2 a.m. the following morning. Each chapter employs its own literary style, and parodies a specific episode in Homer's Odyssey. Furthermore, each chapter is associated with a specific colour, art or science, and bodily organ. This combination of kaleidoscopic writing with an extreme formal schematic structure renders the book a major contribution to the development of 20th-century modernist literature. The use of classical mythology as an organising framework, the near-obsessive focus on external detail, and the occurrence of significant action within the minds of characters have also contributed to the development of literary modernism. Nevertheless, Joyce complained that, "I may have oversystematised "Ulysses"," and played down the mythic correspondences by eliminating the chapter titles that had been taken from Homer. Joyce was reluctant to publish the chapter titles because he wanted his work to stand separately from the Greek form. It was only when Stuart Gilbert published his critical work on "Ulysses" in 1930 that the schema was supplied by Joyce to Gilbert. But as Terrence Killeen points out this schema was developed after the novel had been written and was not something that Joyce consulted as he wrote the novel. A first edition copy of Ulysses is on display at The Little Museum of Dublin 
"Finnegans Wake".
Having completed work on "Ulysses", Joyce was so exhausted that he did not write a line of prose for a year. On 10 March 1923 he informed a patron, Harriet Weaver: "Yesterday I wrote two pages—the first I have since the final "Yes" of "Ulysses". Having found a pen, with some difficulty I copied them out in a large handwriting on a double sheet of foolscap so that I could read them. "Il lupo perde il pelo ma non il vizio", the Italians say. 'The wolf may lose his skin but not his vice' or 'the leopard cannot change his spots.'" Thus was born a text that became known, first, as "Work in Progress" and later "Finnegans Wake".
By 1926 Joyce had completed the first two parts of the book. In that year, he met Eugene and Maria Jolas who offered to serialise the book in their magazine "transition". For the next few years, Joyce worked rapidly on the new book, but in the 1930s, progress slowed considerably. This was due to a number of factors, including the death of his father in 1931, concern over the mental health of his daughter Lucia and his own health problems, including failing eyesight. Much of the work was done with the assistance of younger admirers, including Samuel Beckett. For some years, Joyce nursed the eccentric plan of turning over the book to his friend James Stephens to complete, on the grounds that Stephens was born in the same hospital as Joyce exactly one week later, and shared the first name of both Joyce and of Joyce's fictional alter-ego, an example of Joyce's superstitions.
Reaction to the work was mixed, including negative comment from early supporters of Joyce's work, such as Pound and the author's brother, Stanislaus Joyce. To counteract this hostile reception, a book of essays by supporters of the new work, including Beckett, William Carlos Williams and others was organised and published in 1929 under the title "Our Exagmination Round His Factification for Incamination of Work in Progress". At his 57th birthday party at the Jolases' home, Joyce revealed the final title of the work and "Finnegans Wake" was published in book form on 4 May 1939. Later, further negative comments surfaced from doctor and author Hervey Cleckley, who questioned the significance others had placed on the work. In his book, "The Mask of Sanity", Cleckley refers to "Finnegans Wake" as "a 628-page collection of erudite gibberish indistinguishable to most people from the familiar word salad produced by hebephrenic patients on the back wards of any state hospital."
Joyce's method of stream of consciousness, literary allusions and free dream associations was pushed to the limit in "Finnegans Wake", which abandoned all conventions of plot and character construction and is written in a peculiar and obscure language, based mainly on complex multi-level puns. This approach is similar to, but far more extensive than that used by Lewis Carroll in "Jabberwocky". This has led many readers and critics to apply Joyce's oft-quoted description in the "Wake" of "Ulysses" as his "usylessly unreadable Blue Book of Eccles" to the "Wake" itself. However, readers have been able to reach a consensus about the central cast of characters and general plot.
Much of the wordplay in the book stems from the use of multilingual puns which draw on a wide range of languages. The role played by Beckett and other assistants included collating words from these languages on cards for Joyce to use and, as Joyce's eyesight worsened, of writing the text from the author's dictation.
The view of history propounded in this text is very strongly influenced by Giambattista Vico, and the metaphysics of Giordano Bruno of Nola are important to the interplay of the "characters." Vico propounded a cyclical view of history, in which civilisation rose from chaos, passed through theocratic, aristocratic, and democratic phases, and then lapsed back into chaos. The most obvious example of the influence of Vico's cyclical theory of history is to be found in the opening and closing words of the book. "Finnegans Wake" opens with the words "riverrun, past Eve and Adam's, from swerve of shore to bend of bay, brings us by a commodius vicus of recirculation back to Howth Castle and Environs." ("vicus" is a pun on Vico) and ends "A way a lone a last a loved a long the." In other words, the book ends with the beginning of a sentence and begins with the end of the same sentence, turning the book into one great cycle. Indeed, Joyce said that the ideal reader of the "Wake" would suffer from "ideal insomnia" and, on completing the book, would turn to page one and start again, and so on in an endless cycle of reading.
Legacy.
Joyce's work has been subject to intense scrutiny by scholars of all types. He has also been an important influence on writers and scholars as diverse as Samuel Beckett, Seán Ó Ríordáin, Jorge Luis Borges, Flann O'Brien, Salman Rushdie, Robert Anton Wilson, John Updike, David Lodge and Joseph Campbell. "Ulysses" has been called "a demonstration and summation of the entire [Modernist] movement". French literary theorist Julia Kristéva characterised Joyce's novel writing as "polyphonic" and a hallmark of postmodernity alongside poets Mallarmé and Rimbaud.
Some scholars, most notably Vladimir Nabokov, have mixed feelings on his work, often championing some of his fiction while condemning other works. In Nabokov's opinion, "Ulysses" was brilliant, "Finnegans Wake" horrible—an attitude Jorge Luis Borges shared.
Joyce's influence is also evident in fields other than literature. The sentence "Three quarks for Muster Mark!" in Joyce's "Finnegans Wake" is the source of the word "quark", the name of one of the elementary particles, proposed by the physicist, Murray Gell-Mann in 1963. The French philosopher Jacques Derrida has written a book on the use of language in "Ulysses", and the American philosopher Donald Davidson has written similarly on "Finnegans Wake" in comparison with Lewis Carroll. Psychoanalyst Jacques Lacan used Joyce's writings to explain his concept of the "sinthome". According to Lacan, Joyce's writing is the supplementary cord which kept Joyce from psychosis.
In 1999, "Time Magazine" named Joyce one of the , and stated; "Joyce ... revolutionised 20th century fiction". In 1998, the Modern Library, US publisher of Joyce's works, ranked "Ulysses" No. 1, "A Portrait of the Artist as a Young Man" No. 3, and "Finnegans Wake" No. 77, on its list of the 100 best English-language novels of the 20th century.
The work and life of Joyce is celebrated annually on 16 June, known as Bloomsday, in Dublin and in an increasing number of cities worldwide, and critical studies in scholarly publications, such as the "James Joyce Quarterly", continue. Both popular and academic uses of Joyce's work were hampered by restrictions placed by Stephen J. Joyce, Joyce's grandson and executor of his literary estate. On 1 January 2012, those restrictions were lessened by the expiry of copyright protection for much of the published work of James Joyce.
In April 2013 the Central Bank of Ireland issued a silver €10 commemorative coin in honour of Joyce that misquoted a famous line from his masterwork "Ulysses" despite being warned on at least two occasions by the Department of Finance over difficulties with copyright and design.
On 9 July 2013 it was announced that the second ship of the Samuel Beckett-class offshore patrol vessel (OPV) would be named in Joyce's honour. The LÉ James Joyce (P62) is due to be delivered to the Irish Naval Service in May 2015.
References.
</dl>
External links.
Joyce Papers
Resources
Portraits
Audio

</doc>
<doc id="15601" url="http://en.wikipedia.org/wiki?curid=15601" title="Judo">
Judo

Judo (柔道, jūdō, meaning "gentle way") is a modern martial art, combat and Olympic sport created in Japan in 1882 by Jigoro Kano (嘉納治五郎). Its most prominent feature is its competitive element, where the objective is to either throw or takedown an opponent to the ground, immobilize or otherwise subdue an opponent with a pin, or force an opponent to submit with a joint lock or a choke. Strikes and thrusts by hands and feet as well as weapons defenses are a part of judo, but only in pre-arranged forms (kata, 形) and are not allowed in judo competition or free practice (randori, 乱取り). A judo practitioner is called a judoka.
The philosophy and subsequent pedagogy developed for judo became the model for other modern Japanese martial arts that developed from "koryū" (古流, traditional schools). The worldwide spread of judo has led to the development of a number of offshoots such as Sambo and Brazilian jiu-jitsu.
History and philosophy.
Early life of the founder.
The early history of judo is inseparable from its founder, Japanese polymath and educator Jigoro Kano (嘉納 治五郎, Kanō Jigorō, 1860–1938), born Shinnosuke Kano (嘉納 新之助, Kanō Shinnosuke). Kano was born into a relatively affluent family. His father, Jirosaku, was the second son of the head priest of the Shinto Hiyoshi shrine in Shiga Prefecture. He married Sadako Kano, daughter of the owner of Kiku-Masamune sake brewing company and was adopted by the family, changing his name to Kano, and ultimately became an official in the Bakufu government.
Jigoro Kano had an academic upbringing and, from the age of seven, he studied English, Japanese calligraphy (書道, shodō) and the Four Confucian Texts (四書, Shisho) under a number of tutors. When he was fourteen, Kano began boarding at an English-medium school, Ikuei-Gijuku in Shiba, Tokyo. The culture of bullying endemic at this school was the catalyst that caused Kano to seek out a Jujutsu (柔術, Jūjutsu) dojo (道場, dōjō, training place) at which to train.
Early attempts to find a jujutsu teacher who was willing to take him on met with little success. With the fall of the Tokugawa shogunate in the Meiji Restoration of 1868, jujutsu had become unfashionable in an increasingly westernised Japan. Many of those who had once taught the art had been forced out of teaching or become so disillusioned with it that they had simply given up. Nakai Umenari, an acquaintance of Kanō's father and a former soldier, agreed to show him "kata", but not to teach him. The caretaker of his father's second house, Katagiri Ryuji, also knew jujutsu, but would not teach it as he believed it was no longer of practical use. Another frequent visitor to Kanō's father's house, Imai Genshiro of "Kyūshin-ryū" (扱心流) school of jujutsu, also refused. Several years passed before he finally found a willing teacher.
In 1877, as a student at the Tokyo-"Kaisei" school (soon to become part of the newly founded Tokyo Imperial University), Kano learned that many jujutsu teachers had been forced to pursue alternative careers, frequently opening "Seikotsu-in" (整骨院, traditional osteopathy practices). After inquiring at a number of these, Kano was referred to Fukuda Hachinosuke (c.1828–1880), a teacher of the "Tenjin Shin'yō-ryū" (天神真楊流) of jujutsu, who had a small nine mat dojo where he taught five students. Fukuda is said to have emphasized technique over formal exercise, sowing the seeds of Kano's emphasis on randori (乱取り, randori, free practice) in judo.
On Fukuda's death in 1880, Kano, who had become his keenest and most able student in both "randori" and kata (形, kata, pre-arranged forms), was given the "densho" (伝書, scrolls) of the Fukuda dojo. Kano chose to continue his studies at another "Tenjin Shin'yō-ryū" school, that of Iso Masatomo (c.1820–1881). Iso placed more emphasis on the practice of kata, and entrusted "randori" instruction to assistants, increasingly to Kano. Iso died in June 1881 and Kano went on to study at the dojo of Iikubo Tsunetoshi (1835–1889) of "Kitō-ryū" (起倒流). Like Fukuda, Iikubo placed much emphasis on "randori", with "Kitō-ryū" having a greater focus on "nage-waza" (投げ技, throwing techniques).
Founding of the Kodokan.
In February 1882, Kano founded a school and dojo at the "Eisho-ji" (永昌寺), a Buddhist temple in what was then the Shitaya ward of Tokyo (now the Higashi Ueno district of Taitō ward). Iikubo, Kano's "Kitō-ryū" instructor, attended the dojo three days a week to help teach and, although two years would pass before the temple would be called by the name Kodokan (講道館, Kōdōkan, "place for expounding the way"), and Kano had not yet received his "Menkyo" (免許, certificate of mastery) in "Kitō-ryū", this is now regarded as the Kodokan founding.
The "Eisho-ji" dojo was a relatively small affair, consisting of a twelve mat training area. Kano took in resident and non-resident students, the first two being Tsunejiro Tomita and Shiro Saigo. In August, the following year, the pair were granted shodan (初段, first rank) grades, the first that had been awarded in any martial art.
Judo versus jujutsu.
Central to Kano's vision for judo were the principles of "seiryoku zen'yō" (精力善用, maximum efficiency, minimum effort) and "jita kyōei" (自他共栄, mutual welfare and benefit). He illustrated the application of "seiryoku zen'yō" with the concept of "jū yoku gō o seisu" (柔よく剛を制す, softness controls hardness):
 In short, resisting a more powerful opponent will result in your defeat, whilst adjusting to and evading your opponent's attack will cause him to lose his balance, his power will be reduced, and you will defeat him. This can apply whatever the relative values of power, thus making it possible for weaker opponents to beat significantly stronger ones. This is the theory of "ju yoku go o seisu".
Kano realised that "seiryoku zen'yō", initially conceived as a jujutsu concept, had a wider philosophical application. Coupled with the Confucianist-influenced "jita kyōei", the wider application shaped the development of judo from a martial art (武術, bujutsu) to a martial way (武道, budō). Kano rejected techniques that did not conform to these principles and emphasised the importance of efficiency in the execution of techniques. He was convinced that practice of jujutsu while conforming to these ideals was a route to self-improvement and the betterment of society in general. He was, however, acutely conscious of the Japanese public's negative perception of jujutsu:
 At the time a few bujitsu (martial arts) experts still existed but bujitsu was almost abandoned by the nation at large. Even if I wanted to teach jujitsu most people had now stopped thinking about it. So I thought it better to teach under a different name principally because my objectives were much wider than jujitsu.
Kano believed that "jūjutsu" was insufficient to describe his art: although "Jutsu" (術) means "art" or "means", it implies a method consisting of a collection of physical techniques. Accordingly, he changed the second character to "dō" (道), meaning way, road or path, which implies a more philosophical context than "jutsu" and has a common origin with the Chinese concept of "tao". Thus Kano renamed it judo (柔道, Jūdō).
Judo "waza" (techniques).
There are three basic categories of "waza" (技, techniques) in judo: "nage-waza" (投げ技, throwing techniques), "katame-waza" (固技, grappling techniques) and "atemi-waza" (当て身技, striking techniques). Judo is most known for "nage-waza" and "katame-waza".
Judo practitioners typically devote a portion of each practice session to "ukemi" (受け身, break-falls), in order that "nage-waza" can be practiced without significant risk of injury. Several distinct types of "ukemi" exist, including "ushiro ukemi" (後ろ受身, rear breakfalls); "yoko ukemi" (横受け身, side breakfalls); "mae ukemi" (前受け身, front breakfalls); and "zenpo kaiten ukemi" (前方回転受身, rolling breakfalls)
The person who performs a "Waza" is known as "tori" (取り, literally "taker") and the person to whom it is performed is known as "uke" (受け, literally "receiver").
"Nage waza" (throwing techniques).
"Nage waza" include all techniques in which "tori" attempts to throw or trip "uke", usually with the aim of placing "uke" on his back. Each technique has three distinct stages:
"Nage waza" are typically drilled by the use of "uchi komi" (内込), repeated turning-in, taking the throw up to the point of "kake".
Traditionally, "nage waza" are further categorised into "tachi-waza" (立ち技, standing techniques), throws that are performed with "tori" maintaining an upright position, and "sutemi-waza" (捨身技, sacrifice techniques), throws in which "tori" sacrifices his upright position in order to throw "uke".
"Tachi-waza" are further subdivided into "te-waza" (手技, hand techniques), in which "tori" predominantly uses his arms to throw "uke"; "koshi-waza" (腰技, hip techniques) throws that predominantly use a lifting motion from the hips; and "ashi-waza" (足技, foot and leg techniques), throws in which "tori" predominantly utilises his legs.
"Katame-waza" (grappling techniques).
"Katame-waza" is further categorised into "osaekomi-waza" (押込技, holding techniques), in which "tori" traps and pins "uke" on his back on the floor; "shime-waza" (絞技, strangulation techniques), in which "tori" attempts to force a submission by choking or strangling "uke"; and "kansetsu-waza" (関節技, joint techniques), in which "tori" attempts to submit "uke" by painful manipulation of his joints.
A related concept is that of "ne-waza" (寝技, prone techniques), in which "waza" are applied from a non-standing position.
In competitive judo, "Kansetsu-waza" is currently limited to elbow joint manipulation. Manipulation and locking of other joints can be found in various kata, such as "Katame-no-kata" and "Kodokan goshin jutsu".
"Atemi-waza" (striking techniques).
"Atemi-waza" are techniques in which "tori" disables "uke" with a strike to a vital point. "Atemi-waza" are not permitted outside of "kata".
Pedagogy.
"Randori" (free practice).
Judo pedagogy emphasizes randori (乱取り, literally "taking chaos", but meaning "free practice"). This term covers a variety of forms of practice, and the intensity at which it is carried out varies depending on intent and the level of expertise of the participants. At one extreme, is a compliant style of randori, known as "Yakusoku geiko" (約束稽古, prearranged practice), in which neither participant offers resistance to their partner's attempts to throw. A related concept is that of "Sute geiko" (捨稽古, throw-away practice), in which an experienced judoka allows himself to be thrown by his less-experienced partner. At the opposite extreme from "yakusoku geiko" is the hard style of randori that seeks to emulate the style of judo seen in competition. While hard randori is the cornerstone of judo, over-emphasis of the competitive aspect is seen as undesirable by traditionalists if the intent of the randori is to "win" rather than to learn.
Randori is usually limited to either "tachi waza" (standing techniques) or "ne waza" (ground work) and, when one partner is thrown in "tachi waza" randori, practice is resumed with both partners on their feet.
Kata (forms).
Kata (形, kata, Forms) are pre-arranged patterns of techniques and in judo, with the exception of the "Seiryoku-Zen'yō Kokumin-Taiiku", they are all practised with a partner. Their purposes include illustrating the basic principles of judo, demonstrating the correct execution of a technique, teaching the philosophical tenets upon which judo is based, allowing for the practice of techniques that are not allowed in randori, and to preserve ancient techniques that are historically important but are no longer used in contemporary judo.
There are ten kata that are recognized by the Kodokan today:
In addition, there are a number of commonly practiced kata that are not recognised by the Kodokan. Some of the more common kata include:
Competitive judo.
History of competitive judo.
Contest (試合, shiai or jiai with rendaku) is a vitally important aspect of judo. Early examples include the Kodokan Monthly Tournament (月次試合, Tsukinami jiai) and the biannual Red and White Tournament (紅白試合, Kohaku jiai), both of which started in 1884 and continue to the present day.
In 1899, Kano was asked to chair a committee of the Dai Nippon Butoku Kai to draw up the first formal set of contest rules for jujutsu. These rules were intended to cover contests between different various traditional schools of jujutsu as well as practitioners of Kodokan judo. Contests were 15 minutes long and were judged on the basis of nage waza and katame waza, excluding atemi waza. Wins were by two ippons, awarded in every four-main different path of winning alternatives, by "Throwing", where the opponent's back strikes flat onto the mat with sufficient force, by "Pinning" them on their back for a "sufficient" amount of time, or by Submission, which could be achieved via "Shime-waza" or "Kansetsu-waza", in which the opponent was forced to give himself or herself up or summon a referee's or corner-judge's stoppage. Finger, toe and ankle locks were prohibited. In 1900, these rules were adopted by the Kodokan with amendments made to prohibit all joint locks for kyu grades and added wrist locks to the prohibited kansetsu-waza for dan grades. It was also stated that the ratio of tachi-waza to ne-waza should be between 70% to 80% for kyu grades and 60% to 70% for dan grades.
In 1916, additional rulings were brought in to further limit "kansetsu waza" with the prohibition of "ashi garami" and neck locks, as well as "do jime". These were further added to in 1925, in response to Kosen judo (高專柔道, Kōsen jūdō), which concentrated on "ne waza" at the expense of "tachi waza". The new rules banned all remaining joint locks except those applied to the elbow and prohibited the dragging down of an opponent to enter "ne waza".
The All-Japan Judo Championships (全日本柔道選手権大会, Zennihon jūdō senshuken taikai) were first held in 1930 and have been held every year, with the exception of the wartime period between 1941 and 1948, and continue to be the highest profile tournament in Japan.
Judo's international profile was boosted by the introduction of the World Judo Championships in 1956. The championships were initially a fairly small affair, with 31 athletes attending from 21 countries in the first year. Competitors were exclusively male until the introduction of the Women's Championships in 1980, which took place on alternate years to the Men's Championships. The championships were combined in 1987 to create an event that takes place annually, except for the years in which Olympic games are held. Participation has steadily increased such that, in the most recent championships in 2011, 871 competitors from 132 countries took part.
The first time judo was seen in the Olympic Games was in an informal demonstration hosted by Kano at the 1932 Games. However, Kano was ambivalent about judo's potential inclusion as an Olympic sport:
 I have been asked by people of various sections as to the wisdom and possibility of judo being introduced with other games and sports at the Olympic Games. My view on the matter, at present, is rather passive. If it be the desire of other member countries, I have no objection. But I do not feel inclined to take any initiative. For one thing, judo in reality is not a mere sport or game. I regard it as a principle of life, art and science. In fact, it is a means for personal cultural attainment. Only one of the forms of judo training, so-called randori or free practice can be classed as a form of sport. Certainly, to some extent, the same may be said of boxing and fencing, but today they are practiced and conducted as sports. Then the Olympic Games are so strongly flavored with nationalism that it is possible to be influenced by it and to develop "Contest Judo", a retrograde form as ju-jitsu was before the Kodokan was founded. Judo should be free as art and science from any external influences, political, national, racial, and financial or any other organized interest. And all things connected with it should be directed to its ultimate object, the "Benefit of Humanity". Human sacrifice is a matter of ancient history.
Nevertheless, judo became an Olympic sport for men in the 1964 Games in Tokyo. The Olympic Committee initially dropped judo for the 1968 Olympics, meeting protests. Dutchman Anton Geesink won the first Olympic gold medal in the open division of judo by defeating Akio Kaminaga of Japan. The women's event was introduced at the Olympics in 1988 as a demonstration event, and an official medal event in 1992. Paralympic judo has been a Paralympic sport (for the visually impaired) since 1988; it is also one of the sports at the Special Olympics.
Current international contest rules.
The traditional rules of judo are intended to provide a basis under which to test skill in judo, while avoiding significant risk of injury to the competitors. Additionally, the rules are also intended to enforce proper "reigi" (礼儀, etiquette).
Penalties may be given for: passivity or preventing progress in the match; for safety infringements for example by using prohibited techniques, or for behavior that is deemed to be against the spirit of judo. Fighting must be stopped if a participant is outside the designated area on the mat.
Weight divisions.
There are currently seven weight divisions, subject to change by governing bodies, and may be modified based on the age of the competitors:
Competition scoring.
A throw that places the opponent on his back with impetus and control scores an ippon (一本), winning the contest. A lesser throw, where the opponent is thrown onto his back, but with insufficient force to merit an ippon, scores a waza-ari (技あり). Two scores of waza-ari equal an ippon  (技あり合わせて一本, waza-ari awasete ippon). A throw that places the opponent onto his side scores a yuko (有効). No amount of yukos equal a waza-ari, they are only considered in the event of an otherwise tied contest.
Ippon is scored in "ne-waza" for pinning an opponent on his back with a recognised "osaekomi-waza" for 20 seconds or by forcing a submission through "shime-waza" or "kansetsu-waza". A submission is signalled by tapping the mat or the opponent at least twice with the hand or foot, or by saying maitta (まいった, I surrender). A pin lasting for less than 20 seconds, but more than 15 seconds scores waza-ari and one lasting less than 15 seconds but more than 10 seconds scores a yuko.
Formerly, there was an additional score that was lesser to yuko, that of Koka (効果). This has since been removed.
If the scores are identical at the end of the match, the contest is resolved by the "Golden Score" rule. "Golden Score" is a sudden death situation where the clock is reset to match-time, and the first contestant to achieve any score wins. If there is no score during this period, then the winner is decided by Hantei (判定), the majority opinion of the referee and the two corner judges.
There have been changes to the scoring. In January 2013, the Hantei was removed and the "Golden Score" no longer has a time limit. The match would continue until a judoka scored through a technique or if the opponent is penalised (Shido).
Penalties.
Minor rules infractions are penalised with a shido (指導, literally "guidance"). This is treated as a warning and anything up to three shido make no contribution to the overall score. A fourth shido or serious rules violation yields a hansoku make (反則負け, literally "foul-play defeat"), resulting in disqualification of the penalised competitor.
Formerly, there were two additional levels of penalty between shido and hansoku make: chui (注意, literally "caution"), equivalent to a yuko and keikoku (警告, literally "warning") equivalent to waza-ari.
Representation of scores.
Judo scoreboards show the number of "waza-ari" and "yuko" scores scored by each player. (A score of "koka" was also displayed until its use was abandoned in 2009.) Often an "ippon" is not represented on the scoreboard, because upon award of an "ippon" the match is immediately terminated. Some computerized scoreboards will briefly indicate that an "ippon" has been scored.
Scoreboards normally also show the number of penalties imposed on each player, and sometimes the number of medical visits for each. (Only two "medical" attentions are allowed for each competitor during a match—most often for minor bleeds.)
Electronic scoreboards also usually include timers for measuring both competition time and "osaekomi" time.
In mixed martial arts.
Several judo practitioners have made an impact in mixed martial arts. Notable judo trained MMA fighters include Olympic medalists Hidehiko Yoshida (Gold, 1992), Naoya Ogawa (Silver, 1992), Pawel Nastula (Gold, 1996), Makoto Takimoto (Gold, 2000), Satoshi Ishii (Gold, 2008) and Ronda Rousey (Bronze, 2008), former Russian national judo championship Bronze medalist Fedor Emelianenko, Karo Parisyan, Don Frye, Antônio Silva, Oleg Taktarov, Rick Hawn, Hector Lombard, Yoshihiro Akiyama and Dong-Sik Yoon.
Judo has been one of the primary martial arts displayed in Mixed Martial Arts (MMA) competitions since MMA’s inception.
The first official MMA fight, the Ultimate Fighting Championship (UFC), was held in 1993. It was advertised as a “no holds barred” fight. At the time, the public perception was that a larger/stronger human could dominate a smaller/weaker human. The fighters came from various martial arts or sports fighting backgrounds, including judo, karate, jiu-jitsu, boxing, wrestling, tae kwon do and kickboxing. They fought until the opponent was knocked out, tapped out (stopping the fight by tapping their hand), or the referee stopped the fight. The winner of the first Ultimate Fighting Championship was a small Brazilian, Royce Gracie, who used judo/Jujitsu submissions to defeat opponents twice his size
Many MMA fights are ended by submission holds most of which derive from judo. “Chokeholds are common in Jujitsu, judo and submission grappling, and most trainers discuss how fighter need to experience these techniques in order to learn how to resist them”
“Judo resembles MMA's grappling aspects, particularly when both fighters are on the ground. MMA fans would easily recognize judo's submissions -- a fighter might "tap out," or concede the fight, when caught in a chokehold or an armbar”
Brazilian jiu-jitsu is derived from Judo, which was brought to Brazil by Esai Maeda, but with reduced emphasis on throws and increased emphasis on groundwork and submissions compared to modern Judo.
The Japanese found judo superior to Japanese Jujitsu due to proof in competition where it involved both stand up and grappling techniques. After a match-up between older styles of ju-jitsu and judo at the Tokyo police headquarters, judo was named the national martial art in Japan. It was the official art used by law enforcement in the late 1800s, and continues to be popular to this day. During World War II, many U.S. soldiers were exposed to the art of judo and brought it back to America with them. The first issue of Black Belt magazine in America (1961) featured a sketch of a judo throw and was a special judo issue”
Ronda Rousey, a female MMA champion, won a bronze medal in judo at the 2008 Olympics. “Rousey eventually got an offer to try MMA and had her first professional fight in 2011. She has since defeated all but one of her opponents with a judo arm lock and is now a champion in the combat sport”.
Related arts and derivatives.
Kano Jigoro's Kodokan judo is the most popular and well-known style of judo, but is not the only one. The terms judo and jujutsu were quite interchangeable in the early years, so some of these forms of judo are still known as jujutsu or jiu-jitsu either for that reason, or simply to differentiate them from mainstream judo. From Kano's original style of judo, several related forms have evolved—some now widely considered to be distinct arts:
Safety.
Kano's vision for judo was one of a martial way that could be practiced realistically. Randori (free practice) was a central part of judo pedagogy and shiai (competition) a crucial test of a judoka's understanding of judo. Safety necessitated some basic innovations that shaped judo's development. Atemi waza (striking techniques) were entirely limited to kata (prearranged forms) early in judo's history. Kansetsu waza (joint manipulation techniques) were limited to techniques that focused on the elbow joint. Various throwing techniques that were judged to be too dangerous to practice safely were also prohibited in shiai. To maximise safety in nage waza (throwing techniques), judoka trained in ukemi (break falls) and practiced on tatami (rice straw mats).
"Kansetsu" and "shime waza".
The application of joint manipulation and strangulation/choking techniques is generally safe under controlled conditions typical of judo dojo and in competition. It is usual for there to be age restrictions on the practice and application of these types of techniques, but the exact nature of these restrictions will vary from country to country and from organization to organization.
"Nage waza".
Safety in the practice of throwing techniques depends on the skill level of both tori and uke. Inexpertly applied throws have the potential to injure both tori and uke, for instance when tori compensates for poor technique by powering through the throw. Similarly, poor ukemi can result in injury, particularly from more powerful throws that uke lacks the skill to breakfall from. For these reasons, throws are normally taught in order of difficulty for both tori and uke. This is exemplified in the "Gokyo" (五教, literally "five teachings"), a traditional grouping of throws arranged in order of difficulty of ukemi. Those grouped in "Dai ikkyo" (第一教, literally "first teaching") are relatively simple to breakfall from whereas those grouped in "dai gokyo" (第五教, literally "fifth teaching") are difficult to breakfall from.
Judoka (practitioner).
A practitioner of judo is known as a Judoka (柔道家, jūdōka, judo practitioner), though traditionally only those of 4th "dan" or higher were called "Judoka". The suffix "-ka" (家), when added to a noun, means a person with expertise or special knowledge on that subject. Other practitioners below the rank of 4th "dan" used to be called "kenkyu-sei" (研究生, trainees). The modern meaning of "Judoka" in English refers to a judo practitioner of any level of expertise.
A judo teacher is called "sensei" (先生). The word "sensei" comes from "sen" or "saki" (before) and "sei" (life) – i.e. one who has preceded you. In Western dojo, it is common to call any instructor of "dan" grade "sensei". Traditionally, that title was reserved for instructors of 4th "dan" and above.
Judogi (uniform).
Judo practitioners traditionally wear white uniforms called keikogi (稽古着, keikogi) practice clothing or judogi (柔道着, jūdōgi, judo clothing). sometimes abbreviated in the west as "gi". It comprises a heavy cotton kimono-like jacket called an "uwagi" (上衣, jacket), similar to traditional "hanten" (半纏, workers jackets) fastened by an obi (帯, obi, belt), coloured to indicate rank, and cotton draw-string "zubon" (ズボン, trousers). Early examples of keikogi had short sleeves and trouser legs and the modern long-sleeved judogi was adopted in 1906.
The modern use of the blue judogi for high level competition was first suggested by Anton Geesink at the 1986 Maastricht IJF DC Meeting.
For competition, a blue judogi is worn by one of the two competitors for ease of distinction by judges, referees, and spectators. In Japan, both judoka use a white judogi and the traditional red obi (based on the colors of the Japanese flag) is affixed to the belt of one competitor. Outside Japan, a colored obi may also be used for convenience in minor competitions, the blue judogi only being mandatory at the regional or higher levels, depending on organization. Japanese practitioners and traditionalists tend to look down on the use of blue because of the fact that judo is considered a pure sport, and replacing the pure white judogi for the impure blue, is an offense.
For events organized under the auspices of the International judo Federation (IJF), judogi have to bear the IJF Official Logo Mark Label. This label demonstrates that the judogi has passed a number of quality control tests to ensure it conforms to construction regulations ensuring it is not too stiff, flexible, rigid or slippery to allow the opponent to grip or to perform techniques.
Organizations.
The international governing body for judo is the International Judo Federation (IJF), founded in 1951. Members of the IJF include the African Judo Union (AJU), the Pan-American Judo Confederation (PJC), the Judo Union of Asia (JUA), the European Judo Union (EJU) and the Oceania Judo Union (OJU), each comprising a number of national judo associations. The IJF is responsible for organising international competition and hosts the World Judo Championships and is involved in running the Olympic Judo events.
Rank and grading.
Judo is a hierarchical art, where seniority of judoka is designated by what is known as the kyu (級, kyū) -dan (段, dan) ranking system. This system was developed by Jigoro Kano and was based on the ranking system in the board game Go. Beginning students progress through kyu grades towards dan grades.
A judoka's position within the kyu-dan ranking system is displayed by the color of their belt. Beginning students typically wear a white belt, progressing through descending kyu ranks until they are deemed to have achieved a level of competence sufficient to be a dan grade, at which point they wear the kuro obi (黒帯, black belt). The kyu-dan ranking system has since been widely adopted by modern martial arts.
The highest grade ever awarded "jūdan" (tenth degree black belt) has no formal requirements and is decided by the president of the Kodokan, currently Kano Jigoro's grandson Yukimitsu Kano. As of 2011, fifteen Japanese men have been promoted to this rank by the Kodokan, three of whom are still alive; the IJF and Western national federations have promoted another seven who are not recognized (at that level of rank) by the Kodokan. On July 28, 2011, the promotion board of USA Judo awarded Keiko Fukuda the rank of 10th "dan", who is the first woman to be promoted to judo's highest level, albeit not a Kodokan-recognized rank.
Although "dan" ranks tend to be consistent between national organizations there is more variation in the "kyū" grades, with some countries having more "kyū" grades. Although initially "kyū" grade belt colours were uniformly white, today a variety of colours are used. The first black belts to denote a Dan rank in the 1880s, initially the wide obi was used; as practitioners trained in kimono, only white and black obi were used. It was not until the early 1900s, after the introduction of the judogi, that an expanded colored belt system of awarding rank was created.
Research articles.
Body composition and fat percentage of Thai judo athletes * 
Callister R. Physiological characteristics of elite judo athletes * 

</doc>
<doc id="15604" url="http://en.wikipedia.org/wiki?curid=15604" title="James Bond">
James Bond

The James Bond series focuses on a fictional British Secret Service agent created in 1953 by writer Ian Fleming, who featured him in twelve novels and two short-story collections. Since Fleming's death in 1964, seven other authors have written authorised Bond novels or novelizations: Kingsley Amis, Christopher Wood, John Gardner, Raymond Benson, Sebastian Faulks, Jeffery Deaver and William Boyd, with a further instalment due in September 2015 by Anthony Horowitz. Additionally Charlie Higson wrote a series on a young James Bond, and Kate Westbrook wrote three novels based on the diaries of a recurring series character, Moneypenny.
The character has also been adapted for television, radio, comic strip, video games and film. The films are the longest continually running and the third-highest grossing film series to date, which started in 1962 with "Dr. No", starring Sean Connery as Bond. As of 2015, there have been twenty-three films in the Eon Productions series. The most recent Bond film, "Skyfall" (2012), stars Daniel Craig in his third portrayal of Bond; he is the sixth actor to play Bond in the Eon series. There have also been two independent productions of Bond films: "Casino Royale" (a 1967 spoof) and "Never Say Never Again" (a 1983 remake of an earlier Eon-produced film, "Thunderball").
The Bond films are renowned for a number of features, including the musical accompaniment, with the theme songs having received Academy Award nominations on several occasions, and one win. Other important elements which run through most of the films include Bond's cars, his guns, and the gadgets with which he is supplied by Q Branch. The films are also noted for Bond's relationships with various women, who are sometimes referred to as "Bond girls".
Publication history.
Creation and inspiration.
As the central figure for his works, Ian Fleming created the fictional character of James Bond, an intelligence officer in the Secret Intelligence Service, commonly known as MI6. Bond was also known by his code number, 007, and was a Royal Naval Reserve Commander.
Fleming based his fictional creation on a number of individuals he came across during his time in the Naval Intelligence Division during World War II, admitting that Bond "was a compound of all the secret agents and commando types I met during the war". Among those types were his brother, Peter, who had been involved in behind-the-lines operations in Norway and Greece during the war. Aside from Fleming's brother, a number of others also provided some aspects of Bond's make up, including Conrad O'Brien-ffrench, Patrick Dalzel-Job and Bill "Biffy" Dunderdale.
The name James Bond came from that of the American ornithologist James Bond, a Caribbean bird expert and author of the definitive field guide "Birds of the West Indies". Fleming, a keen birdwatcher himself, had a copy of Bond's guide and he later explained to the ornithologist's wife that "It struck me that this brief, unromantic, Anglo-Saxon and yet very masculine name was just what I needed, and so a second James Bond was born". He further explained that:
When I wrote the first one in 1953, I wanted Bond to be an extremely dull, uninteresting man to whom things happened; I wanted him to be a blunt instrument ... when I was casting around for a name for my protagonist I thought by God, (James Bond) is the dullest name I ever heard.—Ian Fleming, "The New Yorker", 21 April 1962
On another occasion, Fleming said: "I wanted the simplest, dullest, plainest-sounding name I could find, 'James Bond' was much better than something more interesting, like 'Peregrine Carruthers'. Exotic things would happen to and around him, but he would be a neutral figure—an anonymous, blunt instrument wielded by a government department."
Fleming decided that Bond should resemble both American singer Hoagy Carmichael and himself and in "Casino Royale", Vesper Lynd remarks, "Bond reminds me rather of Hoagy Carmichael, but there is something cold and ruthless." Likewise, in "Moonraker", Special Branch Officer Gala Brand thinks that Bond is "certainly good-looking ... Rather like Hoagy Carmichael in a way. That black hair falling down over the right eyebrow. Much the same bones. But there was something a bit cruel in the mouth, and the eyes were cold."
Fleming also endowed Bond with many of his own traits, including sharing the same golf handicap, the taste for scrambled eggs and using the same brand of toiletries. Bond's tastes are also often taken from Fleming's own as was his behaviour, with Bond's love of golf and gambling mirroring Fleming's own. Fleming used his experiences of his espionage career and all other aspects of his life as inspiration when writing, including using names of school friends, acquaintances, relatives and lovers throughout his books.
It was not until the penultimate novel, "You Only Live Twice", that Fleming gave Bond a sense of family background. The book was the first to be written after the release of "Dr. No" in cinemas and Sean Connery's depiction of Bond affected Fleming's interpretation of the character, to give Bond both a sense of humour and Scottish antecedents that were not present in the previous stories. In a fictional obituary, purportedly published in "The Times", Bond's parents were given as Andrew Bond, from the village of Glencoe, Scotland, and Monique Delacroix, from the canton of Vaud, Switzerland. Fleming did not provide Bond's date of birth, but John Pearson's fictional biography of Bond, "", gives Bond a birth date on 11 November 1920, while a study by John Griswold puts the date at 11 November 1921.
Novels and related works.
Ian Fleming novels.
Whilst serving in the Naval Intelligence Division, Fleming had planned to become an author and had told a friend, "I am going to write the spy story to end all spy stories." On 17 February 1952, he began writing his first James Bond novel, "Casino Royale" at his Goldeneye estate in Jamaica, where he wrote all his Bond novels during the months of January and February each year. He started the story shortly before his wedding to his pregnant girlfriend, Ann Charteris, in order to distract himself from his forthcoming nuptials.
After completing the manuscript for "Casino Royale", Fleming showed the manuscript to his friend (and later editor) William Plomer to read. Plomer liked it and submitted it to the publishers, Jonathan Cape, who did not like it as much. Cape finally published it in 1953 on the recommendation of Fleming's older brother Peter, an established travel writer. Between 1953 and 1966, two years after his death, twelve novels and two short-story collections were published, with the last two books – "The Man with the Golden Gun" and "Octopussy and The Living Daylights" – published posthumously. All the books were published in the UK through Jonathan Cape.
Post-Fleming novels.
After Fleming's death a continuation novel, "Colonel Sun", was written by Kingsley Amis (as Robert Markham) and published in 1968. Amis had already written a literary study of Fleming's Bond novels in his 1965 work "The James Bond Dossier". Although novelizations of two of the Eon Productions Bond films appeared in print, "James Bond, The Spy Who Loved Me" and "James Bond and Moonraker", both written by screenwriter Christopher Wood, the series of novels did not continue until the 1980s. In 1981, thriller writer John Gardner picked up the series with "Licence Renewed". Gardner went on to write sixteen Bond books in total; two of the books he wrote – "Licence to Kill" and "GoldenEye" – were novelizations of Eon Productions films of the same name. Gardner moved the Bond series into the 1980s, although he retained the ages of the characters as they were when Fleming had left them. In 1996, Gardner retired from writing James Bond books due to ill health.
In 1996 American author Raymond Benson became the author of the Bond novels. Benson had previously been the author of "The James Bond Bedside Companion", first published in 1984.
By the time he moved on to other, non-Bond related projects in 2002, Benson had written six Bond novels, three novelizations and three short stories.
After a gap of six years, Sebastian Faulks was commissioned by Ian Fleming Publications to write a new Bond novel, which was released on 28 May 2008, the 100th anniversary of Ian Fleming's birth. The book—titled "Devil May Care"—was published in the UK by Penguin Books and by Doubleday in the US. American writer Jeffery Deaver was then commissioned by Ian Fleming Publications to produce "Carte Blanche", which was published on 26 May 2011. The book updated Bond into a post-9/11 agent, independent of MI5 or MI6. On 26 September 2013 "Solo", written by William Boyd, was published, set in 1969. In October 2014 it was announced that Anthony Horowitz was to write a further "Bond" book. The book is due to be set in the 1950s, and it contains material written, but previously unreleased, by Fleming; it is due for release in September 2015.
Young Bond.
The "Young Bond" series of novels was started by Charlie Higson and, between 2005 and 2009, five novels and one short story were published. The first Young Bond novel, "SilverFin" was also adapted and released as a graphic novel on 2 October 2008 by Puffin Books. In October 2013 Ian Fleming Publications announced that Stephen Cole would continue the series, with the first edition scheduled to be released in Autumn 2014.
"The Moneypenny Diaries".
"The Moneypenny Diaries" are a trilogy of novels chronicling the life of Miss Moneypenny, M's personal secretary. The novels are penned by Samantha Weinberg under the pseudonym Kate Westbrook, who is depicted as the book's "editor". The first instalment of the trilogy, subtitled ', was released on 10 October 2005 in the UK. A second volume, subtitled ' was released on 2 November 2006 in the UK, published by John Murray. A third volume, subtitled "" was released on 1 May 2008.
Adaptations.
Television.
In 1954 CBS paid Ian Fleming $1,000 ($ in 2015 dollars) to adapt his novel "Casino Royale" into a one-hour television adventure as part of its "Climax!" series. The episode aired live on 21 October 1954 and starred Barry Nelson as "Card Sense" James 'Jimmy' Bond and Peter Lorre as Le Chiffre. The novel was adapted for American audiences to show Bond as an American agent working for "Combined Intelligence", while the character Felix Leiter—American in the novel—became British onscreen and was renamed "Clarence Leiter".
In 1973 a BBC documentary "Omnibus: The British Hero" featured Christopher Cazenove playing a number of such title characters (e.g. Richard Hannay and Bulldog Drummond). The documentary included James Bond in dramatised scenes from
"Goldfinger"—notably featuring 007 being threatened with the novel's circular saw, rather than the film's laser beam—and "Diamonds Are Forever". In 1991 a TV cartoon series "James Bond Jr." was produced with Corey Burton in the role of Bond's nephew, also called James Bond.
Radio.
In 1956, the novel "Moonraker" was adapted for broadcast on South African radio, with Bob Holness providing the voice of Bond. According to "The Independent", "listeners across the Union thrilled to Bob's cultured tones as he defeated evil master criminals in search of world domination".
The BBC have adapted five of the Fleming novels for broadcast: in 1990, "You Only Live Twice" was adapted into a 90-minute radio play for BBC Radio 4 with Michael Jayston playing James Bond. The production was repeated a number of times between 2008 and 2011. On 24 May 2008, BBC Radio 4 broadcast an adaptation of "Dr. No". The actor Toby Stephens, who played Bond villain Gustav Graves in the Eon Productions version of "Die Another Day", played Bond, while Dr. No was played by David Suchet. Following its success, a second story was adapted and on 3 April 2010, BBC Radio 4 broadcast "Goldfinger" with Stephens again playing Bond. Sir Ian McKellen was Goldfinger and Stephens' "Die Another Day" co-star Rosamund Pike played Pussy Galore. The play was adapted from Fleming's novel by Archie Scottney and was directed by Martin Jarvis.
In 2012 the novel "From Russia, with Love" was dramatized for Radio 4; it featured a full cast again starring Stephens as Bond. In May 2014 Stephens again played Bond, in "On Her Majesty's Secret Service", with Alfred Molina as Blofeld, and Joanna Lumley as Irma Bunt.
Comics medium.
In 1957, the "Daily Express" approached Ian Fleming to adapt his stories into comic strips, offering him £1,500 per novel and a share of takings from syndication. After initial reluctance, Fleming, who felt the strips would lack the quality of his writing, agreed. To aid the "Daily Express" in illustrating Bond, Fleming commissioned an artist to create a sketch of how he believed James Bond looked. The illustrator, John McLusky, however, felt that Fleming's 007 looked too "outdated" and "pre-war" and changed Bond to give him a more masculine look. The first strip, "Casino Royale" was published from 7 July 1958 to 13 December 1958 and was written by Anthony Hern and illustrated by John McLusky.
Most of the Bond novels and short stories have since been adapted for illustration, as well as Kingsley Amis's "Colonel Sun"; the works were written by Henry Gammidge or Jim Lawrence with Yaroslav Horak replacing McClusky as artist in 1966. After the Fleming and Amis material had been adapted, original stories were produced, continuing in the "Daily Express" and "Sunday Express" until May 1977.
Several comic book adaptations of the James Bond films have been published through the years: at the time of "Dr. No"'s release in October 1962, a comic book adaptation of the screenplay, written by Norman J. Nodel, was published in Britain as part of the "Classics Illustrated" anthology series. It was later reprinted in the United States by DC Comics as part of its "Showcase" anthology series, in January 1963. This was the first American comic book appearance of James Bond and is noteworthy for being a relatively rare example of a British comic being reprinted in a fairly high-profile American comic. It was also one of the earliest comics to be censored on racial grounds (some skin tones and dialogue were changed for the American market).
With the release of the 1981 film "For Your Eyes Only", Marvel Comics published a two-issue comic book adaptation of the film. When "Octopussy" was released in the cinemas in 1983, Marvel published an accompanying comic; Eclipse also produced a one-off comic for "Licence to Kill", although Timothy Dalton refused to allow his likeness to be used. New Bond stories were also drawn up and published from 1989 onwards through Marvel, Eclipse Comics and Dark Horse Comics.
Films.
The Eon Productions films.
In 1962 Eon Productions, the company of Canadian Harry Saltzman and American Albert R. "Cubby" Broccoli, released the first cinema adaptation of an Ian Fleming novel, "Dr. No", featuring Sean Connery as 007. Connery starred in a further four films before leaving the role after "You Only Live Twice", which was taken up by George Lazenby for "On Her Majesty's Secret Service". Lazenby left the role after just one appearance and Connery was tempted back for his last Eon-produced film "Diamonds Are Forever".
In 1973, Roger Moore was appointed to the role of 007 for "Live and Let Die" and played Bond a further six times over twelve years before being replaced by Timothy Dalton for two films. After a six-year hiatus, during which a legal wrangle threatened Eon's productions of the Bond films, Irish actor Pierce Brosnan was cast as Bond in "GoldenEye", released in 1995; he remained in the role for a total of four films, before leaving in 2002. In 2006, Daniel Craig was given the role of Bond for "Casino Royale", which rebooted the series. The twenty-third Eon produced film, "Skyfall", was released on 26 October 2012. The series has grossed just over $6 billion to date, making it the third-highest-grossing film series (behind "Harry Potter" and the "Marvel Cinematic Universe"), and the single most successful adjusted for inflation.
Non-Eon films.
In 1967, "Casino Royale" was adapted into a parody Bond film starring David Niven as Sir James Bond and Ursula Andress as Vesper Lynd. David Niven had been Ian Fleming's preference for the part of James Bond. The result of a court case in the High Court in London in 1963 allowed Kevin McClory to produce a remake of "Thunderball" titled "Never Say Never Again" in 1983. The film, starring Sean Connery as Bond, was not part of the Eon series of Bond films. In 1997 the Sony Corporation acquired all or some of McClory's rights in an undisclosed deal, which were then subsequently acquired by MGM, whilst on 4 December 1997, MGM announced that the company had purchased the rights to "Never Say Never Again" from Schwartzman's company Taliafilm. Eon now currently (as of 2015) holds the full adaptation rights to all of Fleming's Bond novels.
Music.
The "James Bond Theme" was written by Monty Norman and was first orchestrated by the John Barry Orchestra for 1962's "Dr. No", although the actual authorship of the music has been a matter of controversy for many years. In 2001, Norman won £30,000 in libel damages from the "The Sunday Times" newspaper, which suggested that Barry was entirely responsible for the composition. The theme, as written by Norman and arranged by Barry, was described by another Bond film composer, David Arnold, as "bebop-swing vibe coupled with that vicious, dark, distorted electric guitar, definitely an instrument of rock 'n' roll ... it represented everything about the character you would want: It was cocky, swaggering, confident, dark, dangerous, suggestive, sexy, unstoppable. And he did it in two minutes." Barry composed the scores for eleven Bond films and had an uncredited contribution to "Dr. No" with his arrangement of the Bond Theme.
A Bond film staple are the theme songs heard during their title sequences sung by well-known popular singers. Several of the songs produced for the films have been nominated for Academy Awards for Original Song, including Paul McCartney's "Live and Let Die", Carly Simon's "Nobody Does It Better", Sheena Easton's "For Your Eyes Only" and Adele's "Skyfall". Adele won the award at the 85th Academy Awards. For the non-Eon produced "Casino Royale", Burt Bacharach's score included "The Look of Love", which was nominated for an Academy Award for Best Song.
Video games.
In 1983, the first Bond video game, developed and published by Parker Brothers, was released for the Atari 2600, the Atari 5200, the Atari 800, the Commodore 64 and the ColecoVision. Since then, there have been numerous video games either based on the films or using original storylines. In 1997, the first-person shooter video game "GoldenEye 007" was developed by Rare for the Nintendo 64, based on the 1995 Pierce Brosnan film "GoldenEye". The game received very positive reviews, won the BAFTA Interactive Entertainment Award for UK Developer of the Year in 1998 and sold over eight million copies worldwide, grossing $250 million.
In 1999, Electronic Arts acquired the licence and released "Tomorrow Never Dies" on 16 December 1999. In October 2000, they released "The World Is Not Enough" for the Nintendo 64 followed by "007 Racing" for the PlayStation on 21 November 2000. In 2003, the company released ', which included the likenesses and voices of Pierce Brosnan, Willem Dafoe, Heidi Klum, Judi Dench and John Cleese, amongst others. In November 2005, Electronic Arts released a video game adaptation of ', which involved Sean Connery's image and voice-over for Bond. In 2006 Electronic Arts announced a game based on then-upcoming film "Casino Royale": the game was cancelled because it would not be ready by the film's release in November of that year. With MGM losing revenue from lost licensing fees, the franchise was removed from EA to Activision. Activision subsequently released the "" game on 31 October 2008, based on the film of the same name.
A new version of "GoldenEye 007" featuring Daniel Craig was released exclusively for the Nintendo Wii and a handheld version for the Nintendo DS in November 2010. A year later another new version was released for Xbox 360 and PlayStation 3 under the title "". In October 2012 "007 Legends" was released, which featured one mission from each of the Bond actors of the Eon Productions' series.
Guns, vehicles and gadgets.
Guns.
For the first five novels, Fleming armed Bond with a 6.35mm Beretta 418 until he received a letter from a thirty-one-year-old Bond enthusiast and gun expert, Geoffrey Boothroyd, criticising Fleming's choice of firearm for Bond, calling it "a lady's gun – and not a very nice lady at that!" Boothroyd suggested that Bond should swap his Beretta for a 7.65mm Walther PPK and this exchange of arms made it to "Dr. No". Boothroyd also gave Fleming advice on the Berns-Martin triple draw shoulder holster and a number of the weapons used by SMERSH and other villains. In thanks, Fleming gave the MI6 Armourer in his novels the name Major Boothroyd and, in "Dr. No", M introduces him to Bond as "the greatest small-arms expert in the world". Bond also used a variety of rifles, including the Savage Model 99 in "For Your Eyes Only" and a Winchester .308 target rifle in "The Living Daylights". Other handguns used by Bond in the Fleming books included the Colt Detective Special and a long-barrelled Colt .45 Army Special.
The first Bond film, "Dr. No", saw M ordering Bond to leave his Beretta behind and take up the Walther PPK, which the film Bond used in eighteen films. In "Tomorrow Never Dies" and the two subsequent films, Bond's main weapon was the Walther P99 semi-automatic pistol.
Vehicles.
In the early Bond stories Fleming gave Bond a battleship-grey Bentley 4½ Litre with an Amherst Villiers supercharger. After Bond's car was written off by Hugo Drax in "Moonraker", Fleming gave Bond a Mark II Continental Bentley, which he used in the remaining books of the series. During "Goldfinger", Bond was issued with an Aston Martin DB Mark III with a homing device, which he used to track Goldfinger across France. Bond returned to his Bentley for the subsequent novels.
The Bond of the films has driven a number of cars, including the Aston Martin V8 Vantage, during the 1980s, the V12 Vanquish and DBS during the 2000s, as well as the Lotus Esprit; the BMW Z3, BMW 750iL and the BMW Z8. He has, however, also needed to drive a number of other vehicles, ranging from a Citroën 2CV to a Routemaster Bus, amongst others.
Bond's most famous car is the silver grey Aston Martin DB5, first seen in "Goldfinger"; it later featured in "Thunderball", "GoldenEye", "Tomorrow Never Dies", "Casino Royale" and "Skyfall". The films have used a number of different Aston Martins for filming and publicity, one of which was sold in January 2006 at an auction in the US for $2,090,000 to an unnamed European collector.
Gadgets.
Fleming's novels and early screen adaptations presented minimal equipment such as the booby-trapped attaché case in "From Russia with Love", although this situation changed dramatically with the films. However, the effects of the two Eon-produced Bond films "Dr. No" and "From Russia with Love" had an effect on the novel "The Man with the Golden Gun", through the increased number of devices used in Fleming's final story.
For the film adaptations of Bond, the pre-mission briefing by Q Branch became one of the motifs that ran through the series. "Dr. No" provided no spy-related gadgets, but a Geiger counter was used; industrial designer Andy Davey observed that the first ever onscreen spy-gadget was the attaché case shown in "From Russia with Love", which he described as "a classic 007 product". The gadgets assumed a higher profile in the 1964 film "Goldfinger". The film's success encouraged further espionage equipment from Q Branch to be supplied to Bond, although the increased use of technology led to an accusation that Bond was over-reliant on equipment, particularly in the later films.
If it hadn't been for Q Branch, you'd have been dead long ago!
Q, to Bond, "Licence to Kill"
Davey noted that "Bond's gizmos follow the zeitgeist more closely than any other ... nuance in the films" as they moved from the potential representations of the future in the early films, through to the brand-name obsessions of the later films. It is also noticeable that, although Bond uses a number of pieces of equipment from Q Branch, including the Little Nellie autogyro, a jet pack and the exploding attaché case, the villains are also well-equipped with custom-made devices, including Scaramanga's golden gun, Rosa Klebb's poison-tipped shoes, Oddjob's steel-rimmed bowler hat and Blofeld's communication devices in his agents' vanity case.
Cultural impact.
Cinematically, Bond has been a major influence within the spy genre since the release of "Dr. No" in 1962, with 22 secret agent films released in 1966 alone attempting to capitalise on its popularity and success. The first parody was the 1964 film "Carry On Spying" showing the villain Dr. Crow being overcome by agents who included James Bind (Charles Hawtry) and Daphne Honeybutt (Barbara Windsor). One of the films that reacted against the portrayal of Bond was the Harry Palmer series, whose first film, "The Ipcress File" was released in 1965. The eponymous hero of the series was what academic Jeremy Packer called an "anti-Bond", or what Christoph Lindner calls "the thinking man's Bond". The Palmer series were produced by Harry Saltzman, who also used key crew members from the Bond series, including designer Ken Adam, editor Peter R. Hunt and composer John Barry. The four "Matt Helm" films starring Dean Martin were released between 1966 and 1969, the "Flint" series starring James Coburn provided two films in 1966 and 1969, whilst "The Man from U.N.C.L.E." also moved onto the cinema screen, with eight films released: all were testaments to Bond's prominence in popular culture. More recently, the "Austin Powers" series by writer, producer and comedian Mike Myers and other parodies such as the 2003 film "Johnny English" have also used elements from or parodied the Bond films.
Following the release of the film "Dr. No" in 1962, the line "Bond ... James Bond", became a catch phrase that entered the lexicon of Western popular culture: writers Cork and Scivally said of the introduction in "Dr. No" that the "signature introduction would become the most famous and loved film line ever". In 2001 it was voted as the "best-loved one-liner in cinema" by British cinema goers and in 2005, it was honoured as the 22nd greatest quotation in cinema history by the American Film Institute as part of their 100 Years Series. The 2005 American Film Institute's '100 Years' series also recognised the character of James Bond himself in the film as the third greatest film hero. He was also placed at number eleven on a similar list by "Empire". "Premiere" also listed Bond as the fifth greatest movie character of all time.
The twenty-three James Bond films produced by Eon Productions, which have grossed $4,910,000,000 in box office returns alone, have made the series one of the highest-grossing ever. It is estimated that since "Dr. No", a quarter of the world's population have seen at least one Bond film. The UK Film Distributors' Association have stated that the importance of the Bond series of films to the British film industry cannot be overstated, as they "form the backbone of the industry".
Television also saw the effect of Bond films, with the NBC series "The Man from U.N.C.L.E.", which was described as the "first network television imitation" of Bond, largely because Fleming provided advice and ideas on the development of the series, even giving the main character the name Napoleon Solo. Other 1960s television series inspired by Bond included "I Spy", and "Get Smart".
By 2012, James Bond had become such a symbol of the United Kingdom that the character, played by Craig, appeared in the opening ceremony of the 2012 London Olympics as Queen Elizabeth II's escort.
Throughout the life of the film series, there have been a number of tie-in products released.
Bibliography.
</dl>

</doc>
<doc id="15606" url="http://en.wikipedia.org/wiki?curid=15606" title="Japanese language">
Japanese language

Japanese (日本語, Nihongo, [nihõŋɡo], [nihõŋŋo]) is an East Asian language spoken by about 125 million speakers, primarily in Japan, where it is the national language. It is a member of the Japonic (or Japanese-Ryukyuan) language family, whose relation to other language groups, particularly to Korean and the suggested Altaic language family, is debated.
Little is known of the language's prehistory, or when it first appeared in Japan. Chinese documents from the 3rd century recorded a few Japanese words, but substantial texts did not appear until the 8th century. During the Heian period (794–1185), Chinese had a considerable influence on the vocabulary and phonology of Old Japanese. Late Middle Japanese (1185–1600) saw changes in features that brought it closer to the modern language, as well as the first appearance of European loanwords. The standard dialect moved from the Kansai region to the Edo (modern Tokyo) region in the Early Modern Japanese period (early 17th century–mid-19th century). Following the end in 1853 of Japan's self-imposed isolation, the flow of loanwords from European languages increased significantly. English loanwords in particular have become frequent, and Japanese words from English roots have proliferated.
Japanese is an agglutinative, mora-timed language with simple phonotactics, a pure vowel system, phonemic vowel and consonant length, and a lexically significant pitch-accent. Word order is normally subject–object–verb with particles marking the grammatical function of words, and sentence structure is topic–comment. Sentence-final particles are used to add emotional or emphatic impact, or make questions. Nouns have no grammatical number or gender, and there are no articles. Verbs are conjugated, primarily for tense and voice, but not person. Japanese equivalents of adjectives are also conjugated. Japanese has a complex system of honorifics with verb forms and vocabulary to indicate the relative status of the speaker, the listener, and persons mentioned.
Japanese has no genetic relationship with Chinese, but it makes extensive use of Chinese characters, or "kanji" (漢字), in its writing system, and a large portion of its vocabulary is borrowed from Chinese. Along with "kanji", the Japanese writing system primarily uses two syllabic (or moraic) scripts, "hiragana" (ひらがな or 平仮名) and "katakana" (カタカナ or 片仮名). Latin script is used in a limited fashion, such as for imported acronyms, and the numeral system uses mostly Arabic numerals alongside traditional Chinese numerals.
History.
Prehistory.
A common ancestor of Japanese and Ryukyuan languages or dialects is thought to have been brought to Japan by settlers coming from either continental Asia or nearby Pacific islands (or both) sometime in the early- to mid-2nd century BC (the Yayoi period), replacing the languages of the original Jōmon inhabitants, including the ancestor of the modern Ainu language. Very little is known about the Japanese of this period – because writing had yet to be introduced from China, there is no direct evidence; so anything that can be discerned about this period of Japanese must be based on the reconstructions of Old Japanese.
Old Japanese.
Old Japanese is the oldest attested stage of the Japanese language. Through the spread of Buddhism, the Chinese writing system was imported to Japan. The earliest texts found in Japan are written in Classical Chinese, but they may have been meant to be read as Japanese by the kanbun method. Some of these Chinese texts show the influences of Japanese grammar, such as the word order (for example, placing the verb after the object). In these "hybrid" texts, Chinese characters are also occasionally used phonetically to represent Japanese particles. The earliest text, the "Kojiki", dates to the early 8th century, and was written entirely in Chinese characters. The end of Old Japanese coincides with the end of the Nara period in 794. Old Japanese uses the Man'yōgana system of writing, which uses "kanji" for their phonetic as well as semantic values. Based on the Man'yōgana system, Old Japanese can be reconstructed as having 88 distinct syllables. Texts written with Man'yōgana use two different "kanji" for each of the syllables now pronounced き ki, ひ hi, み mi, け ke, へ he, め me, こ ko, そ so, と to, の no, も mo, よ yo and ろ ro. (The "Kojiki" has 88, but all later texts have 87. The distinction between mo1 and mo2 apparently was lost immediately following its composition.) This set of syllables shrank to 67 in Early Middle Japanese, though some were added through Chinese influence.
Due to these extra syllables, it has been hypothesized that Old Japanese's vowel system was larger than that of Modern Japanese – it perhaps contained up to eight vowels. According to Shinkichi Hashimoto, the extra syllables in Man'yōgana derive from differences between the vowels of the syllables in question. These differences would indicate that Old Japanese had an eight-vowel system, in contrast to the five vowels of later Japanese. The vowel system would have to have shrunk some time between these texts and the invention of the "kana" ("hiragana" and "katakana") in the early 9th century. According to this view, the eight-vowel system of ancient Japanese would resemble that of the Uralic and Altaic language families. However, it is not fully certain that the alternation between syllables necessarily reflects a difference in the vowels rather than the consonants – at the moment, the only undisputed fact is that they are different syllables.
Old Japanese does not have /h/, but rather /ɸ/ (preserved in modern "fu", /ɸɯ/), which has been reconstructed to an earlier */p/. Man'yōgana also has a symbol for /je/, which merges with /e/ before the end of the period.
Several fossilizations of Old Japanese grammatical elements remain in the modern language – the genitive particle "tsu" (superseded by modern "no") is preserved in words such as "matsuge" ("eyelash", lit. "hair of the eye"); modern "mieru" ("to be visible") and "kikoeru" ("to be audible") retain what may have been a mediopassive suffix -"yu(ru)" ("kikoyu" → "kikoyuru" (the attributive form, which slowly replaced the plain form starting in the late Heian period) > "kikoeru" (as all shimo-nidan verbs in modern Japanese did)); and the genitive particle "ga" remains in intentionally archaic speech.
Early Middle Japanese.
Early Middle Japanese is the Japanese of the Heian period, from 794 to 1185. Early Middle Japanese sees a significant amount of Chinese influence on the language's phonology – length distinctions become phonemic for both consonants and vowels, and series of both labialised (e.g. "kwa") and palatalised ("kya") consonants are added. Intervocalic /ɸ/ merges with /w/ by the 11th century.
The end of Early Middle Japanese sees the beginning of a shift where the attributive form (Japanese "rentaikei") slowly replaces the uninflected form ("shūshikei") for those verb classes where the two were distinct.
Late Middle Japanese.
Late Middle Japanese covers the years from 1185 to 1600, and is normally divided into two sections, roughly equivalent to the Kamakura period and the Muromachi period, respectively. The later forms of Late Middle Japanese are the first to be described by non-native sources, in this case the Jesuit and Franciscan missionaries; and thus there is better documentation of Late Middle Japanese phonology than for previous forms (for instance, the Arte da Lingoa de Iapam). Among other sound changes, the sequence /au/ merges to /ɔː/, in contrast with /oː/; /p/ is reintroduced from Chinese; and /we/ merges with /je/. Some forms rather more familiar to Modern Japanese speakers begin to appear – the continuative ending -"te" begins to reduce onto the verb (e.g. "yonde" for earlier "yomite"), the -k- in the final syllable of adjectives drops out ("shiroi" for earlier "shiroki"); and some forms exist where modern standard Japanese has retained the earlier form (e.g. "hayaku" > "hayau" > "hayɔɔ", where modern Japanese just has "hayaku", though the alternative form is preserved in the standard greeting "o-hayō gozaimasu" "good morning"; this ending is also seen in "o-medetō" "congratulations", from "medetaku").
Late Middle Japanese has the first loanwords from European languages – now-common words borrowed into Japanese in this period include "pan" ("bread") and "tabako" ("tobacco", now "cigarette"), both from Portuguese.
Modern Japanese.
Modern Japanese is considered to begin with the Edo period in 1600. Since Old Japanese, the de facto standard Japanese had been the Kansai dialect, especially that of Kyoto. However, during the Edo period, Edo (now Tokyo) developed into the largest city in Japan, and the Edo-area dialect became standard Japanese. Since the end of Japan's self-imposed isolation in 1853, the flow of loanwords from European languages has increased significantly. The period since 1945 has seen a large number of words borrowed from English, especially relating to technology—for example, "pasokon" (short for "personal computer"); "intānetto" ("internet"), and "kamera" ("camera"). Due to the large quantity of English loanwords, modern Japanese has developed a distinction between /tɕi/ and /ti/, and /dʑi/ and /di/, with the latter in each pair only found in loanwords.
Geographic distribution.
Although Japanese is spoken almost exclusively in Japan, it has been spoken outside. Before and during World War II, through Japanese annexation of Taiwan and Korea, as well as partial occupation of China, the Philippines, and various Pacific islands, locals in those countries learned Japanese as the language of the empire. As a result, many elderly people in these countries can still speak Japanese.
Japanese emigrant communities (the largest of which are to be found in Brazil, with 1.4 million to 1.5 million Japanese immigrants and descendants, according to Brazilian IBGE data, more than the 1.2 million of the United States) sometimes employ Japanese as their primary language. Approximately 12% of Hawaii residents speak Japanese, with an estimated 12.6% of the population of Japanese ancestry in 2008. Japanese emigrants can also be found in Peru, Argentina, Australia (especially in the eastern states), Canada (especially in Vancouver where 1.4% of the population has Japanese ancestry), the United States (notably California, where 1.2% of the population has Japanese ancestry, and Hawaii), and the Philippines (particularly in Davao and Laguna).
Official status.
Japanese has no official status, but is the "de facto" national language. There is a form of the language considered standard: "hyōjungo" (標準語), meaning "standard Japanese", or "kyōtsūgo" (共通語), "common language". The meaning of the two terms are almost the same. "Hyōjungo" or "kyōtsūgo" is a conception that forms the counterpart of dialect. This normative language was born after the Meiji Restoration (明治維新, meiji ishin, 1868) from the language spoken in the higher-class areas of Tokyo (see Yamanote) for communicating necessity. "Hyōjungo" is taught in schools and used on television and even in official communications. It is the version of Japanese discussed in this article.
Formerly, standard Japanese in writing (文語, bungo, "literary language") was different from colloquial language (口語, kōgo). The two systems have different rules of grammar and some variance in vocabulary. "Bungo" was the main method of writing Japanese until about 1900; since then "kōgo" gradually extended its influence and the two methods were both used in writing until the 1940s. "Bungo" still has some relevance for historians, literary scholars, and lawyers (many Japanese laws that survived World War II are still written in "bungo", although there are ongoing efforts to modernize their language). "Kōgo" is the dominant method of both speaking and writing Japanese today, although "bungo" grammar and vocabulary are occasionally used in modern Japanese for effect.
Dialects.
Dozens of dialects are spoken in Japan. The profusion is due to many factors, including the length of time the archipelago has been inhabited, its mountainous island terrain, and Japan's long history of both external and internal isolation. Dialects typically differ in terms of pitch accent, inflectional morphology, vocabulary, and particle usage. Some even differ in vowel and consonant inventories, although this is uncommon.
The main distinction in Japanese accents is between Tokyo-type (東京式, Tōkyō-shiki) and Kyoto-Osaka-type (京阪式, Keihan-shiki). Within each type are several subdivisions. Kyoto-Osaka-type dialects are in the central region, roughly formed by Kansai, Shikoku, and western Hokuriku regions.
Dialects from peripheral regions, such as Tōhoku or Kagoshima, may be unintelligible to speakers from the other parts of the country. There are some language islands in mountain villages or isolated islands such as Hachijō-jima island whose dialect are descended from the Eastern dialect of Old Japanese. Dialects of the Kansai region are spoken or known by many Japanese, and Osaka dialect in particular is associated with comedy (see Kansai dialect). Dialects of Tōhoku and North Kantō are associated with typical farmers.
The Ryūkyūan languages, spoken in Okinawa and the Amami Islands (politically part of Kagoshima), are distinct enough to be considered a separate branch of the Japonic family; not only is each language unintelligible to Japanese speakers, but most are unintelligible to those who speak other Ryūkyūan languages. However, in contrast to linguists, many ordinary Japanese people tend to consider the Ryūkyūan languages as dialects of Japanese. This is the result of the official language policy of the Japanese government, which has declared those languages to be dialects and prohibited their use in schools.
Standard Japanese has become prevalent nationwide (including the Ryūkyū islands) due to education, mass media, and an increase of mobility within Japan, as well as economic integration.
Classification.
Japanese is a member of the Japonic languages family, which also includes the languages spoken throughout the Ryūkyū Islands. As these closely related languages are commonly treated as dialects of the same language, Japanese is often called a language isolate.
According to Martine Irma Robbeets, Japanese has been subject to more attempts to show its relation to other languages than any other language in the world. Since Japanese first gained the consideration of linguists in the late 19th century, attempts have been made to show its genealogical relation to languages or language families such as Ainu, Korean, Chinese, Tibeto-Burman, Ural-Altaic, Altaic, Uralic, Mon–Khmer, Malayo-Polynesian and Ryukyuan. At the fringe, some linguists have suggested a link to Indo-European languages, including Greek, and to Lepcha. As it stands, only the link to Ryukyuan has wide support, though linguist Kurakichi Shiratori maintained that Japanese was a language isolate.
Korean hypothesis.
Similarities between Korean and Japanese were noted by Arai Hakuseki in 1717, and the idea that the two might be related was first proposed in 1781 by Japanese scholar Teikan Fujii. The idea received little attention until William George Aston proposed it again in 1879. Japanese scholar Shōsaburō Kanazawa took it up in 1910, as did Shinpei Ogura in 1934. Shirō Hattori was nearly alone when he criticised these theories in 1959. Samuel Martin furthered the idea in 1966 with his "Lexical evidence relating Korean to Japanese", as did John Whitman with his dissertation on the subject in 1985. Despite this, definitive proof of the relation has yet to be provided. Historical linguists studying Japanese and Korean tend to accept the genealogical relation, while general linguists and historical linguists in Japan and Korea have remained skeptical. Alexander Vovin suggests that, while typologically modern Korean and Japanese share similarities that sometimes allow word-to-word translations, studies of the pre-modern languages show greater differences. According to Vovin, this suggests linguistic convergence rather than divergence, which he believes is amongst the evidence of the languages not having a genealogical connection.
Altaic hypothesis.
The proposed Altaic family of languages, which would include languages from far eastern Europe to northeastern Asia, has had its supporters and detractors over its history. The most controversial aspect of the hypothesis is the proposed inclusion of Korean and Japanese, which even some proponents of Altaic have rejected. Philipp Franz von Siebold suggested the connection in 1832, but the inclusion first attracted significant attention in the early 1970s. Roy Andrew Miller published "Japanese and the Other Altaic Languages", and dedicated much of his later career to the subject. Sergei Starostin published a 1991 monograph which was another significant stepping stone in Japanese—Altaic research. A team of scholars made a database of Altaic etymologies available over the internet, from which the three-volume "Etymological Dictionary of the Altaic Languages" was published in 2003. Scholars such as Yevgeny Polivanov and Yoshizo Itabashi, on the other hand, have proposed a hybrid origin of Japanese, in which Austronesian and Altaic elements became mixed.
Skepticism over the Japanese relation to Altaic is widespread amongst both amateurs and professionals, in part because of the large number of unsuccessful attempts to establish genealogical relationships with Japanese and other languages. Opinions are polarized, with many strongly convinced of the Altaic relation, and others strongly convinced of the lack of one. While some sources are undecided, often strong proponents of either view will not even acknowledge the claims of the other side.
Phonology.
All Japanese vowels are pure—that is, there are no diphthongs, only monophthongs. The only unusual vowel is the high back vowel /ɯ/   , which is like /u/, but compressed instead of rounded. Japanese has five vowels, and vowel length is phonemic, with each having both a short and a long version. Elongated vowels are usually denoted with a line over the vowel (a macron) in rōmaji, a repeated vowel character in hiragana, or a chōonpu succeeding the vowel in katakana.
Some Japanese consonants have several allophones, which may give the impression of a larger inventory of sounds. However, some of these allophones have since become phonemic. For example, in the Japanese language up to and including the first half of the 20th century, the phonemic sequence /ti/ was palatalized and realized phonetically as [tɕi], approximately "chi"   ; however, now /ti/ and /tɕi/ are distinct, as evidenced by words like "tī" [tiː] "Western style tea" and "chii" [tɕii] "social status".
The "r" of the Japanese language (technically a lateral apical postalveolar flap), is of particular interest, sounding to most English speakers to be something between an "l" and a retroflex "r" depending on its position in a word. The "g" is also notable; unless it starts a sentence, it is pronounced /ŋ/, like the "ng" in "sing," in the Kanto prestige dialect and in other eastern dialects.
The syllabic structure and the phonotactics are very simple: the only consonant clusters allowed within a syllable consist of one of a subset of the consonants plus /j/. This type of cluster only occurs in onsets. However, consonant clusters across syllables are allowed as long as the two consonants are a nasal followed by a homorganic consonant. Consonant length (gemination) is also phonemic.
The phonology of Japanese also includes a pitch accent system.
Grammar.
Sentence structure.
Japanese word order is classified as subject–object–verb. Unlike many Indo-European languages, the only strict rule of word order is that the verb must be placed at the end of a sentence (possibly followed by sentence-end particles). This is because Japanese sentence elements are marked with particles that identify their grammatical functions.
The basic sentence structure is topic–comment. For example, "Kochira wa Tanaka-san desu" (こちらは田中さんです). "kochira" ("this") is the topic of the sentence, indicated by the particle " wa". The verb "de aru" ("desu" is a contraction of its polite form "de arimasu") is a copula, commonly translated as "to be" or "it is" (though there are other verbs that can be translated as "to be"), though technically it holds no meaning and is used to give a sentence 'politeness'. As a phrase, "Tanaka-san desu" is the comment. This sentence literally translates to "As for this person, (it) is Mr./Ms. Tanaka." Thus Japanese, like many other Asian languages, is often called a topic-prominent language, which means it has a strong tendency to indicate the topic separately from the subject, and that the two do not always coincide. The sentence "Zō wa hana ga nagai " (象は鼻が長い) literally means, "As for elephant(s), (the) nose(s) (is/are) long". The topic is "zō" "elephant", and the subject is "hana" "nose".
In Japanese, the subject or object of a sentence need not be stated if it is obvious from context. As a result of this grammatical permissiveness, there is a tendency to gravitate towards brevity; Japanese speakers tend to omit pronouns on the theory they are inferred from the previous sentence, and are therefore understood. In the context of the above example, "hana-ga nagai" would mean "[their] noses are long," while "nagai" by itself would mean "[they] are long." A single verb can be a complete sentence: "Yatta!" (やった!)"[I / we / they / etc] did [it]!". In addition, since adjectives can form the predicate in a Japanese sentence (below), a single adjective can be a complete sentence: "Urayamashii!" (羨ましい!)"[I'm] jealous [of it]!".
While the language has some words that are typically translated as pronouns, these are not used as frequently as pronouns in some Indo-European languages, and function differently. In some cases Japanese relies on special verb forms and auxiliary verbs to indicate the direction of benefit of an action: "down" to indicate the out-group gives a benefit to the in-group; and "up" to indicate the in-group gives a benefit to the out-group. Here, the in-group includes the speaker and the out-group does not, and their boundary depends on context. For example, "oshiete moratta" (教えてもらった) (literally, "explained" with a benefit from the out-group to the in-group) means "[he/she/they] explained [it] to [me/us]". Similarly, "oshiete ageta" (教えてあげた) (literally, "explained" with a benefit from the in-group to the out-group) means "[I/we] explained [it] to [him/her/them]". Such beneficiary auxiliary verbs thus serve a function comparable to that of pronouns and prepositions in Indo-European languages to indicate the actor and the recipient of an action.
Japanese "pronouns" also function differently from most modern Indo-European pronouns (and more like nouns) in that they can take modifiers as any other noun may. For instance, one does not say in English:
But one "can" grammatically say essentially the same thing in Japanese:
This is partly because these words evolved from regular nouns, such as "kimi" "you" (君 "lord"), "anata" "you" (あなた "that side, yonder"), and "boku" "I" (僕 "servant"). This is why some linguists do not classify Japanese "pronouns" as pronouns, but rather as referential nouns, much like Spanish "usted" (contracted from "vuestra merced", "your [(flattering majestic) plural] grace") or Portuguese "o senhor". Japanese personal pronouns are generally used only in situations requiring special emphasis as to who is doing what to whom.
The choice of words used as pronouns is correlated with the sex of the speaker and the social situation in which they are spoken: men and women alike in a formal situation generally refer to themselves as "watashi" (私 "private") or "watakushi" (also 私), while men in rougher or intimate conversation are much more likely to use the word "ore" (俺 "oneself", "myself") or "boku". Similarly, different words such as "anata", "kimi", and "omae" (お前, more formally 御前 "the one before me") may be used to refer to a listener depending on the listener's relative social position and the degree of familiarity between the speaker and the listener. When used in different social relationships, the same word may have positive (intimate or respectful) or negative (distant or disrespectful) connotations.
Japanese often use titles of the person referred to where pronouns would be used in English. For example, when speaking to one's teacher, it is appropriate to use "sensei" (先生, teacher), but inappropriate to use "anata". This is because "anata" is used to refer to people of equal or lower status, and one's teacher has higher status.
Inflection and conjugation.
Japanese nouns have no grammatical number, gender or article aspect. The noun "hon" (本) may refer to a single book or several books; "hito" (人) can mean "person" or "people"; and "ki" (木) can be "tree" or "trees". Where number is important, it can be indicated by providing a quantity (often with a counter word) or (rarely) by adding a suffix, or sometimes by duplication (e.g. 人人, "hitobito", usually written with an iteration mark as 人々). Words for people are usually understood as singular. Thus "Tanaka-san" usually means "Mr./Ms. Tanaka". Words that refer to people and animals can be made to indicate a group of individuals through the addition of a collective suffix (a noun suffix that indicates a group), such as "-tachi", but this is not a true plural: the meaning is closer to the English phrase "and company". A group described as "Tanaka-san-tachi" may include people not named Tanaka. Some Japanese nouns are effectively plural, such as "hitobito" "people" and "wareware" "we/us", while the word "tomodachi" "friend" is considered singular, although plural in form.
Verbs are conjugated to show tenses, of which there are two: past and present (or non-past) which is used for the present and the future. For verbs that represent an ongoing process, the "-te iru" form indicates a continuous (or progressive) aspect, similar to the suffix "ing" in English. For others that represent a change of state, the "-te iru" form indicates a perfect aspect. For example, "kite iru" means "He has come (and is still here)", but "tabete iru" means "He is eating".
Questions (both with an interrogative pronoun and yes/no questions) have the same structure as affirmative sentences, but with intonation rising at the end. In the formal register, the question particle "-ka" is added. For example, "ii desu" (いいです) "It is OK" becomes "ii desu-ka" (いいですか。) "Is it OK?". In a more informal tone sometimes the particle "-no" (の) is added instead to show a personal interest of the speaker: "Dōshite konai-no?" "Why aren't (you) coming?". Some simple queries are formed simply by mentioning the topic with an interrogative intonation to call for the hearer's attention: "Kore wa?" "(What about) this?"; "O-namae wa?" (お名前は？) "(What's your) name?".
Negatives are formed by inflecting the verb. For example, "Pan o taberu" (パンを食べる。) "I will eat bread" or "I eat bread" becomes "Pan o tabenai" (パンを食べない。) "I will not eat bread" or "I do not eat bread". Plain negative forms are actually "i"-adjectives (see below) and inflect as such, e.g. "Pan o tabenakatta" (パンを食べなかった。) "I did not eat bread".
The so-called "-te" verb form is used for a variety of purposes: either progressive or perfect aspect (see above); combining verbs in a temporal sequence ("Asagohan o tabete sugu dekakeru" "I'll eat breakfast and leave at once"), simple commands, conditional statements and permissions ("Dekakete-mo ii?" "May I go out?"), etc.
The word "da" (plain), "desu" (polite) is the copula verb. It corresponds approximately to the English "be", but often takes on other roles, including a marker for tense, when the verb is conjugated into its past form "datta" (plain), "deshita" (polite). This comes into use because only "i"-adjectives and verbs can carry tense in Japanese. Two additional common verbs are used to indicate existence ("there is") or, in some contexts, property: "aru" (negative "nai") and "iru" (negative "inai"), for inanimate and animate things, respectively. For example, "Neko ga iru" "There's a cat", "Ii kangae-ga nai" "[I] haven't got a good idea".
The verb "to do" ("suru", polite form "shimasu") is often used to make verbs from nouns ("ryōri suru" "to cook", "benkyō suru" "to study", etc.) and has been productive in creating modern slang words. Japanese also has a huge number of compound verbs to express concepts that are described in English using a verb and an adverbial particle (e.g. "tobidasu" "to fly out, to flee," from "tobu" "to fly, to jump" + "dasu" "to put out, to emit").
There are three types of adjective (see Japanese adjectives):
Both "keiyōshi" and "keiyōdōshi" may predicate sentences. For example,
Both inflect, though they do not show the full range of conjugation found in true verbs.
The "rentaishi" in Modern Japanese are few in number, and unlike the other words, are limited to directly modifying nouns. They never predicate sentences. Examples include "ookina" "big", "kono" "this", "iwayuru" "so-called" and "taishita" "amazing".
Both "keiyōdōshi" and "keiyōshi" form adverbs, by following with "ni" in the case of "keiyōdōshi":
and by changing "i" to "ku" in the case of "keiyōshi":
The grammatical function of nouns is indicated by postpositions, also called particles. These include for example:
It is also used for the lative case, indicating a motion to a location.
Note: The subtle difference between wa and ga in Japanese cannot be derived from the English language as such, because the distinction between sentence topic and subject is not made there. While "wa" indicates the topic, which the rest of the sentence describes or acts upon, it carries the implication that the subject indicated by "wa" is not unique, or may be part of a larger group.
Absence of "wa" often means the subject is the focus of the sentence.
Politeness.
Japanese has an extensive grammatical system to express politeness and formality.
The Japanese language can express differing levels in social status. The differences in social position are determined by a variety of factors including job, age, experience, or even psychological state (e.g., a person asking a favour tends to do so politely). The person in the lower position is expected to use a polite form of speech, whereas the other person might use a plainer form. Strangers will also speak to each other politely. Japanese children rarely use polite speech until they are teens, at which point they are expected to begin speaking in a more adult manner. "See uchi-soto".
Whereas "teineigo" (丁寧語) (polite language) is commonly an inflectional system, "sonkeigo" (尊敬語) (respectful language) and "kenjōgo" (謙譲語) (humble language) often employ many special honorific and humble alternate verbs: "iku" "go" becomes "ikimasu" in polite form, but is replaced by "irassharu" in honorific speech and "ukagau" or "mairu" in humble speech.
The difference between honorific and humble speech is particularly pronounced in the Japanese language. Humble language is used to talk about oneself or one's own group (company, family) whilst honorific language is mostly used when describing the interlocutor and their group. For example, the "-san" suffix ("Mr" "Mrs." or "Miss") is an example of honorific language. It is not used to talk about oneself or when talking about someone from one's company to an external person, since the company is the speaker's "group". When speaking directly to one's superior in one's company or when speaking with other employees within one's company about a superior, a Japanese person will use vocabulary and inflections of the honorific register to refer to the in-group superior and their speech and actions. When speaking to a person from another company (i.e., a member of an out-group), however, a Japanese person will use the plain or the humble register to refer to the speech and actions of their own in-group superiors. In short, the register used in Japanese to refer to the person, speech, or actions of any particular individual varies depending on the relationship (either in-group or out-group) between the speaker and listener, as well as depending on the relative status of the speaker, listener, and third-person referents.
Most nouns in the Japanese language may be made polite by the addition of "o-" or "go-" as a prefix. "o-" is generally used for words of native Japanese origin, whereas "go-" is affixed to words of Chinese derivation. In some cases, the prefix has become a fixed part of the word, and is included even in regular speech, such as "gohan" 'cooked rice; meal.' Such a construction often indicates deference to either the item's owner or to the object itself. For example, the word "tomodachi" 'friend,' would become "o-tomodachi" when referring to the friend of someone of higher status (though mothers often use this form to refer to their children's friends). On the other hand, a polite speaker may sometimes refer to "mizu" 'water' as "o-mizu" in order to show politeness.
Most Japanese people employ politeness to indicate a lack of familiarity. That is, they use polite forms for new acquaintances, but if a relationship becomes more intimate, they no longer use them. This occurs regardless of age, social class, or gender.
Vocabulary.
The original language of Japan, or at least the original language of a certain population that was ancestral to a significant portion of the historical and present Japanese nation, was the so-called "yamato kotoba" (大和言葉 or infrequently 大和詞, i.e. "Yamato words"), which in scholarly contexts is sometimes referred to as "wago" (和語 or rarely 倭語, i.e. the "Wa words"). In addition to words from this original language, present-day Japanese includes a number of words that were either borrowed from Chinese or constructed from Chinese roots following Chinese patterns. These words, known as "kango" (漢語), entered the language from the 5th century onwards via contact with Chinese culture. According to the "Shinsen Kokugo Jiten" (新選国語辞典) Japanese dictionary, "kango" comprise 49.1% of the total vocabulary, "wago" make up 33.8%, other foreign words or "gairaigo" (外来語) account for 8.8%, and the remaining 8.3% constitute hybridized words or "konshugo" (混種語) that draw elements from more than one language.
There are also a great number of words of mimetic origin in Japanese, with Japanese having a rich collection of sound symbolism, both onomatopoeia for physical sounds, and more abstract words. A small number of words have come into Japanese from the Ainu language. "Tonakai" (reindeer), "rakko" (sea otter) and "shishamo" (smelt, a type of fish) are well-known examples of words of Ainu origin.
Words of different origins occupy different registers in Japanese. Like Latin-derived words in English, "kango" words are typically perceived as somewhat formal or academic compared to equivalent Yamato words. Indeed, it is generally fair to say that an English word derived from Latin/French roots typically corresponds to a Sino-Japanese word in Japanese, whereas a simpler Anglo-Saxon word would best be translated by a Yamato equivalent.
Incorporating vocabulary from European languages began with borrowings from Portuguese in the 16th century, followed by words from Dutch during Japan's long isolation of the Edo period. With the Meiji Restoration and the reopening of Japan in the 19th century, borrowing occurred from German, French, and English. Today most borrowings are from English.
In the Meiji era, the Japanese also coined many neologisms using Chinese roots and morphology to translate European concepts; these are known as wasei kango (Japanese-made Chinese words). Many of these were then imported into Chinese, Korean, and Vietnamese via their kanji in the late 19th and early 20th centuries. For example, "seiji" 政治 ("politics"), and "kagaku" 化学 ("chemistry") are words derived from Chinese roots that were first created and used by the Japanese, and only later borrowed into Chinese and other East Asian languages. As a result, Japanese, Chinese, Korean, and Vietnamese share a large common corpus of vocabulary in the same way a large number of Greek- and Latin-derived words – both inherited or borrowed into European languages, or modern coinages from Greek or Latin roots – are shared among modern European languages – see classical compound.
In the past few decades, "wasei-eigo" ("made-in-Japan English") has become a prominent phenomenon. Words such as "wanpatān" ワンパターン (< "one" + "pattern", "to be in a rut", "to have a one-track mind") and "sukinshippu" スキンシップ (< "skin" + "-ship", "physical contact"), although coined by compounding English roots, are nonsensical in most non-Japanese contexts; exceptions exist in nearby languages such as Korean however, which often use words such as "skinship" and "rimokon" (remote control) in the same way as in Japanese.
The popularity of many Japanese cultural exports has made some native Japanese words familiar in English, including "futon, haiku, judo, kamikaze, karaoke, karate, ninja, origami, rickshaw" (from 人力車 "jinrikisha"), "samurai, sayonara, sudoku, sumo, sushi, tsunami, tycoon". See list of English words of Japanese origin for more.
Writing system.
Literacy was introduced to Japan in the form of the Chinese writing system, by way of Baekje before the 5th century. Using this language, the Japanese king Bu presented a petition to Emperor Shun of Liu Song in AD 478. After the ruin of Baekje, Japan invited scholars from China to learn more of the Chinese writing system. Japanese emperors gave an official rank to Chinese scholars (続守言/薩弘格/ 袁晋卿) and spread the use of Chinese characters from the 7th century to the 8th century.
At first, the Japanese wrote in Classical Chinese, with Japanese names represented by characters used for their meanings and not their sounds. Later, during the 7th century AD, the Chinese-sounding phoneme principle was used to write pure Japanese poetry and prose, but some Japanese words were still written with characters for their meaning and not the original Chinese sound. This is when the history of Japanese as a written language begins in its own right. By this time, the Japanese language was already very distinct from the Ryukyuan languages.
An example of this mixed style is the Kojiki, which was written in AD 712. They then started to use Chinese characters to write Japanese in a style known as "man'yōgana", a syllabic script which used Chinese characters for their sounds in order to transcribe the words of Japanese speech syllable by syllable.
Over time, a writing system evolved. Chinese characters (kanji) were used to write either words borrowed from Chinese, or Japanese words with the same or similar meanings. Chinese characters were also used to write grammatical elements, were simplified, and eventually became two syllabic scripts: hiragana and katakana which were developed based on Manyogana from Baekje. However this hypothesis "Manyogana from Baekje" is denied by other scholars.
Modern Japanese is written in a mixture of three main systems: kanji, characters of Chinese origin used to represent both Chinese loanwords into Japanese and a number of native Japanese morphemes; and two syllabaries: hiragana and katakana. The Latin script (or romaji in Japanese) is used to a certain extent, such as for imported acronyms and to transcribe Japanese names and in other instances where non-Japanese speakers need to know how to pronounce a word (such as "ramen" at a restaurant). Arabic numerals are much more common than the kanji when used in counting, but kanji numerals are still used in compounds, such as 統一 "tōitsu" ("unification").
"Hiragana" are used for words without kanji representation, for words no longer written in kanji, and also following kanji to show conjugational endings. Because of the way verbs (and adjectives) in Japanese are conjugated, kanji alone cannot fully convey Japanese tense and mood, as kanji cannot be subject to variation when written without losing its meaning. For this reason, hiragana are suffixed to the ends of kanji to show verb and adjective conjugations. Hiragana used in this way are called okurigana. Hiragana can also be written in a superscript called furigana above or beside a kanji to show the proper reading. This is done to facilitate learning, as well as to clarify particularly old or obscure (or sometimes invented) readings.
"Katakana", like hiragana, are a syllabary; katakana are primarily used to write foreign words, plant and animal names, and for emphasis. For example "Australia" has been adapted as "Ōsutoraria" (オーストラリア), and "supermarket" has been adapted and shortened into "sūpā" (スーパー).
Historically, attempts to limit the number of kanji in use commenced in the mid-19th century, but did not become a matter of government intervention until after Japan's defeat in the Second World War. During the period of post-war occupation (and influenced by the views of some U.S. officials), various schemes including the complete abolition of kanji and exclusive use of rōmaji were considered. The "jōyō kanji" ("common use kanji", originally called "tōyō kanji" [kanji for general use]) scheme arose as a compromise solution.
Japanese students begin to learn kanji from their first year at elementary school. A guideline created by the Japanese Ministry of Education, the list of "kyōiku kanji" ("education kanji", a subset of "jōyō kanji"), specifies the 1,006 simple characters a child is to learn by the end of sixth grade. Children continue to study another 1,130 characters in junior high school, covering in total 2,136 "jōyō kanji". The official list of "jōyō kanji" was revised several times, but the total number of officially sanctioned characters remained largely unchanged.
As for kanji for personal names, the circumstances are somewhat complicated. "Jōyō kanji" and "jinmeiyō kanji" (an appendix of additional characters for names) are approved for registering personal names. Names containing unapproved characters are denied registration. However, as with the list of "jōyō kanji", criteria for inclusion were often arbitrary and led to many common and popular characters being disapproved for use. Under popular pressure and following a court decision holding the exclusion of common characters unlawful, the list of "jinmeiyō kanji" was substantially extended from 92 in 1951 (the year it was first decreed) to 983 in 2004. Furthermore, families whose names are not on these lists were permitted to continue using the older forms.
Study by non-native speakers.
Many major universities throughout the world provide Japanese language courses, and a number of secondary and even primary schools worldwide offer courses in the language. This is much changed from before World War II; in 1940, only 65 Americans not of Japanese descent were able to read, write and understand the language.
International interest in the Japanese language dates from the 19th century but has become more prevalent following Japan's economic bubble of the 1980s and the global popularity of Japanese popular culture (such as anime and video games) since the 1990s. Near of 4 million people studied the language worldwide in 2012: more than 1 million Chinese, 872,000 Indonesian, 840,000 South Koreans studied Japanese in lower and higher educational institutions. In the three years from 2009 to 2012 the number of students studying Japanese in China increased by 26.5 percent/three years, and by 21.8 percent Indonesia, but dropped 12.8 percent in South Korea.
In Japan, more than 90,000 foreign students studied at Japanese universities and Japanese language schools, including 77,000 Chinese and 15,000 South Koreans in 2003. In addition, local governments and some NPO groups provide free Japanese language classes for foreign residents, including Japanese Brazilians and foreigners married to Japanese nationals. In the United Kingdom, study of the Japanese language is supported by the British Association for Japanese Studies. In Ireland, Japanese is offered as a language in the Leaving Certificate in some schools. 
The Japanese government provides standardized tests to measure spoken and written comprehension of Japanese for second language learners; the most prominent is the Japanese Language Proficiency Test (JLPT), which features five levels of exams (changed from four levels in 2010), ranging from elementary (N5) to advanced (N1). The JLPT is offered twice a year. The Japanese External Trade Organization JETRO organizes the "Business Japanese Proficiency Test" which tests the learner's ability to understand Japanese in a business setting. The Japan Kanji Aptitude Testing Foundation, which took over the BJT from JETRO in 2009, announced in August 2010 that the test would be discontinued in 2011 due to financial pressures on the Foundation. However, it has since issued a statement to the effect that the test will continue to be available as a result of support from the Japanese government.
Works cited.
</dl>

</doc>
<doc id="15608" url="http://en.wikipedia.org/wiki?curid=15608" title="Johnny Got His Gun">
Johnny Got His Gun

Johnny Got His Gun is an anti-war novel written in 1938 by American novelist and screenwriter Dalton Trumbo
and published September 1939 by J. B. Lippincott.
The novel won one of the early National Book Awards: the Most Original Book of 1939.
Plot.
Joe Bonham, a young American soldier serving in World War I, awakens in a hospital bed after being caught in the blast of an exploding artillery shell. He gradually realizes that he has lost his arms, legs, and all of his face (including his eyes, ears, teeth, and tongue), but that his mind functions perfectly, leaving him a prisoner in his own body.
Joe attempts suicide by suffocation, but finds that he had been given a tracheotomy which he can neither remove nor control. At first Joe wishes to die, but later decides that he desires to be placed in a glass box and toured around the country in order to show others the true horrors of war. Joe successfully communicates these desires with military officials by banging his head on his pillow in Morse code. However, he realizes that neither desire will be granted, and it is implied that he will live the rest of his natural life in his condition.
As Joe drifts between reality and fantasy, he remembers his old life with his family and girlfriend, and reflects upon the myths and realities of war.
Title and context.
The title is a play on the phrase "Johnny get your gun",
a rallying call that was commonly used to encourage young American men to enlist in the military in the late 19th and early 20th century. That phrase was popularized in the George M. Cohan song "Over There", which was widely recorded in the first year of American involvement in World War I; the versions by Al Jolson, Enrico Caruso, and Nora Bayes are believed to have sold the most copies on phonograph records at the time. "Johnny Get Your Gun" is also the name of a 1919 film directed by Donald Crisp.
Many of protagonist Joe Bonham's early memories are based on Dalton Trumbo's early life in Colorado and Los Angeles. The novel was inspired by an article he read about the Prince of Wales' visit to a Canadian veterans hospital to see a soldier who had lost all of his senses and his limbs. "Though the novel was a pacifist piece published in wartime, it was well reviewed and won an American Booksellers Award in 1940."
Publication.
Serialized in the "Daily Worker" in March 1940,
the book became "a rally point for the political left" which had opposed involvement in World War II during the period of the Hitler-Stalin pact. Shortly after the 1941 German invasion of the Soviet Union, Trumbo and his publishers decided to suspend reprinting the book until the end of the war. After receiving letters from right-wing isolationists requesting copies of the book, Trumbo contacted the FBI and turned these letters over to them. Trumbo regretted this decision, which he later called "foolish," after two FBI agents showed up at his home and it became clear that "their interest lay not in the letters but in me."
Adaptations.
On March 9, 1940, a radio adaptation of "Johnny Got His Gun" was produced and directed by Arch Oboler, based on his script, and presented on the NBC Radio series "Arch Oboler's Plays." James Cagney played Joe Bonham on that broadcast.
In 1971, Trumbo directed a film adaptation of the novel, starring Timothy Bottoms as Joe Bonham. In 1982, "Johnny Got His Gun" was adapted into a stage play by Bradley Rand Smith, which has since been performed worldwide. Its first off-Broadway run starred Jeff Daniels.
In the 1991 film, December, starring Wil Wheaton, the book plays a role in convincing Wheaton's character, Kipp Gibbs, not to enlist in the Army following the bombing of Pearl Harbor. This occurs after a faculty member attempts to burn the book so that it cannot influence the student body of the prep school that Gibbs and his brother attend.
In 2008, actor Benjamin McKenzie earned critical acclaim for his solo performance (as Joe Bonham) in the "live on stage, on film" version of the 1982 Off-Broadway play based on the novel, McKenzie's fourth starring role in a feature film.
In early 2009, the 1971 film made its U.S. DVD debut, produced by Shout! Factory. The DVD included the original, uncut film, plus a 2005 documentary ("Dalton Trumbo: Rebel In Hollywood"), new cast interviews, Metallica's music video "One," behind-the-scenes footage with commentary by stars Timothy Bottoms and Jules Brenner, the 1940 radio adaptation, and the original theatrical trailer.
In October 2010, a special educational DVD of the 2008 film version starring McKenzie became available free of charge to every high school library in the U.S. The educational DVD contains both a pre-screening and post-screening discussion guide for students, in addition to a 15-minute featurette on the making of the film, the original movie's theatrical trailer, and a history of the original novel. 
In the Italian anti-war comic strip "Sturmtruppen", a young soldier called "Sigfrid Von Nibelunghen", who voluntarily enlisted before coming of age due to the pro-war propaganda he was subject to at school, suffers the same fate as Joe Bonham. In the end, when he considers the harsh reality of his condition against the idealized vision of war and heroic death he had been led to believe, he simply thinks to himself "I fear that I have been duped".
The UK stage premiere of the Bradley Rand Smith version will be directed by David Mercatali and starring Jack Holden at the Southwark Playhouse from 21 May 2014 to 14 June 2014.

</doc>
<doc id="15611" url="http://en.wikipedia.org/wiki?curid=15611" title="Simon–Ehrlich wager">
Simon–Ehrlich wager

Julian L. Simon and Paul Ehrlich entered in a widely followed scientific wager in 1980, betting on a mutually agreed-upon measure of resource scarcity over the decade leading up to 1990. Simon had Ehrlich choose five commodity metals. Copper, chromium, nickel, tin, and tungsten were chosen and Simon bet that their prices would decrease, while Ehrlich bet they would increase.
The contest occurred in the pages of Social Science Quarterly, where Simon challenged Ehrlich to put his money where his mouth was. In response to Ehrlich's published claim that "If I were a gambler, I would take even money that England will not exist in the year 2000"—a proposition Simon regarded as too silly to bother with—Simon countered with "a public offer to stake US$10,000 ... on my belief that the cost of non-government-controlled raw materials (including grain and oil) will not rise in the long run."
Ehrlich lost the bet, as all five commodities that were bet on declined in price from 1980 through 1990, the wager period. However, economists later showed that Ehrlich would have won in the majority of 10-year periods over the last century, and if the wager was extended by 30 years to 2011, he would have won on four out of the five metals.
Background.
In 1968, Ehrlich published "The Population Bomb", which argued that mankind was facing a demographic catastrophe with the rate of population growth quickly outstripping growth in the supply of food and resources. Simon was highly skeptical of such claims, so proposed a wager, telling Ehrlich to select any raw material he wanted and select "any date more than a year away," and Simon would bet that the commodity's price on that date would be lower than what it was at the time of the wager.
Ehrlich and his colleagues picked five metals that they thought would undergo big price increases: chromium, copper, nickel, tin, and tungsten. Then, on paper, they bought $200 worth of each, for a total bet of $1,000, using the prices on September 29, 1980, as an index. They designated September 29, 1990, 10 years hence, as the payoff date. If the inflation-adjusted prices of the various metals rose in the interim, Simon would pay Ehrlich the combined difference. If the prices fell, Ehrlich et al. would pay Simon.
Between 1980 and 1990, the world's population grew by more than 800 million, the largest increase in one decade in all of history. But by September 1990, the price of each of Ehrlich's selected metals had fallen. Chromium, which had sold for $3.90 a pound in 1980, was down to $3.70 in 1990. Tin, which was $8.72 a pound in 1980, was down to $3.88 a decade later.
As a result, in October 1990, Paul Ehrlich mailed Julian Simon a check for $576.07 to settle the wager in Simon's favor.
Analysis.
According to Paul Ehrlich's website:
In 1980, Julian Simon repeatedly challenged environmental scientists to bet against him on trends in prices of commodities, asserting that humanity would never run out of anything... Paul and the other scientists knew that the five metals in the proposed wager were not critical indicators and said so at the time... They emphasized that the depletion of so-called renewable resources — environmental resources such as soils, forests, species diversity, and groundwater — is much more indicative of the deteriorating state of society's life-support systems... Nonetheless, after consulting with many colleagues, Paul and Berkeley physicists John Harte and John Holdren accepted Simon's challenge in late 1980...
Julian Simon won because the price of three of the five metals went down in nominal terms and all five of the metals fell in price in inflation-adjusted terms, with both tin and tungsten falling by more than half. So per the terms of the wager, Ehrlich paid Simon the difference in price between the same quantity of metals in 1980 and 1990 (which was $576.07). The prices of all five metals increased between 1950 and 1975, but Ehrlich believes three of the five went down during the 1980s because of the price of oil doubling in 1979, and because of a world-wide recession in the early 1980s.
Simon offered to raise the wager to $20,000 and to use any resources at any time that Ehrlich preferred. Ehrlich countered with a challenge to bet that temperatures would increase in the future. The two were unable to reach an agreement on the terms of a second wager before Simon died.
Ehrlich would likely have won if the bet had been for a different ten-year period.
Asset manager Jeremy Grantham wrote that if the Simon–Ehrlich wager had been for a longer period (from 1980 to 2011), then Simon would have lost on four of the five metals. He also noted that if the wager had been expanded to "all of the most important commodities," instead of just five metals, over that longer period of 1980 to 2011, then Simon would have lost "by a lot." 
Aftermath.
The price of raw and other natural commodities such as oil, gold, and uranium have risen substantially in recent years, due to increased demand from China, India, and other industrializing countries. However, Simon has argued that this price increase is not necessarily contrary to his cornucopian theory. Ehrlich has dismissed the bet as a side issue and stated that the main worry is environmental problems like the ozone hole, acid rain, and global warming.
The proposed second wager.
Understanding that Simon wanted to bet again, Ehrlich and climatologist Stephen Schneider counter-offered, challenging Simon to bet on 15 current trends, betting $1000 that each will get worse (as in the previous wager) over a ten-year future period.
The trends they bet would continue to worsen were:
Simon declined Ehrlich and Schneider's offer to bet, and used the following analogy to explain why he did so:
 Let me characterize their offer as follows. I predict, and this is for real, that the average performances in the next Olympics will be better than those in the last Olympics. On average, the performances have gotten better, Olympics to Olympics, for a variety of reasons. What Ehrlich and others says is that they don't want to bet on athletic performances, they want to bet on the conditions of the track, or the weather, or the officials, or any other such indirect measure.
Other wagers.
In 1996, Simon bet $1000 with David South, professor of the Auburn University School of Forestry, that the inflation-adjusted price of timber would decrease in the following five years. Simon paid out early on the bet in 1997 (before his death in 1998) based on his expectation that prices would remain above 1996 levels (which they did).
In 1999, when "The Economist" headlined an article entitled, "$5 a barrel oil soon?" and with oil trading in the $12/barrel range, David South offered $1000 to any economist who would bet with him that the price of oil would be greater than $12/barrel in 2010. No economist took him up on the offer. However, in October 2000, Zagros Madjd-Sadjadi, an economist with The University of the West Indies, bet $1000 with David South that the inflation-adjusted price of oil would decrease to an inflation-adjusted price of $25 by 2010 (down from what was then $30/barrel). Madjd-Sadjadi paid South an inflation-adjusted $1,242 in January 2010. The price of oil at the time was $81/barrel.

</doc>
<doc id="15612" url="http://en.wikipedia.org/wiki?curid=15612" title="John Tenniel">
John Tenniel

Sir John Tenniel (28 February 1820 – 25 February 1914) was an English illustrator, graphic humourist and political cartoonist whose work was prominent during the second half of the 19th century. Tenniel is considered important to the study of that period’s social, literary, and art histories. Tenniel was knighted by Victoria for his artistic achievements in 1893.
Tenniel is most noted for two major accomplishments: he was the principal political cartoonist for Britain’s "Punch" magazine for more than 50 years, and he was the artist who illustrated Lewis Carroll’s "Alice’s Adventures in Wonderland" and "Through the Looking-Glass".
Early life.
John Tenniel Junior was born on 28 February 1820 in Bayswater, West London, by his parents John Baptist and Eliza Maria. Tenniel had five other siblings two brothers and three sisters. Tenniel was a quiet and introverted person, both as a boy and as an adult. He was content to remain firmly out of the limelight and seemed unaffected by competition or change. As Engen said in his book, Tenniel’s “life and career was that of the supreme gentlemanly outside, living on the edge of respectability.” 
In 1840 Tenniel, while practising fencing with his father, received a serious wound in his eye from his father's foil, which had accidentally lost its protective tip. Over the years Tenniel gradually lost sight in his right eye; he never told his father of the severity of the wound, as he did not wish to upset his father to any greater degree than he had been.
In spite of his tendency towards high art, Tenniel was already known and appreciated as a humorist, and his early companionship with Charles Keene fostered and developed his talent for scholarly caricature.
Training.
While Tenniel’s more formal training at the Royal Academy and at other institutions was beneficial in nurturing his artistic ambitions, it failed in Tenniel’s mind through his disagreement with the school’s teaching methods, resulting in Tenniel educating himself for his career. Tenniel was a probationer (he was only admitted if he made several copies of classical sculptures to provide the necessary admission portfolio), and then a student of the Royal Academy of Arts in 1842. Tenniel studied classical sculptures through painting; however, Tenniel was frustrated that he never learned how to draw. So, it was here that Tenniel returned to his earlier independent education. Tenniel would draw the classical statues at the London’s Townley Gallery, copied illustrations from books of costumes and armor in the British museum, drew the animals from the zoo in Regent’s Park as well as the actors from the London theatres, which were drawn from the pits. It was in these studies that Tenniel learned to love detail; however, he became impatient with his work and was the happiest when he could draw from memory. Tenniel was blessed with a photographic memory, undermining his early training and seriously restricting his artistic ambitions.
Another “formal” means of training was Tenniel’s participation in an artists group, free from the rules of the Academy that stifled Tenniel. In the mid 1840s Tenniel joined the Artist’s Society or Clipstone Street Life Academy, and it could be said here that Tenniel first emerged as a satirical draftsman.
Early career.
Tenniel’s first book illustration was for Samuel Carter Hall's "The Book of British Ballads", in 1842. While engaged with his first book illustrations, various contests were taking place in London, as a way in which the government could combat the growing Germanic Nazarenes style and promote a truly national English school of art. Tenniel planned to enter the 1845 House of Lords competition amongst artists to win the opportunity to design the mural decoration of the new Palace of Westminster. Despite missing the deadline, he submitted a 16 ft cartoon, "An Allegory of Justice", to a competition for designs for the mural decoration of the new Palace of Westminster. For this he received a £200 premium and a commission to paint a fresco in the Upper Waiting Hall (or Hall of Poets) in the House of Lords.
Political cartoons at Punch.
As the influential result of his position as the chief cartoon artist for "Punch" (published 1841–1992, 1996–2002), John Tenniel, through satirical, often radical and at times vitriolic images of the world, for five decades was and remained Great Britain’s steadfast social witness to the sweeping national changes in that nation’s moment of political and social reform.
At Christmas 1850 he was invited by Mark Lemon to fill the position of joint cartoonist (with John Leech) on "Punch". He had been selected on the strength of his recent illustrations to Aesop's "Fables". He contributed his first drawing in the initial letter appearing on p. 224, vol. xix. His first cartoon was "Lord Jack the Giant Killer", which showed Lord John Russell assailing Cardinal Wiseman.
In 1861, Tenniel was offered John Leech’s position at Punch, as political cartoonist; however, Tenniel still maintained some sense of decorum and restraint into the heated social and political issues of the day.
Because his task was to construct the willful choices of his "Punch" editors, who probably took their cue from "The Times" and would have felt the suggestions of political tensions from Parliament as well, Tenniel’s work, as was its design, could be scathing in effect. The restlessness of the Victorian period’s issues of working class radicalism, labor, war, economy, and other national themes were the targets of "Punch", which in turn commanded the nature of Tenniel’s subjects. Tenniel's cartoons published in the 1860s made popular the portrait of the Irishman as a subhuman being, wanton in his appetites and most resembling an orang-utan in both facial features and posture. Many of Tenniel's political cartoons expressed strong hostility to Irish Nationalism, with Fenians and Land leagues depicted as monstrous, ape-like brutes, while "Hibernia"—the personification of Ireland—was depicted as a beautiful, helpless young girl threatened by these "monsters" and turning for protection to "her elder sister", the powerful armoured Britannia.
His drawing of 'An unequal match', published in "Punch" on 8 October 1881, depicted a police officer fighting a criminal with only a 'baton' for protection, trying to put a point across to the public that policing methods needed to be changed.
When examined separately from the book illustrations he did over time, Tenniel’s work at "Punch" alone, expressing decades of editorial viewpoints, often controversial and socially sensitive, was created to echo the voices of the British public. Tenniel drew 2,165 cartoons for "Punch", a liberal and politically active publication that mirrored the Victorian public's mood for liberal social changes; thus Tenniel, in his cartoons, represented for years the conscience of the British majority.
In his career Tenniel contributed around 2,300 cartoons, innumerable minor drawings, double-page cartoons for "Punch's Almanac" and other special numbers, and 250 designs for "Punch's Pocket-books". By 1866 he was "able to command ten to fifteen guineas for the reworking of a single Punch cartoon as a pencil sketch", alongside his "comfortable" Punch salary "of about £800 a year".According to the Bank of England inflation calculator, £800 in 1866 would buy goods and services worth over £78,000 in 2011.
"Alice's Adventures in Wonderland" and "Through the Looking Glass".
Despite the thousands of political cartoons and hundreds of illustrative works attributed to him, much of Tenniel’s fame stems from his illustrations for "Alice". Tenniel drew ninety-two drawings for Lewis Carroll’s "Alice’s Adventures in Wonderland" (London: Macmillan, 1865) and "Through the Looking-Glass and What Alice Found There" (London: Macmillan, 1871).
Lewis Carroll originally illustrated "Wonderland" himself, but his artistic abilities were limited. Engraver Orlando Jewitt, who had worked for Carroll in 1859 and had reviewed Carroll’s drawings for "Wonderland", suggested that he employ a professional illustrator. Carroll was a regular reader of "Punch" and was therefore familiar with Tenniel. In 1865 Tenniel, after long talks with Carroll, illustrated the first edition of "Alice's Adventures in Wonderland".
The first print run of 2,000 was sold in the United States, rather than England, because Tenniel objected to the print quality. A new edition was released in December 1865, carrying an 1866 date, and became an instant best-seller, increasing Tenniel's fame. His drawings for both books have become some of the most famous literary illustrations. After 1872, when the Carroll projects were finished, Tenniel largely abandoned literary illustration. Carroll did later approach Tenniel to undertake another project for him. To this Tenniel replied:
“It is a curious fact that with ‘Looking-Glass’ the faculty of making drawings for book illustrations departed from me, and [...] I have done nothing in that direction since.”
Tenniel's illustrations for the "Alice" books were engraved onto blocks of deal wood by the Brothers Dalziel. These engravings were then used as masters for making the electrotype copies for the actual printing of the books. The original wood blocks are now in the collection of the Bodleian Library in Oxford. They are not usually on public display, but were exhibited in 2003.
Style.
Influence from the German Nazarenes.
The style associated with the Nazarene movement of the nineteenth century influenced many subsequent artists including Tenniel. This style can be characterized as “shaded outlines” where the lines on the side of figures or objects are given extra thickness or are drawn as double lines in order to suggest shading or volume. Additionally, this style is extremely precise, with the artist making a hard clear outline along its figures, creating dignified figures and compositions, as well as a restraint in expression and paleness of tone. While, Tenniel’s early illustrations done in the Nazarene style were not well received, his encounter with the style pointed him in the right direction.
An Eye for Detail.
After the 1850s, Tenniel’s style modernized to incorporate more detail in backgrounds and in figures. The inclusion of background details corrected the previously weak Germanic staging of his illustrations. Tenniel’s precisely designed illustrations, previous generalized scenes to more specific moments of time, locale and individual character.
In addition to a change in specificity of background, Tenniel developed a new interest in human types, expressions, and individualized representation, something that would carry over into Tenniel’s illustrations of Wonderland. Referred to by many as theatricalism, this hallmark of Tenniel’s style probably stemmed from his earlier interest in caricature. In Tenniel’s first years on Punch he developed this caricaturist’s interest in the uniqueness of persons and things, almost giving a human like personality to the objects in the environment. For example, in a comparison to one of John Everett Millais’s illustration of a girl in a chair with Tenniel’s illustration of Alice in a chair, one can see how where Millais’s chair is just a prop, Tenniel’s chair possesses a menacing and towering presence.
Another change in style was his shaded lines. These transformed from mechanical horizontal lines to vigorously hand-drawn hatching that greatly intensified darker areas.
Grotesque.
Tenniel’s “grotesqueness” was one of the main reasons why Lewis Carroll wanted Tenniel as his illustrator for the Alice books. The grotesque is an abnormality that imparts the disturbing sense that the real world may have ceased to be reliable. Tenniel’s style was characteristically grotesque in his dark atmospheric compositions of exaggerated fantasy creatures that were carefully drawn in outline. Often though, the mechanism was to use animal heads on recognizable human bodies or vice versa, as Grandville had done with such effect in the pages of the Parisian satirical journal, "Charivari". In Tenniel’s illustrations, the grotesque is found also in the merging of beings and things, deformities of and violence to the human body (as seen in the illustration when Alice drinks the potion and gets large), a proclivity to deal with the ordinary things of this world while exhibiting such phenomena. Most notably done in grotesque fashion is that of Tenniel’s famous Jabberwock drawing in Alice.
What is so fascinating and why so effective are the Alice illustrations is their ability to combine the fantasy and the real. Scholars such as Morris say that Tenniel’s stylistic change can be attributed to the late 1850s trend towards realism. For the grotesque to operate, “it is our world which has to be transformed and not some fantasy realm." In the illustrations we are constantly but subtly reminded of the real world, such as some of Tenniel’s scenes being derived from a medieval town, the portico of Georgian town, or the checked jacket on the white rabbit. Additionally, Tenniel closely follows the text provided by Carroll so readers are ensured that what they are reading, they are seeing in his illustrations. These subtle points of realism help convince readers that all these seemingly grotesque habitants of Wonderland are simply themselves, are simply real, they are not performing.
Image and Text Relationship in "Alice's Adventures in Wonderland".
One of the most unusual elements of the Alice books is the placement of Tenniel’s illustrations on the pages. There was a physical relation of the illustrations to the text, intended to subtly mesh illustrations with certain points of the text. Carroll and Tenniel expressed this in various ways; one of them bracketing. Two relevant sentences would bracket an image, which might better define the moment that Tenniel was trying to illustrate. It is this precise bracketing of Tenniel’s pictures by the text that adds to their “dramatic immediacy." However, other illustrations work with the texts in that they act as captions, though it is not as frequent as bracketing.
Another way in which the illustrations correspond with the text is by having broader and narrower illustrations. Broader illustrations are meant to be centered on the page, where as narrower illustrations are meant to be “let in” or run flush to the margin to be set alongside a narrowed column of the continuing text. Still, words run in parallel with the depiction of those things. For example, in this image, we see how when Alice says, “Oh, my poor little feet,” it not only occurs at the foot of the page but is directly next to her feet in the illustration. Part of these narrower illustrations was the “L” shaped illustration, which was of great importance, being that these are where Tenniel did some of his most memorable work. The top or base of these illustrations run the full width of the page but then the other end would has some room on one side of the quadrant for the text.
Retirement and death.
An ultimate tribute came to an elderly Tenniel as he was honored as a living national treasure and for his public service was knighted in 1893 by Queen Victoria. The first such honour ever bequeathed on an illustrator or cartoonist, his fellows saw his knighting coming as gratitude for “raising what had been a fairly lowly profession to an unprecedented level of respectability.” With knighthood, Tenniel elevated the social status of the black and white illustrator, and sparked a new sense of recognition of and occupational honour to his lifelong profession.
When he retired in January 1901, Tenniel was honoured with a farewell banquet (12 June), at which AJ Balfour, then Leader of the House of Commons, presided. "Punch" historian M. H. Spielmann, who knew Tenniel, understood that the political clout contained in his "Punch" cartoons was capable of “swaying parties and people, too... (the cartoons) exercised great influence” on the ideas of popular reform skirting throughout the British public. Early tributes as to what Tenniel in his role as a national observer meant to the British nation around the time of his death came in as high praise; in 1914 "New York Tribune" journalist George W. Smalley referred to John Tenniel as “one of the greatest intellectual forces of his time, (who) understood social laws and political energies.”
Legacy.
On 27 February 1914, two days after his death, the "Daily Graphic" recalled Tenniel: "He had an influence on the political feeling of this time which is hardly measurable...While Tenniel was drawing them (his subjects), we always looked to the Punch cartoon to crystallize the national and international situation, and the popular feeling about it—and never looked in vain." This condition of social influence resulted from the weekly publishing over a fifty-year span of his political cartoons, whereby Tenniel's fame allowed for a want and need for his particular illustrative work, away from the newspaper. Tenniel became not only one of Victorian Britain’s most published illustrators, but as a "Punch" cartoonist he became one of the “supreme social observers” of British society, and an integral component of a powerful journalistic force.
Public exhibitions of Sir John Tenniel's work were held in 1895 and in 1900. Sir John Tenniel is also the author of one of the mosaics, "Leonardo da Vinci", in the South Court in the Victoria and Albert Museum; while his highly stippled watercolour drawings appeared from time to time in the exhibitions of the Royal Institute of Painters in Water Colours, of which he had been elected a member in 1874.
A Bayswater street, Tenniel Close, near his former studio, is named after him.
Works.
He also contributed to "Once a Week", the Art Union publications, etc.

</doc>
<doc id="15613" url="http://en.wikipedia.org/wiki?curid=15613" title="Jazz">
Jazz

Jazz is a genre of music that originated in African American communities during the late 19th and early 20th century. It emerged in many parts of the United States in the form of independent popular musical styles, all linked by the common bonds of African American and European American musical parentage with a performance orientation. Jazz spans a period of over 100 years and encompasses a range of music from ragtime to the present day, and has proved to be very difficult to define. Jazz makes heavy use of improvisation, polyrhythms, syncopation and the swung note, as well as aspects of European harmony, American popular music, the brass band tradition, and African musical elements such as blue notes and ragtime. The birth of Jazz in the multicultural society of America has led intellectuals from around the world to hail Jazz as "one of America's original art forms".
As jazz spread around the world, it drew on different national, regional, and local musical cultures, giving rise to many distinctive styles. New Orleans jazz began in the early 1910s, combining earlier brass band marches, French quadrilles, biguine, ragtime and blues with collective polyphonic improvisation. In the 1930s, heavily arranged dance-oriented swing big bands, Kansas City jazz, a hard-swinging, bluesy, improvisational style and Gypsy jazz (a style that emphasized Musette waltzes) were the prominent styles. Bebop emerged in the 1940s, shifting jazz from danceable popular music towards a more challenging "musician's music" which was played at faster tempos and used more chord-based improvisation. Cool jazz developed in the end of the 1940s, introducing calmer, smoother sounds and long, linear melodic lines. 
The 1950s saw the emergence of free jazz, which explored playing without regular meter, beat and formal structures, and in the mid-1950s, hard bop, which introduced influences from rhythm and blues, gospel music, and blues, especially in the saxophone and piano playing. Modal jazz developed in the late 1950s, using the mode, or musical scale, as the basis of musical structure and improvisation. Jazz-rock fusion appeared in the late 1960s and early 1970s, combining jazz improvisation with rock rhythms, electric instruments and the highly amplified stage sound of rock. In the early 1980s, a commercial form of jazz fusion called smooth jazz became successful, garnering significant radio airplay. Other jazz styles include Afro-Cuban jazz, West Coast jazz, ska jazz, Indo jazz, avant-garde jazz, soul jazz, chamber jazz, Latin jazz, jazz funk, loft jazz, punk jazz, acid jazz, ethno jazz, jazz rap, M-Base, spiritual jazz and nu jazz.
Prominent jazz musician Louis Armstrong observed: "At one time they were calling it levee camp music, then in my day it was ragtime. When I got up North I commenced to hear about jazz, Chicago style, Dixieland, swing. All refinements of what we played in New Orleans... There ain't nothing new." Or as jazz musician J. J. Johnson put it in a 1988 interview: "Jazz is restless. It won't stay put and it never will."
Definitions.
Jazz has proved to be very difficult to define, since it encompasses such a wide range of music spanning a period of over 100 years, from ragtime to the present day. Attempts have been made to define jazz from the perspective of other musical traditions, such as European music history or African music. But critic Joachim-Ernst Berendt argues that its terms of reference and its definition should be broader, defining jazz as a "form of art music which originated in the United States through the confrontation of the Negro with European music" and arguing that it differs from European music in that jazz has a "special relationship to time defined as 'swing'", involves "a spontaneity and vitality of musical production in which improvisation plays a role" and contains a "sonority and manner of phrasing which mirror the individuality of the performing jazz musician".
A broader definition that encompasses all of the radically different eras of jazz has been proposed by Travis Jackson: "it is music that includes qualities such as swing, improvising, group interaction, developing an 'individual voice', and being open to different musical possibilities". Krin Gibbard has provided an overview of the discussion on definitions, arguing that "jazz is a construct" that, while artificial, still is useful to designate "a number of musics with enough in common to be understood as part of a coherent tradition". In contrast to the efforts of commentators and enthusiasts of certain types of jazz, who have argued for narrower definitions that exclude other types, the musicians themselves are often reluctant to define the music they play. As Duke Ellington, one of jazz's most famous figures, said: "It's all music".
Importance of improvisation.
Although jazz is considered difficult to define, improvisation is consistently regarded as being one of its key elements. The centrality of improvisation in jazz is attributed to influential earlier forms of music: the early blues, a form of folk music which arose in part from the work songs and field hollers of the African-American workers on plantations. These were commonly structured around a repetitive call-and-response pattern, but early blues was also highly improvisational. European classical music performance is evaluated by its fidelity to the text, with discretion over interpretation, ornamentation and accompaniment: the classical performer's primary goal is to play a composition as it was written. In contrast, jazz is often characterized as the product of group creativity, interaction, and collaboration, which places varying degrees of value on the contributions of composer (if there is one) and performers. In jazz, the skilled performer will interpret a tune in very individual ways, never playing the same composition exactly the same way twice: depending upon the performer's mood and personal experience, interactions with other musicians, or even members of the audience, a jazz musician may alter melodies, harmonies or time signature at will.
The approach to improvisation has developed enormously over the history of the music. In early New Orleans and Dixieland jazz, performers took turns playing the melody, while others improvised countermelodies. By the swing era, big bands were coming to rely more on arranged music: arrangements were either written or learned by ear and memorized, while individual soloists would improvise within these arrangements. Later, in bebop the focus shifted back towards small groups and minimal arrangements; the melody would be stated briefly at the start and end of a piece, but the core of the performance would be the series of improvisations. Later styles such as modal jazz abandoned the strict notion of a chord progression, allowing the individual musicians to improvise even more freely within the context of a given scale or mode. In many forms of jazz a soloist is often supported by a rhythm section who accompany by playing chords and rhythms that outline the song structure and complement the soloist. In avant-garde and free jazz idioms, the separation of soloist and band is reduced, and there is license, or even a requirement, for the abandoning of chords, scales, and rhythmic meters.
Debates.
Since at least the emergence of bebop, forms of jazz that are commercially oriented or influenced by popular music have been criticized by purists. According to Bruce Johnson, there has always been a "tension between jazz as a commercial music and an art form". Traditional jazz enthusiasts have dismissed bebop, free jazz, the 1970s jazz fusion era and much else as periods of debasement of the music and betrayals of the tradition. An alternative viewpoint is that jazz is able to absorb and transform influences from diverse musical styles, and that, by avoiding the creation of 'norms', other newer, avant-garde forms of jazz will be free to emerge.
To some African Americans, jazz has highlighted their contribution to American society and helped bring attention to black history and culture, but for others, the music and term "jazz" are reminders of "an oppressive and racist society and restrictions on their artistic visions".
Etymology.
The question of the origin of the word "jazz" has resulted in considerable research, and its history is well documented. The word began [under various spellings] as West Coast slang around 1912, the meaning of which varied but did not refer to music. The use of the word in a musical context was documented as early as 1915 in the "Chicago Daily Tribune." Its first documented use in a musical context in New Orleans was in a November 14, 1916 "Times-Picayune" article about "jas bands." The American Dialect Society named it the Word of the Twentieth Century.
Race.
Amiri Baraka argues that there is a distinct "white jazz" music genre expressive of whiteness. White jazz musicians appeared in the early 1920s in the Midwestern United States, as well as other areas. Bix Beiderbecke was one of the most prominent white jazz musicians. An influential style referred to as the Chicago School (or Chicago Style) was developed by white musicians including Bud Freeman, Jimmy McPartland. Frank Teschemacher, Dave Tough, and Eddie Condon. Others from Chicago such as Benny Goodman and Gene Krupa became leading members of big-band swing during the 1930s.
Women in jazz.
When thinking of jazz music, women are normally the singers of genre, however dating back to the early 1920s women instrumentalists can be found, with the piano being one of the earliest instruments used which allowed female artists a degree of social acceptance. Some well known artists of the time consists of Sweet Emma Barrett, Billie Pierce, Jeanette Kimball and Lovie Austin. These women have done a lot for the genre.
When the men got drafted for the war numerous all women big band jazz bands took over. However with the division of skin color, there was no real band that any one society listened to. The International Sweethearts of Rhythm was the all women jazz band best known during these times. Despite the harsh dress code of women at the time of strapless dresses and high heeled shoes, women were being hired into many of the big league big bands such as Woody Herman's and Gerald Wilson. 
After the war many musicians came back to reclaim their jobs. Beacause of that, many women were removed from music scene, while some stayed to play music they went through harsh ridicule, and sexual harassment from their bandmates. 
Women's Jazz Festival.
Dr. Billy Taylor (1921-2010), late Kennedy Center Artistic Director for Jazz, created this festival dedicated to the composer and pianist Mary-Lou Williams, in honor of her extraordinary talent. The Mary-Lou Williams Jazz Festival has existed for sixteen years, showcasing women of any age or race.
History.
Jazz originated in the late 19th to early 20th century as interpretations of American and European classical music entwined with African and slave folk songs and the influences of West African culture. Its composition and style have changed many times throughout the years with each performer's personal interpretation and improvisation, which is also one of the greatest appeals of the genre.
Origins.
Blended African and European music sensibilities.
By 1808, the Atlantic slave trade had brought almost half a million Africans to the United States. The slaves came largely from West Africa and the greater Congo River basin, and brought strong musical traditions with them. The African traditions primarily made use of a single-line melody and call-and-response pattern, and the rhythms had a counter-metric structure and reflected African speech patterns.
Lavish festivals featuring African-based dances to drums were organized on Sundays at "Place Congo", or Congo Square, in New Orleans until 1843. There are historical accounts of other music and dance gatherings elsewhere in the southern United States. Robert Palmer said of percussive slave music:
Usually such music was associated with annual festivals, when the year's crop was harvested and several days were set aside for celebration. As late as 1861, a traveler in North Carolina saw dancers dressed in costumes that included horned headdresses and cow tails and heard music provided by a sheepskin-covered "gumbo box", apparently a frame drum; triangles and jawbones furnished the auxiliary percussion. There are quite a few [accounts] from the southeastern states and Louisiana dating from the period 1820–1850. Some of the earliest [Mississippi] Delta settlers came from the vicinity of New Orleans, where drumming was never actively discouraged for very long and homemade drums were used to accompany public dancing until the outbreak of the Civil War.
Another influence came from the harmonic style of hymns of the church, which black slaves had learned and incorporated into their own music as spirituals. The origins of the blues are undocumented, though they can be seen as the secular counterpart of the spirituals. However, as Gerhard Kubik points out, whereas the spirituals are homophonic, rural blues and early jazz "was largely based on concepts of heterophony."
During the early 19th century an increasing number of black musicians learned to play European instruments, particularly the violin, which they used to parody European dance music in their own cakewalk dances. In turn, European-American minstrel show performers in blackface popularized the music internationally, combining syncopation with European harmonic accompaniment. In the mid-1800s the white New Orleans composer Louis Moreau Gottschalk adapted slave rhythms and melodies from Cuba and other Caribbean islands into piano salon music. New Orleans was the main nexus between the Afro-Caribbean and African-American cultures.
African rhythmic retention.
The "Black Codes" outlawed drumming by slaves, which meant that African drumming traditions were not preserved in North America, unlike in Cuba, Haiti, and elsewhere in the Caribbean. African-based rhythmic patterns were retained in the United States in large part through "body rhythms" such as stomping, clapping, and patting juba.
In the opinion of jazz historian Ernest Borneman, what preceded New Orleans jazz before 1890 was "Afro-Latin music", similar to what was played in the Caribbean at the time. A three-stroke pattern known in Cuban music as tresillo is a fundamental rhythmic figure heard in many different slave musics of the Caribbean, as well as the Afro-Caribbean folk dances performed in New Orleans Congo Square and Gottschalk's compositions (for example "Souvenirs From Havana" (1859)). Tresillo is the most basic and most prevalent duple-pulse rhythmic cell in sub-Saharan African music traditions and the music of the African Diaspora.
Tresillo is heard prominently in New Orleans second line music and in other forms of popular music from that city from the turn of the 20th century to present. "By and large the simpler African rhythmic patterns survived in jazz ... because they could be adapted more readily to European rhythmic conceptions," the Jazz historian Gunther Schuller observed. "Some survived, others were discarded as the Europeanization progressed."
In the post-Civil War period (after 1865), African Americans were able to obtain surplus military bass drums, snare drums and fifes, and an original African-American drum and fife music emerged, featuring tresillo and related syncopated rhythmic figures. This was a drumming tradition that was distinct from its Caribbean counterparts, expressing a uniquely African-American sensibility. "The snare and bass drummers played syncopated cross-rhythms," observed the writer Robert Palmer (writer), speculating that "this tradition must have dated back to the latter half of the nineteenth century, and it could have not have developed in the first place if there hadn't been a reservoir of polyrhythmic sophistication in the culture it nurtured."
"Spanish tinge"—the Afro-Cuban rhythmic influence.
African-American music began incorporating Afro-Cuban rhythmic motifs in the 19th century, when the habanera (Cuban contradanza) gained international popularity. Musicians from Havana and New Orleans would take the twice-daily ferry between both cities to perform, and the habanera quickly took root in the musically fertile Crescent City. John Storm Roberts states that the musical genre habanera "reached the U.S. twenty years before the first rag was published." For the more than quarter-century in which the cakewalk, ragtime, and proto-jazz were forming and developing, the habanera was a consistent part of African-American popular music.
Habaneras were widely available as sheet music, and were the first written music which was rhythmically based on an African motif (1803), From the perspective of African-American music, the "habanera rhythm" (also known as "congo", "tango-congo", or "tango".) can be thought of as a combination of tresillo and the backbeat. The habanera was the first of many Cuban music genres which enjoyed periods of popularity in the United States, and reinforced and inspired the use of tresillo-based rhythms in African-American music. 
New Orleans native Louis Moreau Gottschalk's piano piece "Ojos Criollos (Danse Cubaine)" (1860) was influenced by the composer's studies in Cuba: the habanera rhythm is clearly heard in the left hand. In Gottschalk's symphonic work "A Night in the Tropics" (1859), the tresillo variant cinquillo appears extensively. The figure was later used by Scott Joplin and other ragtime composers.
Comparing the music of New Orleans with the music of Cuba, Wynton Marsalis observes that tresillo is the New Orleans "clave", a Spanish word meaning 'code' or 'key', as in the key to a puzzle, or mystery. Although technically the pattern is only half a clave, Marsalis makes the point that the single-celled figure is the guide-pattern of New Orleans music. Jelly Roll Morton called the rhythmic figure the "Spanish tinge", and considered it an essential ingredient of jazz.
1890s–1910s.
Ragtime.
The abolition of slavery in 1865 led to new opportunities for the education of freed African Americans. Although strict segregation limited employment opportunities for most blacks, many were able to find work in entertainment. Black musicians were able to provide entertainment in dances, minstrel shows, and in vaudeville, during which time many marching bands were formed. Black pianists played in bars, clubs and brothels, as ragtime developed.
Ragtime appeared as sheet music, popularized by African-American musicians such as the entertainer Ernest Hogan, whose hit songs appeared in 1895. Two years later, Vess Ossman recorded a medley of these songs as a banjo solo known as "Rag Time Medley". Also in 1897, the white composer William H. Krell published his "Mississippi Rag" as the first written piano instrumental ragtime piece, and Tom Turpin published his "Harlem Rag", the first rag published by an African-American.
The classically trained pianist Scott Joplin produced his "Original Rags" in 1898, and in 1899 had an international hit with "Maple Leaf Rag", a multi-strain ragtime march with four parts that feature recurring themes and a bass line with copious seventh chords. Its structure was the basis for many other rags, and the syncopations in the right hand, especially in the transition between the first and second strain, were novel at the time.
African-based rhythmic patterns such as tresillo and its variants, the habanera rhythm and cinquillo, are heard in the ragtime compositions of Joplin, Turpin, and others. Joplin's "Solace" (1909) is generally considered to be within the habanera genre: both of the pianist's hands play in a syncopated fashion, completely abandoning any sense of a march rhythm. Ned Sublette postulates that the tresillo/habanera rhythm "found its way into ragtime and the cakewalk," whilst Roberts suggests that "the habanera influence may have been part of what freed black music from ragtime's European bass."
Blues.
African genesis.
Blues is the name given to both a musical form and a music genre, which originated in African-American communities of primarily the "Deep South" of the United States at the end of the 19th century from their spirituals, work songs, field hollers, shouts and chants and rhymed simple narrative ballads.
The African use of pentatonic scales contributed to the development of blue notes in blues and jazz. As Kubik explains:
Many of the rural blues of the Deep South are "stylistically" an extension and merger of basically two broad accompanied song-style traditions in the west central Sudanic belt:
W. C. Handy: early published blues.
W. C. Handy became intrigued by the folk blues of the Deep South whilst traveling through the Mississippi Delta. In this folk blues form, the singer would improvise freely within a limited melodic range, sounding like a field holler, and the guitar accompaniment was slapped rather than strummed, like a small drum which responded in syncopated accents, functioning as another "voice". Handy and his band members were formally trained African-American musicians who had not grown up with the blues, yet he was able to adapt the blues to a larger band instrument format, and arrange them in a popular music form.
Handy wrote about his adopting of the blues:
The primitive southern Negro, as he sang, was sure to bear down on the third and seventh tone of the scale, slurring between major and minor. Whether in the cotton field of the Delta or on the Levee up St. Louis way, it was always the same. Till then, however, I had never heard this slur used by a more sophisticated Negro, or by any white man. I tried to convey this effect ... by introducing flat thirds and sevenths (now called blue notes) into my song, although its prevailing key was major ..., and I carried this device into my melody as well.
The publication of his "Memphis Blues" sheet music in 1912 introduced the 12-bar blues to the world (although Gunther Schuller argues that it is not really a blues, but "more like a cakewalk"). This composition, as well as his later "St. Louis Blues" and others, included the habanera rhythm, and would become jazz standards. Handy's music career began in the pre-jazz era, and contributed to the codification of jazz through the publication of some of the first jazz sheet music.
Within the context of Western harmony.
The blues form which is ubiquitous in jazz is characterized by specific chord progressions, of which the twelve-bar blues progression is the most common. An important part of the sound are the blue notes which, for expressive purposes, are sung or played flattened, or gradually bent (minor 3rd to major 3rd) in relation to the pitch of the major scale. The blues were the key that opened up an entirely new approach to Western harmony, ultimately leading to a high level of harmonic complexity in jazz.
New Orleans.
The music of New Orleans had a profound effect on the creation of early jazz. Many early jazz performers played in venues throughout the city, such as the brothels and bars of the red-light district around Basin Street, known as "Storyville". In addition to dance bands, there were numerous marching bands who played at lavish funerals (later called jazz funerals), which were arranged by the African-American and European American communities. The instruments used in marching bands and dance bands became the basic instruments of jazz: brass, reeds tuned in the European 12-tone scale, and drums. Small bands which mixed self-taught and well educated African-American musicians, many of whom came from the funeral-procession tradition of New Orleans, played a seminal role in the development and dissemination of early jazz. These bands travelled throughout Black communities in the Deep South and, from around 1914 onwards, Afro-Creole and African-American musicians played in vaudeville shows which took jazz to western and northern US cities.
Syncopation.
The cornetist Buddy Bolden led a band who are often mentioned as one of the prime originators of the style later to be called "jazz". He played in New Orleans around 1895–1906, before developing a mental illness; there are no recordings of him playing. Bolden's band is credited with creating the "big four", the first syncopated bass drum pattern to deviate from the standard on-the-beat march. As the example below shows, the second half of the big four pattern is the habanera rhythm.
Afro-Creole pianist Jelly Roll Morton began his career in Storyville. From 1904, he toured with vaudeville shows around southern cities, also playing in Chicago and New York. In 1905 he composed his "Jelly Roll Blues", which on its publication in 1915 became the first jazz arrangement in print, introducing more musicians to the New Orleans style.
Morton considered the tresillo/habanera (which he called the Spanish tinge) to be an essential ingredient of jazz. In his own words: Now in one of my earliest tunes, "New Orleans Blues," you can notice the Spanish tinge. In fact, if you can't manage to put tinges of Spanish in your tunes, you will never be able to get the right seasoning, I call it, for jazz.
Morton was a crucial innovator in the evolution from the early jazz form known as ragtime to jazz piano, and could perform pieces in either style; in 1938, Morton made a series of recordings for the Library of Congress, in which he demonstrated the difference between the two styles. Morton's solos however were still close to ragtime, and were not merely improvisations over chord changes as in later jazz; but his use of the blues was of equal importance.
Swing.
Morton loosened ragtime's rigid rhythmic feeling, decreasing its embellishments and employing a swing feeling. Swing is the most important and enduring African-based rhythmic technique used in jazz. An oft quoted definition of swing by Louis Armstrong is: "if you don't feel it, you'll never know it." "The New Harvard Dictionary of Music" states that swing is: "An intangible rhythmic momentum in jazz ... Swing defies analysis; claims to its presence may inspire arguments." The dictionary does nonetheless provide the useful description of triple subdivisions of the beat contrasted with duple subdivisions: swing superimposes six subdivisions of the beat over a basic pulse structure or four subdivisions. This aspect of swing is far more prevalent in African-American music than in Afro-Caribbean music. One aspect of swing, which is heard in more rhythmically complex Diaspora musics, places strokes in-between the triple and duple-pulse "grids".
New Orleans brass bands are a lasting influence, contributing horn players to the world of professional jazz with the distinct sound of the city whilst helping black children escape poverty. The leader of New Orleans' Camelia Brass Band, D'Jalma Ganier, taught Louis Armstrong to play trumpet; Armstrong would then popularize the New Orleans style of trumpet playing, and then expand it. Like Jelly Roll Morton, Armstrong is also credited with the abandonment of ragtime's stiffness in favor of swung notes. Armstrong, perhaps more than any other musician, codified the rhythmic technique of swing in jazz, and broadened the jazz solo vocabulary.
The Original Dixieland Jass Band made the music's first recordings early in 1917, and their "Livery Stable Blues" became the earliest released jazz record. That year, numerous other bands made recordings featuring "jazz" in the title or band name, but most were ragtime or novelty records rather than jazz. In February 1918 during World War I, James Reese Europe's "Hellfighters" infantry band took ragtime to Europe, then on their return recorded Dixieland standards including "Darktown Strutters' Ball".
Other regions.
In the northeastern United States, a "hot" style of playing ragtime had developed, notably James Reese Europe's symphonic Clef Club orchestra in New York, which played a benefit concert at Carnegie Hall in 1912. The Baltimore rag style of Eubie Blake influenced James P. Johnson's development of stride piano playing, in which the right hand plays the melody, while the left hand provides the rhythm and bassline.
In Ohio and elsewhere in the midwest the major influence was ragtime, until about 1919. Around 1912, when the four-string banjo and saxophone came in, musicians began to improvise the melody line, but the harmony and rhythm remained unchanged. A contemporary account states that blues could only be heard in jazz in the gut-bucket cabarets, which were generally looked down upon by the Black middle-class.
1920s and 1930s.
The Jazz Age.
 From 1920 to 1933 Prohibition in the United States banned the sale of alcoholic drinks, resulting in illicit speakeasies which became lively venues of the "Jazz Age", hosting popular music including current dance songs, novelty songs and show tunes.
Jazz began to get a reputation as being immoral, and many members of the older generations saw it as threatening the old cultural values and promoting the new decadent values of the Roaring 20s. Professor Henry van Dyke of Princeton University wrote: "... it is not music at all. It's merely an irritation of the nerves of hearing, a sensual teasing of the strings of physical passion." The media too began to denigrate jazz. "The New York Times" used stories and headlines to pick at jazz: Siberian villagers were said by the paper to have used jazz to scare off bears, when in fact they had used pots and pans; another story claimed that the death of a celebrated conductor had been caused by jazz, rather than his fatal heart attack (natural cause).
From 1919, Kid Ory's Original Creole Jazz Band of musicians from New Orleans played in San Francisco and Los Angeles, where in 1922 they became the first black jazz band of New Orleans origin to make recordings. That year also saw the first recording by Bessie Smith, the most famous of the 1920s blues singers. Chicago meanwhile was the main center developing the new "Hot Jazz", where King Oliver joined Bill Johnson. Bix Beiderbecke formed The Wolverines in 1924.
In 1924, Louis Armstrong joined the Fletcher Henderson dance band for a year, as featured soloist. The original New Orleans style was polyphonic, with theme variation and simultaneous collective improvisation. Armstrong was a master of his hometown style, but by the time he joined Henderson's band, he was already a trailblazer in a new phase of jazz, with its emphasis on arrangements and soloists. Armstrong's solos went well beyond the theme-improvisation concept, and extemporized on chords, rather than melodies. According to Schuller, by comparison, the solos by Armstrong's bandmates (including a young Coleman Hawkins), sounded "stiff, stodgy," with "jerky rhythms and a grey undistinguished tone quality." The following example shows a short excerpt of the straight melody of "Mandy, Make Up Your Mind" by George W. Meyer and Arthur Johnston (top), compared with Armstrong's solo improvisations (below) (recorded 1924). (The example approximates Armstrong's solo, as it doesn't convey his use of swing.)
Armstrong's solos were a significant factor in making jazz a true 20th-century language. After leaving Henderson's group, Armstrong formed his virtuosic Hot Five band, where he popularized scat singing.
Jelly Roll Morton recorded with the New Orleans Rhythm Kings in an early mixed-race collaboration, then in 1926 formed his Red Hot Peppers. There was a larger market for jazzy dance music played by white orchestras, such as Jean Goldkette's orchestra and Paul Whiteman's orchestra. In 1924 Whiteman commissioned Gershwin's "Rhapsody in Blue", which was premiered by Whiteman's Orchestra. Other influential large ensembles included Fletcher Henderson's band, Duke Ellington's band (which opened an influential residency at the Cotton Club in 1927) in New York, and Earl Hines' Band in Chicago (who opened in The Grand Terrace Cafe there in 1928). All significantly influenced the development of big band-style swing jazz. By 1930, the New Orleans-style ensemble was a relic, and jazz belonged to the world.
Swing.
The 1930s belonged to popular swing big bands, in which some virtuoso soloists became as famous as the band leaders. Key figures in developing the "big" jazz band included bandleaders and arrangers Count Basie, Cab Calloway, Jimmy and Tommy Dorsey, Duke Ellington, Benny Goodman, Fletcher Henderson, Earl Hines, Glenn Miller and Artie Shaw. Although it was a collective sound, swing also offered individual musicians a chance to "solo" and improvise melodic, thematic solos which could at times be very complex and "important" music.
Swing was also dance music. It was broadcast on the radio "live" nightly across America for many years, especially by Earl Hines and his Grand Terrace Cafe Orchestra broadcasting coast-to-coast from Chicago (well placed for "live" US time-zones).
Over time, social strictures regarding racial segregation began to relax in America: white bandleaders began to recruit black musicians and black bandleaders white ones. In the mid-1930s, Benny Goodman hired pianist Teddy Wilson, vibraphonist Lionel Hampton and guitarist Charlie Christian to join small groups. In the 1930s, Kansas City Jazz as exemplified by tenor saxophonist Lester Young marked the transition from big bands to the bebop influence of the 1940s. An early 1940s style known as "jumping the blues" or jump blues used small combos, uptempo music and blues chord progressions, drawing on boogie-woogie from the 1930s.
Beginnings of European jazz.
As only a limited amount of American jazz records were released in Europe, European jazz traces many of its roots to American artists such as James Reese Europe, Paul Whiteman and Lonnie Johnson, who visited Europe during and after World War I. It was their live performances which inspired European audiences' interest in jazz, as well as the interest in all things American (and therefore exotic) which accompanied the economic and political woes of Europe during this time. The beginnings of a distinct European style of jazz began to emerge in this interwar period.
This distinct style entered full swing in France with the Quintette du Hot Club de France, which began in 1934. Much of this French jazz was a combination of African-American jazz and the symphonic styles in which French musicians were well-trained; in this, it is easy to see the inspiration taken from Paul Whiteman, since his style was also a fusion of the two. Belgian guitar virtuoso Django Reinhardt popularized gypsy jazz, a mix of 1930s American swing, French dance hall "musette" and Eastern European folk with a languid, seductive feel; the main instruments are steel stringed guitar, violin, and double bass, and solos pass from one player to another as the guitar and bass play the role of the rhythm section. Some music researchers hold that it was Philadelphia's Eddie Lang and Joe Venuti who pioneered the guitar-violin partnership typical of the genre, which was brought to France after they had been heard live or on Okeh Records in the late 1920s.
1940s and 1950s.
"American music"—the influence of Ellington.
By the 1940s, Duke Ellington's music had transcended the bounds of swing, bridging jazz and art music in a natural synthesis. Ellington called his music "American Music" rather than jazz, and liked to describe those who impressed him as "beyond category." These included many of the musicians who were members of his orchestra, some of whom are considered among the best in jazz in their own right, but it was Ellington who melded them into one of the most well-known jazz orchestral units in the history of jazz. He often composed specifically for the style and skills of these individuals, such as "Jeep's Blues" for Johnny Hodges, "Concerto for Cootie" for Cootie Williams (which later became "Do Nothing Till You Hear from Me" with Bob Russell's lyrics), and "The Mooche" for Tricky Sam Nanton and Bubber Miley. He also recorded songs written by his bandsmen, such as Juan Tizol's "Caravan" and "Perdido", which brought the "Spanish Tinge" to big-band jazz. Several members of the orchestra remained with him for several decades. The band reached a creative peak in the early 1940s, when Ellington and a small hand-picked group of his composers and arrangers wrote for an orchestra of distinctive voices who displayed tremendous creativity.
Bebop.
In the early 1940s, bebop-style performers began to shift jazz from danceable popular music towards a more challenging "musician's music." The most influential bebop musicians included saxophonist Charlie Parker, pianists Bud Powell and Thelonious Monk, trumpeters Dizzy Gillespie and Clifford Brown, and drummer Max Roach. Divorcing itself from dance music, bebop established itself more as an art form, thus lessening its potential popular and commercial appeal.
Composer Gunther Schuller wrote: 
 ... In 1943 I heard the great Earl Hines band which had Bird in it and all those other great musicians. They were playing all the flatted fifth chords and all the modern harmonies and substitutions and Dizzy Gillespie runs in the trumpet section work. Two years later I read that that was 'bop' and the beginning of modern jazz ... but the band never made recordings. 
Dizzy Gillespie wrote: 
 ... People talk about the Hines band being 'the incubator of bop' and the leading exponents of that music ended up in the Hines band. But people also have the erroneous impression that the music was new. It was not. The music evolved from what went before. It was the same basic music. The difference was in how you got from here to here to here ... naturally each age has got its own shit.
Rhythm.
Since bebop was meant to be listened to, not danced to, it could use faster tempos. Drumming shifted to a more elusive and explosive style, in which the ride cymbal was used to keep time while the snare and bass drum were used for accents. This led to a highly syncopated linear rhythmic complexity.
Harmony.
Bebop musicians employed several harmonic devices which were not previously typical in jazz, engaging in a more abstracted form of chord-based improvisation. Bebop scales are traditional scales with an added chromatic passing note; bebop also uses "passing" chords, substitute chords, and altered chords. New forms of chromaticism and dissonance were introduced into jazz, and the dissonant tritone (or "flatted fifth") interval became the "most important interval of bebop" Chord progressions for bebop tunes were often taken directly from popular swing-era songs and reused with a new and more complex melody to form new compositions, a practice which was already well-established in earlier jazz, but came to be central to the bebop style. Bebop made use of several relatively common chord progressions, such as blues (at base, I-IV-V, but infused with II-V motion) and 'rhythm changes' (I-VI-II-V) - the chords to the 1930s pop standard "I Got Rhythm." Late bop also moved towards extended forms that represented a departure from pop and show tunes.
The harmonic development in bebop is often traced back to a transcendent moment experienced by Charlie Parker while performing "Cherokee" at Clark Monroe's Uptown House, New York, in early 1942:
 I'd been getting bored with the stereotyped changes that were being used, ... and I kept thinking there's bound to be something else. I could hear it sometimes. I couldn't play it... I was working over 'Cherokee,' and, as I did, I found that by using the higher intervals of a chord as a melody line and backing them with appropriately related changes, I could play the thing I'd been hearing. It came alive—Parker.
Gerhard Kubik postulates that the harmonic development in bebop sprang from the blues and other African-related tonal sensibilities, rather than 20th-century Western art music as some have suggested: 
 Auditory inclinations were the African legacy in [Parker's] life, reconfirmed by the experience of the blues tonal system, a sound world at odds with the Western diatonic chord categories. Bebop musicians eliminated Western-style functional harmony in their music while retaining the strong central tonality of the blues as a basis for drawing upon various African matrices.
Samuel Floyd states that blues were both the bedrock and propelling force of bebop, bringing about three main developments:
As Kubik explained:
While for an outside observer, the harmonic innovations in bebop would appear to be inspired by experiences in Western "serious" music, from Claude Debussy to Arnold Schoenberg, such a scheme cannot be sustained by the evidence from a cognitive approach. Claude Debussy did have some influence on jazz, for example, on Bix Beiderbecke's piano playing. And it is also true that Duke Ellington adopted and reinterpreted some harmonic devices in European contemporary music. West Coast jazz would run into such debts as would several forms of cool jazz, but bebop has hardly any such debts in the sense of direct borrowings. On the contrary, ideologically, bebop was a strong statement of rejection of any kind of eclecticism, propelled by a desire to activate something deeply buried in self. Bebop then revived tonal-harmonic ideas transmitted through the blues and reconstructed and expanded others in a basically non-Western harmonic approach. The ultimate significance of all this is that the experiments in jazz during the 1940s brought back to African-American music several structural principles and techniques rooted in African traditions
These divergences from the jazz mainstream of the time initially met with a divided, sometimes hostile, response among fans and fellow musicians, especially established swing players, who bristled at the new harmonic sounds. To hostile critics, bebop seemed to be filled with "racing, nervous phrases". But despite the initial friction, by the 1950s bebop had become an accepted part of the jazz vocabulary.
Afro-Cuban jazz (cu-bop).
Machito and Mario Bauza.
The general consensus among musicians and musicologists is that the first original jazz piece to be overtly based in clave was "Tanga" (1943), composed by Cuban-born Mario Bauza and recorded by Machito and his Afro-Cubans in New York City. "Tanga" began as a spontaneous descarga (Cuban jam session), with jazz solos superimposed on top.
This was the birth of Afro-Cuban jazz. The use of clave brought the African "timeline", or "key pattern", into jazz. Music organized around key patterns convey a two-celled (binary) structure, which is a complex level of African cross-rhythm. Within the context of jazz however, harmony is the primary referent, not rhythm. The harmonic progression can begin on either side of clave, and the harmonic "one" is always understood to be "one". If the progression begins on the "three-side" of clave, it is said to be in "3-2 clave". If the progression begins on the "two-side", its in "2-3 clave".
Bobby Sanabria mentions several innovations of Machito's Afro-Cubans, citing them as the first band: to wed big band jazz arranging techniques within an original composition, with jazz oriented soloists utilizing an authentic Afro-Cuban based rhythm section in a successful manner; to explore modal harmony (a concept explored much later by Miles Davis and Gil Evans) from a jazz arranging perspective; and to "overtly" explore the concept of clave conterpoint from an arranging standpoint (the ability to weave seamlessly from one side of the clave to the other without breaking its rhythmic integrity within the structure of a musical arrangement). They were also the first band in the United States to publicly utilize the term "Afro-Cuban" as the band's moniker, thus identifying itself and acknowledging the West African roots of the musical form they were playing. It forced New York City's Latino and African-American communities to deal with their common West African musical roots in a direct way, whether they wanted to acknowledge it publicly or not.
Dizzy Gillespie and Chano Pozo.
It was Mario Bauzá who introduced bebop innovator Dizzy Gillespie to the Cuban conga drummer and composer Chano Pozo. Gillespie and Pozo's brief collaboration produced some of the most enduring Afro-Cuban jazz standards. "Manteca" (1947) is the first jazz standard to be rhythmically based on clave. According to Gillespie, Pozo composed the layered, contrapuntal guajeos (Afro-Cuban ostinatos) of the A section and the introduction, while Gillespie wrote the bridge. Gillespie recounted: "If I'd let it go like [Chano] wanted it, it would have been strickly Afro-Cuban all the way. There wouldn't have been a bridge. I thought I was writing an eight-bar bridge, but ... I had to keep going and ended up writing a sixteen-bar bridge." The bridge gave "Manteca" a typical jazz harmonic structure, setting the piece apart from Bauza's modal "Tanga" of a few years earlier.
Gillespie's collaboration with Pozo brought specific African-based rhythms into bebop. While pushing the boundaries of harmonic improvisation, "cu-bop", as it was called, also drew more directly from African rhythmic structures. Jazz arrangements with a "Latin" A section and a swung B section, with all choruses swung during solos, became common practice with many "Latin tunes" of the jazz standard repertoire. This approach can be heard on pre-1980 recordings of "Manteca", "A Night in Tunisia", "Tin Tin Deo", and "On Green Dolphin Street".
African cross-rhythm.
Cuban percussionist Mongo Santamaria first recorded his composition "Afro Blue" in 1959.
"Afro Blue" was the first jazz standard built upon a typical African three-against-two (3:2) cross-rhythm, or hemiola. The song begins with the bass repeatedly playing 6 cross-beats per each measure of 12/8, or 6 cross-beats per 4 main beats—6:4 (two cells of 3:2). The following example shows the original ostinato "Afro Blue" bass line; the slashed noteheads indicate the main beats (not bass notes), where you would normally tap your foot to "keep time."
When John Coltrane covered "Afro Blue" in 1963, he inverted the metric hierarchy, interpreting the tune as a 3/4 jazz waltz with duple cross-beats superimposed (2:3). Originally a Bb pentatonic blues, Coltrane expanded the harmonic structure of "Afro Blue."
Perhaps the most respected Afro-cuban jazz combo of the late 1950s was vibraphonist Cal Tjader's band. Tjader had Mongo Santamaria, Armando Peraza, and Willie Bobo on his early recording dates.
Dixieland revival.
In the late 1940s there was a revival of "Dixieland" music, harking back to the original contrapuntal New Orleans style. This was driven in large part by record company reissues of early jazz classics by the Oliver, Morton, and Armstrong bands of the 1930s. There were two types of musicians involved in the revival: the first group was made up of those who had begun their careers playing in the traditional style and were returning to it (or continuing what they had been playing all along), such as Bob Crosby's Bobcats, Max Kaminsky, Eddie Condon, and Wild Bill Davison. Most of these players were originally Midwesterners, although there were a small number of New Orleans musicians involved. The second group of revivalists consisted of younger musicians, such as those in the Lu Watters band, Conrad Janis, and Ward Kimball and his Firehouse Five Plus Two Jazz Band. By the late 1940s, Louis Armstrong's Allstars band became a leading ensemble. Through the 1950s and 1960s, Dixieland was one of the most commercially popular jazz styles in the US, Europe, and Japan, although critics paid little attention to it.
Cool jazz.
By the end of the 1940s, the nervous energy and tension of bebop was replaced with a tendency towards calm and smoothness with the sounds of cool jazz, which favoured long, linear melodic lines. It emerged in New York City, and dominated jazz in the first half of the 1950s. The starting point was a collection of 1949 and 1950 singles by a nonet led by Miles Davis, released as the "Birth of the Cool". Later cool jazz recordings by musicians such as Chet Baker, Dave Brubeck, Bill Evans, Gil Evans, Stan Getz and the Modern Jazz Quartet usually had a "lighter" sound that avoided the aggressive tempos and harmonic abstraction of bebop.
Cool jazz later became strongly identified with the West Coast jazz scene, but also had a particular resonance in Europe, especially Scandinavia, where figures such as baritone saxophonist Lars Gullin and pianist Bengt Hallberg emerged. The theoretical underpinnings of cool jazz were set out by the Chicago pianist Lennie Tristano, and its influence stretches into such later developments as bossa nova, modal jazz, and even free jazz.
Hard bop.
Hard bop is an extension of bebop (or "bop") music which incorporates influences from rhythm and blues, gospel music and blues, especially in the saxophone and piano playing. Hard bop was developed in the mid-1950s, coalescing in 1953 and 1954; it developed partly in response to the vogue for cool jazz in the early 1950s, and paralleled the rise of rhythm and blues. Miles Davis' 1954 performance of "Walkin'" at the first Newport Jazz Festival announced the style to the jazz world. The quintet Art Blakey and the Jazz Messengers, fronted by Blakey and featuring pianist Horace Silver and trumpeter Clifford Brown, were leaders in the hard bop movement along with Davis.
Modal jazz.
Modal jazz is a development which began in the later 1950s which takes the mode, or musical scale, as the basis of musical structure and improvisation. Previously, a solo was meant to fit into a given chord progression, but with modal jazz the soloist creates a melody using one, or a small number of modes. The emphasis is thus shifted from harmony to melody: "Historically, this caused a seismic shift among jazz musicians, away from thinking vertically (the chord), and towards a more horizontal approach (the scale)," explained pianist Mark Levine.
The modal theory stems from a work by George Russell. Miles Davis introduced the concept to the greater jazz world with "Kind of Blue" (1959), an exploration of the possibilities of modal jazz which would become the best selling jazz album of all time. In contrast to Davis' earlier work with hard bop and its complex chord progression and improvisation, the entire "Kind of Blue" album was composed as a series of "modal sketches", in which each performer was given a set of scales that defined the parameters of their improvisation and style. "I didn't write out the music for "Kind of Blue", but brought in sketches for what everybody was supposed to play because I wanted a lot of spontaneity," recalled Davis. The track "So What" has only two chords: D-7 and E♭-7.
Other innovators in this style include Jackie McLean, and two of the musicians who had also played on "Kind of Blue": John Coltrane and Bill Evans.
By the 1950s, Afro-Cuban jazz had been using modes for at least a decade, as much of it borrowed from Cuban popular dance forms which are structured around multiple ostinatos with only a few chords. A case in point is Mario Bauza's "Tanga" (1943), the first Afro-Cuban jazz piece. Machito's Afro-Cubans recorded modal tunes in the 1940s, featuring jazz soloists such as Howard McGhee, Brew Moore, Charlie Parker and Flip Phillips. However, there is no evidence that Davis or other mainstream jazz musicians were influenced by the use of modes in Afro-Cuban jazz, or other branches of Latin jazz.
Free jazz.
Free jazz, and the related form of avant-garde jazz, broke through into an open space of "free tonality" in which meter, beat, and formal symmetry all disappeared, and a range of World music from India, Africa and Arabia were melded into an intense, even religiously ecstatic or orgiastic style of playing. While loosely inspired by bebop, free jazz tunes gave players much more latitude; the loose harmony and tempo was deemed controversial when this approach was first developed. The bassist Charles Mingus is also frequently associated with the avant-garde in jazz, although his compositions draw from myriad styles and genres.
The first major stirrings came in the 1950s, with the early work of Ornette Coleman and Cecil Taylor. In the 1960s, exponents included Archie Shepp, Sun Ra, Albert Ayler, Pharaoh Sanders and John Coltrane. In developing his late style, Coltrane was especially influenced by the dissonance of Ayler's trio with bassist Gary Peacock and drummer Sunny Murray, a rhythm section honed with Cecil Taylor as leader. Coltrane championed many younger free jazz musicians, notably Archie Shepp), and under his influence Impulse! Records became a leading free jazz record label.
A series of recordings with the Classic Quartet in the first half of 1965 show Coltrane's playing becoming increasingly abstract, with greater incorporation of devices like multiphonics, utilization of overtones, and playing in the altissimo register, as well as a mutated return to Coltrane's sheets of sound. In the studio, he all but abandoned his soprano to concentrate on the tenor saxophone. In addition, the quartet responded to the leader by playing with increasing freedom. The group's evolution can be traced through the recordings "The John Coltrane Quartet Plays", "Living Space" and "Transition" (both June 1965), "New Thing at Newport" (July 1965), "Sun Ship" (August 1965) and "First Meditations" (September 1965).
In June 1965, Coltrane and ten other musicians recorded "Ascension", a 40-minute long piece that included adventurous solos by young avant-garde musicians as well as Coltrane, and was controversial primarily for the collective improvisation sections that separated the solos. After recording with the quartet over the next few months, Coltrane invited Pharoah Sanders to join the band in September 1965. While Coltrane used over-blowing frequently as an emotional exclamation-point, Sanders would opt to overblow his entire solo, resulting in a constant screaming and screeching in the altissimo range of the instrument.
Free jazz quickly found a foothold in Europe, in part because musicians such as Ayler, Taylor, Steve Lacy and Eric Dolphy spent extended periods there. A distinctive European contemporary jazz (often incorporating elements of free jazz but not limited to it) also flourished because of the emergence of European musicians (such as John Surman, Zbigniew Namyslowski, Albert Mangelsdorff, Kenny Wheeler and Mike Westbrook) who were anxious to develop new approaches reflecting their national and regional musical cultures and contexts. Ever since the 1960s, various creative centers of jazz have developed in Europe, such as the creative jazz scene in Amsterdam. Following the work of veteran drummer Han Bennink and pianist Misha Mengelberg, musicians started to explore free music by collectively improvising until a certain form (melody, rhythm, or even famous song) is found by the band. Jazz critic Kevin Whithead documented the free jazz scene in Amsterdam and some of its main exponents such as the ICP (Instant Composers Pool) orchestra in his book "New Dutch Swing". Throughout the 1990s and 2000s, Keith Jarrett has been prominent in defending free jazz from criticism by traditionalists .
1960s and 1970s.
Latin jazz.
Latin jazz is the term used to describe jazz which employs Latin American rhythms, and is generally understood to have a more specific meaning than simply jazz from Latin America. A more precise term might be Afro-Latin jazz, as the jazz subgenre typically employs rhythms that either have a direct analog in Africa, or exhibit an African rhythmic influence beyond what is ordinarily heard in other jazz. The two main categories of Latin jazz are Afro-Cuban jazz and Brazilian jazz.
In the 1960s and 1970s many jazz musicians had only a basic understanding of Cuban and Brazilian music, and jazz compositions which used Cuban or Brazilian elements were often referred to as "Latin tunes", with no distinction between a Cuban son montuno and a Brazilian bossa nova. Even as late as 2000, in Mark Gridley's "Jazz Styles: History and Analysis", a bossa nova bass line is referred to as a "Latin bass figure." It was not uncommon during the 1960s and 1970s to hear a conga playing a Cuban tumbao while the drumset and bass played a Brazilian bossa nova pattern. Many jazz standards such as "Manteca", "On Green Dolphin Street" and "Song for My Father" have a "Latin" A section and a swung B section. Typically, the band would only play an even-eighth "Latin" feel in the A section of the head, and swing throughout all of the solos. Latin jazz specialists like Cal Tjader tended to be the exception. For example, on a 1959 live Tjader recording of "A Night in Tunisia", pianist Vince Guaraldi soloed through the entire form over an authentic mambo.
Afro-Cuban jazz.
Afro-Cuban jazz often uses Afro-Cuban instruments such as congas, timbales, güiro and claves, combined with piano, double bass, etc. Afro-Cuban jazz began with Machito's Afro-Cubans in the early 1940s, but took off and entered the mainstream in the late 1940s when bebop musicians such as Dizzy Gillespie and Billy Taylor began experimenting with Cuban rhythms. Mongo Santamaria and Cal Tjader further refined the genre in the late 1950s. 
Although a great deal of Cuban-based Latin jazz is modal, Latin jazz is not always modal: it can be as harmonically expansive as post-bop jazz. For example, Tito Puente recorded an arrangement of "Giant Steps" done to an Afro-Cuban guaguancó. A Latin jazz piece may momentarily contract harmonically, as in the case of a percussion solo over a one or two-chord piano guajeo.
Guajeos.
Guajeo is the name for the typical Afro-Cuban ostinato melodies which are commonly used motifs in Latin jazz compositions. They originated in the genre known as son. Guajeos provide a rhythmic and melodic framework that may be varied within certain parameters, whilst still maintaining a repetitive - and thus "danceable" - structure. Most guajeos are rhythmically based on clave (rhythm). 
Guajeos are one of the most important elements of the vocabulary of Afro-Cuban descarga (jazz-inspired instrumental jams), providing a means of tension and resolution and a sense of forward momentum, within a relatively simple harmonic structure. The use of multiple, contrapuntal guajeos in Latin jazz facilitates simultaneous collective improvisation based on theme variation. In a way, this polyphonic texture is reminiscent of the original New Orleans style of jazz.
Afro-Cuban jazz renaissance.
For most of its history, Afro-Cuban jazz had been a matter of superimposing jazz phrasing over Cuban rhythms. But by the end of the 1970s a new generation of New York City musicians had emerged who were fluent in both salsa dance music and jazz, leading to a new level of integration of jazz and Cuban rhythms. This era of creativity and vitality is best represented by the Gonzalez brothers Jerry (congas and trumpet) and Andy (bass). During 1974-1976 they were members of one of Eddie Palmieri's most experimental salsa groups: salsa was the medium, but Palmieri was stretching the form in new ways. He incorporated parallel fourths, with McCoy Tyner-type vamps. The innovations of Palmieri, the Gonzalez brothers and others led to an Afro-Cuban jazz renaissance in New York City.
This occurred in parallel with developments in Cuba The first Cuban band of this new wave was Irakere. Their "Chékere-son" (1976) introduced a style of "Cubanized" bebop-flavored horn lines that departed from the more angular guajeo-based lines which were typical of Cuban popular music and Latin jazz up until that time. It was based on Charlie Parker's composition "Billie's Bounce", jumbled together in a way that fused clave and bebop horn lines. In spite of the ambivalence of some band members towards Irakere's Afro-Cuban folkloric / jazz fusion, their experiments forever changed Cuban jazz: their innovations are still heard in the high level of harmonic and rhythmic complexity in Cuban jazz, and in the jazzy and complex contemporary form of popular dance music known as timba.
Afro-Brazilian jazz.
Brazilian jazz such as bossa nova is derived from samba, with influences from jazz and other 20th-century classical and popular music styles. Bossa is generally moderately paced, with melodies sung in Portuguese or English, whilst he related term jazz-samba describes an adaptation of street samba into jazz. 
The bossa nova style was pioneered by Brazilians João Gilberto and Antônio Carlos Jobim, and was made popular by Elizete Cardoso's recording of "Chega de Saudade" on the "Canção do Amor Demais" LP. Gilberto's initial releases, and the 1959 film "Black Orpheus", achieved significant popularity in Latin America; this spread to North America via visiting American jazz musicians. The resulting recordings by Charlie Byrd and Stan Getz cemented bossa nova's popularity and led to a worldwide boom, with 1963's "Getz/Gilberto", numerous recordings by famous jazz performers such as Ella Fitzgerald and Frank Sinatra, and the eventual entrenchment of the bossa nova style as a lasting influence in world music.
Brazilian percussionists such as Airto Moreira and Naná Vasconcelos also influenced jazz internationally by introducing Afro-Brazilian folkloric instruments and rhythms into a wide variety of jazz styles, thus attracting a greater audience to them.
Post-bop.
Post-bop jazz is a form of small-combo jazz derived from earlier bop styles. The genre's origins lie in seminal work by John Coltrane, Miles Davis, Bill Evans, Charles Mingus, Wayne Shorter and Herbie Hancock. Generally, the term post-bop is taken to mean jazz from the mid-sixties onward that assimilates influence from hard bop, modal jazz, the avant-garde, and free jazz, without necessarily being immediately identifiable as any of the above.
Much post-bop was recorded on Blue Note Records. Key albums include "Speak No Evil" by Shorter; "The Real McCoy" by McCoy Tyner; "Maiden Voyage" by Hancock; "Miles Smiles" by Davis; and "Search for the New Land" by Lee Morgan (an artist not typically associated with the post-bop genre). Most post-bop artists worked in other genres as well, with a particularly strong overlap with later hard bop.
Soul jazz.
Soul jazz was a development of hard bop which incorporated strong influences from blues, gospel and rhythm and blues in music for small groups, often the organ trio, which partnered a Hammond organ player with a drummer and a tenor saxophonist. Unlike hard bop, soul jazz generally emphasized repetitive grooves and melodic hooks, and improvisations were often less complex than in other jazz styles. Horace Silver had a large influence on the soul jazz style, with songs that used funky and often gospel-based piano vamps. It often had a steadier "funk" style groove, different from the swing rhythms typical of much hard bop. Important soul jazz organists included Jimmy McGriff and Jimmy Smith and Johnny Hammond Smith, and influential tenor saxophone players included Eddie "Lockjaw" Davis and Stanley Turrentine.
African-inspired.
Themes.
There was a resurgence of interest in jazz and other forms of African-American cultural expression during the Black Arts Movement and Black nationalist period of the 1960s and 1970s. African themes became popular. There were many new jazz compositions with African-related titles: "Black Nile" (Wayne Shorter), "Blue Nile" (Alice Coltrane), "Obirin African" (Art Blakey), "Zambia" (Lee Morgan), "Appointment in Ghana" (Jackie McLean), "Marabi" (Cannonball Adderley), "Yoruba" (Hubert Laws), and many more. Pianist Randy Weston's music incorporated African elements, for example, the large-scale suite "Uhuru Africa" (with the participation of poet Langston Hughes) and "Highlife: Music From the New African Nations." Both Weston and saxophonist Stanley Turrentine covered the Nigerian Bobby Benson's piece "Niger Mambo", which features Afro-Caribbean and jazz elements within a West African Highlife style. Some musicians, including Pharaoh Sanders, Hubert Laws and Wayne Shorter, began using African instruments such as kalimbas, bells, beaded gourds and other instruments not traditional to jazz.
Rhythm.
During this period, there was an increased use of the typical African 12/8 cross-rhythmic structure in jazz. Herbie Hancock's "Succotash" on "Inventions and Dimensions" (1963) is an open-ended modal, 12/8 improvised jam. Hancock's pattern of attack-points, rather than the pattern of pitches, is the primary focus of his improvisations, accompanied by Paul Chambers on bass, and percussionist Osvaldo Martinez playing a traditional Afro-Cuban chekeré part, and Willie Bobo playing an Abakuá bell pattern on a snare drum with brushes.
The first jazz standard composed by a non-Latino to use an overt African 12/8 cross-rhythm was Wayne Shorter's "Footprints" (1967). On the version recorded on "Miles Smiles" by Miles Davis, the bass switches to a 4/4 tresillo figure at 2:20. "Footprints" is not, however, a Latin jazz tune: African rhythmic structures are accessed directly by Ron Carter (bass) and Tony Williams (drums) via the rhythmic sensibilities of swing. Throughout the piece, the four beats, whether sounded or not, are maintained as the temporal referent. In the example below, the main beats are indicated by slashed noteheads, which do not indicate bass notes.
Pentatonic scales.
The use of pentatonic scales was another African-associated trend. The use of pentatonic scales in Africa probably goes back thousands of years. McCoy Tyner perfected the use of the pentatonic scale in his solos. Tyner also used parallel fifths and fourths, which are common harmonies in West Africa.
The minor pentatonic scale is often used in blues improvisation. Like a blues scale, a minor pentatonic scale can be played over all of the chords in a blues. The following pentatonic lick was played over blues changes by Joe Henderson on Horace Silver's "African Queen" (1965).
Jazz pianist, theorist, and educator Mark Levine refers the scale generated by beginning on the fifth step of a pentatonic scale, as the "V pentatonic scale".
Levine points out that the V pentatonic scale works for all three chords of the standard II-V-I jazz progression. This is a very common progression, used in pieces such as Miles Davis' "Tune Up." The following example shows the V pentatonic scale over a II-V-I progression.
Accordingly, John Coltrane's "Giant Steps" (1960), with its 26 chords per 16 bars, can be played using only three pentatonic scales. Coltrane studied Nicolas Slonimsky's "Thesaurus of Scales and Melodic Patterns", which contains material that is virtually identical to portions of "Giant Steps". The harmonic complexity of "Giant Steps" is on the level of the most advanced 20th-century art music. Superimposing the pentatonic scale over "Giant Steps" is not merely a matter of harmonic simplification, but also a sort of "Africanizing" of the piece, which provides an alternate approach for soloing. Mark Levine observes that when mixed in with more conventional "playing the changes", pentatonic scales provide "structure and a feeling of increased space."
Jazz fusion.
In the late 1960s and early 1970s the hybrid form of jazz-rock fusion was developed by combining jazz improvisation with rock rhythms, electric instruments and the highly amplified stage sound of rock musicians such as Jimi Hendrix and Frank Zappa. Jazz fusion music often uses mixed meters, odd time signatures, syncopation, complex chords and harmonies. All Music Guide states that "until around 1967, the worlds of jazz and rock were nearly completely separate. [However, ...] as rock became more creative and its musicianship improved, and as some in the jazz world became bored with hard bop and did not want to play strictly avant-garde music, the two different idioms began to trade ideas and occasionally combine forces."
Miles Davis' new directions.
In 1969 Davis fully embraced the electric instrument approach to jazz with "In a Silent Way", which can be considered his first fusion album. Composed of two side-long suites edited heavily by producer Teo Macero, this quiet, static album would be equally influential upon the development of ambient music. As Davis recalls: "The music I was really listening to in 1968 was James Brown, the great guitar player Jimi Hendrix, and a new group who had just come out with a hit record, "Dance to the Music", Sly and the Family Stone... I wanted to make it more like rock. When we recorded "In a Silent Way" I just threw out all the chord sheets and told everyone to play off of that." Two contributors to "In a Silent Way" also joined organist Larry Young to create one of the early acclaimed fusion albums: "Emergency!" by The Tony Williams Lifetime.
Psychedelic-jazz.
"Bitches Brew".
Davis's "Bitches Brew" (1970) was his most successful of this era. Although inspired by rock and funk, Davis's fusion creations were original, and brought about a type of new avant-garde, electronic, psychedelic-jazz, as far from pop music as any other Davis work.
Herbie Hancock.
Davis alumnus, pianist Herbie Hancock, released four albums of the short-lived (1970–1973) psychedelic-jazz subgenre: "Mwandishi" (1972), "Crossings" (1973), and "Sextant" (1973). The rhythmic background was a mix of rock, funk, and African-type textures.
Musicians who worked with Davis formed the four most influential fusion groups: Weather Report and Mahavishnu Orchestra emerged in 1971 and were soon followed by Return to Forever and The Headhunters.
Weather Report.
Weather Report's debut album was in the electronic, psychedelic-jazz vein. The self-titled "Weather Report" (1971) caused a sensation in the jazz world on its arrival, thanks to the pedigree of the group's members (including percussionist Airto Moreira), and their unorthodox approach to their music. The album featured a softer sound than would be the case in later years (predominantly using acoustic bass, with Shorter exclusively playing soprano saxophone, and with no synthesizers involved) but is still considered a classic of early fusion. It built on the avant-garde experiments which Zawinul and Shorter had pioneered with Miles Davis on "Bitches Brew" (including an avoidance of head-and-chorus composition in favour of continuous rhythm and movement) but taking the music further. To emphasise the group's rejection of standard methodology, the album opened with the inscrutable avant-garde atmospheric piece "Milky Way" (created by Shorter's extremely muted saxophone inducing vibrations in Zawinul's piano strings while the latter pedalled the instrument). Down Beat described the album as "music beyond category" and awarded it Album of the Year in the magazine's polls that year. Weather Report's subsequent releases were creative funk-jazz works.
Jazz-rock.
Although some jazz purists protested the blend of jazz and rock, many jazz innovators crossed over from the contemporary hard bop scene into fusion. In addition to using the electric instruments of rock, such as the electric guitar, electric bass, electric piano and synthesizer keyboards, fusion also used the powerful amplification, "fuzz" pedals, wah-wah pedals, and other effects used by 1970s-era rock bands. Notable performers of jazz fusion included Miles Davis, Eddie Harris, keyboardists Joe Zawinul, Chick Corea, Herbie Hancock, vibraphonist Gary Burton, drummer Tony Williams, violinist Jean-Luc Ponty, guitarists Larry Coryell, Al Di Meola, John McLaughlin and Frank Zappa, saxophonist Wayne Shorter and bassists Jaco Pastorius and Stanley Clarke. Jazz fusion was also popular in Japan where the band Casiopea released over thirty fusion albums.
In the 21st century, almost all jazz has influences from other nations and styles of music, making jazz fusion as much a common practice as style.
Jazz-funk.
Developed by the mid-1970s, jazz-funk is characterized by a strong back beat (groove), electrified sounds, and often, the presence of electronic analog synthesizers. Jazz-funk also draws influences from traditional African music, Afro-Cuban rhythms and Jamaican reggae, notably Kingston bandleader Sonny Bradshaw. Another feature is the shift of proportions between composition and improvisation: arrangements, melody and overall writing were heavily emphasized. The integration of funk, soul and R&B music into jazz resulted in the creation of a genre whose spectrum is wide and ranges from strong jazz improvisation to soul, funk or disco with jazz arrangements, jazz riffs and jazz solos, and sometimes soul vocals.
Early examples are Herbie Hancock's Headhunters band and the Miles Davis album "On the Corner". The latter, from 1972, began Davis' foray into jazz-funk and was, he claimed, an attempt at reconnecting with the young black audience which had largely forsaken jazz for rock and funk. While there is a discernible rock and funk influence in the timbres of the instruments employed, other tonal and rhythmic textures, such as the Indian tambora and tablas, and Cuban congas and bongos, create a multi-layered soundscape. The album was a culmination of sorts of the "musique concrète" approach that Davis and producer Teo Macero had begun to explore in the late 1960s.
Other trends.
Musicians began improvising jazz tunes on unusual instruments, such as the jazz harp (Alice Coltrane), electrically amplified and wah-wah pedaled jazz violin (Jean-Luc Ponty), and bagpipes (Rufus Harley). Jazz continued to expand and change, influenced by other types of music, such as world music, avant garde classical music, and rock and pop music. Guitarist John McLaughlin's Mahavishnu Orchestra played a mix of rock and jazz infused with East Indian influences. The ECM record label began in Germany in the 1970s with artists including Keith Jarrett, Paul Bley, the Pat Metheny Group, Jan Garbarek, Ralph Towner, Kenny Wheeler, John Taylor, John Surman and Eberhard Weber, establishing a new chamber music aesthetic, featuring mainly acoustic instruments, and sometimes incorporating elements of world music and folk.
1980s.
In 1987, the US House of Representatives and Senate passed a bill proposed by Democratic Representative John Conyers, Jr. to define jazz as a unique form of American music stating, among other things, "... that jazz is hereby designated as a rare and valuable national American treasure to which we should devote our attention, support and resources to make certain it is preserved, understood and promulgated." It passed in the House of Representatives on September 23, 1987 and in the Senate on November 4, 1987.
Resurgence of traditionalism.
The 1980s saw something of a reaction against the Fusion and Free Jazz that had dominated the 1970s. One musician who emerged early in the decade was trumpeter Wynton Marsalis, who strove to create music within what he believed was the tradition, rejecting both fusion and free jazz and creating extensions of the small and large forms initially pioneered by such artists as Louis Armstrong and Duke Ellington as well as the hard bop of the 1950s. Whether Marsalis' critical and commercial success was a cause or a symptom of the reaction against Fusion and Free Jazz and the resurgence of interest in the kind of jazz pioneered in the 1960s (particularly Modal Jazz and Post-Bop) is debatable; nonetheless there were many other manifestations of a resurgence of traditionalism, even if Fusion and Free Jazz were by no means abandoned and continued to develop and evolve.
For example, several musicians who had been prominent in the fusion genre during the 1970s began to record acoustic jazz once more, including Chick Corea and Herbie Hancock. Other musicians who had experimented with electronic instruments in the previous decade had abandoned their use by the 1980s, such Bill Evans, Joe Henderson and Stan Getz. Even the 1980s music of Miles Davis, although still certainly fusion, adopted a far more accessible and recognisably jazz-oriented approach than his abstract work of the mid-1970s had done, such as a return to a theme-and-solos approach.
A similar reaction took place against free jazz: according to Ted Giola,
the very leaders of the avant garde started to signal a retreat from the core principles of Free Jazz. Anthony Braxton began recording standards over familiar chord changes. Cecil Taylor played duets in concert with Mary Lou Williams, and let her set out structured harmonies and familiar jazz vocabulary under his blistering keyboard attack. And the next generation of progressive players would be even more accommodating, moving inside and outside the changes without thinking twice. Musicians such as David Murray or Don Pullen may have felt the call of free-form jazz, but they never forgot all the other ways one could play African-American music for fun and profit.
Pianist Keith Jarrett, whose bands of the 1970s had played only original compositions with prominent free jazz elements, established his so-called 'Standards Trio' in 1983 which, although also occasionally exploring collective improvisation, has primarily performed and recorded jazz standards. Chick Corea similarly began exploring jazz standards in the 1980s having neglected them for the 1970s.
Smooth jazz.
In the early 1980s, a commercial form of jazz fusion called "pop fusion" or "smooth jazz" became successful and garnered significant radio airplay in "quiet storm" time slots at radio stations in urban markets across the U.S. This helped to establish or bolster the careers of vocalists including Al Jarreau, Anita Baker, Chaka Khan and Sade, as well as saxophonists including Grover Washington, Jr., Kenny G, Kirk Whalum, Boney James and David Sanborn. In general, smooth jazz is downtempo (the most widely played tracks are of 90–105 beats per minute), and has a lead, melody-playing instrument; saxophones—especially soprano and tenor—and legato electric guitar are popular.
In his "Newsweek" article "The Problem With Jazz Criticism" Stanley Crouch considers Miles Davis' playing of fusion as a turning point that led to smooth jazz. Critic Aaron J. West has countered the often negative perceptions of smooth jazz, stating: I challenge the prevalent marginalization and malignment of smooth jazz in the standard jazz narrative. Furthermore, I question the assumption that smooth jazz is an unfortunate and unwelcomed evolutionary outcome of the jazz-fusion era. Instead, I argue that smooth jazz is a long-lived musical style that merits multi-disciplinary analyses of its origins, critical dialogues, performance practice, and reception.
Acid jazz, nu jazz and jazz rap.
Acid jazz developed in the UK in the 1980s and 1990s, influenced by jazz-funk and electronic dance music. Jazz-funk musicians such as Roy Ayers and Donald Byrd are often credited as forerunners of acid jazz. While acid jazz often contains various types of electronic composition (sometimes including sampling or live DJ cutting and scratching), it is just as likely to be played live by musicians, who often showcase jazz interpretation as part of their performance.
Nu jazz is influenced by jazz harmony and melodies; there are usually no improvisational aspects. It ranges from combining live instrumentation with beats of jazz house, exemplified by St Germain, Jazzanova and Fila Brazillia, to more band-based improvised jazz with electronic elements, such as that of The Cinematic Orchestra, Kobol, and the Norwegian "future jazz" style pioneered by Bugge Wesseltoft, Jaga Jazzist, Nils Petter Molvær, and others. Nu jazz can be very experimental in nature and can vary widely in sound and concept.
Jazz rap developed in the late 1980s and early 1990s, and incorporates jazz influence into hip hop. In 1988, Gang Starr released the debut single "Words I Manifest", sampling Dizzy Gillespie's 1962 "Night in Tunisia", and Stetsasonic released "Talkin' All That Jazz", sampling Lonnie Liston Smith. Gang Starr's debut LP, "No More Mr. Nice Guy" (1989), and their track "Jazz Thing" (1990), sampled Charlie Parker and Ramsey Lewis. Groups making up the Native Tongues Posse tended towards jazzy releases; these include the Jungle Brothers' debut "Straight Out the Jungle" (1988), and A Tribe Called Quest's "People's Instinctive Travels and the Paths of Rhythm" (1990) and "The Low End Theory" (1991).
Rap duo Pete Rock & CL Smooth incorporated jazz influences on their 1992 debut "Mecca and the Soul Brother". Beginning in 1993, rapper Guru's Jazzmatazz series used jazz musicians during the studio recordings. Though jazz rap had achieved little mainstream success, Miles Davis' final album, "Doo-Bop" (released posthumously in 1992), was based around hip hop beats and collaborations with producer Easy Mo Bee. Davis' ex-bandmate Herbie Hancock returned to hip-hop influences in the mid-1990s, releasing the album "Dis Is Da Drum" in 1994.
Punk jazz and jazzcore.
The relaxation of orthodoxy concurrent with post-punk in London and New York City led to a new appreciation for jazz. In London, the Pop Group began to mix free jazz, along with dub reggae, into their brand of punk rock. In NYC, No Wave took direct inspiration from both free jazz and punk. Examples of this style include Lydia Lunch's "Queen of Siam", the work of James Chance and the Contortions, who mixed Soul with free jazz and punk, Gray, and the Lounge Lizards, who were the first group to call themselves "punk jazz."
John Zorn began to make note of the emphasis on speed and dissonance that was becoming prevalent in punk rock and incorporated this into free jazz. This began in 1986 with the album "Spy vs. Spy", a collection of Ornette Coleman tunes done in the contemporary thrashcore style. The same year, Sonny Sharrock, Peter Brötzmann, Bill Laswell, and Ronald Shannon Jackson recorded the first album under the name Last Exit, a similarly aggressive blend of thrash and free jazz. These developments are the origins of "jazzcore", the fusion of free jazz with hardcore punk.
M-Base.
The M-Base movement was started in the 1980s by a loose collective of young African-American musicians in New York that included Steve Coleman, Greg Osby, and Gary Thomas developed complex but grooving music. In the 1990s most M-Base participants turned to more conventional music, but Coleman, the most active participant, continued developing his music in accordance with the M-Base concept.
Coleman's audience decreased but his music and concepts influenced many musicians—both in terms of music technique and of the music's meaning. Hence, M-Base changed from a movement of a loose collective of young musicians to a kind of informal Coleman "school", with a much advanced but already originally implied concept.
1990s–2010s.
Jazz since the 1990s has been characterised by a pluralism in which no one style dominates but rather a wide range of active styles and genres are popular. Individual performers often play in a variety of styles, sometimes in the same performance. Pianist Brad Mehldau and power trio The Bad Plus have explored contemporary rock music within the context of the traditional jazz acoustic piano trio, for example recording instrumental jazz versions of songs by rock musicians. The Bad Plus have also incorporated elements of free jazz into their music. A firm avant-garde or free jazz stance has been maintained by some players, such as saxophonists Greg Osby and Charles Gayle, while others, such as James Carter, have incorporated free jazz elements into a more traditional framework.
On the other side, even a singer like Harry Connick, Jr. who has ten number-1 US so-called jazz albums, is sometimes called a jazz musician although there are just some elements from jazz history in his mainly pop orientated music. Also other new vocalists, such as Diana Krall, Norah Jones, Cassandra Wilson, Kurt Elling, and Jamie Cullum, have achieved popularity with a mix of traditional jazz and pop/rock forms.
Players emerging since the 1990s and usually performing in largely straight-ahead settings include pianists Jason Moran and Vijay Iyer, guitarist Kurt Rosenwinkel, vibraphonist Stefon Harris, trumpeters Roy Hargrove and Terence Blanchard, saxophonists Chris Potter and Joshua Redman, clarinetist Ken Peplowski, and bassist Christian McBride.
Although jazz-rock fusion reached the height of its popularity in the 1970s, the use of electronic instruments and rock-derived musical elements in jazz continued in the 1990s and 2000s. Musicians using this approach have included Pat Metheny, John Abercrombie, John Scofield, and Swedish group e.s.t.
Steve Coleman's music and M-Base concept gained recognition as "next logical step" after Charlie Parker, John Coltrane, and Ornette Coleman.
References.
</dl>

</doc>
<doc id="15614" url="http://en.wikipedia.org/wiki?curid=15614" title="Jonathan Swift">
Jonathan Swift

Jonathan Swift (30 November 1667 – 19 October 1745) was an Anglo-Irish satirist, essayist, political pamphleteer (first for the Whigs, then for the Tories), poet and cleric who became Dean of St Patrick's Cathedral, Dublin.
Swift is remembered for works such as "Gulliver's Travels", "A Modest Proposal", "A Journal to Stella", "Drapier's Letters", "The Battle of the Books", "An Argument Against Abolishing Christianity" and "A Tale of a Tub". He is regarded by the "Encyclopædia Britannica" as the foremost prose satirist in the English language, and is less well known for his poetry. He originally published all of his works under pseudonyms – such as Lemuel Gulliver, Isaac Bickerstaff, MB Drapier – or anonymously. He is also known for being a master of two styles of satire, the Horatian and Juvenalian styles.
Biography.
Youth.
Jonathan Swift was born in Dublin, Ireland. He was the second child and only son of Jonathan Swift (1640–1667) and his wife Abigail Erick (or Herrick), of Frisby on the Wreake. His father, a native of Goodrich, Herefordshire, accompanied his brothers to Ireland to seek their fortunes in law after their Royalist father's estate was brought to ruin during the English Civil War. Swift's father died in Dublin about seven months before he was born, and his mother returned to England. He was left in the care of his influential uncle, Godwin, a close friend and confidant of Sir John Temple, whose son later employed Swift as his secretary.
Swift's family had several interesting literary connections: his grandmother, Elizabeth (Dryden) Swift, was the niece of Sir Erasmus Dryden, grandfather of the poet John Dryden. The same grandmother's aunt, Katherine (Throckmorton) Dryden, was a first cousin of Elizabeth, wife of Sir Walter Raleigh. His great-great grandmother, Margaret (Godwin) Swift, was the sister of Francis Godwin, author of "The Man in the Moone" which influenced parts of Swift's "Gulliver's Travels". His uncle, Thomas Swift, married a daughter of the poet and playwright Sir William Davenant, a godson of William Shakespeare.
Swift's uncle Godwin Swift (1628–1695), a benefactor, took primary responsibility for the young Jonathan, sending him with one of his cousins to Kilkenny College (also attended by the philosopher George Berkeley). In 1682, financed by Godwin's son Willoughby, he attended Dublin University (Trinity College, Dublin), from which he received his B.A. in 1686, and developed his friendship with William Congreve. Swift was studying for his Master's degree when political troubles in Ireland surrounding the Glorious Revolution forced him in 1688 to leave for England, where his mother helped him get a position as secretary and personal assistant of Sir William Temple at Moor Park, Farnham. Temple was an English diplomat who, having arranged the Triple Alliance of 1668, had retired from public service to his country estate to tend his gardens and write his memoirs. Gaining his employer's confidence, Swift "was often trusted with matters of great importance". Within three years of their acquaintance, Temple had introduced his secretary to William III and sent him to London to urge the King to consent to a bill for triennial Parliaments.
When Swift took up his residence at Moor Park, he met Esther Johnson, then eight years old, the daughter of an impoverished widow who acted as companion to Temple's sister, Lady Giffard. Swift acted as her tutor and mentor, giving her the nickname "Stella", and the two maintained a close but ambiguous relationship for the rest of Esther's life.
In 1690, Swift left Temple for Ireland because of his health but returned to Moor Park the following year. The illness, fits of vertigo or giddiness – now known to be Ménière's disease—would continue to plague Swift throughout his life. During this second stay with Temple, Swift received his M.A. from Hart Hall, Oxford in 1692. Then, apparently despairing of gaining a better position through Temple's patronage, Swift left Moor Park to become an ordained priest in the Established Church of Ireland, and in 1694 he was appointed to the prebend of Kilroot in the Diocese of Connor, with his parish located at Kilroot, near Carrickfergus in County Antrim.
Swift appears to have been miserable in his new position, being isolated in a small, remote community far from the centres of power and influence. While at Kilroot, however, Swift may well have become romantically involved with Jane Waring, whom he called "Varina", the sister of an old college friend. A letter from him survives, offering to remain if she would marry him and promising to leave and never return to Ireland if she refused. She presumably refused, because Swift left his post and returned to England and Temple's service at Moor Park in 1696, and he remained there until Temple's death. There he was employed in helping to prepare Temple's memoirs and correspondence for publication. During this time Swift wrote "The Battle of the Books", a satire responding to critics of Temple's "Essay upon Ancient and Modern Learning" (1690), though "Battle" was not published until 1704.
Temple died on 27 January 1699. Swift, normally a harsh judge of human nature, said that all that was good and amiable in humankind had died with Temple. Swift stayed on briefly in England to complete the editing of Temple's memoirs, and perhaps in the hope that recognition of his work might earn him a suitable position in England. Unfortunately, Swift's work made enemies among some of Temple's family and friends, in particular Temple's formidable sister, Lady Giffard, who objected to indiscretions included in the memoirs. Swift's next move was to approach King William directly, based on his imagined connection through Temple and a belief that he had been promised a position. This failed so miserably that he accepted the lesser post of secretary and chaplain to the Earl of Berkeley, one of the Lords Justice of Ireland. However, when he reached Ireland he found that the secretaryship had already been given to another. He soon obtained the living of Laracor, Agher, and Rathbeggan, and the prebend of Dunlavin in St Patrick's Cathedral, Dublin.
At Laracor, just over four and half miles (7.5 km) from Summerhill, County Meath, and twenty miles (32 km) from Dublin, Swift ministered to a congregation of about fifteen and had abundant leisure for cultivating his garden, making a canal (after the Dutch fashion of Moor Park), planting willows, and rebuilding the vicarage. As chaplain to Lord Berkeley, he spent much of his time in Dublin and travelled to London frequently over the next ten years. In 1701, Swift anonymously published a political pamphlet, "A Discourse on the Contests and Dissentions in Athens and Rome".
Writer.
In February 1702, Swift received his Doctor of Divinity degree from Trinity College, Dublin. That spring he travelled to England and then returned to Ireland in October, accompanied by Esther Johnson—now 20—and his friend Rebecca Dingley, another member of William Temple's household. There is a great mystery and controversy over Swift's relationship with Esther Johnson, nicknamed "Stella". Many, notably his close friend Thomas Sheridan, believed that they were secretly married in 1716; others, like Swift's housekeeper Mrs Brent and Rebecca Dingley (who lived with Stella all through her years in Ireland) dismissed the story as absurd.
During his visits to England in these years, Swift published "A Tale of a Tub" and "The Battle of the Books" (1704) and began to gain a reputation as a writer. This led to close, lifelong friendships with Alexander Pope, John Gay, and John Arbuthnot, forming the core of the Martinus Scriblerus Club (founded in 1713).
Swift became increasingly active politically in these years. From 1707 to 1709 and again in 1710, Swift was in London unsuccessfully urging upon the Whig administration of Lord Godolphin the claims of the Irish clergy to the First-Fruits and Twentieths ("Queen Anne's Bounty"), which brought in about £2,500 a year, already granted to their brethren in England. He found the opposition Tory leadership more sympathetic to his cause, and, when they came to power in 1710, he was recruited to support their cause as editor of "The Examiner". In 1711, Swift published the political pamphlet "The Conduct of the Allies", attacking the Whig government for its inability to end the prolonged war with France. The incoming Tory government conducted secret (and illegal) negotiations with France, resulting in the Treaty of Utrecht (1713) ending the War of the Spanish Succession.
Swift was part of the inner circle of the Tory government, and often acted as mediator between Henry St John (Viscount Bolingbroke), the secretary of state for foreign affairs (1710–15), and Robert Harley (Earl of Oxford), lord treasurer and prime minister (1711–1714). Swift recorded his experiences and thoughts during this difficult time in a long series of letters to Esther Johnson, collected and published after his death as "A Journal to Stella". The animosity between the two Tory leaders eventually led to the dismissal of Harley in 1714. With the death of Queen Anne and accession of George I that year, the Whigs returned to power, and the Tory leaders were tried for treason for conducting secret negotiations with France.
Also during these years in London, Swift became acquainted with the Vanhomrigh family (Dutch merchants who had settled in Ireland, then moved to London) and became involved with one of the daughters, Esther, yet another fatherless young woman and another ambiguous relationship to confuse Swift's biographers. Swift furnished Esther with the nickname "Vanessa", and she features as one of the main characters in his poem "Cadenus and Vanessa". The poem and their correspondence suggest that Esther was infatuated with Swift, and that he may have reciprocated her affections, only to regret this and then try to break off the relationship. Esther followed Swift to Ireland in 1714, and settled at her old family home, Celbridge Abbey. Their uneasy relationship continued for some years; then there appears to have been a confrontation, possibly involving Esther Johnson. Esther Vanhomrigh died in 1723 at the age of 35, having destroyed the will she had made in Swift's favour. Another lady with whom he had a close but less intense relationship was Anne Long, a toast of the Kit-Cat Club.
Maturity.
Before the fall of the Tory government, Swift hoped that his services would be rewarded with a church appointment in England. However, Queen Anne appeared to have taken a dislike to Swift and thwarted these efforts. Her dislike has been attributed to "The Tale of a Tub", which she thought blasphemous, compounded by "The Windsor Prophecy", where Swift, with a surprising lack of tact, advised the Queen on which of her bedchamber ladies she should and which she should not trust. The best position his friends could secure for him was the Deanery of St Patrick's; this was not in the Queen's gift and Anne, who could be a bitter enemy, made it clear that Swift would not have received the preferment if she could have prevented it. With the return of the Whigs, Swift's best move was to leave England and he returned to Ireland in disappointment, a virtual exile, to live "like a rat in a hole".
Once in Ireland, however, Swift began to turn his pamphleteering skills in support of Irish causes, producing some of his most memorable works: "Proposal for Universal Use of Irish Manufacture" (1720), "Drapier's Letters" (1724), and "A Modest Proposal" (1729), earning him the status of an Irish patriot. This new role was unwelcome to the Government, which made clumsy attempts to silence him. His printer, Edward Waters, was convicted of seditious libel in 1720, but four years later a grand jury refused to find that the Drapier's Letters (which, though written under a pseudonym, were universally known to be Swift's work) were seditious. Swift responded with an attack on the Irish judiciary almost unparalleled in its ferocity, his principal target being the "vile and profligate villain" William Whitshed, Lord Chief Justice of Ireland.
Also during these years, he began writing his masterpiece, "Travels into Several Remote Nations of the World, in Four Parts, by Lemuel Gulliver, first a surgeon, and then a captain of several ships", better known as "Gulliver's Travels". Much of the material reflects his political experiences of the preceding decade. For instance, the episode in which the giant Gulliver puts out the Lilliputian palace fire by urinating on it can be seen as a metaphor for the Tories' illegal peace treaty; having done a good thing in an unfortunate manner. In 1726 he paid a long-deferred visit to London, taking with him the manuscript of "Gulliver's Travels". During his visit he stayed with his old friends Alexander Pope, John Arbuthnot and John Gay, who helped him arrange for the anonymous publication of his book. First published in November 1726, it was an immediate hit, with a total of three printings that year and another in early 1727. French, German, and Dutch translations appeared in 1727, and pirated copies were printed in Ireland.
Swift returned to England one more time in 1727 and stayed with Alexander Pope once again. The visit was cut short when Swift received word that Esther Johnson was dying, and rushed back home to be with her. On 28 January 1728, Esther Johnson died; Swift had prayed at her bedside, even composing prayers for her comfort. Swift could not bear to be present at the end, but on the night of her death he began to write his "The Death of Mrs Johnson". He was too ill to attend the funeral at St Patrick's. Many years later, a lock of hair, assumed to be Esther Johnson's, was found in his desk, wrapped in a paper bearing the words, "Only a woman's hair".
Death became a frequent feature of Swift's life from this point. In 1731 he wrote "Verses on the Death of Dr. Swift", his own obituary published in 1739. In 1732, his good friend and collaborator John Gay died. In 1735, John Arbuthnot, another friend from his days in London, died. In 1738 Swift began to show signs of illness, and in 1742 he may have suffered a stroke, losing the ability to speak and realising his worst fears of becoming mentally disabled. ("I shall be like that tree," he once said, "I shall die at the top.") He became increasingly quarrelsome, and long-standing friendships, like that with Thomas Sheridan, ended without sufficient cause. To protect him from unscrupulous hangers on, who had begun to prey on the great man, his closest companions had him declared of "unsound mind and memory". However, it was long believed by many that Swift was actually insane at this point. In his book "Literature and Western Man", author J. B. Priestley even cites the final chapters of "Gulliver's Travels" as proof of Swift's approaching "insanity".
In part VIII of his series, "The Story of Civilization", Will Durant describes the final years of Swift's life as such:
"Definite symptoms of madness appeared in 1738. In 1741 guardians were appointed to take care of his affairs and watch lest in his outbursts of violence he should do himself harm. In 1742 he suffered great pain from the inflammation of his left eye, which swelled to the size of an egg; five attendants had to restrain him from tearing out his eye. He went a whole year without uttering a word."
In 1744, Alexander Pope died. Then on 19 October 1745, Swift, at nearly 80, died. After being laid out in public view for the people of Dublin to pay their last respects, he was buried in his own cathedral by Esther Johnson's side, in accordance with his wishes. The bulk of his fortune (£12,000) was left to found a hospital for the mentally ill, originally known as St Patrick’s Hospital for Imbeciles, which opened in 1757, and which still exists as a psychiatric hospital.
Epitaph.
Jonathan Swift wrote his own epitaph:
W. B. Yeats poetically translated it from the Latin as:
Works.
Swift was a prolific writer, notable for his satires. The most recent collection of his prose works (Herbert Davis, ed. Basil Blackwell, 1965–) comprises fourteen volumes. A recent edition of his complete poetry (Pat Rodges, ed. Penguin, 1983) is 953 pages long. One edition of his correspondence (David Woolley, ed. P. Lang, 1999) fills three volumes.
Major prose works.
Swift's first major prose work, "A Tale of a Tub", demonstrates many of the themes and stylistic techniques he would employ in his later work. It is at once wildly playful and funny while being pointed and harshly critical of its targets. In its main thread, the "Tale" recounts the exploits of three sons, representing the main threads of Christianity, who receive a bequest from their father of a coat each, with the added instructions to make no alterations whatsoever. However, the sons soon find that their coats have fallen out of current fashion, and begin to look for loopholes in their father's will that will let them make the needed alterations. As each finds his own means of getting around their father's admonition, they struggle with each other for power and dominance. Inserted into this story, in alternating chapters, the narrator includes a series of whimsical "digressions" on various subjects.
In 1690, Sir William Temple, Swift's patron, published "An Essay upon Ancient and Modern Learning" a defence of classical writing (see Quarrel of the Ancients and the Moderns), holding up the "Epistles of Phalaris" as an example. William Wotton responded to Temple with "Reflections upon Ancient and Modern Learning" (1694), showing that the "Epistles" were a later forgery. A response by the supporters of the Ancients was then made by Charles Boyle (later the 4th Earl of Orrery and father of Swift's first biographer). A further retort on the Modern side came from Richard Bentley, one of the pre-eminent scholars of the day, in his essay "Dissertation upon the Epistles of Phalaris" (1699). The final words on the topic belong to Swift in his "Battle of the Books" (1697, published 1704) in which he makes a humorous defence on behalf of Temple and the cause of the Ancients.
In 1708, a cobbler named John Partridge published a popular almanac of astrological predictions. Because Partridge falsely determined the deaths of several church officials, Swift attacked Partridge in "Predictions for the Ensuing Year" by Isaac Bickerstaff, a parody predicting that Partridge would die on 29 March. Swift followed up with a pamphlet issued on 30 March claiming that Partridge had in fact died, which was widely believed despite Partridge's statements to the contrary. According to other sources, Richard Steele uses the personae of Isaac Bickerstaff and was the one who wrote about the "death" of John Partridge and published it in "The Spectator," not Jonathan Swift.
"Drapier's Letters" (1724) was a series of pamphlets against the monopoly granted by the English government to William Wood to provide the Irish with copper coinage. It was widely believed that Wood would need to flood Ireland with debased coinage in order make a profit. In these "letters" Swift posed as a shop-keeper—a draper—to criticise the plan. Swift's writing was so effective in undermining opinion in the project that a reward was offered by the government to anyone disclosing the true identity of the author. Though hardly a secret (on returning to Dublin after one of his trips to England, Swift was greeted with a banner, "Welcome Home, Drapier") no one turned Swift in, although there was an unsuccessful attempt to prosecute the publisher Harding. The government eventually resorted to hiring none other than Sir Isaac Newton to certify the soundness of Wood's coinage to counter Swift's accusations. In "Verses on the Death of Dr. Swift" (1739) Swift recalled this as one of his best achievements.
"Gulliver's Travels", a large portion of which Swift wrote at Woodbrook House in County Laois, was published in 1726. It is regarded as his masterpiece. As with his other writings, the "Travels" was published under a pseudonym, the fictional Lemuel Gulliver, a ship's surgeon and later a sea captain. Some of the correspondence between printer Benj. Motte and Gulliver's also-fictional cousin negotiating the book's publication has survived. Though it has often been mistakenly thought of and published in bowdlerised form as a children's book, it is a great and sophisticated satire of human nature based on Swift's experience of his times. "Gulliver's Travels" is an anatomy of human nature, a sardonic looking-glass, often criticised for its apparent misanthropy. It asks its readers to refute it, to deny that it has adequately characterised human nature and society. Each of the four books—recounting four voyages to mostly fictional exotic lands—has a different theme, but all are attempts to deflate human pride. Critics hail the work as a satiric reflection on the shortcomings of Enlightenment thought.
In 1729, Swift published "A Modest Proposal for Preventing the Children of Poor People in Ireland Being a Burden on Their Parents or Country, and for Making Them Beneficial to the Publick", a satire in which the narrator, with intentionally grotesque arguments, recommends that Ireland's poor escape their poverty by selling their children as food to the rich: "I have been assured by a very knowing American of my acquaintance in London, that a young healthy child well nursed is at a year old a most delicious nourishing and wholesome food..." Following the satirical form, he introduces the reforms he is actually suggesting by deriding them:
Therefore let no man talk to me of other expedients...taxing our absentees...using [nothing] except what is of our own growth and manufacture...rejecting...foreign luxury...introducing a vein of parsimony, prudence and temperance...learning to love our country...quitting our animosities and factions...teaching landlords to have at least one degree of mercy towards their tenants...Therefore I repeat, let no man talk to me of these and the like expedients, 'till he hath at least some glympse of hope, that there will ever be some hearty and sincere attempt to put them into practice.
Legacy.
John Ruskin named him as one of the three people in history who were the most influential for him.
George Orwell named him as one of the writers he most admired, despite disagreeing with him on almost every moral and political issue.
Swift crater, a crater on Mars's moon Deimos, is named after Jonathan Swift, who predicted the existence of the moons of Mars.
References.
</dl>
External links.
Online works

</doc>
<doc id="15616" url="http://en.wikipedia.org/wiki?curid=15616" title="Jello Biafra">
Jello Biafra

Jello Biafra (born Eric Reed Boucher; June 17, 1958) is the former lead singer and songwriter for San Francisco punk rock band Dead Kennedys, and is currently a musician and spoken word artist. After he left the Dead Kennedys, he took over the influential independent record label Alternative Tentacles, which he had co-founded in 1979 with Dead Kennedys bandmate East Bay Ray. Although now focused primarily on spoken word, he has continued as a musician in numerous collaborations.
Politically, Biafra is a member of the Green Party of the United States and actively supports various political causes. He ran for the party's Presidential nomination in 2000, finishing second to Ralph Nader. He is a staunch believer in a free society, who utilizes shock value and advocates direct action and pranksterism in the name of political causes. Biafra is known to use absurdist media tactics, in the leftist tradition of the Yippies, to highlight issues of civil rights and social justice.
Early life.
Eric Boucher was born in Boulder, Colorado, the son of Virginia (née Parker), a librarian, and Stanley Wayne Boucher, a psychiatric social worker and poet. He also had a sister, Julie J. Boucher, the Associate Director of the Library Research Service at the Colorado State Library (who died in a mountain-climbing accident on October 12, 1996). As a child, Eric Boucher developed an interest in international politics that was encouraged by his parents. An avid news watcher, one of his earliest memories was of the John F. Kennedy assassination. Biafra says he has been a fan of rock music since first hearing it in 1965, when his parents accidentally tuned in to a rock radio station.
He began his career in music in January 1977 as a roadie for the punk rock band The Ravers (who later changed their name to The Nails), soon joining his friend John Greenway in a band called The Healers. The Healers became infamous locally for their mainly improvised lyrics and avant garde music. In the autumn of that year, he began attending the University of California, Santa Cruz.
Musical career.
The Dead Kennedys.
In June 1978, he responded to an advertisement placed in a store by guitarist East Bay Ray, stating "Guitarist wants to form punk band", and together they formed the Dead Kennedys. He began performing with the band under the stage name Occupant, but soon began to use his current stage name, a combination of the brand name Jell-O and the short-lived African state Biafra. Biafra wrote the band's lyrics, most of which were political in nature and displayed a sardonic, sometimes absurdist, sense of humor despite their serious subject matter. In the tradition of UK anarcho-punk bands like Crass, the Dead Kennedys were one of the first US punk bands to write politically themed songs. The lyrics Biafra wrote helped popularize the use of humorous lyrics in hardcore. Biafra cites Joey Ramone as the inspiration for his use of humor in his songs (as well as being the musician who made him interested in punk rock), noting in particular songs by The Ramones such as "Beat on the Brat" and "Now I Wanna Sniff Some Glue".
Biafra initially attempted to compose music on guitar, but his lack of experience on the instrument and his own admission of being "a fumbler with my hands" led Dead Kennedys bassist Klaus Flouride to suggest that Biafra simply sing the parts he envisioned to the band.<ref name="re/search">V. Vale, "Incredibly Strange Music, Vol. 2", RE/Search Publications, 1995</ref> Biafra sang his riffs and melodies into a tape recorder, which he brought to the band's rehearsal and/or recording sessions. This later became a problem when the other members of the Dead Kennedys sued Biafra over royalties and publishing rights. By all accounts, including his own, Biafra is not a conventionally skilled musician, though he and his collaborators (Joey Shithead of D.O.A. in particular) attest that he is a skilled composer and his work, particularly with the Dead Kennedys, is highly respected by punk-oriented critics and fans.
Biafra's first popular song was the first single by the Dead Kennedys, "California Über Alles". The song, which spoofed California governor Jerry Brown, was the first of many political songs by the group and Biafra. The song's popularity resulted in its being covered by other musicians, such as The Disposable Heroes of Hiphoprisy (who rewrote the lyrics to parody Pete Wilson), John Linnell of They Might Be Giants and Six Feet Under on their Graveyard Classics album of cover versions. Not long after, the Dead Kennedys had a second and bigger hit with "Holiday in Cambodia" from their debut album "Fresh Fruit for Rotting Vegetables". "AllMusic" cites this song as "possibly the most successful single of the American hardcore scene" and Biafra counts it as his personal favorite Dead Kennedy's song. Minor hits from the album included "Kill the Poor" (about potential abuse of the then-new neutron bomb) and a satirical cover of Elvis Presley's "Viva Las Vegas".
The Dead Kennedys received some controversy in the spring of 1981 over the single "Too Drunk to Fuck". The song became a big hit in Britain, and the BBC feared that it would manage to be a big enough hit to appear among the top 30 songs on the national charts, requiring a mention on "Top of the Pops". However, the single peaked at number 31 in the charts.
Later albums also contained memorable songs, but with less popularity than the earlier ones. The EP "In God We Trust, Inc." contained the song "Nazi Punks Fuck Off!" as well as "We've Got A Bigger Problem Now", a rewritten version of "California Über Alles" about Ronald Reagan. Punk musician and scholar Vic Bondi considers the latter song to be the song that "defined the lyrical agenda of much of hardcore music, and represented its break with punk". The band's most controversial album, "Frankenchrist", brought with it the song "MTV Get Off the Air", which accused MTV of promoting poor quality music and sedating the public. The album also contained a controversial poster by Swiss surrealist artist H. R. Giger entitled "Penis Landscape".
The Dead Kennedys toured widely during their career, starting in the late 1970s. They began playing at San Francisco's Mabuhay Gardens (their home base) and other Bay Area venues, later branching out to shows in southern Californian clubs (most notably the Whisky a Go Go), but eventually they moved to major clubs across the country, including CBGB in New York. Later, they played to larger audiences such as at the 1980 Bay Area Music Awards (where they played the notorious "Pull My Strings" for the only time), and headlined the 1983 Rock Against Reagan festival.
On May 7, 1994, Punk Rock fans who believed Biafra was a "sell out" attacked him at the 924 Gilman Street club in Berkeley, California. Biafra claims that he was attacked by a man nicknamed Cretin, who crashed into him while moshing. The crash injured Biafra's leg, causing an argument between the two men. During the argument, Cretin pushed Biafra to the floor and five or six friends of Cretin assaulted Biafra while he was down, yelling "Sellout rock star, kick him", and attempting to pull out his hair. Biafra was later hospitalized with serious injuries. The attack derailed Biafra's plans for both a Canadian spoken-word tour and an accompanying album, and the production of "Pure Chewing Satisfaction" was halted. However, Biafra returned to the Gilman club a few months after the incident to perform a spoken-word performance as an act of reconciliation with the club.
Biafra has been a prominent figure of the Californian punk scene and was one of the third generation members of the San Francisco punk community. Many later hardcore bands have cited the Dead Kennedys as a major influence. Hardcore punk author Steven Blush describes Biafra as hardcore's "biggest star" who was a "powerful presence whose political insurgence and rabid fandom made him the father figure of a burgeoning subculture [and an] inspirational force [who] could also be a real prick... Biafra was a visionary, incendiary [performer]."
After the Dead Kennedys disbanded, Biafra's new songs were recorded with other bands, and he released only spoken word albums as solo projects. These collaborations had less popularity than Biafra's earlier work. However, his song "That's Progress", originally recorded with D.O.A. for the album "Last Scream of the Missing Neighbors", received considerable exposure when it appeared on the album "Rock Against Bush, Vol. 1".
Obscenity prosecution.
In April 1986, police officers raided his house in response to complaints by the Parents Music Resource Center (PMRC). In June 1986, L.A. deputy city attorney Michael Guarino, working under City Attorney James Hahn, brought Biafra to trial in Los Angeles for distributing "harmful material to minors" in the Dead Kennedys album "Frankenchrist". In actuality, the dispute was about neither the music nor the lyrics from the album, but rather the print of the H. R. Giger poster "Landscape XX" ("Penis Landscape") included with the album. Biafra believes the trial was politically motivated; it was often reported that the PMRC took Biafra to court as a cost-effective way of sending a message out to other musicians with content considered offensive in their music.
Music author Reebee Garofalo argued that Biafra and Alternative Tentacles may have been targeted because the label was a "small, self-managed and self-supported company that could ill afford a protracted legal battle." Facing the possible sentence of a year in jail and a $2000 fine, Biafra, Dirk Dirksen, and Suzanne Stefanac founded the No More Censorship Defense Fund, a benefit made up of several punk rock bands, to help pay for his legal fees, which neither he nor his record label could afford. The jury deadlocked 5 to 7 in favor of acquittal, prompting a mistrial; despite a motion to re-try the case, the judge ordered all charges dropped. The Dead Kennedys disbanded during the trial, in December 1986, due to the mounting legal costs; in the wake of their disbandment, Biafra made a career of his spoken word performances.
Biafra has a cameo role in the 1988 film "Tapeheads". He plays an FBI agent who arrests the two protagonists (played by Tim Robbins and John Cusack). Whilst arresting them his character asks "Remember what we did to Jello Biafra?" lampooning the obscenity prosecution.
On March 25, 2005, Biafra appeared on the U.S. radio program "This American Life", , which featured a phone call between Jello Biafra and Michael Guarino, the prosecutor in the "Frankenchrist" trial.
Lawsuit by former band members and reunion activities.
In October 1998, former members of the Dead Kennedys sued Biafra for nonpayment of royalties. According to Biafra, the suit resulted from his refusal to allow one of the band's most well known singles, "Holiday in Cambodia", to be used in a commercial for Levi's Dockers; Biafra opposes Levi's because they use unfair business practices and sweatshop labor. The three former members claimed that their motive had nothing to do with advertising, and that they had filed suit because Biafra had denied them royalties and failed to promote their albums. Biafra maintained that he had never denied them royalties, and that he himself had not even received royalties for rereleases of their albums or "posthumous" live albums which had been licensed to other labels by the Decay Music partnership. Decay Music denied this charge and have posted what they say are his cashed royalty checks. Biafra also complained about the songwriting credits in new reissues and archival live albums of songs that Biafra claims he composed himself to the entire band. In May 2000, a jury found Biafra liable for fraud and malice and ordered him to pay $200,000, including $20,000 in punitive damages, to the band members. After an appeal by Biafra’s lawyers, in June 2003, the California Court of Appeal unanimously upheld all the conditions of the 2000 verdict against Biafra and Alternative Tentacles.
Other bands.
In the early 1980s, Biafra collaborated with musicians Christian Lunch and Adrian Borland (of The Sound) for the synthpunk musical project The Witch Trials, releasing one self-titled EP in its lifetime.
In 1988, Biafra, with Al Jourgensen and Paul Barker of the band Ministry, and Jeff Ward, formed Lard. The band became yet another side project for Ministry, with Biafra providing vocals and lyrics. According to a March 2009 interview with Jourgensen, he and Biafra are working on a new Lard album, which is being recorded in Jourgensen's El Paso studio. While working on the film "Terminal City Ricochet" in 1989, Biafra did a song for the film's soundtrack with D.O.A.. As a result, Biafra worked with D.O.A. on the album "Last Scream of the Missing Neighbors". Biafra also worked with Nomeansno on the soundtrack, which led to their collaboration on the album "The Sky Is Falling and I Want My Mommy" the following year. Biafra also provided lyrics for the song "Biotech is Godzilla" for Sepultura's 1993 album "Chaos A.D.".
In 1999, Biafra and other members of the anti-globalization movement protested the WTO Meeting of 1999 in Seattle. Along with other prominent West Coast musicians, he formed the short-lived band the No WTO Combo to help promote the movement's cause. The band was originally scheduled to play during the protest, but the performance was canceled due to riots. The band performed a short set the following night at the Showbox in downtown Seattle (outside the designated area), along with the hiphop group Spearhead. No WTO Combo later released a CD of recordings from the concert, entitled "Live from the Battle in Seattle".
As of late 2005, Biafra was performing with the band The Melvins under the name "Jello Biafra and the Melvins", though fans sometimes refer to them as "The Jelvins." Together they have released two albums, and have been working on material for a third collaborative release, much of which was premiered live at two concerts at the Great American Music Hall in San Francisco during an event called Biafra Five-O, commemorating Biafra's 50th birthday, the 30th anniversary of the founding of the Dead Kennedys, and the beginning of legalized same-sex marriage in California. Biafra is also working with a new band known as Jello Biafra and the Guantanamo School of Medicine, which includes Ralph Spight of Victims Family on guitar and Billy Gould of Faith No More on bass. This group debuted during Biafra Five-O.
In 2011, Biafra appeared in a singular concert event with an all-star cast of Southern musicians including members from Cowboy Mouth, Dash Rip Rock, Mojo Nixon and Down entitled, "Jello Biafra and the New Orleans Raunch & Soul All Stars" who performed an array of classic Soul covers to a packed house at the 12-Bar in New Orleans, Louisiana. He would later reunite with many of the same musicians during the Carnival season 2014 to revisit many of these classics at Siberia, New Orleans. A live album from the 2011 performance, "Walk on Jindal's Splinters", and a companion single, "Fannie May"/"Just a Little Bit", were released in 2015.
Alternative Tentacles.
In June 1979, Biafra co-founded the record label Alternative Tentacles, with which the Dead Kennedys released their first single, "California Über Alles". The label was created to allow the band to release albums without having to deal with pressure from major labels to change their music (although the major labels were not willing to sign the band due to their songs being deemed too controversial). After dealing with Cherry Red in the UK and IRS Records in the US for their first album "Fresh Fruit for Rotting Vegetables", the band released all later albums (and later pressings of "Fresh Fruit") on Alternative Tentacles (with the exception of live albums released after the band's break-up, which the other band members compiled from recordings in the band partnership's vaults without Biafra's input or endorsement). Biafra has been the owner of the company ever since its founding, though he does not receive a salary for his position (Biafra has referred to his position in the company as "absentee thoughtlord").
Biafra is an ardent collector of unusual vinyl records of all kinds, from 1950s and 1960s ethno-pop recordings by the likes of Les Baxter and Esquivel to vanity pressings that have circulated regionally, to German crooner Heino; he cites his always growing collection as one of his biggest musical influences. In 1993 he gave an interview to RE/Search Publications for their second "Incredibly Strange Music" book focusing primarily on these records. His heavy interest in such recordings (often categorized as outsider music) eventually led to his discovery of the prolific (and schizophrenic) singer/songwriter/artist Wesley Willis, whom he signed to Alternative Tentacles in 1994, preceding Willis' major label deal with American Recordings. His collection grew so large that on October 1, 2005, Biafra donated a portion of his collection to an annual yard sale co-promoted by Alternative Tentacles and held at their warehouse in Emeryville, California.
In 2006, along with Alternative Tentacles employee and The Frisk lead singer Jesse Luscious, Biafra began co-hosting "The Alternative Tentacles Batcast", a downloadable podcast hosted by alternativetentacles.com. The show primarily focuses on interviews with artists and bands that are currently signed to the Alternative Tentacles label, although there are also occasional episodes where Biafra devoted the show to answering fan questions.
Spoken word.
Biafra became a spoken word artist in January 1986 with a performance at University of California, Los Angeles. In his performance he combined humor with his political beliefs, much in the same way that he did with the lyrics to his songs. Despite his continued spoken word performances, he did not begin recording spoken word albums until after the disbanding of the Dead Kennedys.
His ninth spoken word album, "In the Grip of Official Treason", was released in October 2006.
Biafra was also featured in the British band Pitchshifter's song "As Seen on TV" reciting the words of dystopian futuristic radio advertisements.
Politics.
Biafra was an anarchist in the 1980s, but has shifted away from his former anti-government views. In a 2012 interview, Biafra said "I'm very pro-tax as long as it goes for the right things. I don’t mind paying more money as long as it’s going to provide shelter for people sleeping in the street or getting the schools getting fixed back up, getting the infrastructure up to the standards of other countries, including a high speed rail system. I’m totally down with that."
Mayoral campaign.
For those of them who have seen my candidacy as a publicity stunt or a joke, they should keep in mind that it is no more of a joke, and no less of a joke, than anyone else they care to name.
Jello Biafra, "Dead Kennedys: The Early Years"
In the autumn of 1979, Biafra ran for mayor of San Francisco, using the Jell-O ad campaign catchphrase, "There's always room for Jello", as his campaign slogan. Having entered the race before creating a campaign platform, Biafra later wrote his platform on a napkin while attending a Pere Ubu concert where Dead Kennedys drummer Ted told Biafra, "Biafra, you have such a big mouth that you should run for Mayor." As he campaigned, Biafra wore campaign T-shirts from his opponent Quentin Kopp's previous campaign and at one point vacuumed leaves off the front lawn of another opponent, current U.S. Senator Dianne Feinstein, to mock her publicity stunt of sweeping streets in downtown San Francisco for a few hours. He also made a whistlestop campaign tour along the BART line. Supporters committed equally odd actions; two well known signs held by supporters said "If he doesn't win I'll kill myself" and "What if he does win?"
In San Francisco any individual could legally run for mayor if a petition was signed by 1500 people or if $1500 was paid. Biafra paid $900 and got signatures over time and eventually became a legal candidate, meaning he received statements put in voters' pamphlets and equal news coverage.
His platform included unconventional points such as forcing businessmen to wear clown suits within city limits, erecting statues of Dan White (who assassinated Mayor George Moscone and City Supervisor Harvey Milk in 1978) all over town and allowing the parks department to sell eggs and tomatoes with which people could pelt them, hiring out of job workers, due to a tax initiative, to become pan handlers in wealthy neighborhoods (one being where Dianne Feinstein lives), and a citywide ban on cars (although the last point was not considered completely outlandish by many voters at the time, as the city was suffering from serious pollution). Biafra has expressed irritation that these parts of his platform attained such notoriety, preferring instead to be remembered for serious proposals such as legalizing squatting in vacant, tax-delinquent buildings and requiring police officers to keep their jobs by running for election voted by the people of the neighborhoods they patrol.
He finished fourth out of a field of ten, receiving 3.79% of the vote (6,591 votes); the election ended in a runoff that did not involve him (Feinstein was declared the winner).
Presidential campaign.
In 2000, the New York State Green Party drafted Biafra as a candidate for the Green Party presidential nomination, and a few supporters were elected to the party's nominating convention in Denver, Colorado. Biafra chose death row inmate Mumia Abu-Jamal as his running mate. The party overwhelmingly chose Ralph Nader as the presidential candidate with 295 of the 319 delegate votes. Biafra received 10 votes.
Biafra, along with a camera crew (dubbed by Biafra as "The Camcorder Truth Jihad"), later reported for the Independent Media Center at the Republican and Democratic conventions.
Post-2000.
After losing the 2000 nomination, Jello became highly active in Ralph Nader's presidential campaign, as well as in 2004 and 2008. During the 2008 campaign Jello played at rallies and answered questions for journalists in support of Ralph Nader. When gay rights activists accused Nader of costing Al Gore the 2000 election, Biafra reminded them that Tipper Gore's Parents Music Resource Center wanted warning stickers on albums with homosexual content.
After Barack Obama won the general election, Jello wrote an open letter making suggestions on how to run his term as president. Biafra criticized Obama during his term, stating that "Obama even won the award for best advertising campaign of 2008." Biafra dubbed Obama "Barackstar O'Bummer". Biafra refused to support Obama in 2012. Biafra has stated that he feels that Obama continued many of George W. Bush's policies, summarizing Obama's policies as containing "worse and worse laws against human rights and more and more illegal unconstitutional spying."
Boycott of Israel.
In the summer of 2011 Jello Biafra and his band were scheduled to play at the Barby Club in Tel Aviv. They came under heavy pressure by the pro-Palestinian Boycott, Divestment and Sanctions (BDS) campaign, and finally decided to cancel the gig – after a debate which according to Biafra "deeply tore at the fabric of our band [. . .] This whole controversy has been one of the most intense situations of my life - and I thrive on intense situations".
Biafra then decided to travel to Israel and the Palestinian Occupied Territories, at his own expense, and talk with Israeli and Palestinian activists as well as with fans disappointed at his cancellation. In the article stating his conclusions he wrote:
"I will not perform in Israel unless it is a pro-human rights, anti-occupation event, that does not violate the spirit of the boycott. Each musician, artist, etc. must decide this for themselves. I am staying away for now, but am also really creeped out by the attitudes of some of the hardliners and hope some day to find a way to contribute something positive here. I will not march or sign on with anyone who runs around calling people Zionazis and is more interested in making threats than making friends."
Personal life.
Biafra married Theresa Soder, a.k.a. Ninotchka, lead singer of San Francisco-area punk band The Situations on October 31, 1981. The wedding was conducted by Flipper vocalist/bassist Bruce Loose, who became a Universal Life Church minister just to conduct the ceremony, which took place in a graveyard. The wedding reception, which members of Flipper, Black Flag, and D.O.A. attended, was held at director Joe Rees' Target Video studios. The marriage ended in 1986.
Biafra lives in San Francisco, California
Partial discography.
"For a more complete list, see the Jello Biafra discography."

</doc>
<doc id="15621" url="http://en.wikipedia.org/wiki?curid=15621" title="John Grierson">
John Grierson

John Grierson (26 April 1898 – 19 February 1972) was a pioneering Scottish documentary maker, often considered the father of British and Canadian documentary film. In 1926, Grierson coined the term "documentary" in a review of Robert Flaherty's "Moana".
Early life.
Grierson was born in Deanston, near Doune, Scotland. His father was a schoolmaster, his mother a suffragette and ardent Labour Party activist. From an early age, both parents steeped their son in liberal politics, humanistic ideals, and Calvinist moral and religious philosophies, particularly the notion that education was essential to individual freedom and that hard and meaningful work was the way to prove oneself worthy in the sight of God.
Grierson entered the University of Glasgow in 1916, but then saw service on minesweepers in the Royal Navy during World War I. He returned to University in 1919, graduating in 1922 with a MA in English and Moral Philosophy. He spent a good part of his academic career enmeshed in impassioned political discussion and leftist political activism.
In 1923, he received a Rockefeller Research Fellowship to study in the United States at the University of Chicago, and later at Columbia and the University of Wisconsin–Madison. His research focus was the psychology of propaganda—the impact of the press, film, and other mass media on forming public opinion. Grierson was particularly interested in the popular appeal and influence of the "yellow" (tabloid) press, and the influence and role of these journals on the education of new American citizens from abroad.
Social critic.
In his review of Robert Flaherty's film "Moana" (1926) in the "New York Sun" (8 February 1926), Grierson wrote that it had 'documentary' value.
In his essay "First Principles of Documentary" (1932), Grierson argued that the principles of documentary were that cinema's potential for observing life could be exploited in a new art form; that the "original" actor and "original" scene are better guides than their fiction counterparts to interpreting the modern world; and that materials "thus taken from the raw" can be more real than the acted article. In this regard, Grierson's views align with the Soviet filmmaker Dziga Vertov's contempt for dramatic fiction as "bourgeois excess", though with considerably more subtlety. Grierson's definition of documentary as "creative treatment of actuality" has gained some acceptance, though it presents philosophical questions about documentaries containing stagings and reenactments.
Like a number of other social critics of the time, Grierson was profoundly concerned about what he perceived to be clear threats to democracy. In the US, he encountered a marked tendency toward political reaction, anti-democratic sentiments, and political apathy. He read and agreed with the journalist and political philosopher Walter Lippmann's book "Public Opinion" which blamed the erosion of democracy in part on the fact that the political and social complexities of contemporary society made it difficult if not impossible for the public to comprehend and respond to issues vital to the maintenance of democratic society.
In Grierson's view, a way to counter these problems was to involve citizens in their government with the kind of engaging excitement generated by the popular press, which simplified and dramatized public affairs. It was during this time that Grierson developed a conviction that motion pictures could play a central role in promoting this process. (It has been suggested that some of Grierson's notions regarding the social and political uses of film were influenced by reading Lenin's writing about film as education and propaganda.)
Grierson's emerging view of film was as a form of social and political communication—a mechanism for social reform, education, and perhaps spiritual uplift. His view of Hollywood movie-making was considerably less sanguine:
Film critic.
Grierson's emerging and outspoken film philosophies caught the attention of New York film critics at the time. He was asked to write criticism for the "New York Sun". At the "Sun", Grierson wrote articles on film aesthetics and audience reception, and developed broad contacts in the film world. According to popular myth, in the course of this writing stint, Grierson coined the term "documentary" in writing about Robert J. Flaherty's film "Moana" (1926): "Of course "Moana", being a visual account of events in the daily life of a Polynesian youth and his family, has documentary value."
During this time, Grierson was also involved in scrutinizing the film industries of other countries. He may have been involved in arranging to bring Sergei Eisenstein's groundbreaking film "The Battleship Potemkin" (1925) to US audiences for the first time. Eisenstein's editing techniques and film theories, particularly the use of montage, would have a significant influence on Grierson's own work.
Filmmaker.
Grierson returned to Great Britain in 1927 armed with the sense that film could be enlisted to deal with the problems of the Great Depression, and to build national morale and national consensus. Filmmaking for Grierson was an exalted calling; the Filmmaker a patriot. In all of this there was more than a little elitism, a stance reflected in Grierson's many dicta of the time: "The elect have their duty." "I look on cinema as a pulpit, and use it as a propagandist."
In the US Grierson had met pioneering documentary filmmaker Robert Flaherty. Grierson respected Flaherty immensely for his contributions to documentary form and his attempts to use the camera to bring alive the lives of everyday people and everyday events. Less commendable in Grierson's view was Flaherty's focus on exotic and faraway cultures. ("In the profounder kind of way", wrote Grierson of Flaherty, "we live and prosper each of us by denouncing the other"). In Grierson's view, the focus of film should be on the everyday drama of ordinary people. As Grierson wrote in his diaries: "Beware the ends of the earth and the exotic: the drama is on your doorstep wherever the slums; are, wherever there is malnutrition, wherever there is exploitation and cruelty." "'You keep your savages in the far place Bob; we are going after the savages of Birmingham,' I think I said to him pretty early on. And we did.")
On his return to England, Grierson was employed on a temporary basis as an Assistant Films Officer of the Empire Marketing Board (EMB), a governmental agency which had been established in 1926 to promote British world trade and British unity throughout the empire. One of the major functions of the EMB was publicity, which the Board accomplished through exhibits, posters, and publications and films. It was within the context of this State funded organization that the "documentary" as we know it today really got its start.
In late 1929 Grierson and his cameraman, Basil Emmott, completed his first film, "Drifters", which he wrote, produced and directed. The film, which follows the heroic work of North Sea herring fishermen, was a radical departure from anything being made by the British film industry or Hollywood. A large part of its innovation lie in the fierce boldness in bringing the camera to rugged locations such as a small boat in the middle of a gale, while leaving relatively less of the action staged. The choice of topic was chosen less from Grierson's curiosity than the fact that he discovered that the Financial Secretary had made the herring industry his hobbyhorse. It premiered in a private film club in London in November 1929 on a double-bill with Eisenstein's then controversial film "The Battleship Potemkin" (which was banned from general release in Britain until 1954), and received high praise from both its sponsors and the press.
After this success, Grierson moved away from film direction into a greater focus on production and administration within the EMB. He became a tireless organizer and recruiter for the EMB, enlisting a stable of energetic young filmmakers into the film unit between 1930 and 1933. Those enlisted included filmmakers Basil Wright, Edgar Anstey, Stuart Legg, Paul Rotha, Arthur Elton, Humphrey Jennings, Harry Watt, and Alberto Cavalcanti. This group formed the core of what was to become known as the British Documentary Film Movement. Robert Flaherty himself also worked briefly for the unit.
In 1933 the EMB Film Unit was disbanded, a casualty of Depression era economics. Grierson's boss at the EMB moved to the General Post Office (GPO) as its first public relations officer, with the stipulation that he could bring the EMB film unit with him. Grierson's crew were charged with demonstrating the ways in which the Post Office facilitated modern communication and brought the nation together, a task aimed as much at GPO workers as the general public. During Grierson's administration, the GPO Film Unit produced a series of groundbreaking films, including "Night Mail" (dir. Basil Wright and Harry Watt, 1936) and "Coal Face" (dir. Alberto Cavalcanti, 1935). In 1934 he produced at the GPO Film Unit the award winning "The Song of Ceylon" (dir. Basil Wright) which was sponsored jointly by the Ceylon Tea Propaganda Bureau and the EMB.
Grierson eventually grew restless with having to work within the bureaucratic and budgetary confines of government sponsorship. In response, he sought out private industry sponsorship for film production. He was finally successful in getting the British gas industry to underwrite an annual film program. Perhaps the most significant works produced during this time were "Housing Problems" (dir. Arthur Elton, Edgar Anstey, John Taylor, and Grierson's sister Ruby Grierson, 1935).
In 1938, Grierson was invited by the Canadian government to study the country's film production. He proposed that the government create a national coordinating body for the production of films. In 1939, Canada created the National Film Commission, which would later become the National Film Board of Canada. Grierson was the first Commissioner of the Board. When Canada entered World War II in 1939, the NFB focused on the production of propaganda films, many of which Grierson directed. For example, captured footage of German war activity was incorporated in documentaries that were distributed to the then-neutral United States. After the war, the NFB focused on producing documentaries that reflected the lives of Canadians. The NFB has become recognized around the world for producing quality films, some of which have won Academy Awards.
During WW II, Grierson was a consultant to prime minister William Lyon Mackenzie King as a minister of the Wartime Information Board. He concentrated on documentary film production in New York after resigning this post following the war. In 1945 Grierson was dismissed from his post as Commissioner of the NFB after allegations of communist sympathy regarding several of the films the Board had produced during the war. Following his dismissal, and that of three of his coworkers, Grierson returned to Scotland.
From 1946 to 1948 he was the director of mass communications at UNESCO, and from 1948 to 1950 he was controller of films at Britain's Central Office of Information. During the 1950s he worked at Southall Studios in West London.
From 1957 to 1967 Grierson hosted a successful weekly television program on Scottish television, "This Wonderful World", which showed excerpts from outstanding documentaries. In 1957 he received a special Canadian Film Award.
Filmography.
Filmography as director:
Filmography as producer/creative contributor:
Awards named for John Grierson.
Grierson Documentary Film Awards.
The Grierson Documentary Film Awards were established in 1972 to commemorate John Grierson and
are currently supervised by The Grierson Trust. The aim of the awards is to recognise "outstanding films that demonstrate integrity, originality and technical excellence, together with social or cultural significance".
Grierson Awards are presented annually in nine categories:
Other.
The Canadian Film Awards had presented a Grierson Award for "an outstanding contribution to Canadian cinema in the spirit of John Grierson."

</doc>
<doc id="15622" url="http://en.wikipedia.org/wiki?curid=15622" title="James Cameron">
James Cameron

James Francis Cameron (born August 16, 1954) is a Canadian filmmaker, inventor, engineer, philanthropist, and deep-sea explorer who has directed the two biggest box office films of all time. He first found major success with the science-fiction hit "The Terminator" (1984). He then became a popular Hollywood director and was hired to write and direct "Aliens" (1986); three years later he followed up with "The Abyss" (1989).
He found further critical acclaim for his use of special effects in "" (1991). After his film "True Lies" (1994) Cameron took on his biggest film at the time, "Titanic" (1997), which earned him Academy Awards for Best Picture, Best Director and Film Editing. After "Titanic", Cameron began a project that took almost 10 years to make: his science-fiction epic "Avatar" (2009), for which he received nominations for the same three Academy Awards. In the time between making "Titanic" and "Avatar", Cameron spent several years creating many documentary films (specifically underwater documentaries) and co-developed the digital 3D Fusion Camera System. Described by a biographer as part-scientist and part-artist, Cameron has also contributed to underwater filming and remote vehicle technologies. On March 26, 2012, Cameron reached the bottom of the Mariana Trench, the deepest part of the ocean, in the "Deepsea Challenger" submersible. He is the first person to do this in a solo descent, and is only the third person to do so ever.
He has won three Academy Awards for "Titanic". In total, Cameron's directorial efforts have grossed approximately US$2 billion in North America and US$6 billion worldwide. Not adjusted for inflation, Cameron's "Titanic" and "Avatar" are the two highest-grossing films of all time at $2.19 billion and $2.78 billion respectively. In March 2011, he was named Hollywood's top earner by "Vanity Fair", with estimated 2010 earnings of $257 million. In October 2013, a new species of frog "Pristimantis jamescameroni" from Venezuela was named after him in recognition of his efforts in environmental awareness, in addition to his public promotion of veganism.
Early life.
Cameron was born in 1954 in Kapuskasing, Ontario, the son of Shirley (née Lowe), an artist and nurse, and Phillip Cameron. His paternal great-great-great-grandfather emigrated from Balquhidder, Scotland, in 1825.
Cameron grew up in Chippawa, Ontario, and attended Stamford Collegiate School in Niagara Falls, Ontario. His family moved to Brea, California in 1971, when Cameron was 17 years old. He dropped out of Lutheran High School of Orange County, then attended Troy High School to further his secondary education.
Cameron enrolled at Fullerton College, a two-year community college, in 1973 to study physics. He switched to English, then dropped out before the start of the fall 1974 semester. Next, he worked several jobs, including as a truck driver, writing when he had time. During this period he taught himself about special effects: "I'd go down to the USC library and pull any thesis that graduate students had written about optical printing, or front screen projection, or dye transfers, anything that related to film technology. That way I could sit down and read it, and if they'd let me photocopy it, I would. If not, I'd make notes."
Cameron quit his job as a truck driver to enter the film industry after seeing the original "" film in 1977. When Cameron read Syd Field's book "Screenplay", it occurred to him that integrating science and art was possible, and he wrote a 10-minute science-fiction script with two friends, titled "Xenogenesis". They raised money, rented camera, lenses, film stock and studio then shot it in 35mm. They dismantled the camera to understand how to operate it and spent the first half-day of the shoot trying to figure out how to get it running.
Early Career.
His first film was called "Xenogenesis" (1978). He was the director, writer, producer, and production designer for "Xenogenesis". He then became a production assistant on a film called "Rock and Roll High School", though uncredited in 1979. While continuing to educate himself in film-making techniques, Cameron started working as a miniature-model maker at Roger Corman Studios. Making rapidly produced, low-budget productions taught Cameron to work efficiently and effectively. He soon found employment as an art director in the sci-fi movie "Battle Beyond the Stars" (1980). He did special effects work design and direction on John Carpenter's "Escape from New York" (1981), acted as production designer on "Galaxy of Terror" (1981), and consulted on the design of "Android" (1982).
Cameron was hired as the special effects director for the sequel to "Piranha", entitled "" in 1981. The original director, Miller Drake, left the project due to creative differences with producer Ovidio Assonitis, who then gave Cameron his first job as overall director. The interior scenes were filmed in Italy while the underwater sequences were shot at Grand Cayman Island.
The movie was to be produced in Jamaica. On location, production slowed due to numerous problems and adverse weather. James Cameron was fired after failing to get a close up of Carole Davis in her opening scene. Ovidio ordered Cameron to do the close-up the next day before he started on that day’s shooting. Cameron spent the entire day sailing around the resort to reproduce the lighting but still failed to get the close-up. After he was fired, Ovidio invited Cameron to stay on location and assist in the shooting. Once in Rome, Ovidio took over the editing when Cameron was stricken with food poisoning. During his illness, Cameron had a nightmare about an invincible robot hitman sent from the future to kill him, giving him the idea for "The Terminator", which later catapulted his film career.
Major films.
"The Terminator" (1984).
After completing a screenplay for "The Terminator", Cameron decided to sell it so that he could direct the movie. However, the production companies he contacted, while expressing interest in the project, were unwilling to let a largely inexperienced feature film director make the movie. Finally, Cameron found a company called Hemdale Pictures, which was willing to let him direct. Gale Anne Hurd, who had started her own production company, Pacific Western Productions, had previously worked with Cameron in Roger Corman's company and agreed to buy Cameron's screenplay for one dollar, on the condition that Cameron direct the film. Hurd was signed on as producer, and Cameron finally got his first break as director. Orion Pictures distributed the film. Hurd and Cameron were married from 1985 - 1989. 
For the role of the Terminator, Cameron envisioned a man who was not exceptionally muscular, who could "blend into" a crowd. Lance Henriksen, who had starred in "Piranha II: The Spawning", was considered for the title role, but when Arnold Schwarzenegger and Cameron first met over lunch to discuss Schwarzenegger's playing the role of Kyle Reese, both came to the conclusion that the cyborg villain would be the more compelling role for the Austrian bodybuilder; Henriksen got the smaller part of LAPD detective Hal Vukovich and the role of Kyle Reese went to Michael Biehn. In addition, Linda Hamilton first appeared in this film in her iconic role of Sarah Connor, and later married Cameron.
"The Terminator" was a box-office hit, breaking expectations by Orion Pictures executives that the film would be regarded as no more than a sci-fi film and only last a week in theaters. It was a low-budget film which cost $6.5 million to make, cutting expenses in such ways as recording the audio track in mono. However, "The Terminator" eventually earned over $78 million worldwide.
"Rambo: First Blood Part II" (1985).
During the early 1980s, Cameron wrote three screenplays simultaneously: "The Terminator", "Aliens", and the first draft of '. While Cameron continued with "The Terminator" and "Aliens", Sylvester Stallone eventually took over the script of ', creating a final draft which differed radically from Cameron's initial vision.
"Aliens" (1986).
Cameron next began the sequel to "Alien", the 1979 film by Ridley Scott. Cameron named the sequel "Aliens" and again cast Sigourney Weaver in the iconic role of Ellen Ripley. According to Cameron, the crew on "Aliens" was hostile to him, regarding him as a poor substitute for Ridley Scott. Cameron sought to show them "The Terminator" but the majority of the crew refused to watch it and remained skeptical of his direction throughout production. Despite this and other off-screen problems (such as clashing with an uncooperative camera man and having to replace one of the lead actors when Michael Biehn of "Terminator" took James Remar's place as Corporal Hicks), "Aliens" became a box-office success. It received Academy Award nominations for Best Actress in a Leading Role for Weaver, Best Art Direction, Best Film Editing, Best Original Score, Best Sound, and won awards for Best Sound Effects Editing and Best Visual Effects. In addition, the film and its lead actress made the cover of "TIME" magazine as a result of its numerous and extensive scenes of women in combat; these were almost without precedent and expressed the feminist theme of the film very strongly.
"The Abyss" (1989).
Cameron's next project stemmed from an idea that had come up during a high school biology class. The story of oil-rig workers who discover otherworldly underwater creatures became the basis of Cameron's screenplay for "The Abyss", which cast Ed Harris, Mary Elizabeth Mastrantonio and Michael Biehn. Initially budgeted at $41 million U.S. (though the production ran considerably over budget), it was considered to be one of the most expensive films of its time and required cutting-edge effects technology. Because much of the filming took place underwater and the technology wasn't advanced enough to digitally create an underwater environment, Cameron chose to shoot much of the movie "reel-for-real", at depths of up to 40 ft. For creation of the sets, the containment building of an unfinished nuclear power plant was converted, and two huge tanks were used. The main tank was filled with 7500000 usgal of water and the second with 2500000 usgal. The cast and crew resided there for much of the filming.
"Terminator 2: Judgment Day" (1991).
After the success of "The Terminator", there had been talk about a sequel to continue the story of Sarah Connor and her struggle against machines from the future. Although Cameron had come up with a core idea for the sequel and Schwarzenegger expressed interest in continuing the story, there were still problems regarding who had the rights to the story, as well as the logistics of the special effects needed to make the sequel. Finally, in late-1980s, Mario Kassar of Carolco Pictures secured the rights to the sequel, allowing Cameron to greenlight production of the film, now called "".
For the film, Linda Hamilton reprised her iconic role of Sarah Connor. In addition, Schwarzenegger also returned in his role as The Terminator, but this time as a protector. Unlike Schwarzenegger's character—the T-800 Terminator which is made of a metal endoskeleton—the new villain of the sequel, called the T-1000, is a more-advanced Terminator made of liquid metal, and with polymorphic abilities. The T-1000 would also be much less bulky than the T-800. For the role, Cameron cast Robert Patrick, a sharp contrast to Schwarzenegger. Cameron explained, "I wanted someone who was extremely fast and agile. If the T-800 is a human Panzer tank, then the T-1000 is a Porsche."
Cameron had originally wanted to incorporate this advanced-model Terminator into the first film, but the special effects at the time were not advanced enough. The ground-breaking effects used in "The Abyss" to digitally depict the water tentacle convinced Cameron that his liquid metal villain was now possible.
TriStar Pictures agreed to distribute the film, but under a locked release date only about one year after the start of shooting. The movie, co-written by Cameron and his longtime friend, William Wisher, Jr., had to go from screenplay to finished film in just that amount of time. Like Cameron's previous film, it was one of the most expensive films of its era, with a budget of about $100 million. The biggest challenge of the movie was the special effects used in creating the T-1000. Nevertheless, the film was finished on time and released to theaters on July 3, 1991.
"Terminator 2", or "T2", as it was abbreviated, broke box-office records (including the opening weekend record for an R-rated film), earning over $200 million in the United States and Canada, and over $300 million in other territories, and became the highest-grossing film of that year. It won four Academy Awards: Best Makeup, Best Sound, Best Sound Effects Editing, and Best Visual Effects. It was also nominated for Best Cinematography and Best Film Editing, but lost both Awards to "JFK".
James Cameron announced a third "Terminator" film many times during the 1990s, but without coming out with any finished scripts. Kassar and Vajna purchased the rights to the Terminator franchise from a bankruptcy sale of Carolco's assets. "" was eventually made and released in July 2003 without Cameron's involvement. Jonathan Mostow directed the film and Schwarzenegger returned as the Terminator.
Cameron reunited with the main cast of "Terminator 2" to film "", an attraction at Universal Studios Florida, Universal Studios Hollywood and Universal Studios Japan. It was released in 1996 and was a mini-sequel to "Terminator 2: Judgment Day". The show is in two parts: a prequel segment in which a spokesperson talks about Cyberdyne, and a main feature, in which the performers interact with a 3-D movie.
"True Lies" (1994).
Before the release of "T2", Schwarzenegger came to Cameron with the idea of remaking the French comedy "La Totale!" Titled "True Lies", with filming beginning after "T2"'s release, the story revolves around a secret-agent spy who leads a double life as a married man, whose wife believes he is a computer salesman. Schwarzenegger was cast as Harry Tasker, a spy charged with stopping a plan by a terrorist to use nuclear weapons against the United States. Jamie Lee Curtis and Eliza Dushku played the character's family, and Tom Arnold the sidekick.
Cameron's Lightstorm Entertainment signed on with Twentieth Century Fox for production of "True Lies". Made on a budget of $115 million and released in 1994, the film earned $146 million in North America, and $232 million abroad. The film received an Academy Award nomination for Best Visual Effects.
"Strange Days" (1995).
An American science-fiction action thriller film directed by Kathryn Bigelow. It was co-written and produced by her ex-husband James Cameron and co-written by Jay Cocks.
"Titanic" (1997).
Cameron expressed interest in the famous sinking of the ship . He decided to script and film his next project based on this event. The picture revolved around a fictional romance story between two young lovers from different social classes who meet on board. Before production began, he took dives to the bottom of the Atlantic and shot actual footage of the ship underwater, which he inserted into the final film. Much of the film's dialogue was also written during these dives.
Subsequently, Cameron cast Leonardo DiCaprio, Kate Winslet, Billy Zane, Kathy Bates, Frances Fisher, Gloria Stuart, Bernard Hill, Jonathan Hyde, Victor Garber, Danny Nucci, David Warner, Suzy Amis, and Bill Paxton as the film's principal cast. Cameron's budget for the film reached about $200 million, making it the most expensive movie ever made at the time. Before its release, the film was widely ridiculed for its expense and protracted production schedule.
Released to theaters on December 19, 1997, "Titanic" grossed less in its first weekend ($28.6 million) than in its second ($35.4 million), an increase of 23.8%. This is unheard of for a widely released film, which is a testament to the movie's appeal. This was especially noteworthy, considering that the film's running time of more than three hours limited the number of showings each theater could schedule. It held the No. 1 spot on the box-office charts for months, eventually grossing a total of $600.8 million in the United States and Canada and more than $1.84 billion worldwide. "Titanic" became the highest-grossing film of all time, both worldwide and in the United States and Canada, and was also the first film to gross more than $1 billion worldwide. It remained the highest-grossing film since 1998, until Cameron's 2009 film "Avatar" surpassed its gross in 2010.
The CG visuals surrounding the sinking and destruction of the ship were considered spectacular. Despite criticism during production of the film, it received a record-tying 14 Oscar nominations (tied with "All About Eve") at the 1998 Academy Awards. It won 11 Oscars (also tying the record for most Oscar wins with "Ben-Hur" and later ""), including: Best Picture, Best Director, Best Art Direction, Best Cinematography, Best Visual Effects, Best Film Editing, Best Costume Design, Best Sound, Best Sound Effects Editing, Best Original Dramatic Score, Best Original Song. Upon receiving the Best Director Oscar, Cameron exclaimed, "I'm king of the world!", in reference to one of the main characters' lines from the film. After receiving the Best Picture Oscar along with Jon Landau, Cameron asked for a moment of silence for the 1,500 men, women, and children who died when the ship sank.
In March 2010, Cameron revealed that "Titanic" would be re-released in 3D in April 2012, to commemorate the 100th anniversary of the sinking of the real ship. On March 27, 2012, Cameron attended the world première with Kate Winslet at the Royal Albert Hall in London. Following the re-release, "Titanic's" domestic total was pushed to $658.6 million and more than $2.18 billion worldwide. It became the second film to gross more than $2 billion worldwide (the first being Avatar).
"Spider-Man" and "Dark Angel" (2000–2002).
Cameron had initially next planned to do a film of the comic-book character Spider-Man, a project developed by Menahem Golan of Cannon Films. Columbia hired David Koepp to adapt Cameron's treatment into a screenplay, and Koepp's first draft is taken often word-for-word from Cameron's story, though later drafts were heavily rewritten by Koepp himself, Scott Rosenberg, and Alvin Sargent. Columbia preferred to credit David Koepp solely, and none of the scripts before or after his were ever examined by the Writers Guild of America, East to determine proper credit attribution. Cameron and other writers objected, but Columbia and the WGA prevailed. In its release in 2002, "Spider-Man" had its screenplay credited solely to Koepp.
Unable to make "Spider-Man", Cameron moved to television and created "Dark Angel", a superheroine-centered series influenced by cyberpunk, biopunk, contemporary superhero franchises, and third-wave feminism. Co-produced with Charles H. Eglee, "Dark Angel" starred Jessica Alba as Max Guevara, a genetically enhanced super-soldier created by a secretive organization. Cameron's work was said to "bring empowered female warriors back to television screens[...] by mixing the sober feminism of his "The Terminator" and "Aliens" characters with the sexed-up Girl Power of a Britney Spears concert." While a success in its first season, low ratings in the second led to its cancellation. Cameron himself directed the series finale, a two-hour episode wrapping up many of the series' loose ends.
Documentaries (2002–2012).
In 1998 James and John David Cameron formed a digital media company, earthship.tv, which became Earthship Productions. The company produced live multimedia documentaries from the depths of the Atlantic and Pacific oceans. With Earthship Productions, John Cameron's recent projects have included undersea documentaries on the "Bismarck" ("", 2002) and the "Titanic" ("Ghosts of the Abyss" (2003, in IMAX 3D) and "Tony Robinson's Titanic Adventure" (2005)). He was a producer on the 2002 film "Solaris", and narrated "The Exodus Decoded".
Cameron is an advocate for stereoscopic digital 3-D films. In a 2003 interview about his IMAX 2D documentary "Ghosts of the Abyss", he mentioned that he is "going to do everything in 3D now". He has made similar statements in other interviews. "Ghosts of the Abyss" and "Aliens of the Deep" (also an IMAX documentary) were both shot in 3-D and released by Walt Disney Pictures and Walden Media, and Cameron did the same for his new project, "Avatar" for 20th Century Fox & Sony Pictures' Columbia Pictures. He intends to use the same technology for "The Dive", "Sanctum" and an adaptation of the manga series "Battle Angel Alita".
Cameron was the founder and CEO of Digital Domain, a visual-effects production and technology company.
In addition, he plans to create a 3-D project about the first trip to Mars. ("I've been very interested in the Humans to Mars movement—the 'Mars Underground'—and I've done a tremendous amount of personal research for a novel, a miniseries, and a 3-D film.") He is on the science team for the 2011 Mars Science Laboratory.
Cameron announced on February 26, 2007, that he, along with his director, Simcha Jacobovici, have documented the unearthing of the Talpiot Tomb, which is alleged to be the tomb of Jesus. Unearthed in 1981 by Israeli construction workers, the names on the tomb are claimed, in the documentary, to correlate with the names of Jesus and several individuals closely associated with him. The documentary, named "The Lost Tomb of Jesus", was broadcast on the Discovery Channel on March 4, 2007.
As a National Geographic explorer-in-residence, Cameron re-investigated the sinking of the "Titanic" with eight experts in 2012. The investigation was featured in the TV documentary special "Titanic: The Final Word with James Cameron", which premiered on April 8 on the National Geographic Channel. In the conclusion of the analysis, the consensus revised the CGI animation of the sinking conceived in 1995.
"Avatar" (2009).
In June 2005, Cameron was announced to be working on a project tentatively titled "Project 880" (now known to be "Avatar") in parallel with another project, "Battle Angel" (an adaptation of the manga series Battle Angel Alita). Both movies were to be shot in 3D. By December, Cameron stated that he wanted to film "Battle Angel" first, followed by "Avatar". However in February 2006, he switched goals for the two film projects and decided to film "Avatar" first. He mentioned that if both films were successful, he would be interested in seeing a trilogy being made for both.
"Avatar" had an estimated budget of over $300 million and was released on December 18, 2009. This marked his first feature film since 1997's "Titanic". It is composed almost entirely of computer-generated animation, using a more-advanced version of the "performance capture" technique used by director Robert Zemeckis in "The Polar Express". James Cameron had written an 80-page scriptment for "Avatar" in 1995 and announced in 1996 that he would make the film after completing "Titanic". In December 2006, Cameron explained that the delay in producing the film since the 1990s had been to wait until the technology necessary to create his project was advanced enough, since at the time no studio would finance for the development of the visual effects. The film was originally scheduled to be released in May 2009 but was pushed back to December 2009 to allow more time for post-production on the complex CGI and to give more time for theatres worldwide to install 3D projectors. Cameron originally intended "Avatar" to be 3D-only.
"Avatar" broke several box office records during its initial theatrical run. It grossed $749.7 million in the United States and Canada and more than $2.74 billion worldwide, becoming the highest-grossing film of all time in the United States and Canada, surpassing Cameron's "Titanic". "Avatar" also became the first movie to ever earn more than $2 billion worldwide. Including revenue from the re-release of "Avatar" featuring extended footage, it grossed $760.5 million in the U.S. and Canada and more than $2.78 billion worldwide. It was nominated for nine Academy Awards, including Best Picture and Best Director, and won three for Best Art Direction, Best Cinematography and Best Visual Effects.
Avatar's success made Cameron the highest earner in Hollywood for 2010, netting him $257 million as reported by "Vanity Fair".
Disney announced in September 2011 that it would adapt James Cameron's film "Avatar" into Avatar Land, a themed area at Disney's Animal Kingdom in Lake Buena Vista, Florida.
"Sanctum" (2011).
Cameron served as the executive producer of "Sanctum", a film detailing the expedition of a team of underwater cave divers who find themselves trapped in a cave, their exit blocked and with no known way to reach the surface either in person or by radio contact.
Planned films.
In August 2013, Cameron announced his intention to film three sequels to "Avatar" simultaneously, to be released in December 2016, 2017, and 2018. However, on January 14, 2015, Cameron announced that the release dates for the three sequels were each delayed a year with the first sequel scheduled to be released in December 2017. His original plans were to do "Battle Angel" next, but he changed his mind due to "Avatar"'s success; "My intention when I made "Avatar" was to do "Battle Angel" next. However, the positive feedback for "Avatar" and the support of the message of "Avatar", encouraged me to do more of those films." Cameron's Lightstorm Entertainment bought the film rights to the Taylor Stevens novel "The Informationist" in October 2012 with plans for Cameron to direct it. A screenwriter will be hired to adapt the novel while Cameron works on the "Avatar" sequels. Another project Cameron has announced is a personal commitment to shoot a film on the atomic bombings of Hiroshima and Nagasaki as told through the story of Tsutomu Yamaguchi, a man who survived both attacks. Cameron met with Yamaguchi just days before he died in 2010.
Personal life.
Cameron has been married five times to the following spouses: Sharon Williams (1978–1984), Gale Anne Hurd (1985–1989), director Kathryn Bigelow (1989–1991), Linda Hamilton (1997–1999, daughter Josephine born in 1993), and Suzy Amis (2000–present). Cameron had dated Hamilton since 1991. Eight months after the marriage, however, they separated, and within days of Cameron's Oscar victory with "Titanic," the couple announced their divorce. As part of the divorce settlement, Cameron was ordered to pay Hamilton $50 million. Hamilton later revealed that the reason for their divorce was not only Cameron's blind devotion to his work to the exclusion of almost everything else, but also that he had been having an affair with Suzy Amis, an actress he cast as Lizzy Calvert in "Titanic." He married Amis in 2000, and they have one son and two daughters. Cameron lives in New Zealand, a country he fell in love with when he was filming "Avatar".
Hurd was the producer of Cameron's "The Terminator", "Aliens", and "The Abyss", and the executive producer of "". Hamilton played the role of Sarah Connor in both "Terminator" films. Amis played the part of Lizzy Calvert, Rose's granddaughter, in "Titanic". Both Cameron ("Avatar") and Bigelow ("The Hurt Locker") were nominated for the Oscar, the Golden Globe, and the BAFTA Award for Best Director for films released in 2009. Cameron won the Golden Globe, while Bigelow won the Oscar and the BAFTA for Best Director, becoming the first woman to win either.
Cameron is a member of the NASA Advisory Council and is working on the project to put cameras on an upcoming manned Mars mission. Cameron has also given speeches and raised money for the Mars Society, a non-profit organization lobbying for the colonization of Mars.
Cameron became an expert on deep-sea exploration in conjunction with his research and underwater filming for "The Abyss" (1989) and "Titanic" (1997). In June 2010, Cameron met in Washington with the EPA to discuss possible solutions to the 2010 Deepwater Horizon (BP) oil spill. Later that week at the All Things Digital Conference, he attracted some notoriety when he stated, "Over the last few weeks I've watched...and been thinking, 'Those morons don't know what they're doing'." Reportedly, Cameron had offered BP help to plug the oil well, but it declined. The oil spill was eventually stopped using techniques similar to those Cameron recommended.
Although Cameron had resided in the United States since 1971, he remains a Canadian citizen. Cameron applied for American citizenship but withdrew his application after George W. Bush won the presidential election in 2004.
Cameron calls himself "Converted Agnostic", and says "I've sworn off agnosticism, which I now call cowardly atheism". As a child he described the Lord's Prayer as being a "tribal chant".
In June 2013, British artist Roger Dean filed a legal action at a court in New York against Cameron. Dean accused Cameron of "wilful and deliberate copying, dissemination and exploitation" of his original images, relating to Cameron's 2009 film "Avatar" and sought damages of $50m. Dean subsequently lost the case. 
Early in 2014, Cameron purchased the Beaufort Vineyard and Estate Winery in Courtenay, British Columbia, at a price of $2.7 million, as well as a number of other businesses in the area, including cattle ranching operations, to pursue his passion for sustainable agribusiness. Three months prior, Cameron made the local news when he was caught on camera purchasing quality wine at a local liquor store.
Deep sea dives.
On March 7, 2012, Cameron took the "Deepsea Challenger" submersible to the bottom of the New Britain Trench in a five-mile-deep solo dive. On March 26, 2012, Cameron reached the Challenger Deep, the deepest part of the Mariana Trench. He spent more than three hours exploring the ocean floor before returning to the surface. Cameron is the first person to accomplish the trip solo. He was preceded by unmanned dives in 1995 and 2009 and by Jacques Piccard and Don Walsh, who were the first men to reach the bottom of the Mariana Trench aboard the Bathyscaphe Trieste in 1960. Cameron has made a three-dimensional film of his dive. During his dive to the Challenger Deep, the data he collected resulted in interesting new finds in the field of marine biology, including new species of sea cucumber, squid worm, and giant single-celled amoeba, which are exciting finds due to the harshness of the environment.
Veganism.
In 2012, Cameron, his wife and his children adopted a vegan diet. Cameron explains that "By changing what you eat, you will change the entire contract between the human species and the natural world".
When asked what’s the best thing an individual can do to fight climate change, Cameron said, “Stop eating animals.”
Influence.
Cameron's directorial style has provided great influence throughout the film industry. "Buffy the Vampire Slayer" and "Firefly" creator Joss Whedon stated that Cameron's approach to action scenes was influential to those in "The Avengers". He also cited Cameron as "the leader and the teacher and the Yoda".<ref name="http://www.slashfilm.com/film-interview-joss-whedon-writer-director-the-avengers/"></ref> Michael Bay considers Cameron an idol and was convinced by him to use 3D in "".<ref name="http://www.hollywoodreporter.com/news/michael-bay-reveals-james-camerons-191774"></ref> Cameron's approach to 3D also inspired Baz Luhrmann to utilize it in "The Great Gatsby". Other directors that have drawn inspiration from Cameron include Peter Jackson.<ref name="http://web.archive.org/web/20071225035055/http://tbhl.theonering.net/peter/faq.html"></ref>
Reputation.
In 1999, Cameron was labeled selfish and cruel by one collaborator, author Orson Scott Card, who had been hired a decade earlier to work with Cameron on the novelization of "The Abyss". Card said the experience was "hell on wheels. He was very nice to me, because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way. Nor did it motivate people to work faster or better. And unless he changes his way of working with people, I hope he never directs anything of mine. In fact, now that this is in print, I can fairly guarantee that he will never direct anything of mine. Life is too short to collaborate with selfish, cruel people." He later alluded to Cameron in his review of "Me and Orson Welles", where he described witnessing a famous director chew out an assistant for his own error.
After working with Cameron on "Titanic", Kate Winslet decided she would not work with Cameron again unless she earned "a lot of money". She said that Cameron was a nice man, but she found his temper difficult to deal with. In an editorial, the British newspaper "The Independent" said that Cameron "is a nightmare to work with. Studios have come to fear his habit of straying way over schedule and over budget. He is notorious on set for his uncompromising and dictatorial manner, as well as his flaming temper."
Sam Worthington, who worked with Cameron, stated on "The Jay Leno Show" that Cameron had very high expectations from everyone: he would use a nail gun to nail the film crew's cell phones to a wall above an exit door in retaliation for unwanted ringing during production. Other actors, such as Bill Paxton and Sigourney Weaver, have praised Cameron's perfectionism. Weaver said of Cameron: "He really does want us to risk our lives and limbs for the shot, but he doesn't mind risking his own." Michael Biehn has also praised Cameron, claiming "Jim is a really passionate person. He cares more about his movies than other directors care about their movies", but added "I’ve never seen him yell at anybody." However, Biehn did claim Cameron is "not real sensitive when it comes to actors."
Composer James Horner refused to work with Cameron for a decade following their strained working relationship on 1986's "Aliens"; they eventually settled their differences, and Horner went on to score both "Titanic" and "Avatar".
An episode of "South Park" from its 16th season depicted the director as being self-obsessed. He is seen going deep sea diving while playing his own theme song and appearing oblivious to his overtly bored ship's crew. However, the episode also credits him for "raising the bar" on entertainment.
During the 70th Golden Globe Awards ceremony, co-host Amy Poehler prompted laughter and audible gasps when she joked about a controversy around the depiction of torture in "Zero Dark Thirty", a film co-produced and directed by Cameron's ex-wife Kathryn Bigelow. According to Poehler, "when it comes to torture, I trust the lady who spent three years married to James Cameron."
In 2014, Cameron was the keynote speaker at the first annual Fame and Philanthropy, a charity fundraiser which raised money for several high profile celebrity charities. Cameron was one of several guest speakers at the event along with Charlize Theron and Halle Berry.
In a 2015 interview together, actresses Sigourney Weaver and Jamie Lee Curtis, who both worked with Cameron, commented very positively on him. Curtis stated " the truth is he can do every other job [than acting]. I'm talking about every single department, from art direction to props to wardrobe to cameras, he knows more than everyone doing the job." Weaver answered "There are very few geniuses in the world, let alone in our business, and he's certainly one of them." She also said " he's misunderstood in the industry, somewhat. He is so generous to actors."
Awards.
Cameron received the inaugural Bradbury Award from the Science Fiction and Fantasy Writers of America in 1992 for "" ("Avatar" would be a finalist in 2010).
Cameron did not receive any major mainstream filmmaking awards prior to "Titanic". For "Titanic" he won several including Academy Awards for Best Picture (shared with Jon Landau), Best Director and Best Film Editing (shared with Conrad Buff and Richard A. Harris). Cameron is one of the few filmmakers to win three Oscars in a single evening and Golden Globes for Best Motion Picture – Drama and Best Director.
In recognition of "a distinguished career as a Canadian filmmaker", Carleton University, Ottawa, awarded Cameron the honorary degree of Doctor of Fine Arts on June 13, 1998. Cameron accepted the degree in person and gave the Convocation Address.
He also received an honorary doctorate in October 1998 from Brock University in St. Catharines, Ontario, for his accomplishments in the international film industry.
In 1998, Cameron attended convocation to receive an honorary doctorate of Laws from Ryerson University, Toronto. The university awards its highest honor to those who have made extraordinary contributions in Canada, or internationally.
In 1999, Cameron received the honorary Doctor of Fine Arts degree from California State University, Fullerton, where he had been a student in the 1970s. He received the degree at the university's annual Commencement exercises that year, where he gave the keynote speech.
In recognition of his contributions to underwater filming and remote vehicle technology, the University of Southampton awarded Cameron the honorary degree of Doctor of the University. Cameron did not attend the Engineering Sciences graduation ceremony in July 2004 where the degree was awarded but instead received it in person at the National Oceanography Centre.
On June 3, 2008, it was announced that he would be inducted into Canada's Walk of Fame. On December 18, 2009, the same day "Avatar" was released worldwide, Cameron received the 2,396th star on the Hollywood Walk of Fame. After the release of "Avatar", on February 28, 2010, Cameron was also honored with a Visual Effects Society (VES) Lifetime Achievement Award.
For "Avatar", Cameron won numerous awards as well, including: Golden Globes for Best Motion Picture – Drama (shared with Jon Landau) and Best Director. He was nominated for three Academy Awards: Best Picture, Best Director and Best Film Editing (shared with John Refoua and Stephen E. Rivkin). However, Cameron and "Avatar" lost to his former wife Kathryn Bigelow and her film, "The Hurt Locker".
On September 24, 2010, James Cameron was named Number 1 in The 2010 Guardian Film Power 100 list. In a list compiled by the British magazine "New Statesman" in September 2010, he was listed 30th in the list of "The World's 50 Most Influential Figures 2010".
The Science Fiction Hall of Fame inducted Cameron in June 2012.
Awards.
Cameron has received numerous awards; mainly for "Titanic" and "Avatar".
Collaborations.
Cameron often casts certain actors more than once in his films. Cameron has consistently worked with Bill Paxton, Michael Biehn, Lance Henriksen, Jenette Goldstein and Arnold Schwarzenegger.
1 Although Wisher Jr. has written some of Cameron's works, he is listed in the above table as an actor.<br>
2 His reprised role of Reese was cut from the theatrical release, but restored in the DVD's Special Edition Version.
Recurring themes.
Cameron's films have recurring themes and subtexts. These include the conflicts between humanity and technology, the dangers of corporate greed, strong female characters, and a strong romance subplot. In almost all films, the main characters usually get into dramatic crisis situations with significant threats to their own life or even the threat of an impending apocalypse.
While "The Abyss" dealt with deep sea exploration (shot in an unfinished nuclear reactor filled with water) and Cameron himself became an expert in the field of deep-sea wreckage exploration, exploring the wreckage of the "Titanic" and the "Bismarck". Cameron will return to this theme with "The Dive", shooting from a minisub.
Filmography.
Cameron has contributed to many projects as a writer, director, and producer, or as a combination of the three.
Cameron's first film was the 1978 science fiction short film "Xenogenesis", which he directed, wrote and produced. Cameron's films have grossed a total of over $7 billion worldwide.
In addition to works of fiction, Cameron has directed and appeared in several documentaries including "Ghosts of the Abyss" and "Aliens of the Deep". He also contributed to a number of television series including "Dark Angel" and "Entourage". He plans to shoot a small drama film after the Avatar trilogy, just to prove that 3D works even for domestic dramas.
Reception.
Critical, public and commercial reception to films James Cameron has directed as of May 7, 2015.
Further reading.
</dl>

</doc>
<doc id="15624" url="http://en.wikipedia.org/wiki?curid=15624" title="Judaism">
Judaism

Judaism (from the Latin: "Iudaismus", derived from the Greek Ἰουδαϊσμός, and ultimately from the Hebrew יהודה, "Yehudah", "Judah"; in Hebrew: יהדות, "Yahadut", the distinctive characteristics of the Judean ethnos) encompasses the religion, philosophy, culture and way of life of the Jewish people. Judaism is an ancient monotheistic religion, with the Torah as its foundational text (part of the larger text known as the Tanakh or Hebrew Bible), and supplemental oral tradition represented by later texts such as the Midrash and the Talmud. Judaism is considered by religious Jews to be the expression of the covenantal relationship that God established with the Children of Israel.
Judaism includes a wide corpus of texts, practices, theological positions, and forms of organization. Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah. Historically, this assertion was challenged by various groups such as the Sadducees and Hellenistic Judaism during the Second Temple period; the Karaites and Sabbateans during the early and later medieval period; and among segments of the modern reform movements. Liberal movements in modern times such as Humanistic Judaism may be nontheistic. Today, the largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism and Reform Judaism. Major sources of difference between these groups are their approaches to Jewish law, the authority of the Rabbinic tradition, and the significance of the State of Israel. Orthodox Judaism maintains that the Torah and Jewish law are divine in origin, eternal and unalterable, and that they should be strictly followed. Conservative and Reform Judaism are more liberal, with Conservative Judaism generally promoting a more "traditional" interpretation of Judaism's requirements than Reform Judaism. A typical Reform position is that Jewish law should be viewed as a set of general guidelines rather than as a set of restrictions and obligations whose observance is required of all Jews. Historically, special courts enforced Jewish law; today, these courts still exist but the practice of Judaism is mostly voluntary. Authority on theological and legal matters is not vested in any one person or organization, but in the sacred texts and rabbis and scholars who interpret them.
Judaism claims a historical continuity spanning more than 3,000 years. Judaism has its roots as a structured religion in the Middle East during the Bronze Age. Of the major world religions, Judaism is considered one of the oldest monotheistic religions. The Hebrews / Israelites were already referred to as "Jews" in later books of the Tanakh such as the Book of Esther, with the term Jews replacing the title "Children of Israel". Judaism's texts, traditions and values strongly influenced later Abrahamic religions, including Christianity, Islam and the Baha'i Faith. Many aspects of Judaism have also directly or indirectly influenced secular Western ethics and civil law.
The Jews are an ethnoreligious group and include those born Jewish and converts to Judaism. In 2012, the world Jewish population was estimated at about 14 million, or roughly 0.2% of the total world population. About 42% of all Jews reside in Israel and about 42% reside in the United States and Canada, with most of the remainder living in Europe, and other minority groups spread throughout the world in South America, Asia, Africa, and Australia.
Defining characteristics and principles of faith.
Defining characteristics.
Unlike other ancient Near Eastern gods, the Hebrew God is portrayed as unitary and solitary; consequently, the Hebrew God's principal relationships are not with other gods, but with the world, and more specifically, with the people He created. Judaism thus begins with ethical monotheism: the belief that God is one and is concerned with the actions of humankind. According to the Tanakh (Hebrew Bible), God promised Abraham to make of his offspring a great nation. Many generations later, he commanded the nation of Israel to love and worship only one God; that is, the Jewish nation is to reciprocate God's concern for the world. He also commanded the Jewish people to love one another; that is, Jews are to imitate God's love for people. These commandments are but two of a large corpus of commandments and laws that constitute this covenant, which is the substance of Judaism.
Thus, although there is an esoteric tradition in Judaism (Kabbalah), Rabbinic scholar Max Kadushin has characterized normative Judaism as "normal mysticism", because it involves everyday personal experiences of God through ways or modes that are common to all Jews. This is played out through the observance of the Halakha and given verbal expression in the Birkat Ha-Mizvot, the short blessings that are spoken every time a positive commandment is to be fulfilled.
Whereas Jewish philosophers often debate whether God is immanent or transcendent, and whether people have free will or their lives are determined, Halakha is a system through which any Jew acts to bring God into the world.
Ethical monotheism is central in all sacred or normative texts of Judaism. However, monotheism has not always been followed in practice. The Jewish Bible (Tanakh) records and repeatedly condemns the widespread worship of other gods in ancient Israel. In the Greco-Roman era, many different interpretations of monotheism existed in Judaism, including the interpretations that gave rise to Christianity.
Moreover, as a non-creedal religion, some have argued that Judaism does not require one to believe in God. For some, observance of Jewish law is more important than belief in God "per se". In modern times, some liberal Jewish movements do not accept the existence of a personified deity active in history.
Core tenets.
13 Principles of Faith:
—Maimonides
Scholars throughout Jewish history have proposed numerous formulations of Judaism's core tenets, all of which have met with criticism. The most popular formulation is Maimonides' thirteen principles of faith, developed in the 12th century. According to Maimonides, any Jew who rejects even one of these principles would be considered an apostate and a heretic. Jewish scholars have held points of view diverging in various ways from Maimonides' principles.
In Maimonides' time, his list of tenets was criticized by Hasdai Crescas and Joseph Albo. Albo and the Raavad argued that Maimonides' principles contained too many items that, while true, were not fundamentals of the faith.
Along these lines, the ancient historian Josephus emphasized practices and observances rather than religious beliefs, associating apostasy with a failure to observe Jewish law and maintaining that the requirements for conversion to Judaism included circumcision and adherence to traditional customs. Maimonides' principles were largely ignored over the next few centuries. Later, two poetic restatements of these principles (""Ani Ma'amin" and "Yigdal"") became integrated into many Jewish liturgies, leading to their eventual near-universal acceptance.
In modern times, Judaism lacks a centralized authority that would dictate an exact religious dogma. Because of this, many different variations on the basic beliefs are considered within the scope of Judaism. Even so, all Jewish religious movements are, to a greater or lesser extent, based on the principles of the Hebrew Bible and various commentaries such as the Talmud and Midrash. Judaism also universally recognizes the Biblical Covenant between God and the Patriarch Abraham as well as the additional aspects of the Covenant revealed to Moses, who is considered Judaism's greatest prophet. In the Mishnah, a core text of Rabbinic Judaism, acceptance of the Divine origins of this covenant is considered an essential aspect of Judaism and those who reject the Covenant forfeit their share in the World to Come.
Jewish religious texts.
The following is a basic, structured list of the central works of Jewish practice and thought.
Jewish legal literature.
The basis of Jewish law and tradition (halakha) is the Torah (also known as the Pentateuch or the Five Books of Moses). According to rabbinic tradition there are 613 commandments in the Torah. Some of these laws are directed only to men or to women, some only to the ancient priestly groups, the Kohanim and Leviyim (members of the tribe of Levi), some only to farmers within the Land of Israel. Many laws were only applicable when the Temple in Jerusalem existed, and fewer than 300 of these commandments are still applicable today.
While there have been Jewish groups whose beliefs were claimed to be based on the written text of the Torah alone (e.g., the Sadducees, and the Karaites), most Jews believed in what they call the oral law. These oral traditions were transmitted by the Pharisee sect of ancient Judaism, and were later recorded in written form and expanded upon by the rabbis.
Rabbinic Judaism (which derives from the Pharisees) has always held that the books of the Torah (called the written law) have always been transmitted in parallel with an oral tradition. To justify this viewpoint, Jews point to the text of the Torah, where many words are left undefined, and many procedures mentioned without explanation or instructions; this, they argue, means that the reader is assumed to be familiar with the details from other, i.e., oral, sources. This parallel set of material was originally transmitted orally, and came to be known as "the oral law".
By the time of Rabbi Judah haNasi (200 CE), after the destruction of Jerusalem, much of this material was edited together into the Mishnah. Over the next four centuries this law underwent discussion and debate in both of the world's major Jewish communities (in Israel and Babylonia), and the commentaries on the Mishnah from each of these communities eventually came to be edited together into compilations known as the two Talmuds. These have been expounded by commentaries of various Torah scholars during the ages.
Halakha, the rabbinic Jewish way of life, then, is based on a combined reading of the Torah, and the oral tradition - the Mishnah, the halakhic Midrash, the Talmud and its commentaries. The Halakha has developed slowly, through a precedent-based system. The literature of questions to rabbis, and their considered answers, is referred to as responsa (in Hebrew, "Sheelot U-Teshuvot".) Over time, as practices develop, codes of Jewish law are written that are based on the responsa; the most important code, the Shulchan Aruch, largely determines Orthodox religious practice today.
Jewish philosophy.
Jewish philosophy refers to the conjunction between serious study of philosophy and Jewish theology. Major Jewish philosophers include Solomon ibn Gabirol, Saadia Gaon, Judah Halevi, Maimonides, and Gersonides. Major changes occurred in response to the Enlightenment (late 18th to early 19th century) leading to the post-Enlightenment Jewish philosophers. Modern Jewish philosophy consists of both Orthodox and non-Orthodox oriented philosophy. Notable among Orthodox Jewish philosophers are Eliyahu Eliezer Dessler, Joseph B. Soloveitchik, and Yitzchok Hutner. Well-known non-Orthodox Jewish philosophers include Martin Buber, Franz Rosenzweig, Mordecai Kaplan, Abraham Joshua Heschel, Will Herberg, and Emmanuel Lévinas.
Related Topics
Rabbinic hermeneutics.
13 Principles of Hermeneutics:
—R. Ishmael
Orthodox and many other Jews do not believe that the revealed Torah consists solely of its written contents, but of its interpretations as well. The study of Torah (in its widest sense, to include both poetry, narrative, and law, and both the Hebrew Bible and the Talmud) is in Judaism itself a sacred act of central importance. For the sages of the Mishnah and Talmud, and for their successors today, the study of Torah was therefore not merely a means to learn the contents of God's revelation, but an end in itself. According to the Talmud,
In Judaism, "the study of Torah can be a means of experiencing God". Reflecting on the contribution of the Amoraim and Tanaim to contemporary Judaism, Professor Jacob Neusner observed:
To study the Written Torah and the Oral Torah in light of each other is thus also to study "how" to study the word of God.
In the study of Torah, the sages formulated and followed various logical and hermeneutical principles. According to David Stern, all Rabbinic hermeneutics rest on two basic axioms:
These two principles make possible a great variety of interpretations. According to the Talmud,
Observant Jews thus view the Torah as dynamic, because it contains within it a host of interpretations
According to Rabbinic tradition, all valid interpretations of the written Torah were revealed to Moses at Sinai in oral form, and handed down from teacher to pupil (The oral revelation is in effect coextensive with the Talmud itself). When different rabbis forwarded conflicting interpretations, they sometimes appealed to hermeneutic principles to legitimize their arguments; some rabbis claim that these principles were themselves revealed by God to Moses at Sinai.
Thus, Hillel called attention to seven commonly used hermeneutical principles in the interpretation of laws (baraita at the beginning of Sifra); R. Ishmael, thirteen (baraita at the beginning of Sifra; this collection is largely an amplification of that of Hillel). Eliezer b. Jose ha-Gelili listed 32, largely used for the exegesis of narrative elements of Torah. All the hermeneutic rules scattered through the Talmudim and Midrashim have been collected by Malbim in "Ayyelet ha-Shachar," the introduction to his commentary on the Sifra. Nevertheless, R. Ishmael's 13 principles are perhaps the ones most widely known; they constitute an important, and one of Judaism's earliest, contributions to logic, hermeneutics, and jurisprudence. Judah Hadassi incorporated Ishmael's principles into Karaite Judaism in the 12th century. Today R. Ishmael's 13 principles are incorporated into the Jewish prayer book to be read by observant Jews on a daily basis.
Jewish identity.
Origin of the term "Judaism".
The term Judaism derives from "Iudaismus", a Latinized form of the Ancient Greek Ἰουδαϊσμός or "Ioudaïsmos" (from the verb ἰουδαΐζειν, "to side with or imitate the [Judeans]"), and it was ultimately inspired by the Hebrew יהודה, "Yehudah", "Judah"; in Hebrew: יַהֲדוּת, "Yahadut". The term "Ἰουδαϊσμός" first appears in the Hellenistic Greek book of 2 Maccabees in the 2nd century BCE. In the context of the age and period it meant "seeking or forming part of a cultural entity" and resembled its antonym "hellenismos", a word that signified a people's submission unto Hellenic (Greek) cultural norms. The conflict between "iudaismos" and "hellenismos" lay behind the Maccabean revolt and hence the invention of the term "iudaismos". Shaye J. D. Cohen writes in his book "The Beginnings of Jewishness":
The earliest instance in Europe where the term was used to mean "the profession or practice of the Jewish religion; the religious system or polity of the Jews"{cn} is Robert Fabyan's "The newe cronycles of Englande and of Fraunce a 1513". "Judaism" as a direct translation of the Latin "Iudaismus" first occurred in a 1611 English translation of the Apocrypha (Deuterocanon in Catholic and Orthodox Christianity), 2 Macc. ii. 21: "Those that behaved themselues manfully to their honour for Iudaisme."
Distinction between Jews as a people and Judaism.
According to Daniel Boyarin, the underlying distinction between religion and ethnicity is foreign to Judaism itself, and is one form of the dualism between spirit and flesh that has its origin in Platonic philosophy and that permeated Hellenistic Judaism. Consequently, in his view, Judaism does not fit easily into conventional Western categories, such as religion, ethnicity, or culture. Boyarin suggests that this in part reflects the fact that much of Judaism's more than 3,000-year history predates the rise of Western culture and occurred outside the West (that is, Europe, particularly medieval and modern Europe). During this time, Jews experienced slavery, anarchic and theocratic self-government, conquest, occupation, and exile. In the Diaspora, they were in contact with, and influenced by, ancient Egyptian, Babylonian, Persian, and Hellenic cultures, as well as modern movements such as the Enlightenment (see Haskalah) and the rise of nationalism, which would bear fruit in the form of a Jewish state in their ancient homeland, the Land of Israel. They also saw an elite population convert to Judaism (the Khazars), only to disappear as the centers of power in the lands once occupied by that elite fell to the people of Rus and then the Mongols. Thus, Boyarin has argued that "Jewishness disrupts the very categories of identity, because it is not national, not genealogical, not religious, but all of these, in dialectical tension."
In contrast to this point of view, practices such as Humanistic Judaism reject the religious aspects of Judaism, while retaining certain cultural traditions.
Who is a Jew?
According to traditional Jewish Law, a Jew is anyone who was either born of a Jewish mother or who converted to Judaism in accordance with Jewish Law. American Reform Judaism and British Liberal Judaism accept the child of one Jewish parent (father or mother) as Jewish if the parents raise the child with a Jewish identity. All mainstream forms of Judaism today are open to sincere converts, although conversion has traditionally been discouraged since the time of the Talmud. The conversion process is evaluated by an authority, and the convert is examined on his or her sincerity and knowledge. Converts are called "ben Abraham" or "bat Abraham", (son or daughter of Abraham). Conversions have on occasion been overturned. In 2008, Israel's highest religious court invalidated the conversion of 40,000 Jews, mostly from Russian immigrant families, even though they had been approved by an Orthodox rabbi.
Traditional Judaism maintains that a Jew, whether by birth or conversion, is a Jew forever. Thus a Jew who claims to be an atheist or converts to another religion is still considered by traditional Judaism to be Jewish. According to some sources, the Reform movement has maintained that a Jew who has converted to another religion is no longer a Jew, and the Israeli Government has also taken that stance after Supreme Court cases and statutes. However, the Reform movement has indicated that this is not so cut and dried, and different situations call for consideration and differing actions. For example, Jews who have converted under duress may be permitted to return to Judaism "without any action on their part but their desire to rejoin the Jewish community" and "A proselyte who has become an apostate remains, nevertheless, a Jew". (p. 100-106).
The question of what determines Jewish identity in the State of Israel was given new impetus when, in the 1950s, David Ben-Gurion requested opinions on "mihu Yehudi" ("who is a Jew") from Jewish religious authorities and intellectuals worldwide in order to settle citizenship questions. This is still not settled, and occasionally resurfaces in Israeli politics.
Jewish demographics.
The total number of Jews worldwide is difficult to assess because the definition of "who is a Jew" is problematic; not all Jews identify themselves as Jewish, and some who identify as Jewish are not considered so by other Jews. According to the "Jewish Year Book" (1901), the global Jewish population in 1900 was around 11 million. The latest available data is from the World Jewish Population Survey of 2002 and the Jewish Year Calendar (2005). In 2002, according to the Jewish Population Survey, there were 13.3 million Jews around the world. The Jewish Year Calendar cites 14.6 million. Jewish population growth is currently near zero percent, with 0.3% growth from 2000 to 2001.
Jewish religious movements.
Rabbinic Judaism.
Rabbinic Judaism (or in some Christian traditions, Rabbinism) (Hebrew: "Yahadut Rabanit" – יהדות רבנית) has been the mainstream form of Judaism since the 6th century CE, after the codification of the Talmud. It is characterised by the belief that the Written Torah (Written Law) cannot be correctly interpreted without reference to the Oral Torah and the voluminous literature specifying what behavior is sanctioned by the Law.
The Jewish Enlightenment of the late 18th century resulted in the division of Ashkenazi (Western) Jewry into religious movements or denominations, especially in North America and Anglophone countries. The main denominations today outside Israel (where the situation is rather different) are Orthodox, Conservative, and Reform.
Jewish movements in Israel.
Most Jewish Israelis classify themselves as "secular" ("hiloni"), "traditional" ("masorti"), "religious" ("dati") or "Haredi". The term "secular" is more popular as a self-description among Israeli families of western (European) origin, whose Jewish identity may be a very powerful force in their lives, but who see it as largely independent of traditional religious belief and practice. This portion of the population largely ignores organized religious life, be it of the official Israeli rabbinate (Orthodox) or of the liberal movements common to diaspora Judaism (Reform, Conservative).
The term "traditional" ("masorti") is most common as a self-description among Israeli families of "eastern" origin (i.e., the Middle East, Central Asia, and North Africa). This term, as commonly used, has nothing to do with the official Masorti (Conservative) movement. There is a great deal of ambiguity in the ways "secular" and "traditional" are used in Israel: they often overlap, and they cover an extremely wide range in terms of ideology and religious observance. The term "Orthodox" is not popular in Israeli discourse, although the percentage of Jews who come under that category is far greater than in the diaspora. What would be called "Orthodox" in the diaspora includes what is commonly called "dati" (religious) or "haredi" (ultra-Orthodox) in Israel. The former term includes what is called "Religious Zionism" or the "National Religious" community, as well as what has become known over the past decade or so as "haredi-leumi" (nationalist "haredi"), or "Hardal", which combines a largely "haredi" lifestyle with nationalist ideology. (Some people, in Yiddish, also refer to observant Orthodox Jews as "frum", as opposed to "frei" (more liberal Jews)).
"Haredi" applies to a populace that can be roughly divided into three separate groups along both ethnic and ideological lines: (1) "Lithuanian" (non-hasidic) "haredim" of Ashkenazic origin; (2) Hasidic "haredim" of Ashkenazic origin; and (3) Sephardic "haredim".
Alternative Judaism.
Karaite Judaism defines itself as the remnants of the non-Rabbinic Jewish sects of the Second Temple period, such as the Sadducees. The Karaites ("Scripturalists") accept only the Hebrew Bible and what they view as the Peshat ("simple" meaning); they do not accept non-biblical writings as authoritative. Some European Karaites do not see themselves as part of the Jewish community at all, although most do. 
The Samaritans, a very small community located entirely around Mount Gerizim in the Nablus/Shechem region of the West Bank and in Holon, near Tel Aviv in Israel, regard themselves as the descendants of the Israelites of the Iron Age kingdom of Israel. Their religious practices are based on the literal text of the written Torah (Five Books of Moses), which they view as the only authoritative scripture (with a special regard also for the Samaritan Book of Joshua).
Jewish observances.
Jewish ethics.
Jewish ethics may be guided by halakhic traditions, by other moral principles, or by central Jewish virtues. Jewish ethical practice is typically understood to be marked by values such as justice, truth, peace, loving-kindness (chesed), compassion, humility, and self-respect. Specific Jewish ethical practices include practices of charity (tzedakah) and refraining from negative speech (lashon hara). Proper ethical practices regarding sexuality and many other issues are subjects of dispute among Jews.
Prayers.
Traditionally, Jews recite prayers three times daily, Shacharit, Mincha, and Ma'ariv with a fourth prayer, Mussaf added on Shabbat and holidays. At the heart of each service is the "Amidah" or "Shemoneh Esrei". Another key prayer in many services is the declaration of faith, the "Shema Yisrael" (or "Shema"). The "Shema" is the recitation of a verse from the Torah (Deuteronomy 6:4): "Shema Yisrael Adonai Eloheinu Adonai Echad"—"Hear, O Israel! The Lord is our God! The Lord is One!"
Most of the prayers in a traditional Jewish service can be recited in solitary prayer, although communal prayer is preferred. Communal prayer requires a quorum of ten adult Jews, called a "minyan". In nearly all Orthodox and a few Conservative circles, only male Jews are counted toward a "minyan"; most Conservative Jews and members of other Jewish denominations count female Jews as well.
In addition to prayer services, observant traditional Jews recite prayers and benedictions throughout the day when performing various acts. Prayers are recited upon waking up in the morning, before eating or drinking different foods, after eating a meal, and so on.
The approach to prayer varies among the Jewish denominations. Differences can include the texts of prayers, the frequency of prayer, the number of prayers recited at various religious events, the use of musical instruments and choral music, and whether prayers are recited in the traditional liturgical languages or the vernacular. In general, Orthodox and Conservative congregations adhere most closely to tradition, and Reform and Reconstructionist synagogues are more likely to incorporate translations and contemporary writings in their services. Also, in most Conservative synagogues, and all Reform and Reconstructionist congregations, women participate in prayer services on an equal basis with men, including roles traditionally filled only by men, such as reading from the Torah. In addition, many Reform temples use musical accompaniment such as organs and mixed choirs.
Religious clothing.
A "kippah" (Hebrew: כִּפָּה, plural "kippot"; Yiddish: יאַרמלקע, "yarmulke") is a slightly rounded brimless skullcap worn by many Jews while praying, eating, reciting blessings, or studying Jewish religious texts, and at all times by some Jewish men. In Orthodox communities, only men wear kippot; in non-Orthodox communities, some women also wear kippot. "Kippot" range in size from a small round beanie that covers only the back of the head, to a large, snug cap that covers the whole crown.
"Tzitzit" (Hebrew: צִיציִת) (Ashkenazi pronunciation: "tzitzis") are special knotted "fringes" or "tassels" found on the four corners of the "tallit" (Hebrew: טַלִּית) (Ashkenazi pronunciation: "tallis"), or prayer shawl. The "tallit" is worn by Jewish men and some Jewish women during the prayer service. Customs vary regarding when a Jew begins wearing a tallit. In the Sephardi community, boys wear a tallit from bar mitzvah age. In some Ashkenazi communities it is customary to wear one only after marriage. A "tallit katan" (small tallit) is a fringed garment worn under the clothing throughout the day. In some Orthodox circles, the fringes are allowed to hang freely outside the clothing.
Tefillin (Hebrew: תְפִלִּין), known in English as phylacteries (from the Greek word φυλακτήριον, meaning "safeguard" or "amulet"), are two square leather boxes containing biblical verses, attached to the forehead and wound around the left arm by leather straps. They are worn during weekday morning prayer by observant Jewish men and some Jewish women.
A "kittel" (Yiddish: קיטל), a white knee-length overgarment, is worn by prayer leaders and some observant traditional Jews on the High Holidays. It is traditional for the head of the household to wear a kittel at the Passover seder in some communities, and some grooms wear one under the wedding canopy. Jewish males are buried in a "tallit" and sometimes also a "kittel" which are part of the "tachrichim" (burial garments).
Jewish holidays.
Jewish holidays are special days in the Jewish calendar, which celebrate moments in Jewish history, as well as central themes in the relationship between God and the world, such as creation, revelation, and redemption.
Shabbat.
"Shabbat", the weekly day of rest lasting from shortly before sundown on Friday night to nightfall Saturday night, commemorates God's day of rest after six days of creation. It plays a pivotal role in Jewish practice and is governed by a large corpus of religious law. At sundown on Friday, the woman of the house welcomes the Shabbat by lighting two or more candles and reciting a blessing. The evening meal begins with the Kiddush, a blessing recited aloud over a cup of wine, and the Mohtzi, a blessing recited over the bread. It is customary to have challah, two braided loaves of bread, on the table. During Shabbat Jews are forbidden to engage in any activity that falls under 39 categories of "melakhah", translated literally as "work". In fact the activities banned on the Sabbath are not "work" in the usual sense: They include such actions as lighting a fire, writing, using money and carrying in the public domain. The prohibition of lighting a fire has been extended in the modern era to driving a car, which involves burning fuel, and using electricity.
Three pilgrimage festivals.
Jewish holy days ("chaggim"), celebrate landmark events in Jewish history, such as the Exodus from Egypt and the giving of the Torah, and sometimes mark the change of seasons and transitions in the agricultural cycle. The three major festivals, Sukkot, Passover and Shavuot, are called "regalim" (derived from the Hebrew word "regel", or foot). On the three regalim, it was customary for the Israelites to make pilgrimages to Jerusalem to offer sacrifices in the Temple.
High Holy Days.
The High Holidays ("Yamim Noraim" or "Days of Awe") revolve around judgment and forgiveness.
Purim.
Purim (Hebrew:    "Pûrîm" "lots") is a joyous Jewish holiday that commemorates the deliverance of the Persian Jews from the plot of the evil Haman, who sought to exterminate them, as recorded in the biblical Book of Esther. It is characterized by public recitation of the Book of Esther, mutual gifts of food and drink, charity to the poor, and a celebratory meal (Esther 9:22). Other customs include drinking wine, eating special pastries called hamantashen, dressing up in masks and costumes, and organizing carnivals and parties.
Purim is celebrated annually on the 14th of the Hebrew month of Adar, which occurs in February or March of the Gregorian calendar.
Hanukkah.
Hanukkah (Hebrew: חֲנֻכָּה‎, "dedication") also known as the Festival of Lights, is an eight-day Jewish holiday that starts on the 25th day of Kislev (Hebrew calendar). The festival is observed in Jewish homes by the kindling of lights on each of the festival's eight nights, one on the first night, two on the second night and so on.
The holiday was called Hanukkah (meaning "dedication") because it marks the re-dedication of the Temple after its desecration by Antiochus IV Epiphanes. Spiritually, Hanukkah commemorates the "Miracle of the Oil". According to the Talmud, at the re-dedication of the Temple in Jerusalem following the victory of the Maccabees over the Seleucid Empire, there was only enough consecrated oil to fuel the eternal flame in the Temple for one day. Miraculously, the oil burned for eight days - which was the length of time it took to press, prepare and consecrate new oil.
Hanukkah is not mentioned in the Bible and was never considered a major holiday in Judaism, but it has become much more visible and widely celebrated in modern times, mainly because it falls around the same time as Christmas and has national Jewish overtones that have been emphasized since the establishment of the State of Israel.
Other days.
Tisha B'Av (Hebrew: תשעה באב‎ or ט׳ באב, "the Ninth of Av") is a day of mourning and fasting commemorating the destruction of the First and Second Temples, and in later times, the expulsion of the Jews from Spain.
The modern holidays of Yom Ha-shoah (Holocaust Remembrance Day) and Yom Ha'atzmaut (Israeli Independence Day) commemorate the horrors of the Holocaust and the achievement of Israel independence, respectively.
Torah readings.
The core of festival and Shabbat prayer services is the public reading of the Torah, along with connected readings from the other books of the Tanakh, called Haftarah. Over the course of a year, the whole Torah is read, with the cycle starting over in the autumn, on Simchat Torah.
Synagogues and religious buildings.
Synagogues are Jewish houses of prayer and study. They usually contain separate rooms for prayer (the main sanctuary), smaller rooms for study, and often an area for community or educational use. There is no set blueprint for synagogues and the architectural shapes and interior designs of synagogues vary greatly. The Reform movement mostly refer to their synagogues as temples. Some traditional features of a synagogue are:
In addition to synagogues, other buildings of significance in Judaism include yeshivas, or institutions of Jewish learning, and mikvahs, which are ritual baths.
Dietary laws: "kashrut".
The Jewish dietary laws are known as "kashrut". Food prepared in accordance with them is termed kosher, and food that is not kosher is also known as "treifah" or "treif". People who observe these laws are colloquially said to be "keeping kosher".
Many of the laws apply to animal-based foods. For example, in order to be considered kosher, mammals must have split hooves and chew their cud. The pig is arguably the most well-known example of a non-kosher animal. Although it has split hooves, it does not chew its cud. For seafood to be kosher, the animal must have fins and scales. Certain types of seafood, such as shellfish, crustaceans, and eels, are therefore considered non-kosher. Concerning birds, a list of non-kosher species is given in the Torah. The exact translations of many of the species have not survived, and some non-kosher birds' identities are no longer certain. However, traditions exist about the "kashrut" status of a few birds. For example, both chickens and turkeys are permitted in most communities. Other types of animals, such as amphibians, reptiles, and most insects, are prohibited altogether.
In addition to the requirement that the species be considered kosher, meat and poultry (but not fish) must come from a healthy animal slaughtered in a process known as "shechitah". Without the proper slaughtering practices even an otherwise kosher animal will be rendered "treif". The slaughtering process is intended to be quick and relatively painless to the animal. Forbidden parts of animals include the blood, some fats, and the area in and around the sciatic nerve.
Jewish law also forbids the consumption of meat and dairy products together. The waiting period between eating meat and eating dairy varies by the order in which they are consumed and by community, and can extend for up to six hours. Based on the Biblical injunction against cooking a kid in its mother's milk, this rule is mostly derived from the Oral Torah, the Talmud and Rabbinic law. Chicken and other kosher birds are considered the same as meat under the laws of "kashrut", but the prohibition is Rabbinic, not Biblical.
The use of dishes, serving utensils, and ovens may make food "treif" that would otherwise be kosher. Utensils that have been used to prepare non-kosher food, or dishes that have held meat and are now used for dairy products, render the food "treif" under certain conditions.
Furthermore, all Orthodox and some Conservative authorities forbid the consumption of processed grape products made by non-Jews, due to ancient pagan practices of using wine in rituals. Some Conservative authorities permit wine and grape juice made without rabbinic supervision.
The Torah does not give specific reasons for most of the laws of "kashrut". However, a number of explanations have been offered, including maintaining ritual purity, teaching impulse control, encouraging obedience to God, improving health, reducing cruelty to animals and preserving the distinctness of the Jewish community. The various categories of dietary laws may have developed for different reasons, and some may exist for multiple reasons. For example, people are forbidden from consuming the blood of birds and mammals because, according to the Torah, this is where animal souls are contained. In contrast, the Torah forbids Israelites from eating non-kosher species because "they are unclean". The Kabbalah describes sparks of holiness that are released by the act of eating kosher foods, but are too tightly bound in non-kosher foods to be released by eating.
Survival concerns supersede all the laws of "kashrut", as they do for most halakhot.
Laws of ritual purity.
The Tanakh describes circumstances in which a person who is "tahor" or ritually pure may become "tamei" or ritually impure. Some of these circumstances are contact with human corpses or graves, seminal flux, vaginal flux, menstruation, and contact with people who have become impure from any of these. In Rabbinic Judaism, Kohanim, members of the hereditary caste that served as priests in the time of the Temple, are mostly restricted from entering grave sites and touching dead bodies. During the Temple period, such priests (Kohanim) were required to eat their bread offering (Terumah) in a state of ritual purity, which laws eventually led to more rigid laws being enacted, such as hand-washing which became a requisite of all Jews before consuming ordinary bread.
Family purity.
An important subcategory of the ritual purity laws relates to the segregation of menstruating women. These laws are also known as "niddah", literally "separation", or family purity. Vital aspects of halakha for traditionally observant Jews, they are not usually followed by Jews in liberal denominations.
Especially in Orthodox Judaism, the Biblical laws are augmented by Rabbinical injunctions. For example, the Torah mandates that a woman in her normal menstrual period must abstain from sexual intercourse for seven days. A woman whose menstruation is prolonged must continue to abstain for seven more days after bleeding has stopped. The Rabbis conflated ordinary "niddah" with this extended menstrual period, known in the Torah as "zavah", and mandated that a woman may not have sexual intercourse with her husband from the time she begins her menstrual flow until seven days after it ends. In addition, Rabbinical law forbids the husband from touching or sharing a bed with his wife during this period. Afterwards, purification can occur in a ritual bath called a mikveh.
Traditional Ethiopian Jews keep menstruating women in separate huts and, similar to Karaite practice, do not allow menstruating women into their temples because of a temple's special sanctity. Emigration to Israel and the influence of other Jewish denominations have led to Ethiopian Jews adopting more normative Jewish practices.
Life-cycle events.
Life-cycle events, or rites of passage, occur throughout a Jew's life that serve to strengthen Jewish identity and bind him/her to the entire community.
Community leadership.
Classical priesthood.
The role of the priesthood in Judaism has significantly diminished since the destruction of the Second Temple in 70 CE, when priests attended to the Temple and sacrifices. The priesthood is an inherited position, and although priests no longer have any but ceremonial duties, they are still honored in many Jewish communities. Many Orthodox Jewish communities believe that they will be needed again for a future Third Temple and need to remain in readiness for future duty.
Prayer leaders.
From the time of the Mishnah and Talmud to the present, Judaism has required specialists or authorities for the practice of very few rituals or ceremonies. A Jew can fulfill most requirements for prayer by himself. Some activities—reading the Torah and "haftarah" (a supplementary portion from the Prophets or Writings), the prayer for mourners, the blessings for bridegroom and bride, the complete grace after meals—require a "minyan", the presence of ten Jews.
The most common professional clergy in a synagogue are:
Jewish prayer services do involve two specified roles, which are sometimes, but not always, filled by a rabbi or hazzan in many congregations. In other congregations these roles are filled on an ad-hoc basis by members of the congregation who lead portions of services on a rotating basis:
Many congregations, especially larger ones, also rely on a:
The three preceding positions are usually voluntary and considered an honor. Since the Enlightenment large synagogues have often adopted the practice of hiring rabbis and hazzans to act as "shatz" and "baal kriyah", and this is still typically the case in many Conservative and Reform congregations. However, in most Orthodox synagogues these positions are filled by laypeople on a rotating or ad-hoc basis. Although most congregations hire one or more Rabbis, the use of a professional hazzan is generally declining in American congregations, and the use of professionals for other offices is rarer still.
History.
Origins.
At its core, the Tanakh is an account of the Israelites' relationship with God from their earliest history until the building of the Second Temple (c. 535 BCE). Abraham is hailed as the first Hebrew and the father of the Jewish people. As a reward for his act of faith in one God, he was promised that Isaac, his second son, would inherit the Land of Israel (then called Canaan). Later, the descendants of Isaac's son Jacob were enslaved in Egypt, and God commanded Moses to lead the Exodus from Egypt. At Mount Sinai they received the Torah—the five books of Moses. These books, together with Nevi'im and Ketuvim are known as "Torah Shebikhtav" as opposed to the Oral Torah, which refers to the Mishnah and the Talmud. Eventually, God led them to the land of Israel where the tabernacle was planted in the city of Shiloh for over 300 years to rally the nation against attacking enemies. As time went on, the spiritual level of the nation declined to the point that God allowed the Philistines to capture the tabernacle. The people of Israel then told Samuel the prophet that they needed to be governed by a permanent king, and Samuel appointed Saul to be their King. When the people pressured Saul into going against a command conveyed to him by Samuel, God told Samuel to appoint David in his stead.
Once King David was established, he told the prophet Nathan that he would like to build a permanent temple, and as a reward for his actions, God promised David that he would allow his son, Solomon, to build the First Temple and the throne would never depart from his children.
Rabbinic tradition holds that the details and interpretation of the law, which are called the "Oral Torah" or "oral law", were originally an unwritten tradition based upon what God told Moses on Mount Sinai. However, as the persecutions of the Jews increased and the details were in danger of being forgotten, these oral laws were recorded by Rabbi Judah HaNasi (Judah the Prince) in the Mishnah, redacted "circa" 200 CE. The Talmud was a compilation of both the Mishnah and the Gemara, rabbinic commentaries redacted over the next three centuries. The Gemara originated in two major centers of Jewish scholarship, Palestine and Babylonia. Correspondingly, two bodies of analysis developed, and two works of Talmud were created. The older compilation is called the Jerusalem Talmud. It was compiled sometime during the 4th century in Israel. The Babylonian Talmud was compiled from discussions in the houses of study by the scholars Ravina I, Ravina II, and Rav Ashi by 500 CE, although it continued to be edited later.
Some critical scholars oppose the view that the sacred texts, including the Hebrew Bible, were divinely inspired. Many of these scholars accept the general principles of the documentary hypothesis and suggest that the Torah consists of inconsistent texts edited together in a way that calls attention to divergent accounts. Many suggest that during the First Temple period, the people of Israel believed that each nation had its own god, but that their god was superior to other gods. Some suggest that strict monotheism developed during the Babylonian Exile, perhaps in reaction to Zoroastrian dualism. In this view, it was only by the Hellenic period that most Jews came to believe that their god was the only god, and that the notion of a clearly bounded Jewish nation identical with the Jewish religion formed.
John Day argues that the origins of biblical Yahweh, El, Asherah, and Ba'al, may be rooted in earlier Canaanite religion, which was centered on a pantheon of gods much like the Greek pantheon.
Antiquity.
According to the Hebrew Bible, the United Monarchy was established under Saul and continued under King David and Solomon with its capital in Jerusalem. After Solomon's reign the nation split into two kingdoms, the Kingdom of Israel (in the north) and the Kingdom of Judah (in the south). The Kingdom of Israel was conquered by the Assyrian ruler Sargon II in the late 8th century BCE with many people from the capital Samaria being taken captive to Media and the Khabur River valley. The Kingdom of Judah continued as an independent state until it was conquered by a Babylonian army in the early 6th century BCE, destroying the First Temple that was at the center of ancient Jewish worship. The Judean elite were exiled to Babylonia and this is regarded as the first Jewish Diaspora. Later many of them returned to their homeland after the subsequent conquest of Babylonia by the Persians seventy years later, a period known as the Babylonian Captivity. A new Second Temple was constructed, and old religious practices were resumed.
During the early years of the Second Temple, the highest religious authority was a council known as the Great Assembly, led by Ezra of the Book of Ezra. Among other accomplishments of the Great Assembly, the last books of the Bible were written at this time and the canon sealed.
Hellenistic Judaism spread to Ptolemaic Egypt from the 3rd century BCE. After the Great Revolt (66–73 CE), the Romans destroyed the Temple. Hadrian built a pagan idol on the Temple grounds and prohibited circumcision; these acts of ethnocide provoked the Bar Kokhba revolt 132–136 CE after which the Romans banned the study of the Torah and the celebration of Jewish holidays, and forcibly removed virtually all Jews from Judea. In 200 CE, however, Jews were granted Roman citizenship and Judaism was recognized as a "religio licita" ("legitimate religion"), until the rise of Gnosticism and Early Christianity in the fourth century.
Following the destruction of Jerusalem and the expulsion of the Jews, Jewish worship stopped being centrally organized around the Temple, prayer took the place of sacrifice, and worship was rebuilt around the community (represented by a minimum of ten adult men) and the establishment of the authority of rabbis who acted as teachers and leaders of individual communities (see Jewish diaspora).
Historical Jewish groupings (to 1700).
Around the 1st century CE there were several small Jewish sects: the Pharisees, Sadducees, Zealots, Essenes, and Christians. After the destruction of the Second Temple in 70 CE, these sects vanished. Christianity survived, but by breaking with Judaism and becoming a separate religion; the Pharisees survived but in the form of Rabbinic Judaism (today, known simply as "Judaism"). The Sadducees rejected the divine inspiration of the Prophets and the Writings, relying only on the Torah as divinely inspired. Consequently, a number of other core tenets of the Pharisees' belief system (which became the basis for modern Judaism), were also dismissed by the Sadducees. (The Samaritans practiced a similar religion, which is traditionally considered separate from Judaism.)
Like the Sadducees who relied only on the Torah, some Jews in the 8th and 9th centuries rejected the authority and divine inspiration of the oral law as recorded in the Mishnah (and developed by later rabbis in the two Talmuds), relying instead only upon the Tanakh. These included the Isunians, the Yudganites, the Malikites, and others. They soon developed oral traditions of their own, which differed from the rabbinic traditions, and eventually formed the Karaite sect. Karaites exist in small numbers today, mostly living in Israel. Rabbinical and Karaite Jews each hold that the others are Jews, but that the other faith is erroneous.
Over a long time, Jews formed distinct ethnic groups in several different geographic areas — amongst others, the Ashkenazi Jews (of central and Eastern Europe), the Sephardi Jews (of Spain, Portugal, and North Africa), the Beta Israel of Ethiopia, and the Yemenite Jews from the southern tip of the Arabian Peninsula. Many of these groups have developed differences in their prayers, traditions and accepted canons; however these distinctions are mainly the result of their being formed at some cultural distance from normative (rabbinic) Judaism, rather than based on any doctrinal dispute.
Persecutions.
Antisemitism arose during the Middle Ages, in the form of persecutions, pogroms, forced conversion, expulsions, social restrictions and ghettoization.
This was different in quality to any repressions of Jews in ancient times. Ancient repression was politically motivated and Jews were treated the same way as any other ethnic group would have been. With the rise of the Churches, attacks on Jews became motivated instead by theological considerations specifically deriving from Christian views about Jews and Judaism. During the Middle Ages, Jewish people under Muslim rule generally experienced tolerance and integration, but there were occasional outbreaks of violence like Almohad's persecutions.
Hasidism.
Hasidic Judaism was founded by Yisroel ben Eliezer (1700–1760), also known as the "Ba'al Shem Tov" (or "Besht"). It originated in a time of persecution of the Jewish people, when European Jews had turned inward to Talmud study; many felt that most expressions of Jewish life had become too "academic", and that they no longer had any emphasis on spirituality or joy. His disciples attracted many followers; they themselves established numerous Hasidic sects across Europe. Hasidic Judaism eventually became the way of life for many Jews in Europe. Waves of Jewish immigration in the 1880s carried it to the United States.
The movement itself claims to be nothing new, but a "refreshment" of original Judaism. Or as some have put it: " "they merely re-emphasized that which the generations had lost"". Nevertheless, early on there was a serious schism between Hasidic and non-Hasidic Jews. European Jews who rejected the Hasidic movement were dubbed by the Hasidim as Misnagdim, (lit. "opponents"). Some of the reasons for the rejection of Hasidic Judaism were the overwhelming exuberance of Hasidic worship, its untraditional ascriptions of infallibility and alleged miracle-working to their leaders, and the concern that it might become a messianic sect. Since then differences between the Hasidim and their opponents have slowly diminished and both groups are now considered part of Haredi Judaism.
The Enlightenment and new religious movements.
In the late 18th century CE, Europe was swept by a group of intellectual, social and political movements known as the Enlightenment. The Enlightenment led to reductions in the European laws that prohibited Jews to interact with the wider secular world, thus allowing Jews access to secular education and experience. A parallel Jewish movement, Haskalah or the "Jewish Enlightenment", began, especially in Central Europe and Western Europe, in response to both the Enlightenment and these new freedoms. It placed an emphasis on integration with secular society and a pursuit of non-religious knowledge through reason. With the promise of political emancipation many Jews saw no reason to continue to observe Jewish law and increasing numbers of Jews assimilated into Christian Europe. Modern religious movements of Judaism all formed in reaction to this trend.
In Central Europe, followed by Great Britain and the United States, Reform Judaism and Liberal Judaism developed, relaxing legal obligations (especially those that limited Jewish relations with non-Jews), emulating Protestant decorum in prayer, and emphasizing the ethical values of Judaism's Prophetic tradition. Modern Orthodox Judaism developed in reaction to Reform Judaism, by leaders who argued that Jews could participate in public life as citizens equal to Christians, while maintaining the observance of Jewish law. Meanwhile, in the United States, wealthy Reform Jews helped European scholars, who were Orthodox in practice but critical (and skeptical) in their study of the Bible and Talmud, to establish a seminary to train rabbis for immigrants from Eastern Europe. These left-wing Orthodox rabbis were joined by right-wing Reform rabbis who felt that Jewish law should not be entirely abandoned, to form the Conservative movement. Orthodox Jews who opposed the Haskalah formed Haredi Orthodox Judaism. After massive movements of Jews following The Holocaust and the creation of the state of Israel, these movements have competed for followers from among traditional Jews in or from other countries.
Spectrum of observance.
Countries such as the United States, Israel, Canada, United Kingdom, Argentina and South Africa contain large Jewish populations. Jewish religious practice varies widely through all levels of observance. According to the of the National Jewish Population Survey, in the United States' Jewish community—the world's second largest—4.3 million Jews out of 5.1 million had some sort of connection to the religion. Of that population of connected Jews, 80% participated in some sort of Jewish religious observance, but only 48% belonged to a synagogue, and fewer than 16% attend regularly.
Birth rates for American Jews have dropped from 2.0 to 1.7. (Replacement rate is 2.1.) Intermarriage rates range from 40-50% in the US, and only about a third of children of intermarried couples are raised as Jews. Due to intermarriage and low birth rates, the Jewish population in the US shrank from 5.5 million in 1990 to 5.1 million in 2001. This is indicative of the general population trends among the Jewish community in the Diaspora, but a focus on total population obscures growth trends in some denominations and communities, such as Haredi Judaism. The Baal teshuva movement is a movement of Jews who have "returned" to religion or become more observant.
Judaism and other religions.
Christianity and Judaism.
Christianity was originally a sect of Second Temple Judaism, but the two religions diverged in the first century. The differences between Christianity and Judaism originally centered on whether Jesus was the Jewish Messiah, but eventually became irreconcilable. Major differences between the two faiths include the nature of the Messiah, of atonement and sin, the status of God's commandments to Israel, and perhaps most significantly of the nature of God himself. Due to these differences, Judaism traditionally regards Christianity as Shituf, or worship of the God of Israel which is not monotheistic. Christianity has traditionally regarded Judaism as obsolete with the invention of Christianity and Jews as a people replaced by the Church, though a Christian belief in dual-covenant theology emerged as a phenomenon following Christian reflection on how their theology influenced the Nazi Holocaust.
Until their emancipation in the late 18th and the 19th century, Jews in Christian lands were subject to humiliating legal restrictions and limitations. They included provisions requiring Jews to wear specific and identifying clothing such as the Jewish hat and the yellow badge, restricting Jews to certain cities and towns or in certain parts of towns (ghettos), and forbidding Jews to enter certain trades (for example selling new clothes in medieval Sweden). Disabilities also included special taxes levied on Jews, exclusion from public life, restraints on the performance of religious ceremonies, and linguistic censorship. Some countries went even further and completely expelled Jews, for example England in 1290 (Jews were readmitted in 1655) and Spain in 1492 (readmitted in 1868). The first Jewish settlers in North America arrived in the Dutch colony of New Amsterdam in 1654; they were forbidden to hold public office, open a retail shop, or establish a synagogue. When the colony was seized by the British in 1664 Jewish rights remained unchanged, but by 1671 Asser Levy was the first Jew to serve on a jury in North America.
In 1791, Revolutionary France was the first country to abolish disabilities altogether, followed by Prussia in 1848. Emancipation of the Jews in the United Kingdom was achieved in 1858 after an almost 30-year struggle championed by Isaac Lyon Goldsmid with the ability of Jews to sit in parliament with the passing of the Jews Relief Act 1858. The newly united German Empire in 1871 abolished Jewish disabilities in Germany, which were reinstated in the Nuremberg Laws in 1935.
Jewish life in Christian lands was marked by frequent blood libels, expulsions, forced conversions and massacres. An underlying source of prejudice against Jews in Europe was religious. Christian rhetoric and antipathy towards Jews developed in the early years of Christianity and was reinforced by ever increasing anti-Jewish measures over the ensuing centuries. The action taken by Christians against Jews included acts of violence, and murder culminating in the Holocaust.:21:169 These attitudes were reinforced in Christian preaching, art and popular teaching for two millennia, containing contempt for Jews, as well as statutes which were designed to humiliate and stigmatise Jews.
Islam and Judaism.
Both Judaism and Islamic religion arose from the patriarch Abraham, and are therefore considered Abrahamic religions. In both Jewish and Muslim tradition, the Jewish and Arab peoples are descended from the two sons of Abraham—Isaac and Ishmael, respectively. While both religions are monotheistic and share many commonalities, they differ in that Jews do not consider Jesus or Muhammad to be prophets. The religions' adherents have interacted with each other since the 7th century, when Islam originated and spread in the Arabian peninsula. Indeed, the years 712 to 1066 CE under the Ummayad and the Abbasid rulers have been called the Golden age of Jewish culture in Spain. Non-Muslim monotheists living in these countries, including Jews, were known as dhimmis. Dhimmis were allowed to practice their religion and to administer their internal affairs, but they were subject to certain restrictions that were not imposed on Muslims. For example, they had to pay the jizya, a per capita tax imposed on free adult non-Muslim males, and they were also forbidden to bear arms or testify in court cases involving Muslims. Many of the laws regarding dhimmis were highly symbolic. For example, dhimmis in some countries were required to wear distinctive clothing, a practice not found in either the Qur'an or hadiths but invented in early medieval Baghdad and inconsistently enforced. Jews in Muslim countries were not entirely free from persecution—for example, many were killed, exiled or forcibly converted in the 12th century, in Persia, and by the rulers of the Almohad dynasty in North Africa and Al-Andalus, as well as by the Zaydi imams of Yemen in the 17th century (see: Mawza Exile). At times, Jews were also restricted in their choice of residence—in Morocco, for example, Jews were confined to walled quarters (mellahs) beginning in the 15th century and increasingly since the early 19th century.
In the mid-20th century, Jews were expelled from nearly all of the Arab countries. Most have chosen to live in Israel. Today, antisemitic themes including Holocaust denial have become commonplace in the propaganda of Islamic movements such as Hizbullah and Hamas, in the pronouncements of various agencies of the Islamic Republic of Iran, and even in the newspapers and other publications of Refah Partisi.
Syncretic movements incorporating Judaism.
There are some movements that combine elements of Judaism with those of other religions. The most well-known of these is Messianic Judaism, a religious movement, which arose in the 1960s, that incorporates elements of Judaism with the tenets of Christianity. The movement states that Jesus is the Jewish Messiah, and generally that he is part of the Trinity, and salvation is only achieved through acceptance of Jesus as one's savior. Some members argue that Messianic Judaism is a sect of Judaism. Jewish organizations of every denomination reject this, stating that Messianic Judaism is a Christian sect, as it harbors identical creeds to that of Pauline Christianity.
Other examples of syncretism include Semitic neopaganism, a loosely organized sect which incorporates pagan or Wiccan beliefs with some Jewish religious practices; Jewish Buddhists, another loosely organized group that incorporates elements of Asian spirituality in their faith; and some Renewal Jews who borrow freely and openly from Buddhism, Sufism, Native American religion, and other faiths.
The Kabbalah Centre, which employs teachers from multiple religions, is a New Age movement that claims to popularize the kabbalah, part of the Jewish esoteric tradition.
Bibliography.
Jews in Islamic countries:
External links.
See also Torah database for links to more Judaism e-texts.
Text study projects at . In many instances, the Hebrew versions of these projects are more fully developed than the English.

</doc>
<doc id="15626" url="http://en.wikipedia.org/wiki?curid=15626" title="John Stuart Mill">
John Stuart Mill

John Stuart Mill (20 May 1806 – 8 May 1873) was a British philosopher, political economist and civil servant. He was an influential contributor to social theory, political theory and political economy. He has been called "the most influential English-speaking philosopher of the nineteenth century". Mill's conception of liberty justified the freedom of the individual in opposition to unlimited state control.
Mill expresses his view on freedom by illustrating how an individual's amelioration of personal quality and self-improvement is the sole source of true freedom. Only when an individual is able to attain such a beneficial standard of one's self, whilst in the absence of rendering external onerosity upon others, in their own journey to procure a higher calibre of self-worth, can true freedom prevail. 
Mill's attitude toward freedom and individual accomplishment through self-improvement has inspired many. By establishing an appreciable level of worthiness concerned with one's ability to fulfill personal standards of notability and merit, Mill was able to provide many with a principal example of how they should achieve such particular values.
He was a proponent of utilitarianism, an ethical theory developed by Jeremy Bentham. He worked on the theory of the scientific method. Mill was also a Member of Parliament and an important figure in liberal political philosophy.
Biography.
John Stuart Mill was born on Rodney Street in the Pentonville area of London, the eldest son of the Scottish philosopher, historian and economist James Mill, and Harriet Burrow. John Stuart was educated by his father, with the advice and assistance of Jeremy Bentham and Francis Place. He was given an extremely rigorous upbringing, and was deliberately shielded from association with children his own age other than his siblings. His father, a follower of Bentham and an adherent of associationism, had as his explicit aim to create a genius intellect that would carry on the cause of
utilitarianism and its implementation after he and Bentham had died.
Mill was a notably precocious child. He describes his education in his autobiography. At the age of three he was taught Greek. By the age of eight, he had read "Aesop's Fables", Xenophon's "Anabasis", and the whole of Herodotus, and was acquainted with Lucian, Diogenes Laërtius, Isocrates and six dialogues of Plato. He had also read a great deal of history in English and had been taught arithmetic, physics and astronomy.
At the age of eight, Mill began studying Latin, the works of Euclid, and algebra, and was appointed schoolmaster to the younger children of the family. His main reading was still history, but he went through all the commonly taught Latin and Greek authors and by the age of ten could read Plato and Demosthenes with ease. His father also thought that it was important for Mill to study and compose poetry. One of Mill's earliest poetry compositions was a continuation of the Iliad. In his spare time, he also enjoyed reading about natural sciences and popular novels, such as "Don Quixote" and "Robinson Crusoe".
His father's work, "The History of British India" was published in 1818; immediately thereafter, about the age of twelve, Mill began a thorough study of the scholastic logic, at the same time reading Aristotle's logical treatises in the original language. In the following year he was introduced to political economy and studied Adam Smith and David Ricardo with his father, ultimately completing their classical economic view of factors of production. Mill's "comptes rendus" of his daily economy lessons helped his father in writing "Elements of Political Economy" in 1821, a textbook to promote the ideas of Ricardian economics; however, the book lacked popular support. Ricardo, who was a close friend of his father, used to invite the young Mill to his house for a walk in order to talk about political economy.
At the age of fourteen, Mill stayed a year in France with the family of Sir Samuel Bentham, brother of Jeremy Bentham. The mountain scenery he saw led to a lifelong taste for mountain landscapes. The lively and friendly way of life of the French also left a deep impression on him. In Montpellier, he attended the winter courses on chemistry, zoology, logic of the "Faculté des Sciences", as well as taking a course of the higher mathematics. While coming and going from France, he stayed in Paris for a few days in the house of the renowned economist Jean-Baptiste Say, a friend of Mill's father. There he met many leaders of the Liberal party, as well as other notable Parisians, including Henri Saint-Simon.
This intensive study however had injurious effects on Mill's mental health, and state of mind. At the age of twenty he suffered a nervous breakdown. In chapter V of his "Autobiography", he claims that this was caused by the great physical and mental arduousness of his studies which had suppressed any feelings he might have developed normally in childhood. Nevertheless, this depression eventually began to dissipate, as he began to find solace in the "Mémoires" of Jean-François Marmontel and the poetry of William Wordsworth.
Mill had been engaged in a pen-friendship with Auguste Comte, the founder of positivism and sociology, since Mill first contacted Comte in November 1841. Comte's "sociologie" was more an early philosophy of science than we perhaps know it today, and the "positive" philosophy aided in Mill's broad rejection of Benthamism.
As a nonconformist who refused to subscribe to the Thirty-Nine Articles of the Church of England, Mill was not eligible to study at the University of Oxford or the University of Cambridge. Instead he followed his father to work for the East India Company until 1858, and attended University College, London, to hear the lectures of John Austin, the first Professor of Jurisprudence. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1856.
In 1851, Mill married Harriet Taylor after 21 years of an intimate friendship. Taylor was married when they met, and their relationship was close but generally believed to be chaste during the years before her first husband died. Brilliant in her own right, Taylor was a significant influence on Mill's work and ideas during both friendship and marriage. His relationship with Harriet Taylor reinforced Mill's advocacy of women's rights. He cites her influence in his final revision of "On Liberty", which was published shortly after her death. Taylor died in 1858 after developing severe lung congestion, after only seven years of marriage to Mill.
Between the years 1865 and 1868 Mill served as Lord Rector of the University of St. Andrews. During the same period, 1865–68, he was a Member of Parliament for City and Westminster, sitting for the Liberal Party. During his time as an MP, Mill advocated easing the burdens on Ireland. In 1866, Mill became the first person in the history of Parliament to call for women to be given the right to vote, vigorously defending this position in subsequent debate. Mill became a strong advocate of such social reforms as labour unions and farm cooperatives. In "Considerations on Representative Government", Mill called for various reforms of Parliament and voting, especially proportional representation, the Single Transferable Vote, and the extension of suffrage.
He was godfather to the philosopher Bertrand Russell.
In his views on religion, Mill was an atheist.
Mill died in 1873 of erysipelas in Avignon, France, where he was buried alongside his wife.
Works.
Theory of liberty.
Mill's "On Liberty" addresses the nature and limits of the power that can be legitimately exercised by society over the individual. However Mill is clear that his concern for liberty does not extend to all individuals and all societies. He states that "Despotism is a legitimate mode of government in dealing with barbarians".
Mill states that it is acceptable to harm oneself as long the person doing so is not harming others. He also argues that individuals should be prevented from doing lasting, serious harm to themselves or their property by the harm principle. Because no one exists in isolation, harm done to oneself may also harm others, and destroying property deprives the community as well as oneself. Mill excuses those who are "incapable of self-government" from this principle, such as young children or those living in "backward states of society".
Though this principle seems clear, there are a number of complications. For example, Mill explicitly states that "harms" may include acts of omission as well as acts of commission. Thus, failing to rescue a drowning child counts as a harmful act, as does failing to pay taxes, or failing to appear as a witness in court. All such harmful omissions may be regulated, according to Mill. By contrast, it does not count as harming someone if – without force or fraud – the affected individual consents to assume the risk: thus one may permissibly offer unsafe employment to others, provided there is no deception involved. (Mill does, however, recognise one limit to consent: society should not permit people to sell themselves into slavery). In these and other cases, it is important to bear in mind that the arguments in "On Liberty" are grounded on the principle of Utility, and not on appeals to natural rights.
The question of what counts as a self-regarding action and what actions, whether of omission or commission, constitute harmful actions subject to regulation, continues to exercise interpreters of Mill. It is important to emphasise that Mill did not consider giving offence to constitute "harm"; an action could not be restricted because it violated the conventions or morals of a given society.
"On Liberty" involves an impassioned defence of free speech. Mill argues that free discourse is a necessary condition for intellectual and social progress. We can never be sure, he contends, that a silenced opinion does not contain some element of the truth. He also argues that allowing people to air false opinions is productive for two reasons. First, individuals are more likely to abandon erroneous beliefs if they are engaged in an open exchange of ideas. Second, by forcing other individuals to re-examine and re-affirm their beliefs in the process of debate, these beliefs are kept from declining into mere dogma. It is not enough for Mill that one simply has an unexamined belief that happens to be true; one must understand why the belief in question is the true one. Along those same lines Mill wrote, "unmeasured vituperation, employed on the side of prevailing opinion, really does deter people from expressing contrary opinions, and from listening to those who express them."
Social liberty and tyranny of majority.
Mill believed that "the struggle between Liberty and Authority is the most conspicuous feature in the portions of history." For him, liberty in antiquity was a "contest... between subjects, or some classes of subjects, and the government." Mill defined "social liberty" as protection from "the tyranny of political rulers." He introduced a number of different concepts of the form tyranny can take, referred to as social tyranny, and tyranny of the majority respectively.
Social liberty for Mill meant putting limits on the ruler's power so that he would not be able to use his power on his own wishes and make decisions which could harm society; in other words, people should have the right to have a say in the government's decisions. He said that social liberty was "the nature and limits of the power which can be legitimately exercised by society over the individual". It was attempted in two ways: first, by obtaining recognition of certain immunities, called political liberties or rights; second, by establishment of a system of "constitutional checks".
However, in Mill's view, limiting the power of government was not enough. He stated, "Society can and does execute its own mandates: and if it issues wrong mandates instead of right, or any mandates at all in things with which it ought not to meddle, it practices a social tyranny more formidable than many kinds of political oppression, since, though not usually upheld by such extreme penalties, it leaves fewer means of escape, penetrating much more deeply into the details of life, and enslaving the soul itself."
Liberty.
John Stuart Mill's view on liberty, which was influenced by Joseph Priestley and Josiah Warren, is that the individual ought to be free to do as he wishes unless he harms others. Individuals are rational enough to make decisions about their well being. Government should interfere when it is for the protection of society. Mill explained:
The sole end for which mankind are warranted, individually or collectively, in interfering with the liberty of action of any of their number, is self-protection. That the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others. His own good, either physical or moral, is not sufficient warrant. He cannot rightfully be compelled to do or forbear because it will be better for him to do so, because it will make him happier, because, in the opinion of others, to do so would be wise, or even right...The only part of the conduct of anyone, for which he is amenable to society, is that which concerns others. In the part which merely concerns him, his independence is, of right, absolute. Over himself, over his own body and mind, the individual is sovereign.
Mill added: "Despotism is a legitimate mode of government in dealing with barbarians, provided the end be their improvement, and the means justified by actually effecting that end. Liberty, as a principle, has no application to any state of things anterior to the time when mankind have become capable of being improved by free and equal discussion."
Freedom of speech.
An influential advocate of freedom of speech, Mill objected to censorship. He says:
I choose, by preference the cases which are least favourable to me – In which the argument opposing freedom of opinion, both on truth and that of utility, is considered the strongest. Let the opinions impugned be the belief of God and in a future state, or any of the commonly received doctrines of morality... But I must be permitted to observe that it is not the feeling sure of a doctrine (be it what it may) which I call an assumption of infallibility. It is the undertaking to decide that question "for others", without allowing them to hear what can be said on the contrary side. And I denounce and reprobate this pretension not the less if it is put forth on the side of my most solemn convictions. However, positive anyone's persuasion may be, not only of the faculty but of the pernicious consequences, but (to adopt expressions which I altogether condemn) the immorality and impiety of opinion. – yet if, in pursuance of that private judgement, though backed by the public judgement of his country or contemporaries, he prevents the opinion from being heard in its defence, he assumes infallibility. And so far from the assumption being less objectionable or less dangerous because the opinion is called immoral or impious, this is the case of all others in which it is most fatal.
Mill outlines the benefits of 'searching for and discovering the truth' as a way to further knowledge. He argued that even if an opinion is false, the truth can be better understood by refuting the error. And as most opinions are neither completely true nor completely false, he points out that allowing free expression allows the airing of competing views as a way to preserve partial truth in various opinions. Worried about minority views being suppressed, Mill also argued in support of freedom of speech on political grounds, stating that it is a critical component for a representative government to have in order to empower debate over public policy. Mill also eloquently argued that freedom of expression allows for personal growth and self-realization. He said that freedom of speech was a vital way to develop talents and realise a person's potential and creativity. He repeatedly said that eccentricity was preferable to uniformity and stagnation.
Colonialism.
Mill, an employee for the British East India Company from 1823 to 1858, argued in support of what he called a 'benevolent despotism' with regard to the colonies. Mill argued that "To suppose that the same international customs, and the same rules of international morality, can obtain between one civilized nation and another, and between civilized nations and barbarians, is a grave error...To characterize any conduct whatever towards a barbarous people as a violation of the law of nations, only shows that he who so speaks has never considered the subject."
Slavery.
Domenico Losurdo argues that "In Mill's view, 'any means were licit for those who took on the task of educating 'savage tribes'; 'slavery' was sometimes a mandatory stage for inducing them to work and making them useful to civilization and progress."
In 1850, Mill sent an anonymous letter (which came to be known under the title "The Negro Question"), in rebuttal to Thomas Carlyle's anonymous letter to "Fraser's Magazine for Town and Country" which argued for slavery. Mill supported abolition in the United States.
Women's rights.
Mill's view of history was that right up until his time "the whole of the female" and "the great majority of the male sex" were simply "slaves". He countered arguments to the contrary, arguing that relations between sexes simply amounted to "the legal subordination of one sex to the other — [which] is wrong itself, and now one of the chief hindrances to human improvement; and that it ought to be replaced by a principle of perfect equality." With this, Mill can be considered among the earliest women's rights advocates. His book "The Subjection of Women" (1861, published 1869) is one of the earliest written on this subject by a male author. In "The Subjection of Women" Mill attempts to make a case for perfect equality. He talks about the role of women in marriage and how it needed to be changed. There, Mill comments on three major facets of women's lives that he felt are hindering them: society and gender construction, education, and marriage. He argued that the oppression of women was one of the few remaining relics from ancient times, a set of prejudices that severely impeded the progress of humanity. 
Mill's ideas were opposed by Ernest Belfort Bax in his treatise, "The Legal Subjection of Men".
John Stuart Mill stressed the attention of the subjection of women . From ancient time, to his time, and now to our time this issue has existed in many different societies. Mill argues that the issue persists because of the way society looks at women. Mill argued our society was shaped to subconsciously oppress women from the roots it grew from, marriage. Along with several other roots as well that Mill mentions, such as the predetermined idea of the woman’s role in the family. Mill exploited the differences between male and female privilege and equality. This inequality in marriage is what hurts the image of women in today’s society.
Mill argues that the image of women is at risk because of inequality in marriage and in the household. In particular, he discusses the ways in which the subordination of women negatively affects not only the women, but also the men and children in the family. This subordination stunts the moral and intellectual development of women by restricting their field of activities, pushing them either into self-sacrifice or into selfishness and pettiness. This leads to the gap in male and female development and equality. If women are subconsciously oppressed in any form by society it automatically has a negative effect on a woman’s interests or dreams. 
John Stuart Mill desired equality in the family household. The idea that the female plays the role of the housewife, while the male goes out and works and provides for the family is an age-old family attribute that is embedded in our way of thinking. Mill believes that this ancient social issue is exactly what needs to be addressed and rectified. If the public has their attention brought to this issue we can start to realize how we actually look at the female of the house. Mill argues that because our society has thought this way for so long it is common and therefore remains unchanged. When in reality he is arguing that it is hurting the development of females as well as their position in society. 
Once the role of women is changed, then females can strive without oppression. Changing the social relations between men and women to ones in which they play equal roles will require them to change their self-interests and to broaden their social sympathies. Women must be liberated from the shackles they are trained to voluntarily impose upon themselves. It is in their own interest and in the interests of society. This will better the interest of not only women, but men too. Mill is arguing for equality, not for women to strive over men in the household. This means if it is in a males interest to be a stay at home dad for example, then society shouldn’t depict that being any lesser to any other occupation just because it was a female’s role. This idea itself that it is a female’s role is what must be eliminated. That will truly free women of the metaphoric shackles that bound them to society’s expectations.
Mill believes that society is not going to be able to change its image of the female in the household until the affects are actually illustrated. Because it is common for a woman to be successful, there is no true threat on the surface that people realize. There is no immediate inequality that is present on a daily basis. However Mill argues that it is the underlying subliminal dominance the male owns that is exactly what causes a domino effect. Women’s true natures cannot be verified, however, for they are repressed in some areas and unnaturally stimulated in others, according to Mill. Furthermore, women have seldom been allowed to testify to their own natures; rather, the men who exercise power over them have described them. Because of this women have not been allowed their true potential. Mill’s intentions were not only to change this, but to have people understand that this is what the problem was and it was present. Mill’s theory and ideas were summarized perfectly by the following, “Since women have never been allowed to develop naturally without the repression, stimulation, or guidance of men, a system of subordination founded on women’s “natural” sensitivity and lack of more “masculine” qualities is not inherently more valid than any other system based on theory alone.”
Utilitarianism.
The canonical statement of Mill's utilitarianism can be found in "Utilitarianism." This philosophy has a long tradition, although Mill's account is primarily influenced by Jeremy Bentham and Mill's father James Mill.
Jeremy Bentham's famous formulation of utilitarianism is known as the "greatest-happiness principle". It holds that one must always act so as to produce the greatest aggregate happiness among all sentient beings, within reason. Mill's major contribution to utilitarianism is his argument for the qualitative separation of pleasures. Bentham treats all forms of happiness as equal, whereas Mill argues that intellectual and moral pleasures (higher pleasures) are superior to more physical forms of pleasure (lower pleasures). Mill distinguishes between happiness and contentment, claiming that the former is of higher value than the latter, a belief wittily encapsulated in the statement that "it is better to be a human being dissatisfied than a pig satisfied; better to be Socrates dissatisfied than a fool satisfied. And if the fool, or the pig, are of a different opinion, it is because they only know their own side of the question."
Mill defines the difference between higher and lower forms of happiness with the principle that those who have experienced both tend to prefer one over the other. This is, perhaps, in direct contrast with Bentham's statement that "Quantity of pleasure being equal, push-pin is as good as poetry", that, if a simple child's game like hopscotch causes more pleasure to more people than a night at the opera house, it is more imperative upon a society to devote more resources to propagating hopscotch than running opera houses. Mill's argument is that the "simple pleasures" tend to be preferred by people who have no experience with high art, and are therefore not in a proper position to judge. Mill also argues that people who, for example, are noble or practice philosophy, benefit society more than those who engage in individualist practices for pleasure, which are lower forms of happiness. It is not the agent's own greatest happiness that matters "but the greatest amount of happiness altogether".
Mill supported legislation that would have granted extra voting power to university graduates on the grounds that they were in a better position to judge what would be best for society. (For he believed that education itself, not the intrinsic nature of educated people, qualified them to have more influence in government.)
The qualitative account of happiness that Mill advocates thus sheds light on his account presented in "On Liberty". As Mill suggests in that text, utility is to be conceived in relation to humanity "as a progressive being", which includes the development and exercise of rational capacities as we strive to achieve a "higher mode of existence". The rejection of censorship and paternalism is intended to provide the necessary social conditions for the achievement of knowledge and the greatest ability for the greatest number to develop and exercise their deliberative and rational capacities.
Economic philosophy.
Mill's early economic philosophy was one of free markets. However, he accepted interventions in the economy, such as a tax on alcohol, if there were sufficient utilitarian grounds. He also accepted the principle of legislative intervention for the purpose of animal welfare. Mill originally believed that "equality of taxation" meant "equality of sacrifice" and that progressive taxation penalised those who worked harder and saved more and was therefore "a mild form of robbery".
Given an equal tax rate regardless of income, Mill agreed that inheritance should be taxed. A utilitarian society would agree that everyone should be equal one way or another. Therefore receiving inheritance would put one ahead of society unless taxed on the inheritance.
Those who donate should consider and choose carefully where their money goes—some charities are more deserving than others. Considering public charities boards such as a government will disburse the money equally. However, a private charity board like a church would disburse the monies fairly to those who are in more need than others.
Later he altered his views toward a more socialist bent, adding chapters to his Principles of Political Economy in defence of a socialist outlook, and defending some socialist causes. Within this revised work he also made the radical proposal that the whole wage system be abolished in favour of a co-operative wage system. Nonetheless, some of his views on the idea of flat taxation remained, albeit altered in the third edition of the "Principles of Political Economy" to reflect a concern for differentiating restrictions on "unearned" incomes, which he favoured, and those on "earned" incomes, which he did not favour.
Mill's "Principles", first published in 1848, was one of the most widely read of all books on economics in the period. As Adam Smith's "Wealth of Nations" had during an earlier period, Mill's "Principles" dominated economics teaching. In the case of Oxford University it was the standard text until 1919, when it was replaced by Marshall's "Principles of Economics".
Economic democracy.
Mill promoted economic democracy instead of capitalism, in the manner of substituting capitalist businesses with worker cooperatives. He says:
The form of association, however, which if mankind continue to improve, must be expected in the end to predominate, is not that which can exist between a capitalist as chief, and work-people without a voice in the management, but the association of the labourers themselves on terms of equality, collectively owning the capital with which they carry on their operations, and working under managers elected and removable by themselves.
Political democracy.
Mill's major work on political democracy, "Considerations on Representative Government", defends two fundamental principles, extensive participation by citizens and enlightened competence of rulers. The two values are obviously in tension, and some readers have concluded that he is an elitist democrat, while others count him as an earlier participatory democrat. In one section he appears to defend plural voting, in which more competent citizens are given extra votes (a view he later repudiated). But in chapter 3 he presents what is still one of the most eloquent cases for the value of participation by all citizens. He believed that the incompetence of the masses could eventually be overcome if they were given a chance to take part in politics, especially at the local level.
Mill is one of the few political philosophers ever to serve in government as an elected official. In his three years in Parliament, he was more willing to compromise than the "radical" principles expressed in his writing would lead one to expect.
The environment.
Mill demonstrated an early insight into the value of the natural world – in particular in Book IV, chapter VI of "Principles of Political Economy": "Of the Stationary State" in which Mill recognised wealth beyond the material, and argued that the logical conclusion of unlimited growth was destruction of the environment and a reduced quality of life. He concluded that a stationary state could be preferable to unending economic growth:
I cannot, therefore, regard the stationary states of capital and wealth with the unaffected aversion so generally manifested towards it by political economists of the old school.
If the earth must lose that great portion of its pleasantness which it owes to things that the unlimited increase of wealth and population would extirpate from it, for the mere purpose of enabling it to support a larger, but not a better or a happier population, I sincerely hope, for the sake of posterity, that they will be content to be stationary, long before necessity compel them to it.
Economic development.
Mill regarded economic development as a function of land, labour and capital. While land and labour are the two original factors of production, capital is "a stock, previously accumulated of the products of former labour." Increase in wealth is possible only if land and capital help to increase production faster than the labour force. It is productive labour that is productive of wealth and capital accumulation. "The rate of capital accumulation is the function of the proportion of the labour force employed ' productively. Profits earned by employing unproductive labours are merely transfers of income; unproductive labour does not generate wealth or income" . It is productive labourers who do productive consumption. Productive consumption is that "which maintains and increase the productive capacity of the community." It implies that productive consumption is an input necessary to maintain productive labourers.
Control of population growth.
Mill supported the Malthusian theory of population. By population he meant the number of the working class only. He was therefore concerned about the growth in number of labourers who worked for hire. He believed that population control was essential for improving the condition of the working class so that they might enjoy the fruits of the technological progress and capital accumulation. Mill advocated birth control. In 1823 Mill and a friend were arrested while distributing pamphlets on birth control by Francis Place to women in working class areas.
Wage fund.
According to Mill, supply is very elastic in response to wages. Wages generally exceed the minimum subsistence level, and are paid out of capital. Hence, wages are limited by existing capital for paying wages. Thus, wage per worker can be derived by dividing the total circulating capital by the size of the working population. Wages can increase by an increase in the capital used in paying wages, or by decrease in the number of workers. If wages rise, supply of labour will rise. Competition among workers not only brings down wages, but also keeps some workers out of employment. This is based on Mill's notion that "demand for commodities is not demand for labourers". It means that income invested as advances of wages to labour creates employment, and not income spent on consumer goods. An increase in consumption causes a decline in investment. So increased investment leads to increases in the wage fund and to economic progress.
Rate of capital accumulation.
According to Mill, the rate of capital accumulation depends on: (1) "the amount of fund from which saving can be made" or "the size of the net produce of the industry", and (2) the "disposition to save". Capital is the result of savings, and the savings come from the "abstinence from present consumption for the sake of future goods". Although capital is the result of saving, it is nevertheless consumed. This means saving is spending. Since saving depends on the net produce of the industry, it grows with profits and rent which go into making the net produce. On the other hand, the disposition to save depends on (1) the rate of profit and (2) the desire to save, or what Mill called "effective desire of accumulation". However, profit also depends on the cost of labour, and the rate of profit is the ratio of profits to wages. When profits rise or wages fall, the rate of profits increases, which in turn increases the rate of capital accumulation. Similarly, it is the desire to save which tends to increase the rate of capital accumulation.
Rate of profit.
According to Mill, the ultimate tendency in an economy is for the rate of profit to decline due to diminishing returns in agriculture and increase in population at a Malthusian rate.
References.
</dl>
External links.
Mill's works.
</dl>
Secondary works.
</dl>
Further information.
</dl>

</doc>
<doc id="15627" url="http://en.wikipedia.org/wiki?curid=15627" title="Junk science">
Junk science

In the United States, junk science is any scientific data, research, or analysis considered to be spurious or fraudulent. The concept is often invoked in political and legal contexts where facts and scientific results have a great amount of weight in making a determination. It usually conveys a pejorative connotation that the research has been untowardly driven by political, ideological, financial, or otherwise unscientific motives.
The concept was first invoked in relation to expert testimony in civil litigation. More recently, invoking the concept has been a tactic to criticize research on the harmful environmental or public health effects of corporate activities, and occasionally in response to such criticism. In these contexts, junk science is counterposed to the "sound science" or "solid science" that favors one's own point of view. This dichotomy has been particularly promoted by Steven Milloy and the Advancement of Sound Science Center, and is somewhat different from pseudoscience and fringe science.
History.
The phrase "junk science" appears to have been in use prior to 1985. A 1985 United States Department of Justice report by the Tort Policy Working Group noted:
"The use of such invalid scientific evidence (commonly referred to as 'junk science') has resulted in findings of causation which simply cannot be justified or understood from the standpoint of the current state of credible scientific or medical knowledge."
In 1989, the climate scientist Jerry Mahlman (Director of the Geophysical Fluid Dynamics Laboratory) characterized the theory that global warming was due to solar variation (presented in "Scientific Perspectives on the Greenhouse Problem" by Frederick Seitz et al.) as "noisy junk science."
Peter W. Huber popularized the term with respect to litigation in his 1991 book "Galileo's Revenge: Junk Science in the Courtroom." The book has been cited in over 100 legal textbooks and references; as a consequence, some sources cite Huber as the first to coin the term. By 1997, the term had entered the legal lexicon as seen in an opinion by Supreme Court of the United States Justice John Paul Stevens: 
"An example of 'junk science' that should be excluded under the Daubert standard as too unreliable would be the testimony of a phrenologist who would purport to prove a defendant's future dangerousness based on the contours of the defendant's skull." Lower courts have subsequently set guidelines for identifying junk science, such as the 2005 opinion of United States Court of Appeals for the Seventh Circuit Judge Easterbrook:
"Positive reports about magnetic water treatment are not replicable; this plus the lack of a physical explanation for any effects are hallmarks of junk science."
As the subtitle of Huber's book, "Junk Science in the Courtroom", suggests, his emphasis was on the use or misuse of expert testimony in civil litigation. One prominent example cited in the book was litigation over casual contact in the spread of AIDS. A California school district sought to prevent a young boy with AIDS, Ryan Thomas, from attending kindergarten. The school district produced an expert witness, Dr. Steven Armentrout, who testified that a possibility existed that AIDS could be transmitted to schoolmates through yet undiscovered "vectors." However, five experts testified on behalf of Thomas that AIDS is not transmitted through casual contact, and the court affirmed the "solid science" (as Mr. Huber called it) and rejected Dr. Armentrout's argument.
In 1999, Paul Ehrlich and others advocated public policies to improve the dissemination of valid environmental scientific knowledge and discourage junk science: 
"The Intergovernmental Panel on Climate Change reports offer an antidote to junk science by articulating the current consensus on the prospects for climate change, by outlining the extent of the uncertainties, and by describing the potential benefits and costs of policies to address climate change."
In a 2003 study about changes in environmental activism in the Crown of the Continent (Flathead) Ecosystem, Pedynowski noted that junk science can undermine the credibility of science over a much broader scale because misrepresentation by special interests casts doubt on more defensible claims and undermines the credibility of all research.
In his 2006 book "Junk Science", Dan Agin emphasized two main causes of junk science: fraud, and ignorance. In the first case, Agin discussed falsified results in the development of organic transistors: 
"As far as understanding junk science is concerned, the important aspect is that both Bell Laboratories and the international physics community were fooled until someone noticed that noise records published by Jan Hendrik Schön in several papers were identical—which means physically impossible."
In the second case, he cites an example that demonstrates ignorance of statistical principles in the lay press: 
"Since no such proof is possible [that genetically modified food is harmless], the article in The New York Times was what is called a "bad rap" against the U.S. Department of Agriculture—a bad rap based on a junk-science belief that it's possible to prove a null hypothesis."
Agin asks the reader to step back from the rhetoric, as "how things are labeled does not make a science junk science." In its place, he offers that junk science is ultimately motivated by the desire to hide undesirable truths from the public.
Use as corporate PR.
John Stauber and Sheldon Rampton of "PR Watch" say the concept of junk science has come to be invoked in attempts to dismiss scientific findings that stand in the way of short-term corporate profits. In their book "Trust Us, We're Experts" (2001), they write that industries have launched multi-million-dollar campaigns to position certain theories as junk science in the popular mind, often failing to employ the scientific method themselves. For example, the tobacco industry has described research demonstrating the harmful effects of smoking and second-hand smoke as junk science, through the vehicle of various astroturf groups.
Theories more favorable to corporate activities are portrayed in words as "sound science." Past examples where "sound science" was used include the research into the toxicity of Alar, which was heavily criticized by antiregulatory advocates, and Herbert Needleman's research into low dose lead poisoning. Needleman was accused of fraud and personally attacked.
Fox News commentator Steven Milloy often invokes the concept of junk science to attack the results of credible scientific research on topics like global warming, ozone depletion, and passive smoking. The credibility of Milloy's website junkscience.com was questioned by Paul D. Thacker, a writer for "The New Republic", in the wake of evidence that Milloy had received funding from Philip Morris, RJR Tobacco, and Exxon Mobil. Thacker also noted that Milloy was receiving almost $100,000 a year in consulting fees from Philip Morris while he criticized the evidence regarding the hazards of second-hand smoke as junk science. Following the publication of this article, the Cato Institute, which had hosted the junkscience.com site, ceased its association with the site and removed Milloy from its list of adjunct scholars.
Tobacco industry documents reveal that Philip Morris executives conceived of the "Whitecoat Project" in the 1980s as a response to emerging scientific data on the harmfulness of second-hand smoke. The goal of the Whitecoat Project, as conceived by Philip Morris and other tobacco companies, was to use ostensibly independent "scientific consultants" to spread doubt in the public mind about scientific data through invoking concepts like junk science. According to epidemiologist David Michaels, Assistant Secretary of Energy for Environment, Safety, and Health in the Clinton Administration, the tobacco industry invented the "sound science" movement in the 1980s as part of their campaign against the regulation of second-hand smoke.
David Michaels has argued that, since the U.S. Supreme Court ruling in "Daubert v. Merrell Dow Pharmaceuticals, Inc.", lay judges have become "gatekeepers" of scientific testimony and, as a result, respected scientists have sometimes been unable to provide testimony so that corporate defendants are "increasingly emboldened" to accuse adversaries of practicing junk science.
Use by scientists.
In 1995, the Union of Concerned Scientists launched the Sound Science Initiative, a national network of scientists committed to debunking junk science through media outreach, lobbying, and developing joint strategies to participate in town meetings or public hearings. The American Association for the Advancement of Science also recognized the need for increased understanding between scientists and lawmakers in its newsletter on Science and Technology in Congress, "Although most individuals would agree that sound science is preferable to junk science, fewer recognize what makes a scientific study 'good' or 'bad'." The American Dietetic Association, criticizing marketing claims made for food products, has created a list of "Ten Red Flags of Junk Science."
Individual scientists have also invoked the concept.

</doc>
<doc id="15628" url="http://en.wikipedia.org/wiki?curid=15628" title="Java (disambiguation)">
Java (disambiguation)

Java is the world's most populous island, located in Indonesia.
Java may also refer to:

</doc>
<doc id="15630" url="http://en.wikipedia.org/wiki?curid=15630" title="James Cook">
James Cook

Captain James Cook, FRS, RN (7 November 1728 – 14 February 1779) was a British explorer, navigator, cartographer, and captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first recorded European contact with the eastern coastline of Australia and the Hawaiian Islands, and the first recorded circumnavigation of New Zealand.
Cook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War, and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This notice came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1766 as commander of for the first of three Pacific voyages.
In three voyages Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery he surveyed and named features, and recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage and an ability to lead men in adverse conditions.
Cook was killed in Hawaii in a fight with Hawaiians during his third exploratory voyage in the Pacific in 1779. He left a legacy of scientific and geographical knowledge which was to influence his successors well into the 20th century and numerous memorials worldwide have been dedicated to him.
Early life and family.
James Cook was born on 27 October 1728 in the village of Marton in Yorkshire and baptised on 3 November in the local church of St. Cuthbert, where his name can be seen in the church register. He was the second of eight children of James Cook, a Scottish farm labourer from Ednam near Kelso, and his locally born wife, Grace Pace, from Thornaby-on-Tees. In 1736, his family moved to Airey Holme farm at Great Ayton, where his father's employer, Thomas Skottowe, paid for him to attend the local school. In 1741, after five years schooling, he began work for his father, who had by now been promoted to farm manager. For leisure, he would climb a nearby hill, Roseberry Topping, enjoying the opportunity for solitude. Cooks' Cottage, his parents' last home, which he is likely to have visited, is now in Melbourne, having been moved from England and reassembled, brick by brick, in 1934.
In 1745, when he was 16, Cook moved 20 mi to the fishing village of Staithes, to be apprenticed as a shop boy to grocer and haberdasher William Sanderson. Historians have speculated that this is where Cook first felt the lure of the sea while gazing out of the shop window.
After 18 months, not proving suitable for shop work, Cook travelled to the nearby port town of Whitby to be introduced to friends of Sanderson's, John and Henry Walker. The Walkers were prominent local ship-owners and Quakers, and were in the coal trade. Their house is now the Captain Cook Memorial Museum. Cook was taken on as a merchant navy apprentice in their small fleet of vessels, plying coal along the English coast. His first assignment was aboard the collier "Freelove", and he spent several years on this and various other coasters, sailing between the Tyne and London. As part of his apprenticeship, Cook applied himself to the study of algebra, geometry, trigonometry, navigation and astronomy—all skills he would need one day to command his own ship.
His three-year apprenticeship completed, Cook began working on trading ships in the Baltic Sea. After passing his examinations in 1752, he soon progressed through the merchant navy ranks, starting with his promotion in that year to mate aboard the collier brig "Friendship". In 1755, within a month of being offered command of this vessel, he volunteered for service in the Royal Navy, when Britain was re-arming for what was to become the Seven Years' War. Despite the need to start back at the bottom of the naval hierarchy, Cook realised his career would advance more quickly in military service and entered the Navy at Wapping on 17 June 1755.
Cook married Elizabeth Batts (1742–1835), the daughter of Samuel Batts, keeper of the Bell Inn, Wapping and one of his mentors, on 21 December 1762 at St. Margaret's Church in Barking, Essex. The couple had six children: James (1763–94), Nathaniel (1764–80, lost aboard HMS "Thunderer" which foundered with all hands in a hurricane in the West Indies), Elizabeth (1767–71), Joseph (1768–68), George (1772–72) and Hugh (1776–93), the last of whom died of scarlet fever while a student at Christ's College, Cambridge. When not at sea, Cook lived in the East End of London. He attended St Paul's Church, Shadwell, where his son James was baptised. Cook has no known direct descendants—all his recorded children either pre-deceased him or died without issue.
Start of Royal Navy career.
Cook's first posting was with HMS "Eagle", serving as able seaman and master's mate under Captain Joseph Hamar for his first year aboard, and Captain Hugh Palliser thereafter. In October and November 1755 he took part in "Eagle"'s capture of one French warship and the sinking of another, following which he was promoted to boatswain in addition to his other duties. His first temporary command was in March 1756 when he was briefly master of the "Cruizer", a small cutter attached to the "Eagle" while on patrol.
In June 1757 Cook passed his master's examinations at Trinity House, Deptford, which qualified him to navigate and handle a ship of the King's fleet. He then joined the frigate HMS "Solebay" as master under Captain Robert Craig.
Conquest of Canada (1758–63).
During the Seven Years' War, Cook served in North America as master of "Pembroke" (1757). In 1758 he took part in the major amphibious assault that captured the Fortress of Louisbourg from the French, after which he participated in the siege of Quebec City and then the Battle of the Plains of Abraham in 1759. He showed a talent for surveying and cartography, and was responsible for mapping much of the entrance to the Saint Lawrence River during the siege, thus allowing General Wolfe to make his famous stealth attack on the Plains of Abraham.
Cook's surveying ability was put to good use mapping the jagged coast of Newfoundland in the 1760s, aboard HMS "Grenville". He surveyed the north-west stretch in 1763 and 1764, the south coast between the Burin Peninsula and Cape Ray in 1765 and 1766, and the west coast in 1767. At this time Cook employed local pilots to point out the "rocks and hidden dangers" along the south and west coasts. During the 1765 season, four pilots were engaged at a daily pay of 4 shillings each: John Beck for the coast west of "Great St. Lawrence", Morgan Snook for Fortune Bay, John Dawson for Connaigre and Hermitage Bay, and John Peck for the "Bay of Despair."
His five seasons in Newfoundland produced the first large-scale and accurate maps of the island's coasts and were the first scientific, large scale, hydrographic surveys to use precise triangulation to establish land outlines. They also gave Cook his mastery of practical surveying, achieved under often adverse conditions, and brought him to the attention of the Admiralty and Royal Society at a crucial moment both in his career and in the direction of British overseas discovery. Cook's map would be used into the 20th century—copies of it being referenced by those sailing Newfoundland's waters for 200 years.
Following on from his exertions in Newfoundland, it was at this time that Cook wrote that he intended to go not only "farther than any man has been before me, but as far as I think it is possible for a man to go."
Voyages of exploration.
First voyage (1768–71).
In 1766 the Royal Society engaged Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. Cook, at the age of 39, was promoted to lieutenant and named as commander of the expedition. The Royal Society also agreed that Cook would receive a one hundred guinea gratuity in addition to his Naval pay.
The expedition sailed from England on 26 August 1768, rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. However, the result of the observations was not as conclusive or accurate as had been hoped. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of "Terra Australis".
Cook then sailed to New Zealand and mapped the complete coastline, making only some minor errors. He then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline.
On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: "…and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not." On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. Cook originally christened the area as "Stingray Bay", but he later crossed it out and named it "Botany Bay" after the unique specimens retrieved by the botanists Joseph Banks and Daniel Solander. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.
After his departure from Botany Bay he continued northwards. On 11 June a mishap occurred when the HMS "Endeavour" ran aground on a shoal of the Great Barrier Reef, and then "nursed into a river mouth on 18 June 1770". The ship was badly damaged and his voyage was delayed almost seven weeks while repairs were carried out on the beach (near the docks of modern Cooktown, Queensland, at the mouth of the Endeavour River). The voyage then continued, sailing through Torres Strait and on 22 August Cook landed on Possession Island, where he claimed the entire coastline that he had just explored as British territory. He returned to England via Batavia (modern Jakarta, Indonesia where many in his crew succumbed to malaria), the Cape of Good Hope, and arriving on the island of Saint Helena on 12 July 1771.
Interlude.
Cook's journals were published upon his return, and he became something of a hero among the scientific community. Among the general public, however, the aristocratic botanist Joseph Banks was a greater hero. Banks even attempted to take command of Cook's second voyage, but removed himself from the voyage before it began, and Johann Reinhold Forster and his son Georg Forster were taken on as scientists for the voyage. Cook's son George was born five days before he left for his second voyage.
Second voyage (1772–75).
Shortly after his return from the first voyage, Cook was promoted in August 1771, to the rank of commander. In 1772 the Royal Society commissioned him to search for the hypothetical Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed to lie further south. Despite this evidence to the contrary, Alexander Dalrymple and others of the Royal Society still believed that a massive southern continent should exist.
Cook commanded HMS "Resolution" on this voyage, while Tobias Furneaux commanded its companion ship, HMS "Adventure". Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, "Resolution" and "Adventure" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.
Cook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.
Before returning to England, Cook made a final sweep across the South Atlantic from Cape Horn and surveyed, mapped and took possession for Britain of South Georgia, which had been explored by Anthony de la Roché in 1675. Cook also discovered and named Clerke Rocks and the South Sandwich Islands ("Sandwich Land"). He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.
Cook's second voyage marked a successful employment of Larcum Kendall's K1 copy of John Harrison's H4 marine chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for this time-piece which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.
Upon his return, Cook was promoted to the rank of post-captain and given an honorary retirement from the Royal Navy, with a posting as an officer of the Greenwich Hospital. He reluctantly accepted, insisting that he be allowed to quit the post if an opportunity for active duty should arise. His fame now extended beyond the Admiralty; he was made a Fellow of the Royal Society, and awarded the Copley Gold Medal for completing his second voyage without losing a man to scurvy. Nathaniel Dance-Holland painted his portrait; he dined with James Boswell; he was described in the House of Lords as "the first navigator in Europe". But he could not be kept away from the sea. A third voyage was planned and Cook volunteered to find the Northwest Passage. He travelled to the Pacific and hoped to travel east to the Atlantic, while a simultaneous voyage travelled the opposite route.
Third voyage (1776–79).
On his last voyage, Cook again commanded HMS "Resolution", while Captain Charles Clerke commanded HMS "Discovery". The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a Northwest Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to begin formal contact with the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.
From the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather. Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward. He unknowingly sailed past the Strait of Juan de Fuca, and soon after entered Nootka Sound on Vancouver Island. He anchored near the First Nations village of Yuquot. Cook's two ships remained in Nootka Sound from 29 March to 26 April 1778, in what Cook called Ship Cove, now Resolution Cove, at the south end of Bligh Island, about 5 mi east across Nootka Sound from Yuquot, lay a Nuu-chah-nulth village (whose chief Cook did not identify but may have been Maquinna). Relations between Cook's crew and the people of Yuquot were cordial if sometimes strained. In trading, the people of Yuquot demanded much more valuable items than the usual trinkets that had worked in Hawaii. Metal objects were much desired, but the lead, pewter, and tin traded at first soon fell into disrepute. The most valuable items which the British received in trade were sea otter pelts. During the stay, the Yuquot "hosts" essentially controlled the trade with the British vessels; the natives usually visited the British vessels at Resolution Cove instead of the British visiting the village of Yuquot at Friendly Cove.
After leaving Nootka Sound, Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.
The Bering Strait proved to be impassable, although he made several attempts to sail through it. He became increasingly frustrated on this voyage, and perhaps began to suffer from a stomach ailment; it has been speculated that this led to irrational behaviour towards his crew, such as forcing them to eat walrus meat, which they had pronounced inedible.
Return to Hawaii.
Cook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the "Makahiki", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS "Resolution", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.
Death.
After a month's stay, Cook attempted to resume his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the "Resolution"'s foremast broke, so the ships returned to Kealakekua Bay for repairs.
Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians at Kealakekua Bay. An unknown group of Hawaiians took one of Cook's small boats. The evening when the cutter was taken, the people had become "insolent" even with threats to fire upon them. Cook was forced into a wild goose chase that ended with his return to the ship frustrated. He attempted to kidnap and ransom the King of Hawaiʻi, Kalaniʻōpuʻu.
That following day, 14 February 1779, Cook marched through the village to retrieve the King. Cook took the aliʻi nui by his own hand and led him willingly away. One of Kalaniʻōpuʻu's favorite wives, Kanekapolei and two chiefs approached the group as they were heading to boats. They pleaded with the king not to go until he stopped and sat where he stood. An old Kahuna (priest), chanting rapidly while holding out a coconut, attempted to distract Cook and his men as a large crowd began to form at the shore. The king began to understand that Cook was his enemy. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. He was first struck on the head with a club by a chief named Kalaimanokahoʻowaha or Kanaʻina (namesake of Charles Kana'ina) and then stabbed by one of the king's attendants, Nuaa. The Hawaiians carried his body away towards the back of the town, still visible to the ship through their spyglass. Four marines, Corporal James Thomas, Private Theophilus Hinks, Private Thomas Fatchett and Private John Allen, were also killed and two others were wounded in the confrontation.
Aftermath.
The esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.
Clerke assumed leadership of the expedition, and made a final attempt to pass through the Bering Strait. Following the death of Clerke, "Resolution" and "Discovery" returned home in October 1780 commanded by John Gore, a veteran of Cook's first voyage, and Captain James King. After their arrival in England, King completed Cook's account of the voyage.
David Samwell, who sailed with Cook on the "Resolution", wrote of him: "He was a modest man, and rather bashful; of an agreeable lively conversation, sensible and intelligent. In temper he was somewhat hasty, but of a disposition the most friendly, benevolent and humane. His person was above six feet high: and, though a good looking man, he was plain both in dress and appearance. His face was full of expression: his nose extremely well shaped: his eyes which were small and of a brown cast, were quick and piercing; his eyebrows prominent, which gave his countenance altogether an air of austerity."
Legacy.
The standard author abbreviation Cook is used to indicate this individual as the author when citing a botanical name.
Ethnographic collections.
The Australian Museum acquired its Cook Collection in 1894 from the Government of New South Wales. At that time the collection consisted of 115 artefacts collected on Cook's three voyages throughout the Pacific Ocean, during the period 1768–1780, along with documents and memorabilia related to these voyages. Many of the ethnographic artifacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Cook's widow Elizabeth Cook, and her descendants, until 1886. In this year John Mackrell, the great-nephew of Isaac Smith, Elizabeth Cook's cousin, organised the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H. M. C. Alexander, and William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.
Navigation and science.
Cook's 12 years sailing around the Pacific Ocean contributed much to European knowledge of the area. Several islands such as Sandwich Islands (Hawaii) were encountered for the first time by Europeans, and his more accurate navigational charting of large areas of the Pacific was a major achievement.
To create accurate maps, latitude and longitude must be accurately determined. Navigators had been able to work out latitude accurately for centuries by measuring the angle of the sun or a star above the horizon with an instrument such as a backstaff or quadrant. Longitude was more difficult to measure accurately because it requires precise knowledge of the time difference between points on the surface of the earth. The Earth turns a full 360 degrees relative to the sun each day. Thus longitude corresponds to time: 15 degrees every hour, or 1 degree every 4 minutes.
Cook gathered accurate longitude measurements during his first voyage due to his navigational skills, the help of astronomer Charles Green and by using the newly published Nautical Almanac tables, via the lunar distance method—measuring the angular distance from the moon to either the sun during daytime or one of eight bright stars during night-time to determine the time at the Royal Observatory, Greenwich, and comparing that to his local time determined via the altitude of the sun, moon, or stars. On his second voyage Cook used the K1 chronometer made by Larcum Kendall, which was the shape of a large pocket watch, 5 in in diameter. It was a copy of the H4 clock made by John Harrison, which proved to be the first to keep accurate time at sea when used on the ship "Deptford's" journey to Jamaica, 1761–62.
Cook succeeded in circumnavigating the world on his first voyage without losing a single man to scurvy, an unusual accomplishment at the time. He tested several preventive measures but the most important was frequent replenishment of fresh food. It was for presenting a paper on this aspect of the voyage to the Royal Society that he was presented with the Copley Medal in 1776. Ever the observer, Cook was the first European to have extensive contact with various people of the Pacific. He correctly postulated a link among all the Pacific peoples, despite their being separated by great ocean stretches (see Malayo-Polynesian languages). Cook theorised that Polynesians originated from Asia, which scientist Bryan Sykes later verified. In New Zealand the coming of Cook is often used to signify the onset of colonisation.
Cook carried several scientists on his voyages; they made several significant observations and discoveries. Two botanists, Joseph Banks, and Swede Daniel Solander, were on the first Cook voyage. The two collected over 3,000 plant species. Banks subsequently strongly promoted British settlement of Australia.
Several artists also sailed on Cook's first voyage. Sydney Parkinson was heavily involved in documenting the botanists' findings, completing 264 drawings before his death near the end of the voyage. They were of immense scientific value to British botanists. Cook's second expedition included William Hodges, who produced notable landscape paintings of Tahiti, Easter Island, and other locations.
Several officers who served under Cook went on to distinctive accomplishments. William Bligh, Cook's sailing master, was given command of HMS "Bounty" in 1787 to sail to Tahiti and return with breadfruit. Bligh is most known for the mutiny of his crew which resulted in his being set adrift in 1789. He later became governor of New South Wales, where he was subject of another mutiny—the only successful armed takeover of an Australian government. George Vancouver, one of Cook's midshipmen, later led a voyage of exploration to the Pacific Coast of North America from 1791 to 1794. In honour of his former commander, Vancouver's new ship was also christened . George Dixon sailed under Cook on his third expedition, and later commanded his own expedition. A lieutenant under Cook, Henry Roberts, spent many years after that voyage preparing the detailed charts that went into Cook's posthumous Atlas, published around 1784.
Cook's contributions to knowledge were internationally recognised during his lifetime. In 1779, while the American colonies were fighting Britain for their independence, Benjamin Franklin wrote to captains of colonial warships at sea, recommending that if they came into contact with Cook's vessel, they were to "not consider her an enemy, nor suffer any plunder to be made of the effects contained in her, nor obstruct her immediate return to England by detaining her or sending her into any other part of Europe or to America; but that you treat the said Captain Cook and his people with all civility and kindness, ... as common friends to mankind." Unknown to Franklin, Cook had met his death a month before this "passport" was written.
Cook's voyages were involved in another unusual first: The first female to circumnavigate the globe was a goat ("The Goat"), who made that memorable journey twice; the first time on HMS Dolphin, under Samuel Wallis. She was then pressed into service as the personal milk provider for Cook, aboard HMS Endeavor. When they returned to England, Cook presented her with a silver collar engraved with lines from Samuel Johnson: "Perpetui, ambita bis terra, praemia lactis Haec habet altrici Capra secunda Jovis.". She was put to pasture on Cook's farm outside London, and also was reportedly admitted to the privileges of the Royal Naval hospital at Greenwich. Cook's journal recorded the date of The Goat's death: 28 March 1772.
Memorials.
A US coin, the 1928 Hawaiian Sesquicentennial half dollar carries Cook's image. Minted for the 150th anniversary of his discovery of the islands, its low mintage (10,008) has made this example of Early United States commemorative coins both scarce and expensive. The site where he was killed in Hawaii was marked in 1874 by a white obelisk set on 25 sqft of chained-off beach. This land, although in Hawaii, was deeded to the United Kingdom. A nearby town is named Captain Cook, Hawaii; several Hawaiian businesses also carry his name. The Apollo 15 Command/Service Module "Endeavour" was named after Cook's ship, HMS "Endeavour", as was the space shuttle Space Shuttle "Endeavour". Another shuttle, "Discovery", was named after Cook's HMS "Discovery".
The first institution of higher education in North Queensland, Australia was named after him, with James Cook University opening in Townsville in 1970. In Australian rhyming slang the expression ""Captain Cook" means "look"". Numerous institutions, landmarks and place names reflect the importance of Cook's contributions, including the Cook Islands, the Cook Strait, Cook Inlet, and the Cook crater on the Moon. Aoraki/Mount Cook, the highest summit in New Zealand, is named for him. Another Mount Cook is on the border between the US state of Alaska and the Canadian Yukon Territory, and is designated Boundary Peak 182 as one of the official Boundary Peaks of the Hay–Herbert Treaty.
One of the earliest monuments to Cook in the United Kingdom is located at The Vache, erected in 1780 by Admiral Hugh Palliser, a contemporary of Cook and one-time owner of the estate. A huge obelisk was built in 1827 as a monument to Cook on Easby Moor overlooking his boyhood village of Great Ayton, along with a smaller monument at the former location of Cook's cottage. There is also a monument to Cook in the church of St Andrew the Great, St Andrew's Street, Cambridge, where his son Hugh, a student at Christ's College, was buried. Cook's widow Elizabeth was also buried in the church and in her will left money for the memorial's upkeep. The 250th anniversary of Cook's birth was marked at the site of his birthplace in Marton, by the opening of the Captain Cook Birthplace Museum, located within Stewart Park (1978). A granite vase just to the south of the museum marks the approximate spot where he was born. Tributes also abound in post-industrial Middlesbrough, including a primary school, shopping square and the "Bottle 'O Notes", a public artwork by Claes Oldenburg, that was erected in the town's Central Gardens in 1993. Also named after Cook is the James Cook University Hospital, a major teaching hospital which opened in 2003.
The Royal Research Ship RRS "James Cook" was built in 2006 to replace the RRS "Charles Darwin" in the UK's Royal Research Fleet, and Stepney Historical Trust placed a plaque on Free Trade Wharf in the Highway, Shadwell to commemorate his life in the East End of London. In 2002 Cook was placed at number 12 in the BBC's poll of the 100 Greatest Britons.
Popular culture.
The death of Cook was interesting to artists and writers of the time because its brutality was a contrast to the idea that primitive humans were naturally good-natured – the "noble savage". There were no eye-witness accounts of the death and so the incident was open to artistic interpretation and myth-making.

</doc>
<doc id="15632" url="http://en.wikipedia.org/wiki?curid=15632" title="John Baskerville">
John Baskerville

John Baskerville (28 January 1706 – 8 January 1775) was an English businessman, in areas including japanning and papier-mâché, but he is best remembered as a printer and type designer.
Life.
Baskerville was born in the village of Wolverley, near Kidderminster in Worcestershire and was a printer in Birmingham, England. He was a member of the Royal Society of Arts, and an associate of some of the members of the Lunar Society. He directed his punchcutter, John Handy, in the design of many typefaces of broadly similar appearance. In 1757, Baskerville published a remarkable quarto edition of Virgil on wove paper, using his own type. It took three years to complete, but it made such an impact that he was appointed printer to the University of Cambridge the following year.
John Baskerville printed works for the University of Cambridge in 1758 and, although an atheist, printed a splendid folio Bible in 1763. His typefaces were greatly admired by Benjamin Franklin, a printer and fellow member of the Royal Society of Arts, who took the designs back to the newly created United States, where they were adopted for most federal government publishing. Baskerville's work was criticised by jealous competitors and soon fell out of favour, but since the 1920s many new fonts have been released by Linotype, Monotype, and other type foundries – revivals of his work and mostly called 'Baskerville'. Emigre released a popular revival of this typeface in 1996 called Mrs Eaves, named for Baskerville's wife, Sarah Eaves. Baskerville's most notable typeface Baskerville represents the peak of transitional type face and bridges the gap between Old Style and Modern type design.
Baskerville also was responsible for significant innovations in printing, paper and ink production. He developed a technique which produced a smoother whiter paper which showcased his strong black type. Baskerville also pioneered a completely new style of typography adding wide margins and leading between each line.
Death and interments.
Baskerville died in January 1775 at his home, "Easy Hill". He requested that his body be placed
in a Conical Building in my own premises Hearetofore used as a mill which I have lately Raised Higher and painted and in a vault which I have prepared for It. This Doubtless to many may appear a Whim perhaps It is so—But it is a whim for many years Resolve'd upon, as I have a Hearty Contempt for all Superstition the Farce of a Consecrated Ground the Irish Barbarism of Sure and Certain Hopes &c I also consider Revelation as it is call'd Exclusive of the Scraps of Morality casually Intermixt with It to be the most Impudent Abuse of Common Sense which Ever was Invented to Befool Mankind.
However in 1821 a canal was built through the land and his body was placed on show by the landowner until Baskerville's family and friends arranged to have it moved to the crypt of Christ Church, Birmingham. Christ Church was demolished in 1897 so his remains were then moved, with other bodies from the crypt, to consecrated catacombs at Warstone Lane Cemetery. In 1963 a petition was presented to Bimingham City Council requesting that he be reburied in unconsecrated ground according to his wishes.
Baskerville House was built on the grounds of "Easy Hill".
Commemoration.
A Portland stone sculpture of the Baskerville typeface, "Industry and Genius", in his honour stands in front of Baskerville House in Centenary Square, Birmingham. It was created by local artist David Patten.
Gallery.
Some examples of volumes published by Baskerville.
External links.
Death:

</doc>
<doc id="15640" url="http://en.wikipedia.org/wiki?curid=15640" title="John Young">
John Young

John Young may refer to:

</doc>
<doc id="15641" url="http://en.wikipedia.org/wiki?curid=15641" title="Joseph Stalin">
Joseph Stalin

Joseph Stalin or Iosif Vissarionovich Stalin (Russian: Ио́сиф Виссарио́нович Ста́лин, ]; born Ioseb Besarionis Dze Jugashvili, Georgian: იოსებ ბესარიონის ძე ჯუღაშვილი, ]; 18 December 1878 – 5 March 1953) was the leader of the Soviet Union from the mid-1920s until his death in 1953.
He was one of the seven members of the first Politburo, founded in 1917 in order to manage the Bolshevik Revolution: Lenin, Zinoviev, Kamenev, Trotsky, Stalin, Sokolnikov and Bubnov.Among the Bolshevik revolutionaries who took part in the Russian Revolution of 1917, Stalin was appointed general secretary of the party's Central Committee in 1922. He subsequently managed to consolidate power following the 1924 death of Vladimir Lenin through suppressing Lenin's criticisms (in the postscript of his testament) and expanding the functions of his role, all the while eliminating any opposition. He remained general secretary until the post was abolished in 1952, concurrently serving as the Premier of the Soviet Union from 1941 onward.
Under Stalin's rule, the concept of "Socialism in One Country" became a central tenet of Soviet society, contrary to Leon Trotsky's view that socialism must be spread through continuous international revolutions. He replaced the New Economic Policy introduced by Lenin in the early 1920s with a highly centralised command economy, launching a period of industrialization and collectivization that resulted in the rapid transformation of the USSR from an agrarian society into an industrial power. However, the economic changes coincided with the imprisonment of millions of people in Gulag labour camps. The initial upheaval in agriculture disrupted food production and contributed to the catastrophic Soviet famine of 1932–33, known as the Holodomor in Ukraine. Between 1934 and 1939 he organized and led a massive purge (known as "Great Purge") of the party, government, armed forces and intelligentsia, in which millions of so-called "enemies of the Soviet people" were imprisoned, exiled or executed. In a period that lasted from 1936 to 1939, Stalin instituted a campaign against enemies within his regime. Major figures in the Communist Party, such as the old Bolsheviks, Leon Trotsky, and most of the Red Army generals, were killed after being convicted of plotting to overthrow the government and Stalin.
In August 1939, after failed attempts to conclude anti-Hitler pacts with other major European powers, Stalin entered into a non-aggression pact with Nazi Germany that divided their influence and territory within Eastern Europe, resulting in their invasion of Poland in September of that year, but Germany later violated the agreement and launched a massive invasion of the Soviet Union in June 1941. Despite heavy human and territorial losses, Soviet forces managed to halt the Nazi incursion after the decisive Battles of Moscow and Stalingrad. After defeating the Axis powers on the Eastern Front, the Red Army captured Berlin in May 1945, effectively ending the war in Europe for the Allies. The Soviet Union subsequently emerged as one of two recognized world superpowers, the other being the United States. The Yalta and Potsdam conferences established communist governments loyal to the Soviet Union in the Eastern Bloc countries as buffer states. He also fostered close relations with Mao Zedong in China and Kim Il-sung in North Korea.
Stalin led the Soviet Union through its post-war reconstruction phase, which saw a significant rise in tension with the Western world that would later be known as the Cold War. During this period, the USSR became the second country in the world to successfully develop a nuclear weapon, as well as launching the Great Plan for the Transformation of Nature in response to another widespread famine and the Great Construction Projects of Communism.
In the years following his death, Stalin and his regime have been condemned on numerous occasions, most notably in 1956 when his successor Nikita Khrushchev denounced his legacy and initiated a process of de-Stalinization. He remains a controversial figure today, with many regarding him as a tyrant. However, popular opinion within the Russian Federation is mixed. The exact number of deaths caused by Stalin's regime is a subject of debate, but it is widely agreed upon that it is on the order of millions.
Early life.
Stalin was born Ioseb Besarionis dze Jughashvili (Georgian: იოსებ ბესარიონის ძე ჯუღაშვილი) on 18 December 1878 in the town of Gori, Tiflis Governorate, Russian Empire (present-day Georgia). His father was Besarion Jughashvili (1850–1909), a cobbler, while his mother was Ketevan Geladze (1858–1937), a housemaid. Although an ethnic Georgian, he later disliked Georgians and denied his origin, and identified as a Russian.
As a child, Ioseb was plagued with numerous health issues. He was born with two adjoined toes on his left foot. His face was permanently scarred by smallpox at the age of 7. At age 12, he injured his left arm in an accident involving a horse-drawn carriage, rendering it shorter and stiffer than its counterpart.
Ioseb's father slid into alcoholism, which made him abusive to his family and caused his business to fail. When Ioseb's mother enrolled him into a Greek Orthodox priesthood school against her husband's wishes, his enraged father went on a drunken rampage. He was banished from Gori for assaulting its police chief. He subsequently moved to Tiflis (Tbilisi), leaving his family behind.
When Ioseb was sixteen, he received a scholarship to attend the Tiflis Spiritual Seminary, the leading Russian Orthodox seminary in Tbilisi; the language of instruction was Russian. He was a voracious reader and became a Georgian cultural nationalist. He anonymously published poetry in Georgian in the local press and engaged in student politics. Although his performance had been good, he was expelled in 1899 after missing his final exams. The seminary's records also suggest that he was unable to pay his tuition fees. Around this time, Ioseb discovered the writings of Vladimir Lenin and joined the Russian Social-Democratic Labour Party, a Marxist group.
Out of school, Jughashvili briefly worked as a part-time clerk in a meteorological office, but after a state crackdown on revolutionaries, he went underground and became a full-time revolutionary, living off donations.
When Lenin formed the Bolsheviks, Jughashvili eagerly joined up with him. Jughashvili proved to be a very effective organizer of men as well as a capable intellectual. Among other activities, he distributed propaganda, provoked strikes, staged bank robberies, and ordered assassinations. In 1907 Jughashvili made use of his reputation as a poet to stage the 1907 Tiflis bank robbery. He used a former school friend who was also a fan of his poetry as the inside man for a bank robbery that left 40 dead and stole millions of roubles for Lenin. This demonstrated to Lenin his need for Jughashvili. Jughashvili was arrested and exiled to Siberia numerous times, but often escaped. His skill and charm won him the respect of Lenin, and he rose rapidly through the ranks of the Bolsheviks.
Jughashvili married his first wife, Ekaterina Svanidze, in 1906, who bore him a son. She died the following year of typhus. In 1911, he met his future second wife, Nadezhda Alliluyeva, during one of his many exiles in Siberia.
It is believed that sometime between 1910 and 1912, he began using the alias "Stalin" in his writings.
Revolution, Civil War, and Polish-Soviet War.
Role during the Russian Revolution of 1917.
After returning to Petrograd from his final exile, Stalin ousted Vyacheslav Molotov and Alexander Shlyapnikov as editors of "Pravda". He then took a position in favor of supporting Alexander Kerensky's provisional government. However, after Lenin prevailed at the April 1917 Communist Party conference, Stalin and "Pravda" shifted to opposing the provisional government. At this conference, Stalin was elected to the Bolshevik Central Committee. In October 1917, the Bolshevik Central Committee voted in favor of an insurrection. On 7 November, from the Smolny Institute, Trotsky, Lenin and the rest of the Central Committee coordinated the insurrection against Kerensky in the 1917 October Revolution. By 8 November, the Bolsheviks had stormed the Winter Palace and Kerensky's Cabinet had been arrested.
Role in the Russian Civil War, 1917–1919.
Upon the October Revolution, Stalin was appointed People's Commissar for Nationalities' Affairs. Thereafter, civil war broke out in Russia, pitting Lenin's Red Army against the White Army, a loose alliance of anti-Bolshevik forces. Lenin formed a five-member Politburo, which included Stalin and Trotsky. In May 1918, Lenin dispatched Stalin to the city of Tsaritsyn. Through his new allies, Kliment Voroshilov and Semyon Budyonny, Stalin imposed his influence on the military.
Stalin challenged many of the decisions of Trotsky, ordered the killings of many counter-revolutionaries and former Tsarist officers in the Red Army and burned villages in order to intimidate the peasantry into submission and discourage bandit raids on food shipments. In May 1919, in order to stem mass desertions on the Western front, Stalin had deserters and renegades publicly executed as traitors.
Role in the Polish-Soviet War, 1919–1921.
As Bolshevik victories in the Russian Civil War of 1917–1922 established the Bolshevik position more securely, Soviet Russia started a push towards world revolution, which formed part of the communist ideology to transform the whole world into socialist states. (Tukhachevsky: "There can be no doubt that if we had been victorious on the Vistula (i.e. in Poland), the revolutionary fires would have reached the entire continent."). Looking toward Western Europe, the Bolsheviks encountered the newly reborn independent — and expansionist-minded — state of Poland. Conflicts began in what became known as the Polish–Soviet War of 1919–1921. After the Polish Army achieved initial successes, the Bolsheviks pushed the Polish forces back into central Poland in the northern-hemisphere summer of 1920. As the people's commissar to the high command of the southern front, Stalin was determined to take the then Polish city of Lwów (now Lviv in Ukraine). This conflicted with the general strategy set by Lenin and Trotsky, which focused on the capture of Warsaw further north.
Tukhachevsky's forces engaged those of Polish commanders Józef Piłsudski and Władysław Sikorski at the pivotal Battle of Warsaw (12–25 August 1920), but Stalin refused to redirect his troops from Lwów to help Tukhachevsky. Consequently, the Poles totally routed the four invading armies of Soviet Russia fighting for the Polish capital. The Bolsheviks lost the battles for both Lwów and Warsaw, and Stalin was blamed. In August 1920 Stalin returned to Moscow, where he defended himself and resigned his military command. At the Ninth Party Conference of March–April 1920, on 22 September 1920, Trotsky openly criticized Stalin's behavior.
Rise to power.
Stalin played a decisive role in the 1921 Red Army invasion of Georgia, after which he adopted particularly hardline, centralist policies towards Soviet Georgia. This led to the Georgian Affair of 1922 and other repressions. Stalin's actions in Georgia created a rift with Lenin, who believed that all the Soviet states should stand equal.
Lenin nonetheless considered Stalin a loyal ally, and when he got mired in squabbles with Trotsky and other politicians, he decided to support Stalin. With the help of Lev Kamenev, Lenin appointed Stalin General Secretary in 1922. This post enabled Stalin to appoint many of his allies to government positions.
Lenin suffered a stroke in 1922, forcing him into semi-retirement in Gorki. Stalin visited him often, acting as his intermediary with the outside world, but the pair quarreled and their relationship deteriorated. Lenin dictated increasingly disparaging notes on Stalin in what would become his testament. He criticized Stalin's political views, rude manners, and excessive power and ambition, and suggested that Stalin should be removed from the position of general secretary. During Lenin's semi-retirement, Stalin forged an alliance with Kamenev and Grigory Zinoviev against Trotsky. These allies prevented "Lenin's Testament" from being revealed to the Twelfth Party Congress in April 1923 (after Lenin's death the testament was read to selected groups of deputies to the Thirteenth Party Congress in May 1924 but it was forbidden to be mentioned at the plenary assemblies or any documents of the Congress).
Lenin died of a stroke on 21 January 1924. Following Lenin's death, a power struggle began, which involved the following seven Politburo members: Nikolai Bukharin, Lev Kamenev, Alexei Rykov, Joseph Stalin, Mikhail Tomsky, Leon Trotsky, Grigory Zinoviev.
Again, Kamenev and Zinoviev helped to keep Lenin's Testament from going public. Thereafter, Stalin's disputes with Kamenev and Zinoviev intensified. Trotsky, Kamenev and Zinoviev grew increasingly isolated, and were eventually ejected from the Central Committee and then from the Party itself. Kamenev and Zinoviev were later readmitted, but Trotsky was exiled from the Soviet Union.
The Northern Expedition in China became a point of contention over foreign policy by Stalin and Trotsky. Stalin wanted the Communist Party of China to ally itself with the Nationalist Kuomintang, rather than attempt to implement a communist revolution. Trotsky urged the party to oppose the Kuomintang and launch a full-scale revolution. Stalin funded the KMT during the expedition. Stalin countered Trotsky's criticisms by making a secret speech in which he said that the Kuomintang were the only ones capable of defeating the imperialists, that Chiang Kai-shek had funding from the rich merchants, and that his forces were to be utilized until squeezed for all usefulness like a lemon before being discarded. However, Chiang quickly reversed the tables in the Shanghai massacre of 1927 by massacring the membership of the Communist party in Shanghai midway through the Northern Expedition.
Stalin pushed for more rapid industrialization and central control of the economy, contravening Lenin's New Economic Policy (NEP). At the end of 1927, a critical shortfall in grain supplies prompted Stalin to push for the collectivisation of agriculture and order the seizure of grain hoards from kulak farmers. Nikolai Bukharin and Premier Alexey Rykov opposed these policies and advocated a return to the NEP, but the rest of the Politburo sided with Stalin and removed Bukharin from the Politburo in November 1929. Rykov was fired the following year and was replaced by Vyacheslav Molotov on Stalin's recommendation.
In December 1934, the popular Communist Party boss in Leningrad, Sergei Kirov, was murdered. Stalin blamed Kirov's murder on a vast conspiracy of saboteurs and Trotskyites. He launched a massive purge against these internal enemies, putting them on rigged show trials and then having them executed or imprisoned in Siberian Gulags. Among these victims were old enemies, including Bukharin, Rykov, Kamenev and Zinoviev. Stalin made the loyal Nikolai Yezhov head of the secret police, the NKVD, and had him purge the NKVD of veteran Bolsheviks. With no serious opponents left in power, Stalin ended the purges in 1938. Yezhov was held to blame for the excesses of the Great Terror. He was dismissed from office and later executed.
Changes to Soviet society, 1927–1939.
Bolstering Soviet secret service and intelligence.
Stalin vastly increased the scope and power of the state's secret police and intelligence agencies. Under his guiding hand, Soviet intelligence forces began to set up intelligence networks in most of the major nations of the world, including Germany (the famous "Rote Kappelle" spy ring), Great Britain, France, Japan, and the United States. Stalin made considerable use of the Communist International movement in order to and to ensure that foreign Communist parties remained pro-Soviet and pro-Stalin.
One of the best examples of Stalin's ability to integrate secret police and foreign espionage came in 1940, when he gave approval to the secret police to have Leon Trotsky assassinated in Mexico.
Cult of personality.
A cult of personality developed in the Soviet Union around both Stalin and Lenin. Many personality cults in history have been frequently measured and compared to his. Numerous towns, villages and cities were renamed after the Soviet leader (see List of places named after Stalin) and the Stalin Prize and Stalin Peace Prize were named in his honor. He accepted grandiloquent titles (e.g., "Coryphaeus of Science," "Father of Nations," "Brilliant Genius of Humanity," "Great Architect of Communism," "Gardener of Human Happiness," and others), and helped rewrite Soviet history to provide himself a more significant role in the revolution of 1917. At the same time, according to Nikita Khrushchev, he insisted that he be remembered for "the extraordinary modesty characteristic of truly great people." Statues of Stalin depict him at a height and build approximating the very tall Tsar Alexander III, sources suggest he was approximately 5 ft 4 in (163 cm).
Trotsky criticized the cult of personality built around Stalin. It reached new levels during World War II, with Stalin's name included in the new Soviet national anthem. Stalin became the focus of literature, poetry, music, paintings and film that exhibited fawning devotion. He was sometimes credited with almost god-like qualities, including the suggestion that he single-handedly won the Second World War. The degree to which Stalin himself relished the cult surrounding him is debatable. The Finnish communist Arvo Tuominen records a sarcastic toast proposed by Stalin at a New Year Party in 1935 in which he said "Comrades! I want to propose a toast to our Patriarch, life and sun, liberator of nations, architect of socialism [he rattled off all the appellations applied to him in those days] – Josef Vissarionovich Stalin, and I hope this is the first and last speech made to that genius this evening."
In a 1956 speech, Nikita Khrushchev denounced Stalin's cult of personality with these words: "It is impermissible and foreign to the spirit of Marxism-Leninism to elevate one person, to transform him into a superman possessing supernatural characteristics akin to those of a god."
Purges and deportations.
Purges and executions.
Stalin, as head of the Politburo of the Central Committee of the Communist Party of the Soviet Union, consolidated near-absolute power in the 1930s with a Great Purge of the party that was justified as an attempt to expel "opportunists" and "counter-revolutionary infiltrators". Those targeted by the purge were often expelled from the party, however more severe measures ranged from banishment to the Gulag labor camps to execution after trials held by NKVD troikas.
In the 1930s, Stalin apparently became increasingly worried about the growing popularity of the Leningrad party boss Sergey Kirov. At the 1934 Party Congress where the vote for the new Central Committee was held, Kirov received only three negative votes, the fewest of any candidate, while Stalin received at least over a hundred negative votes. After the assassination of Kirov, which may have been orchestrated by Stalin, Stalin invented a detailed scheme to implicate opposition leaders in the murder, including Trotsky, Kamenev and Zinoviev. The investigations and trials expanded. Stalin passed a new law on "terrorist organizations and terrorist acts" that were to be investigated for no more than ten days, with no prosecution, defense attorneys or appeals, followed by a sentence to be executed "quickly."
Thereafter, several trials known as the Moscow Trials were held, but the procedures were replicated throughout the country. Article 58 of the legal code, which listed prohibited anti-Soviet activities as counterrevolutionary crime, was applied in the broadest manner. The flimsiest pretexts were often enough to brand someone an "enemy of the people", starting the cycle of public persecution and abuse, often proceeding to interrogation, torture and deportation, if not death. The Russian word troika gained a new meaning: a quick, simplified trial by a committee of three subordinated to NKVD -NKVD troika- with sentencing carried out within 24 hours. Stalin's hand-picked executioner, Vasili Blokhin, was entrusted with carrying out some of the high profile executions in this period.
Many military leaders were convicted of treason and a large-scale purge of Red Army officers followed. The repression of so many formerly high-ranking revolutionaries and party members led Leon Trotsky to claim that a "river of blood" separated Stalin's regime from that of Lenin. In August 1940, Trotsky was assassinated in Mexico, where he had lived in exile since January 1937; this eliminated the last of Stalin's opponents among the former Party leadership.
With the exception of Vladimir Milyutin (who died in prison in 1937) and Joseph Stalin himself, all of the members of Lenin's original cabinet who had not succumbed to death from natural causes before the purge were executed.
Mass operations of the NKVD also targeted "national contingents" (foreign ethnicities) such as Poles, ethnic Germans, Koreans, etc. A total of 350,000 (144,000 of them Poles) were arrested and 247,157 (110,000 Poles) were executed. Many Americans who had emigrated to the Soviet Union during the worst of the Great Depression were executed; others were sent to prison camps or gulags. Concurrent with the purges, efforts were made to rewrite the history in Soviet textbooks and other propaganda materials. Notable people executed by NKVD were removed from the texts and photographs as though they never existed. Gradually, the history of revolution was transformed to a story about just two key characters: Lenin and Stalin.
In light of revelations from Soviet archives, historians now estimate that nearly 700,000 people (353,074 in 1937 and 328,612 in 1938) were executed in the course of the terror, with the great mass of victims merely "ordinary" Soviet citizens: workers, peasants, homemakers, teachers, priests, musicians, soldiers, pensioners, ballerinas, beggars. Many of the executed were interred in mass graves, with some of the major killing and burial sites being Bykivnia, Kurapaty and Butovo.
Some Western experts believe the evidence released from the Soviet archives is understated, incomplete or unreliable.
Stalin personally signed 357 proscription lists in 1937 and 1938 that condemned to execution some 40,000 people, and about 90% of these are confirmed to have been shot. At the time, while reviewing one such list, Stalin reportedly muttered to no one in particular: "Who's going to remember all this riff-raff in ten or twenty years time? No one. Who remembers the names now of the boyars Ivan the Terrible got rid of? No one." In addition, Stalin dispatched a contingent of NKVD operatives to Mongolia, established a Mongolian version of the NKVD troika, and unleashed a bloody purge in which tens of thousands were executed as "Japanese Spies." Mongolian ruler Khorloogiin Choibalsan closely followed Stalin's lead.
During the 1930s and 1940s, the Soviet leadership sent NKVD squads into other countries to murder defectors and other opponents of the Soviet regime. Victims of such plots included Yevhen Konovalets, Ignace Poretsky, Rudolf Klement, Alexander Kutepov, Evgeny Miller, Leon Trotsky and the Workers' Party of Marxist Unification (POUM) leadership in Catalonia (e.g., Andreu Nin).
Deportations.
Shortly before, during and immediately after World War II, Stalin conducted a series of deportations on a huge scale that profoundly affected the ethnic map of the Soviet Union. It is estimated that between 1941 and 1949 nearly 3.3 million were deported to Siberia and the Central Asian republics. By some estimates up to 43% of the resettled population died of diseases and malnutrition.
Separatism, resistance to Soviet rule and collaboration with the invading Germans were cited as the official reasons for the deportations, rightly or wrongly. Individual circumstances of those spending time in German-occupied territories were not examined. After the brief Nazi occupation of the Caucasus, the entire population of five of the small highland peoples and the Crimean Tatars – more than a million people in total – were deported without notice or any opportunity to take their possessions.
As a result of Stalin's lack of trust in the loyalty of particular ethnicities, ethnic groups such as the Soviet Koreans, the Volga Germans, the Crimean Tatars, the Chechens, and many Poles were forcibly moved out of strategic areas and relocated to places in the central Soviet Union, especially Kazakhstan in Soviet Central Asia. By some estimates, hundreds of thousands of deportees may have died en route.
According to official Soviet estimates, more than 14 million people passed through the "Gulag" from 1929 to 1953, with a further 7 to 8 million being deported and exiled to remote areas of the Soviet Union (including the entire nationalities in several cases).
In February 1956, Nikita Khrushchev condemned the deportations as a violation of Leninism, and reversed most of them, although it was not until 1991 that the Tatars, Meskhetians and Volga Germans were allowed to return "en masse" to their homelands. The deportations had a profound effect on the peoples of the Soviet Union. The memory of the deportations has played a major part in the separatist movements in the Baltic States, Tatarstan and Chechnya, even today.
Forced labor.
Under the reign of Joseph Stalin, forced labour in the Soviet Union was used in order to achieve the economic goals of the Five-Year Plan. Forced labour was a vital part of the rapid industrialization and economic growth of the Soviet Union. Between 1932-1946 the Soviet secret police detained approximately 18,207,150 prisoners. The Gulag prison system had put into practice the use of forced labour by imprisoning not only dangerous criminals but also people convicted of political crimes against the communistic government.
Labourers had to work in freezing climates, unhygienic conditions, dangerous circumstances and worked for extensive time periods without rest. Many prisoners were able to perform the forced labour necessary but a large number of prisoners were too hungry, sick, or injured from the intense working conditions to complete the labour
Collectivization.
Stalin's regime moved to force collectivization of agriculture. This was intended to increase agricultural output from large-scale mechanized farms, to bring the peasantry under more direct political control, and to make tax collection more efficient. Collectivization brought social change on a scale not seen since the abolition of serfdom in 1861 and alienation from control of the land and its produce. Collectivization also meant a drastic drop in living standards for many peasants, and it faced violent reaction among the peasantry.
In the first years of collectivization it was estimated that industrial production would rise by 200% and agricultural production by 50%, but these expectations were not realized. Stalin blamed this unanticipated failure on kulaks (rich peasants), who resisted collectivization. However, kulaks proper made up only 4% of the peasant population; the "kulaks" that Stalin targeted included the slightly better-off peasants who took the brunt of violence from the OGPU and the Komsomol. These peasants were about 60% of the population. Those officially defined as "kulaks", "kulak helpers", and, later, "ex-kulaks" were to be shot, placed into Gulag labor camps, or deported to remote areas of the country, depending on the charge. Archival data indicates that 20,201 people were executed during 1930, the year of Dekulakization.
The two-stage progress of collectivization—interrupted for a year by Stalin's famous editorials, "Dizzy with Success" and "Reply to Collective Farm Comrades"—is a prime example of his capacity for tactical political withdrawal followed by intensification of initial strategies.
Famines.
Famine affected Ukraine, southern Russia and other parts of the USSR. The death toll from famine in the Soviet Union at this time is estimated at between 5 and 10 million people. The worst crop failure of late tsarist Russia, in 1892, had caused 375,000 to 400,000 deaths. Most modern scholars agree that the famine was caused by the policies of the government of the Soviet Union under Stalin, rather than by natural reasons. According to Alan Bullock, "the total Soviet grain crop was no worse than that of 1931 ... it was not a crop failure but the excessive demands of the state, ruthlessly enforced, that cost the lives of as many as five million Ukrainian peasants." Stalin refused to release large grain reserves that could have alleviated the famine, while continuing to export grain; he was convinced that the Ukrainian peasants had hidden grain away and strictly enforced draconian new collective-farm theft laws in response. Other historians hold it was largely the insufficient harvests of 1931 and 1932 caused by a variety of natural disasters that resulted in famine, with the successful harvest of 1933 ending the famine. Soviet and other historians have argued that the rapid collectivization of agriculture was necessary in order to achieve an equally rapid industrialization of the Soviet Union and ultimately win World War II. Alec Nove claims that the Soviet Union industrialized in spite of, rather than because of, its collectivized agriculture.
The USSR also experienced a major famine in 1947 as a result of war damage and severe droughts, but economist Michael Ellman argues that it could have been prevented if the government had not mismanaged its grain reserves. The famine cost an estimated 1 to 1.5 million lives as well as secondary population losses due to reduced fertility.
Ukrainian famine.
The Holodomor famine is sometimes referred to as the Ukrainian Genocide, implying it was engineered by the Soviet government, specifically targeting the Ukrainian people to destroy the Ukrainian nation as a political factor and social entity. While historians continue to disagree whether the policies that led to Holodomor fall under the legal definition of genocide, twenty-six countries have officially recognized the Holodomor as such. On 28 November 2006, the Ukrainian Parliament approved a bill declaring the Soviet-era forced famine an act of genocide against the Ukrainian people. Professor Michael Ellman concludes that Ukrainians were victims of genocide in 1932–33 according to a more relaxed definition that is favored by some specialists in the field of genocide studies. He asserts that Soviet policies greatly exacerbated the famine's death toll. Although 1.8 million tonnes of grain were exported during the height of the starvation — enough to feed 5 million people for one year — the use of torture and execution to extract grain under the Law of Spikelets, the use of force to prevent starving peasants from fleeing the worst-affected areas, and the refusal to import grain or secure international humanitarian aid to alleviate conditions led to incalculable human suffering in the Ukraine. It would appear that Stalin intended to use the starvation as a cheap and efficient means (as opposed to deportations and shootings) to kill off those deemed to be "counterrevolutionaries," "idlers," and "thieves," but not to annihilate the Ukrainian peasantry as a whole. Ellman also claims that, while this was not the only Soviet genocide (e.g., the Polish operation of the NKVD), it was the worst in terms of mass casualties.
Current estimates on the total number of casualties within Soviet Ukraine range mostly from 2.2 million
to 4 to 5 million.
A Ukrainian court found Josef Stalin, Lazar Kaganovich, Stanislav Kosior and other leaders of the former Soviet Union guilty of genocide by "organizing mass famine in Ukraine in 1932–1933" in January 2010. However, the court "dropped criminal proceedings over the suspects' deaths".
Industrialization.
The Russian Civil War and wartime communism had a devastating effect on the country's economy. Industrial output in 1922 was 13% of that in 1914. A recovery followed under the New Economic Policy, which allowed a degree of market flexibility within the context of socialism. Under Stalin's direction, this was replaced by a system of centrally ordained "Five-Year Plans" in the late 1920s. These called for a highly ambitious program of state-guided crash industrialization and the collectivization of agriculture.
With seed capital unavailable because of international reaction to Communist policies, little international trade, and virtually no modern infrastructure, Stalin's government financed industrialization both by restraining consumption on the part of ordinary Soviet citizens to ensure that capital went for re-investment into industry and by ruthless extraction of wealth from the kulaks.
In 1933 workers' real earnings sank to about one-tenth of the 1926 level. Common and political prisoners in labor camps were forced to perform unpaid labor, and communists and Komsomol members were frequently "mobilized" for various construction projects. The Soviet Union used numerous foreign experts to design new factories, supervise construction, instruct workers, and improve manufacturing processes. The most notable foreign contractor was Albert Kahn's firm that designed and built 521 factories between 1930 and 1932. As a rule, factories were supplied with imported equipment.
In spite of early breakdowns and failures, the first two Five-Year Plans achieved rapid industrialization from a very low economic base. While it is generally agreed that the Soviet Union achieved significant levels of economic growth under Stalin, the precise rate of growth is disputed. It is not disputed, however, that these gains were accomplished at the cost of millions of lives. Official Soviet estimates stated the annual rate of growth at 13.9%; Russian and Western estimates gave lower figures of 5.8% and even 2.9%. Indeed, one estimate is that Soviet growth became temporarily much higher after Stalin's death.
According to Robert Lewis, the Five-Year Plan substantially helped to modernize the previously backward Soviet economy. New products were developed, and the scale and efficiency of existing production greatly increased. Some innovations were based on indigenous technical developments, others on imported foreign technology. Despite its costs, the industrialization effort allowed the Soviet Union to fight, and ultimately win, World War II.
Science.
Science in the Soviet Union was under strict ideological control by Stalin and his government, along with art and literature. There was significant progress in "ideologically safe" domains, owing to the free Soviet education system and state-financed research. However, the most notable legacy during Stalin's time was his public endorsement of the agronomist Trofim Lysenko, who rejected Mendelian genetics as "bourgeois pseudoscience" and instead advocated Lamarckian inheritance and hybridization theories (which had been discredited by most Western countries by the 1920s in favor of Darwinian Evolution), that caused widespread agricultural destruction and major setbacks in Soviet knowledge in biology. Many scientists came out publicly against his views, but the majority of them, including Nikolai Vavilov (who was later hailed as a pioneer in modern Genetics), were imprisoned or executed. Some areas of physics were criticized.
Social services.
Under the Soviet government people benefited from some social liberalization. Girls were given an adequate, equal education and women had equal rights in employment, improving lives for women and families. Stalinist development also contributed to advances in health care, which significantly increased the lifespan and quality of life of the typical Soviet citizen. Stalin's policies granted the Soviet people universal access to healthcare and education, effectively creating the first generation free from the fear of typhus, cholera, and malaria. The occurrences of these diseases dropped to record low numbers, increasing life spans by decades.
Soviet women under Stalin were the first generation of women able to give birth in the safety of a hospital with access to prenatal care. Education was also an example of an increase in the standard of living after economic development. The generation born during Stalin's rule was the first near-universally literate generation. Millions benefited from mass literacy campaigns in the 1930s, and from workers training schemes. Engineers were sent abroad to learn industrial technology, and hundreds of foreign engineers were brought to Russia on contract. Transport links were improved and many new railways built. Workers who exceeded their quotas, "Stakhanovites", received many incentives for their work; they could afford to buy the goods that were mass-produced by the rapidly expanding Soviet economy.
The increase in demand due to industrialization and the decrease in the workforce due to World War II and repressions generated a major expansion in job opportunities for the survivors, especially for women.
Culture.
Although he was Georgian by birth, some western historians claim that Stalin became a Russian nationalist and significantly promoted Russian history, language, and Russian national heroes, particularly during the 1930s and 1940s. There are also claims that he held the Russian people up as the elder brothers of the non-Russian minorities.
During Stalin's reign, the official and long-lived style of Socialist Realism was established for painting, sculpture, music, drama and literature. Previously fashionable "revolutionary" expressionism, abstract art, and avant-garde experimentation were discouraged or denounced as "formalism".
The degree of Stalin's personal involvement in general, and in specific instances, has been the subject of discussion. Stalin's favorite novel "Pharaoh", shared similarities with Sergei Eisenstein's film, "Ivan the Terrible", produced under Stalin's tutelage.
In architecture, a Stalinist Empire Style (basically, updated neoclassicism on a very large scale, exemplified by the Seven Sisters of Moscow) replaced the constructivism of the 1920s. Stalin's rule had a largely disruptive effect on indigenous cultures within the Soviet Union, though the politics of Korenizatsiya and forced development were possibly beneficial to the integration of later generations of indigenous cultures.
Religion.
Raised in the Georgian Orthodox faith, Stalin became an atheist. His government promoted atheism through special atheistic education in schools, anti-religious propaganda, the anti-religious work of public institutions (Society of the Godless), discriminatory laws, and a terror campaign against religious believers. By the late 1930s, it had become dangerous to be publicly associated with religion.
Stalin's role in the fortunes of the Russian Orthodox Church is complex. Continuous persecution in the 1930s resulted in its near-extinction as a public institution: by 1939, active parishes numbered in the low hundreds (down from 54,000 in 1917), many churches had been leveled, and tens of thousands of priests, monks and nuns were persecuted and killed. Over 100,000 were shot during the purges of 1937–1938. During World War II, the Church was allowed a revival as a patriotic organization, and thousands of parishes were reactivated until a further round of suppression during Khrushchev's rule. The Russian Orthodox Church Synod's recognition of the Soviet government and of Stalin personally led to a schism with the Russian Orthodox Church Outside Russia.
Just days before Stalin's death, certain religious sects were outlawed and persecuted. Many religions popular in ethnic regions of the Soviet Union, including the Roman Catholic Church, Eastern Catholic Churches, Baptists, Islam, Buddhism, and Judaism underwent ordeals similar to that which the Orthodox churches in other parts of the country suffered: thousands of monks were persecuted, and hundreds of churches, synagogues, mosques, temples, sacred monuments, monasteries and other religious buildings were razed. Stalin had a different policy outside the Soviet Union; he supported the Communist Uyghur Muslim separatists under Ehmetjan Qasim in the Ili Rebellion against the anti-Communist Republic of China regime. In addition to this, he supplied weapons to the Uyghur Ili army and Red Army support against Chinese forces, and helped them establish the Second East Turkestan Republic of which Islam was the official state religion.
Theorist.
Stalin and his supporters have highlighted the notion that socialism can be built and consolidated by a country ("Socialism in One Country") as underdeveloped as Russia during the 1920s. Indeed this might be the only means in which it could be built in a hostile environment. In 1933, Stalin put forward the theory of aggravation of the class struggle along with the development of socialism, arguing that the further the country would move forward, the more acute forms of struggle will be used by the doomed remnants of exploiter classes in their last desperate efforts – and that, therefore, political repression was necessary.
In 1936, Stalin announced that the society of the Soviet Union consisted of two non-antagonistic classes: workers and kolkhoz peasantry. These corresponded to the two different forms of property over the means of production that existed in the Soviet Union: state property (for the workers) and collective property (for the peasantry). In addition to these, Stalin distinguished the stratum of intelligentsia. The concept of "non-antagonistic classes" was entirely new to Leninist theory. Among Stalin's contributions to Communist theoretical literature were "Dialectical and Historical Materialism," "Marxism and the National Question", "Trotskyism or Leninism", and "The Principles of Leninism."
Calculating the number of victims.
Before the 1991 dissolution of the Soviet Union, researchers who attempted to count the number of people killed under Stalin's regime produced estimates ranging from 3 to 60 million. After the Soviet Union dissolved, evidence from the Soviet archives also became available, containing official records of 799,455 executions (1921–1953), around 1.7 million deaths in the Gulag and some 390,000 deaths during kulak forced resettlement – with a total of about 2.9 million officially recorded victims in these categories.
The official Soviet archival records do not contain comprehensive figures for some categories of victims, such as those of ethnic deportations or of German population transfers in the aftermath of World War II. Eric D. Weitz wrote, "By 1948, according to Nicolas Werth, the mortality rate of the 600,000 people deported from the Caucasus between 1943 and 1944 had reached 25%." Other notable exclusions from NKVD data on repression deaths include the Katyn massacre, other killings in the newly occupied areas, and the mass shootings of Red Army personnel (deserters and so-called deserters) in 1941. The Soviets executed 158,000 soldiers for desertion during the war, and the "blocking detachments" of the NKVD shot thousands more. Also, the official statistics on Gulag mortality exclude deaths of prisoners taking place shortly after their release but which resulted from the harsh treatment in the camps. Some historians also believe that the official archival figures of the categories that were recorded by Soviet authorities are unreliable and incomplete. In addition to failures regarding comprehensive recordings, as one additional example, Robert Gellately and Simon Sebag Montefiore argue that the many suspects beaten and tortured to death while in "investigative custody" were likely not to have been counted amongst the executed.
Historians working after the Soviet Union's dissolution have estimated victim totals ranging from approximately 4 million to nearly 10 million, not including those who died in famines. Russian writer Vadim Erlikman, for example, makes the following estimates: executions, 1.5 million; gulags, 5 million; deportations, 1.7 million out of 7.5 million deported; and POWs and German civilians, 1 million – a total of about 9 million victims of repression.
Some have also included the deaths of 6 to 8 million people in the 1932–1933 famine among the victims of Stalin's repression. This categorization is controversial however, as historians differ as to whether the famine was a deliberate part of the campaign of repression against kulaks and others, or simply an unintended consequence of the struggle over forced collectivization.
Accordingly, if famine victims are included, a minimum of around 10 million deaths—6 million from famine and 4 million from other causes—are attributable to the regime, with a number of recent historians suggesting a likely total of around 20 million, citing much higher victim totals from executions, Gulag camps, deportations and other causes. Adding 6–8 million famine victims to Erlikman's estimates above, for example, would yield a total of between 15 and 17 million victims. Researcher Robert Conquest, meanwhile, has revised his original estimate of up to 30 million victims down to 20 million. In his most recent edition of "The Great Terror" (2007), Conquest states that while exact numbers may never be known with complete certainty, the various terror campaigns launched by the Soviet government claimed no fewer than 15 million lives. RJ Rummel maintains that the earlier higher victim total estimates are correct, although he includes those killed by the Soviet government in other Eastern European countries as well.
World War II, 1939–1945.
Pact with Hitler.
After a failed attempt to sign an anti-German military alliance with France and Britain and talks with Germany regarding a potential political deal, on 23 August 1939, the Soviet Union entered into a non-aggression pact with Nazi Germany, negotiated by Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. Officially a non-aggression treaty only, an appended secret protocol, also reached on 23 August 1939, divided the whole of eastern Europe into German and Soviet spheres of influence.
The eastern part of Poland, Latvia, Estonia, Finland and part of Romania were recognized as parts of the Soviet sphere of influence, with Lithuania added in a second secret protocol in September 1939. Stalin and Ribbentrop traded toasts on the night of the signing discussing past hostilities between the countries. Cooperation between the Soviet and Nazis was considerable to the point that in 1939 Trotsky called Stalin as the "Hitler' quartermaster".
Implementing the division of Eastern Europe and other invasions.
On 1 September 1939, the German invasion of its agreed upon portion of Poland started World War II. On 17 September the Red Army invaded eastern Poland and occupied the Polish territory assigned to it by the Molotov-Ribbentrop Pact, followed by co-ordination with German forces in Poland. Eleven days later, the secret protocol of the Molotov-Ribbentrop Pact was modified, allotting Germany a larger part of Poland, while ceding most of Lithuania to the Soviet Union.
After Stalin declared that he was going to "solve the Baltic problem", by June 1940, Lithuania, Latvia and Estonia were merged into the Soviet Union, after repressions and actions therein brought about the deaths of over 160,000 citizens of these states. After facing stiff resistance in an invasion of Finland, an interim peace was entered, granting the Soviet Union the eastern region of Karelia (10% of Finnish territory).
After this campaign, Stalin took actions to bolster the Soviet military, modify training and improve propaganda efforts in the Soviet military. In June 1940, Stalin directed the Soviet annexation of Bessarabia and northern Bukovina, proclaiming this formerly Romanian territory part of the Moldavian Soviet Socialist Republic. But in annexing northern Bukovina, Stalin had gone beyond the agreed limits of the secret protocol.
After the Tripartite Pact was signed by Axis Powers Germany, Japan and Italy, in October 1940, Stalin traded letters with Ribbentrop, with Stalin writing about entering an agreement regarding a "permanent basis" for their "mutual interests." After a conference in Berlin between Hitler, Molotov and Ribbentrop, Germany presented Molotov with a proposed written agreement for Axis entry. On 25 November, Stalin responded with a proposed written agreement for Axis entry which was never answered by Germany. Shortly thereafter, Hitler issued a secret directive on the eventual attempts to invade the Soviet Union. In an effort to demonstrate peaceful intentions toward Germany, on 13 April 1941, Stalin oversaw the signing of a neutrality pact with Axis power Japan.
On 6 May, Stalin replaced Molotov as Premier of the Soviet Union. Although Stalin had been the "de facto" head of government for a decade and a half, he had concluded relations with Nazi Germany had deteriorated to such an extent that he needed to deal with the problem as "de jure" head of government as well.
Hitler breaks the pact.
During the early morning of 22 June 1941, Adolf Hitler broke the pact by implementing Operation Barbarossa, the German invasion of the Soviet Union that began the war on the Eastern Front. Already in autumn 1940 Stalin received a warning from the Dutch Communist Party, via the network of the Red Orchestra, that Hitler was preparing for a winter war by allowing the construction of thousands of snow landing gears for the Junkers Ju 52 transport planes. Although Stalin had received warnings from spies and his generals, he felt that Germany would not attack the Soviet Union until Germany had defeated Britain. In the initial hours after the German attack commenced, Stalin hesitated, wanting to ensure that the German attack was sanctioned by Hitler, rather than the unauthorized action of a rogue general.
Accounts by Nikita Khrushchev and Anastas Mikoyan claim that, after the invasion, Stalin retreated to his dacha in despair for several days and did not participate in leadership decisions. However, some documentary evidence of orders given by Stalin contradicts these accounts, leading some historians to speculate that Khrushchev's account is inaccurate. By the end of 1941, the Soviet military had suffered 4.3 million casualties and German forces had advanced 1,050 miles (1,690 kilometers).
Soviets stop the Germans.
While the Germans pressed forward, Stalin was confident of an eventual Allied victory over Germany. In September 1941, Stalin told British diplomats that he wanted two agreements: (1) a mutual assistance/aid pact and (2) a recognition that, after the war, the Soviet Union would gain the territories in countries that it had taken pursuant to its division of Eastern Europe with Hitler in the Molotov–Ribbentrop Pact. The British agreed to assistance but refused to agree upon the territorial gains, which Stalin accepted months later as the military situation deteriorated somewhat in mid-1942. By December 1941, Hitler's troops had advanced to within 20 miles of the Kremlin in Moscow. On 5 December, the Soviets launched a counteroffensive, pushing German troops back 40–50 miles from Moscow, the Wehrmacht's first significant defeat of the war.
In 1942, Hitler shifted his primary goal from an immediate victory in the East, to the more long-term goal of securing the southern Soviet Union to conquer oil fields vital to a long-term German war effort. In July 1942, Hitler praised the efficiency of the Soviet military industry and Stalin:
Stalin, too, must command our unconditional respect. In his own way he is one hell of a fellow! (German: "ein genialer Kerl") He knows his models, Genghiz Khan and the others, very well, and the scope of his industrial planning is exceeded only by our own Four Year Plan.
While Red Army generals saw evidence that Hitler would shift efforts south, Stalin considered this to be a flanking campaign in efforts to take Moscow. During the war, "Time" magazine named Stalin Time Person of the Year twice and he was also one of the nominees for "Time" Person of the Century title.
Soviet push to Germany.
The Soviets repulsed the important German strategic southern campaign and, although there were 2.5 million Soviet casualties in that effort, it permitted the Soviets to take the offensive for most of the rest of the war on the Eastern Front.
Germany attempted an encirclement attack at Kursk, which was successfully repulsed by the Soviets. Kursk marked the beginning of a period where Stalin became more willing to listen to the advice of his generals. By the end of 1943, the Soviets occupied half of the territory taken by the Germans from 1941 to 1942. Soviet military industrial output also had increased substantially from late 1941 to early 1943 after Stalin had moved factories well to the East of the front, safe from German invasion and air attack.
In November 1943, Stalin met with Churchill and Roosevelt in Tehran. The parties later agreed that Britain and America would launch a cross-channel invasion of France in May 1944, along with a separate invasion of southern France. Stalin insisted that, after the war, the Soviet Union should incorporate the portions of Poland it occupied pursuant to the Molotov-Ribbentrop Pact with Germany, which Churchill opposed.
In 1944, the Soviet Union made significant advances across Eastern Europe toward Germany, including Operation Bagration, a massive offensive in Belorussia against the German Army Group Centre.
Final victory.
By April 1945, Nazi Germany faced its last days with 1.9 million German soldiers in the East fighting 6.4 million Red Army soldiers while 1 million German soldiers in the West battled 4 million Western Allied soldiers. While initial talk existed of a race to Berlin by the Allies, after Stalin successfully lobbied for Eastern Germany to fall within the Soviet "sphere of influence" at Yalta, no plans were made by the Western Allies to seize the city by a ground operation.
On 30 April, Hitler and Eva Braun committed suicide, after which Soviet forces found their remains, which had been burned at Hitler's directive. German forces surrendered a few days later. Despite the Soviets' possession of Hitler's remains, Stalin refused to believe that his old nemesis was actually dead, a belief that remained with him for years after the war ended.
Fending off the German invasion and pressing to victory in the East required a tremendous sacrifice by the Soviet Union. Soviet military casualties totaled approximately 35 million (official figures 28.2 million) with approximately 14.7 million killed, missing or captured (official figures 11.285 million). Although figures vary, the Soviet civilian death toll probably reached 20 million. One in four Soviets was killed or wounded. Some 1,710 towns and 70,000 villages were destroyed. Thereafter, Stalin was at times referred to as one of the most influential men in human history.
Nobel Peace Prize nominations.
Stalin was nominated for the Nobel Peace Prize in 1945 and 1948.
Human rights abuses.
After taking around 300,000 Polish prisoners in 1939 and early 1940, 25,700 Polish POWs were executed on 5 March 1940, pursuant to a note to Stalin from Lavrenty Beria, in what became known as the Katyn massacre. While Stalin personally told a Polish general they'd "lost track" of the officers in Manchuria, Polish railroad workers found the mass grave after the 1941 Nazi invasion. The massacre became a source of political controversy, with the Soviets eventually claiming that Germany committed the executions when the Soviet Union retook Poland in 1944. The Soviets did not admit responsibility until 1990.
Stalin introduced controversial military orders, such as Order No. 270 in August 1941, requiring superiors to shoot deserters on the spot while their family members were subject to arrest. Thereafter, Stalin also conducted a purge of several military commanders that were shot for "cowardice" without a trial. Stalin issued Order No. 227 in July 1942, directing that commanders permitting retreat without permission to be subject to a military tribunal, and soldiers guilty of disciplinary procedures to be forced into "penal battalions", which were sent to the most dangerous sections of the front lines. From 1942 to 1945, 427,910 soldiers were assigned to penal battalions. The order also directed "blocking detachments" to shoot fleeing panicked troops at the rear.
In June 1941, weeks after the German invasion began, Stalin also directed employing a scorched earth policy of destroying the infrastructure and food supplies of areas before the Germans could seize them, and that partisans were to be set up in evacuated areas. He also ordered the NKVD to murder around one hundred thousand political prisoners in areas where the Wehrmacht approached, while others were deported east.
After the capture of Berlin, Soviet troops reportedly raped from tens of thousands to two million women, and 50,000 during and after the occupation of Budapest. Many of these women died or committed suicide as a result of rape. In former Axis countries, such as Germany, Romania and Hungary, Red Army officers generally viewed cities, villages and farms as being open to pillaging and looting.
In the Soviet Occupation Zone of post-war Germany, the Soviets set up ten NKVD-run "special camps" subordinate to the gulag. These "special camps" were former Stalags, prisons, or Nazi concentration camps such as Sachsenhausen (special camp number 7) and Buchenwald (special camp number 2). According to German government estimates, "65,000 people died in those Soviet-run camps or in transportation to them."
According to recent figures, of an estimated four million POWs taken by the Soviets, including Germans, Japanese, Hungarians, Romanians and others, some 580,000 never returned, presumably victims of privation or the Gulags. German estimates put the actual death toll of German POWs in the USSR at about 1.0 million, they maintain that among those reported as missing were men who actually died as POW. Soviet POWs and forced laborers who survived German captivity were sent to special "transit" or "filtration" camps to determine which were potential traitors.
Of the approximately 4 million to be repatriated 2,660,013 were civilians and 1,539,475 were former POWs. Of the total, 2,427,906 were sent home and 801,152 were reconscripted into the armed forces. 608,095 were enrolled in the work battalions of the defense ministry. 272,867 were transferred to the authority of the NKVD for punishment, which meant a transfer to the Gulag system. 89,468 remained in the transit camps as reception personnel until the repatriation process was finally wound up in the early 1950s.
Allied conferences on post-war Europe.
Stalin met in several conferences with British Prime Minister Winston Churchill (and later Clement Attlee) and/or U.S. President Franklin D. Roosevelt (and later Harry Truman) to plan military strategy and, later, to discuss Europe's postwar reorganization. Very early conferences, such as that with British diplomats in Moscow in 1941 and with Churchill and American diplomats in Moscow in 1942, focused mostly upon war planning and supply, though some preliminary postwar reorganization discussion also occurred. In 1943, Stalin met with Churchill and Roosevelt in the Tehran Conference. In 1944, Stalin met with Churchill in the Moscow Conference. Beginning in late 1944, the Red Army occupied much of Eastern Europe during these conferences and the discussions shifted to a more intense focus on the reorganization of postwar Europe.
In February 1945, at the conference at Yalta, Stalin demanded a Soviet sphere of political influence in Eastern Europe. Stalin eventually was convinced by Churchill and Roosevelt not to dismember Germany. Stalin also stated that the Polish government-in-exile demands for self-rule were not negotiable, such that the Soviet Union would keep the territory of eastern Poland they had already taken by invasion with German consent in 1939, and wanted the pro-Soviet Polish government installed. After resistance by Churchill and Roosevelt, Stalin promised a re-organization of the current Communist puppet government on a broader democratic basis in Poland. He stated the new government's primary task would be to prepare elections.
The parties at Yalta further agreed that the countries of liberated Europe and former Axis satellites would be allowed to "create democratic institutions of their own choice", pursuant to "the right of all peoples to choose the form of government under which they will live." The parties also agreed to help those countries form interim governments "pledged to the earliest possible establishment through free elections" and "facilitate where necessary the holding of such elections." After the re-organization of the Provisional Government of the Republic of Poland, the parties agreed that the new party shall "be pledged to the holding of free and unfettered elections as soon as possible on the basis of universal suffrage and secret ballot." One month after Yalta, the Soviet NKVD arrested 16 Polish leaders wishing to participate in provisional government negotiations, for alleged "crimes" and "diversions", which drew protest from the West. The fraudulent Polish elections, held in January 1947 resulted in Poland's official transformation to undemocratic communist state by 1949.
At the Potsdam Conference from July to August 1945, though Germany had surrendered months earlier, instead of withdrawing Soviet forces from Eastern European countries, Stalin had not moved those forces. At the beginning of the conference, Stalin repeated previous promises to Churchill that he would refrain from a "Sovietization" of Eastern Europe. Stalin pushed for reparations from Germany without regard to the base minimum supply for German citizens' survival, which worried Truman and Churchill who thought that Germany would become a financial burden for Western powers.
In addition to reparations, Stalin pushed for "war booty", which would permit the Soviet Union to directly seize property from conquered nations without quantitative or qualitative limitation, and a clause was added permitting this to occur with some limitations. By July 1945, Stalin's troops effectively controlled the Baltic States, Poland, Czechoslovakia, Hungary, Bulgaria, and Romania, and refugees were fleeing out of these countries fearing a Communist take-over. The western allies, and especially Churchill, were suspicious of the motives of Stalin, who had already installed communist governments in the central European countries under his influence.
In these conferences, his first appearances on the world stage, Stalin proved to be a formidable negotiator. Anthony Eden, the British Foreign Secretary noted: "Marshal Stalin as a negotiator was the toughest proposition of all. Indeed, after something like thirty years' experience of international conferences of one kind and another, if I had to pick a team for going into a conference room, Stalin would be my first choice. Of course the man was ruthless and of course he knew his purpose. He never wasted a word. He never stormed, he was seldom even irritated."
Post-war era, 1945–1953.
The Iron Curtain and the Eastern Bloc.
After Soviet forces remained in Eastern and Central European countries, with the beginnings of communist puppet regimes in those countries, Churchill referred to the region as being behind an "Iron Curtain" of control from Moscow. The countries under Soviet control in Eastern and Central Europe were sometimes called the "Eastern bloc" or "Soviet Bloc".
In Soviet-controlled East Germany, the major task of the ruling communist party in Germany was to channel Soviet orders down to both the administrative apparatus and the other bloc parties pretending that these were initiatives of its own, with deviations potentially leading to reprimands, imprisonment, torture and even death. Property and industry were nationalized.
The German Democratic Republic was declared on 7 October 1949, with a new constitution which enshrined socialism and gave the Soviet-controlled Socialist Unity Party (SED) control. In Berlin, after citizens strongly rejected communist candidates in an election, in June 1948, the Soviet Union blockaded West Berlin, the portion of Berlin not under Soviet control, cutting off all supply of food and other items. The blockade failed due to the unexpected massive aerial resupply campaign carried out by the Western powers known as the Berlin Airlift. In 1949, Stalin conceded defeat and ended the blockade.
While Stalin had promised at the Yalta Conference that free elections would be held in Poland, after an election failure in "3 times YES" elections, vote rigging was employed to win a majority in the carefully controlled poll. Following the forged referendum, the Polish economy started to become nationalized.
In Hungary, when the Soviets installed a communist government, Mátyás Rákosi, who described himself as "Stalin's best Hungarian disciple" and "Stalin's best pupil", took power. Rákosi employed "salami tactics", slicing up these enemies like pieces of salami, to battle the initial postwar political majority ready to establish a democracy. Rákosi, employed Stalinist political and economic programs, and was dubbed the "bald murderer" for establishing one of the harshest dictatorships in Europe. Approximately 350,000 Hungarian officials and intellectuals were purged from 1948 to 1956.
During World War II, in Bulgaria, the Red Army crossed the border and created the conditions for a communist coup d'état on the following night. The Soviet military commander in Sofia assumed supreme authority, and the communists whom he instructed, including Kimon Georgiev, took full control of domestic politics.
In 1949, the Soviet Union, Bulgaria, Czechoslovakia, Hungary, Poland, and Romania founded the Comecon in accordance with Stalin's desire to enforce Soviet domination of the lesser states of Central Europe and to mollify some states that had expressed interest in the Marshall Plan, and which were now, increasingly, cut off from their traditional markets and suppliers in Western Europe. Czechoslovakia, Hungary, and Poland had remained interested in Marshall aid despite the requirements for a convertible currency and market economies. In July 1947, Stalin ordered these communist-dominated governments to pull out of the Paris Conference on the European Recovery Programme. This has been described as "the moment of truth" in the post–World War II division of Europe.
In Greece, Britain and the United States supported the anti-communists in the Greek Civil War and suspected the Soviets of supporting the Greek communists, although Stalin refrained from getting involved in Greece, dismissing the movement as premature. Albania remained an ally of the Soviet Union, but Yugoslavia broke with the USSR in 1948.
In Stalin's last year of life, one of his last major foreign policy initiatives was the 1952 Stalin Note for German reunification and Superpower disengagement from Central Europe, but Britain, France, and the United States viewed this with suspicion and rejected the offer.
Sino-Soviet relations.
In Asia, the Red Army had overrun Manchuria in the last month of the war and then also occupied Korea above the 38th parallel north. Mao Zedong's Communist Party of China, though receptive to minimal Soviet support, defeated the pro-Western and heavily American-assisted Chinese Nationalist Party (Kuomintang, KMT) in the Chinese Civil War.
There was friction between Stalin and Mao from the beginning. During World War II Stalin had supported the dictator of China, Chiang Kai-Shek, as a bulwark against Japan and had turned a blind eye to Chiang's mass killings of communists. He generally put his alliance with Chiang against Japan ahead of helping his ideological allies in China in his priorities. Even after the war Stalin concluded a non-aggression pact between the USSR and Chiang's KMT regime in China and instructed Mao and the Chinese communists to cooperate with Chiang and the KMT after the war. Mao did not follow Stalin's instructions though and started a communist revolution against Chiang. Stalin did not believe Mao would be successful so he was less than enthusiastic in helping Mao. The USSR continued to maintain diplomatic relations with Chiang's KMT regime until 1949 when it became clear Mao would win.
Stalin supported the Turkic Muslims known today as Uyghur in seeking their own state, Second East Turkestan Republic during the Ili Rebellion against the Republic of China. He backed the Uyghur Communist Muslim leader Ehmetjan Qasim against the anti Communist Chinese Kuomintang forces.
Stalin did conclude a new friendship and alliance treaty with Mao after he defeated Chiang. But there was still a lot of tension between the two leaders and resentment by Mao for Stalin's less than enthusiastic help during the civil war in China.
The Communists controlled mainland China while the Nationalists held a rump state on the island of Taiwan. The Soviet Union soon after recognized Mao's People's Republic of China, which it regarded as a new ally. The People's Republic claimed Taiwan, though it had never held authority there.
Diplomatic relations between the Soviet Union and China reached a high point with the signing of the 1950 Sino-Soviet Treaty of Friendship and Alliance. Both countries provided military support to a new friendly state in North Korea. After various Korean border conflicts, war broke out with U.S.-allied South Korea in 1950, starting the Korean War.
However, not surprisingly, the relations with the Kuomintang deteriorated. In 1951, in Taiwan, the Chinese Muslim Kuomintang General Bai Chongxi made a speech broadcast on radio to the entire Muslim world calling for a war against Russia, claiming that the "imperialist ogre" leader Stalin was engineering World War III, and Bai also called upon Muslims to avoid the Indian leader Jawaharlal Nehru, accusing him of being blind to Soviet imperialism.
North Korea and the Korean War.
Contrary to America's policy which restrained armament (limited equipment was provided for infantry and police forces) to South Korea, Stalin extensively armed Kim Il Sung's North Korean army and air forces with military equipment and "advisors" far in excess of those required for defensive purposes in order to facilitate Kim's (a former Soviet Officer) aim of conquering the rest of the Korean peninsula.
The North Korean Army struck in the pre-dawn hours of Sunday, 25 June 1950, crossing the 38th parallel behind a firestorm of artillery, beginning their invasion of South Korea. During the Korean War, Soviet pilots flew Soviet aircraft from Chinese bases against United Nations aircraft defending South Korea. Post-Cold War research in Soviet Archives has revealed that the Korean War was begun by Kim Il-sung with the express permission of Stalin.
Israel.
Stalin originally supported the creation of Israel in 1948. The USSR was one of the first nations to recognize the new country. Golda Meir came to Moscow as the first Israeli Ambassador to the USSR that year. However, after providing war materiel for Israel through Czechoslovakia from 1947 to 1949, Stalin later changed his mind and came out against Israel.
"Falsifiers of History".
In 1948, Stalin personally edited and rewrote by hand sections of the cold war book "Falsifiers of History". "Falsifiers" was published in response to the documents made public in "Nazi-Soviet Relations, 1939–1941: Documents from the Archives of The German Foreign Office", which included the secret protocols of the Molotov-Ribbentrop Pact and other secret German-Soviet relations documents. "Falsifiers" originally appeared as a series of articles in "Pravda" in February 1948, and was subsequently published in numerous languages and distributed worldwide.
The book did not attempt to directly counter or deal with the documents published in "Nazi-Soviet Relations" and rather, focused upon Western culpability for the outbreak of war in 1939. It argues that "Western powers" aided Nazi rearmament and aggression, including that American bankers and industrialists provided capital for the growth of German war industries, while deliberately encouraging Hitler to expand eastward. It depicted the Soviet Union as striving to negotiate a collective security against Hitler, while being thwarted by double-dealing Anglo-French appeasers who, despite appearances, had no intention of a Soviet alliance and were secretly negotiating with Berlin. It casts the Munich agreement, not just as Anglo-French short-sightedness or cowardice, but as a "secret" agreement that was "a highly important phase in their policy aimed at goading the Hitlerite aggressors against the Soviet Union." The book also included the claim that, during the Pact's operation, Stalin rejected Hitler's offer to share in a division of the world, without mentioning the Soviet offers to join the Axis. Historical studies, official accounts, memoirs and textbooks published in the Soviet Union used that depiction of events until the Soviet Union's dissolution.
Domestic support.
Domestically, Stalin was seen as a great wartime leader who had led the Soviets to victory against the Nazis.
An increasingly nationalistic emphasis on Russian history and achievements became a salient feature of Soviet culture in the 1940s. At the end of May 1945, Stalin proposed a victory toast to the Soviet people, and to the virtues of the Russian majority in particular:
I should like to propose a toast to the health of our Soviet people, and in the first place, the Russian people. (Loud and prolonged applause and shouts of 'Hurrah.')
I drink in the first place to the health of the Russian people because it is the most outstanding nation of all the nations forming the Soviet Union.
I propose a toast to the health of the Russian people because it has won in this war universal recognition as the leading force of the Soviet Union among all the peoples of our country.
I propose a toast to the health of the Russian people not only because it is the leading people, but also because it possesses a clear mind, a staunch character, and patience.
Stalin's military-territorial actions during World War II were supported by Russian nationalists inside and outside the Soviet Union (Russian exile Pavel Milyukov during Winter War: "I feel pity for the Finns, but I am for the Vyborg guberniya") for the recovering of the lands lost during the Russo-Japanese War of 1905 and most of the lands lost by the former Russian Empire in World War I through the Treaty of Brest-Litovsk signed by Trotsky and the Central Powers in 1918. Also, by 1945–1948 for the first time since the Middle Ages the Eastern Slavic lands and peoples were reunited in a single country, and all Slavic nations were outside German (with the definitive termination of the "Drang nach Osten"), Turkish or other Western European influence and under the orbit of Moscow – an old dream cherished by Russian nationalists and Pan-Slavists alike.
Various foreign scientific discoveries and inventions (such as the Wright Brothers' airplane) were attributed to Russians in post-war Soviet propaganda. Examples include the boiler, reclaimed by father and son Cherepanovs; the electric light, by Yablochkov and Lodygin; the radio, by Popov; and the airplane, by Mozhaysky. Stalin's internal repressive policies continued (including in newly acquired territories), but never reached the extremes of the 1930s.
"Doctors' plot".
The "Doctors' plot" was a plot outlined by Stalin and Soviet officials in 1952 and 1953 whereby several doctors (over half of whom were Jewish) allegedly attempted to kill Soviet officials. The prevailing opinion of many scholars outside the Soviet Union is that Stalin intended to use the resulting doctors' trial to launch a massive party purge. The plot is also viewed by many historians as an antisemitic provocation. It followed on the heels of the 1952 show trials of the Jewish Anti-Fascist Committee and the secret execution of thirteen members on Stalin's orders in the Night of the Murdered Poets.
Thereafter, in a December Politburo session, Stalin announced that "Every Jewish nationalist is the agent of the American intelligence service. Jewish nationalists think that their nation was saved by the United States (there you can become rich, bourgeois, etc.). They think they're indebted to the Americans. Among doctors, there are many Jewish nationalists." To mobilize the Soviet people for his campaign, Stalin ordered "TASS" and "Pravda" to issue stories along with Stalin's alleged uncovering of a "Doctors Plot" to assassinate top Soviet leaders, including Stalin, in order to set the stage for show trials.
The next month, Pravda published stories with text regarding the purported "Jewish bourgeois-nationalist" plotters. Nikita Khrushchev wrote that Stalin hinted him to incite anti-Semitism in the Ukraine, telling him that "the good workers at the factory should be given clubs so they can beat the hell out of those Jews." Stalin also ordered falsely accused physicians to be tortured "to death". Regarding the origins of the plot, people who knew Stalin, such as Khrushchev, suggest that Stalin had long harbored negative sentiments toward Jews, and anti-Semitic trends in the Kremlin's policies were further fueled by the exile of Leon Trotsky. In 1946, Stalin allegedly said privately that "every Jew is a potential spy." At the end of January 1953, Stalin's personal physician Miron Vovsi (cousin of Solomon Mikhoels, who was assassinated in 1948 at the orders of Stalin) was arrested within the frame of the plot. Vovsi was released by Beria after Stalin's death in 1953, as was his son-in-law, the composer Mieczyslaw Weinberg.
Some historians have argued that Stalin was also planning to send millions of Jews to four large newly built labor camps in Western Russia using a "Deportation Commission" that would purportedly act to save Soviet Jews from an enraged Soviet population after the Doctors Plot trials. Others argue that any charge of an alleged mass deportation lacks specific documentary evidence. Regardless of whether a plot to deport Jews was planned, in his "Secret Speech" in 1956, Soviet Premier Nikita Khrushchev stated that the Doctors Plot was "fabricated ... set up by Stalin", that Stalin told the judge to beat confessions from the defendants and had told Politburo members "You are blind like young kittens. What will happen without me? The country will perish because you do not know how to recognize enemies."
Death and legacy.
Stalin's health deteriorated towards the end of World War II. He suffered from atherosclerosis from his heavy smoking. He suffered a mild stroke around the time of the Victory Parade, and a severe heart attack in October 1945.
In the early morning hours of 1 March 1953, after an all-night dinner and a movie, Stalin arrived at his Kuntsevo residence 15 km west of Moscow centre, with interior minister Lavrentiy Beria and future premiers Georgy Malenkov, Nikolai Bulganin and Nikita Khrushchev, where he retired to his bedroom to sleep. At dawn, Stalin did not emerge from his room.
Although his guards thought that it was strange not to see him awake at his usual time, they were strictly instructed not to bother him and left him alone the entire day. At around 10 p.m., he was discovered by Peter Lozgachev, the Deputy Commandant of Kuntsevo, who entered his bedroom to check up on him and recalled the scene of Stalin lying on his back on the floor of his room beside his bed wearing pyjama bottoms and an undershirt with his clothes soaked in stale urine. A frightened Lozgachev asked Stalin what happened to him, but all he could get out of him was unintelligible responses that sounded like "Dzhhhhh." Lozgachev used the bedroom telephone where he frantically called a few party officials telling them that Stalin may have had a stroke and asked them to send good doctors to the Kuntsevo residence immediately. Lavrentiy Beria was informed and arrived a few hours afterwards. The doctors arrived in the early morning of 2 March when they changed Stalin's bedclothes and tended to him. They diagnosed him with a cerebral hemorrhage (stroke) caused by hypertension (high blood pressure), with stomach hemorrhage facilitating. He was treated in his dacha with leeches, as was customary at the time. On March 3 his double Felix Dadaev was called back from vacation to Moscow "to be ready to stand in for Stalin if needed", but he never needed to. On March 4 Stalin's illness was broadcast in the media with surprising detail such as pulse, blood pressure and urinalysis; for convenience the time of his stroke was said to be March 2 and his location as Moscow. The bedridden Stalin died on 5 March 1953, at the age of 74.
Suggestions of assassination.
The political memoirs of Vyacheslav Molotov, published in 1993, claimed that Beria had boasted to Molotov that he poisoned Stalin: "I took him out."
Stomach hemorrhage is usually not caused by high blood pressure, but is, along with stroke, consistent with overdose of warfarin, a colorless, tasteless anticoagulant drug. In the treating physicians' final report submitted to the Central Committee in July 1953, any mention of the stomach hemorrhage was "deleted or vastly subordinated to other information." In 2004, American historian Jonathan Brent and Russia's Presidential Commission for the Rehabilitation of Repressed Persons executive secretary Vladimir Naumov published a book proposing that Beria, with the complicity of Khrushchev, slipped warfarin into Stalin's wine on the night of his death.
Stalin's autopsy, conducted by the Soviet Ministry of Health in March 1953 but not released until 2011, confirmed the cause of death as stroke resulting from high blood pressure, and that hypertension had caused cardiac hemorrhage (not usually caused by high blood pressure) and gastrointestinal hemorrhage as well. In 2011 Miguel A. Faria, President of Mercer University School of Medicine, retired clinical professor of neurosurgery and adjunct professor of medical history, interpreted the autopsy's composition as the examiners' desire to demonstrate for posterity, that they had fulfilled their professional duties as best they could by mentioning the non-cerebral hemorrhages. At the same time they would have provided themselves political cover by purposely attributing the hemorrhages to hypertension instead of poisoning by warfarin. Faria noted that when the autopsy was performed, "Stalin was worshipped as a demigod, and his assassination would have been unacceptable to the Russian populace." He also notes that Stalin experienced renal hemorrhages during his death, which is unlikely to be caused by high blood pressure.
It has also been suggested by Jože Pirjevec that Stalin was assassinated by the order of Josip Broz Tito in retaliation for assassination attempts on Tito. A letter was found in Stalin's office from Tito that read: "Stop sending people to kill me. We've already captured five of them, one of them with a bomb and another with a rifle ... If you don't stop sending killers, I'll send one to Moscow, and I won't have to send a second."
Announcement.
Yuri Levitan, the announcer who during the war brought the Soviet people news of victories—but never of defeats—announced Stalin's death. Slowly, solemnly, with a voice brimming over with emotion, he read:
The Central Committee of the Communist party, the Council of Ministers and the Presidium of the Supreme Soviet of the USSR announce with deep grief to the party and all workers that on 5 March, at 9.50 p.m., Iosif Vissarionovich Stalin, Secretary of the Central Committee of the Communist party and Chairman of the Council of Ministers, has died after a serious illness. The heart of the collaborator and follower of the genius of Lenin's work, the wise leader and teacher of the Communist party and of the Soviet people, has stopped beating.
After a visitation of 1.5 million people, his embalmed body was laid to rest on March 9, 1953 in Lenin's Mausoleum. On 31 October 1961 his body was removed from the mausoleum and buried in the Kremlin Wall Necropolis next to the Kremlin walls as part of the process of de-Stalinization.
Aftermath.
His demise arrived at a convenient time for Lavrentiy Beria and others, who feared being swept away in yet another purge. It is believed that Stalin felt Beria's power was too great and threatened his own.
After Stalin's death a power struggle for his vacant position took place between the following eight senior members of the Presidium of the Central Committee of the Communist Party of the Soviet Union listed according to the order of precedence presented formally on 5 March 1953: Georgy Malenkov, Lavrentiy Beria, Vyacheslav Molotov, Klim Voroshilov, Nikita Khrushchev, Nikolai Bulganin, Lazar Kaganovich, Anastas Mikoyan.
This struggle lasted until 1958 and eventually Khrushchev won, having defeated all his potential rivals in the Presidium.
Reaction by successors.
The harshness with which Soviet affairs were conducted during Stalin's rule was subsequently repudiated by his successors in the Communist Party leadership, most notably by Nikita Khrushchev's repudiation of Stalinism in February 1956. In his "Secret Speech", "On the Personality Cult and its Consequences", delivered to a closed session of the 20th Congress of the Communist Party of the Soviet Union, Khrushchev denounced Stalin for his cult of personality, and his regime for "violation of Leninist norms of legality".
A 1974 Soviet work describes Stalin's leadership in the following manner:
 J. V. Stalin had held, since 1922, the post of General Secretary of the Communist Party Central Committee. He had made important contributions to the implementation of the Party's policy of socialist construction in the USSR, and he had won great popularity by his relentless fight against the anti-Leninist groups of the Trotskyites and Bukharinites. Since the early 1930s, however, all the successes achieved by the Soviet people in the building of socialism began to be arbitrarily attributed to Stalin. Already in a letter written back in 1922 Lenin warned the Party Central Committee: "Comrade Stalin," he wrote, "having become general secretary, has concentrated boundless authority in his hands, and I am not sure whether he will always be able to exercise that authority with sufficient discretion." During the first few years after Lenin's death Stalin reckoned with his critical remarks. As time passed, however, he abused his position of General Secretary of the Party Central Committee more and more frequently, violating the principle of collective leadership and making independent decisions on important Party and state issues. Those personal shortcomings of which Lenin had warned manifested themselves with greater and greater insistence: his rudeness, capriciousness, intolerance of criticism, arbitrariness, excessive suspiciousness, etc. This led to unjustified restrictions of democracy, gross violations of socialist legality and repressions against prominent Party, government and military leaders and other people.
—"A Short History of the World in Two Volumes" Vol. II.
Views on Stalin in the Russian Federation.
Results of a controversial poll taken in 2006 stated that over 35% of Russians would vote for Stalin if he were still alive. Fewer than a third of all Russians regarded Stalin as a "murderous tyrant"; however, a Russian court in 2009, ruling on a suit by Stalin's grandson, Yevgeny Dzhugashvili, against the newspaper, "Novaya Gazeta", ruled that referring to Stalin as a "bloodthirsty cannibal" was not libel. In a July 2007 poll, 54% of the Russian youth agreed that Stalin did more good than bad while 46% (of them) disagreed that Stalin was a "cruel tyrant". Half of the respondents, aged from 16 to 19, agreed Stalin was a wise leader.
In December 2008, Stalin was voted third in the nationwide television project "Name of Russia" (narrowly behind 13th-century prince Alexander Nevsky and Pyotr Stolypin, one of Nicholas II's prime ministers). The Communist Party accused the Kremlin in rigging the poll in order to prevent him or Lenin being given first place.
On 3 July 2009, Russia's delegates walked out of an Organization for Security and Co-operation in Europe session to demonstrate their objections to a resolution for a remembrance day for the "victims of both Nazism and Stalinism". Only eight out of 385 assembly members voted against the resolution.
In a Kremlin video blog posted on 29 October 2009, Russian President Dmitry Medvedev denounced the efforts of people seeking to rehabilitate Stalin's image. He said the mass extermination during the Stalin era cannot be justified.
In a 2013 Q&A session, when asked whether Russia should restore statues of its Soviet-era leaders, Russian President Vladimir Putin replied "What is the essential difference between (Oliver) Cromwell and (Joseph) Stalin? Can you tell me? No difference...,(Cromwell's) monument is standing, (and) no one is going to remove it. The essence is not in these symbols, but in the need to treat with respect every period of our history."
Views on Stalin in other former Soviet states.
The BBC News reported that "Lasha Bakradze, a professor of Soviet history at Tbilisi University, recently presented a new survey commissioned by the Carnegie Endowment for International Peace, which found that 45% of Georgians expressed a positive attitude to Stalin".
In a poll taken by Kyiv International Institute of Sociology in February 2013 37% of all Ukrainians had "a negative attitude to the figure of Stalin" and 22% "a positive". Positive attitudes prevailed in East Ukraine (36%) and South Ukraine (27%), and negative attitudes in West Ukraine (64%) and Central Ukraine (39%). In the age group 18–29 years 16% had positive feelings towards Stalin.
Early 2010 a Ukrainian court convicted Stalin of genocide against the Ukrainian nation during the Soviet famine of 1932–1933.
In the spring of 2010 a new monument in honor of Stalin was erected in Zaporizhia. In late December 2010 the statue had his head cut off by unidentified vandals and the following New Year's Eve it was completely destroyed in an explosion. On 25 February 2011 Ukrainian President Viktor Yanukovych stated "Ukraine will definitely not revise its negative view" on Stalin. Ukraine and Poland unveiled a memorial (outside Kiev) to the thousands of Ukrainians, Poles and others killed by Stalin's secret police ahead of World War II in September 2012.
According to a 2012 study, 72% of Armenians do not want to live in a country led by someone like Stalin.
Personal life.
Origin of name, nicknames and pseudonyms.
Stalin's original Georgian name is transliterated as "Ioseb Besarionis dze Jughashvili" (Georgian: იოსებ ბესარიონის ძე ჯუღაშვილი). The Russian transliteration of his name Ио́сиф Виссарио́нович Джугашви́ли is in turn transliterated to English as "Iosif Vissarionovich Dzhugashvili". Like other Bolsheviks, he became commonly known by one of his revolutionary "noms de guerre", of which "Stalin" was only the last. "Stalin" is based on the Russian word сталь "stal", meaning "steel", and the name as a whole is supposed to mean "man of steel". Prior nicknames included "Koba", "Soselo", "Ivanov" and many others.
Stalin is believed to have started using the name "K. Stalin" sometime in 1912 as a pen name.
During Stalin's reign his nicknames included:
Appearance.
While photographs and portraits portray Stalin as physically massive and majestic (he had several painters shot who did not depict him "right"), he was only 5 ft tall. (President Harry S. Truman, who stood 5 ft himself, described Stalin as "a little squirt".) His mustached face was pock-marked from small-pox during childhood. After a carriage accident in his youth, his left arm was shortened and stiffened at the elbow, while his right hand was thinner than his left and frequently hidden. Bronze casts made in 1990 from plaster death mask and plaster cards of his hands clearly show a normal right hand and a withered left hand. He could be charming and polite, mainly towards visiting statesmen. In movies, Stalin was often played by Mikheil Gelovani and, less frequently, by Aleksei Dikiy.
Marriages and family.
Stalin married his first wife Ekaterina Svanidze in 1906, with whom he had a son, Yakov. Yakov shot himself because of Stalin's harshness toward him, but survived. After this, Stalin said, "He can't even shoot straight." Yakov served in the Red Army during World War II and was captured by the Germans. They offered to exchange him for Field Marshal Friedrich Paulus, who had surrendered after Stalingrad, but Stalin turned the offer down, stating, "You have in your hands not only my son Yakov, but millions of my sons. Either you free them all or my son will share their fate." Afterwards, Yakov is said to have committed suicide, running into an electric fence in Sachsenhausen concentration camp, where he was being held. Yakov had a son Yevgeny, who is recently noted for defending his grandfather's legacy in Russian courts. Yevgeny is married to a Georgian woman, has two sons, and seven grandchildren.
With his second wife Nadezhda Alliluyeva Stalin had a son, Vasiliy, and a daughter, Svetlana. Nadezhda died in 1932, officially of illness. She may have committed suicide by shooting herself after a quarrel with Stalin, leaving a suicide note which according to their daughter was "partly personal, partly political." According to A&E Biography, there is also a belief among some Russians that Stalin himself murdered his wife after the quarrel, which apparently took place at a dinner in which Stalin tauntingly flicked cigarettes across the table at her.
Vasiliy rose through the ranks of the Soviet Air Force, officially dying of alcoholism in 1962; however, this is still in question. He distinguished himself in World War II as a capable airman. Svetlana defected to the United States in 1967, where she later married William Wesley Peters, the apprentice of Frank Lloyd Wright. She died in Richland Center, Wisconsin on November 22, 2011, from complications of colon cancer. Olga, her daughter with Peters, now goes by Chrese Evans and lives in Portland, Oregon.
In March 2001, Russian Independent Television NTV interviewed a previously unknown grandson living in Novokuznetsk, Yuri Davydov, who stated that his father had told him of his lineage, but, was told to keep quiet because of the campaign against Stalin's cult of personality.
Beside his suite in the Kremlin, Stalin had numerous domiciles. In 1919, he started with a country house near Usovo, he added dachas at Zuvalova and Kuntsevo ("Blizhny dacha" built by Miron Merzhanov). Before World War II he added the Lipki estate and Semyonovskaya, and had at least four dachas in the south by 1937, including one near Sochi. A luxury villa near Gagri was given to him by Beria. In Abkhazia he maintained a mountain retreat. After the war he added dachas at Novy Afon, near Sukhumi, in the Valdai Hills, and at Lake Mitsa. Another estate was near Zelyony Myss on the Black Sea. All these dachas, estates, and palaces were staffed, well-furnished and equipped, kept safe by security forces, and were mainly used privately, rarely for diplomatic purposes. Between places Stalin would travel by car or train, never by air; he flew only once when attending the 1943 Tehran conference.
Habits.
Stalin enjoyed drinking, and would often force those around him to join in. He preferred Georgian wine over Russian vodka, but usually ate traditional Russian food.
Khrushchev reports in his memoirs that Stalin was fond of American cowboy movies. He would often sleep until evening in his dacha, and after waking up summon high-ranking Soviet politicians to watch foreign movies with him in the Kremlin movie theater. The movies, being in foreign languages, were given a running translation by Ivan Bolshakov, people's commissar of cinema. The translations were hilarious for the audience as Bolshakov spoke very basic English. His favourite films were westerns and Charlie Chaplin episodes. He banned any hint of nudity. When Ivan showed a film with a naked woman Stalin shouted: "Are you making a brothel here, Bolshakov?" After a movie had ended, Stalin often invited the audience for dinner, even though the clock was usually past midnight. In the aftermath of the war, he took control over all of Joseph Goebbels' films.
Stalin was an accomplished billiards player, and could read 500 pages a day, having a library of over 20,000 books.
Religion.
Although raised in the Georgian Orthodox faith, Stalin was an atheist. Stalin had a complex relationship with religious institutions in the Soviet Union. Historians Vladislav Zubok and Constantine Pleshakov have suggested that "[Stalin's] atheism remained rooted in some vague idea of a God of nature."
During the Second World War, Stalin reopened the churches. One reason could have been to motivate the majority of the population who had Christian beliefs. The reasoning behind this is that by changing the official policy of the party and the state towards religion, the Church and its clergymen could be at his disposal in mobilizing the war effort. On 4 September 1943, Stalin invited Metropolitan Sergius, Metropolitan Alexius and Metropolitan Nicholas to the Kremlin and proposed to reestablish the Moscow Patriarchate, which had been suspended since 1925, and elect the Patriarch. On 8 September 1943, Metropolitan Sergius was elected patriarch.
The CPSU Central Committee continued to promote atheism and the elimination of religion during the remainder of Stalin's lifetime after the 1943 concordat. Stalin's greater tolerance for religion after 1943 was limited by party machinations. Whether persecutions after World War II were more aimed at certain sections of society over and above detractors is a disputed point.
Hypotheses, rumors and misconceptions about Stalin.
There are conflicting accounts of Stalin's birth, who listed his birth year in various documents as being in 1878 before coming to power in 1922. The phrase "death of one man is a tragedy, death of a million is a statistic" is sometimes attributed to Stalin, although there is no proof of him saying that. In addition, hypotheses and popular rumors exist about Stalin's biological father having been explorer Nicolay Przhevalsky. Some Bolsheviks and others have accused Stalin of being an agent for the Okhrana. It is also widely believed that the Red Terror was begun by Stalin.
Works.
Stalin was also a well-regarded poet in his youth. Some of his poems were published in Ilia Chavchavadze's journal "Iveria" and later anthologized.
References.
</dl>
External links.
</dl>

</doc>
<doc id="15642" url="http://en.wikipedia.org/wiki?curid=15642" title="January">
January

January ( ) is the first month of the year in the Julian and Gregorian calendars and one of seven months with the length of 31 days. The first day of the month is known as New Year's Day. It is, on average, the coldest month of the year within most of the Northern Hemisphere (where it is the second month of winter) and the warmest month of the year within most of the Southern Hemisphere (where it is the second month of summer). In the Southern hemisphere, January is the seasonal equivalent of July in the Northern hemisphere and vice versa.
January starts on the same day of the week as October in common years, and starts on the same day of the week as April and July in leap years. January ends on the same day of the week as February and October in a common year, and ends on the same day of the week as July in a leap year. In all years, January begins and ends on the same day of the week as May of the previous year. January in common years immediately before other common years begins on the same day of the week as April and July of the following year and in leap years and years immediately before that, January begins on the same day of the week as September and December of the following year. In common years immediately before other common years, January finishes on the same day of the week as July of the following year while in leap years and years immediately before that, January finishes on the same day of the week as April and December of the following year.
History.
January (in Latin, "Ianuarius") is named after Janus, the god of beginnings and transitions; the name has its beginnings in Roman mythology, coming from the Latin word for door ("ianua") since January is the door to the year.
Traditionally, the original Roman calendar consisted of 10 months totaling 304 days, winter being considered a month-less period. Around 713 BC, the semi-mythical successor of Romulus, King Numa Pompilius, is supposed to have added the months of January and February, allowing the calendar to equal a standard lunar year (354 days). Although March was originally the first month in the old Roman Calendar, January became the first month of the calendar year under either Numa or the Decemvirs about 450 BC (Roman writers differ). In contrast, specific years pertaining to dates were identified by naming two consuls, who entered office on May 1 and March 15 until 153 BC, when they began to enter office on January 1.
Various Christian feast dates were used for the New Year in Europe during the Middle Ages, including March 25 and December 25. However, medieval calendars were still displayed in the Roman fashion of twelve columns from January to December. Beginning in the 16th century, European countries began officially making January 1 the start of the New Year once again—sometimes called "Circumcision Style" because this was the date of the Feast of the Circumcision, being the seventh day after December 25.
Historical names for January include its original Roman designation, Ianuarius, the Saxon term Wulf-monath (meaning wolf month) and Charlemagne's designation Wintarmanoth (winter / cold month). In Slovene, it is traditionally called "prosinec". The name, associated with millet bread and the act of asking for something, was first written in 1466 in the Škofja Loka manuscript.
According to Theodor Mommsen (The History of Rome, volume 4, The Revolution, ISBN 1-4353-4597-5, page 4), 1 January became the first day of the year in 600 AUC of the Roman Calendar (153 BC), due to disasters in the Lusitanian War. A Lusitanian chief called Punicus invaded the Roman territory, defeated two Roman governors, and slew their troops. The Romans resolved to send a consul to Hispania, and in order to accelerate the dispatch of aid, "they even made the new consuls enter on office two months and a half before the legal time" (15th of March).

</doc>
<doc id="15644" url="http://en.wikipedia.org/wiki?curid=15644" title="Johnny Unitas">
Johnny Unitas

John Constantine "Johnny" Unitas (; May 7, 1933 – September 11, 2002), nicknamed "Johnny U", or "The Golden Arm", was an American professional American football player who played from the 1950s through the 1970s. He spent the majority of his career playing for the Baltimore Colts. He was a record-setting quarterback, and the National Football League's most valuable player in 1959, 1964 and 1967. For 52 years he held the record for most consecutive games with a touchdown pass (which he set between 1956–1960), until New Orleans Saints quarterback Drew Brees broke his long standing record on October 7, 2012. Unitas was the prototype of the modern era marquee quarterback with a strong passing game, media fanfare, and widespread popularity. He has been consistently listed as one of the greatest NFL players of all time.
Early life.
John Constantine Unitas was born to Francis J. Unitas and Lithuanian immigrant Helen Superfisky in Pittsburgh, Pennsylvania, in 1933, and grew up in the Mt. Washington neighborhood. His father died when Johnny was four years old of cardiovascular renal disease complicated by pneumonia, and he was raised by his mother, who worked two jobs to support the family. His unusual surname was a result of a phonetic transliteration of a common Lithuanian last name "Jonaitis". Attending St Justin's High School in Pittsburgh, Unitas played halfback and quarterback. After high school, Unitas looked for an opportunity to play college football.
The University of Louisville came through and Unitas left home for Kentucky.
College career.
In his younger years Johnny always dreamt about being part of the Fighting Irish of Notre Dame. But, when he tried out for them the coach just simpily said that he was just too skinny and he'd "get murdered" if he was put on the field.
In his four-year career as a Louisville Cardinal, Unitas completed 245 passes for 3,139 yards and 27 touchdowns. Reportedly, the 6-foot-1 Johnny Unitas weighed 145 pounds on his first day of practice at Louisville. Unitas' first start was in the fifth game of the 1951 season against St. Bonaventure. That game, the freshman threw 11 consecutive passes and three touchdowns to give the Cardinals a 21–19 lead. Though Louisville lost the game 22–21 on a disputed field goal, it had found a talented quarterback. Unitas completed 12 of 19 passes for 240 yards and four touchdowns in a 35–28 victory over Houston. The team finished the season 5–5 overall and 4–1 with Unitas as the starting quarterback. As a freshman, Unitas completed 46 of 99 passes for 602 yards and nine touchdowns (44).
By the 1952 season, the university decided to de-emphasize sports. The new president at Louisville, Dr. Philip Grant Davidson, reduced the amount of athletic aid, and tightened academic standards for athletes. As a result, 15 returning players could not meet the new standards and lost their scholarships. But Unitas maintained his scholarship by taking on a new elective: square dancing. In 1952 Coach Camp switched the team to two-way football. Unitas not only played safety or linebacker on defense and quarterback on offense but returned kicks and punts on special teams. The Cards won their first game against Wayne State, and then Florida State in the second game. Unitas completed 16 of 21 passes for 198 yards and three touchdowns. It was said that Unitas put on such a show at the Florida State game that he threw a pass under his legs for 15 yards. The rest of the season was a struggle for the Cards, who finished 3–5. Unitas completed 106 of 198 passes for 1,540 yards and 12 touchdowns in his sophomore year.
The team won their first game in 1953, against Murray State, and lost the rest for a record of 1–7. One of the most memorable games of the season came in a 59–6 loss against Tennessee. Unitas completed 9 of 19 passes for 73 yards, rushed 9 times for 52 yards, returned 6 kickoffs for eighty-five yards, 1 punt for three yards, and had 86 percent of the team's tackles. The only touchdown the team scored was in the fourth quarter when Unitas made a fake pitch to the running back and ran the ball 23 yards for a touchdown. Unitas was hurt later in the fourth quarter while trying to run the ball. On his way off the field received a standing ovation. When he got to the locker room he was so worn that his jersey and shoulder pads had to be cut off because he could not lift his arms. Louisville ended the season with 20–13 loss to Eastern Kentucky. In his junior year, Unitas completed 49 of 95 passes for 470 yards and three touchdowns.
Unitas was elected captain for the 1954 season, but due to an early injury did not see much playing time. His first start was the third game of the season, against Florida State. Of the 34-man team, 21 were freshmen. The 1954 Louisville Cardinals went 3–6, with the last win at home against Morehead State. Unitas was slowed by so many injuries his senior year his 527 passing yards ended second to Jim Houser's 560.
Professional career.
Pittsburgh Steelers.
After college, the Pittsburgh Steelers of the NFL drafted Unitas in the ninth round. However, Unitas was released before the season began as the odd man out among four quarterbacks trying to fill three spots. Steelers Head Coach Walt Kiesling had made up his mind about Unitas; he thought he was not smart enough to quarterback an NFL team, and Unitas was not given any snaps in practice with the Steelers. Among those edging out Unitas was Ted Marchibroda, future NFL quarterback and longtime NFL head coach. Out of pro football, Unitas—by this time married—worked in construction in Pittsburgh to support his family. On the weekends, he played quarterback, safety and punter on a local semi-professional team called the Bloomfield Rams for $6 a game.
Baltimore Colts.
In 1956, Unitas joined the Baltimore Colts of the NFL under legendary coach Weeb Ewbank, after being asked at the last minute to join Bloomfield Rams lineman Jim Deglau, a Croatian steel worker with a life much like Unitas', at the latter's scheduled Colts tryout. The pair borrowed money from friends to pay for the gas to make the trip. Deglau later told a reporter after Unitas' death, "[His] uncle told him not to come. [He] was worried that if he came down and the Colts passed on him, it would look bad (to other NFL teams)." The Colts signed Unitas, much to the chagrin of the Cleveland Browns, who had hoped to claim the rejected Steeler quarterback.
Unitas made his NFL debut with an inauspicious "mop-up" appearance against Detroit, going 0-2 with one interception. Two weeks later starting quarterback George Shaw suffered a broken leg against the Chicago Bears. In his first serious action, Unitas' initial pass was intercepted and returned for a touchdown. Then he botched a hand-off on his next play, a fumble recovered by the Bears. Unitas rebounded quickly from that 58–27 loss, leading the Colts to an upset of Green Bay and their first win over Cleveland. He threw nine touchdown passes that year, including one in the season finale that started his record 47-game streak. His 55.6-percent completion mark was a rookie record.
In 1957, his first season as the Colts full-time starter at quarterback, Unitas finished first in the NFL in passing yards (2,550) and touchdown passes (24) as he helped lead the Colts to a 7–5 record, the first winning record in franchise history. At season's end, Unitas was named the NFL's Most Valuable Player by the Newspaper Enterprise Association (NEA).
1958: "The Greatest Game Ever Played".
Unitas continued his prowess in 1958 passing for 2007 yards and 19 touchdowns as the Colts won the Western Conference title. The Colts won the NFL championship under Unitas' leadership on December 28, 1958, by defeating the New York Giants 23–17 in sudden death overtime. It was the first overtime game in NFL history, and is often referred to as the "greatest game ever played." The game, nationally televised by NBC, has been credited for sparking the rise in popularity of professional football during the 1960s.
First MVP in 1959.
In 1959, Unitas was named the NFL's MVP by the Associated Press (AP) and UPI (See: National Football League Most Valuable Player Award) for the first time, leading the NFL in passing yards (2,899), touchdown passes (32) and completions (193). Unitas then led the Colts to a repeat championship, beating the Giants again 31–16 in the title game.
The AP Award was considered the MVP award, adopting that verbiage in 1961. The Associated Press had always called the pre-1961 'Players of the Year"' their MVP until 2008, when it was revealed to them that they had made certain errors in their listings, namely that Jim Brown was the 1958 MVP/Player of the Year rather than Gino Marchetti and that Unitas was the true winner in 1959, not Y.A. Tittle. Rather than correct the winners, the AP "disavowed" that the pre-1961 winners were indeed "MVPs" claiming that it was a different award. Nonetheless, UPI also voted Unitas the top player award and there is little doubt that through sports history "MVP" and "Player of the Year" are terms that are interchangeable.
Beginning of the 1960s.
As the 1960s began, the Colts' fortunes (and win totals) declined. Injuries to key players such as Alan Ameche, Raymond Berry and Lenny Moore were a contributing factor. Unitas' streak of 47 straight games with at least one touchdown pass ended against the Los Angeles Rams in week 11 of the 1960 season. In spite of this, Unitas topped the 3000 yard passing mark for the first time and lead the league in touchdown passes for the fourth consecutive season.
After three middle-of-the-pack seasons, Colts owner Carroll Rosenbloom fired Weeb Ewbank and replaced him with Don Shula, who at the time was the youngest head coach in NFL history (33 years of age when he was hired). The Colts finished 8–6 in Shula's first season at the helm, good enough for only third place in the NFL's Western Conference but they did end the season on a strong note by winning their final three games. The season was very successful for Unitas personally as he led the NFL in passing yards with a career-best total of 3,481 and also led in completions with 237.
Second MVP in 1964.
The 1964 season would see the Colts return to the top of the Western Conference. After dropping their season opener to the Vikings, the Colts ran off 10 straight victories to finish with a 12–2 record. The season was one of Unitas' best as he finished with 2,824 yards passing, a league-best 9.26 yards per pass attempt, 19 touchdown passes and only 6 interceptions. He was named the NFL's Most Valuable Player by the AP and UPI for a second time. However, the season would end on a disappointing note for the Colts as they were upset by the Cleveland Browns in the 1964 NFL Championship Game, losing 27–0.
Unitas resumed his torrid passing in 1965, as he threw for 2,530 yards, 23 touchdowns and finished with a league-high and career best 97.1 passer rating. But he was lost for the balance of the season due to a knee injury in a week 12 loss to the Bears. More postseason heartbreak would follow in 1965. The Colts and Packers finished in a tie for first place in the Western Conference and a one-game playoff was played in Green Bay to decide who would be the conference representative in the 1965 NFL Championship Game. The Colts lost in overtime 13–10 due in large part to a game-tying field goal by Don Chandler that many say was incorrectly ruled good. Backup quarterback Gary Cuozzo also suffered a season-ending injury the following week and it would be running back Tom Matte who filled in as the emergency QB for the regular-season finale and the playoff loss to the Packers.
Unitas, healthy once more, threw for 2748 yards and 22 touchdowns in 1966 in a return to Pro Bowl form. However he posted a league-high 24 interceptions.
Third MVP in 1967.
After once again finishing 2nd in the Western Conference in 1966, the Colts rebounded to finish 11–1–2 in 1967 tying the Los Angeles Rams for the NFL's best record. In winning his third MVP awards from the AP and UPI in 1967 (and his second from the NEA), Unitas had a league-high 58.5 completion percentage and passed for 3,428 yards and 20 touchdowns. He openly complained about having tennis elbow and he threw eight interceptions and only three touchdown passes in the final five games. Once again the season ended in heartbreak for the Colts, as they were shut out of the newly instituted four team NFL playoff after losing the divisional tiebreaker to the Rams, a 34–10 rout in the regular season finale.
Super Bowls and final Colt years.
In the final game of the 1968 preseason, the muscles in Unitas' arm were torn when he was hit by a member of the Dallas Cowboys defense. Unitas wrote in his autobiography that he felt his arm was initially injured by the use of the "night ball" that the NFL was testing for better TV visibility during night games. In a post-game interview the previous year, he noted having constant pain in his elbow for several years prior. He would spend most of the season sitting on the bench. But the Colts still marched to a league-best 13–1 record behind backup quarterback and ultimate 1968 NFL MVP Earl Morrall. Although he was injured through most of the season, Unitas came off the bench to play in Super Bowl III, the famous game where Joe Namath guaranteed a New York Jets win despite conventional wisdom. Unitas' insertion was a desperation move in an attempt to retrieve dominance of the NFL over the upstart AFL. Although the Colts finally won an NFL Championship in 1968, they lost the Super Bowl to the AFL Champion, New York Jets. Unitas helped put together the Colts' only score, a touchdown late in the game. Despite not playing until late in the third quarter, Unitas still finished with more passing yards than the team's starter, Morrall.
After an off-season of rehabilitation on his elbow, Unitas rebounded in 1969, passing for 2342 yards and twelve touchdowns with 20 interceptions. But the Colts were a disappointing 8-5-1 and missed the playoffs.
In 1970, the NFL and AFL had merged into one league and the Colts moved to the new American Football Conference along with the Cleveland Browns and Pittsburgh Steelers. Unitas threw for 2213 yards and 14 touchdowns while leading the Colts to an 11-2-1 season. In their first rematch with the Jets, Unitas and Namath threw a combined nine interceptions in a 29-22 Colt win. Namath threw 62 passes and broke his hand on the final play, ending his season.
Unitas played well in the AFC playoffs, throwing for 390 yards, three touchdowns and no interceptions in victories over Cincinnati and Oakland. In Super Bowl V against the Dallas Cowboys he was knocked out of the game with a rib injury in the second quarter, after throwing a 75-yard touchdown pass (setting a then-Super Bowl record). However, he had also tossed two interceptions. Earl Morrall came in to lead the team to a last second 16-13 victory.
Final years.
In 1971 Unitas split playing time with Morrall, throwing only three touchdown passes. He started both playoff games, a win over Cleveland that sent the Colts to the AFC Championship game against the Miami Dolphins which they lost 21–0. Unitas threw three interceptions, one being returned for a touchdown.
1972 saw the Colts declining into mediocrity. After losing the season opener, Unitas was involved in the second and final regular season head-to-head meeting with "Broadway" Joe Namath. The first was in 1970 (won by the Colts, 29–22). The last meeting was a memorable one. On September 24, 1972, at Memorial Stadium, Unitas threw for 376 yards and three touchdowns, but Namath upstaged him again, bombing the Colts for 496 yards and six touchdowns in a 44–34 Jets victory – their first over Baltimore since the 1970 merger. After losing four of their first five games, the Colts fired Head Coach Don McCafferty and benched Unitas.
One of the more memorable moments in football history came on Unitas' last game in a Colt uniform at Memorial Stadium in a game against Buffalo. Unitas was not the starter for this game, but the Colts were blowing the Bills out 28–0; Unitas came on due to the fans chanting "We want Unitas!!!" and a plan devised by the head coach to convince Unitas that the starting quarterback was injured. Unitas came onto the field and proceeded to throw his last pass at home as a Colt, which was a short pass that wide receiver Eddie Hinton was able to turn into a long touchdown as Baltimore won 35–7.
San Diego, retirement and records.
Unitas was traded to the San Diego Chargers in 1973 after posting a 5-9 record in 1972 with Baltimore, but he was far past his prime. He replaced former Chargers quarterback John Hadl. He was replaced in Baltimore by Marty Domres acquired from the San Diego Chargers in August, 1972. Domres was ultimately replaced by LSU's Bert Jones, drafted with the number two pick. Unitas started the season with a 38-0 loss to the Washington Redskins. He threw for just 55 yards and 3 interceptions and was sacked 8 times. His final victory as a starter was against the Buffalo Bills in week two. Unitas was 10-18 for 175 yards, two touchdown passes and no interceptions in a 34-7 rout . After the victory against Buffalo, it looked like Unitas still had a chance to win for San Diego. Unitas was clearly not the same player he was years ago and many were questioning his role as a starter after a loss to Cincinnati in week three. Two weeks later he threw two first half interceptions, threw for 19 yards and went 2 for 9 against the Pittsburgh Steelers and was replaced by rookie quarterback, future Hall of Famer Dan Fouts. After posting a 1-3 record as a starter. Unitas retired in the preseason of 1974.
Unitas finished his 18 NFL seasons with 2,830 completions in 5,186 attempts for 40,239 yards and 290 touchdowns, with 253 interceptions. He also rushed for 1,777 yards and 13 touchdowns. Plagued by arm trouble in his later seasons, he threw more interceptions (64) than touchdowns (38) in 1968-1973. After averaging 215.8 yards per game in his first twelve seasons, his production fell to 124.4 in his final six. His Passer Rating plummeted from 82.9 to 60.4 for the same periods. Even so, Unitas set many passing records during his career. He was the first quarterback to throw for more than 40,000 yards, despite playing during an era when NFL teams played shorter seasons of 12 or 14 games (as opposed to today's 16-game seasons) and prior to modern passing-friendly rules implemented in 1978. His 32 touchdown passes in 1959 were a record at the time, making Unitas the first QB to hit the 30 touchdown mark in a season. His 47-game consecutive touchdown streak between 1956 and 1960 was a record considered by many to be unbreakable. The streak stood for 52 years before being broken by New Orleans Saints quarterback Drew Brees in a game against the San Diego Chargers on October 7, 2012.
Post-playing days.
After his playing days were finished, Unitas settled in Baltimore where he raised his family while also pursuing a career in broadcasting, doing color commentary for NFL games on CBS in the 1970s. He was elected to the Pro Football Hall of Fame in 1979. After Robert Irsay moved the Colts franchise to Indianapolis in 1984, a move reviled to this day in Baltimore as "Bob Irsay's Midnight Ride," Unitas was so outraged that he cut all ties to the relocated team (though his #19 jersey is still retired by the Colts). Other prominent old-time Colts followed his lead. He asked the Pro Football Hall of Fame on numerous occasions (including on Roy Firestone's "Up Close") to remove his display unless it was listed as belonging to the Baltimore Colts. The Hall of Fame has never complied with the request. Unitas donated his Colts memorabilia to the Babe Ruth Museum in Baltimore; they are now on display in the Sports Legends Museum at Camden Yards.
Johnny Unitas was inducted into the .
Unitas actively lobbied for another NFL team to come to Baltimore. After the NFL returned to Baltimore in 1996 as the Ravens, Unitas and most of the other old-time Colts regarded the Ravens as the true successors to the Baltimore Colts. Unitas was frequently seen on the Ravens' sidelines at home games (most prominently in 1998 when the now-Indianapolis Colts played the Ravens) and received a thunderous ovation every time he was pictured on each of the huge widescreens at M&T Bank Stadium. He was often seen on the 30-yard line on the Ravens side. When the NFL celebrated its first 50 years, Unitas was voted the league's best player. Retired Bears quarterback Sid Luckman said of Unitas, "He was better than me, better than Sammy Baugh, better than anyone."
Unitas lived most of the final years of his life severely hobbled. Due to an elbow injury suffered during his playing career, he was unable to use his right hand, and could not perform any physical activity more strenuous than golf due to his artificial knees.
Personal life.
At the age of 21, Unitas was married by his uncle to his high school sweetheart Dorothy Hoelle on November 20, 1954; they lived in Towson, Maryland and had five children before divorcing. Unitas' second wife was Sandra Lemon, whom he married on June 26, 1972; they had three children, lived in Baldwin, and remained married until Unitas' death on September 11, 2002.
Death.
On September 11, 2002, Unitas died suddenly of a heart attack while working out at the Kernan Physical Therapy Center (now The University of Maryland Rehabilitation & Orthopaedic Institute) in Baltimore, Maryland. After his death, many fans of the Baltimore Ravens petitioned the renaming of the Ravens' home stadium (owned by the State of Maryland) after Unitas. These requests were unsuccessful since the lucrative naming rights had already been leased by the Ravens to Buffalo, New York based M&T Bank. However, a statue of Unitas was erected as the centerpiece of the plaza in front of the Stadium named in Unitas' honor. Large banners depicting the NFL Hall of Famer in his Baltimore Colts heyday flank the entrance to the stadium. Towson University, where Unitas was a major fund-raiser and which his children attended, named its football and lacrosse complex Johnny Unitas Stadium in recognition of both his football career and service to the University.
Toward the end of his life, Unitas brought media attention to the many permanent physical disabilities that he and his fellow players suffered during their careers before heavy padding and other safety features became popular. Unitas himself lost almost total use of his right hand, with the middle finger and thumb noticeably disfigured from being repeatedly broken during games.
He is buried at Dulaney Valley Memorial Gardens in Timonium, Maryland.
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="15645" url="http://en.wikipedia.org/wiki?curid=15645" title="John Jacob Astor">
John Jacob Astor

John Jacob Astor (July 17, 1763 – March 29, 1848), born Johann Jakob Astor, was a German-born American businessman, merchant, fur trader, and investor who was the first prominent member of the Astor family and the first multi-millionaire in the United States. He was the creator of the first trust in the United States. He emigrated to England as a teenager and worked as a musical instrument manufacturer. 
Astor moved to the United States after the American Revolutionary War. He entered the fur trade and built a monopoly, managing a business empire that extended to the Great Lakes region and Canada, and later expanded into the American West and Pacific coast. Seeing the decline of demand, he got out of the fur trade in 1830, diversifying by investing in New York City real estate and later becoming a famed patron of the arts.
Early life and family.
John Jacob Astor was born in Walldorf, near Heidelberg in the old Palatinate. It became part of the Duchy of Baden in 1803 (now in Rhein-Neckar-Kreis in the state of Baden-Württemberg, Germany). He was the youngest son of butcher Johann Jacob Astor (July 7, 1724 – April 18, 1816) and Maria Magdalena Vorfelder (1730–1766). His three elder brothers were George (born Georg) (April 28, 1752 – December 1813), Henry (born Heinrich) (1754–1833), and Melchior (1759–1829). Astor's career began in Germany, where he worked as an assistant in his father's business, as a dairy salesman. In 1779, at age 16, he moved to London, where he anglicized his name and learned English while working for his eldest brother George, manufacturing musical instruments.
Emigration to the US.
Astor immigrated to New York City in the United States in March 1784, just after the end of the Revolutionary War. His second brother Henry had preceded him to New York, establishing a butcher shop with which Astor was initially involved. He began trading furs with Native Americans and in the late 1780s started a fur goods shop in New York City. He also became the New York agent of his brother's musical instrument business. 
Henry was also a horse racing enthusiast, and purchased a thoroughbred named Messenger (horse), who had been shipped from England to the United States in 1788. The horse became the founding sire of the Standardbred breed in the US; it became the primary breed used for trotters and pacers. 
Marriage and family.
On September 19, 1785, Astor married Sarah Cox Todd (1762–1834), the daughter of Scottish immigrants Adam Todd and Sarah Cox. Although she brought him a dowry of only $300, she possessed a frugal mind and a business judgment that he declared better than that of most merchants. She assisted him in the practical details of his business. 
They had eight children:
Fraternal organizations.
Astor was a Freemason, and served as Master of Holland Lodge #8, New York City in 1788. Later he served as Grand Treasurer for the Grand Lodge of New York.
Fortune from fur trade.
Astor took advantage of the Jay Treaty between England and the United States in 1794, which opened new markets in Canada and the Great Lakes region. In London, Astor at once made a contract with the North West Company, who from Montreal rivaled the trade interests of the Hudson's Bay Company, then based in London. 
Astor imported furs from Montreal to New York and shipped them to Europe. By 1800, he had amassed almost a quarter of a million dollars, and had become one of the leading figures in the fur trade. His agents worked throughout the western areas and were ruthless in competition. In 1800, following the example of the "Empress of China", the first American trading vessel to China, Astor traded furs, teas, and sandalwood with Canton in China, and greatly benefited from it.
The U.S. Embargo Act in 1807, however, disrupted Astor's import/export business because it closed off trade with Canada. With the permission of President Thomas Jefferson, Astor established the American Fur Company on April 6, 1808. He later formed subsidiaries: the Pacific Fur Company, and the Southwest Fur Company (in which Canadians had a part), in order to control fur trading in the Great Lakes areas and Columbia River region. His Columbia River trading post at Fort Astoria (established in April 1811) was the first United States community on the Pacific coast. He financed the overland Astor Expedition in 1810–12 to reach the outpost. Members of the expedition were to discover South Pass, through which hundreds of thousands of settlers on the Oregon, Mormon, and California trails passed through the Rocky Mountains.
Astor's fur trading ventures were disrupted during the War of 1812, when the English captured his trading posts. In 1816, he joined the opium-smuggling trade. His American Fur Company purchased ten tons of Turkish opium, then shipped the contraband item to Canton on the packet ship "Macedonian". Astor later left the China opium trade and sold solely to England.
Astor's business rebounded in 1817 after the U.S. Congress passed a protectionist law that barred foreign fur traders from U.S. territories. The American Fur Company came to dominate trading in the area around the Great Lakes. In 1822, Astor established the Astor House on Mackinac Island as headquarters for the reorganized American Fur Company, making the island a metropolis of the fur trade. A lengthy description based on documents, diaries, etc. was given by Washington Irving in his travelogue "Astoria". Astor's commercial connections extended over the entire globe, and his ships were found in every sea.
In 1804, Astor purchased from Vice President Aaron Burr Jr. what remained of a 99-year lease on property in Manhattan. At the time, Burr was serving under President Thomas Jefferson and desperately needed the purchase price of $62,500. The lease was to run until 1866. Astor began subdividing the land into nearly 250 lots and subleased them. His conditions were that the tenant could do whatever they wish with the lots for twenty-one years, after which they must renew the lease or Astor would take back the lot.
Real estate and retirement.
Astor began buying land in New York in 1799 and acquired sizable holdings along the waterfront. After the start of the 19th century, flush with China trade profits, he became more systematic, ambitious, and calculating by investing in New York real estate. In 1803, he bought a 70-acre farm that ran west of Broadway to the Hudson River between 42nd and 46th streets. That same year, and the following year, he bought considerable holdings from the disgraced Aaron Burr.
In the 1830s, Astor foresaw that the next big boom would be the build-up of New York, which would soon emerge as one of the world's greatest cities. Astor withdrew from the American Fur Company, as well as all his other ventures, and used the money to buy and develop large tracts of Manhattan real estate. Astor correctly predicted New York's rapid growth northward on Manhattan Island, and he purchased more and more land beyond the then-existing city limits. Astor rarely built on his land, and leased it to others for rent and their use. After retiring from his business, Astor spent the rest of his life as a patron of culture. He supported the ornithologist John James Audubon in his studies, art work and travels, and the presidential campaign of Henry Clay.
Last will and legacy.
At the time of his death in 1848, Astor was the wealthiest person in the United States, leaving an estate estimated to be worth at least $20 million. His estimated net worth, if calculated as a fraction of the U.S. gross domestic product at the time, would have been equivalent to $110.1 billion in 2006 U.S. dollars, making him the fifth-richest person in American history. An estimate based on inflation from the legally-set American gold standard rate of $21 per ounce in the 1850s would result in a much more conservative net worth of $1.272 billion in 2011 dollars.
In his will, Astor bequeathed $400,000 to build the Astor Library for the New York public (later consolidated with other libraries to form New York Public Library), and $50,000 for a poorhouse and orphanage in his German hometown of Walldorf. The "Astorhaus" is now operated as a museum honoring Astor and serves as a renowned fest hall for marriages. Astor donated $25,000 to the German Society of the City of New York, whose chairman he was from 1837 until 1841. Also, he bequeathed $30,000 for a professor's chair in German literature at Columbia University, but due to differences he had with the deanship, he erased this donation from the testament.
Astor left the bulk of his fortune to his second son William, because his eldest son, John Jr., was sickly and mentally unstable. Astor left enough money to care for John Jr. for the rest of his life. Astor is buried in Trinity Church Cemetery in Manhattan, New York. Many members of his family had joined its congregation but Astor remained a member of the local German Reformed congregation to his death. Herman Melville used Astor as a symbol of men who made the earliest fortunes in New York in his novella, "Bartleby, the Scrivener".
The pair of marble lions that sit by the entrance of the New York Public Library at Fifth Avenue and 42nd Street were originally named Leo Astor and Leo Lenox, after Astor and James Lenox, who founded the library from his own collection. Next, they were called Lord Astor and Lady Lenox (both lions are males). Mayor Fiorello La Guardia renamed them "Patience" and "Fortitude" during the Great Depression. 
In 1908, when the association football club FC Astoria Walldorf was formed in Astor's birthplace in Germany, the group added "Astoria" to its name in his, and the family's, honor.

</doc>
<doc id="15651" url="http://en.wikipedia.org/wiki?curid=15651" title="Julian calendar">
Julian calendar

The Julian calendar, introduced by Julius Caesar in 46 BC (708 AUC), was a reform of the Roman calendar. It took effect in 45 BC (709 AUC), shortly after the Roman conquest of Egypt. It was the predominant calendar in the Roman world, most of Europe, and in European settlements in the Americas and elsewhere, until it was refined and gradually replaced by the Gregorian calendar, promulgated in 1582 by Pope Gregory XIII. The difference in the average length of the year between Julian (365.25 days) and Gregorian (365.2425 days) is 0.002%.
The Julian calendar has a regular year of 365 days divided into 12 months, as listed in Table of months. A leap day is added to February every four years. The Julian year is, therefore, on average 365.25 days long. It was intended to approximate the tropical (solar) year. Although Greek astronomers had known, at least since Hipparchus, a century before the Julian reform, that the tropical year was a few minutes shorter than 365.25 days, the calendar did not compensate for this difference. As a result, the calendar year gained about three days every four centuries compared to observed equinox times and the seasons. This discrepancy was corrected by the Gregorian reform of 1582. The Gregorian calendar has the same months and month lengths as the Julian calendar, but, in the Gregorian calendar, years evenly divisible by 100 are not leap years, except that years evenly divisible by 400 remain leap years. Consequently, the Julian calendar is currently 13 days behind the Gregorian calendar; for instance, 1 January in the Julian calendar is 14 January in the Gregorian. Old Style (O.S.) and New Style (N.S.) are sometimes used with dates to indicate either whether the start of the Julian year has been adjusted to start on 1 January (N.S.) even though documents written at the time use a different start of year (O.S.), or whether a date conforms to the Julian calendar (O.S.) rather than the Gregorian (N.S.). Dual dating uses two consecutive years because of differences in the starting date of the year, or includes both the Julian and Gregorian dates.
The Julian calendar has been replaced as the civil calendar by the Gregorian calendar in all countries which formerly used it, although it continued to be the civil calendar of some countries into the 20th century. Among the last countries to convert to the Gregorian calendar were Russia (in 1918) and Greece (in 1923). As of 1930, all countries that were using the Julian calendar had discontinued it. Most Christian denominations in the West and areas evangelized by Western churches have also replaced the Julian calendar with the Gregorian as the basis for their liturgical calendars. However, most branches of the Eastern Orthodox Church still use the Julian calendar for calculating the dates of moveable feasts, including Easter (Pascha). Some Orthodox churches have adopted the Revised Julian calendar for the observance of fixed feasts, while other Orthodox churches retain the Julian calendar for all purposes. The Julian calendar is still used by the Berber people of North Africa, and on Mount Athos. In the form of the Alexandrian calendar, it is the basis for the Ethiopian calendar, which is the civil calendar of Ethiopia.
Motivation.
The ordinary year in the previous Roman calendar consisted of 12 months, for a total of 355 days. In addition, a 27 or 28 - day intercalary month, the Mensis Intercalaris, was sometimes inserted between February and March. This intercalary month was formed by inserting 22 or 23 days after the first 23 days of February; the last five days of February, which counted down toward the start of March, became the last five days of Intercalaris. The net effect was to add 22 or 23 days to the year, forming an intercalary year of 377 or 378 days. Some say the "mensis intercalaris" always had 27 days and began on either the first or the second day after the Terminalia (23 February).
According to the later writers Censorinus and Macrobius, the ideal intercalary cycle consisted of ordinary years of 355 days alternating with intercalary years, alternately 377 and 378 days long. In this system, the average Roman year would have had 366¼ days over four years, giving it an average drift of one day per year relative to any solstice or equinox. Macrobius describes a further refinement whereby, in one 8-year period within a 24 year cycle, there were only three intercalary years, each of 377 days (thus 11 intercalary years out of 24). This refinement averages the length of the year to 365.25 days over 24 years.
In practice, intercalations did not occur systematically according to any of these ideal systems, but were determined by the Pontifices. So far as can be determined from the historical evidence, they were much less regular than these ideal schemes suggest. They usually occurred every second or third year, but were sometimes omitted for much longer, and occasionally occurred in two consecutive years.
If managed correctly this system could have allowed the Roman year to stay roughly aligned to a tropical year. However, since the Pontifices were often politicians, and because a Roman magistrate's term of office corresponded with a calendar year, this power was prone to abuse: a Pontifex could lengthen a year in which he or one of his political allies was in office, or refuse to lengthen one in which his opponents were in power.
If too many intercalations were omitted, as happened after the Second Punic War and during the Civil Wars, the calendar would drift out of alignment with the tropical year. Moreover, because intercalations were often determined quite late, the average Roman citizen often did not know the date, particularly if s/he was some distance from the city. For these reasons, the last years of the pre-Julian calendar were later known as "years of confusion". The problems became particularly acute during the years of Julius Caesar's pontificate before the reform, 63–46 BC, when there were only five intercalary months (instead of eight), none of which were during the five Roman years before 46 BC.
Caesar's reform was intended to solve this problem permanently, by creating a calendar that remained aligned to the sun without any human intervention. This proved useful very soon after the new calendar came into effect. Varro used it in 37 BC to fix calendar dates for the start of the four seasons, which would have been impossible only 8 years earlier. A century later, when Pliny dated the winter solstice to 25 December because the sun entered the 8th degree of Capricorn on that date, this stability had become an ordinary fact of life.
Context of the reform.
The approximation of 365¼ days for the tropical year had been known for a long time but was not used directly since ancient calendars were not solar although the Egyptian calendar and Iranian calendar did come very close with unadjusted years of 365 days. The octaeteris, a cycle of 8 lunar years popularized by Cleostratus (and also commonly attributed to Eudoxus) which was used in some early Greek calendars, notably in Athens, is 1.53 days longer than eight Julian years. The length of nineteen years in the cycle of Meton was 6,940 days, six hours longer than the mean Julian year. The mean Julian year was the basis of the 76 years cycle devised by Callippus (a student under Eudoxus) to improve the Metonic cycle.
In Persia (Iran) after the reform in the Persian calendar by introduction of the Persian Zoroastrian (i. e. Young Avestan) calendar in 503 BC and afterwards, the first day of the year (1 Farvardin=Nowruz) slipped against the vernal equinox at the rate of approximately one day every four years.
Likewise in Egypt, a fixed year of 365 days was in use, drifting by one day against the sun in four years. An unsuccessful attempt to add an extra day every fourth year was made in 238 BC (Decree of Canopus). Caesar probably experienced this "wandering" or "vague" calendar in that country. He landed in the Nile delta in October 48 BC and soon became embroiled in the Ptolemaic dynastic war, especially after Cleopatra managed to be "introduced" to him in Alexandria.
Caesar imposed a peace, and a banquet was held to celebrate the event. Lucan depicted Caesar talking to a wise man called Acoreus during the feast, stating his intention to create a calendar more perfect than that of Eudoxus (Eudoxus was popularly credited with having determined the length of the year to be 365¼ days). But the war soon resumed and Caesar was attacked by the Egyptian army for several months until he achieved victory. He then enjoyed a long cruise on the Nile with Cleopatra before leaving the country in June 47 BC.
Caesar returned to Rome in 46 BC and, according to Plutarch, called in the best philosophers and mathematicians of his time to solve the problem of the calendar. Pliny says that Caesar was aided in his reform by the astronomer Sosigenes of Alexandria who is generally considered the principal designer of the reform. Sosigenes may also have been the author of the astronomical almanac published by Caesar to facilitate the reform. Eventually, it was decided to establish a calendar that would be a combination between the old Roman months, the fixed length of the Egyptian calendar, and the 365¼ days of the Greek astronomy. According to Macrobius, Caesar was assisted in this by a certain Marcus Flavius.
Julian reform.
Realignment of the year.
The first step of the reform was to realign the start of the calendar year (1 January) to the tropical year by making 46 BC (708 AUC) 445 days long, compensating for the intercalations which had been missed during Caesar's pontificate. This year had already been extended from 355 to 378 days by the insertion of a regular intercalary month in February. When Caesar decreed the reform, probably shortly after his return from the African campaign in late Quintilis (July), he added 67 more days by inserting two extraordinary intercalary months between November and December.
These months are called "Intercalaris Prior" and "Intercalaris Posterior" in letters of Cicero written at the time; there is no basis for the statement sometimes seen that they were called "Undecimber" and "Duodecimber". Their individual lengths are unknown, as is the position of the Nones and Ides within them.
Because 46 BC was the last of a series of irregular years, this extra-long year was, and is, referred to as the "last year of confusion". The new calendar began operation after the realignment had been completed, in 45 BC.
Changes to the months.
The Julian months were formed by adding ten days to a regular pre-Julian Roman year of 355 days, creating a regular Julian year of 365 days. Two extra days were added to January, Sextilis (August) and December, and one extra day was added to April, June, September and November. February was not changed in ordinary years, and so continued to be the traditional 28 days. Thus, the ordinary (i.e., non leap year) lengths of all of the months were set by the Julian calendar to the same values they still hold today. (See Sacrobosco's theory on month lengths below for stories purporting otherwise).
The Julian reform did not change the method used to account days of the month in the pre-Julian calendar, based on the Kalends, Nones and Ides, nor did it change the positions of these three dates within the months. Macrobius states that the extra days were added immediately before the last day of each month to avoid disturbing the position of the established religious ceremonies relative to the Nones and Ides of the month. However, since Roman dates after the Ides of the month counted down toward the start of the next month, the extra days had the effect of raising the initial value of the count of the day following the Ides in the lengthened months. Thus, in January, Sextilis and December the 14th day of the month became a.d. XIX Kal. instead of a.d. XVII Kal., while in April, June, September and November it became a.d. XVIII Kal.
Romans of the time born after the Ides of a month responded differently to the effect of this change on their birthdays. Mark Antony kept his birthday on 14 January, which changed its date from a.d. XVII Kal. Feb. to a.d. XIX Kal. Feb., a date that had previously not existed. Livia kept the date of her birthday unchanged at a.d. III Kal. Feb., which moved it from 28 to 30 January, a day that had previously not existed. Augustus kept his on 23 September, but both the old date (a.d. VIII Kal. Oct.) and the new (a.d. IX Kal. Oct.) were celebrated in some places.
The inserted days were all initially characterised as "dies fasti" (F -- see Roman calendar). The character of a few festival days was changed. In the early Julio-Claudian period a large number of festivals were decreed to celebrate events of dynastic importance, which caused the character of the associated dates to be changed to NP. However, this practice was discontinued around the reign of Claudius, and the practice of characterising days fell into disuse around the end of the first century AD: the Antonine jurist Gaius speaks of "dies nefasti" as a thing of the past.
Intercalation.
The old intercalary month was abolished. The new leap day was dated as "ante diem bis sextum Kalendas Martias" ('the sixth doubled before the Kalends of March'), usually abbreviated as "a.d. bis VI Kal. Mart."; hence it is called in English the bissextile day. The year in which it occurred was termed "annus bissextus", in English the bissextile year.
There is debate about the exact position of the bissextile day in the early Julian calendar. The earliest direct evidence is a statement of the 2nd century jurist Celsus, who states that there were two halves of a 48-hour day, and that the intercalated day was the "posterior" half. An inscription from AD 168 states that "a.d. V Kal. Mart." was the day after the bissextile day. The 19th century chronologist Ideler argued that Celsus used the term "posterior" in a technical fashion to refer to the earlier of the two days, which requires the inscription to refer to the whole 48-hour day as the bissextile. Some later historians share this view. Others, following Mommsen, take the view that Celsus was using the ordinary Latin (and English) meaning of "posterior". A third view is that neither half of the 48-hour "bis sextum" was originally formally designated as intercalated, but that the need to do so arose as the concept of a 48-hour day became obsolete.
There is no doubt that the bissextile day eventually became the earlier of the two days for most purposes. In 238 Censorinus stated that it was inserted after the Terminalia (23 February) and was followed by the last five days of February, i.e. a.d. VI, V, IV, III and prid. Kal. Mart. (which would be 24 to 28 February in a common year and the 25th to 29th in a leap year). Hence he regarded the bissextum as the first half of the doubled day. All later writers, including Macrobius about 430, Bede in 725, and other medieval computists (calculators of Easter) followed this rule, as does the liturgical calendar of the Roman Catholic Church. However, Celsus' definition continued to be used for legal purposes. It was incorporated into Justinian's Digest, and in the English statute of 1236, which was not formally repealed until 1879.
The effect of the bissextile day on the nundinal cycle is not discussed in the sources. According to Dio Cassius, a leap day was inserted in 41 BC to ensure that the first market day of 40 BC did not fall on 1 January, which implies that the old 8-day cycle was not immediately affected by the Julian reform. However, he also reports that in AD 44, and on some previous occasions, the market day was changed to avoid a conflict with a religious festival. This may indicate that a single nundinal letter was assigned to both halves of the 48-hour bissextile day by this time, so that the Regifugium and the market day might fall on the same date but on different days. In any case, the 8-day nundinal cycle began to be displaced by the 7-day week in the first century AD, and dominical letters began to appear alongside nundinal letters in the fasti.
During the late Middle Ages days in the month came to be numbered in consecutive day order. Consequently, the leap day was considered to be the last day in February in leap years, i.e. 29 February, which is its current position.
Sacrobosco's theory on month lengths.
The Julian reform set the lengths of the months to their modern values. However, a 13th-century scholar, Sacrobosco, proposed a different explanation for the lengths of Julian months which is still widely repeated but is certainly wrong.
According to Sacrobosco, the month lengths for ordinary years in the Roman Republican calendar, from January to December, were:
Sacrobosco then thought that Julius Caesar added one day to every month except February, a total of 11 more days to regular months, giving the ordinary Julian year of 365 days. A single leap day could now be added to this extra short February:
He then said Augustus changed this, by taking one day from February to add it to Sextilis, and then modifying the alternation of the following months, to:
so that the length of "Augustus" (August) would not be shorter than (and therefore inferior to) the length of "Iulius" (July), giving us the irregular month lengths which are still in use.
There is abundant evidence disproving this theory. First, a wall painting of a Roman calendar predating the Julian reform has survived, which confirms the literary accounts that the months were already irregular before Julius Caesar reformed them, with an ordinary year of 355 days, not 354, with month lengths arranged as:
Also, the Julian reform did not change the dates of the Nones and Ides. In particular, the Ides were late (on the 15th rather than 13th) in March, May, July and October, showing that these months always had 31 days in the Roman calendar, whereas Sacrobosco's theory requires that March, May and July were originally 30 days long and that the length of October was changed from 29 to 30 days by Caesar and to 31 days by Augustus. Further, Sacrobosco's theory is explicitly contradicted by the 3rd and 5th century authors Censorinus and Macrobius, and it is inconsistent with seasonal lengths given by Varro, writing in 37 BC, before Sextilis was renamed for Augustus in 8 BC, with the 31-day Sextilis given by an Egyptian papyrus from 24 BC, and with the 28-day February shown in the "Fasti Caeretani", which is dated before 12 BC.
Adoption of the Julian calendar.
Caesar's reform only applied to the Roman calendar. However, in the following decades many of the local civic and provincial calendars of the empire and neighbouring client kingdoms were aligned to the Julian calendar by transforming them into calendars with years of 365 days with an extra day intercalated every four years. The reformed calendars typically retained many features of the unreformed calendars. In many cases, the New Year was not on 1 January, the leap day was not on the bissextile day, the old month names were retained, the lengths of the reformed months did not match the lengths of Julian months, and, even if they did, their first days did not match the first day of the corresponding Julian month. Nevertheless, since the reformed calendars had fixed relationships to each other and to the Julian calendar, the process of converting dates between them became quite straightforward, through the use of conversion tables known as "hemerologia". Several of the reformed calendars are only known through surviving hemerologia.
The three most important of these calendars are the Alexandrian calendar, the Asian calendar and the Syro-Macedonian calendar. Other reformed calendars are known from Cappadocia, Cyprus and the cities of Syria and Palestine. Most reformed calendars were adopted under Augustus, though the calendar of Nabatea was reformed after the kingdom became the Roman province of Arabia in AD 106. There is no evidence that local calendars were aligned to the Julian calendar in the Western empire. Unreformed calendars continued to be used in Gaul, Greece, Macedon, the Balkans and parts of Palestine, most notably in Judea.
The Alexandrian calendar adapted the Egyptian calendar by adding a 6th epagomenal day as the last day of the year in every fourth year, falling on 29 August preceding a bissextile day. It was otherwise identical to the Egyptian calendar. The first leap day was in 22 BC, and they occurred every four years from the beginning, even though Roman leap days occurred every three years at this time (see Leap year error). This calendar influenced the structure of several other reformed calendars, such as those of the cities of Gaza and Ascalon in Palestine, Salamis in Cyprus, and the province of Arabia. It was adopted by the Coptic church and remains in use both as the liturgical calendar of the Coptic church and as the civil calendar of Ethiopia.
The Asian calendar was an adaptation of the Macedonian calendar used in the province of Asia and, with minor variations, in nearby cities and provinces. It is known in detail through the survival of decrees promulgating it issued in 8BC by the proconsul Paullus Fabius Maximus. It renamed the first month Dios as "Kaisar", and arranged the months such that each month started on the ninth day before the kalends of the corresponding Roman month; thus the year began on 23 September, Augustus' birthday. Since Greek months typically had 29 or 30 days, the 31st day of 31 day months was named "Sebaste"—the emperor's day—and was the first day of these months. The leap day was a second Sebaste day in the month of Xandikos, i.e. 24 February. This calendar remained in use at least until the middle of the fifth century AD.
The SyroMacedonian calendar was an adaptation of the Macedonian calendar used in Antioch and other parts of Syria. The months were exactly aligned to the Julian calendar, but they retained their Macedonian names and the year began in Dios = November until the fifth century, when the start of the year was moved to Gorpiaios = September.
These reformed calendars generally remained in use until the fifth or sixth century. Around that time most of them were replaced as civil calendars by the Julian calendar, but with a year starting in September to reflect the year of the indiction cycle.
The Julian calendar spread beyond the borders of the Roman Empire through its use as the Christian liturgical calendar. When a people or a country was converted to Christianity, they generally also adopted the Christian calendar of the church responsible for conversion. Thus, Christian Nubia and Ethiopia adopted the Alexandrian calendar, while Christian Europe adopted the Julian calendar, in either the Catholic or Orthodox variant. Starting in the 16th century, European settlements in the Americas and elsewhere likewise inherited the Julian calendar of the mother country, until they adopted the Gregorian reform. The last country to adopt the Julian calendar was the Ottoman Empire, which had used it for financial purposes for some time, but adopted it officially as the civil calendar (the "Rumi calendar") in 1840.
Leap year error.
Although the new calendar was much simpler than the pre-Julian calendar, the pontifices initially added a leap day every three years, instead of every four. There are accounts of this in Solinus, Pliny, Ammianus, Suetonius, and Censorinus.
Macrobius gives the following account of the introduction of the Julian calendar:
"Caesar’s regulation of the civil year to accord with his revised measurement was proclaimed publicly by edict, and the arrangement might have continued to stand had not the correction itself of the calendar led the priests to introduce a new error of their own; for they proceeded to insert the intercalary day, which represented the four quarter-days, at the beginning of each fourth year instead of at its end, although the intercalation ought to have been made at the end of each fourth year and before the beginning of the fifth.
"This error continued for thirty-six years by which time twelve intercalary days had been inserted instead of the number actually due, namely nine. But when this error was at length recognised, it too was corrected, by an order of Augustus, that twelve years should be allowed to pass without an intercalary day, since the sequence of twelve such years would account for the three days which, in the course of thirty-six years, had been introduced by the premature actions of the priests."
So, according to Macrobius,
(1) the year was considered to begin after the Terminalia (23 February) 
(2) the calendar was operated correctly from its introduction on 1 January 45BC until the beginning of the fourth year (February 42BC) at which point the priests inserted the first intercalation
(4) the priests made a further eleven intercalations after 42BC at three-year intervals so that the twelfth intercalation fell in 9BC
(5) had Caesar’s intention been followed there would have been intercalations every four years after 41BC, so that the ninth intercalation would have been in 9BC
(6) after 9BC, there were twelve years without leap years, so that the leap days Caesar would have had in 5BC, 1BC and AD4 were omitted
(7) after AD4 the calendar was operated as Caesar intended, so that the next leap year was AD8 and then leap years followed every fourth year thereafter.
Some people have had different ideas as to how the leap years went. The above scheme is that of Scaliger in the table below. He established that the Augustan reform was instituted in 8 BC. The table shows for each reconstruction the implied proleptic Julian date for the first day of Caesar's reformed calendar (Kal. Ian. AUC 709) and the first Julian date on which the Roman calendar date matches the Julian calendar after the completion of Augustus' reform.
Alexander Jones claims that the correct Julian calendar was in use in Egypt in 24 BC, implying that the first day of the reform in both Egypt and Rome, 1 January 45 BC, was the Julian date 1 January if 45 BC was a leap year and 2 January if it was not. This necessitates fourteen leap days up to and including AD 8 if 45 BC was a leap year and thirteen if it was not.
Pierre Brind'Amour argued that "only one day was intercalated between 1/1/45 and 1/1/40 (disregarding a momentary 'fiddling' in December of 41 to avoid the nundinum falling on Kal. Ian."
In 1999 a papyrus was discovered which gives the dates of astronomical phenomena in 24 BC in both the Egyptian and Roman calendars. From 30 August 26 BC (Julian) Egypt had two calendars: the old Egyptian in which every year had 365 days and the new Alexandrian in which every fourth year had 366 days. Up to 28 August 22 BC (Julian) the date in both calendars was the same. The dates in the Alexandrian and Julian calendars are in one-to-one correspondence except for the period from 29 August in the year preceding a Julian leap year to the following 24 February. From a comparison of the astronomical data with the Egyptian and Roman dates Alexander Jones concluded that the Egyptian astronomers (as opposed to travellers from Rome) used the correct Julian calendar.
An inscription has been discovered which orders a new calendar to be used in Asia to replace the previous Greek lunar calendar. According to one translation
"Intercalation shall commence on the day after 14 Peritius [a.d. IX Kal. Feb., which would have been 15 Peritius] as it is currently constituted in the third year following promulgation of the decree. Xanthicus shall have 32 days in this intercalary year."
This is historically correct. It was decreed by the proconsul that the first day of the year in the new calendar shall be Augustus' birthday, a.d. IX Kal. Oct. Every month begins on the ninth day before the kalends. The date of introduction, the day after 14 Peritius, was 1 Dystrus, the next month. The month after that was Xanthicus. Thus Xanthicus began on a.d. IX Kal. Mart., and normally contained 31 days. In leap year, however, it contained an extra "Sebaste day", the Roman leap day, and thus had 32 days. From the lunar nature of the old calendar we can fix the starting date of the new one as 24 January, a.d. IX Kal. Feb. 5 BC in the Julian calendar, which was a leap year. Thus from inception the dates of the reformed Asian calendar are in one-to-one correspondence with the Julian.
Another translation of this inscription is
"Intercalation shall commence on the day after the fourteenth day in the current month of Peritius [a.d. IX Kal. Feb.], occurring every third year. Xanthicus shall have 32 days in this intercalary year."
This would move the starting date back three years to 8 BC, and from the lunar synchronism back to 26 January (Julian). But since the corresponding Roman date in the inscription is 24 January, this must be according to the incorrect calendar which in 8 BC Augustus had ordered to be corrected by the omission of leap days. As the authors of the previous paper point out, with the correct four-year cycle being used in Egypt and the three-year cycle abolished in Rome it is unlikely that Augustus would have ordered the three-year cycle to be introduced in Asia.
Month names.
The Julian reform did not immediately cause the names of any months to be changed. The old intercalary month was abolished and replaced with a single intercalary day at the same point (i.e. five days before the end of February). January continued to be the first month of the year.
The Romans later renamed months after Julius Caesar and Augustus, renaming Quintilis as "Iulius" (July) in 44 BC and Sextilis as "Augustus" (August) in 8 BC. Quintilis was renamed to honour Caesar because it was the month of his birth. According to a "senatus consultum" quoted by Macrobius, Sextilis was renamed to honour Augustus because several of the most significant events in his rise to power, culminating in the fall of Alexandria, occurred in that month.
Other months were renamed by other emperors, but apparently none of the later changes survived their deaths. In AD 37, Caligula renamed September as "Germanicus" after his father; in AD 65, Nero renamed April as "Neroneus", May as "Claudius" and June as "Germanicus"; and in AD 84 Domitian renamed September as "Germanicus" and October as "Domitianus". Commodus was unique in renaming all twelve months after his own adopted names (January to December): "Amazonius", "Invictus", "Felix", "Pius", "Lucius", "Aelius", "Aurelius", "Commodus", "Augustus", "Herculeus", "Romanus", and "Exsuperatorius". The emperor Tacitus is said to have ordered that September, the month of his birth and accession, be renamed after him, but the story is doubtful since he did not become emperor before November 275. Similar honorific month names were implemented in many of the provincial calendars that were aligned to the Julian calendar.
Other name changes were proposed but were never implemented. Tiberius rejected a senatorial proposal to rename September as "Tiberius" and October as "Livius", after his mother Livia. Antoninus Pius rejected a senatorial decree renaming September as "Antoninus" and November as "Faustina", after his empress.
Much more lasting than the ephemeral month names of the post-Augustan Roman emperors were the Old High German names introduced by Charlemagne. According to his biographer, Charlemagne renamed all of the months agriculturally into German. These names were used until the 15th century, over 700 years after his rule, and continued, with some modifications, to see some use as "traditional" month names until the late 18th century. The names (January to December) were: "Wintarmanoth" ("winter month"), "Hornung", "Lentzinmanoth" ("spring month", "Lent month"), "Ostarmanoth" ("Easter month"), "Wonnemanoth" ("joy-month"), "Brachmanoth" ("fallow-month"), "Heuvimanoth" ("hay month"), "Aranmanoth" ("reaping month"), "Witumanoth" ("wood month"), "Windumemanoth" ("vintage month"), "Herbistmanoth" ("harvest month"), and "Heilagmanoth" ("holy month").
The calendar month names used in western and northern Europe, in Byzantium, and by the Berbers, were derived from the Latin names. However, in eastern Europe older seasonal month names continued to be used into the 19th century, and in some cases are still in use, in many languages, including: Belarusian, Bulgarian, Croatian, Czech, , Georgian, Lithuanian, Macedonian, Polish, Romanian, Slovene, Ukrainian. When the Ottoman Empire adopted the Julian calendar, in the form of the Rumi calendar, the month names reflected Ottoman tradition.
Year numbering.
The principal method that the Romans used to identify a year for dating purposes was to name it after the two consuls who took office in it, so this eponymous or named year was called the consular year. Since 153 BC, they had taken office on 1 January, the start of the calendar year. The calendar year is the order that the months were listed in calendars or "fasti" displayed on painted walls or on stone tablets, and has been January to December since about 450 BC according to Ovid or since about 713 BC according to Macrobius and Plutarch (see Roman calendar). Julius Caesar did not change the beginning of either the consular year or the calendar year. In addition to consular years, the Romans sometimes used the regnal year of the emperor, and by the late 4th century documents were also being dated according to the 15-year cycle of the indiction. In 537, Justinian required that henceforth the date must include the name of the emperor and his regnal year, in addition to the indiction and the consul, while also allowing the use of local eras.
In 309 and 310, and from time to time thereafter, no consuls were appointed. When this happened, the consular date was given a count of years since the last consul (so-called "post-consular" dating). After 541, only the reigning emperor held the consulate, typically for only one year in his reign, and so post-consular dating became the norm. Similar post-consular dates were also known in the West in the early 6th century. The system of consular dating, long obsolete, was formally abolished in the law code of Leo VI, issued in 888.
Only rarely did the Romans number the year from the founding of the city (of Rome), "ab urbe condita" (AUC). This method was used by Roman historians to determine the number of years from one event to another, not to date a year. Different historians had several different dates for the founding. The "Fasti Capitolini", an inscription containing an official list of the consuls which was published by Augustus, used an epoch of 752 BC. The epoch used by Varro, 753 BC, has been adopted by modern historians. Indeed, Renaissance editors often added it to the manuscripts that they published, giving the false impression that the Romans numbered their years. Most modern historians tacitly assume that it began on the day the consuls took office, and ancient documents such as the "Fasti Capitolini" which use other AUC systems do so in the same way. However, Censorinus, writing in the 3rd century AD, states that, in his time, the AUC year began with the Parilia, celebrated on 21 April, which was regarded as the actual anniversary of the foundation of Rome.
Many local eras, such as the Era of Actium and the Spanish Era, were adopted for the Julian calendar or its local equivalent in the provinces and cities of the Roman Empire. Some of these were used for a considerable time. Perhaps the best known is the Era of Martyrs, sometimes also called "Anno Diocletiani" (after Diocletian), which was associated with the Alexandrian calendar and often used by the Alexandrian Christians to number their Easters during the 4th and 5th centuries, and continues to be used by the Coptic and Ethiopian churches.
In the Eastern Mediterranean, the efforts of Christian chronographers such as Annianus of Alexandria to date the Biblical creation of the world led to the introduction of Anno Mundi eras based on this event. The most important of these was the Etos Kosmou, used throughout the Byzantine world from the 10th century and in Russia until 1700. In the West, the kingdoms succeeding the empire initially used indictions and regnal years, alone or in combination. The chronicler Prosper of Aquitaine, in the fifth century, used an era dated from the Passion of Christ, but this era was not widely adopted. Dionysius Exiguus proposed the system of Anno Domini in 525. This era gradually spread through the western Christian world, once the system was adopted by Bede.
The Julian calendar was also used in some Muslim countries. The Rumi calendar, the Julian calendar used in the later years of the Ottoman Empire, adopted an era derived from the lunar AH year equivalent to AD 1840, i.e. the effective Rumi epoch was AD 585. In recent years, some users of the Berber calendar have adopted an era starting in 950 BC, the approximate date that the Libyan pharaoh Sheshonq I came to power in Egypt.
New Year's Day.
The Roman calendar began the year on 1 January, and this remained the start of the year after the Julian reform. However, even after local calendars were aligned to the Julian calendar, they started the new year on different dates. The Alexandrian calendar in Egypt started on 29 August (30 August after an Alexandrian leap year). Several local provincial calendars were aligned to start on the birthday of Augustus, 23 September. The indiction caused the Byzantine year, which used the Julian calendar, to begin on :1 September; this date is still used in the Eastern Orthodox Church for the beginning of the liturgical year. When the Julian calendar was adopted in AD 988 by Vladimir I of Kiev, the year was numbered Anno Mundi 6496, beginning on 1 March, six months after the start of the Byzantine Anno Mundi year with the same number. In 1492 (AM 7000), Ivan III, according to church tradition, realigned the start of the year to 1 September, so that AM 7000 only lasted for six months in Russia, from 1 March to 31 August 1492.
During the Middle Ages 1 January retained the name "New Year's Day" (or an equivalent name) in all Western European countries (affiliated with the Roman Catholic Church), since the medieval calendar continued to display the months from January to December (in twelve columns containing 28 to 31 days each), just as the Romans had. However, most of those countries began their numbered year on 25 December (the Nativity of Jesus), 25 March (the Incarnation of Jesus), or even Easter, as in France (see the Liturgical year article for more details).
In Anglo-Saxon England, the year most commonly began on 25 December, which, as the winter solstice, had marked the start of the year in pagan times, though 25 March is occasionally documented in the 11th century. Sometimes the start of the year was reckoned as 24 September, the start of the so-called "western indiction" introduced by Bede. These practices changed after the Norman conquest. From 1087 to 1155 the English year began on 1 January, and from 1155 to 1751 on 25 March. In 1752 it was moved back to 1 January.
Even before 1752, 1 January was sometimes treated as the start of the new year – for example by Pepys – while the "year starting 25th March was called the Civil or Legal Year". To reduce misunderstandings on the date, it was not uncommon for a date between 1 January and 24 March to be written as "1661/62". This was to explain to the reader that the year was 1661 counting from March and 1662 counting from January as the start of the year.
Most Western European countries shifted the first day of their numbered year to 1 January while they were still using the Julian calendar, "before" they adopted the Gregorian calendar, many during the 16th century. The following table shows the years in which various countries adopted 1 January as the start of the year. Eastern European countries, with populations showing allegiance to the Orthodox Church, began the year on 1 September from about 988. The Rumi calendar used in the Ottoman Empire began the civil year on 1 March until 1918.
From Julian to Gregorian.
The Julian calendar was in general use in Europe and Northern Africa until 1582, when Pope Gregory XIII promulgated the Gregorian calendar. Reform was required because too many leap days are added with respect to the astronomical seasons on the Julian scheme. On average, the astronomical solstices and the equinoxes advance by about 11 minutes per year against the Julian year. As a result, the calculated date of Easter gradually moved out of alignment with the March equinox.
While Hipparchus and presumably Sosigenes were aware of the discrepancy, although not of its correct value, it was evidently felt to be of little importance at the time of the Julian reform. However, it accumulated significantly over time: the Julian calendar gained a day about every 134 years. By 1582, it was ten days out of alignment from where it supposedly had been in 325 during the Council of Nicaea.
The Gregorian calendar was soon adopted by most Catholic countries (e.g. Spain, Portugal, Poland, most of Italy). Protestant countries followed later, and the countries of Eastern Europe adopted the "new calendar" even later. In the British Empire (including the American colonies), Wednesday 2 September 1752 was followed by Thursday 14 September 1752. For 12 years from 1700 Sweden used a modified Julian calendar, and adopted the Gregorian calendar in 1753, but Russia remained on the Julian calendar until 1918 (1 February became 14 February), while Greece continued to use it until 1 March 1923 (Gregorian).
Since the Julian and Gregorian calendars were long used simultaneously, although in different places, calendar dates in the transition period are often ambiguous, unless it is specified which calendar was being used. In some circumstances, double dates might be used, one in each calendar. The notation "Old Style" (O.S.) is sometimes used to indicate a date in the Julian calendar, as opposed to "New Style" (N.S.), which either represents the Julian date with the start of the year as 1 January or a full mapping onto the Gregorian calendar. This notation is used to clarify dates from countries which continued to use the Julian calendar after the Gregorian reform, such as Great Britain, which did not switch to the reformed calendar until 1752, or Russia, which did not switch until 1918.
Throughout the long transition period, the Julian calendar has continued to diverge from the Gregorian. This has happened in whole-day steps, as leap days which were dropped in certain centennial years in the Gregorian calendar continued to be present in the Julian calendar. Thus, in the year 1700 the difference increased to 11 days; in 1800, 12; and in 1900, 13. Since 2000 was a leap year according to both the Julian and Gregorian calendars, the difference of 13 days did not change in that year: 29 February 2000 (Gregorian) fell on 16 February 2000 (Julian). This difference will persist through the last day of February, 2100 (Gregorian), since 2100 is "not" a Gregorian leap year, but "is" a Julian leap year. Monday 1 March 2100 (Gregorian) falls on Monday 16 February 2100 (Julian).
Eastern Orthodox usage.
Although all Eastern Orthodox countries (most of them in Eastern or Southeastern Europe) had adopted the Gregorian calendar by 1924, their national churches had not. The "Revised Julian calendar" was proposed during a synod in Constantinople in May 1923, consisting of a solar part which was and will be identical to the Gregorian calendar until the year 2800, and a lunar part which calculated Pascha (Easter) astronomically at Jerusalem. All Orthodox churches refused to accept the lunar part, so almost all Orthodox churches continue to celebrate Pascha according to the Julian calendar (with the exception of the Estonian Orthodox Church
and the Finnish Orthodox Church).
The solar part of the Revised Julian calendar was accepted by only some Orthodox churches. Those that did accept it, with hope for improved dialogue and negotiations with the Western denominations, were the Ecumenical Patriarchate of Constantinople, the Patriarchates of Alexandria, Antioch, the Orthodox Churches of Greece, Cyprus, Romania, Poland (from 1924 to 2014 but remain permitted to use the Revised Julian calendar in parishes that want it), Bulgaria (the last in 1963), and the Orthodox Church in America (although some OCA parishes are permitted to use the Julian calendar). Thus these churches celebrate the Nativity on the same day that Western Christians do, 25 December Gregorian until 2799.
The Orthodox Churches of Jerusalem, Russia, Serbia, Poland (from 15 June 2014), Macedonia, Georgia, Ukraine, and the Greek Old Calendarists and other groups continue to use the Julian calendar, thus they celebrate the Nativity on 25 December Julian (which is 7 January Gregorian until 2100). The Russian Orthodox Church has some parishes in the West which celebrate the Nativity on 25 December Gregorian.
Parishes of the Orthodox Church in America Bulgarian Diocese, both before and after the 1976 transfer of that diocese from the Russian Orthodox Church Outside Russia to the Orthodox Church in America, were permitted to use the 25 December Gregorian date. Some Old Calendarist groups which stand in opposition to the state churches of their homelands will use the Great Feast of the Theophany (6 January Julian/19 January Gregorian) as a day for religious processions and the Great Blessing of Waters, to publicize their cause.
The Oriental Orthodox Churches generally use the local calendar of their homelands. However, when calculating the Nativity Feast, most observe the Julian calendar. This was traditionally for the sake of unity throughout Christendom. In the West, some Oriental Orthodox Churches either use the Gregorian calendar or are permitted to observe the Nativity according to it. The Armenian Apostolic Orthodox Church celebrates the Nativity as part of the Feast of Theophany according to its traditional calendar.

</doc>
<doc id="15654" url="http://en.wikipedia.org/wiki?curid=15654" title="John Quincy Adams">
John Quincy Adams

John Quincy Adams (; July 11, 1767 – February 23, 1848) was an American statesman who served as the sixth President of the United States from 1825 to 1829. He also served as a diplomat, a Senator and member of the House of Representatives. He was a member of the Federalist, Democratic-Republican, National Republican, and later Anti-Masonic and Whig parties.
In his biography, Samuel Flagg Bemis argues that Adams was able to: "gather together, formulate, and practice the fundamentals of American foreign-policy – self-determination, independence, noncolonization, nonintervention, nonentanglement in European politics, Freedom of the Seas, [and] freedom of commerce."
Quincy Adams was the son of former President John Adams and Abigail Adams. As a diplomat, Adams played an important role in negotiating key treaties, most notably the Treaty of Ghent, which ended the War of 1812. As Secretary of State, he negotiated with Britain over the United States' northern border with Canada, negotiated with Spain the annexation of Florida, and drafted the Monroe Doctrine. Historians agree he was one of the greatest diplomats and secretaries of state in American history.
As president he sought to modernize the American economy and promote education. Adams enacted a part of his agenda and paid off much of the national debt. However he was stymied time and again by a Congress controlled by his enemies, and his lack of patronage networks helped politicians eager to undercut him. He lost his 1828 bid for re-election to Andrew Jackson.
Adams is best known as a diplomat who shaped America's foreign policy in line with his ardently nationalist commitment to America's republican values. More recently, he has been portrayed as the exemplar and moral leader in an era of modernization. During Adams' lifetime, technological innovations and new means of communication spread messages of religious revival, social reform, and party politics. Goods, money, and people traveled more rapidly and efficiently than ever before.
Adams was elected as U.S. Representative from Massachusetts after leaving office, serving for the last 17 years of his life with far greater acclaim than he had achieved as president. Animated by his growing revulsion against slavery, Adams became a leading opponent of the Slave Power. He predicted that if a civil war were to break out, the president could abolish slavery by using his powers. Adams also predicted the Unions' dissolution over the slavery issue, but said that if the South became independent there would be a series of bloody slave revolts.
Early life, education, and early career.
John Quincy Adams was born on July 11, 1767, to John Adams and his wife Abigail Adams (née Smith) in a part of Braintree, Massachusetts that is now Quincy. John Quincy Adams did not attend school, but was tutored by his cousin James Thax and his father's law clerk, Nathan Rice. He was named for his mother's maternal grandfather, Colonel John Quincy, after whom Quincy, Massachusetts, is named. His namesake great-grandfather died only two days after he was born.
Adams first learned of the Declaration of Independence from the letters his father wrote his mother from the Second Continental Congress in Philadelphia. In 1779, Adams began a diary that he kept until just before he died in 1848. The massive fifty volumes are one of the most extensive collections of first-hand information from the period of the early republic and are widely cited by modern historians.
Much of Adams' youth was spent accompanying his father overseas. John Adams served as an American envoy to France from 1778 until 1779 and to the Netherlands from 1780 until 1782, and the younger Adams accompanied his father on these diplomatic missions. Adams acquired an education at institutions such as Leiden University. He matriculated in Leiden January 10, 1781. For nearly three years, beginning at the age of 14, he accompanied Francis Dana as a secretary on a mission to Saint Petersburg, Russia, to obtain recognition of the new United States. He spent time in Finland, Sweden, and Denmark and, in 1804, published a travel report of Silesia. During these years overseas, Adams became fluent in French and Dutch and became familiar with German and other European languages. Adams, mainly through the influence of his father, had also excelled in classical studies and reached high fluency of Latin and Greek. Upon entering Harvard he had already translated Virgil, Horace, Plutarch, and Aristotle. He entered Harvard College and was graduated in 1787 with a Bachelor of Arts degree, Phi Beta Kappa. Adams House at Harvard College is named in honor of Adams and his father. He later earned an M.A. from Harvard in 1790. He apprenticed as an attorney with Theophilus Parsons in Newburyport, Massachusetts, from 1787 to 1789. He gained admittance to the bar in 1791 and began practicing law in Boston.
Early political career (1796–1817).
Washington administration.
Adams first won national recognition when he published a series of widely read articles supporting Washington's decision to keep America out of the growing hostilities surrounding the French Revolution. Soon after, George Washington appointed Adams minister to the Netherlands (at the age of 26) in 1793. He did not want the position, preferring to maintain his quiet life of reading in Massachusetts, and probably would have rejected it if his father had not persuaded him to take it. On his way to the Netherlands, he was to deliver a set of documents to John Jay, who was negotiating the Jay Treaty. After spending some time with Jay, Adams wrote home to his father, in support of the emerging treaty because he thought America should stay out of European affairs. Historian Paul Nagel has noted that this letter reached Washington, and that parts of it were used by Washington when drafting his farewell address. 
While going back and forth between The Hague and London, he met and proposed to his future wife, Louisa Catherine Johnson. Though he wanted to return to private life at the end of his appointment, Washington appointed him minister to Portugal in 1796, where he was soon appointed to the Berlin Legation. Though his talents were far greater than his desire to serve, he was finally convinced to remain in public service when he learned how highly Washington thought of his abilities. Washington called Adams "the most valuable of America's officials abroad," and Nagel believes that it was at this time that Adams first came to terms with a lifetime of public service.
He became a Fellow of the American Academy of Arts and Sciences in 1797. When the elder Adams became president, he appointed his son in 1797 as Minister to Prussia at Washington's urging. There Adams signed the renewal of the very liberal Prussian-American Treaty of Amity and Commerce after negotiations with Prussian Foreign Minister Count Karl-Wilhelm Finck von Finckenstein. He served at that post until 1801.
While serving abroad, in 1797 Adams also married Louisa Catherine Johnson, the daughter of a poor American merchant, in a ceremony at the church of All Hallows-by-the-Tower, London. Adams remains the only president to have married a First Lady born outside of the United States.
Massachusetts politics.
On his return to the United States, Adams was appointed a Commissioner of Monetary Affairs in Boston by a Federal District Judge; however, Thomas Jefferson rescinded this appointment. He again tried his hand as an attorney, but shortly afterward entered politics. John Quincy Adams was elected a member of the Massachusetts State Senate in April 1802. In November 1802 he ran as a Federalist for the United States House of Representatives and lost.
The Massachusetts General Court elected Adams as a Federalist to the U.S. Senate soon after, and he served from March 4, 1803, until 1808, when he broke with the Federalist Party. Adams, as a senator, had supported the Louisiana Purchase and Jefferson's Embargo Act, actions which made him very unpopular with Massachusetts Federalists. The Federalist-controlled Massachusetts Legislature chose a replacement for Adams on June 3, 1808, several months early. On June 8, Adams broke with the Federalists, resigned his Senate seat, and became a Republican.
Harvard professor.
While a member of the Senate, Adams also served as a professor of logic at Brown University. Disowned by the Federalists and not fully accepted by the Republicans, Adams used his Boylston Professorship of Rhetoric and Oratory at Harvard as a new base. Adams' devotion to classical rhetoric shaped his response to public issues. He remained inspired by classical rhetorical ideals long after the neo-classicalism and deferential politics of the founding generation had been eclipsed by the commercial ethos and mass democracy of the Jacksonian Era. Many of Adams' idiosyncratic positions were rooted in his abiding devotion to the Ciceronian ideal of the citizen-orator "speaking well" to promote the welfare of the polis.
Adams was influenced by the classical republican ideal of civic eloquence espoused by British philosopher David Hume. Adams adapted these classical republican ideals of public oratory to America, viewing the multilevel political structure as ripe for "the renaissance of Demosthenic eloquence." Adams' "Lectures on Rhetoric and Oratory" (1810) looks at the fate of ancient oratory, the necessity of liberty for it to flourish, and its importance as a unifying element for a new nation of diverse cultures and beliefs. Just as civic eloquence failed to gain popularity in Britain, in the United States interest faded in the second decade of the 19th century as the "public spheres of heated oratory" disappeared in favor of the private sphere.
First U.S. minister to Russia.
President James Madison appointed Adams as the first ever United States Minister to Russia in 1809 (though Francis Dana and William Short had previously been nominated to the post, neither presented his credentials at Saint Petersburg). After resigning his post at Harvard, Adams and his wife Louisa boarded a merchant ship in Boston on Aug. 5, 1809. Their youngest son was with them during the long and tedious voyage to St. Petersburg. Their voyage was temporarily interrupted outside the southern coast of Norway due to the Gunboat War. They were at first boarded by a British officer who examined their papers and then, later that day, by a Norwegian officer who ordered the ship to Christiansand. In Christiansand, Adams discovered thirty-eight U.S. vessels which had been detained by the Norwegians and decided to take whatever action necessary to gain the release of both ships and crew as soon as possible. Norway was under Danish rule at the time, so the officer and pirates were actually Norwegian. The voyage to St. Petersburg resumed but was once again stopped by a British squadron. Adams showed his commission to Admiral Albermarle Bertie, the commander of the Squadron who recognized the usage of nations and Adams as an ambassador. The usage of nations is common laws of nations founded on custom. Because of the many delays, the Adams' did not arrive in St. Petersburg until October 23, 1809.
Diplomatic relations.
Count Nikolay Rumyantsev, Chancellor of the empire, formally received Adams, and requested a copy of his credential letter. Romanzoff assured Adams that his appointment pleased him personally. Adams presentation to the emperor was postponed however because of the temporary indisposition of Alexander I. Rumyantsev immediately invited Adams to a diplomatic dinner which included the French ambassador, Armand Augustin Louis de Caulaincourt, Duke of Vicenza, numerous foreign ministers then at the Russian Court, and many of the nobility. This was the same mansion Adams had dined in 1781, as secretary of Francis Dana.
Tsar Alexander I received Adams alone in his cabinet where he expressed his pleasure at Adams' appointment. Adams told Alexander that "the President of the United States had desired him to express the hope that his mission would be considered as a proof of respect for the person and character of his majesty, as an acknowledgment of the many testimonies of good-will he had already given to the United States, and of a desire to strengthen commercial relations between them and his provinces." Alexander replied, that, "in everything depending on him, he should be happy to contribute to the increase of their friendly relations; that it was his wish to establish a just system of maritime rights, and that he should adhere invariably to those he had declared." After these official diplomatic greetings, Alexander and Adams discussed several other issues such as the policies of the different European powers, trade and commerce, and other mutually beneficial prospects, and that the Russian and U.S. could be very useful to each other.
Adams was also given private audiences with the empress and the dowager empress, who also received Louisa Adams. While not officially a diplomat, Louisa Adams did serve an invaluable role as wife-of-diplomat, becoming a favorite of the czar and making up for her husband's utter lack of charm. She was an indispensable part of the American mission.
Adams requested Rumyantsev to ask Alexander to act on behalf of the United States in securing the release of the American sailors and ships being held by the Danish. The Tsar ordered the Chancellor to request the release of the American property as soon as possible, which the Danish government complied with. Adams spent a great deal of time securing the release of American vessels and seamen from various "seizures and sequestrations."
In 1811, Adams received a commission from the Secretary of State as an Associate Justice of the Supreme Court of the United States. Adams immediately declined and remained in St. Petersburg. In 1812, Adams reported the news of Napoleon's invasion of Russia and Napoleon's disastrous retreat. Also in 1812, Rumyantsev asked if he should request Alexander to mediate a pacification of hostilities between the United States and Great Britain. The U.S. accepted the offer and in July 1813, two associates of Adams, Albert Gallatin and James A. Bayard arrived in St. Petersburg to begin negotiations under mediation by Alexander. Gallatin was at that time Secretary of Treasury and the Senate rejected his appointment to the diplomatic mission as incompatible under the Constitution. However, this rejection did not occur until after Gallatin and Bayard had already left for St. Petersburg. In September, Lord William Cathcart delivered a British memoir to Alexander explaining their reasons for declining the mediation. Thus ended President Madison's hope that Alexander could end the war.
Adams in Russian society.
Adams was well liked by the Russian Court and often would be met on walks by Alexander. The tsar asked Adams if he would be taking a house in the country over the summer. When Adams hesitated, the emperor stated with good humor that perhaps it was a financial consideration and Adams was able to respond in kind that it was in large part. Adams was a man who endeavored to live within the means provided by the American government.
The Adams' were also provided with several invitations to various entertainments. "The formalities of these court presentations," Mr. Adams remarked, "are so trifling and insignificant in themselves, and so important in the eyes of princes and courtiers, that they are much more embarrassing to an American than business of greater importance. It is not safe or prudent to despise them, nor practicable for a person of rational understanding to value them." Adams was concerned that the various balls and parties given by the Tsar and the foreign ministers, took too much time away from his official duties. Often these events would last until 4 a.m., so Adams stopped attending those he was able to avoid. The numerous diplomatic visits also annoyed Adams who wrote "I have been engaged the whole forenoon; and though I rise at six o'clock, I am sometimes unable to find time to write only part of a private letter in the course of the day. These visits take up so much of my time, that I sometimes think of taking a resolution not to receive them; but, on the other hand, so much information important to be possessed, and particularly relative to current political events, is to be collected from them, that they are rather to be encouraged than discountenanced."
In 1814, Adams was recalled from Russia to serve as chief negotiator of the U.S. commission for the Treaty of Ghent, which ended the War of 1812 between the United States and United Kingdom.
Minister to the court of St. James's.
Finally, he was sent to be minister to the Court of St. James's (Britain) from 1815 until 1817, a post that was first held by his father. The name is derived from its location at St. James's Palace. In London, Adams was part of a U.S. Legation consisting of himself, two young secretaries and a small office in Craven Street, London WC2. Since they were not particularly well paid, Adams and his wife Louisa lived in Ealing, at that time a village in the countryside, in order to maintain the expensive carriages and liveries which social appearance demanded.
U.S. Secretary of State (1817–1825).
Adams served as Secretary of State in the Cabinet of President James Monroe from 1817 until 1825. Typically, his views concurred with those espoused by Monroe. As Secretary of State, he negotiated the Adams–Onís Treaty (which acquired Florida for the United States), the Treaty of 1818, and wrote the Monroe Doctrine. Many historians regard him as one of the greatest Secretaries of State in American history.
The Florida's, still a Spanish territory but with no Spanish presence to speak of, became a refuge for runaway slaves and native Americans. Monroe sent in General Andrew Jackson who pushed the Seminole Indians south, executed two British merchants who were supplying weapons, deposed one governor and named another, and left an American garrison in occupation. President Monroe and all his cabinet, except Adams, believed Jackson had exceeded his instructions. Adams argued that since Spain had proved incapable of policing her territories, the United States was obliged to act in self-defense. Adams so ably justified Jackson's conduct that he silenced protests from either Spain or Britain; Congress refused to punish Jackson. Adams used the events that had unfolded in Florida to negotiate the Florida Treaty with Spain in 1819 that turned Florida over to the U.S. and resolved border issues regarding the Louisiana Purchase.
With the ongoing Oregon boundary dispute, Adams sought to negotiate a settlement with England to decide the border between the western United States and Canada. This would become the Treaty of 1818. Along with the Rush–Bagot Treaty of 1817, this marked the beginning of improved relations between the British Empire and its former colonies, and paved the way for better relations between the U.S. and Canada. The treaty had several provisions, but in particular it set the boundary between British North America and the United States along the 49th parallel through the Rocky Mountains. This settled a boundary dispute caused by ignorance of actual geography in the boundary agreed to in the 1783 Treaty of Paris that ended the American Revolutionary War. That earlier treaty had used the Mississippi River to determine the border, but assumed that the river extended further north than it did, and so that earlier settlement was unworkable.
By the time Monroe became president, several European powers, in particular Spain, were attempting to re-establish control over South America. On Independence Day 1821, in response to those who advocated American support for independence movements in many South American countries, Adams gave a speech in which he said that American policy was moral support for independence movements but not armed intervention. Adams foresaw what would befall the United States if it sacrificed its republican spirit on the altar of empire. He stated that America "goes not abroad in search of monsters to destroy" lest she "involve herself beyond power of extrication, in all wars of interest and intrigue, of individual avarice, envy, and ambition, which assume the colors and usurp the standard of freedom. The fundamental maxims of her policy would insensibly change from liberty to force." The United States, Adams warned, might "become the dictatress of the world [but] she would be no longer the ruler of her own spirit." From this, Adams authored what came to be known as the Monroe Doctrine, which was introduced on December 2, 1823. It stated that further efforts by European countries to colonize land or interfere with states in the Americas would be viewed as acts of aggression requiring U.S. intervention. The United States, reflecting concerns raised by Great Britain, ultimately hoped to avoid having any European power take over Spain's colonies. It became a defining moment in the foreign policy of the United States and one of its longest-standing tenets, and would be invoked by many U.S. statesmen and several U.S. presidents, including Theodore Roosevelt, Calvin Coolidge, Herbert Hoover, John F. Kennedy, Ronald Reagan and others.
1824 presidential election.
As the 1824 election drew near people began looking for candidates. New England voters admired Adams' patriotism and political skills and it was mainly due to their support that he entered the race. The old caucus system of the Democratic-Republican Party had collapsed; indeed the entire First Party System had collapsed and the election was a fight based on regional support. Adams had a strong base in New England. His opponents included John C. Calhoun, William H. Crawford, Henry Clay, and the hero of New Orleans, Andrew Jackson. During the campaign Calhoun dropped out, and Crawford fell ill giving further support to the other candidates. When Election Day arrived, Andrew Jackson won, although narrowly, pluralities of the popular and electoral votes, but not the necessary majority of electoral votes. With just over one-fourth voter turnout for the election, combined with Adams receiving less than one-third of the popular vote, Adams scored only 113,142 votes.
 BEP engraved portrait of Adams as president 
Under the terms of the Twelfth Amendment, the presidential election fell to the House of Representatives, which was to choose from the top three candidates: Jackson, Adams, and Crawford. Clay had come in fourth place and thus was not on the ballot, but he retained considerable power and influence as Speaker of the House.
Clay's personal dislike for Jackson and the similarity of his American System to Adams' position on tariffs and internal improvements caused him to throw his support to Adams, who was elected by the House on February 9, 1825, on the first ballot. Adams' victory shocked Jackson, who had won the most electoral and popular votes and fully expected to be elected president. When Adams appointed Clay as Secretary of State—the position that Adams and his three predecessors had held before becoming president—Jacksonian Democrats were outraged, and claimed that Adams and Clay had struck a "corrupt bargain". This contention overshadowed Adams' term and greatly contributed to Adams' loss to Jackson four years later, in the 1828 election.
Presidency (1825–1829).
Adams served as the sixth President of the United States from March 4, 1825, to March 4, 1829. He took the oath of office on a book of constitutional law, instead of the more traditional Bible. Adams proposed an elaborate program of internal improvements (roads, ports and canals), a national university, and federal support for the arts and sciences. He favored a high tariff to encourage the building of factories, and restricted land sales to slow the movement west. Opposition from the states' rights faction of a hostile congress killed many of his proposals. He also reduced the national debt from $16 million to $5 million, the remainder of which was paid off by his immediate successor, Andrew Jackson.
Paul Nagel argues that his political acumen was not any less developed than others were in his day, and notes that Henry Clay, one of the era's most astute politicians, was a principal adviser to Adams and supporter throughout his presidency. Nagel argues that Adams' political problems were the result of an unusually hostile Jacksonian faction, and Adams' own dislike of the office. Although a product of the political culture of his day, he refused to play politics according to the usual rules and was not as aggressive in courting political support as he could have been. He was attacked by the followers of Jackson, who accused him of being a partner to a "corrupt bargain" to obtain Clay's support in the election and then appoint him Secretary of State. Jackson defeated Adams in 1828, and created the modern Democratic party thus inaugurating the Second Party System.
Domestic policies.
During his term, Adams worked on transforming America into a world power through "internal improvements," as a part of the "American System". It consisted of a high tariff to support internal improvements such as road-building, and a national bank to encourage productive enterprise and form a national currency. In his first annual message to Congress, Adams presented an ambitious program for modernization that included roads, canals, a national university, an astronomical observatory, and other initiatives. The support for his proposals was mixed, mainly due to opposition from Jackson's followers.
Some of his proposals were adopted, specifically the extension of the Cumberland Road into Ohio with surveys for its continuation west to St. Louis; the beginning of the Chesapeake and Ohio Canal, the construction of the Chesapeake and Delaware Canal and the Louisville and Portland Canal around the falls of the Ohio; the connection of the Great Lakes to the Ohio River system in Ohio and Indiana; and the enlargement and rebuilding of the Dismal Swamp Canal in North Carolina. One of the issues which divided the administration was protective tariffs, of which Henry Clay was a leading advocate. After Adams lost control of Congress in 1827, the situation became more complicated. By signing into law the Tariff of 1828, quite unpopular in parts of the south, he further antagonized the Jacksonian s.
Adams' generous policy toward Native Americans caused him trouble. Settlers on the frontier, who were constantly seeking to move westward, cried for a more expansionist policy. When the federal government tried to assert authority on behalf of the Cherokees, the governor of Georgia took up arms. Adams defended his domestic agenda as continuing Monroe's policies. In contrast, Andrew Jackson and Martin Van Buren instigated the policy of Indian removal to the west (i.e. the Trail of Tears).
Foreign policies.
Adams is regarded as one of the greatest diplomats in American history, and during his tenure as Secretary of State, he was the chief designer of the Monroe Doctrine. He had witnessed the First Barbary War and the Second Barbary War against the Arab pirates of North Africa, and the Greek War of Independence from the Ottoman Turks. Public opinion in the U.S. strongly favored the Greek cause and such leaders as Henry Clay called for intervention. Adams strongly opposed any entanglement in European affairs. According to Charles Edel, Adams believed that, "Intervention would accomplish little, retard the cause of republicanism, and distract the country from its primary goal of continental expansion". Moreover, fearful that U.S. intentions would outstrip its capabilities, Adams thought that projecting U.S. power abroad would weaken its gravitational force on the North American continent.
On July 4, 1821, he gave an address to Congress:
... But she [the United States of America] goes not abroad, in search of monsters to destroy. She is the well-wisher to the freedom and independence of all. She is the champion and vindicator only of her own.
During his term as president, however, Adams achieved little of long-term consequence in foreign affairs. A reason for this was the opposition he faced in Congress, where his rivals prevented him from succeeding. Among his diplomatic achievements were treaties of reciprocity with a number of nations, including Denmark, Mexico, the Hanseatic League, the Scandinavian countries, Prussia and Austria. However, thanks to the successes of Adams' diplomacy during his previous eight years as secretary of state, most of the foreign policy issues he would have faced had been resolved by the time he became president.
Departure.
John Quincy Adams left office on March 4, 1829, after losing the election of 1828 to Andrew Jackson. Adams did not attend the inauguration of his successor, Andrew Jackson, who had openly snubbed him by refusing to pay the traditional "courtesy call" to the outgoing president during the weeks before his own inauguration. He was one of only four presidents who chose not to attend their respective successor's inauguration; the others were his father, Andrew Johnson, and Richard Nixon.
1828 presidential election.
After the inauguration of Adams in 1825, Jackson resigned from his senate seat. For four years he worked hard, with help from his supporters in Congress, to defeat Adams in the presidential election of 1828. The campaign was very much a personal one. As was the tradition of the day and age in American presidential politics, neither candidate personally campaigned, but their political followers organized many campaign events. Both candidates were rhetorically attacked in the press. This reached a low point when the press accused Jackson's wife Rachel of bigamy. She died a few weeks after the elections. Jackson said he would forgive those who insulted him, but he would never forgive the ones who had attacked his wife.
Adams lost the election by a decisive margin. He won all the same states that his father had won in the election of 1800: the New England states, New Jersey, and Delaware, as well as parts of New York and a majority of Maryland. Jackson won the rest of the states, picking up 178 electoral votes to Adams' 83 votes, and succeeded him. Adams and his father were the only U.S. presidents to serve a single term during the first 48 years of the Presidency (1789–1837). Historian Thomas Bailey observed, "Seldom has the public mind been so successfully poisoned against an honest and high-minded man."
Later congressional career (1830–1848).
Adams did not retire after leaving office. Instead he ran for and won a seat in the United States House of Representatives in the 1830 elections. This went against the generally held opinion that former Presidents should not run for public office. He was the first President to serve in Congress after his term of office, and one of only two former presidents to do so (Andrew Johnson later served in the Senate). He was elected to nine terms, serving as a Representative for 17 years, from 1831 until his death.
Adams ran for Governor of Massachusetts in 1833 on the Anti-Masonic ticket. Incumbent National Republican Governor Levi Lincoln Jr. was retiring and Adams faced National Republican John Davis, Democrat Marcus Morton and Samuel L. Allen of the Working Men's Party. Davis won a plurality, 40%, and Adams took 29% with Morton taking 25% and Allen 6%. Because no candidate had won a majority, the election was sent to the state legislature to decide. Adams withdrew and endorsed Davis, preferring him over Morton, and Davis was chosen by the legislature in January 1834.
In authoring a change to the Tariff of 1828, he was instrumental to the compromise that ended the Nullification Crisis. When James Smithson died and left his estate to the U.S. government to build an institution of learning, many in Congress wanted to use the money for other purposes. Adams was key to ensuring that the money was instead used to build the Smithsonian Institution.
Committee assignments.
In Congress, he was chair of the Committee on Commerce and Manufactures, the Committee on Indian Affairs and the Committee on Foreign Affairs.
Slavery.
A longtime opponent of slavery, Adams used his new role in Congress to fight it. In 1836, Southern Representatives voted in a “gag rule” that immediately tabled any petitions about slavery, thus preventing any discussion or debate of the slavery issue. He became a forceful opponent of this rule and conceived a way around it, attacking slavery in the House for two weeks.
The gag rule prevented him from bringing slavery petitions to the floor, but he brought one anyway. It was a petition from a Georgia citizen urging disunion due to the continuation of slavery in the South. Though he certainly did not support it and made that clear at the time, his intent was to antagonize the pro-slavery faction of Congress into an open fight on the matter. The plan worked.
The petition infuriated his Congressional enemies, many of whom were agitating for disunion themselves. They moved for his censure over the matter, enabling Adams to discuss slavery openly during his subsequent defense. Taking advantage of his right to defend himself, Adams delivered prepared and impromptu remarks against slavery and in favor of abolition. Knowing that he would probably be acquitted, he changed the focus from his own actions to those of the slaveholders, speaking against the slave trade and the ownership of slaves. He decided that if he were censured, he would merely resign, run for the office again, and probably win easily. When his opponents realized that they played into his political strategy, they tried to bury the censure. Adams made sure this did not happen, and the debate continued. He attacked slavery and slaveholders as immoral and condemned the institution while calling for it to end. After two weeks, a vote was held, and he was not censured. He delighted in the misery he was inflicting on the slaveholders he so hated, and prided himself on being "obnoxious to the slave faction."
Although the censure of Adams over the slavery petition was ultimately abandoned, the House did address the issue of petitions from enslaved persons at a later time. Adams again argued that the right to petition was a universal right, granted by God, so that those in the weakest positions might always have recourse to those in the most powerful. Adams also called into question the actions of a House that would limit its own ability to debate and resolve questions internally. After this debate, the gag rule was ultimately retained.
The discussion ignited by his actions and the attempts of others to quiet him raised questions of the right to petition, the right to legislative debate, and the morality of slavery. During the censure debate, Adams said that he took delight in the fact that southerners would forever remember him as "the acutest, the astutest, the archest enemy of southern slavery that ever existed".
In 1844, he chaired a committee for reform of the rules of Congress, and he used this opportunity to try once again to repeal the gag rule. He spent two months building support for this move, but due to northern opposition, the rule narrowly survived. He fiercely criticized northern Representatives and Senators, in particular Stephen A. Douglas, who seemed to cater to the slave faction in exchange for southern support. His opposition to slavery made him, along with Henry Clay, one of the leading opponents of Texas annexation and the Mexican–American War. He correctly predicted that both would contribute to civil war. After one of his reelection victories, he said that he must "bring about a day prophesied when slavery and war shall be banished from the face of the earth." He wrote in his private journal in 1820:
The discussion of this Missouri question has betrayed the secret of their souls. In the abstract they admit that slavery is an evil, they disclaim it, and cast it all upon the shoulder of…Great Britain. But when probed to the quick upon it, they show at the bottom of their souls pride and vainglory in their condition of masterdom. They look down upon the simplicity of a Yankee’s manners, because he has no habits of overbearing like theirs and cannot treat negroes like dogs. It is among the evils of slavery that it taints the very sources of moral principle. It establishes false estimates of virtue and vice: for what can be more false and heartless than this doctrine which makes the first and holiest rights of humanity to depend upon the color of the skin?
In 1841, at the request of Lewis Tappan and Ellis Gray, Adams joined the case of "United States v. The Amistad". Adams went before the Supreme Court on behalf of African slaves who had revolted and seized the Spanish ship "Amistad". Adams appeared on 24 February 1841, and spoke for four hours. His argument succeeded; the Court ruled in favor of the Africans, who were declared free and returned to their homes.
Photography.
In 1843, Adams sat for the earliest confirmed photograph still in existence of a U.S. president, although other sources contend that William Henry Harrison had posed even earlier for his portrait, in 1841. The original daguerreotype is in the collection of the National Portrait Gallery of the Smithsonian Institution.
Living Bridge between American Revolution and Civil War.
Although there is no indication that the two were close, Adams met Abraham Lincoln during the latter's sole term as a member of the House of Representatives, from 1847 until Adams' death. Thus, it has been suggested that Adams is the only major figure in American history who knew both the Founding Fathers and Abraham Lincoln, though Martin Van Buren met Founding Fathers Thomas Jefferson and John Adams, knew Founder Aaron Burr (Van Buren's mentor), and met the young Lincoln while on a campaign trip through Illinois.
Nullification crisis.
Besides his opposition to slavery and the gag rule (discussed above), his congressional career is remembered for several other key accomplishments. Shortly after Adams entered Congress, the Nullification Crisis threatened civil war over the Tariff of 1828. Adams authored an alteration to the tariff, which weakened it and diffused the crisis. Congress also passed the Force Bill which authorized President Andrew Jackson to use military force if Adams' compromise bill did not force the belligerent states to capitulate. There was no need, however, because Adams' compromise defused the issue. The compromise actually did not alter the tariff as much as the southern states had hoped, though they agreed not to continue pursuing the issue for fear of civil war.
Advancement of science.
Adams also became a leading force for the advancement of science. As president, he had proposed a national observatory, which did not win much support. In 1829 British scientist James Smithson died, and left his fortune for the "increase and diffusion of knowledge." In Smithson's will, he stated that should his nephew, Henry James Hungerford, die without heirs, the Smithson estate would go to the government of the United States to create an "Establishment for the increase & diffusion of Knowledge among men." After the nephew died without heirs in 1835, President Andrew Jackson informed Congress of the bequest, which amounted to about US$500,000 ($75,000,000 in 2008 U.S. dollars after inflation). Adams realized that this might allow the United States to realize his dream of building a national institution of science and learning. Adams thus became Congress' primary supporter of what would become the Smithsonian Institution. He also relentlessly pursued support for astronomical efforts and observatories, seeking a national observatory for the United States. His efforts eventually led to what is now the United States' oldest, still-operational scientific institution, the United States Naval Observatory. In 1825 Adams signed a bill for the creation of a national observatory just before leaving presidential office – which became the Naval Observatory. Adams in fact spent many nights at the Observatory, with celebrated national astronomer and oceanographer Matthew Fontaine Maury, watching and charting the stars, which had always been one of Adams' avocations.
As for efforts to found the Smithsonian Institution, the money was invested in shaky state bonds, which quickly defaulted. After heated debate in Congress, Adams successfully argued to restore the lost funds with interest. Though Congress wanted to use the money for other purposes, Adams successfully persuaded Congress to preserve the money for an institution of science and learning. Congress also debated whether the federal government had the authority to accept the gift, though with Adams leading the initiative, Congress decided to accept the legacy bequeathed to the nation and pledged the faith of the United States to the charitable trust on July 1, 1836.
Death.
In 1846, the 78-year old former president suffered a stroke that left him partially paralyzed. After a few months of rest, he made a full recovery and resumed his duties in Congress. When Adams entered the House chamber, everyone "stood up and applauded." On February 21, 1848, the House of Representatives was discussing the matter of honoring U.S. Army officers who served in the Mexican–American War. Adams had been a vehement critic of the war, and as Congressmen rose up to say, "Aye!" in favor of the measure, he instead yelled, "No!" He rose to answer a question put forth by the Speaker of the House. Immediately thereafter, Adams collapsed, having suffered a massive cerebral hemorrhage. Two days later, on February 23, he died with his wife and youngest son at his side in the Speaker's Room inside the Capitol Building in Washington, D.C. His last words were "This is the last of earth. I am content." He died at 7:20 p.m.
His original interment was temporary, in the public vault at the Congressional Cemetery in Washington, D.C. Later, he was interred in the family burial ground in Quincy, Massachusetts, across from the First Parish Church, called Hancock Cemetery. After Louisa's death in 1852, his son Charles Francis Adams had his parents reinterred in the expanded family crypt in the United First Parish Church across the street, next to John and Abigail. Both tombs are viewable by the public. Adams' original tomb at Hancock Cemetery is still there and marked simply "J.Q. Adams".
Personal life.
John Quincy Adams and Louisa Catherine Adams had three sons and a daughter. Their daughter, Louisa, was born in 1811 but died in 1812 while the family was in Russia. They named their first son George Washington Adams (1801–1829) after the first president. Both George and their second son, John (1803–1834), led troubled lives and died in early adulthood. (George committed suicide and John was expelled from Harvard before his 1823 graduation.)
Adams' youngest son, Charles Francis Adams (who named his own son John Quincy), also pursued a career in diplomacy and politics. In 1870 Charles Francis built the first memorial presidential library in the United States, to honor his father. The Stone Library includes over 14,000 books written in twelve languages. The library is located in the "Old House" at Adams National Historical Park in Quincy, Massachusetts.
John Adams and John Quincy Adams were the only father and son to serve as presidents until George H. W. Bush (1989–1993) and George W. Bush (2001–2009).
Legacy.
John Quincy Adams Birthplace is now part of Adams National Historical Park and open to the public. The name Quincy has been used for at least nineteen other places in the United States. Those places were either directly or indirectly named for John Quincy Adams (for example, Quincy, Illinois, was named in honor of Adams while Quincy, California, was named for Quincy, Illinois).
Adams was the first president to have his photograph taken. He also became the first president to adopt a short haircut instead of long hair tied in a queue and to regularly wear long trousers instead of knee breeches. He is probably best known as a diplomat who shaped America's foreign policy in accordance with his ardently nationalist views, and is widely considered by historians to have been one of the greatest diplomats in American history. He was key to the negotiation of several important treaties, such as the Treaty of Ghent, which ended the War of 1812, and the Florida Treaty, which resulted in the annexation of Florida. He also formulated the Monroe Doctrine, which is still evoked to the present day. He is viewed by many as the exemplar and moral leader in an era of modernization. During this era, new technologies and networks of infrastructure and communication brought to the people messages of religious revival, social reform, and party politics, as well as moving goods, money, and people ever more rapidly and efficiently.
Though he was always quite hostile to slavery, nearly to be point of being an abolitionist (although he doubted the abolitionists could successfully end slavery), he grew even more hostile to it later in life. Adams became a leading opponent of slave power and articulated a theory whereby the president could abolish slavery by using his war powers, a correct prediction of Abraham Lincoln's use of the Emancipation Proclamation in 1863. Adams predicted the likelihood of the Union's dissolution over the slavery issue, and was a key opponent of the Mexican–American War for this reason. Though he later described his presidency as the unhappiest time of his life, scholars rate John Quincy Adams in the second quartile in the majority of historical presidential rankings.
Historians have often included Adams among the leading conservatives of his day. Russell Kirk, however, sees Adams as a flawed conservative who was imprudent in opposing slavery.
Diaries.
One of Adams' most important legacies is his massive diary, which he began at age 11 with the simple entry "A journal, by me, J.Q.A." The Diary, housed at the Massachusetts Historical Society, covers, in extraordinary detail, Adams' life and experiences up to his death in 1848. The massive fifty volumes are one of the most extensive collections of first-hand information from that period of the early American republic, and are cited by historians in a wide range of matters pertaining to that period.
Personality.
Adams' personality was much like that of his father, as were his political beliefs. Throughout his life, he always preferred reading in seclusion to attending social engagements, and several times had to be pressured by others to remain in public service. Historian Paul Nagel argues that, like Abraham Lincoln after him, Adams suffered from depression for much of his life. Early in his life he sought some form of treatment. Adams thought his depression was due to the high expectations demanded of him by his father and mother. Throughout his life he felt inadequate and socially awkward because of his depression, and was constantly bothered by his physical appearance. He was closer to his father, whom he spent much of his early life with abroad, than he was to his mother. When he was younger and the American Revolution was going on, his mother told her children what their father was doing, and what he was risking, and because of this Adams grew to greatly respect his father. His relationship with his mother was rocky; she had high expectations of him and was afraid her children might end up a dead alcoholic like her brother. As Abigail Adams had feared, John Quincy's brother, Charles, would eventually follow this fate.
John Quincy fell in love shortly after he finished school, but his mother did not approve of his considering marriage when he was still dependent on his parents for support, and the relationship ended. When he fell in love with his future wife, Louisa Johnson, his mother disapproved of this relationship as well. His biographer, Nagel, concludes that this disapproval motivated him to marry Johnson in 1797, despite Adams' reservations that Johnson, like his mother, had a strong a personality.
Marquis de Lafayette once gave Adams an alligator as a gift, which he lodged for months in the unfinished East Room of the White House, before building it its own lodge. Reports indicate he enjoyed showing it to visitors.
Antislavery advocacy.
Before 1820, Adams was best known as an exponent of American nationalism. Later in life, especially after his election to the House, he was famous as the most prominent national leader opposing slavery. He was not an abolitionist, say biographers Nagel and Parsons. Remini notes that Adams thought the end of slavery would come by either civil war or the consent of the slave South, but definitely not through the work of abolitionists.
The turning point came with the debate on the Missouri Compromise in 1820 when he broke with his friend John C. Calhoun, who became the most outspoken national leader in favor of slavery. They became bitter enemies. Adams vilified slavery as a terrible evil and preached total abolition, while Calhoun countered that the right to own slaves had to be protected from interference from the federal government to keep the nation alive. Adams said slavery contradicted the principles of republicanism, while Calhoun said that slavery was essential to American democracy, for it made all white men equal. Both men pulled away from nationalism, and started to consider dissolution of the Union as a way of resolving the slavery predicament. Adams predicted that if the South formed a new nation, it would be torn apart by an extremely violent slave insurrection. If the two nations went to war, Adams predicted the president of the United States would use his war powers to abolish slavery. The two men became ideological leaders of the North and the South. In the House Adams became a champion of free speech, demanding that petitions against slavery be heard despite a "gag rule" that said they could not be heard.
In 1841, Adams had the case of a lifetime, representing the defendants in "United States v. The Amistad Africans" in the Supreme Court of the United States. He successfully argued that the Africans, who had seized control of a Spanish ship on which they were being transported illegally as slaves, should not be extradited or deported to Cuba (a Spanish colony where slavery was legal) but should be considered free. Under President Martin Van Buren, the government argued the Africans should be deported for having mutinied and killed officers on the ship. Adams won their freedom, with the chance to stay in the United States or return to Africa. Adams made the argument because the U.S. had prohibited the international slave trade, although it allowed internal slavery. He never billed for his services in the "Amistad" case. The speech was directed not only at the justices of this Supreme Court hearing the case, but also to the broad national audience he instructed in the evils of slavery.
Adams repeatedly spoke out against the "Slave Power", that is the organized political power of the slave owners who dominated all the southern states and their representation in Congress. He vehemently attacked the annexation of Texas (1845) and the Mexican War (1846–48) as part of a "conspiracy" to extend slavery.
Film and television.
Adams occasionally is featured in the mass media. In the PBS miniseries "The Adams Chronicles" (1976), he was portrayed by David Birney, William Daniels, Marcel Trenchard, Steven Grover and Mark Winkworth. He was also portrayed by Anthony Hopkins in the 1997 film "Amistad", and again by Ebon Moss-Bachrach and Steven Hinkle in the 2008 HBO television miniseries "John Adams"; the HBO series received criticism for needless historical and temporal distortions in its portrayal.
Bibliography: works by Adams.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "John Quincy Adams" article dated 2009-01-08, and does not reflect subsequent edits to the article. ()
More spoken articles
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="15655" url="http://en.wikipedia.org/wiki?curid=15655" title="Jurassic">
Jurassic

The Jurassic (; from Jura Mountains) is a geologic period and system that extends from ± 0.6 Ma (million years ago) to  Ma; from the end of the Triassic to the beginning of the Cretaceous. The Jurassic constitutes the middle period of the Mesozoic Era, also known as the Age of Reptiles. The start of the period is marked by the major Triassic–Jurassic extinction event. Two other extinction events occurred during the period: the Late Pliensbachian/Early Toarcian event in the Early Jurassic, and the Late Tithonian event at the end; however, neither event ranks among the 'Big Five' mass extinctions. The Jurassic is named after the Jura Mountains within the European Alps, where limestone strata from the period were first identified.
By the beginning of the Jurassic, the supercontinent Pangaea had begun rifting into two landmasses, Laurasia to the north and Gondwana to the south. This created more coastlines and shifted the continental climate from dry to humid, and many of the arid deserts of the Triassic were replaced by lush rainforests. On land, the fauna transitioned from the Triassic fauna, dominated by both dinosauromorph and crocodylomorph archosaurs, to one dominated by dinosaurs alone. The first birds also appeared during the Jurassic, having evolved from a branch of theropod dinosaurs. Other major events include the appearance of the earliest lizards, and the evolution of therian mammals, including primitive placentals. Crocodylians made the transition from a terrestrial to an aquatic mode of life. The oceans were inhabited by marine reptiles such as ichthyosaurs and plesiosaurs, while pterosaurs were the dominant flying vertebrates.
Etymology.
The chronostratigraphic term "Jurassic" is directly linked to the Jura Mountains. Alexander von Humboldt recognized the mainly limestone dominated mountain range of the Jura Mountains as a separate formation that had not been included in the established stratigraphic system defined by Abraham Gottlob Werner, and he named it "Jurakalk" in 1795. The name "Jura" is derived from the Celtic root "jor", which was Latinised into "juria", meaning forest (i.e., "Jura" is forest mountains).
Divisions.
Key events in the Jurassic</div Scale><div id=ScaleBar style="width:1px; float:left; height:36em; padding:0; background-color:#242020" />em;
 height:em;
 margin-left:0em;
 width:0.72em;
">em; 
">Mesozoic
em;
 height:2.08125em;
 margin-left:0.8em;
 width:7.2em;
">em; 
">Triassic
em;
 height:31.56875em;
 margin-left:0.8em;
 width:0.96em;
">em; 
">J<br>u<br>r<br>a<br>s<br>s<br>i<br>c
em;
 height:2.0em;
 margin-left:0.8em;
 width:7.2em;
">em; 
">Cretaceous
em;
 height:15.2em;
 margin-left:1.76em;
 width:1.0em;
">em; 
">Early
em;
 height:5.8625em;
 margin-left:1.76em;
 width:1.0em;
">em; 
">Middle
em;
 height:10.30625em;
 margin-left:1.76em;
 width:1.0em;
">em; 
">Late
em;
 height:1.025em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Hettangian
em;
 height:4.68125em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Sinemurian
em;
 height:4.45625em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Pliensbachian
em;
 height:4.7375em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Toarcian
em;
 height:2.0375em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Aalenian
em;
 height:1.025em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Bajocian
em;
 height:1.1375em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Bathonian
em;
 height:1.3625em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Callovian
em;
 height:3.3875em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Oxfordian
em;
 height:2.825em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Kimmeridgian
em;
 height:3.89375em;
 margin-left:2.76em;
 width:5.24em;
">em; 
">Tithonian
em;
 height:36em;
 margin-left:0.72em;
 width:0.08em;
">em; 
"> em;
 height:31.66875em;
 margin-left:1.76em;
 width:0.04em;
">em; 
"> em;
 height:31.66875em;
 margin-left:1.76em;
 width:0.04em;
">em; 
"> em;
 height:31.66875em;
 margin-left:2.72em;
 width:0.04em;
">em; 
"> </div Timeline>em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
"></div containsTSN></div Legend>An approximate timescale of key Jurassic events.<br>Vertical axis: millions of years ago.<br></div caption></div Container>
The Jurassic period is divided into the Early Jurassic, Middle, and Late Jurassic epochs. The Jurassic System, in stratigraphy, is divided into the Lower Jurassic, Middle, and Upper Jurassic series of rock formations, also known as "Lias", "Dogger" and "Malm" in Europe. The separation of the term Jurassic into three sections goes back to Leopold von Buch. The faunal stages from youngest to oldest are:
Paleogeography and tectonics.
During the early Jurassic period, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous period, when Gondwana itself rifted apart. The Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed.
The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic "lagerstätten" of Holzmaden and Solnhofen. In contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation.
The Jurassic was a time of calcite sea geochemistry in which low-magnesium calcite was the primary inorganic marine precipitate of calcium carbonate. Carbonate hardgrounds were thus very common, along with calcitic ooids, calcitic cements, and invertebrate faunas with dominantly calcitic skeletons (Stanley and Hardie, 1998, 1999).
The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.
In Africa, Early Jurassic strata are distributed in a similar fashion to Late Triassic beds, with more common outcrops in the south and less common fossil beds which are predominated by tracks to the north. As the Jurassic proceeded, larger and more iconic groups of dinosaurs like sauropods and ornithopods proliferated in Africa. Middle Jurassic strata are neither well represented nor well studied in Africa. Late Jurassic strata are also poorly represented apart from the spectacular Tendeguru fauna in Tanzania. The Late Jurassic life of Tendeguru is very similar to that found in western North America's Morrison Formation.
Fauna.
Aquatic and marine.
During the Jurassic period, the primary vertebrates living in the sea were fish and marine reptiles. The latter include ichthyosaurs, who were at the peak of their diversity, plesiosaurs, pliosaurs, and marine crocodiles of the families Teleosauridae and Metriorhynchidae. Numerous turtles could be found in lakes and rivers.
In the invertebrate world, several new groups appeared, including rudists (a reef-forming variety of bivalves) and belemnites. Calcareous sabellids ("Glomerula") appeared in the Early Jurassic. The Jurassic also had diverse encrusting and boring (sclerobiont) communities, and it saw a significant rise in the bioerosion of carbonate shells and hardgrounds. Especially common is the ichnogenus (trace fossil) "Gastrochaenolites".
During the Jurassic period, about four or five of the twelve clades of planktonic organisms that exist in the fossil record either experienced a massive evolutionary radiation or appeared for the first time.
Terrestrial.
On land, various archosaurian reptiles remained dominant. The Jurassic was a golden age for the large herbivorous dinosaurs known as the sauropods—"Camarasaurus", "Apatosaurus", "Diplodocus", "Brachiosaurus", and many others—that roamed the land late in the period; their mainstays were either the prairies of ferns, palm-like cycads and bennettitales, or the higher coniferous growth, according to their adaptations. They were preyed upon by large theropods, such as "Ceratosaurus", "Megalosaurus", "Torvosaurus" and "Allosaurus". All these belong to the 'lizard hipped' or saurischian branch of the dinosaurs.
During the Late Jurassic, the first avialans, like "Archaeopteryx", evolved from small coelurosaurian dinosaurs. Ornithischian dinosaurs were less predominant than saurischian dinosaurs, although some, like stegosaurs and small ornithopods, played important roles as small and medium-to-large (but not sauropod-sized) herbivores. In the air, pterosaurs were common; they ruled the skies, filling many ecological roles now taken by birds. Within the undergrowth were various types of early mammals, as well as tritylodonts, lizard-like sphenodonts, and early lissamphibians.
The rest of the Lissamphibia evolved in this period, introducing the first salamanders and caecilians.
Flora.
The arid, continental conditions characteristic of the Triassic steadily eased during the Jurassic period, especially at higher latitudes; the warm, humid climate allowed lush jungles to cover much of the landscape. Gymnosperms were relatively diverse during the Jurassic period. The Conifers in particular dominated the flora, as during the Triassic; they were the most diverse group and constituted the majority of large trees.
Extant conifer families that flourished during the Jurassic included the Araucariaceae, Cephalotaxaceae, Pinaceae, Podocarpaceae, Taxaceae and Taxodiaceae. The extinct Mesozoic conifer family Cheirolepidiaceae dominated low latitude vegetation, as did the shrubby Bennettitales. Cycads were also common, as were ginkgos and Dicksoniaceous tree ferns in the forest. Smaller ferns were probably the dominant undergrowth. Caytoniaceous seed ferns were another group of important plants during this time and are thought to have been shrub to small-tree sized. Ginkgo plants were particularly common in the mid- to high northern latitudes. In the Southern Hemisphere, podocarps were especially successful, while Ginkgos and Czekanowskiales were rare.
In the oceans, modern coralline algae appeared for the first time. However, they were a part of another major extinction that happened within the next major time period.
External links.
Listen to this article ()
This audio file was created from a revision of the "Jurassic" article dated 2010-08-19, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="15656" url="http://en.wikipedia.org/wiki?curid=15656" title="John Wyndham">
John Wyndham

John Wyndham Parkes Lucas Beynon Harris (; 10 July 1903 – 11 March 1969) was an English science fiction writer who usually used the pen name John Wyndham, although he also used other combinations of his names, such as John Beynon and Lucas Parkes. Many of his works were set in post-apocalyptic landscapes.
Biography.
Early life.
Wyndham was born in the village of Dorridge near Knowle in West Midlands, England, the son of George Beynon Harris, a barrister, and Gertrude Parkes, the daughter of a Birmingham ironmaster.
His early childhood was spent in Edgbaston in Birmingham, but when he was 8 years old his parents separated and he and his brother, the writer Vivian Beynon Harris, spent the rest of their childhood at a number of English preparatory and boarding schools, including Blundell's School in Tiverton, Devon during the First World War. His longest and final stay was at Bedales School near Petersfield in Hampshire (1918–21), which he left at the age of 18, and where he blossomed and was happy.
After leaving school, Wyndham tried several careers including farming, law, commercial art and advertising, but mostly relied on an allowance from his family. He eventually turned to writing for money in 1925, and by 1931 was selling short stories and serial fiction to American science fiction magazines, most under the pen names of "John Beynon" or "John Beynon Harris", although he also wrote some detective stories.
The Second World War.
During the Second World War Wyndham first served as a censor in the Ministry of Information, then joined the British Army, serving as a Corporal cipher operator in the Royal Corps of Signals. He participated in the Normandy landings, although was not involved in the first days of the operation.
Postwar.
After the war, Wyndham returned to writing, inspired by the success of his brother who had four novels published. He altered his writing style and, by 1951, using the John Wyndham pen name for the first time, wrote the novel "The Day of the Triffids". His pre-war writing career was not mentioned in the book's publicity, and people were allowed to assume that it was a first novel from a previously unknown writer.
The book proved to be an enormous success and established Wyndham as an important exponent of science fiction. During his lifetime he wrote and published six more novels under the name John Wyndham. In 1963 he married Grace Wilson, whom he had known for more than 20 years; the couple remained married until he died. He moved out of the Penn Club, London, and lived near Petersfield, Hampshire, just outside the grounds of Bedales School.
He died aged 65 at his home in Petersfield, survived by his wife and brother. Subsequently, some of his unsold work was published and his earlier work re-published. His archive was acquired by Liverpool University.
Books.
Critical reception.
His reputation rests mainly on the first four of the novels published in his lifetime as by John Wyndham. "The Day of the Triffids" remains his best-known, but some of his readers consider that "The Chrysalids" was really his best.
He also penned several short stories, ranging from hard science fiction to whimsical fantasy. A few have been filmed: "Consider Her Ways", "Random Quest", "Dumb Martian", "Jizzle" (filmed as "Maria") and "Time to Rest" (filmed as "No Place Like Earth"). There is also a radio version of "Survival".
Most of Wyndham's novels have a contemporary 1950s English middle-class setting. Brian Aldiss, another British science fiction writer, has disparagingly labelled some of them as "cosy catastrophes", especially his novel "The Day of the Triffids". The critic LJ Hurst dismissed Aldiss's accusations, pointing out that in "Triffids" the main character witnesses several murders, suicides, and misadventures, and is frequently in mortal danger himself.
References.
Notes
Citations
Bibliography

</doc>
