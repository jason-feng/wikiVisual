<doc id="18310" url="http://en.wikipedia.org/wiki?curid=18310" title="Lambda phage">
Lambda phage

Enterobacteria phage λ (lambda phage, coliphage λ) is a bacterial virus, or bacteriophage, that infects the bacterial species "Escherichia coli" ("E. coli"). It was discovered by Esther Lederberg in 1950 when she noticed that streaks of mixtures of two "E. coli" strains, one of which treated with ultraviolet light, was "nibbled and plaqued". This virus has a temperate lifecycle that allows it to either reside within the genome of its host through lysogeny or enter into a lytic phase (during which it kills and lyses the cell to produce offspring).
The phage particle consists of a head (also known as a capsid), a tail, and tail fibers (see image of virus below). The head contains the phage's double-strand linear DNA genome. During infection, the phage particle recognizes and binds to its host, "E. coli", causing DNA in the head of the phage to be ejected through the tail into the cytoplasm of the bacterial cell. Usually, a "lytic cycle" ensues, where the lambda DNA is replicated and new phage particles produced within the cell. This is followed by cell lysis, releasing the cell contents, including virions that have been assembled, into the environment. However, under certain conditions, the phage DNA may integrate itself into the host cell chromosome in the lysogenic pathway. In this state, the λ DNA is called a prophage and stays resident within the host's genome without apparent harm to the host. The host is termed a lysogen when a prophage is present. This prophage may enter the lytic cycle when the lysogen enters a stressed condition.
Anatomy.
The virus particle consists of a head and a tail that can have tail fibers. The whole particle consists of 12–14 different proteins with more than 1000 protein molecules total and one DNA molecule located in the phage head. However, it is still not entirely clear whether the L and M proteins are part of the virion.
The genome contains 48,490 base pairs of double-stranded, linear DNA, with 12-base single-strand segments at both 5' ends. These two single-stranded segments are the "sticky ends" of what is called the "cos" site. The "cos" site circularizes the DNA in the host cytoplasm. In its circular form, the phage genome, therefore, is 48,502 base pairs in length. The lambda genome can be inserted into the" E. coli" chromosome and is then called a prophage. See section below for details.
Life cycle.
Infection.
Lambda phage is a non-contractile tailed phage, meaning during an infection event it cannot 'force' its DNA through a bacterial cell membrane. It must instead use an existing pathway to invade the host cell, having evolved the tip of its tail to interact with a specific pore to allow entry of its DNA to the hosts.
N antitermination.
This occurs without the N protein interacting with the DNA; the protein instead binds to the freshly transcribed mRNA. Nut sites contain 3 conserved "boxes," of which only BoxB is essential.
Lytic life cycle.
This is the lifecycle that the phage follows following most infections, where the cII protein does not reach a high enough concentration due to degradation, so does not activate its promoters.
Rightward transcription.
Rightward transcription expresses the O, P and Q genes. O and P are responsible for initiating replication, and Q is another antiterminator that allows the expression of head, tail, and lysis genes from "PR’".
Q antitermination.
Q is similar to N in its effect: Q binds to RNA polymerase in "Qut" sites and the resulting complex can ignore terminators, however the mechanism is very different; the Q protein first associates with a DNA sequence rather than an mRNA sequence.
Leftward transcription.
Leftward transcription expresses the "gam", "red", "xis", and "int" genes. Gam and red proteins are involved in recombination. Gam is also important in that it inhibits the host RecBCD nuclease from degrading the 3’ ends in rolling circle replication. Int and xis are integration and excision proteins vital to lysogeny.
Lysogenic (or lysenogenic) life cycle.
The lysogenic lifecycle begins once the cII protein reaches a high enough concentration to activate its promoters, after a small number of infections.
The prophage is duplicated with every subsequent cell division of the host. The phage genes expressed in this dormant state code for proteins that repress expression of other phage genes (such as the structural and lysis genes) in order to prevent entry into the lytic cycle. These repressive proteins are broken down when the host cell is under stress, resulting in the expression of the repressed phage genes. Stress can be from starvation, poisons (like antibiotics), or other factors that can damage or destroy the host. In response to stress, the activated prophage is excised from the DNA of the host cell by one of the newly expressed gene products and enters its lytic pathway.
Prophage integration.
The integration of phage λ takes place at a special attachment site in the bacterial and phage genomes, called "attλ". The sequence of the bacterial att site is called "attB", between the "gal" and "bio" operons, and consists of the parts B-O-B', whereas the complementary sequence in the circular phage genome is called "attP" and consists of the parts P-O-P'. The integration itself is a sequential exchange (see genetic recombination) via a Holliday junction and requires both the phage protein Int and the bacterial protein IHF ("integration host factor"). Both Int and IHF bind to "attP" and form an intasome, a DNA-protein-complex designed for site-specific recombination of the phage and host DNA. The original B-O-B' sequence is changed by the integration to B-O-P'-phage DNA-P-O-B'. The phage DNA is now part of the host's genome.
Induction.
The classic induction of a lysogen involved irradiating the infected cells with UV light. Any situation where a lysogen undergoes DNA damage or the SOS response of the host is otherwise stimulated leads to induction.
Multiplicity reactivation and prophage reactivation.
Multiplicity reactivation (MR) is the process by which multiple viral genomes, each containing inactivating genome damage, interact within an infected cell to form a viable viral genome. MR was originally discovered with phage T4, but was subsequently found in phage λ (as well as in numerous other bacterial and mammalian viruses). MR of phage λ inactivated by UV light depends on the recombination function of either the host or of the infecting phage. Absence of both recombination systems leads to a loss of MR.
Survival of UV-irradiated phage λ is increased when the E. coli host is lysogenic for an homologous prophage, a phenomenon termed prophage reactivation. Prophage reactivation in phage λ appears to occur by a recombinational repair process similar to that of MR.
Repressor.
The repressor found in the phage lambda is a notable example of the level of control possible over gene expression by a very simple system. It forms a 'binary switch' with two genes under mutually exclusive expression, as discovered by Barbara J. Meyer.
The lambda repressor gene system consists of (from left to right on the chromosome):
The lambda repressor is a self assembling dimer also known as the cI protein. It binds DNA in the helix-turn-helix binding motif. It regulates the transcription of the cI protein and the Cro protein.
The life cycle of lambda phages is controlled by cI and Cro proteins. The lambda phage will remain in the lysogenic state if cI proteins predominate, but will be transformed into the lytic cycle if cro proteins predominate.
The cI dimer may bind to any of three operators, OR1, OR2, and OR3, in the order OR1 = OR2 > OR3.
Binding of a cI dimer to OR1 enhances binding of a second cI dimer to OR2, an effect called cooperativity. Thus, OR1 and OR2 are almost always simultaneously occupied by cI. However, this does not increase the affinity between cI and OR3, which will be occupied only when the cI concentration is high.
At high concentrations of cI, the dimers will also bind to operators OL1 and OL2 (which are over 2 kb downstream from the R operators). When cI dimers are bound to OL1, OL2, OR1, and OR2 a loop is induced in the DNA, allowing these dimers to bind together to form an octamer. This is a phenomenon called "long-range cooperativity". Upon formation of the octamer, cI dimers may cooperatively bind to OL3 and OR3, repressing transcription of cI. This "autonegative" regulation ensures a stable minimum concentration of the repressor molecule and, should SOS signals arise, allows for more efficient prophage induction.
Lytic or lysogenic?
An important distinction here is that between the two decisions; lysogeny and lysis on infection, and continuing lysogeny or lysis from a prophage. The latter is determined solely by the activation of RecA in the SOS response of the cell, as detailed in the section on induction. The former will also be affected by this; a cell undergoing an SOS response will always be lysed, as no cI protein will be allowed to build up. However, the initial lytic/lysogenic decision on infection is also dependent on the cII and cIII proteins.
In cells with sufficient nutrients, protease activity is high, which breaks down cII. This leads to the lytic lifestyle. In cells with limited nutrients, protease activity is low, making cII stable. This leads to the lysogenic lifestyle. cIII appears to stabilize cII, both directly and by acting as a competitive inhibitor to the relevant proteases. This means that a cell "in trouble", i.e. lacking in nutrients and in a more dormant state, is more likely to lysogenise. This would be selected for because the phage can now lie dormant in the bacterium until it falls on better times, and so the phage can create more copies of itself with the additional resources available and with the more likely proximity of further infectable cells.
A full biophysical model for lambda's lysis-lysogeny decision remains to be developed. Computer modeling and simulation suggest that random processes during infection drive the selection of lysis or lysogeny within individual cells. However, recent experiments suggest that physical differences among cells, that exist prior to infection, predetermine whether a cell will lyse or become a lysogen.
Lambda as a genetic tool.
Lambda phage has been used heavily as a model organism, and has been a rich source for useful tools in microbial genetics, and later in molecular genetics. Uses include its application as a vector for the cloning of recombinant DNA; the use of its site-specific recombinase (int) for the shuffling of cloned DNAs by the gateway method; and the application of its Red operon, including the proteins Red alpha (also called 'exo'), beta and gamma in the DNA engineering method called recombineering. Lambda phage has also been of major importance in the study of specialized transduction.

</doc>
<doc id="18313" url="http://en.wikipedia.org/wiki?curid=18313" title="Louis Armstrong">
Louis Armstrong

Louis Armstrong (August 4, 1901 – July 6, 1971), nicknamed Satchmo or Pops, was an American jazz trumpeter, singer, and an influential figure in jazz music.
Coming to prominence in the 1920s as an "inventive" trumpet and cornet player, Armstrong was a foundational influence in jazz, shifting the focus of the music from collective improvisation to solo performance. With his instantly recognizable gravelly voice, Armstrong was also an influential singer, demonstrating great dexterity as an improviser, bending the lyrics and melody of a song for expressive purposes. He was also skilled at scat singing (vocalizing using sounds and syllables instead of actual lyrics).
Renowned for his charismatic stage presence and voice almost as much as for his trumpet-playing, Armstrong's influence extends well beyond jazz music, and by the end of his career in the 1960s, he was widely regarded as a profound influence on popular music in general. Armstrong was one of the first truly popular African-American entertainers to "cross over", whose skin color was secondary to his music in an America that was severely racially divided. He rarely publicly politicized his race, often to the dismay of fellow African-Americans, but took a well-publicized stand for desegregation during the Little Rock Crisis. His artistry and personality allowed him socially acceptable access to the upper echelons of American society that were highly restricted for men of color.
Early life.
Armstrong often stated that he was born on July 4, 1900, a date that has been noted in many biographies. Although he died in 1971, it was not until the mid-1980s that his true birth date of August 4, 1901 was discovered by researcher Tad Jones through the examination of baptismal records. Armstrong was born into a very poor family in New Orleans, Louisiana, the grandson of slaves. He spent his youth in poverty, in a rough neighborhood, known as “the Battlefield”, which was part of the Storyville legal prostitution district. His father, William Armstrong (1881–1922), abandoned the family when Louis was an infant and took up with another woman. His mother, Mary "Mayann" Albert (1886–1927), then left Louis and his younger sister, Beatrice Armstrong Collins (1903–1987), in the care of his grandmother, Josephine Armstrong, and at times, his Uncle Isaac. At five, he moved back to live with his mother and her relatives, and saw his father only in parades. 
He attended the Fisk School for Boys, where he likely had early exposure to music. He brought in some money as a paperboy and also by finding discarded food and selling it to restaurants, but it was not enough to keep his mother from prostitution. He hung out in dance halls close to home, where he observed everything from licentious dancing to the quadrille. For extra money he also hauled coal to Storyville, the famed red-light district, and listened to the bands playing in the brothels and dance halls, especially Pete Lala's where Joe "King" Oliver performed and other famous musicians would drop in to jam.
After dropping out of the Fisk School at age eleven, Armstrong joined a quartet of boys who sang in the streets for money. But he also started to get into trouble. Cornet player Bunk Johnson said he taught Armstrong (then 11) to play by ear at Dago Tony's Tonk in New Orleans, although in his later years Armstrong gave the credit to Oliver. Armstrong hardly looked back at his youth as the worst of times but instead drew inspiration from it, “Every time I close my eyes blowing that trumpet of mine—I look right in the heart of good old New Orleans... It has given me something to live for.”
He also worked for a Lithuanian-Jewish immigrant family, the Karnofskys, who had a junk hauling business and gave him odd jobs. They took him in and treated him as almost a family member, knowing he lived without a father, and would feed and nurture him. He later wrote a memoir of his relationship with the Karnofskys titled, "Louis Armstrong + the Jewish Family in New Orleans, La., the Year of 1907." In it he describes his discovery that this family was also subject to discrimination by "other white folks' nationalities who felt that they were better than the Jewish race... I was only seven years old but I could easily see the ungodly treatment that the White Folks were handing the poor Jewish family whom I worked for."
Armstrong wore a Star of David pendant for the rest of his life and wrote about what he learned from them: "how to live—real life and determination." The influence of Karnofsky is remembered in New Orleans by the Karnofsky Project, a non-profit organization dedicated to accepting donated musical instruments to "put them into the hands of an eager child who could not otherwise take part in a wonderful learning experience."
Armstrong developed his cornet playing skills by playing in the band of the New Orleans Home for Colored Waifs, where he had been sent multiple times for general delinquency, most notably for a long term after firing his stepfather's pistol into the air at a New Year's Eve celebration, as police records confirm. Professor Peter Davis (who frequently appeared at the Home at the request of its administrator, Captain Joseph Jones) instilled discipline in and provided musical training to the otherwise self-taught Armstrong. Eventually, Davis made Armstrong the band leader. The Home band played around New Orleans and the thirteen-year-old Louis began to draw attention by his cornet playing, starting him on a musical career. At fourteen he was released from the Home, living again with his father and new stepmother and then back with his mother and also back to the streets and their temptations. Armstrong got his first dance hall job at Henry Ponce’s where Black Benny became his protector and guide. He hauled coal by day and played his cornet at night.
He played in the city's frequent brass band parades and listened to older musicians every chance he got, learning from Bunk Johnson, Buddy Petit, Kid Ory, and above all, Joe "King" Oliver, who acted as a mentor and father figure to the young musician. Later, he played in the brass bands and riverboats of New Orleans, and began traveling with the well-regarded band of Fate Marable, which toured on a steamboat up and down the Mississippi River. He described his time with Marable as "going to the University," since it gave him a much wider experience working with written arrangements.
In 1919, Joe Oliver decided to go north and resigned his position in Kid Ory's band; Armstrong replaced him. He also became second trumpet for the Tuxedo Brass Band, a society band.
Career.
Through all his riverboat experience Armstrong’s musicianship began to mature and expand. At twenty, he could read music and he started to be featured in extended trumpet solos, one of the first jazzmen to do this, injecting his own personality and style into his solo turns. He had learned how to create a unique sound and also started using singing and patter in his performances. In 1922, Armstrong joined the exodus to Chicago, where he had been invited by his mentor, Joe "King" Oliver, to join his Creole Jazz Band and where he could make a sufficient income so that he no longer needed to supplement his music with day labor jobs. It was a boom time in Chicago and though race relations were poor, the “Windy City” was teeming with jobs for black people, who were making good wages in factories and had plenty to spend on entertainment.
Oliver's band was the best and most influential hot jazz band in Chicago in the early 1920s, at a time when Chicago was the center of the jazz universe. Armstrong lived luxuriously in Chicago, in his own apartment with his own private bath (his first). Excited as he was to be in Chicago, he began his career-long pastime of writing nostalgic letters to friends in New Orleans. As Armstrong’s reputation grew, he was challenged to “cutting contests” by hornmen trying to displace the new phenom, who could blow two hundred high C’s in a row. Armstrong made his first recordings on the Gennett and Okeh labels (jazz records were starting to boom across the country), including taking some solos and breaks, while playing second cornet in Oliver's band in 1923. At this time, he met Hoagy Carmichael (with whom he would collaborate later) who was introduced by friend Bix Beiderbecke, who now had his own Chicago band.
Armstrong enjoyed working with Oliver, but Louis' second wife, pianist Lil Hardin Armstrong, urged him to seek more prominent billing and develop his newer style away from the influence of Oliver. Armstrong took the advice of his wife and left Oliver's band. For a year Armstrong played in Fletcher Henderson's band in New York on many recordings. After playing in New York, Armstrong returned to Chicago, playing in large orchestras; there he created his most important early recordings. Lil had her husband play classical music in church concerts to broaden his skill and improve his solo play and she prodded him into wearing more stylish attire to make him look sharp and to better offset his growing girth. Lil’s influence eventually undermined Armstrong’s relationship with his mentor, especially concerning his salary and additional moneys that Oliver held back from Armstrong and other band members. Armstrong and Oliver parted amicably in 1924. Shortly afterward, Armstrong received an invitation to go to New York City to play with the Fletcher Henderson Orchestra, the top African-American band of the day. Armstrong switched to the trumpet to blend in better with the other musicians in his section. His influence upon Henderson's tenor sax soloist, Coleman Hawkins, can be judged by listening to the records made by the band during this period.
Armstrong quickly adapted to the more tightly controlled style of Henderson, playing trumpet and even experimenting with the trombone and the other members quickly took up Armstrong’s emotional, expressive pulse. Soon his act included singing and telling tales of New Orleans characters, especially preachers. The Henderson Orchestra was playing in the best venues for white-only patrons, including the famed Roseland Ballroom, featuring the classy arrangements of Don Redman. Duke Ellington’s orchestra would go to Roseland to catch Armstrong’s performances and young hornmen around town tried in vain to outplay him, splitting their lips in their attempts.
During this time, Armstrong made many recordings on the side, arranged by an old friend from New Orleans, pianist Clarence Williams; these included small jazz band sides with the Williams Blue Five (some of the best pairing Armstrong with one of Armstrong's few rivals in fiery technique and ideas, Sidney Bechet) and a series of accompaniments with blues singers, including Bessie Smith, Ma Rainey, and Alberta Hunter.
Armstrong returned to Chicago in 1925 due mostly to the urging of his wife, who wanted to pump up Armstrong’s career and income. He was content in New York but later would concede that she was right and that the Henderson Orchestra was limiting his artistic growth. In publicity, much to his chagrin, she billed him as “the World’s Greatest Trumpet Player”. At first, he was actually a member of the Lil Hardin Armstrong Band and working for his wife. He began recording under his own name for Okeh with his famous Hot Five and Hot Seven groups, producing hits such as "Potato Head Blues", "Muggles", (a reference to marijuana, for which Armstrong had a lifelong fondness), and "West End Blues", the music of which set the standard and the agenda for jazz for many years to come.
The group included Kid Ory (trombone), Johnny Dodds (clarinet), Johnny St. Cyr (banjo), wife Lil on piano, and usually no drummer. Armstrong’s bandleading style was easygoing, as St. Cyr noted, "One felt so relaxed working with him, and he was very broad-minded . . . always did his best to feature each individual." His recordings soon after with pianist Earl "Fatha" Hines (most famously their 1928 "Weatherbird" duet) and Armstrong's trumpet introduction to "West End Blues" remain some of the most famous and influential improvisations in jazz history. Armstrong was now free to develop his personal style as he wished, which included a heavy dose of effervescent jive, such as "whip that thing, Miss Lil" and "Mr. Johnny Dodds, Aw, do that clarinet, boy!"
Armstrong also played with Erskine Tate’s Little Symphony, actually a quintet, which played mostly at the Vendome Theatre. They furnished music for silent movies and live shows, including jazz versions of classical music, such as "Madame Butterfly," which gave Armstrong experience with longer forms of music and with hosting before a large audience. He began to scat sing (improvised vocal jazz using nonsensical words) and was among the first to record it, on "Heebie Jeebies" in 1926. The recording was so popular that the group became the most famous jazz band in the United States, even though they had not performed live to any great extent. Young musicians across the country, black or white, were turned on by Armstrong’s new type of jazz.
After separating from Lil, Armstrong started to play at the Sunset Café for Al Capone's associate Joe Glaser in the Carroll Dickerson Orchestra, with Earl Hines on piano, which was soon renamed "Louis Armstrong and his Stompers", though Hines was the music director and Glaser managed the orchestra. Hines and Armstrong became fast friends and successful collaborators.
Armstrong returned to New York, in 1929, where he played in the pit orchestra of the successful musical "Hot Chocolate", an all-black revue written by Andy Razaf and pianist/composer Fats Waller. He also made a cameo appearance as a vocalist, regularly stealing the show with his rendition of "Ain't Misbehavin'", his version of the song becoming his biggest selling record to date.
Armstrong started to work at Connie's Inn in Harlem, chief rival to the Cotton Club, a venue for elaborately staged floor shows, and a front for gangster Dutch Schultz. Armstrong also had considerable success with vocal recordings, including versions of famous songs composed by his old friend Hoagy Carmichael. His 1930s recordings took full advantage of the new RCA ribbon microphone, introduced in 1931, which imparted a characteristic warmth to vocals and immediately became an intrinsic part of the 'crooning' sound of artists like Bing Crosby. Armstrong's famous interpretation of Hoagy Carmichael's "Stardust" became one of the most successful versions of this song ever recorded, showcasing Armstrong's unique vocal sound and style and his innovative approach to singing songs that had already become standards.
Armstrong's radical re-working of Sidney Arodin and Carmichael's "Lazy River" (recorded in 1931) encapsulated many features of his groundbreaking approach to melody and phrasing. The song begins with a brief trumpet solo, then the main melody is stated by sobbing horns, memorably punctuated by Armstrong's growling interjections at the end of each bar: "Yeah! ..."Uh-huh" ..."Sure" ... "Way down, way down." In the first verse, he ignores the notated melody entirely and sings as if playing a trumpet solo, pitching most of the first line on a single note and using strongly syncopated phrasing. In the second stanza he breaks into an almost fully improvised melody, which then evolves into a classic passage of Armstrong "scat singing".
As with his trumpet playing, Armstrong's vocal innovations served as a foundation stone for the art of jazz vocal interpretation. The uniquely gritty coloration of his voice became a musical archetype that was much imitated and endlessly impersonated. His scat singing style was enriched by his matchless experience as a trumpet soloist. His resonant, velvety lower-register tone and bubbling cadences on sides such as "Lazy River" exerted a huge influence on younger white singers such as Bing Crosby.
The Great Depression of the early 1930s was especially hard on the jazz scene. The Cotton Club closed in 1936 after a long downward spiral, and many musicians stopped playing altogether as club dates evaporated. Bix Beiderbecke died and Fletcher Henderson’s band broke up. King Oliver made a few records but otherwise struggled. Sidney Bechet became a tailor and Kid Ory returned to New Orleans and raised chickens.
Armstrong moved to Los Angeles in 1930 to seek new opportunities. He played at the New Cotton Club in Los Angeles with Lionel Hampton on drums. The band drew the Hollywood crowd, which could still afford a lavish night life, while radio broadcasts from the club connected with younger audiences at home. Bing Crosby and many other celebrities were regulars at the club. In 1931, Armstrong appeared in his first movie, "Ex-Flame". Armstrong was convicted of marijuana possession but received a suspended sentence. He returned to Chicago in late 1931 and played in bands more in the Guy Lombardo vein and he recorded more standards. When the mob insisted that he get out of town, Armstrong visited New Orleans, got a hero’s welcome and saw old friends. He sponsored a local baseball team known as “Armstrong’s Secret Nine” and had a cigar named after him. But soon he was on the road again and after a tour across the country shadowed by the mob, Armstrong decided to go to Europe to escape.
After returning to the United States, he undertook several exhausting tours. His agent Johnny Collins’ erratic behavior and his own spending ways left Armstrong short of cash. Breach of contract violations plagued him. Finally, he hired Joe Glaser as his new manager, a tough mob-connected wheeler-dealer, who began to straighten out his legal mess, his mob troubles, and his debts. Armstrong also began to experience problems with his fingers and lips, which were aggravated by his unorthodox playing style. As a result he branched out, developing his vocal style and making his first theatrical appearances. He appeared in movies again, including Crosby's 1936 hit "Pennies from Heaven". In 1937, Armstrong substituted for Rudy Vallee on the CBS radio network and became the first African American to host a sponsored, national broadcast.
After spending many years on the road, Armstrong settled permanently in Queens, New York in 1943 in contentment with his fourth wife, Lucille. Although subject to the vicissitudes of Tin Pan Alley and the gangster-ridden music business, as well as anti-black prejudice, he continued to develop his playing. He recorded Hoagy Carmichael's Rockin' Chair for Okeh Records.
During the subsequent thirty years, Armstrong played more than three hundred gigs a year. Bookings for big bands tapered off during the 1940s due to changes in public tastes: ballrooms closed, and there was competition from television and from other types of music becoming more popular than big band music. It became impossible under such circumstances to support and finance a 16-piece touring band.
The All Stars.
Following a highly successful small-group jazz concert at New York Town Hall on May 17, 1947, featuring Armstrong with trombonist/singer Jack Teagarden, Armstrong's manager Joe Glaser dissolved the Armstrong big band on August 13, 1947 and established a six-piece small group featuring Armstrong with (initially) Teagarden, Earl Hines and other top swing and dixieland musicians, most of them ex-big band leaders. The new group was announced at the opening of Billy Berg's Supper Club.
This group was called Louis Armstrong and his All Stars and included at various times Earl "Fatha" Hines, Barney Bigard, Edmond Hall, Jack Teagarden, Trummy Young, Arvell Shaw, Billy Kyle, Marty Napoleon, Big Sid Catlett, Cozy Cole, Tyree Glenn, Barrett Deems, Joe Darensbourg and the Filipino-American percussionist Danny Barcelona. During this period, Armstrong made many recordings and appeared in over thirty films. He was the first jazz musician to appear on the cover of "Time Magazine" on February 21, 1949.)
In 1948, he participated in the Nice Jazz Festival where Suzy Delair sings for the first time in public "C'est si bon" by Henri Betti and André Hornez. Love this song, Armstrong asked the editor if he can make a recording in America what the publisher allows it. Armstrong recorded the first American version of "C'est si bon" June 26, 1950 in New York with English lyrics by Jerry Seelen. On its release, the disc becomes a worldwide success.
In 1964, he recorded his biggest-selling record, "Hello, Dolly!", a song by Jerry Herman, originally sung by Carol Channing. Armstrong's cover of the song, which lasted 22 weeks on the Hot 100, longer than any other record that year, went to No. 1 on the pop chart, making Armstrong (age 62 years, 9 months, 5 days) the oldest person to date to ever accomplish that feat. In the process, Armstrong dislodged The Beatles from the No. 1 position they had occupied for 14 consecutive weeks with three different songs.
Armstrong kept up his busy tour schedule until a few years before his death in 1971. In his later years he would sometimes play some of his numerous gigs by rote, but other times would enliven the most mundane gig with his vigorous playing, often to the astonishment of his band. He also toured Africa, Europe, and Asia under sponsorship of the US State Department with great success, earning the nickname "Ambassador Satch " and inspiring Dave Brubeck to compose his jazz musical "The Real Ambassadors" 
While failing health restricted his schedule in his last years, within those limitations he continued playing until the day he died.
Death.
Armstrong died of a heart attack in his sleep on July 6, 1971, a month before his 70th birthday, 11 months after playing a famous show at the Waldorf-Astoria's Empire Room. He was residing in Corona, Queens, New York City, at the time of his death. He was interred in Flushing Cemetery, Flushing, in Queens, New York City. 
His honorary pallbearers included Bing Crosby, Ella Fitzgerald, Dizzy Gillespie, Pearl Bailey, Count Basie, Harry James, Frank Sinatra, Ed Sullivan, Earl Wilson, Alan King, Johnny Carson and David Frost. Peggy Lee sang The Lord's Prayer at the services while Al Hibbler sang "Nobody Knows the Trouble I've Seen" and Fred Robbins, a long-time friend, gave the eulogy.
Personal life.
Pronunciation of name.
The Louis Armstrong House Museum website states:
Judging from home recorded tapes now in our Museum Collections, Louis pronounced his own name as “Lewis.” On his 1964 record “Hello, Dolly,” he sings, “This is Lewis, Dolly” but in 1933 he made a record called “Laughin’ Louie.” Many broadcast announcers, fans, and acquaintances called him “Louie” and in a videotaped interview from 1983 Lucille Armstrong calls her late husband “Louie” as well. Musicians and close friends usually called him “Pops.”
 In a memoir written for Robert Goffin between 1943 and 1944, Armstrong states, "All white folks call me Louie," suggesting that he himself did not. That said, Armstrong was registered as "Lewie" for the 1920 U.S. Census. On various live records he's called "Louie" on stage, such as on the 1952 "Can Anyone Explain?" from the live album "In Scandinavia vol.1". It should also be noted that "Lewie" is the French pronunciation of "Louis" and is commonly used in Louisiana.
Family.
On March 19, 1918, Louis married Daisy Parker, a prostitute from Gretna, Louisiana. They adopted a 3-year-old boy, Clarence Armstrong, whose mother, Louis' cousin Flora, died soon after giving birth. Clarence Armstrong was mentally disabled (the result of a head injury at an early age) and Louis would spend the rest of his life taking care of him. Louis' marriage to Parker failed quickly and they separated in 1923.
On February 4, 1924, Louis married Lil Hardin Armstrong, who was Oliver's pianist and had also divorced her first spouse only a few years earlier. His second wife was instrumental in developing his career, but in the late 1920s Hardin and Louis grew apart. They separated in 1931 and divorced in 1938, after which Louis married longtime girlfriend Alpha Smith. His marriage to his third wife lasted four years, and they divorced in 1942. Louis then married Lucille Wilson, a singer at the Cotton Club, to whom he was married until his death in 1971.
Armstrong's marriages never produced any offspring, though he loved children. However, in December 2012, 57-year-old Sharon Preston-Folta claimed to be his daughter, from a 1950s affair between Armstrong and Lucille "Sweets" Preston, a dancer at the Cotton Club. In a 1955 letter to his manager, Joe Glaser, Armstrong affirmed his belief that Preston's newborn baby was his daughter, and ordered Glaser to pay a monthly allowance of $400 to mother and child.
Personality.
Armstrong was noted for his colorful and charismatic personality. His own biography vexed some biographers and historians, as he had a habit of telling tales, particularly of his early childhood, when he was less scrutinized, and his embellishments of his history often lack consistency.
He was not only an entertainer, Armstrong was also a leading personality of the day. He was beloved by an American public that gave even the greatest African American performers little access beyond their public celebrity, and he was able to live a private life of access and privilege accorded to few other African Americans during that era.
He generally remained politically neutral, which at times alienated him from members of the black community who looked to him to use his prominence with white America to become more of an outspoken figure during the Civil Rights Era of U.S. history.
Nicknames.
The nicknames Satchmo and Satch are short for Satchelmouth. Like many things in Armstrong's life, which was filled with colorful stories both real and imagined, many of his own telling, the nickname has many possible origins.
The most common tale that biographers tell is the story of Armstrong as a young boy dancing for pennies in the streets of New Orleans, who would scoop up the coins off of the streets and stick them into his mouth to avoid having the bigger children steal them from him. Someone dubbed him "satchel mouth" for his mouth acting as a satchel. Another tale is that because of his large mouth, he was nicknamed "satchel mouth" which became shortened to Satchmo.
Early on he was also known as Dipper, short for Dippermouth, a reference to the piece "Dippermouth Blues". and something of a riff on his unusual embouchure.
The nickname Pops came from Armstrong's own tendency to forget people's names and simply call them "pops" instead. The nickname was soon turned on Armstrong himself. It was used as the title of a 2010 biography of Armstrong by Terry Teachout.They also called him the king of jazz.
Armstrong and race.
Armstrong was largely accepted into white society, both on stage and off, a privilege reserved for very few African-American public figures, and usually those of either exceptional talent or fair skin tone. As his fame grew, so did his access to the finer things in life usually denied to a black man, even a famous one. His renown was such that he dined in the best restaurants and stayed in hotels usually exclusively for whites.
It was a power and privilege that he enjoyed, although he was very careful not to flaunt it with fellow performers of color, and privately, he shared what access that he could with friends and fellow musicians.
That still did not prevent members of the African-American community, particularly in the late 1950s to the early 1970s, from calling him an "Uncle Tom", a black-on-black racial epithet for someone who kowtowed to white society at the expense of their own racial identity. Billie Holiday countered, however, "Of course Pops toms, but he toms from the heart."
He was criticized for accepting the title of "King of The Zulus" for Mardi Gras in 1949. In the New Orleans African-American community it is an honored role as the head of leading black Carnival Krewe, but bewildering or offensive to outsiders with their traditional costume of grass-skirts and blackface makeup satirizing southern white attitudes.
Some musicians criticized Armstrong for playing in front of segregated audiences, and for not taking a strong enough stand in the civil rights movement.
The few exceptions made it more effective when he did speak out. Armstrong's criticism of President Eisenhower, calling him "two-faced" and "gutless" because of his inaction during the conflict over school desegregation in Little Rock, Arkansas in 1957 made national news.
As a protest, Armstrong canceled a planned tour of the Soviet Union on behalf of the State Department saying "The way they're treating my people in the South, the government can go to hell" and that he could not represent his government abroad when it was in conflict with its own people. Six days after Armstrong's comments, Eisenhower ordered Federal troops to Little Rock to escort students into the school.
The FBI kept a file on Armstrong, for his outspokenness about integration.
Religion.
When asked about his religion, Armstrong would answer that he was raised a Baptist, always wore a Star of David, and was friends with the Pope. Armstrong wore the Star of David in honor of the Karnofsky family, who took him in as a child and lent him the money to buy his first cornet. Louis Armstrong was, in fact, baptized as a Catholic at the Sacred Heart of Jesus Church in New Orleans, and he met popes Pius XII and Paul VI, though there is no evidence that he considered himself Catholic. Armstrong seems to have been tolerant towards various religions, but also found humor in them.
Personal habits.
Purging.
Armstrong was also greatly concerned with his health. He made frequent use of laxatives as a means of controlling his weight, a practice he advocated both to personal acquaintances and in the diet plans he published under the title "Lose Weight the Satchmo Way". Armstrong's laxative of preference in his younger days was Pluto Water, but he then became an enthusiastic convert when he discovered the herbal remedy Swiss Kriss. He would extol its virtues to anyone who would listen and pass out packets to everyone he encountered, including members of the British Royal Family. (Armstrong also appeared in humorous, albeit risqué, cards that he had printed to send out to friends; the cards bore a picture of him sitting on a toilet—as viewed through a keyhole—with the slogan "Satch says, 'Leave it all behind ya!'") The cards have sometimes been incorrectly described as ads for Swiss Kriss.
In a live recording of "Baby, It's Cold Outside" with Velma Middleton, he changes the lyric from "Put another record on while I pour" to "Take some Swiss Kriss while I pour."
Love of food.
The concern with his health and weight was balanced by his love of food, reflected in such songs as "Cheesecake", "Cornet Chop Suey," though "Struttin’ with Some Barbecue" was written about a fine-looking companion, not about food. He kept a strong connection throughout his life to the cooking of New Orleans, always signing his letters, "Red beans and ricely yours..."
Writings.
Armstrong’s gregariousness extended to writing. On the road, he wrote constantly, sharing favorite themes of his life with correspondents around the world. He avidly typed or wrote on whatever stationery was at hand, recording instant takes on music, sex, food, childhood memories, his heavy "medicinal" marijuana use—and even his bowel movements, which he gleefully described. He had a fondness for lewd jokes and dirty limericks as well.
Social organizations.
Louis Armstrong was not, as is often claimed, a Freemason. Although he is usually listed as being a member of Montgomery Lodge No. 18 (Prince Hall) in New York, no such lodge has ever existed. Armstrong states in his autobiography, however, that he was a member of the Knights of Pythias, which is not a Masonic group.
Music.
Horn playing and early jazz.
In his early years, Armstrong was best known for his virtuosity with the cornet and trumpet. The greatest trumpet playing of his early years can be heard on his Hot Five and Hot Seven records, as well as the Red Onion Jazz Babies. Armstrong's improvisations were daring and sophisticated for the time, while often subtle and melodic.
He often essentially re-composed pop-tunes he played, making them more interesting. Armstrong's playing is filled with original melodies, creative leaps, and subtle relaxed or driving rhythms. Armstrong's playing technique, honed by constant practice, extended the range, tone and capabilities of the trumpet. In these records, Armstrong almost single-handedly created the role of the jazz soloist, taking what was essentially a collective folk music and turning it into an art form with tremendous possibilities for individual expression.
Armstrong's work in the 1920s shows him playing at the outer limits of his abilities. The Hot Five records, especially, often have minor flubs and missed notes, which do little to detract from listening enjoyment since the energy of the spontaneous performance comes through. By the mid-1930s, Armstrong achieved a smooth assurance, knowing exactly what he could do and carrying out his ideas to perfection.
He was one of the first artists to use recordings of his performances to improve himself. Armstrong was an avid audiophile. He had a large collection of recordings, including reel-to-reel tapes, which he took on the road with him in a trunk during his later career. He enjoyed listening to his own recordings, and comparing his performances musically. In the den of his home, he had the latest audio equipment and would sometimes rehearse and record along with his older recordings or the radio.
Vocal popularity.
As his music progressed and popularity grew, his singing also became very important. Armstrong was not the first to record scat singing, but he was masterful at it and helped popularize it. He had a hit with his playing and scat singing on "Heebie Jeebies" when, according to some legends, the sheet music fell on the floor and he simply started singing nonsense syllables. Armstrong stated in his memoirs that this actually occurred. He also sang out "I done forgot the words" in the middle of recording "I'm A Ding Dong Daddy From Dumas."
Such records were hits and scat singing became a major part of his performances. Long before this, however, Armstrong was playing around with his vocals, shortening and lengthening phrases, interjecting improvisations, using his voice as creatively as his trumpet.
Colleagues and followers.
During his long career he played and sang with some of the most important instrumentalists and vocalists of the time; among them were Bing Crosby, Duke Ellington, Fletcher Henderson, Earl Hines, the singing brakeman Jimmie Rodgers, Bessie Smith and perhaps most famously Ella Fitzgerald.
His influence upon Bing Crosby is particularly important with regard to the subsequent development of popular music: Crosby admired and copied Armstrong, as is evident on many of his early recordings, notably "Just One More Chance" (1931). The "New Grove Dictionary of Jazz" describes Crosby's debt to Armstrong in precise detail, although it does not acknowledge Armstrong by name:
Crosby... was important in introducing into the mainstream of popular singing an Afro-American concept of song as a lyrical extension of speech... His techniques—easing the weight of the breath on the vocal cords, passing into a head voice at a low register, using forward production to aid distinct enunciation, singing on consonants (a practice of black singers), and making discreet use of appoggiaturas, mordents, and slurs to emphasize the text—were emulated by nearly all later popular singers.
Armstrong recorded two albums with Ella Fitzgerald: "Ella and Louis", and "Ella and Louis Again" for Verve Records, with the sessions featuring the backing musicianship of the Oscar Peterson Trio and drummers Buddy Rich (on the first album), and Louie Bellson (on the second). Norman Granz then had the vision for Ella and Louis to record "Porgy and Bess" which is the most famous and critically acclaimed version of the Gerswhin brothers' masterpiece.
His recordings for Columbia Records, "Louis Armstrong Plays W.C. Handy" (1954) and "Satch Plays Fats" (all Fats Waller tunes) (1955) were both being considered masterpieces, as well as moderately well selling. In 1961 the All Stars participated in two albums - "The Great Summit" and "The Great Reunion" (now together as a single disc) with Duke Ellington. The albums feature many of Ellington's most famous compositions (as well as two exclusive cuts) with Duke sitting in on piano. His participation in Dave Brubeck's high-concept jazz musical "The Real Ambassadors" (1963) was critically acclaimed, and features "Summer Song," one of Armstrong's most popular vocal efforts.
In 1964 his recording of the song "Hello Dolly" went to number one. An album of the same title was quickly created around the song, and also shot to number one (knocking The Beatles off the top of the chart). The album sold very well for the rest of the year, quickly going "Gold" (500,000). His performance of "Hello Dolly" won for best male pop vocal performance at the 1964 Grammy Awards.
Hits and later career.
Armstrong had many hit records including "Stardust", "What a Wonderful World", "When The Saints Go Marching In", "Dream a Little Dream of Me", "Ain't Misbehavin'", "You Rascal You", and "Stompin' at the Savoy". "We Have All the Time in the World" was featured on the soundtrack of the James Bond film "On Her Majesty's Secret Service", and enjoyed renewed popularity in the UK in 1994 when it featured on a Guinness advert. It reached number 3 in the charts on being re-released.
In 1964, Armstrong knocked The Beatles off the top of the "Billboard" Hot 100 chart with "Hello, Dolly!", which gave the 63-year-old performer a U.S. record as the oldest artist to have a number one song. His 1964 song "Bout Time" was later featured in the film "Bewitched".
Armstrong performed in Italy at the 1968 Sanremo Music Festival where he sang "Mi Va di Cantare" alongside his friend, the Eritrean-born Italian singer Lara Saint Paul. In February 1968, he also appeared with Lara Saint Paul on the Italian RAI television channel where he performed "Grassa e Bella," a track he sang in Italian for the Italian market and C.D.I. label.
In 1968, Armstrong scored one last popular hit in the United Kingdom with "What a Wonderful World", which topped the British charts for a month; however, the single did not chart at all in America. The song gained greater currency in the popular consciousness when it was used in the 1987 movie "Good Morning, Vietnam", its subsequent re-release topping many charts around the world. Armstrong even appeared on the October 28, 1970, "Johnny Cash Show", where he sang Nat King Cole's hit "Rambling Rose" and joined Cash to re-create his performance backing Jimmie Rodgers on "Blue Yodel No. 9".
Stylistic range.
Armstrong enjoyed many types of music, from blues to the arrangements of Guy Lombardo, to Latin American folksongs, to classical symphonies and opera. Armstrong incorporated influences from all these sources into his performances, sometimes to the bewilderment of fans who wanted him to stay in convenient narrow categories. Armstrong was inducted into the Rock and Roll Hall of Fame as an "early influence". Some of his solos from the 1950s, such as the hard rocking version of "St. Louis Blues" from the "WC Handy" album, show that the influence went in both directions.
Literature, radio, films and TV.
Armstrong appeared in more than a dozen Hollywood films, usually playing a band leader or musician. His most familiar role was as the bandleader "cum" narrator in the 1956 musical, "High Society", in which he sang the title song and performed a duet with Bing Crosby on "Now You Has Jazz". In 1947, he played himself in the movie "New Orleans" opposite Billie Holiday, which chronicled the demise of the Storyville district and the ensuing exodus of musicians from New Orleans to Chicago. In the 1959 film, The Five Pennies (the story of the cornetist Red Nichols), Armstrong played himself as well as singing and playing several classic numbers. With Danny Kaye Armstrong performed a duet of "When the Saints Go Marching In" during which Kaye impersonated Armstrong. Armstrong also had a part in the film alongside James Stewart in "The Glenn Miller Story" in which Glenn (played by Stewart) jammed with Armstrong and a few other noted musicians of the time.
He was the first African American to host a nationally broadcast radio show in the 1930s. In 1969, Armstrong had a cameo role in the film version of "Hello, Dolly!" as the bandleader, Louis, to which he sang the title song with actress Barbra Streisand. His solo recording of "Hello, Dolly!" is one of his most recognizable performances.
He was heard on such radio programs as "The Story of Swing" (1937) and "This Is Jazz" (1947), and he also made countless television appearances, especially in the 1950s and 1960s, including appearances on "The Tonight Show Starring Johnny Carson".
Many of Armstrong's recordings remain popular. More than four decades since his death, a larger number of his recordings from all periods of his career are more widely available than at any time during his lifetime. His songs are broadcast and listened to every day throughout the world, and are honored in various movies, TV series, commercials, and even anime and video games. "A Kiss to Build a Dream On" was included in the video game "Fallout 2", accompanying the intro cinematic. It was also used in the 1993 film "Sleepless in Seattle" and the 2005 film "Lord of War". "Melancholy Blues," performed by Armstrong and his Hot Seven was included on the Voyager Golden Record sent into outer space to represent one of the greatest achievements of humanity. Most familiar to modern listeners is his ubiquitous rendition of "What a Wonderful World". In 2008, Armstrong's recording of Edith Piaf's famous "La Vie En Rose" was used in a scene of the popular Disney/Pixar film "WALL-E". The song was also used in parts, especially the opening trumpets, in the French film "Jeux d'enfants" ("Love Me If You Dare".)
Argentine writer Julio Cortázar, a self-described Armstrong admirer, asserted that a 1952 Louis Armstrong concert at the Théâtre des Champs-Élysées in Paris played a significant role in inspiring him to create the fictional creatures called Cronopios that are the subject of a number of Cortázar's short stories. Cortázar once called Armstrong himself "Grandísimo Cronopio" (The Great Cronopio).
Armstrong appears as a minor fictionalized character in Harry Turtledove's Southern Victory Series. When he and his band escape from a Nazi-like Confederacy, they enhance the insipid mainstream music of the North. A young Armstrong also appears as a minor fictionalized character in Patrick Neate's 2001 novel "Twelve Bar Blues", part of which is set in New Orleans, and which was a winner at that year's Whitbread Book Awards.
There is a pivotal scene in "Stardust Memories" (1980) in which Woody Allen is overwhelmed by a recording of Armstrong's "Stardust" and experiences a nostalgic epiphany. The combination of the music and the perfect moment is the catalyst for much of the film's action, prompting the protagonist to fall in love with an ill-advised woman.
Terry Teachout wrote a one-man play about Armstrong called "Satchmo at the Waldorf" that was premiered in 2011 in Orlando, Fla., and has since been produced by Shakespeare & Company, Long Wharf Theater, and the Wilma Theater. The production ran off Broadway in 2014.
Awards and honors.
Grammy Awards.
Armstrong was posthumously awarded the Grammy Lifetime Achievement Award in 1972 by the Academy of Recording Arts and Sciences. This Special Merit Award is presented by vote of the Recording Academy's National Trustees to performers who, during their lifetimes, have made creative contributions of outstanding artistic significance to the field of recording.
Grammy Hall of Fame.
Recordings of Armstrong were inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least 25 years old, and that have "qualitative or historical significance."
Rock and Roll Hall of Fame.
The Rock and Roll Hall of Fame listed Armstrong's "West End Blues" on the list of 500 songs that shaped Rock and Roll.
Inductions and honors.
In 1995, the U.S. Post Office issued a Louis Armstrong 32 cents commemorative postage stamp.
Legacy.
The influence of Armstrong on the development of jazz is virtually immeasurable. Yet, his irrepressible personality both as a performer, and as a public figure later in his career, was so strong that to some it sometimes overshadowed his contributions as a musician and singer.
As a virtuoso trumpet player, Armstrong had a unique tone and an extraordinary talent for melodic improvisation. Through his playing, the trumpet emerged as a solo instrument in jazz and is used widely today. He was a masterful accompanist and ensemble player in addition to his extraordinary skills as a soloist. With his innovations, he raised the bar musically for all who came after him.
Though Armstrong is widely recognized as a pioneer of scat singing, Ethel Waters precedes his scatting on record in the 1930s according to Gary Giddins and others. Billie Holiday and Frank Sinatra are just two singers who were greatly indebted to him. Holiday said that she always wanted Bessie Smith's 'big' sound and Armstrong's feeling in her singing. Even special musicians like Duke Ellington have praised Armstrong through strong testimonials. Duke Ellington said, "If anybody was a master, it was Louis Armstrong." In 1950, Bing Crosby, the most successful vocalist of the first half of the 20th century, said, "He is the beginning and the end of music in America."
In the summer of 2001, in commemoration of the centennial of Armstrong's birth, New Orleans's main airport was renamed Louis Armstrong New Orleans International Airport.
In 2002, the Louis Armstrong's Hot Five and Hot Seven recordings (1925–1928) were preserved in the United States National Recording Registry, a registry of recordings selected yearly by the National Recording Preservation Board for preservation in the National Recording Registry of the Library of Congress.
The US Open tennis tournament's former main stadium was named Louis Armstrong Stadium in honor of Armstrong who had lived a few blocks from the site.
Today, there are many bands worldwide dedicated to preserving and honoring the music and style of Satchmo, including the Louis Armstrong Society located in New Orleans, Louisiana.
House.
The house where Armstrong lived for almost 28 years was declared a National Historic Landmark in 1977 and is now a museum. The Louis Armstrong House Museum, at 34-56 107th Street (between 34th and 37th Avenues) in Corona, Queens, presents concerts and educational programs, operates as a historic house museum and makes materials in its archives of writings, books, recordings and memorabilia available to the public for research. The museum is operated by the City University of New York's Queens College, following the dictates of Lucille Armstrong's will. The museum opened to the public on October 15, 2003. A new visitors center is planned.
Discography.
Louis Armstrong's works by album or song:

</doc>
<doc id="18315" url="http://en.wikipedia.org/wiki?curid=18315" title="Long Island">
Long Island

Long Island is an island located just off the northeast coast of the United States and a region within the U.S. state of New York. Stretching east-northeast from New York Harbor into the Atlantic Ocean, the island comprises four counties: Kings and Queens (these form the New York City boroughs of Brooklyn and Queens) to the west; then Nassau and Suffolk to the east. However, most people in the New York metropolitan area (even those living in Queens and Brooklyn) use the term "Long Island" (or "The Island") exclusively to refer to the Nassau-Suffolk county area collectively, which is mainly suburban in character. North of the island is Long Island Sound, across which are the states of Connecticut and Rhode Island.
With a Census-estimated population of 7,804,968 in 2014, embodying nearly 40% of New York State's population, Long Island is the most populated island in any U.S. state or territory, and the 17th-most populous island in the world (ahead of Ireland, Jamaica, and Hokkaidō). Its population density is 5571 PD/sqmi. If Long Island geographically constituted an independent metropolitan statistical area, it would rank fourth most populous in the United States; while if it were a U.S. state, Long Island would rank 13th in population and first in population density.
Both the longest
and the largest island in the contiguous United States, Long Island extends 118 mi eastward from New York Harbor to Montauk Point, with a maximum north-to-south distance of 23 mi between Long Island Sound and the Atlantic coast.
With a land area of 1,401 square miles (3,629 km2), Long Island is the 11th-largest island in the United States and the 148th-largest island in the world — larger than the 1,214 sqmi of the smallest state, Rhode Island.
Two of the New York City area's major airports, LaGuardia Airport and JFK International Airport, are located on Long Island, in Queens. Nine bridges and 13 tunnels (including railroad tunnels) connect Brooklyn and Queens (and thus Long Island) to the three other boroughs of New York City. Ferries connect Suffolk County northward across Long Island Sound to the state of Connecticut.
History.
Early history.
At the time of European contact, the Lenape people (named the "Delaware" by Europeans) inhabited the western end of Long Island, and spoke the Munsee dialect of Lenape, one of the Algonquian language family. Giovanni da Verrazzano was the first European to record an encounter with these people, after entering what is now New York Bay in 1524. The eastern portion of the island was inhabited by speakers of the Mohegan-Montauk-Narragansett language group of Algonquian languages; they were part of the Pequot and Narrangansett peoples inhabiting what is now Connecticut and Rhode Island.
In 1609, the English navigator Henry Hudson explored the harbor and purportedly landed at Coney Island. Adriaen Block followed in 1615 and is credited as the first European to determine that both Manhattan and Long Island are islands.
Indian land deeds recorded by the Dutch from 1636 state that the Indians referred to Long Island as "Sewanhaka" ("Sewanhacky" and "Sewanhacking" were other spellings in the transliteration of Lenape). "Sewan" was one of the terms for wampum (commemorative stringed shell beads, for a while also used as currency by colonists in trades with the Lenape), and is also translated as "loose" or "scattered", which may refer either to the wampum or to Long Island.
The name " 't Lange Eylandt alias Matouwacs" (later shortened to "Lange Eylandt") appears in Dutch maps from the 1650s.
Later, the English referred to the land as "Nassau Island",
after the Dutch Prince William of Nassau, Prince of Orange (who later also ruled as King William III of England). It is unclear when the name "Nassau Island" was discontinued.
The very first settlements on Long Island were by settlers from England and its colonies in present-day New England. Lion Gardiner settled nearby Gardiners Island in 1637. The first settlement on the geographic Long Island itself was on October 21, 1640, when Southold was established by settlers from New Haven, Connecticut. Southampton was settled in the same year. Hempstead followed in 1644, East Hampton in 1648, Huntington in 1653, and Brookhaven in 1655.
While the eastern region of Long Island was first settled by English, the western portion of Long Island was settled by the Dutch. Until 1664, the jurisdiction of Long Island was split, roughly at the present border between Nassau County and Suffolk County. The Dutch founded six towns in present-day Brooklyn beginning in 1645. These included: Brooklyn, Gravesend, Flatlands, Flatbush, New Utrecht, and Bushwick. The Dutch had granted an English settlement in Hempstead, New York (now in Nassau county) in 1644 but, after a boundary dispute, drove out English settlers from the Oyster Bay area. In 1664, the English took over the Dutch colony of New Amsterdam, including all their lands on Long Island, and controlled the 120-mile expanse (except for all the territories inhabited by indigenous peoples.)
The 1664 land patent granted to the Duke of York included all islands in Long Island Sound. The Duke of York held a grudge against Connecticut, as New Haven had hidden three of the judges who sentenced the Duke's father, King Charles I, to death in 1649. Settlers throughout Suffolk County pressed to stay part of Connecticut, but Governor Sir Edmund Andros threatened to eliminate the settlers' rights to land if they did not yield, which they did by 1676.
All of Long Island (as well as the islands between it and Connecticut) became part of the Province of New York within the Shire of York.
Present-day Suffolk County was designated as the East Riding (of Yorkshire), present-day Brooklyn was part of the West Riding, and present-day Queens and Nassau were part of the larger North Riding. In 1683, Yorkshire was dissolved and the three original counties on Long Island were established: Kings, Queens, and Suffolk.
18th and 19th centuries.
Early in the American Revolutionary War, the island was captured by the British from General George Washington in the Battle of Long Island, a decisive battle after which Washington narrowly evacuated his troops from Brooklyn Heights under a dense fog. After the British victory on Long Island, many Patriots fled, leaving mostly Loyalists behind. The island remained a British stronghold until the end of the war in 1783.
General Washington based his espionage activities on Long Island, due to the western part of the island's proximity to the British military headquarters in New York City. The Culper Spy Ring included agents operating between Setauket and Manhattan. This ring alerted Washington to valuable British secrets, including the treason of Benedict Arnold and a plan to use counterfeiting to induce economic sabotage.
Long Island's colonists served both Loyalist and Patriot causes, with many prominent families divided among both sides. During the occupation British troops repurposed a number of civilian structures for defense and demanded to be quartered in the homes of civilians. A number of structures from this era remain. Among these are Raynham Hall, the Oyster Bay home of patriot spy Robert Townsend, and the Caroline Church in Setauket, which contains bullet holes from a skirmish known as the Battle of Setauket. Also in existence is a reconstruction of Brooklyn's Old Stone House, on the site of the Maryland 400's celebrated last stand during the Battle of Long Island.
In the 19th century, Long Island was still mainly rural and devoted to agriculture. The predecessor to the Long Island Rail Road (LIRR) began service in 1836 from the South Ferry, Brooklyn, through Brooklyn to Jamaica in Queens. The line was completed to the east end of Long Island in 1844 (as part of a plan for transportation to Boston). Competing railroads (soon absorbed by the LIRR) were built along the south shore to accommodate travellers from those more populated areas. For the century from 1830 until 1930, total population roughly doubled every twenty years, with more dense development in areas near Manhattan. Several cities were incorporated, such as the City of Brooklyn in Kings County, and Long Island City in Queens.
Until the 1883 completion of the Brooklyn Bridge, the only connection between Long Island and the rest of the United States was by boat and ship. As other bridges and tunnels were constructed, areas of the island began to be developed as residential suburbs, first around the railroads that offered commuting into the city. On January 1, 1898, Kings County and portions of Queens were consolidated into "The City of Greater New York", abolishing all cities and towns within them. The easternmost 280 sqmi of Queens County, which were not part of the consolidation plan,
separated from Queens in 1899 to form Nassau County.
At the close of the 19th century, wealthy industrialists who made vast fortunes during the Gilded Age began to construct large "baronial" country estates in Nassau County communities along the North Shore of Long Island, favoring the many lots with water views. Proximity to Manhattan attracted such men as J. P. Morgan, William K. Vanderbilt, and Charles Pratt, whose estates led to this area being nicknamed the Gold Coast. This period and the area was immortalized in fiction, such as F. Scott Fitzgerald's "The Great Gatsby" (which has also been adapted in films.)
20th and 21st centuries.
Charles Lindbergh lifted off from Roosevelt Field with his "Spirit of Saint Louis" for his historic solo flight to Europe. From the 1920s to the 1940s, Long Island began the transformation from backwoods and farms as developers created numerous suburbs. Numerous branches of the LIRR already enabled commuting from the suburbs to the city. Robert Moses engineered various automobile parkway projects to span the island, and developed beaches and state parks for the enjoyment of residents and visitors from the city. Gradually, development also followed these parkways, with various communities springing up along the more traveled routes.
After World War II, suburban development increased with incentives under the GI Bill, and Long Island's population skyrocketed, mostly in Nassau County and western Suffolk County. Second and third-generation children of immigrants moved out to Long Island to settle in new housing developments built during the post-war boom. Levittown became noted as a suburb, where house construction was simplified to be produced on a large scale. These provided opportunities for GIs returning home to buy houses and start a family.
The descendants of late 19th and early 20th-century immigrants from southern and eastern Europe, and black migrants from the South, have been followed by more recent ones from Latin America. These have created a diversity on Long Island lacked in other American regions. The population has many ethnic Irish, Jews and Italians, as well as an increasing number of Asians and Hispanics reflecting later migrations.
In 1985, the United States Supreme Court ruled in "United States v. Maine" that Long Island was not an island for legal purposes. 
By the start of the 21st century a number of Long Island communities had successfully repurposed their assets from industrial uses to post-industrial roles. Brooklyn reversed decades of population decline and factory closings to resurface as a globally renowned cultural and intellectual hotbed. Gentrification has affected much of Brooklyn and a portion of Queens, relocating a sizeable swath of New York City's population. On eastern Long Island, such villages as Port Jefferson, Patchogue, and Riverhead have been repurposed from inactive shipbuilding and mill towns into tourist-centric commercial centers with cultural attractions.
A flood occurred on August 13, 2014 in Long Island after record-setting rainfall deposited two months' worth of precipitation on the area.
Geography.
The westernmost end of Long Island contains the New York City boroughs of Brooklyn (Kings County) and Queens (Queens County). The central and eastern portions contain the suburban Nassau and Suffolk counties. However, colloquial usage of the term "Long Island" usually refers only to Nassau and Suffolk counties. For example, the Federal Reserve Bank of New York has a district named "Long Island (Nassau-Suffolk Metro Division)." At least as late as 1911, locations in Queens were still commonly referred to as being on Long Island. Some institutions in the New York City section of the island use the island's names, like Long Island University and Long Island Jewish Medical Center. The New York Islanders National Hockey League team intends to retain its name after their move from Nassau County to Brooklyn in 2015.
Nassau County is more densely developed than Suffolk County. While affluent overall, Nassau County has pockets of more pronounced wealth with estates covering greater acreage within the Gold Coast of the North Shore and the Five Towns area on the South Shore. South Shore communities are built along protected wetlands of the island and contain white sandy beaches of Outer Barrier Islands fronting on the Atlantic Ocean. Dutch and English settlers from the time before the American Revolutionary War, as well as communities of Native Americans, populated the island. The 19th century saw the infusion of the wealthiest Americans in the so-called Gold Coast of the North Shore, where wealthy Americans and Europeans in the Gilded Age built lavish country homes.
Owing to economic growth and the suburbanization after World War II, Nassau was the fastest growing county in the United States from the 1950s to the 1970s. Suffolk County remains less congested, despite substantial growth in the high technology and light manufacturing sectors since 1990, although traffic has been increasing in recent years. In its easternmost sections, Suffolk remains small-town rural, as in Greenport on the North Fork and some of the outward areas of The Hamptons, although summer tourism swells the population in those areas. Western Suffolk, such as the towns of Huntington and Babylon, are becoming increasingly populated and are beginning to resemble towns in Nassau.
The North Fork peninsula of Suffolk County's East End has developed a burgeoning Wine Country region. The South Fork peninsula is known for beach communities, including the prominently known Hamptons, and for the Montauk Point Lighthouse at the eastern tip of the island. The Pine Barrens is a preserved pine forest encompassing much of eastern Suffolk County.
Geology.
Long Island, as part of the Outer Lands region, is formed largely of two spines of glacial moraine, with a large, sandy outwash plain beyond. These moraines consist of gravel and loose rock left behind during the two most recent pulses of Wisconsin glaciation during the Ice Ages some 21,000 years ago (19,000 BC). The northern moraine, which directly abuts the North Shore of Long Island at points, is known as the Harbor Hill moraine. The more southerly moraine, known as the Ronkonkoma moraine, forms the "backbone" of Long Island; it runs primarily through the very center of Long Island, roughly coinciding with the length of the Long Island Expressway.
The land to the south of this moraine to the South Shore is the outwash plain of the last glacier. One part of the outwash plain was known as the Hempstead Plains, and this land contained one of the few natural prairies to exist east of the Appalachian Mountains.
The glaciers melted and receded to the north, resulting in the difference between the North Shore beaches and the South Shore beaches. The North Shore beaches are rocky from the remaining glacial debris, while the South Shore's are crisp, clear, outwash sand. Jayne's Hill, at 401 ft, within Suffolk County near its border with Nassau County, is the highest hill along either moraine; another well-known summit is Bald Hill in Brookhaven Town, not far from its geographical center at Middle Island. The glaciers also formed Lake Ronkonkoma in Suffolk County and Lake Success in Nassau County, each a deep kettle lake.
Climate.
Long Island has a climate similar to that of other coastal areas of the Northeastern United States; it has warm, humid summers and cool, wet winters. Under the Köppen climate classification, most of Long Island lies in a transition zone between a humid subtropical climate (Cfa) and a humid continental climate (Dfa). The oceanic climate zone (Cfb) only exists on the North Fork, the eastern end of the South Fork, islands in Peconic Bay, and Fishers Island, and is rare in eastern North America. Parts of the Harbor Hill Moraine are affected by a subtropical highland climate. The Atlantic Ocean brings afternoon sea breezes that temper the heat in the warmer months and limit the frequency and severity of thunderstorms. Long Island has a moderately sunny climate, averaging between 2,400 and 2,800 hours of sunshine annually.
Due to its coastal location, Long Island temperatures are somewhat mild compared to the rest of New York state. The coldest month is January, when average temperatures range from 30 to 35 F, and the warmest month is July, when average temperatures range from 70 to 80 F. Temperatures seldom fall below 5 F or rise above 95 F. Long Island temperatures vary from west to east, with the western part (Nassau County, Queens, and Brooklyn) generally warmer than the east (Suffolk County). This is due to several factors: the western part is closer to the mainland and more densely developed, causing the "urban heat island" effect, and Long Island's land mass veers northward as one travels east. Also, daytime high temperatures on the eastern part of Long Island are cooler on most occasions due to moderation of the Atlantic Ocean and Long Island Sound. On dry nights with no clouds or wind, the Pine Barrens forest of eastern Suffolk County can be almost 20 degrees Fahrenheit (11 degrees Celsius) cooler than the rest of the island due to radiational cooling.
Precipitation is distributed fairly uniformly throughout the year, with approximately 3-4 inch on average during each month. Average yearly snowfall totals range from approximately 20 to, with the north shore and western parts averaging more than the south shore and the east end. In any given winter, however, some parts of the island could see up to 75 in of snow or more. There are also some very quiet winters, in which most parts of the island could see less than 10 in of snow.
Long Island is somewhat vulnerable to hurricanes.
Its northern location (and the relatively cool waters around it) tend to weaken tropical cyclones before they reach Long Island; nonetheless, several have stricken the Island as full hurricanes, including a devastating Category 3, the 1938 New England Hurricane (also known as the Long Island Express), and another Category 3, Hurricane Carol in 1954. Other 20th-century storms that made landfall on Long Island at hurricane intensity include the Great Atlantic Hurricane of 1944, Hurricane Donna in 1960, Hurricane Belle in 1976, and Hurricane Gloria in 1985. Also, the eyewall of Hurricane Bob in 1991 brushed the eastern tip. In August 2011, portions of Long Island were evacuated in preparation for Hurricane Irene, a Category 1 hurricane which weakened to a tropical storm before it reached Long Island.
On October 29, 2012, Hurricane Sandy, a slow-moving "superstorm," reached the area causing 90% of Long Island households to lose power and an estimated $18 billion in damages in Nassau & Suffolk Counties alone.
The extent of Sandy's damages is second only to those caused by the 1938 Long Island Express. Although a lower central pressure was recorded in Sandy, the National Hurricane Center estimates that the 1938 hurricane had a lower pressure at landfall.
Additional islands.
Several smaller islands are in close proximity to Long Island and are often grouped with it. These islands include Fire Island, Plum Island, Robins Island, Gardiners Island, Fishers Island, and Shelter Island.
Demographics.
Long Island is one of the most densely populated regions in the United States. As of the United States 2010 Census, the total population of all four counties of Long Island was 7,568,304, which was 39 percent of the population of the State of New York. New York City's portion of the census was 4,735,538, with Brooklyn's population at 2,504,700 and Queens having 2,230,722 residents. Long Island's proportion of New York State's population has been increasing, with Long Island's Census-estimated population increasing 3.1% since 2010, to 7,804,968 in 2014, representing 39.5% of New York State's Census-estimated 2014 population of 19,746,227. According to the US Census Bureau's 2008 American Community Survey, Nassau and Suffolk counties have the 10th and 26th highest median household incomes in the nation, respectively.
As of the 2010 census, the combined population of Nassau and Suffolk counties was 2,832,882 people; Suffolk County's share at 1,493,350 and Nassau County's at 1,339,532. Nassau County had a larger population for decades, but Suffolk County surpassed it in the 1990 census as growth and development continued to spread eastward.
As Suffolk County has more than twice the land area of Nassau County, the latter still has a much higher population density. Combining all four counties, Long Island's population is greater than 38 of the 50 U.S. states. 
Population figures from the U.S. Census Bureau "Census 2010"
show that whites are the largest racial group in all four counties, and are in the majority in Nassau and Suffolk counties. In 2002, "The New York Times" cited a study by the non-profit group ERASE Racism, which determined that Nassau and Suffolk counties constitute the most racially segregated suburbs in the United States.
In contrast, Queens is the most ethnically diverse county in the United States and the most diverse urban area in the world.
According to a 2000 report on religion, which asked congregations to respond, Catholics are the largest religious group on Long Island, with non-affiliated in second place. Catholics make up 52% of the population of Nassau and Suffolk, versus 22% for the country as a whole, with Jews at 16% and 7%, respectively, versus 1.7% nationwide. Only a small percentage of Protestants responded, 7% and 8% respectively, for Nassau and Suffolk counties. This is in contrast with 23% for the entire country on the same survey, and 50% on self-identification surveys. 
More recently, a Little India (लघु भारत) community has emerged in Hicksville, Nassau County, spreading eastward from the more established Little India enclaves in Queens. Rapidly growing Chinatowns have developed in Brooklyn (布魯克林) and Queens (皇后), with Asian immigrants moving into Nassau County, as did earlier European immigrants, such as the Irish and Italians.
Likewise, the Long Island Koreatown (롱 아일랜드 코리아타운) originated in Flushing, Queens. It is expanding eastward along Northern Boulevard and into Nassau County.
A 2010 article in "The New York Times" stated that the expansion of the immigrant workforce on Long Island has not displaced any jobs from other Long Island residents. Half the immigrants on Long Island hold white-collar positions.
Economy.
The counties of Nassau and Suffolk have been long renowned for their affluence.
From about 1930 to about 1990, Long Island was considered one of the aviation centers of the United States, with companies such as Grumman Aircraft, Republic, Fairchild, and Curtiss having their headquarters and factories on Long Island.
Long Island has played a prominent role in scientific research and in engineering. It is the home of the Brookhaven National Laboratory in nuclear physics and Department of Energy research.
In recent decades companies such as Sperry Rand, Computer Associates (headquartered in Islandia), Motorola Enterprise Mobility (now occupying the former headquarters of Symbol Technologies and a former Grumman plant in Holtsville), have made Long Island a center for the computer industry. Stony Brook University of the State University of New York conducts far-ranging medical and technology research.
Long Island is also home to the Cold Spring Harbor Laboratory, which was directed for 35 years by James D. Watson (who, along with Francis Crick and Rosalind Franklin, discovered the double helix structure of DNA).
Long Island is home to the East Coast's largest industrial park, the Hauppauge Industrial Park. The park has over 1,300 companies employing more than 71,000 Long Islanders. Companies in the park and abroad are represented by the Hauppauge Industrial Association.
As many as 20 percent of Long Islanders commute to New York City jobs.
The eastern end of the island is still partly agricultural. In the last 25 years, development of vineyards on the North Fork became a major new industry, replacing potato fields. Pumpkin farms have been added to traditional truck farming. Farms allow fresh fruit picking by Long Islanders for much of the year. Fishing continues to be an important industry, especially at Huntington, Northport, Montauk, and other coastal communities of the East End and South Shore.
Government and politics.
Nassau County and Suffolk County each have their own governments, with a County Executive leading each. Each has a county legislature and countywide-elected officials, such as district attorney, county clerk, and county comptroller. The towns in both counties have their own governments as well, with town supervisors and a town council. Nassau County is divided into three towns and two small incorporated cities (Glen Cove and Long Beach). Suffolk County is divided into ten towns.
Brooklyn and Queens, on the other hand, do not have county governments. As boroughs of New York City, both have Borough Presidents, which have been largely ceremonial offices since the shutdown of the New York City Board of Estimate.
Long Island is home to two Indian reservations, Poospatuck Reservation, and Shinnecock Reservation. Both reservations are in Suffolk County. Numerous island place names are Indian in origin.
Law enforcement.
Queens and Brooklyn are patrolled by the New York City Police Department; Nassau and Suffolk counties respectively have the Nassau County Police Department and Suffolk County Police Department. New York State Police patrol state parks and parkways. Several dozen villages and the two cities in Nassau have their own police departments.
Both Nassau and Suffolk each have a sheriff's office that handles civil process, evictions, warrant service and enforcement, prisoner transport and detention, and operation of the county jail.
Secession proposals.
The secession of Long Island from New York was proposed as early as 1896, but talk was revived towards the latter part of the twentieth century.
On March 28, 2008 Suffolk County, New York Comptroller Joseph Sawicki proposed a plan that would make Long Island (specifically, Nassau and Suffolk counties) the 51st state of the United States of America.
Sawicki says that all the Long Island taxpayers' money would stay on Long Island, rather than the funds being dispersed all over the entire state of New York, and Long Island sending to Albany over three billion dollars more than it receives back. The state of Long Island would include over 2.7 million people (larger than that of fifteen other states). So far Nassau County executives have not expressed interest in joining in the secession proposal, which would need to be approved by the NY State Legislature.
Transportation.
Every major form of transportation serves Long Island, including John F. Kennedy International Airport, LaGuardia Airport, and Long Island MacArthur Airport, multiple smaller airports, railroads, subways, and several major highways. There are historic and modern bridges, recreational and commuter trails, and ferries serving various parts of all of Long Island.
There are currently ten road crossings out of Long Island, all within New York City limits at the extreme western end of the island. Plans for a Long Island Crossing at various locations in Nassau and Suffolk Counties (a proposed bridge or tunnel that would link Long Island to the south with New York or Connecticut to the north across Long Island Sound) have been discussed for decades, but there are currently no firm plans to construct such a crossing.
Public transportation.
The MTA implements mass transportation for the New York metropolitan area including all five boroughs of New York City, the suburban counties of Dutchess, Nassau, Orange, Putnam, Rockland, Suffolk and Westchester, all of which together are the "Metropolitan Commuter Transportation District (MCTD)".
The MTA is the largest public transportation provider in the Western Hemisphere. Its agencies serve 14.6 million people spread over 5,000 square miles (13,000 km²) from New York City through the southeastern section of the state (including Long Island and the lower Hudson Valley), and Connecticut. Combined the MTA agencies now move more than 2.6 billion rail and bus customers a year while employing some 70,000 workers.
Rail.
The Long Island Rail Road (LIRR) is the busiest commuter railroad system in North America, carrying an average of 282,400 passengers each weekday on 728 daily trains. Chartered on April 24, 1834, it is also the oldest railroad still operating under its original name.
Bus.
Nassau Inter-County Express (NICE) provides bus service in Nassau County, while Suffolk County Transit, an agency of the Suffolk County government, provides bus service in Suffolk County. In 2012, NICE replaced the former Long Island Bus in transporting Long Islanders across Nassau County while still allowing them to use MTA MetroCards as payment.
Roads.
The Long Island Expressway, Northern State Parkway, and Southern State Parkway, all products of the automobile-centered planning of Robert Moses, are the island's primary east-west high-speed thoroughfares.
Ground transportation.
Being such a large, populous island with several airports connecting the island to the rest of the world, there are several hundred transportation companies that service the Long Island/New York City area. Winston airport shuttle, the oldest of these companies in business since 1973, was the first to introduce door-to-door shared-ride service to and from the major airports, which almost all transportation companies now utilize.
Education.
Primary and secondary education.
Long Island's Nassau and Suffolk counties are the home of 125 public school districts containing a total of 656 public schools. In contrast, all of Brooklyn and Queens are served by the New York City Department of Education, the largest school district in the United States. Long Island is also home to a number of private and parochial schools.
Colleges and universities.
Long Island is home to a range of higher-education institutions, both public and private.
Brooklyn and Queens contain five of eleven senior colleges within CUNY, the public university system of New York City and one of the largest in the country. Among these are the notable institutions of Brooklyn College and Queens College. Brooklyn also contains private colleges such as Pratt Institute and the New York University Polytechnic School of Engineering, an engineering college that merged with New York University in 2014.
Nassau and Suffolk counties are similarly served by community colleges, as well as several colleges and universities within the SUNY system, notably Stony Brook (for its health sciences research and medical center). Private institutions include the New York Institute of Technology (an internationally recognized private research university), Hofstra University and Adelphi University (both located in the Town of Hempstead), as well as Long Island University (with its C.W. Post campus, located on a former Gold Coast estate in Brookville, and a satellite campus in downtown Brooklyn). Long Island also contains the Webb Institute, a small naval architecture college in Glen Cove. In addition, the island is home to the United States Merchant Marine Academy, a Federal Service Academy located in Kings Point, on the North Shore.
Culture.
Music.
Music on Long Island (Nassau and Suffolk) is strongly influenced by the proximity to New York City and by the youth culture of the suburbs.
Psychedelic rock was widely popular in the 1960s as flocks of disaffected youth travelled to NYC to participate in protest and the culture of the time. R & B also has a history on Long Island, especially in areas close to New York City. In the late 1970s through the 1980s, the influence of radio station WLIR made Long Island one of the first places in the U.S. to hear and embrace European New Wave bands such as Depeche Mode, the Pet Shop Boys, and Culture Club. In the 1990s, hip-hop became very popular with rap pioneers Rakim, EPMD, and Public Enemy growing up on Long Island. Recently, new bands have been making a name for themselves originating from Long Island such as Brand New, Austin Schoeffel and Envy on the Coast.
Famous rock bands that originated on Long Island include The Rascals, Dream Theater, Blue Öyster Cult, Twisted Sister and guitar virtuosos John Petrucci, Steve Vai and Joe Satriani.
The Nassau Coliseum and Nikon at Jones Beach Theater are venues used by national touring acts as performance spaces for concerts. Nikon at Jones Beach Theater is an outdoor amphitheatre, located at Jones Beach State Park. It is a popular place to view summer concerts, with new as well as classic artists performing there during the summer months. It hosts a large Fourth of July fireworks show every year which fills the stands. People also park cars along the highway leading to the show, and others watch from the nearby beaches.
Long Island is also known for its school music programs. Many schools in Suffolk County have distinguished music programs, with high numbers of students who are accepted into the statewide All-State music groups, or even the National All-Eastern Coast music groups. Both the Suffolk County and Nassau County Music Educator's Associations are recognized by The National Association of Music Education (),
and host numerous events, competitions, and other music-related activities.
Cuisine.
Long Island has historically been a center for fishing and seafood. This legacy continues in the Blue Point oyster, a now ubiquitous variety that was originally harvested on the Great South Bay and was the favorite oyster of Queen Victoria. Clams are also a popular food and clam digging a popular recreational pursuit, with Manhattan clam chowder reputed to have Long Island origins.
Of land-based produce, Long Island duck has a history of national recognition since the 19th century, with four duck farms continuing to produce 2 million ducks a year as of 2013. Two symbols of Long Island's duck farming heritage are the Long Island Ducks minor-league baseball team and the Big Duck, a 1931 duck-shaped building that is a historic landmark and tourist attraction. In addition to Long Island's duck industry, Riverhead contains one of the largest buffalo farms on the East coast.
Long Island is well known for its production of alcoholic beverages. Eastern Long Island is a significant producer of wines. Vineyards are most heavily concentrated on Long Island's North Fork, which contains 38 wineries. Most of these contain tasting rooms, which serve as popular tourist attractions for visitors from across the New York metropolitan area. Long Island has also become a producer of diverse craft beers, with 15 microbreweries existing across Nassau and Suffolk counties as of 2013. The largest of these is Blue Point Brewing Company, best known for its "toasted lager". Long Island is also globally known for its signature cocktail, the Long Island Iced Tea, which purportedly was invented at a popular Jones Beach nightclub in the 1970s.
The eateries on Long Island are largely a product of the region's local ethnic populations. Italian cuisine is represented by ubiquitous pizzerias spread throughout the island, with the region hosting an annual competition, the Long Island Pizza Festival & Bake-Off. Jewish cuisine is likewise represented by delicatessens and bagel stores.
Sports.
The New York Mets baseball team plays at Citi Field in Flushing Meadows-Corona Park, Queens. Their former stadium, Shea Stadium was also home for the New York Jets football team from 1964 until 1983. The new stadium is designed with an exterior façade and main entry rotunda inspired by Ebbets Field. The New York Mets had planned to move their Double-A farm team to Long Island, as part of the ambitious but now-defunct plan for Nassau county called The Lighthouse Project. The Brooklyn Cyclones are a minor league baseball team, affiliated with the New York Mets. The Cyclones play at MCU Park just off the boardwalk on Coney Island in Brooklyn. An artificial turf baseball complex named Baseball Heaven is located in Yaphank.
The Barclays Center, a new sports arena, business, and residential complex built partly on a platform over the Atlantic Yards at Atlantic Avenue in Brooklyn, is the current home of the Brooklyn Nets basketball team and the New York Islanders hockey team. The move from New Jersey in the summer of 2012 marked the return to Long Island for the Nets franchise, which played at Nassau Veterans Memorial Coliseum from 1972–1977. The Islanders have played at Nassau Coliseum since their inception in 1972. The Islanders will move to Barclays Center in the 2015–16 NHL season, ensuring that the team will remain on Long Island.
Long Island has a wide variety of golf courses that span all over. One of the most famous is the public Bethpage Black Course that has hosted multiple U.S. Open's as well as The Barclays with the most recent during the year 2012.
Long Island has a professional soccer club, the New York Cosmos, who play in the Division 2 North American Soccer League at James M. Shuart Stadium in Hempstead.
Queens also hosts one of the four tennis grand slams, the US Open. Every August (September, in Olympic years) the best tennis players in the world travel to Long Island to play the championships, which is held in the USTA National Tennis Center, located adjacent to Citi Field in Flushing Meadows Park. The complex also contains the biggest tennis stadium in the world, the Arthur Ashe Stadium.
Ebbets Field, which stood in Brooklyn from 1913 to 1957, was the home of the Brooklyn Dodgers baseball team, who moved to California after the 1957 Major League Baseball season to become the Los Angeles Dodgers. The Dodgers won several National League pennants in the 1940s and 1950s, losing several times in the World Series—often called "Subway Series"—to their Bronx rivals, the New York Yankees. The Dodgers won their lone championship in Brooklyn in the 1955 World Series versus the Yankees.
Long Island has been a hot spot for outdoor lacrosse at the youth and college level, which made way for a Major League Lacrosse team in 2001, the Long Island Lizards. The Lizards play at Mitchel Athletic Complex in Uniondale.
The New York Dragons of the Arena Football League also played their home games at Nassau Coliseum.
Long Island has also been at the forefront of Semi-Professional football. The Empire State Demon Knights of the Five Star Football League have called Long Island their home since they relinquished the name Kings County Wolfpack and moved to Suffolk County.
Long Island is also home to the Long Island Ducks minor league baseball team of the Atlantic League. Their stadium, Bethpage Ballpark, is located in Central Islip. The two main rugby teams are the Long Island RFC in East Meadow and the Suffolk Bull Moose in Stony Brook.
Another category of sporting events popular in this region are Firematic Racing events, involving many local Volunteer fire departments.
Long Island also has two horse racing tracks, Aqueduct Racetrack in Ozone Park, Queens and Belmont Park on the Queens/Nassau border in Elmont, home of the Belmont Stakes. The longest dirt Thoroughbred racecourse in the world is located in the Nassau County community of Elmont at Belmont Park.
Long Island is home to numerous famous athletes, including Hall of Famers Jim Brown, Julius Erving, John Mackey, Whitey Ford, Nick Drahos, and Carl Yastrzemski. Others include Gold Medalists Sue Bird, Sarah Hughes and Derrick Adkins, D'Brickashaw Ferguson, Billy Donovan, Larry Brown, Rick Pitino, John McEnroe, Jumbo Elliott, Mick Foley, Zack Ryder, Matt Serra, Boomer Esiason, Vinny Testaverde, Craig Biggio, Frank Catalanotto, Greg Sacks, Rob Burnett, Steve Park, Frank Viola, Chris Weidman, Marques Colston and Speedy Claxton. Several current NHL Players such as Vancouver Canucks Christopher Higgins and Matt Gilroy, Nashville Predators Eric Nystrom, Toronto Maple Leaf Mike Komisarek, and Pittsburgh Penguin Rob Scuderi were all born and/or raised on Long Island. Both Komisarek and Higgins played on the same Suffolk County Hockey League team at an early age, and later played on the Montreal Canadiens together. Nick Drahos was an All Scholastic and All Long Island honoree at Lawrence High School, Nassau Co. in 1936 and 1937, and a 2-time Unanimous National College All-American in the years of 1939 and 1940 at Cornell University.

</doc>
<doc id="18317" url="http://en.wikipedia.org/wiki?curid=18317" title="Lower Peninsula of Michigan">
Lower Peninsula of Michigan

The Lower Peninsula of Michigan is the southern of the two major landmasses of the U.S. state of Michigan. It is surrounded by water on all sides except its southern border, which it shares with Ohio and Indiana. Geographically, the Lower Peninsula has a recognizable shape that many people associate with a mitten, with the mid-eastern region identified as The Thumb. This has led to several folkloric creation myths for the area, one being that it is a hand print of Paul Bunyan, a giant lumberjack and favorite folk character in Michigan. This has also led to the distinctive phenomenon of Lower Peninsula residents holding out their hands and pointing to a spot on it when asked where they live. The peninsula is sometimes divided into the Northern Lower Peninsula and Southern Lower Peninsula.
The Lower Peninsula has been nicknamed "The Mitten", "Below the Bridge", and occasionally "The L.P." (in parallel with "the U.P." for the Upper Peninsula). Residents of the Lower Peninsula are jokingly referred to as "flat-landers," or "trolls" by residents of the Upper Peninsula, because they live "under the bridge".
Geography.
At its widest points, the Lower Peninsula is 277 miles (446 km) long from north to south and 195 miles (314 km) from east to west. It contains nearly two-thirds of Michigan's total land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is not definitely established but is either Briar Hill at 1,705 feet (520 m), or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at 571 feet (174 m).
The Lower Peninsula is bounded on the south by the states of Ohio and Indiana, sharing both land and water boundaries with both. As a peninsula, the rest of the Lower Peninsula is bound by water. Lake Michigan, Lake Huron, Lake St. Clair, and Lake Erie are the principal bodies of water that form the coastline of the Lower Peninsula. It also shares a water boundary with the Province of Ontario, Canada.
Flora and Fauna.
The American Bird Conservancy and the National Audubon Society have designated several locations as internationally Important Bird Areas.
Geology.
The Lower Peninsula is dominated by a geological basin known as the Michigan Basin. That feature is represented by a nearly circular pattern of geologic sedimentary strata in the area with a nearly uniform structural dip toward the center of the peninsula. The basin is centered in Gladwin County where the Precambrian basement rocks are 16000 ft deep. Around the margins, such as under Mackinaw City, Michigan, the Precambrian surface is around 4000 ft down. This 4000 ft contour on the bedrock clips the northern part of the lower peninsula and continues under Lake Michigan along the west. It crosses the southern counties of Michigan and continues on to the north beneath Lake Huron.
Economy.
The Bureau of Economic Analysis estimated Michigan's 2004 gross state product at $372 billion. Per capita personal income in 2003 was $31,178 and ranked twentieth in the nation. In May 2009, Michigan's unemployment rate rose to 14.1%, the highest in the nation during the recession.
Some of the major industries/products/services include automobiles, cereal products, pizza, information technology, aerospace, military equipment, copper, iron, and furniture. Michigan is the third leading grower of Christmas trees with 60520 acre of land dedicated to Christmas tree farming. Detroit Pharmacist James Vernor created his ginger flavored beverage, Vernors, in 1866 tying it with Hires Root Beer as the oldest soft drink. The Feigenson Brothers established their bottling works in Detroit on November 4, 1907 but felt the name was too long and later shortened it to Faygo Beverages. Two of the top four pizza chains were founded in Michigan and maintain their headquarters there: Mike Ilitch opened his first Little Caesars Pizza in 1959 and in 1960, Tom Monaghan and brother, James, purchased a pizza store that Tom later renamed Domino's Pizza.
Michigan has experienced economic difficulties brought on by volatile stock market disruptions following the September 11, 2001 attacks. This caused a pension and benefit fund crisis for many American companies, including General Motors, Ford, and Chrysler. Since the early 2000s recession and the September 11, 2001 attacks, GM, Ford, and Chrysler have struggled to overcome the benefit funds crisis which followed an ensuing volatile stock market which had caused a severe underfunding condition in the respective U.S. pension and benefit funds (OPEB). Although manufacturing in the state grew 6.6% from 2001 to 2006, the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. During this economic crisis, President George W. Bush extended loans from the Troubled Assets Relief Program (TARP) funds in order to help the GM and Chrysler bridge the recession. In January 2009, President Barack Obama formed an automotive task force in order to help the industry recover and achieve renewed prosperity for the region. With retiree health care costs a significant issue, General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). In spite of these efforts, the severity of the recession required Detroit's automakers to take additional steps to restructure, including idling many plants. With the U.S. Treasury extending the necessary debtor in possession financing, Chrysler and GM filled separate 'pre-packaged' Chapter 11 restructurings in May and June 2009 respectively.
Michigan ranks fourth nationally in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry. Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the United States. Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state. The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S. Michigan ranked second nationally in new corporate facilities and expansions in 2004. From 1997 to 2004, Michigan was listed as the only state to top the 10,000 mark for the number of major new developments; however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan ranked third in a survey among the states for luring new business which measured capital investment and new job creation per one million population. In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020.
As leading research institutions, the University of Michigan, Michigan State University, Western Michigan University, and Wayne State University are important partners in the state's economy. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. Michigan's infrastructure gives it a competitive edge; Michigan has 38 deep water ports. In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank of Troy.
Detroit Metropolitan Wayne County Airport is one of the nation's most modern airports with six major runways, large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and passenger terminals opened in 2002 and 2008. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education.
Regions.
Michigan's Lower Peninsula can be divided into four main regions based on geological, soil, and vegetation differences; amount of urban areas or rural areas; minority populations; and agriculture. The four principal regions listed below can further be separated into sub-regions and overlapping areas.
Great Lakes Circle Tour.
The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.

</doc>
<doc id="18318" url="http://en.wikipedia.org/wiki?curid=18318" title="Lake Toba">
Lake Toba

Lake Toba (Indonesian: Danau Toba) is a large natural lake occupying the caldera of a supervolcano. The lake is about 100 kilometres long, 30 kilometres wide, and up to 505 metres (1,666 ft) deep. Located in the middle of the northern part of the Indonesian island of Sumatra, with a surface elevation of about 900 m, the lake stretches from to . It is the largest lake in Indonesia and also the largest volcanic lake in the world.
Lake Toba is the site of a massive supervolcanic eruption estimated at VEI 8 that occurred 69,000 to 77,000 years ago, representing a climate-changing event. It is the largest known explosive eruption on Earth in the last 25 million years. According to the Toba catastrophe theory, it had global consequences for human populations: it killed most humans living at that time and is believed to have created a population bottleneck in central east Africa and India, which affects the genetic make up of the human world-wide population to the present. 
It has been accepted that the eruption of Toba led to a volcanic winter with a worldwide decrease in temperature between 3 to, and up to 15 C-change in higher latitudes. Additional studies in Lake Malawi in East Africa show significant amounts of ash being deposited from the Toba eruptions, even at that great distance, but little indication of a significant climatic effect in East Africa.
Geology.
The Toba caldera complex in Northern Sumatra, comprises four overlapping volcanic craters that adjoin the Sumatran "volcanic front." The youngest and fourth caldera is the world's largest Quaternary caldera (100 by) and intersects the three older calderas. An estimated 2800 km3 of dense-rock equivalent pyroclastic material, known as the youngest Toba tuff, was released during one of the largest explosive volcanic eruptions in recent geological history. Following this eruption, a resurgent dome formed within the new caldera, joining two half-domes separated by a longitudinal graben.
At least four cones, four stratovolcanoes, and three craters are visible in the lake. The Tandukbenua cone on the northwestern edge of the caldera has only sparse vegetation, suggesting a young age of several hundred years. Also, the Pusubukit (Hill Center) volcano (1971 metres above sea level) on the south edge of the caldera is solfatarically active and is a Geology Sanctuary.
Panoramic view of the town of Ambarita on Samosir, Lake Toba
Major eruption.
The "Toba eruption" (the "Toba event") occurred at what is now Lake Toba about 67,500 to 75,500 years ago. It was the last in a series of at least three caldera-forming eruptions at this location, with earlier calderas having formed around 700,000 and 840,000 years ago. This last eruption had an estimated VEI 8, making it possibly the largest explosive volcanic eruption within the last 25 million years.
Bill Rose and Craig Chesner of Michigan Technological University have estimated that the total amount of material released in the eruption was about 2800 km3—about 2000 km3 of ignimbrite that flowed over the ground, and approximately 800 km3 that fell as ash mostly to the west. The pyroclastic flows of the eruption destroyed an area of 20000 km2, with ash deposits as thick as 600 m by the main vent.
The eruption was large enough to have deposited an ash layer approximately 15 cm thick over all of South Asia; at one site in central India, the Toba ash layer today is up to 6 m thick and parts of Malaysia were covered with 9 m of ash fall. In addition it has been variously calculated that 10000 e6t of sulfurous acid or 6000 e6t of sulfur dioxide were ejected into the atmosphere by the event.
The subsequent collapse formed a caldera that, after filling with water, created Lake Toba. The island in the center of the lake is formed by a resurgent dome.
The exact year of the eruption is unknown, but the pattern of ash deposits suggests that it occurred during the northern summer because only the summer monsoon could have deposited Toba ashfall in the South China Sea. The eruption lasted perhaps two weeks, and the ensuing "volcanic winter" resulted in a decrease in average global temperatures by 3.0 to for several years. Greenland ice cores record a pulse of starkly reduced levels of organic carbon sequestration. Very few plants or animals in southeast Asia would have survived, and it is possible that the eruption caused a planet-wide die-off.
Evidence from studies of mitochondrial DNA suggests that humans may have passed through a genetic bottleneck around this time that reduced genetic diversity below what would be expected given the age of the species. According to the Toba catastrophe theory, proposed by Stanley H. Ambrose of the University of Illinois at Urbana-Champaign in 1998, the effects of the Toba eruption may have decreased the size of human populations to only a few tens of thousands of individuals. However, this hypothesis is not widely accepted because similar effects on other animal species have not been observed.
More recent activity.
Since the major eruption ~70,000 years ago, eruptions of smaller magnitude have also occurred at Toba. The small cone of Pusukbukit formed on the southwestern margin of the caldera and lava domes. The most recent eruption may have been at Tandukbenua on the northwestern caldera edge, suggested by a lack of vegetation that could be due to an eruption within the last few hundred years.
Some parts of the caldera have shown uplift due to partial refilling of the magma chamber, for example, pushing Samosir Island and the Uluan Peninsula above the surface of the lake. The lake sediments on Samosir Island show that it has risen by at least 450 m since the cataclysmic eruption. Such uplifts are common in very large calderas, apparently due to the upward pressure of below-ground magma. Toba is probably the largest resurgent caldera on Earth. Large earthquakes have recently occurred in the vicinity of the volcano, notably in 1987 along the southern shore of the lake at a depth of 11 km. Such earthquakes have also been recorded in 1892, 1916, and 1920–1922.
Lake Toba lies near the Great Sumatran fault, which runs along the centre of Sumatra in the Sumatra Fracture Zone. The volcanoes of Sumatra and Java are part of the Sunda Arc, a result of the northeasterly movement of the Indo-Australian Plate, which is sliding under the eastward-moving Eurasian Plate. The subduction zone in this area is very active: the seabed near the west coast of Sumatra has had several major earthquakes since 1995, including the 9.1 2004 Indian Ocean Earthquake and the 8.7 2005 Sumatra earthquake, the epicenters of which were around 300 km from Toba.
People.
Most of the people who live around Lake Toba are ethnically Bataks. Traditional Batak houses are noted for their distinctive roofs (which curve upwards at each end, as a boat's hull does) and their colorful decor.
Flora and fauna.
The flora of the lake includes various types of phytoplankton, emerged macrophytes, floating macrophytes, and submerged macrophytes, while the surrounding countryside is rainforest including areas of Sumatran tropical pine forests on the higher mountainsides.
The fauna includes several species of zooplankton and benthic animals. Since the lake is oligotrophic (nutrient-poor), the native fish fauna is relatively scarce, and the only endemics are "Rasbora tobana" (strictly speaking near-endemic, since also found in some tributary rivers that run into the lake) and "Neolissochilus thienemanni", locally known as the Batak fish. The latter species is threatened by deforestation (causing siltation), pollution, changes in water level and the numerous fish species that have been introduced to the lake. Other native fishes include species such as "Aplocheilus panchax", "Nemacheilus pfeifferae", "Homaloptera gymnogaster", "Channa gachua", "Channa striata", "Clarias batrachus", "Barbonymus gonionotus", "Barbonymus schwanenfeldii", "Danio albolineatus", "Osteochilus vittatus", "Puntius binotatus", "Rasbora jacobsoni", "Tor tambra", "Betta imbellis", "Betta taeniata" and "Monopterus albus". Among the many introduced species are "Anabas testudineus", "Oreochromis mossambicus", "Oreochromis niloticus", "Ctenopharyngodon idella", "Cyprinus carpio", "Osphronemus goramy", "Trichogaster pectoralis", "Trichopodus trichopterus", "Poecilia reticulata" and "Xiphophorus hellerii".
 Panoramic view of Parapat from Samosir Island, Lake Toba.

</doc>
<doc id="18320" url="http://en.wikipedia.org/wiki?curid=18320" title="Lens (optics)">
Lens (optics)

A lens is a transmissive optical device that affects the focus of a light beam through refraction. A simple lens consists of a single piece of material, while a compound lens consists of several simple lenses ("elements"), usually along a common axis. Lenses are made from transparent materials such as glass, ground and polished to a desired shape. A lens can focus light to form an image, unlike a prism, which refracts light without focusing. Devices that similarly refract radiation other than visible light are also called lenses, such as microwave lenses or acoustic lenses.
History.
The word "lens" comes from the Latin name of the lentil, because a double-convex lens is lentil-shaped. The genus of the lentil plant is "Lens", and the most commonly eaten species is "Lens culinaris". The lentil plant also gives its name to a geometric figure.
The variant spelling lense is sometimes seen. While it is listed as an alternative spelling in some dictionaries, most mainstream dictionaries do not list it as acceptable.
The oldest lens artifact is the Nimrud lens, dating back 2700 years (7th century B.C.) to ancient Assyria. David Brewster proposed that it may have been used as a magnifying glass, or as a burning-glass to start fires by concentrating sunlight. Another early reference to magnification dates back to ancient Egyptian hieroglyphs in the 8th century BC, which depict "simple glass meniscal lenses".
The earliest written records of lenses date to Ancient Greece, with Aristophanes' play "The Clouds" (424 BC) mentioning a burning-glass (a biconvex lens used to focus the sun's rays to produce fire). Some scholars argue that the archeological evidence indicates that there was widespread use of lenses in antiquity, spanning several millennia. Such lenses were used by artisans for fine work, and for authenticating seal impressions. The writings of Pliny the Elder (23–79) show that burning-glasses were known to the Roman Empire, and mentions what is arguably the earliest written reference to a corrective lens: Nero was said to watch the gladiatorial games using an emerald (presumably concave to correct for nearsightedness, though the reference is vague). Both Pliny and Seneca the Younger (3 BC–65) described the magnifying effect of a glass globe filled with water.
Excavations at the Viking harbour town of Fröjel, Gotland, Sweden discovered in 1999 the rock crystal Visby lenses, produced by turning on pole lathes at Fröjel in the 11th to 12th century, with an imaging quality comparable to that of 1950s aspheric lenses. The Viking lenses were capable of concentrating enough sunlight to ignite fires.
Between the 11th and 13th century "reading stones" were invented. Often used by monks to assist in illuminating manuscripts, these were primitive plano-convex lenses initially made by cutting a glass sphere in half. As the stones were experimented with, it was slowly understood that shallower lenses magnified more effectively.
Lenses came into widespread use in Europe with the invention of spectacles, probably in Italy in the 1280s. This was the start of the optical industry of grinding and polishing lenses for spectacles, first in Venice and Florence in the thirteenth century, and later in the spectacle-making centres in both the Netherlands and Germany. Spectacle makers created improved types of lenses for the correction of vision based more on empirical knowledge gained from observing the effects of the lenses (probably without the knowledge of the rudimentary optical theory of the day). The practical development and experimentation with lenses led to the invention of the compound optical microscope around 1595, and the refracting telescope in 1608, both of which appeared in the spectacle-making centres in the Netherlands.
With the invention of the telescope and microscope there was a great deal of experimentation with lens shapes in the 17th and early 18th centuries trying to correct chromatic errors seen in lenses. Opticians tried to construct lenses of varying forms of curvature, wrongly assuming errors arose from defects in the spherical figure of their surfaces. Optical theory on refraction and experimentation was showing no single-element lens could bring all colours to a focus. This led to the invention of the compound achromatic lens by Chester Moore Hall in England in 1733, an invention also claimed by fellow Englishman John Dollond in a 1758 patent.
Construction of simple lenses.
Most lenses are "spherical lenses": their two surfaces are parts of the surfaces of spheres. Each surface can be "convex" (bulging outwards from the lens), "concave" (depressed into the lens), or "planar" (flat). The line joining the centres of the spheres making up the lens surfaces is called the "axis" of the lens. Typically the lens axis passes through the physical centre of the lens, because of the way they are manufactured. Lenses may be cut or ground after manufacturing to give them a different shape or size. The lens axis may then not pass through the physical centre of the lens.
Toric or sphero-cylindrical lenses have surfaces with two different radii of curvature in two orthogonal planes. They have a different focal power in different meridians. This forms an astigmatic lens. An example is eyeglass lenses that are used to correct astigmatism in someone's eye.
More complex are aspheric lenses. These are lenses where one or both surfaces have a shape that is neither spherical nor cylindrical. The more complicated shapes allow such lenses to form images with less aberration than standard simple lenses, but they are more difficult and expensive to produce.
Types of simple lenses.
Lenses are classified by the curvature of the two optical surfaces. A lens is "biconvex" (or "double convex", or just "convex") if both surfaces are convex. If both surfaces have the same radius of curvature, the lens is "equiconvex". A lens with two concave surfaces is "biconcave" (or just "concave"). If one of the surfaces is flat, the lens is "plano-convex" or "plano-concave" depending on the curvature of the other surface. A lens with one convex and one concave side is "convex-concave" or "meniscus". It is this type of lens that is most commonly used in corrective lenses.
If the lens is biconvex or plano-convex, a collimated beam of light passing through the lens converges to a spot (a "focus") behind the lens. In this case, the lens is called a "positive" or "converging" lens. The distance from the lens to the spot is the focal length of the lens, which is commonly abbreviated "f" in diagrams and equations.
If the lens is biconcave or plano-concave, a collimated beam of light passing through the lens is diverged (spread); the lens is thus called a "negative" or "diverging" lens. The beam, after passing through the lens, appears to emanate from a particular point on the axis in front of the lens. The distance from this point to the lens is also known as the focal length, though it is negative with respect to the focal length of a converging lens.
Convex-concave (meniscus) lenses can be either positive or negative, depending on the relative curvatures of the two surfaces. A "negative meniscus" lens has a steeper concave surface and is thinner at the centre than at the periphery. Conversely, a "positive meniscus" lens has a steeper convex surface and is thicker at the centre than at the periphery. An ideal thin lens with two surfaces of equal curvature would have zero optical power, meaning that it would neither converge nor diverge light. All real lenses have nonzero thickness, however, which makes a real lens with identical curved surfaces slightly positive. To obtain exactly zero optical power, a meniscus lens must have slightly unequal curvatures to account for the effect of the lens' thickness.
Lensmaker's equation.
The focal length of a lens "in air" can be calculated from the lensmaker's equation:
where
The focal length "f" is positive for converging lenses, and negative for diverging lenses. The reciprocal of the focal length, 1/"f", is the optical power of the lens. If the focal length is in metres, this gives the optical power in dioptres (inverse metres).
Lenses have the same focal length when light travels from the back to the front as when light goes from the front to the back. Other properties of the lens, such as the aberrations are not the same in both directions.
Sign convention for radii of curvature "R"1 and "R"2.
The signs of the lens' radii of curvature indicate whether the corresponding surfaces are convex or concave. The sign convention used to represent this varies, but in this article a "positive" "R" indicates a surface's center of curvature is further along in the direction of the ray travel (right, in the accompanying diagrams), while "negative" "R" means that rays reaching the surface have already passed the center of curvature. Consequently, for external lens surfaces as diagrammed above, "R"1 > 0 and "R"2 < 0 indicate "convex" surfaces (used to converge light in a positive lens), while "R"1 < 0 and "R"2 > 0 indicate "concave" surfaces. The reciprocal of the radius of curvature is called the curvature. A flat surface has zero curvature, and its radius of curvature is infinity.
Thin lens approximation.
If "d" is small compared to "R"1 and "R"2, then the "thin lens" approximation can be made. For a lens in air, "f" is then given by
Imaging properties.
As mentioned above, a positive or converging lens in air focuses a collimated beam travelling along the lens axis to a spot (known as the focal point) at a distance "f" from the lens. Conversely, a point source of light placed at the focal point is converted into a collimated beam by the lens. These two cases are examples of image formation in lenses. In the former case, an object at an infinite distance (as represented by a collimated beam of waves) is focused to an image at the focal point of the lens. In the latter, an object at the focal length distance from the lens is imaged at infinity. The plane perpendicular to the lens axis situated at a distance "f" from the lens is called the "focal plane".
If the distances from the object to the lens and from the lens to the image are "S"1 and "S"2 respectively, for a lens of negligible thickness, in air, the distances are related by the thin lens formula:
This can also be put into the "Newtonian" form:
where formula_10 and formula_11.
Therefore if an object is placed at a distance "S"1 > "f" from a positive lens of focal length "f", we will find an image distance "S"2 according to this formula. If a screen is placed at a distance "S"2 on the opposite side of the lens, an image is formed on it. This sort of image, which can be projected onto a screen or image sensor, is known as a "real image".
This is the principle of the camera, and of the human eye. The focusing adjustment of a camera adjusts "S"2, as using an image distance different from that required by this formula produces a defocused (fuzzy) image for an object at a distance of "S"1 from the camera. Put another way, modifying "S"2 causes objects at a different "S"1 to come into perfect focus.
In some cases "S"2 is negative, indicating that the image is formed on the opposite side of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they "appear" to form an image, this is called a virtual image. Unlike real images, a virtual image cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that virtual image. Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, "S"1 then being measured from the virtual image location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) virtual image behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a "real image" on the retina.
Using a positive lens of focal length "f", a virtual image results when "S"1 < "f", the lens thus being used a magnifying glass (rather than if "S"1 » "f" as for a camera). Using a negative lens ("f" < 0) with a "real object" ("S"1 > 0) can only produce a virtual image ("S"2 < 0), according to the above formula. It is also possible for the object distance "S"1 to be negative, in which case the lens sees a so-called "virtual object". This happens when the lens is inserted into a converging beam (being focused by a previous lens) "before" the location of its real image. In that case even a negative lens can project a real image, as is done by a Barlow lens.
For a thin lens, the distances "S"1 and "S"2 are measured from the object and image to the position of the lens, as described above. When the thickness of the lens is not much smaller than "S"1 and "S"2 or there are multiple lens elements (a compound lens), one must instead measure from the object and image to the principal planes of the lens. If distances "S"1 or "S"2 pass through a medium other than air or vacuum a more complicated analysis is required.
Magnification.
The linear "magnification" of an imaging system using a single lens is given by
where "M" is the magnification factor defined as the ratio of the size of an image compared to the size of the object. The sign convention here dictates that if "M" is negative, as it is for real images, the image is upside-down with respect to the object. For virtual images "M" is positive, so the image is upright.
Linear magnification "M" is not always the most useful measure of magnifying power. For instance, when characterizing a visual telescope or binoculars that produce only a virtual image, one would be more concerned with the angular magnification—which expresses how much larger a distant object appears through the telescope compared to the naked eye. In the case of a camera one would quote the plate scale, which compares the apparent (angular) size of a distant object to the size of the real image produced at the focus. The plate scale is the reciprocal of the focal length of the camera lens; lenses are categorized as long-focus lenses or wide-angle lenses according to their focal lengths.
Using an inappropriate measurement of magnification can be formally correct but yield a meaningless number. For instance, using a magnifying glass of 5 cm focal length, held 20 cm from the eye and 5 cm from the object, produces a virtual image at infinity of infinite linear size: "M" = ∞. But the "angular magnification" is 5, meaning that the object appears 5 times larger to the eye than without the lens. When taking a picture of the moon using a camera with a 50 mm lens, one is not concerned with the linear magnification "M" ≈ / = . Rather, the plate scale of the camera is about 1°/mm, from which one can conclude that the 0.5 mm image on the film corresponds to an angular size of the moon seen from earth of about 0.5°.
In the extreme case where an object is an infinite distance away, "S"1 = ∞, "S"2 = "f" and "M" = −"f"/∞= 0, indicating that the object would be imaged to a single point in the focal plane. In fact, the diameter of the projected spot is not actually zero, since diffraction places a lower limit on the size of the point spread function. This is called the diffraction limit.
Aberrations.
Lenses do not form perfect images, and a lens always introduces some degree of distortion or "aberration" that makes the image an imperfect replica of the object. Careful design of the lens system for a particular application minimizes the abberation. Several types of aberration affect image quality, including spherical aberration, coma, and chromatic aberration.
Spherical aberration.
"Spherical aberration" occurs because spherical surfaces are not the ideal shape for a lens, but are by far the simplest shape to which glass can be ground and polished, and so are often used. Spherical aberration causes beams parallel to, but distant from, the lens axis to be focused in a slightly different place than beams close to the axis. This manifests itself as a blurring of the image. Lenses in which closer-to-ideal, non-spherical surfaces are used are called "aspheric" lenses. These were formerly complex to make and often extremely expensive, but advances in technology have greatly reduced the manufacturing cost for such lenses. Spherical aberration can be minimised by carefully choosing the surface curvatures for a particular application. For instance, a plano-convex lens, which is used to focus a collimated beam, produces a sharper focal spot when used with the convex side towards the beam source.
Coma.
"Coma", or "comatic aberration", derives its name from the comet-like appearance of the aberrated image. Coma occurs when an object off the optical axis of the lens is imaged, where rays pass through the lens at an angle to the axis θ. Rays that pass through the centre of a lens of focal length "f" are focused at a point with distance "f" tan θ from the axis. Rays passing through the outer margins of the lens are focused at different points, either further from the axis (positive coma) or closer to the axis (negative coma). In general, a bundle of parallel rays passing through the lens at a fixed distance from the centre of the lens are focused to a ring-shaped image in the focal plane, known as a "comatic circle". The sum of all these circles results in a V-shaped or comet-like flare. As with spherical aberration, coma can be minimised (and in some cases eliminated) by choosing the curvature of the two lens surfaces to match the application. Lenses in which both spherical aberration and coma are minimised are called "bestform" lenses.
Chromatic aberration.
"Chromatic aberration" is caused by the dispersion of the lens material—the variation of its refractive index, "n", with the wavelength of light. Since, from the formulae above, "f" is dependent upon "n", it follows that light of different wavelengths is focused to different positions. Chromatic aberration of a lens is seen as fringes of colour around the image. It can be minimised by using an achromatic doublet (or "achromat") in which two materials with differing dispersion are bonded together to form a single lens. This reduces the amount of chromatic aberration over a certain range of wavelengths, though it does not produce perfect correction. The use of achromats was an important step in the development of the optical microscope. An apochromat is a lens or lens system with even better chromatic aberration correction, combined with improved spherical aberration correction. Apochromats are much more expensive than achromats.
Different lens materials may also be used to minimise chromatic aberration, such as specialised coatings or lenses made from the crystal fluorite. This naturally occurring substance has the highest known Abbe number, indicating that the material has low dispersion.
Other types of aberration.
Other kinds of aberration include "field curvature", "barrel " and "pincushion distortion", and "astigmatism".
Aperture diffraction.
Even if a lens is designed to minimize or eliminate the aberrations described above, the image quality is still limited by the diffraction of light passing through the lens' finite aperture. A diffraction-limited lens is one in which aberrations have been reduced to the point where the image quality is primarily limited by diffraction under the design conditions.
Compound lenses.
Simple lenses are subject to the optical aberrations discussed above. In many cases these aberrations can be compensated for to a great extent by using a combination of simple lenses with complementary aberrations. A "compound lens" is a collection of simple lenses of different shapes and made of materials of different refractive indices, arranged one after the other with a common axis.
The simplest case is where lenses are placed in contact: if the lenses of focal lengths "f"1 and "f"2 are "thin", the combined focal length "f" of the lenses is given by
Since 1/"f" is the power of a lens, it can be seen that the powers of thin lenses in contact are additive.
If two thin lenses are separated in air by some distance "d", the focal length for the combined system is given by
The distance from the front focal point of the combined lenses to the first lens is called the "front focal length" (FFL):
Similarly, the distance from the second lens to the rear focal point of the combined system is the "back focal length" (BFL):
As "d" tends to zero, the focal lengths tend to the value of "f" given for thin lenses in contact.
If the separation distance is equal to the sum of the focal lengths ("d" = "f"1+"f"2), the FFL and BFL are infinite. This corresponds to a pair of lenses that transform a parallel (collimated) beam into another collimated beam. This type of system is called an "afocal system", since it produces no net convergence or divergence of the beam. Two lenses at this separation form the simplest type of optical telescope. Although the system does not alter the divergence of a collimated beam, it does alter the width of the beam. The magnification of such a telescope is given by
which is the ratio of the output beam width to the input beam width. Note the sign convention: a telescope with two convex lenses ("f"1 > 0, "f"2 > 0) produces a negative magnification, indicating an inverted image. A convex plus a concave lens ("f"1 > 0 > "f"2) produces a positive magnification and the image is upright.
Other types.
Cylindrical lenses have curvature in only one direction. They are used to focus light into a line, or to convert the elliptical light from a laser diode into a round beam.
A Fresnel lens has its optical surface broken up into narrow rings, allowing the lens to be much thinner and lighter than conventional lenses. Durable Fresnel lenses can be molded from plastic and are inexpensive.
Lenticular lenses are arrays of microlenses that are used in lenticular printing to make images that have an illusion of depth or that change when viewed from different angles.
A gradient index lens has flat optical surfaces, but has a radial or axial variation in index of refraction that causes light passing through the lens to be focused.
An axicon has a conical optical surface. It images a point source into a line "along" the optic axis, or transforms a laser beam into a ring.
Superlenses are made from negative index metamaterials and claim to produce images at spatial resolutions exceeding the diffraction limit. The first superlenses were made in 2004 using such a metamaterial for microwaves. Improved versions have been made by other researchers.s of 2014[ [update]] the superlens has not yet been demonstrated at visible or near-infrared wavelengths.
Uses.
A single convex lens mounted in a frame with a handle or stand is a magnifying glass.
Lenses are used as prosthetics for the correction of visual impairments such as myopia, hyperopia, presbyopia, and astigmatism. (See corrective lens, contact lens, eyeglasses.) Most lenses used for other purposes have strict axial symmetry; eyeglass lenses are only approximately symmetric. They are usually shaped to fit in a roughly oval, not circular, frame; the optical centres are placed over the eyeballs; their curvature may not be axially symmetric to correct for astigmatism. Sunglasses' lenses are designed to attenuate light; sunglass lenses that also correct visual impairments can be custom made.
Other uses are in imaging systems such as monoculars, binoculars, telescopes, microscopes, cameras and projectors. Some of these instruments produce a virtual image when applied to the human eye; others produce a real image that can be captured on photographic film or an optical sensor, or can be viewed on a screen. In these devices lenses are sometimes paired up with curved mirrors to make a catadioptric system where the lens's spherical aberration corrects the opposite aberration in the mirror (such as Schmidt and meniscus correctors).
Convex lenses produce an image of an object at infinity at their focus; if the sun is imaged, much of the visible and infrared light incident on the lens is concentrated into the small image. A large lens creates enough intensity to burn a flammable object at the focal point. Since ignition can be achieved even with a poorly made lens, lenses have been used as burning-glasses for at least 2400 years. A modern application is the use of relatively large lenses to concentrate solar energy on relatively small photovoltaic cells, harvesting more energy without the need to use larger and more expensive cells.
Radio astronomy and radar systems often use dielectric lenses, commonly called a lens antenna to refract electromagnetic radiation into a collector antenna.
Lenses can become scratched and abraded. Abrasion-resistant coatings are available to help control this.

</doc>
<doc id="18322" url="http://en.wikipedia.org/wiki?curid=18322" title="Lamorna Birch">
Lamorna Birch

Samuel John "Lamorna" Birch, RA, RWS (7 June 1869 – 7 January 1955) was an English artist in oils and watercolours. At the suggestion of fellow artist Stanhope Forbes, Birch adopted the "soubriquet" "Lamorna" to distinguish himself from Lionel Birch, an artist who was also working in the area at that time.
Biography.
Lamorna Birch was born in Egremont, Cheshire, England. He was self-taught as an artist, other than for a brief period of study at the Académie Colarossi in Paris during 1895.
Birch settled in Lamorna, Cornwall in 1892 where many of his most famous pictures date from this time and the beautiful Lamorna Cove is usually their subject matter. He was attracted to Cornwall by the Newlyn group of artists but he ended up starting a second group based around his adopted home of Lamorna.
He exhibited at the Royal Academy from 1893. He held his first one-man exhibition at the Fine Art Society in 1906. He is said to have produced more than 20,000 pictures. Like a number of his contemporaries, he was profiled as an 'Artist of Note' in "The Artist" magazine, by Richard Seddon, in the June 1944 edition.
The exhibition "Shades of British Impressionism Lamorna Birch and his Circle" was shown at Warrington Museum & Art Gallery in the Mezzanine in October 2004. This details his links with Henry Scott Tuke and Thomas Cooper Gotch and many others who settled in the artists' colony in the 1880s and 1890s. "These painters helped to change the face of British art. Their emphasis on colour and light, truth and social realism brought about a revolution in British art." says the catalogue for the show.
Today.
Birch has paintings at Penlee House and in the collection of Derby Art Gallery.

</doc>
<doc id="18323" url="http://en.wikipedia.org/wiki?curid=18323" title="LDP">
LDP

LDP may mean:

</doc>
<doc id="18325" url="http://en.wikipedia.org/wiki?curid=18325" title="Labour">
Labour

Labour or Labor may refer to:

</doc>
<doc id="18327" url="http://en.wikipedia.org/wiki?curid=18327" title="Library of Congress Classification">
Library of Congress Classification

The Library of Congress Classification (LCC) is a system of library classification developed by the Library of Congress. It is used by most research and academic libraries in the U.S. and several other countries; in these countries, most public libraries and small academic libraries continue to use the older Dewey Decimal Classification (DDC).
LCC should not be confused with LCCN, the system of Library of Congress Control Numbers assigned to all books (and authors), which also defines URLs of their online catalog entries, such as "82006074" and "http://lccn.loc.gov/82006074". The Classification is also distinct from Library of Congress Subject Headings, the system of labels such as "Boarding schools" and "Boarding schools—Fiction" that describe contents systematically. Finally, the classifications may be distinguished from the call numbers assigned to particular copies of books in the collection, such as "PZ7.J684 Wj 1982 FT MEADE Copy 1" where the classification is "PZ7.J684 Wj 1982".
The classification was invented by Herbert Putnam in 1897, just before he assumed the librarianship of Congress. With advice from Charles Ammi Cutter, it was influenced by his Cutter Expansive Classification (developed in the 1880s) and by the DDC, Dewey (from 1876). It was designed specifically for the purposes and collection of the Library of Congress to replace the fixed location system developed by Thomas Jefferson. By the time Putnam departed from his post in 1939, all the classes except K (Law) and parts of B (Philosophy and Religion) were well developed.
LCC has been criticized for lacking a sound theoretical basis; many of the classification decisions were driven by the practical needs of that library rather than epistemological considerations. Although it divides subjects into broad categories, it is essentially enumerative in nature. That is, it provides a guide to the books actually in one library's collections, not a classification of the world.
The National Library of Medicine classification system (NLM) uses the initial letters "W" and "QS"–"QZ", which are not used by LCC. Some libraries use NLM in conjunction with LCC, eschewing LCC's "R" for Medicine. Others use LCC's "QP"–"QR" schedules and include Medicine "R".
References.
</dl>
External links.
Types
Topics
Places, people and times
Indices

</doc>
<doc id="18328" url="http://en.wikipedia.org/wiki?curid=18328" title="Library classification">
Library classification

A library classification, or a bibliographic classification, is a system according to which library materials or information resources (such as books, serials, audiovisual materials, computer files, maps, manuscripts, realia and documents) are arranged on library shelves, typically according to subject, and allocating a call number. Library classification systems group related materials together, typically arranged in a hierarchical tree structure. A different kind of classification system, called a faceted classification system, is also widely used which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in multiple ways. The library classification is distinct from the International Standard Book Number (ISBN) or International Standard Serial Number (ISSN) systems, which are unique commercial product identifiers, the first for books the second for serial publications, that can be used for tracking individual library materials, such as for lending of the materials by the library.
Description.
Library classification is an aspect of library and information science. It is a form of bibliographic classification (library classifications are used in library catalogs, while "bibliographic classification" also covers classification used in other kinds of bibliographic databases). Library classification is associated with library (descriptive) cataloging under the rubric of "cataloging and classification", sometimes grouped together as "technical services". The library professional who engages in the process of cataloging and classifying library materials is called a "cataloger" or "catalog librarian". Library classification systems are one of the two tools used to facilitate subject access. The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems.
Library classification of a piece of work consists of two steps. Firstly, the "aboutness" of the material is ascertained. Next, a call number (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system. 
It is important to note that unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However in classified catalogs one may have main entries as well as added entries. Most classification systems like the Dewey Decimal Classification (DDC) and Library of Congress Classification also add a cutter number to each work which adds a code for the author of the work.
Classification systems in libraries generally play two roles. Firstly, they facilitate subject access by allowing the user to find out what works or documents the library has on a certain subject. Secondly, they provide a known location for the information source to be located (e.g. where it is shelved).
Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject catalog. In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing.
Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example, Universal Decimal Classification, which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly faceted classification schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order.
Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a "mark and park" classification method, more formally called reader interest classification.
Types.
There are many standard systems of library classification in use, and many more have been proposed over the years. However in general, classification systems can be divided into three types depending on how they are used:
In terms of functionality, classification systems are often described as:
There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the Colon classification of S. R. Ranganathan.
English language universal classification systems.
The most common systems in English-speaking countries are:
Other systems include:
Universal classification systems that rely on synthesis (faceted systems).
Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC.
Comparing classification systems.
As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:

</doc>
<doc id="18329" url="http://en.wikipedia.org/wiki?curid=18329" title="Lexus">
Lexus

Lexus (レクサス, Rekusasu) is the luxury vehicle division of Japanese automaker Toyota. First introduced in 1989 in the USA, Lexus is now sold globally and has become Japan's largest-selling make of premium cars. The Lexus marque is marketed in over 70 countries and territories worldwide, and has ranked among the ten largest Japanese global brands in market value. Lexus is headquartered in Nagoya, Japan. Operational centers are located in Brussels, Belgium and Torrance, California, USA.
Lexus originated from a clandestine flagship sedan project, code-named F1, which began in 1983 and culminated in the launch of the original Lexus LS in 1989. Subsequently, the division added sedan, coupé, convertible, and SUV models. Until 2005 Lexus did not exist as a brand in its home market and all vehicles marketed internationally as Lexus from 1989-2005 were released in Japan under the Toyota marque and an equivalent model name. In 2005, a hybrid version of the RX crossover debuted, and additional hybrid models later joined the division's lineup. In 2007, Lexus launched its own F marque performance division with the debut of the IS F sport sedan, followed by the LFA supercar in 2009.
From the start of production, Lexus vehicles have been produced in Japan, with manufacturing centered in the Chūbu and Kyūshū regions, and in particular at Toyota's Tahara, Aichi, Chūbu and Miyata, Fukuoka, Kyūshū plants. Assembly of the first Lexus built outside the country, the Ontario, Canada–produced RX 330, began in 2003. Following a corporate reorganization from 2001 to 2005, Lexus also operates its own design, engineering, and manufacturing centers, solely responsible for the division's vehicles.
Since the 2000s, Lexus has increased sales outside its largest market, the USA, through an ongoing global expansion. The division inaugurated dealerships in Japan's domestic market in 2005, becoming the first Japanese premium car marque to launch in its country of origin. Further debuts in Southeast Asia, Latin America, Europe, and other export regions have since followed. The division's lineup has also been expanded to reflect regional specifications in model and powertrain configurations.
History.
1980s: The F1 project.
In 1983, Toyota chairman Eiji Toyoda issued a challenge to build the world's best car. This challenge prompted Toyota to embark on a top-secret project, code-named F1 (“Flagship One”). The F1 project, whose finished product was ultimately the Lexus LS 400, aimed to develop a flagship sedan that would expand Toyota’s product line, giving it a foothold in the premium segment and offering both longtime and new customers an upmarket product. The F1 project followed the success of the Toyota Supra sports car and the premium Toyota Cressida models. Both the Supra and Cressida were rear-wheel drive cars with a powerful "7M-GE"/"7M-GTE" inline-six engine. The largest sedan Toyota built at the time was the limited-production, 1960s-vintage Toyota Century, its domestic, hand built, limousine flagship and sole V8-powered model, followed by the inline-six-engined Toyota Crown premium sedan. The Century was conservatively styled for the Japanese market, and along with the Crown not slated for export, despite having undergone a complete restyle in 1982. F1 designers targeted their new sedan at international markets and began development on a new V8 engine.
The opportunity for Japanese manufacturers to export more expensive models had grown in the 1980s due to voluntary export restraints, negotiated by the Japanese government and U.S. trade representatives, restricting mainstream car sales. In 1986, Honda launched its Acura marque in the U.S., influencing Toyota's plans for a luxury division; the initial Acura model was an export version of the Honda Legend, itself launched in Japan in 1985 as a rival to the Toyota Crown, Nissan Cedric/Gloria and Mazda Luce. In 1987, Nissan unveiled its plans for a premium brand, Infiniti, and revised its flagship Nissan President sedan in standard wheelbase form for export as the Infiniti Q45, which it launched in 1990. In 1988, Mazda began selling the Luce as the Mazda 929 in North America, and later began plans to develop an upscale marque, to be called Amati, but its plans did not come to fruition.
Toyota researchers visited the USA in May 1985 to conduct focus groups and market research on luxury consumers. During that time, several F1 designers rented a home in Laguna Beach, California to observe the lifestyles and tastes of American upper class consumers. Meanwhile, F1 engineering teams conducted prototype testing on locations ranging from the German autobahn to U.S. roads. Toyota’s market research concluded that a separate brand and sales channel were needed to present its new flagship sedan, and plans were made to develop a new network of dealerships in the U.S. market.
Brand development.
In 1986, Toyota’s longtime advertising agency Saatchi & Saatchi formed a specialized unit, Team One, to handle marketing for the new premium brand. Image consulting firm Lippincott & Margulies was hired to develop a list of 219 prospective names; Vectre, Verone, Chaparel, Calibre and Alexis were chosen as top candidates. While Alexis quickly became the front runner, concerns were raised that the name applied to people more than cars (being associated with the Alexis Carrington character on the popular 1980s primetime drama "Dynasty"), and as a result the first letter was removed and the "i" replaced with a "u" to morph the name to Lexus.
The etymology of the Lexus name has been attributed to the combination of the words "luxury" and "elegance," and another theory claims it is an acronym for "luxury exports to the U.S." According to Team One interviews, the brand name has no specific meaning and simply denotes a luxurious and technological image. Just prior to the release of the first vehicles, database service LexisNexis obtained a temporary injunction forbidding the name Lexus from being used as they stated it might cause confusion. The injunction threatened to delay the division's launch and marketing efforts. Upon reflection, a U.S. appeals court lifted the injunction, deciding that there was little likelihood of confusion between the two products.
The original Lexus slogan, developed after Team One representatives visited Lexus designers in Japan and noted an obsessive attention to detail, became "The Relentless Pursuit of Perfection." The Lexus logo was developed by Molly Designs and Hunter Communications. The final design for the Lexus logo featured a stylized “"L"” within an oval, and according to Toyota, was rendered using a precise mathematical formula. The first teaser ads featuring the Lexus name and logo, designed by Team One, appeared at the Chicago, Los Angeles, and New York auto shows in 1988.
Launch.
In 1989, after an extended development process involving 60 designers, 24 engineering teams, 1,400 engineers, 2,300 technicians, 220 support workers, approximately 450 prototypes, and over US$ in costs, the F1 project was completed. The resulting flagship, the Lexus LS 400, had a unique design that shared no major elements with previous Toyota vehicles, with a new 4.0 L V8 gasoline engine and rear-wheel drive. The LS 400 debuted in January 1989 at the North American International Auto Show in Detroit, and officially went on sale the following September at a network of 81 new Lexus dealerships across the U.S. The LS 400 was sold along with a smaller sibling, the Toyota Camry-based ES 250. The launch of Lexus was heralded by a multimillion dollar advertising campaign in both television and print media.
At its debut, the LS 400 was widely praised for its quietness, well-appointed and ergonomic interior, engine performance, build quality, aerodynamics, fuel economy, and value, although it was criticized by some automobile columnists for derivative styling and a suspension regarded as too compromising of handling for ride comfort. The LS 400 debuted at US$ in the U.S. (in some markets, it was priced against mid-size six-cylinder Mercedes-Benz and BMW models) and was rated by "Car and Driver" magazine as better than both the US$ Mercedes-Benz 420 SEL and the US$ BMW 735i in terms of ride, handling, and performance. The LS 400 also won major motoring awards from publications including "Automobile Magazine" and "Wheels Magazine". Despite being an upstart, Lexus established instant customer loyalty and its debut was generally regarded as a major shock to the pedigree luxury marques. BMW's and Mercedes-Benz's U.S. sales figures dropped 29% and 19%, respectively, with BMW executives accusing Lexus of dumping in that market, while 35% of Lexus buyers traded in a Lincoln or Cadillac to make their purchase.
In December 1989, Lexus initiated a voluntary recall of all 8,000 LS 400s sold to date, based upon two customer complaints over defective wiring and an overheated brake light. In a sweeping 20-day operation which replaced the parts on all affected vehicles, Lexus sent technicians to pick up, repair, and return cars to customers free of charge, and also flew in personnel and rented garage space for owners in remote locations. This response was lauded in media publications and helped establish the marque's early reputation for customer service.
By 1989's end, 16,392 LS 400 and ES 250 sedans had been sold in the four months following the U.S. launch. Although sales had begun at a slower pace than expected, the final tally matched the division's target of 16,000 units for that year. Following initial models, plans called for the addition of a sports coupe along with a redesigned ES sedan.
1990s: Growth and expansion.
In 1990, during its first full year of sales, Lexus sold 63,594 LS 400 and ES 250 sedans in the U.S., the vast majority being the LS model. That year, Lexus also began limited exports to the United Kingdom, Switzerland, Canada, and Australia. In 1991, Lexus launched its first sports coupe, the SC 400, which shared the LS 400’s V8 engine and rear-wheel drive design. This was followed by the second generation ES 300 sedan, which succeeded the ES 250 and became Lexus' top seller. At the conclusion of 1991, Lexus had become the top-selling premium car import in the U.S., with sales reaching a total of 71,206 vehicles. That year, Lexus ranked highest in J.D. Power and Associates' studies on initial vehicle quality, customer satisfaction, and sales satisfaction for the first time. The marque also began increasing U.S. model prices past those of comparable American premium makes, but still below high-end European models; by 1992, the LS 400's base price had risen 18% to nearly US$.
In 1993, Lexus launched the mid-size GS 300 sports sedan, based on the Toyota Aristo using the Toyota "S" platform from the Toyota Crown, which had sold for two years prior in Japan. The GS 300 was priced below the LS 400 in the marque's lineup. That same year, Lexus also became one of the first marques to debut a certified pre-owned program, with the aim of improving trade-in model values. In 1994, the marque introduced the second generation LS 400, a complete redesign of its flagship model. In May 1995, sales were threatened by the U.S. government's proposal of 100% tariffs on upscale Japanese cars in response to the widening U.S.-Japan trade deficit. SUVs were exempt from the proposed sanctions. Normal sales operations resumed by late 1995 when the Japanese auto manufacturers collectively agreed to greater American investments, and the tariffs were not enacted.
In 1996, Lexus debuted its first sport utility vehicle, the LX 450, followed by the third generation ES 300 sedan. The marque's plans for developing an SUV model had accelerated during the U.S.-Japan tariff discussions of 1995. In 1998, Lexus added the first luxury-branded crossover SUV, the RX 300, and the second generation GS 300 and GS 400 sedans. The RX crossover targeted suburban buyers who desired an upmarket SUV but did not need the LX's off-road capability; it was particularly successful, becoming the marque's top-selling model ahead of the ES 300. The same year, Lexus made its debut in South America's most populous country when it launched sales in Brazil. In 1999, Lexus recorded its one-millionth vehicle sold in the U.S. market, and was ranked as the top-selling premium car maker in the U.S. overall.
2000s: Global reorganization.
In 2000, Lexus introduced the IS line, a new series of entry-level sport sedans. In 2001, the marque debuted its first convertible, the SC 430, a redesigned ES 300, and the third generation LS 430. The GX 470 mid-size SUV debuted in 2002, followed by the second generation RX 330 in 2003. The following year, Lexus recorded its two-millionth U.S. vehicle sale, and debuted the first luxury-branded production hybrid SUV, the RX 400h. This vehicle used a Lexus Hybrid Drive system which combined gasoline and electric motors for increased power, fuel efficiency, and lower emissions relative to gasoline-only equivalents.
In 2005, Lexus completed an organizational separation from parent company Toyota, with dedicated design, engineering, training, and manufacturing centers working exclusively for the division. This effort coincided with Lexus' launch in its home market of Japan and an expanded global launch of the brand in major world markets such as China. Executives aimed to increase Lexus sales outside of its largest market in the U.S. To accompany this expansion, next generation Lexus vehicles were redesigned as "global models" for international release. In the European market, where Lexus had long faced struggling sales owing to low brand recognition, few dedicated dealerships, and 1990s import quotas, the marque announced plans to introduce hybrid and diesel powertrains, increase the number of Lexus dealerships, and expand operations in emerging markets such as Russia.
Lexus' arrival in the Japanese market in July 2005 marked the first introduction of a Japanese premium car marque in the domestic market. New generation LS, IS, ES, GS and RX models subsequently became available in Japan along with the SC 430, ending domestic sales of Toyota-branded models under the Celsior, Altezza, Windom, Aristo, Harrier and Soarer nameplates, respectively. The Altezza and Aristo were previously exclusive to Japanese Toyota retail sales channels called Toyota Vista Store, the Windom was exclusive to Toyota Corolla Store, the Celsior and Harrier were exclusive to Toyopet Store, and the Soarer was previously available at both Toyota Store and Toyopet Store locations. Lexus models sold in Japan featured higher specifications and a price premium (from ¥1-million and up) compared with their discontinued Toyota counterparts. Sales for the first half-year were slower than expected,
affected by the contraction of the domestic auto market and price increases, but improved in subsequent months with an expanded lineup.
Through the mid-2000s, Lexus experienced sales successes in South Korea and Taiwan, becoming the top-selling import make in both markets in 2005; the marque also sold well in the Middle East, where it ranked first or second among rivals in multiple countries, and in Australia, where Lexus reached third in luxury car sales in 2006. Division executives in 2006 announced an expansion goal from 68 countries to 76 worldwide by 2010. By the end of the decade, this expansion resulted in official launches in Malaysia and South Africa in 2006, Indonesia in 2007, Chile in 2008, and the Philippines in 2009.
Hybrids and F models.
In 2006, Lexus began sales of the GS 450h, a V6 hybrid performance sedan, and launched the fourth generation flagship LS line, comprising both standard- and long-wheelbase V8 (LS 460 and LS 460 L) and hybrid (LS 600h and LS 600h L) versions. The fifth generation ES 350 also debuted in the same year. The LS 600h L subsequently went on sale as the most expensive sedan ever produced in Japan, with a sticker price of approximately US$. By the end of 2006, Lexus' annual sales had reached 475,000 vehicles worldwide. In January 2007, Lexus announced a new F marque performance division, which would produce racing-inspired versions of its performance models. The first of this line, the IS F, made its debut at the 2007 North American International Auto Show, accompanied by a supercar concept, the LF-A.
In October 2007, Lexus entered the Specialty Equipment Market Association show in the U.S. for the first time with the IS F, and announced its F-Sport performance trim level and factory-sanctioned accessory line. Automotive columnists noted Lexus' increased emphasis on sporty models as an effort to bolster the marque's performance credentials and target rivals from Mercedes-Benz's AMG and BMW's M divisions. While previous Lexus models such as the SC 400 and GS 400 had received favorable reactions from sport luxury buyers, other Lexus models had been characterized as favoring comfort over sporty road feel and handling, compared with European rivals. By the end of 2007, Lexus annual worldwide sales had surpassed 500,000 vehicles, and the marque ranked as the top-selling premium import in China for the first time. The largest sales markets in order of size for 2007 were the U.S., Japan, the UK, China, Canada, and Russia.
In 2008, amidst the late-2000s recession and a weakened world car market, global sales fell 16% to 435,000, with declines in markets such as the U.S. and Europe where deliveries fell by 21% and 27.5%, respectively. In 2009, the marque launched the HS 250h, a dedicated hybrid sedan for North America and Japan, the RX 450h, the second generation hybrid SUV replacing the earlier RX 400h, and later that year debuted the US$ production LFA exotic coupe. In late 2009, citing higher sales of hybrid models over their petrol counterparts, Lexus announced plans to become a hybrid-only marque in Europe. By the end of the decade, Lexus ranked as the fourth-largest premium car make in the world by volume, and was the number-one-selling premium car marque in the U.S. for ten consecutive years.
2010s: Recent developments.
In 2010, Lexus underwent a gradual sales recovery in North America and Asia as the marque focused on adding hybrids and new model derivatives. Sales in the U.S. held steady despite the 2009–2010 Toyota vehicle recalls, several of which included Lexus models. The ES 350 and certain IS models were affected by a recall for potentially jamming floor mats, while parent company Toyota bore the brunt of negative publicity amid investigations over its series of product recalls and problem rates per-vehicle. The redesigned GX 460 was also voluntarily recalled in April 2010 for a software update, one week after "Consumer Reports" issued a recommendation not to buy the SUV, citing a possible rollover risk following the slow stability control response to a high-speed emergency turn. Although the publication knew of no reported incidents, the GX 460 received updated stability control software.
In late 2010 and early 2011, Lexus began sales of the CT 200h, a compact four-door hybrid hatchback designed for Europe, in multiple markets. Sales of lower-displacement regional models were also expanded, beginning with the ES 240 in China followed by the RX 270; Japan, Russia, and Taiwan were among markets which received model variants intended for reduced emissions or import taxes. In March 2011, the Tōhoku earthquake and tsunami caused severe disruption to Lexus' Japan-based production lines, hindering the marque's near-term sales prospects. Lexus' U.S. executives stated that due to vehicle shortages amidst close competition from BMW, Mercedes-Benz, and Audi, the marque would not remain the country's top-selling premium car brand. 
Cumulative sales results for 2011 indicated a 14% sales drop in the U.S. market, along with sales increases of 40% and 27% in Europe and Japan respectively, for a global sales total of 410,000 units. Lexus' streak of eleven consecutive years as the best-selling luxury marque in the U.S. ended that year, with the title going to BMW followed by Mercedes-Benz. For 2011 while 45 percent of Lexus sales in the United States relied upon the RX luxury crossover SUV, rival Mercedes-Benz's best-selling offering was the E-Class mid-luxury sedan which commands considerably higher prices. Subsequently, Toyota chairman Akio Toyoda vowed to restore passion to the marque and further increase its organizational independence, admitting that "...back then we did not regard Lexus as a brand, but as a distribution channel". As a result of Toyoda's organizational changes, Lexus senior managers report directly to the chairman for the first time in the marque's history.
In January 2012, the marque began sales of the fourth generation GS line, including GS 350 and GS 450h variants, as well as a lower-displacement GS 250 model for select markets. In April 2012, the sixth generation ES line, include ES 350 and ES 300h variants, debuted at the New York International Auto Show.
In August 2014, Toyota announced it would be cutting its Lexus spare parts prices in China by up to 35%. The company admitted the move was in response to a probe foreshadowed earlier in the month by China's National Development and Reform Commission of Lexus spare parts policies, as part of an industry-wide investigation into what the Chinese regulator considers exorbitantly high prices being charged by automakers for spare parts and after-sales servicing.
Corporate affairs.
Management.
Lexus International, headed by managing officer Tokuo Fukuichi, coordinates the worldwide operations of Toyota's luxury division. Other executives at Lexus' global headquarters, located in Nagoya, Aichi, include Mark Templin, executive vice president of Lexus International, and managers of the marque's Japan Sales & Marketing and global Product & Marketing Planning divisions. While organizationally separate from its parent company, Lexus International reports directly to Toyota chief executive officer Akio Toyoda.
In the U.S., Lexus operations are headed by Jeffrey Bracken, group vice president and general manager of the U.S. Lexus division, located in Southern California. In Europe, Lexus operations are headed by Alain Uyttenhoven, vice president of Lexus Europe, located in Brussels. Companion design facilities are located in Southern California and central Japan, with the head design studio devoted entirely to Lexus models in Toyota City, Aichi.
Regional operations.
Lexus sales operations vary in structure by region. In many markets, such as the U.S., the dealership network is a distinct organization from corporate headquarters, with separately owned and operated Lexus showrooms. By contrast, in Japan all 143 dealerships in the country are owned and operated by Lexus. Several markets have a designated, third party regional distributor; for example, in the United Arab Emirates, sales operations are managed by Al-Futtaim Motors LLC, and in Costa Rica, Lexus vehicles are sold via regional distributor Purdy Motors S.A. Other officially sanctioned regional distributors have sold Lexus models prior to the launch of, or in absence of, a dedicated dealership network.
Financial performance.
Financial data of Lexus operations are not disclosed publicly. However, automotive analysts estimate that the Lexus division contributes a disproportionate share of Toyota's profits, relative to its limited production and sales volume. Interviews with retired division officials indicate that depending on sales volume, vehicle product development cycles, and exchange rates, Lexus sales have accounted for as much as half of Toyota's annual U.S. profit in certain years. Division executives have employed pricing strategies aimed at sustaining profit margins rather than sales volume, with historically fewer price incentives than rival brands. In 2006, Lexus entered Interbrand's list of the Top 100 Global Brands for the first time, with an estimated brand value of approximately US$ annually. In 2009, Interbrand ranked Lexus as Japan's seventh largest brand, between Panasonic and Nissan, based on revenue, earnings, and market value.
Automobiles.
Vehicle lineup.
The global Lexus lineup features sedans of different size classes, including the compact IS and HS models, mid-size ES and GS models, and the full-size LS flagship. Convertibles include the IS C models. Sport-utility vehicles range in size from the crossover RX, the mid-size GX, to the full-size LX. Hybrid models include the CT hatchback, HS sedan, and variants of the GS, LS, and RX. The F marque line produces a variant of the IS sedan and the LFA coupe.
F marque.
Lexus produces its highest-performance models under its F marque division. The name refers to Flagship and Fuji Speedway in Japan, whose first corner, 27R, inspired the shape of the "F" emblem. F marque models are developed by the Lexus Vehicle Performance Development Division. The first F marque model, the IS F, went on sale in 2007, followed by the LFA in 2009. A related F-Sport performance trim level and factory-sanctioned accessory line is available for standard Lexus models such as the IS 250 and IS 350. F-Sport succeeded an earlier in-house tuning effort, the TRD-based L-Tuned, which had offered performance packages on the IS and GS sedans in the early 2000s.
The latest editions to the performance F marque include the Lexus RC F and upcoming Lexus GS F.
Model nomenclature.
Lexus production models are named alphanumerically using two-letter designations followed by three digits. The first letter indicates relative status in the Lexus model range (ranking), and the second letter refers to car body style or type (e.g. LS for 'luxury sedan'). The three digits indicate engine displacement in liters multiplied by a factor of one hundred (e.g. 350 for a 3.5 L engine). A space is used between the letters and numbers. The same letter may be used differently depending on the model; 'S' can refer to 'sedan' or 'sport' (e.g. in LS and SC), while 'X' refers to 'luxury utility vehicle' or SUV. On hybrids, the three digits refer to the combined gasoline-electric output. For certain models, a lower case letter placed after the alphanumeric designation indicates powerplant type ('h' for hybrid, 'd' for diesel), while capital letter(s) placed at the end indicates a class subtype (e.g. 'L' for long-wheelbase, 'C' for coupe, 'AWD' for all-wheel drive). On F marque models, the two-letter designation and the letter 'F' are used with no numbers or hyphens (e.g. IS F).
Design and technology.
Lexus design has traditionally placed an emphasis on targeting specific vehicle development standards. Since the marque's inception, design targets have ranged from aerodynamics and ride quality to interior ergonomics. The backronym "IDEAL" ("Impressive, Dynamic, Elegant, Advanced, and Lasting") is used in the development process. Each vehicle is designed according to approximately 500 specific product standards, known as "Lexus Musts," on criteria such as leather seat stitching. Design elements from the marque's concept vehicle line, the LF series (including the 2003 LF-S and 2004 LF-C), have been incorporated in production models.
Vehicle cabins have incorporated electroluminescent Optitron gauges, SmartAccess, a smart key entry and startup system, and multimedia features. Beginning with the 2010 RX and HS models, the Remote Touch system, featuring a computer mouse-like controller with haptic feedback, was introduced; other models have featured touchscreen controls (through the 2009 model year) as a navigation screen interface. In 1989, Lexus became among the first premium car marques to equip models with premium audio systems, in partnership with stereo firm Nakamichi. Since 2001, optional surround sound systems are offered via high-end audio purveyor Mark Levinson. For reduced cabin noise, the first LS 400 introduced sandwich steel plating, and later models added acoustic glass. In 2006, the LS 460 debuted the first ceiling air diffusers and infrared body temperature sensors in a car. Telematics services include G-Book with G-Link in Asia and Lexus Enform in North America.
In 2006, Lexus incorporated the first production eight-speed automatic transmission in an automobile with the LS 460, and the gearbox was later adapted for the GS 460 and IS F models. Continuously variable transmissions, regenerative brakes, and electric motors have been used on all Lexus hybrid models. In 2007, Lexus executives signaled intentions to equip further models with hybrid powertrains, catering to demands for a decrease in both carbon pollution and oil reliance. Hybrid models have been differentiated by separate badging and lighting technology; in 2008, the LS 600h L became the first production vehicle to use LED headlamps.
Safety features on Lexus models range from stability and handling programs (Vehicle Stability Control and Vehicle Dynamics Integrated Management) to backup cameras, swivel headlights, and sonar warning systems. The Lexus Pre-Collision System (PCS) integrates multiple safety systems. In 2007, Lexus introduced the first car safety systems with infrared and pedestrian detection capabilities, lane keep assist, a Driver Monitoring System with facial recognition monitoring of driver attentiveness, and rear pre-collision whiplash protection, as part of the LS 460 PCS. As a safety precaution, Lexus GPS navigation systems in many regions feature a motion lockout when the vehicle reaches a set speed; to prevent distraction, navigation inputs are limited, while voice input and certain buttons are still accessible. This safety feature has attracted criticism because passengers cannot use certain functions when the vehicle is in motion. Pre-2007 models came with a hidden manufacturer override option, and updated European models allow operation in motion.
Production models in development have included convertibles, crossovers, and dedicated hybrids. Under the F marque, Lexus plans to produce high-performance vehicles with its first expressions being the IS F and the LFA. Lexus officials have also discussed standard production model usage of varying platforms. The LS flagship uses a dedicated platform, while the entry-level Lexus ES had been criticized for being too similar to the Toyota Camry, with which it shared platforms until its sixth generation, in both styling and powertrain design. The Nürburgring test track in Germany has also seen Lexus prototype testing.
L-finesse.
Lexus introduced a new design language known as "L-finesse" in the mid-2000s with its LF series concepts and the 2006 Lexus GS. L-finesse is represented by three Japanese kanji characters which translate as "Intriguing Elegance, Incisive Simplicity, and Seamless Anticipation". Design characteristics, including a fastback profile, lower-set grille, and the use of both convex and concave surfaces, are derived from Japanese cultural motifs (e.g. the phrase "kirikaeshi" in arrowhead shapes). While earlier Lexus models were criticized for reserved and derivative styling, and often mistaken for understated domestic market cars, automotive design analyses described L-finesse as adding a distinctive nature and embrace of Japanese design identity. Opinions varied for L-finesse's debut on the GS; "Sports Car International"'s analysis praised the vehicle's in-person appearance; "Automobile Magazine" criticized the daring of its forward styling, and compared subsequent rival models for design similarities. In 2012, the arrival of the redesigned fourth generation Lexus GS featured the introduction of a spindle-shaped grille design, intended to be used on all forthcoming Lexus models. L-finesse exhibitions were presented at Milan's Salone del Mobile from 2005 through 2009.
Production.
Assembly plants.
The first Lexus vehicles were manufactured in Toyota's flagship Tahara plant, a highly sophisticated, computerized manufacturing plant in Japan. Lexus production techniques include methods and standards of quality control that differ from Toyota models. At the Tahara plant, separate assembly lines were developed for Lexus vehicles, along with new molds and specialized manufacturing equipment. Welding processes, body panel fit tolerances, and paint quality requirements are more stringent. Lexus plant workers, typically veteran technicians, are identified via repeated performance evaluations and ranked according to skill grade, with limited applicants accepted. The highest level "takumi" (Japanese for "artisan") engineers are responsible for maintaining production standards at key points in the assembly process, such as testing engine performance. Production vehicles are given visual inspections for flaws, individually test-driven at high speeds, and subjected to vibration tests.
Through the 2000s, most Lexus sedan and SUV production has occurred in Japan at the Tahara plant in Aichi and Miyata plant in Fukuoka. In addition to the Tahara factory, Lexus vehicles have been produced at the Miyata plant (Toyota Motor Kyushu, Inc.) in Miyawaka, Fukuoka, Higashi Fuji plant (Kanto Auto Works, Ltd.) in Susono, Shizuoka, and Sanage plant (Toyota Boshoku Corp.; Araco) in Toyota City, Aichi. Front-wheel drive cars, such as the ES and HS, are produced in the Fukuoka Prefecture. The Kokura plant in Kitakyushu, Fukuoka, which opened in 2008, is a dedicated hybrid production site for Lexus models such as the gasoline-electric RX. The North American–market RX 350 (since the 2004 model year) is produced at the Cambridge plant (Toyota Canada, Inc.) in the city of Cambridge, in Ontario, Canada, which is the first Lexus production site located outside of Japan. In 2005, J.D. Power and Associates bestowed its Platinum award for worldwide plant quality on the Tahara plant for the fourth consecutive year, stating that it has the fewest defects of any manufacturing plant in the world. In 2006, J.D. Power named the Miyata plant, then the site of ES and IS model production, as its recipient of the Platinum award for worldwide plant quality, and in 2009 the Higashi Fuji plant, site of SC production, received the same recognition.
Quality rankings.
In industry ratings of build quality, owner satisfaction, and reliability, Lexus vehicles have outperformed other manufacturers in successive years. J.D. Power and Associates has named Lexus the most reliable brand in the U.S. over fourteen times since 1995, according to its Vehicle Dependability Survey of over 53,000 vehicle owners and the first three years of ownership. In the J.D. Power Owner Satisfaction Survey of over 16,000 vehicle owners in the UK and the first one to three years of ownership, Lexus has scored the highest of all manufacturers for over ten years in a row; in 2011, the marque was again the top manufacturer and the Lexus IS was the highest ranked of all cars surveyed. In the 2000s, "Consumer Reports" named Lexus among the top five most reliable brands in its Annual Car Reliability Surveys of over one million vehicles across the U.S. The marque has also topped the J.D. Power Initial Quality Index, which measures vehicle problems in the first 90 days of ownership.
Service.
Lexus has become known for efforts to project an upscale image, particularly with service provided after the sale. The waiting areas in service departments are replete with amenities, ranging from refreshment bars to indoor putting greens. Dealerships typically offer complimentary loaner cars or "courtesy cars" and free car washes, and some have added on-site cafes and designer boutiques. Service bays are lined with large picture windows for owners to watch the servicing of their vehicle. In 2005, Lexus also began reserving parking lots at major sporting arenas, entertainment events, and shopping malls, with the only requirement for free entry being the ownership of a Lexus vehicle. An online owner publication, "Lexus Magazine", features automotive and lifestyle articles and is published online monthly and on a mobile site.
Since 2002, Lexus has scored consecutive top ratings in the "Auto Express" and 76,000-respondent "Top Gear" customer satisfaction surveys in the UK. Lexus has also repeatedly topped the 79,000-respondent J.D. Power Customer Service Index and Luxury Institute, New York surveys in the U.S. As a result of service satisfaction levels, the marque has one of the highest customer loyalty rates in the industry. To improve customer service, employees are instructed to follow the "Lexus Covenant," the marque's founding promise (which states that "Lexus will treat each customer as we would a guest in our home"), and some dealerships have incorporated training at upscale establishments such as Nordstrom department stores and Ritz-Carlton hotels.
Motorsport.
Lexus first entered the motorsport arena in 1999 when its racing unit, Team Lexus, fielded two GS 400 race vehicles in the Motorola Cup North American Street Stock Championship touring car series. In its 1999 inaugural season, Team Lexus achieved its first victory with its sixth race at Road Atlanta. Led by Sports Car Club of America and International Motor Sports Association driver Chuck Goldsborough, based in Baltimore, Maryland, Team Lexus capitalized on the debut of the first generation Lexus IS by entering three IS 300s in the third race of the 2001 Grand-Am Cup season at Phoenix, Arizona. Team Lexus won its first IS 300 victory that year at the Virginia International Raceway. In 2002, Team Lexus' competitive efforts in the Grand-Am Cup ST1 (Street Tuner) class achieved victories in the Drivers' and Team Championships, as well as a sweep of the top three finishes at Circuit Mont-Tremblant in Quebec, Canada.
After the release of the Lexus brand in the Japanese domestic market in 2005, Lexus sanctioned the entry of four SC 430 coupes in the Super GT series of the All Japan Grand Touring Car Championship in the GT500 class. In the first race of the 2006 series, an SC 430 took the chequered flag, and drivers André Lotterer and Juichi Wakisaka raced the SC 430 to capture the GT500 championship for that year. In 2007, another SC 430 won the GT500 opening round race. In 2006, Lexus raced a hybrid vehicle for the first time, entering a GS 450h performance hybrid sedan in partnership with Sigma Advanced Racing Development at the 24 Hours of Tokachi race in Hokkaido, Japan. Lexus Canada also entered the GS 450h in 2007's Targa Newfoundland event. In 2009, Lexus Super GT Team SC 430 and IS 350 racers won the GT500 and GT300 championships, respectively.
Lexus' participation in endurance racing further includes the Rolex 24 Hours of Daytona, sanctioned by the Grand American Road Racing Association. After entering the Rolex Sports Car Series in 2004, Lexus has won over 15 Rolex Series event races. In 2005, Lexus was runner-up, and in 2006, it won the championship. Although Toyota has won this race in the past, it was the first time that its luxury arm emerged as the winner. In 2007, six Lexus-powered Daytona prototypes were entered in the Rolex 24 Hours of Daytona event at the Daytona International Speedway. Lexus was a repeat winner of the event, with a Lexus-Riley prototype driven by Scott Pruett, Juan Pablo Montoya, and Salvador Durán of Chip Ganassi Racing finishing first; Lexus-Riley prototypes also took three of the top ten spots. In 2008, Lexus won its third consecutive win at Daytona. For the 2010 season, Lexus departed from the Rolex Sports Car Series, and Ganassi Racing switched to BMW/Dinan engines. The LF-A prototype also competed on the Nürburgring since 2008 in VLN endurance races and in the 24 Hours Nürburgring, also with the IS F.
On May 14, 2011, a CT 200h tuned up by Gazoo Racing competed in the Adenauer ADAC Rundstrecken-Trophy, a six-hour endurance race.
Marketing.
From its inception, Lexus has been advertised to luxury consumers using specific marketing strategies, with a consistent motif used for the marque's advertisements. Beginning in 1989, television ads were narrated by actor James Sloyan (the voice of "Mr. Lexus" until 2009), and accompanied by vehicles that performed unusual stunts onscreen. The first decade of Lexus commercials (1989–99) consisted primarily of disjunctive verbal descriptions, such as "relentless," "pursuit," and "perfection," while vehicles were used to claim superiority in precision, idling, and interior quiet and comfort on camera. Examples included the champagne glass "Balance" (1989) and rolling "Ball Bearing" (1992). In the 2000s, commercials included descriptions of novel features, or a narration of the events onscreen, and were often targeted at the marque's German competitors. An annual "December to Remember" campaign featured scenes of family members surprising loved ones with the gift of a new Lexus. The marque returned to the champagne glass theme in a 2006 LS 460 spot showing the sedan maneuvering between two stacks of glasses using its self-parking system, and in a 2010 LFA spot showing its engine sound shattering a glass via resonance frequency.
Industry observers have attributed Lexus' early marketing successes to higher levels of perceived quality and lower prices than competitors, which have enabled the marque to attract customers upgrading from mass-market cars. A reputation for dependability, bolstered by reliability surveys, also became a primary factor in attracting new customers from rival premium makes. Lexus has since grown to command higher price premiums than rival Japanese makes, with new models further increasing in price and reaching the over-US$ ultra-luxury category long dominated by rival European marques.
Automotive analysts have also noted Lexus' relative newcomer status as a marketing challenge for the brand, although some have debated the requirement of a long history. European rivals have marketed their decades of heritage and pedigree, whereas Lexus' reputation rests primarily upon its perceived quality and shared history with parent company Toyota. Several analysts have stated that Lexus will have to develop its own heritage over time by highlighting technological innovations and producing substantial products.
Lexus' marketing efforts have extended to sporting and charity event sponsorships, including the U.S. Open tennis Grand Slam event from 2005 to 2009, and the United States Golf Association's U.S. Open, U.S. Women's Open, U.S. Senior Open, and U.S. Amateur tournaments since 2007. Lexus has organized an annual Champions for Charity golf series in the U.S. since 1989. Endorsement contracts have also been signed with professional athletes Andy Roddick, Annika Sörenstam, and Peter Jacobsen.

</doc>
<doc id="18330" url="http://en.wikipedia.org/wiki?curid=18330" title="Legal aspects of transsexualism">
Legal aspects of transsexualism

A person may be considered to be a transsexual person if their gender identity is incongruent with the sex they were assigned at birth, and consequently also with the gender role and social status that is typically associated with that sex. They may have, or may intend to establish, a new gender status that accords with their gender identity. Most legal jurisdictions recognise the two traditional gender identities and social roles, man and woman, but tend to exclude any other gender identities, and expressions. There is now a greater understanding of the breadth of variation outside the typical categories of 'man, and 'woman', and many self-descriptions are now entering the literature, including pan-gender, polygender, gender queer and non gender. Medically and socially the term 'transsexualism' is being replaced with 'gender dysphoria, and terms such as 'transgender people' and 'trans men and trans women' are replacing the narrow category of transsexual people.
This raises many legal issues and aspects of transsexualism. Most of these issues are generally considered a part of family law, especially the issues of marriage and the question of a transsexual person benefiting from a partner's insurance or social security.
The degree of legal recognition provided to transsexualism varies widely throughout the world. Many countries now legally recognise sex reassignments by permitting a change of legal gender on an individual's birth certificate. Many transsexual people have permanent surgery to change their body, Sexual Reassignment Sugery (SRS) or semi-permanently change their body by hormonal means, Hormone Replacement Therapy (HRT). In many countries, some of these modifications are required for legal recognition. In a few, the legal aspects are directly tied to health care; i.e. the same bodies or doctors decide whether a person can move forward in their treatment, and the subsequent processes automatically incorporate both matters.
In some jurisdictions, transgender people (who are considered non-transsexual) can benefit from the legal recognition given to transsexual people. In some countries, an explicit medical diagnosis of "transsexualism" is (at least formally) necessary. In others, a diagnosis of "gender dysphoria," or simply the fact that one has established a non-conforming gender role, can be sufficient for some or all of the legal recognition available. The DSM-V recognizes Gender Dysphoria as the official diagnoses.
Europe.
A majority of countries in Europe give transsexual people the right to at least change their first name, most of which also provide a way of changing birth certificates. Several European countries recognize the right of transsexuals to marry in accordance with their post-operative sex. Croatia, Czech Republic, Denmark, Finland, France, Germany, Italy, the Netherlands, Norway, Poland, Portugal, Romania, Sweden, Spain, Turkey, and the United Kingdom all recognize this right. The Convention on the recognition of decisions regarding a sex change provides regulations for mutual recognition of sex change decisions and has been signed by five European countries and ratified by Spain and the Netherlands.
France.
In France, there is currently no law that defines sex-change procedures. However, it is possible to ask for a sex- or a name change before the Court. The judge decides to grant or refuse the change.
Germany.
The Transsexuellengesetz.
Since 1980, Germany has a law that regulates the change of first names and legal gender. It is called "Gesetz über die Änderung der Vornamen und die Feststellung der Geschlechtszugehörigkeit in besonderen Fällen ( – TSG)" (Law about the change of first name and determination of gender identity in special cases (Transsexual law – TSG)).
In Germany, as in many countries whose law is at least partly based on the Napoleonic Code, the first name had to be gender-specific. Since a ruling of the supreme Federal Constitutional Court of Germany in 2010, gender-neutral names can be accepted. One can either obtain a change of name alone, and proceed later with a change of legal gender, if possible or desired, or obtain both in a single legal procedure.
For both, two independent medical court experts have to be commissioned by the judge. They are asked to evaluate, whether
Usually, the court will rule according to those opinions.
The name change becomes legally void, if a child of his/her descent is born more than 300 days after name change, 
For the change of legal gender, it was also once required that the person
These requirements were declared unconstitutional by supreme court ruling in a 2011.
Originally, the law stated that neither change of name nor legal gender were available for people under 25 years of age. This condition has been declared void by the courts, and today there is no minimum age. Until 2008, the person had to be unmarried.
The TSG applies only to German citizens; there are exceptions only for non-German citizens with very specific legal status, such as stateless people living legally in Germany, or in case the foreign state has no equivalent law, which would be in accordance with German constitution
Several court decisions have further specified several matters. For example, a person with only a name change has the right to be called "Herr" or "Frau" (Mr. or Mrs.) according to their first name, not their legal gender; similarly, documents have to be issued reflecting their actual gender identity, not legal gender. Job references, certifications and similar from the time before the change of name may be reissued with the new name, so effectively there is no way for a new employer to learn about the change of name and/or legal gender. Also, people with only a name change do not have to divulge their legal gender to employers.
Republic of Ireland.
In the Republic of Ireland, it is not possible for a transsexual person to alter their birth certificate. A case was taken in the High Court by Lydia Foy in 2002, which saw her case being turned down as a birth certificate was deemed to be an historical document. Even so, it is currently possible for anyone to undertake a change of name either through common usage or through a deed of change of name, but this does not amount to legal recognition and transgender persons cannot marry or enter into a civil partnership in their acquired gender.
Foy had taken new proceedings to the High Court relying on the decisions of the European Court of Human Rights in the Goodwin and 'I' cases. Her application was heard between 17 and 26 April 2007, and judgment was reserved. Judgment was given in the High Court on 19 October 2007. The Judge held that the Irish State had failed to respect Foy's rights under Article 8 of the European Convention on Human Rights by not providing any mechanism for her to obtain a new birth certificate in her female gender. He indicated that he would grant a declaration that Irish law in this area was incompatible with the Convention. He also said he would have found that her right to marry under Article 12 of the Convention had been infringed as well if that had been relevant. On 14 February 2008. the Judge granted a declaration that sections of the Civil Registration Act 2004 were incompatible with Article 8 of the Convention. This was the first declaration of incompatibility made under the European Convention of Human Rights Act passed in 2003.
The Government appealed this decision but dropped its appeal in June 2010 and set up an advisory group of civil servants to make recommendations for new legislation. The advisory group's report was publshed in July 2011 but there was controversy over some of its recommendations, notably that married transgender persons would have to divorce before they could be recognised in their acquired gender. At the launch of the report the Minister responsible stated that the Government would introduce gender recognition legislation as soon as possible. No legislation had been introduced by February 2013 and Foy issued new proceedings in the High Court, seeking a declaration that the State was obliged to issue her with a new birth certificate in her female gender, or, alternatively, that the State was in breach of the Irish Constitution or the European Convention on Human Rights because it had failed to provide her with an effective remedy for the violation of her rights 
Poland.
The first milestone sentence in the case of gender shifting was given by Warsaw's Voivode Court in 1964. The court reasoned that it be possible, in face of civil procedure and acting on civil registry records, to change one's legal gender after their genital reassignment surgery had been conducted. In 1983, the Supreme Court ruled that in some cases, when the attributes of the individual's preferred gender were predominant, it is possible to change one's legal gender even before genital reassignment surgery.
Although in the Polish legal system there is not any specific institution or act considering it, the right to change one's legal gender is generally recognized. Article 189 of Polish Civil Procedure Code allows an individual to ask court to determine their right or legal relations in many contexts, including gender and civil registry records. On the grounds of this provision, Polish courts often approve legal claims for modyfing registry records, name and all other public documents. At the same time, it is practically necessary that the court should be provided with medical evidences of one's transsexuality.
In 2011, Anna Grodzka, the first transsexual MP in the history of Europe who underwent a sex-change operation was appointed. In the Polish Parliamentary Election 2011 she gained 19 337 votes (45 079 voted for her party in the constituency) in the City of Cracow and came sixth in her electoral district (928 914 people, voter turnout 55,75%). Grodzka is reportedly the only transsexual person with ministerial responsibilities in the world since 10 November 2011.
Romania.
In Romania, it is legal for transgender people to change their first name to reflect their gender identity, based on personal choice. Since 1996, it is also possible for someone who has gone through genital reassignment surgery to change their legal gender in order to reflect their post-operative sex. Transsexuals then have the right to marry in accordance with their post-operative sex.
Spain.
Since 15 March 2007, a new law in Spain allows transsexual people to modify their name and legal gender in all public documents and records on the basis of a personal request, regardless of whether they have received genital reassignment surgery or not. However, medical (hormonal) treatment for at least two years and a diagnosis of gender dysphoria are both prerequisites. The hormonal treatment is not a prerequisite if there are reasons, relating to health or age, not to follow it.
United Kingdom.
Historically in the United Kingdom, transsexual people have succeeded in having their birth certificates changed and marriages conducted. This was first legally challenged in the 1960s, in the case of Ross Alexander, where the Court of Session ruled that the certificate change was legitimate for the purposes of inheriting a title, a decision later upheld by the Home Secretary. However, the case was held secretly and in a Scottish court, and there was not a publicly reported case in an English court until 1970. That year, in the case of "Corbett v Corbett", Arthur Corbett attempted to annul his marriage to April Ashley on the grounds that transsexuals were not recognised by English law. It was decided that, for the purposes of marriage, a post-operative transsexual was considered to be of the sex they were assigned at birth.
This set the precedent for the coming decades. People who thought that they had existing valid marriages, turned out not to, and the previous unofficial changing of birth certificates was stopped. Even so, transsexual people were able to change their names freely, to have their passports and driving licences altered, to have their National Insurance details changed, and so forth, and the Sex Discrimination Act 1975 made it illegal to discriminate on the ground of anatomical sex in employment, education and the provision of housing, goods, facilities and services.
In the 1980s and 1990s the pressure group, Press for Change, campaigned in support for transgender and transsexual people to be allowed to marry, and helped people take several cases to the European Court of Human Rights. In "Rees v. United Kingdom" (1986), it was decided that the UK was not violating any human rights, but that they should keep the situation under review. The Sex Discrimination (Gender Reassignment) Regulations 1999 extended the existing Sex Discrimination Act, and made it illegal to discriminate against any person on the grounds of gender reassignment, but only in the areas of employment and vocational training.
In the 2002 case "Goodwin v. United Kingdom", it was decided that the rights to privacy and family life were being infringed. In response to its obligation, the UK Parliament passed the Gender Recognition Act 2004, which effectively granted full legal recognition for transgender people.
The Equality Act 2006 also introduced the Gender Equality Duty in Scotland, which made public bodies obliged to take seriously the threat of harassment or discrimination of transsexual individuals in various situations. In 2008, the Sex Discrimination (Amendment of Legislation) Regulations 2008 extended existing regulation to make it illegal to discriminate when providing goods or services to transsexual individuals. The definition of "transsexual" used in the Gender Equality Duty is still technically the same as that in the Sexual Discrimination Act, but this legislation was also taken to mean to prevent discrimination against all transgender individuals.
The Equality Act 2010 officially adds "gender reassignment" as a "protected characteristic," stating that:
A person has the protected characteristic of gender reassignment if the person is proposing to undergo, is undergoing or has undergone a process (or part of a process) for the purpose of reassigning the person's sex by changing physiological or other attributes of sex.
Some trans rights activists, such as "Transgender Equality & Rights in Scotland", advocate adding the category of "gender identity" "in order to be more clearly inclusive of those transgender people who do not identify as transsexual and do not intend to change the gender in which they live". They also want to introduce measures that would more explicitly include intersex people and clarify protections from discrimination in education, certain kinds of employment, and medical insurance.
s of 2010[ [update]], the Green Party supports a reform of the Mental Health Act in order to remove transgender people from the Psychiatric Disorder Register, which they view as discriminatory.
In contrast to some systems elsewhere in the world, the Gender Recognition process does not require applicants to be post-operative. They need only demonstrate that they have suffered gender dysphoria, and have lived as "your new gender" for two years, and intend to continue doing so until death.
Africa.
South Africa.
The Constitution of South Africa forbids discrimination on the basis of sex, gender and sexual orientation (amongst other grounds). The Constitutional Court has indicated that "sexual orientation" includes transsexuality.
In 2003 Parliament enacted the Alteration of Sex Description and Sex Status Act, which allows a transgender person who has undergone medical or surgical gender reassignment to apply to the Department of Home Affairs to have the sex description altered on their birth record. Once the birth record is altered they can be issued with a new birth certificate and identity document, and are considered "for all purposes" to be of the new sex.
The specific definition of gender reassignment in this Act refers to reassigning a person's sex by changing physiological or other sexual characteristics, and includes any part of such a process. Thus the transgender person is not required to have had genital surgery in order to have the sex description altered.
The application must be accompanied by:
Intersexed persons may equally apply to the Department of Home Affairs to have the sex description altered on their birth record. The application has to be accompanied by reports stating the nature and results of any procedures carried out, if any. The act indicates that the intersex person is not obliged to have had hormonal, surgical or medical intervention. The application needs to be accompanied by reports from a medical practitioner indicating that the person is intersex as well as a report from a qualified psychologist or social worker corroborating that the applicant is living and has lived stably and satisfactorily,for an unbroken period of at least two years, in the gender role corresponding to the sex description under which he or she seeks to be registered.
Americas.
Canada.
Jurisdiction over legal classification of characteristic sex in Canada is assigned to the provinces and territories. This includes legal change of gender classification, for which the requirements vary from one sub-federal jurisdiction to another.
Canada'a Parliament is considering Bill C-279 <http://www.parl.gc.ca/HousePublications/Publication.aspx?Language=E&Mode=1&DocId=6053237> which would amend both the Canadian Human Rights Act and Criminal Code to include protection of gender identity.
In the Bill “gender identity” means, in respect of an individual, the individual’s deeply felt internal and individual experience of gender, which may or may not correspond with the sex that the individual was assigned at birth.
The Bill passed the House of Commons, and is before the Canadian Senate as of June 17, 2013.<http://www.parl.gc.ca/Content/Sen/Chamber/411/Debates/175db_2013-06-17-e.htm?Language=E#43>
United States.
Pursuant to the U.S. Const., Amend. 10, which reserves to the states (or to the people) all powers not assigned to the federal government, the legal classification of characteristic sex is state jurisdiction in the United States. The principle is generally extended to the District of Columbia and U.S. territories, though the federal government has power to overrule any decision those non-state entities might make. Thus, the legal gender of a transsexual (as well as a transsex or intersex) individual in the United States does not have one answer but 56 answers – one for each state, the District of Columbia, and the five inhabited territories (American Samoa, Guam, the Northern Marianas Islands, Puerto Rico, and the US Virgin Islands).
Argentina.
In 2012 the Argentine Congress passed the Ley de Género (Gender Law), which allows individuals over 18 to change the gender marker in their DNI (national ID) on the basis of a written declaration only. Argentina thus became the first country to adopt a gender recognition policy based entirely on individual autonomy, without any requirement for third party diagnosis, surgeries or obstacles of any type.
Asia.
Hong Kong.
The Court of Final Appeal of Hong Kong ruled that a transsexual woman has the right to marry her boyfriend. The ruling was made on the 13th of May, 2013.
On 16 September 2013, a Colombian transgender woman coming from Taiwan claimed that she was discriminated and sexually abused by the customs officers, including being subjected to invasive body searches and denied usage of a female toilet, although Hong Kong officers denied the allegations. After being released, she applied for and was granted refugee status by the United Nations High Commissioner for Refugees (UNHCR), rendering her effectively stateless awaiting acceptance to a third country.
Japan.
On 10 July 2003, the National Diet of Japan unanimously approved a new law that enables transsexual people to amend their legal sex. It is called “性同一性障害者の性別の取扱いの特例に関する法律" (Act on Special Cases in Handling Gender for People with Gender Identity Disorder) The law, effective on 16 July 2004, however, has controversial conditions which demand the applicants be both unmarried and childless. On 28 July 2004, Naha Family Court in Okinawa Prefecture returned a verdict to a transsexual woman in her 20's, allowing her family registry record or koseki to be amended as she was born a female. It is generally believed to be the first court approval under the new law. Despite the fact that sex reassignment surgery and hormone replacement therapy are mandatory for a legal sex change, it is not paid for by national health insurance.
Malaysia.
There is no legislation expressly allowing transsexuals to legally change their gender in Malaysia. The relevant legislations are the Births and Deaths Registration Act 1957 and National Registration Act 1959. Therefore, judges currently exercise their discretion in interpreting the law and defining the gender. There are conflicting decisions on this matter. There is a case in 2003 where the court allowed a transsexual to change her gender indicated in the identity card, and granted a declaration that she is a female. However, in 2005, in another case, the court refused to amend the gender of a transsexual in the identity card and birth certificate. Both cases applied the United Kingdom case of Corbett v Corbett in defining legal gender.
Philippines.
The Supreme Court of the Philippines Justice Leonardo Quisumbing on 12 September 2008, allowed Jennifer Cagandahan, 27, to change both his birth certificate, gender and name from Jennifer to Jeff, to male: “We respect respondent’s congenital condition and his mature decision to be a male. Life is already difficult for the ordinary person. We cannot but respect how respondent deals with his unordinary state and thus help make his life easier, considering the unique circumstances in this case. In the absence of a law on the matter, the court will not dictate on respondent concerning a matter so innately private as one’s sexuality and lifestyle preferences, much less on whether or not to undergo medical treatment to reverse the male tendency due to rare medical condition, congenital adrenal hyperplasia. In the absence of evidence that respondent is an ‘incompetent’ and in the absence of evidence to show that classifying respondent as a male will harm other members of society ... the court affirms as valid and justified the respondent’s position and his personal judgment of being a male." Court records showed that – at 6, he had small ovaries; at 13, his ovarian structure was minimized and he had no breasts and did not menstruate. The psychiatrist testified that "he has both male and female sex organs, but was genetically female, and that since his body secreted male hormones, his female organs did not develop normally." The Philippines National Institutes of Health said "people with congenital adrenal hyperplasia lack an enzyme needed by the adrenal gland to make the hormones cortisol and aldosterone.
This, however, applies "only" to cases involving congenital adrenal hyperplasia and other intersex situations. The Philippine Supreme Court has also ruled that Filipino citizens do not have the right to legally change their sex on official documents (driver's license, passport, birth certificate, Social Security records, etc.) if they are transsexual and have undergone sexual reassignment surgery. The Court said that if the man, now anatomically a female, were to be allowed to legally change his sex it would have “serious and wide-ranging legal and public policy consequences,” citing the institution of marriage in particular.
South Korea.
In South Korea, it is possible for transgender individuals to change their legal gender, although it depends on the decision of the judge for each case. Since the 1990s, however, it has been approved in most of the cases. The legal system in Korea does not prevent marriage once a person has changed their legal gender.
In 2006, the Supreme Court of Korea ruled that transsexuals have the right to alter their legal papers to reflect their reassigned sex. A trans woman can be registered, not only as female, but also as being 'born as a woman'.
While same-sex marriage is not approved by South Korean law, a transsexual woman obtains the marital status of 'female' automatically when she marries to a man, even if she has previously been a 'male' on paper.
In 2013 a court ruled that transsexuals can change their legal sex without undergoing genital surgery.
Oceania.
Birth Certificates.
Estelle Asmodelle was Australia's first legal transsexual with the Births, Deaths and Marriages Dept. (NSW Government). As cited by (18 June 1987 – Australian Telegraph Newspaper.) This was the first time in Australian law history that an adult transsexual was permitted to change their birth certificate to a different sex and soon afterwards the passport law also changed allowing transsexuals to be issued passports with the new sex depicted.
Australia is now one of only a few countries where legal status of the new sex following sex affirmation surgery is granted via a new full birth certificate. Birth certificates are within the jurisdiction of the states, whereas marriage and passports are matters for the Commonwealth. All Australian jurisdictions now recognise the affirmed sex of an individual after surgery unless the person is married.
In the landmark case "New South Wales Registrar of Births, Deaths and Marriages v Norrie" [2014] the High Court of Australia held that the "Births Deaths and Marriages Registration Act 1995" (NSW) did not require that a person who, having undergone a sex affirmation procedure, must identify as either a man or a woman. Rather, the Court refuted the binary notion of sex, and the held that the Act itself recognises that a person may be other than male or female and therefore permits the registration of "non-specific".
Marriage.
"Re Kevin - validity of marriage of transsexual" ([2001] FamCA 1074) is a groundbreaking judgment of the Family Court of Australia, concerning both transsexualism as a phenomenon, the human rights of those who experience transsexualism and the right of people who have experienced transsexualism to enter into a legally valid marriage.
Kevin, an affirmed male, married Jennifer before the case started. Prior to the marriage Kevin had affirmed his male sexual identity by underging hormonal and other sex affirmation treatment; including a double mastectomy and full hysterectomy but not the construction of a phallus. His legal sex had been changed on his birth certificate and other documentation and since his affirmation of his male sex (including as at the time of his marriage and the trial) he had lived in the Australian culture and community as a male.
When the Attorney General for the Commonwealth of Australia advised the couple through his department that in his considered opinion their marriage was not a legally valid one and that the couple (or at least Kevin) was liable to be prosecuted and possibly imprisoned, the couple sought the legal advice and representation of Australian lawyer Rachael Wallbank, herself an affirmed female, and commenced proceedings in the Family Court of Australia against the Attorney General for the Commonwealth of Australia to have their marriage declared legally valid.
The question to be determined by the court was whether Kevin was a man for the purposes of the marriage law of Australia and, hence, whether the marriage ceremony he had undertaken with Jennifer was a valid one. Further, at the time of the trial the couple had one child as a result of approved assisted technology. Their second child was born at the time of the hearing of the appeal proceedings. English law had previously decided, in the case of Corbett v Corbett (1971), that sex affirmation including genital reassignment/rehabilitation surgery (then and sometimes still geniticentrically called "Sex Reassignment Surgery") would not be recognized for purposes of marriage. That decision had been generally followed throughout the world; including the United States of America. Justice Richard Chisholm (the judge in Re Kevin) found fault with both the legal bases and internal logic of this decision and held it did not bind or represent Australian law.
Significantly, Justice Chisholm found that the extensive international and Australian expert evidence in Re Kevin did not support the primary "factual" proposition in the Corbett decision that a causal (and hence legal) distinction could and should be made between the natural variations in human sexual formation sometimes called "intersex" (in Corbett and other similar decisions said to have a biological causation) and transsexualism (in Corbett and other similar decisions said to be a psychological disorder). Chisholm J found that on the balance of expert evidence, both as presented in Re Kevin and generally in cases throughout the world dealing with the issue, no such factual distinction was possible and that transsexualism was an example of natural intersexual diversity in human sexual formation and not a psychological disorder or illness.
Justice Chisholm stated that to determine a person's sex for the purpose of the law of marriage in Australia all relevant matters need to be considered, including: the person's biological and physical characteristics at birth (including gonads, genitals and chromosomes); the person's life experiences, including the sex in which he or she is brought up and the person's attitude to it; the person's self-perception as a man or woman; the extent to which the person has functioned in society as a man or a woman; any hormonal, surgical or other medical sex affirmation (including genital reassignment/rehabilitation) treatments the person has undergone, and the consequences of such treatment as well as the person's biological, psychological and physical characteristics at the time of the marriage, including (if they can be identified) any biological features of the person's brain that are associated with a particular sex.
His Honour stated that it is clear from the Australian authorities that "post-operative transsexuals" will normally be members of their affirmed sex. Holding that the sex of a person for the purposes of marriage is their sex at the time of the marriage, the judgement found Kevin to be a man within the ordinary everyday meaning of the word in Australian life and declared the marriage between Kevin and Jennifer to be valid. The Attorney-General appealed.
The Full Court of the Family Court, upholding the decision at first instance, determined that the reasoning of the Family Division of the UK High Court in W v W, an intersex marriage case, was a correct statement of the law in Australia and that people with transsexualism, like others with intersex conditions, should be able to choose their sex, affirm it and marry as a member of that sex.
Re Kevin has been subsequently extensively quoted and relied upon in international jurisprudence (including in the United States of America and in the European Court of Human Rights) concerning the civil and human rights of people who experience transsexualism; including young people with transsexualism who are still regularly deprived of their right to affirm their innate sex without being punished by family and culture, change their legal sex in order to make it intelligably consistent with their affirmed/lived sex as well as being able to freely access medically approved sex affirmation treatment.
Passports.
Passports are issued in the preferred gender, without requiring a change to birth certificates or citizenship certificates. A letter is needed from a medical practitioner which certifies that the person has had or is receiving appropriate treatment for transition.

</doc>
<doc id="18331" url="http://en.wikipedia.org/wiki?curid=18331" title="Ligase">
Ligase

In biochemistry, ligase (from the Latin verb "ligāre" — "to bind" or "to glue together") is an enzyme that can catalyze the joining of two large molecules by forming a new chemical bond, usually with accompanying hydrolysis of a small pendant chemical group on one of the larger molecules or the enzyme catalyzing the linking together of two compounds, e.g., enzymes that catalyze joining of C-O, C-S, C-N, etc. In general, a ligase catalyzes the following reaction:
or sometimes
where the lowercase letters signify the small, dependent groups. Ligase can join two complementary fragments of nucleic acid and repair single stranded breaks that arise in double stranded DNA during replication.
Nomenclature.
The common names of ligase enzymes often include the word "ligase", such as DNA ligase, an enzyme commonly used in molecular biology laboratories to join together DNA fragments. Other common names for ligases include synthetases, because they are used to synthesize new molecules.
Note that, originally, biochemical nomenclature distinguished synthetases and synthases. Under the original definition, synthases "do not" use energy from nucleoside triphosphates (such as ATP, GTP, CTP, TTP, and UTP), whereas synthetases "do" use nucleoside triphosphates. It is also said that a synthase is a lyase (a lyase is an enzyme that catalyzes the breaking of various chemical bonds by means other than hydrolysis and oxidation, often forming a new double bond or a new ring structure) and does not require any energy, whereas a synthetase is a ligase (a ligase is an enzyme that binds two chemicals or compounds) and thus requires energy. However, the Joint Commission on Biochemical Nomenclature (JCBN) dictates that "synthase" can be used with any enzyme that catalyses synthesis (whether or not it uses nucleoside triphosphates), whereas "synthetase" is to be used synonymously.
Classification.
Ligases are classified as EC 6 in the EC number classification of enzymes. Ligases can be further classified into six subclasses:

</doc>
<doc id="18334" url="http://en.wikipedia.org/wiki?curid=18334" title="Logo (programming language)">
Logo (programming language)

Logo is an educational programming language, designed in 1967 by Daniel G. Bobrow, Wally Feurzeig, Seymour Papert and Cynthia Solomon. Today the language is remembered mainly for its use of "turtle graphics", in which commands for movement and drawing produced line graphics either on screen or with a small robot called a "turtle". The language was originally conceived to teach concepts of programming related to LISP and only later to enable what Papert called "body-syntonic reasoning" where students could understand (and predict and reason about) the turtle's motion by imagining what they would do if they were the turtle. There are substantial differences between the many dialects of Logo, and the situation is confused by the regular appearance of turtle graphics programs that mistakenly call themselves Logo.
Logo is a A multi-paradigm adaptation and dialect of Lisp, a functional programming language. There is no standard Logo, but UCBLogo has best facilities for handling lists, files, I/O, and recursion in scripts, and can be used to teach all computer science concepts, as UC Berkeley lecturer Brian Harvey did in his "Computer Science Logo Style" trilogy. For tertiary level teaching, however, Logo has been superseded by Scheme, and scripting languages.
History.
Logo was created in 1967 at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm, by Wally Feurzeig and Seymour Papert. Its intellectual roots are in artificial intelligence, mathematical logic and developmental psychology. The first four years of Logo research, development and teaching work was done at BBN. The first implementation of Logo, called Ghost, was written in LISP on a PDP-1. The goal was to create a math land where kids could play with words and sentences. Modeled on LISP, the design goals of Logo included accessible power and informative error messages. The use of virtual Turtles allowed for immediate visual feedback and debugging of graphic programming.
The first working Logo turtle robot was created in 1969. A display turtle preceded the physical floor turtle. Modern Logo has not changed too much from the basic concepts before the first turtle. The first turtle was a tethered floor roamer, not radio-controlled or wireless. At BBN Paul Wexelblat developed a turtle named Irving that had touch sensors and could move forwards, backwards, rotate, and ding its bell. The earliest year-long school users of Logo were in 1968-69 at Muzzey Jr High, Lexington MA. The virtual and physical turtles were first used by fifth graders at the Bridge School in Lexington, MA in 1970-71.
Name.
"Logo" is not an acronym. It was derived from the Greek "logos" meaning "word" or "thought" by Feurzeig, to distinguish itself from other programming languages that were primarily numbers, not graphics or logic, oriented.
Design.
Logo is generally known as an interpreted language, although recently there have been developed compiled Logo dialects—such as Lhogho or Liogo. Logo is not case-sensitive but retains the case used for formatting. It is a compromise between a sequential programming language with block structures, and a functional programming language.
Turtle and graphics.
Logo's most-known feature is the turtle (derived originally from a robot of the same name), an on-screen "cursor" that showed output from commands for movement and small retractable pen, together producing line graphics. It has traditionally been displayed either as a triangle or a turtle icon (though it can be represented by any icon). Turtle graphics were added to the Logo language by Seymour Papert in the late 1960s to support Papert's version of the turtle robot, a simple robot controlled from the user's workstation that is designed to carry out the drawing functions assigned to it using a small retractable pen set into or attached to the robot's body.
As a practical matter, the use of turtle geometry instead of a more traditional model mimics the actual movement logic of the turtle robot. The turtle moves with commands that are relative to its own position, "LEFT 90" means spin left by 90 degrees. Some Logo implementations, particularly those that allow the use of concurrency and multiple turtles, support collision detection and allow the user to redefine the appearance of the turtle cursor, essentially allowing the Logo turtles to function as sprites.
Multiple turtles are supported by MSWLogo, as well as 3D graphics. Input from COM ports and LPT ports are also allowed by MSWLogo through windows GUI. Interrupts can be triggered via keyboard and mouse events. Simple GIF animations may also be produced on MSWLogo version 6.5 with the "gifsave" command.
Turtle geometry is also sometimes used in environments other than Logo as an alternative to a strictly coordinate-addressed graphics system. For instance, the idea of turtle graphics is also useful in Lindenmayer system for generating fractals.
Implementations.
Some modern derivatives of Logo allow thousands of independently moving turtles. There are two popular implementations: MIT's StarLogo and Northwestern University CCL's NetLogo. They allow for the exploration of emergent phenomena and come with many experiments in social studies, biology, physics, and other areas. NetLogo is widely used in agent-based simulation in the biological and social sciences.
Although there is no single agreed-upon standard, there is a broad consensus on core aspects of the language. As of March 2009 there were 197 implementations and dialects of Logo, each with its own strengths. Most of those 197 are no longer in wide use, but many are still under active development. Commercial Logos that are still widely used in schools include "MicroWorlds Logo" and "Imagine Logo".
Apple Logo, developed by LCSI, was the most broadly used and prevalent early implementation of Logo that peaked in the early to mid-1980s, largely due to marketing by Apple's Apple II computer. TI Logo (for the TI 99/4A computer) was also used in primary schools, emphasizing Logo's usefulness in teaching computing fundamentals to novice programmers. IBM marketed their own version of Logo (P/N 6024076), also developed by LCSI (Logo Computer Systems, Inc), for their then-new IBM PC.
"UCBLogo", also known as Berkeley Logo, is free and cross-platform standard Logo last released in 2009. Subsequently, George Mills at MIT used UCBLogo as the basis for "MSWLogo" which is more refined and also free. After that, Jim Muller wrote "The Great Logo Adventure" which was a complete Logo manual and which used MSWLogo as the demonstration language. The book is now out of print, but Jim has released all the files in pdf format. (Check the Logo Foundation website for details.) MSWLogo has evolved into .
Most Logos are 2D, but the is notable for supporting 3D graphics. Most Logo implementations are interpreted, but some compilers have been built, including the Lhogho compiler, by the same author as Elica. Although most often used for graphics, Logo can also control robots. It was interfaced with Lego bricks, although Lego decided later to use another language in the commercial Lego Mindstorms products. An interface also exists for Cricket robots.
 () is an online learning community and another 3D Logo implementation. () is an online application, which runs in (see WebGL). It is implemented using HTML5, JavaScript, X3D, and X3DOM technologies. Its Logo interpreter is based on with a set of 3D extension. The VRMath2 online learning community empowers members to design, create, and share their 3D virtual worlds. 
ObjectLOGO is a variant with object-oriented extensions.
 a rewrite and enhancement of UCBLogo.
Logo3D is a tridimensional version of Logo and can be found at on SourceForge.net.
, an open source Logo implementation in Java, was designed and implemented by Daniel Azuma, based on BSD Logo (with various extensions). It was later ported to .NET / J# by George Birbilis.
E-Slate Logo is an enhancement of TurtleTracks Logo with object-oriented programming (OOP) primitives (TELL, ASK, EACH and TELLALL). It was designed and implemented by George Birbilis.
In 1990 a version appeared for the Acorn Electron, "Electron User" in February 1990 published Timothy Grantham's simple implementation of this programming language under the article "Talking Graphics", a first glimpse most electron users ever saw of this simple but intuitive programming language.
Work continues to be done in writing Logo implementaions. In 2012, the LibreOffice team developed LibreLogo as an extension to some LibreOffice versions. Libre-Logo is written in Python and allows vector graphics to be written in Writer.
In 2014, a new web based logo interpreter was introduced: , designed and implemented using HTML5, CSS3 and JQuery. This modern web platform offers the possibility to run the logo code with turtle animation. Its Logo interpreter is based on with an animation extension script.
Influence.
Logo was a primary influence on the Smalltalk programming language. It is also the main influence on the Etoys educational programming environment and language, which is essentially a Logo written in Squeak (a variant of Smalltalk).
Logo provided the underlying language for Boxer. Boxer was developed at Berkeley and MIT and is based on a 'literacy model', making it easier to use for everyday people.
KTurtle is a variation of Logo implemented at Qt for the KDE environment loosely based on Logo.
The latest results of Logo's influence is the Kojo, a variant of Scala and Scratch educational programming language, which runs on Squeak, a variant of Smalltalk, which was inspired by Logo.
Further reading.
</dl>

</doc>
<doc id="18337" url="http://en.wikipedia.org/wiki?curid=18337" title="Last rites">
Last rites

The last rites are the last prayers and ministrations given to many Catholics when possible shortly before death. The last rites go by various names and include different practices in different Catholic traditions. They may be administered to those awaiting execution, mortally injured, or terminally ill. 
In the Latin Catholic church.
The ministration known as the last rites in the Catholic Church does not constitute a distinct sacrament in itself. It is rather a set of sacraments given to people who are believed to be near death. These are the sacraments of Anointing of the Sick (which, in spite of not being reserved for those near death, is sometimes mistakenly supposed to be what is meant by "the last rites"), Penance and the Eucharist. If all three are administered immediately one after another, the normal order of administration is: first Penance, then Anointing, then Viaticum.
The last rites are meant to prepare the dying person's soul for death, by providing absolution for sins by penance, sacramental grace and prayers for the relief of suffering through anointing, and the final administration of the Eucharist, known as "Viaticum", which is Latin for "provision for the journey."
Reception of the Eucharist in this form is the only sacrament essentially associated with dying. Accordingly, "the celebration of the Eucharist as Viaticum is the sacrament proper to the dying Christian". In the Roman Ritual's "Pastoral Care of the Sick: Rites of Anointing and Viaticum", Viaticum is the only sacrament dealt with in "Part II: Pastoral Care of the Dying".
Within that part, the chapter on Viaticum is followed by two more chapters, one on "Commendation of the Dying", with short texts, mainly from the Bible, a special form of the litany of the saints, and other prayers, and the other on "Prayers for the Dead". A final chapter provides "Rites for Exceptional Circumstances", namely, the "Continuous Rite of Penance, Anointing, and Viaticum", "Rite for Emergencies", and "Christian Initiation for the Dying". The last of these concerns the administration of Baptism and Confirmation to those who have not received these sacraments.
In addition, the priest has authority to bestow a blessing in the name of the Pope on the dying person, to which a plenary indulgence is attached.
In case of an individual awaiting execution, the person would receive Confession and Viaticum. Without having to fear death by illness, the condemned cannot partake Anointing of the sick.
Eastern Orthodox Church and Byzantine-Rite Catholic Churches.
In the Orthodox Church and those Eastern Catholic Churches which follow the Rite of Constantinople, the last rites consist of the Sacred Mysteries (sacraments) of Confession and the reception of Holy Communion.
Following these sacraments, when a person dies, there are a series of prayers known as "The Office at the Parting of the Soul From the Body". This consists of a blessing by the priest, the usual beginning, and after the Lord's Prayer, Psalm 50. Then a Canon to the Theotokos is chanted, entitled, "On behalf of a man whose soul is departing, and who cannot speak". This is an elongated poem speaking in the person of the one who is dying, asking for forgiveness of sin, the mercy of God, and the intercession of the saints. The rite is concluded by three prayers said by the priest, the last one being said "at the departure of the soul."
There is an alternative rite known as "The Office at the Parting of the Soul from the Body When a Man has Suffered for a Long Time". The outline of this rite is the same as above, except that Psalm 70 and Psalm 143 precede Psalm 50, and the words of the canon and the prayers are different.
The rubric in the Book of Needs (priest's service book) states, "With respect to the Services said at the parting of the soul, we note that if time does not permit to read the whole Canon, then customarily just one of the prayers, found at the end of the Canon, is read by the Priest at the moment of the parting of the soul from the body."
As soon as the person has died the priest begins "The Office After the Departure of the Soul From the Body" (also known as "The First Pannikhida").
In the Orthodox Church Holy Unction is not considered to be solely a part of a person's preparation for death, but is administered to any Orthodox Christian who is ill, physically or spiritually, to ask for God's mercy and forgiveness of sin. There is an abbreviated form of Holy Unction to be performed for a person in imminent danger of death, which does not replace the full rite in other cases.

</doc>
<doc id="18338" url="http://en.wikipedia.org/wiki?curid=18338" title="Lamorna">
Lamorna

Lamorna (Cornish: Nansmornow) is a fishing village and cove in west Cornwall, England, United Kingdom. It is situated on the Penwith peninsula approximately four miles (6 km) south of Penzance. The village and valley lies within the Cornwall Area of Outstanding Natural Beauty (AONB); almost a third of Cornwall has AONB designation, with the same status and protection as a National Park.
Newlyn School of Art and the Lamorna Colony.
In the late nineteenth and early twentieth centuries Lamorna became popular with artists of the Newlyn School. It is particularly associated with the artist S. J. "Lamorna" Birch who lived there from 1908. The colony included Birch, Alfred Munnings, Laura Knight and Harold Knight. This period is dramatised in the 1998 novel "Summer in February" by Jonathan Smith and adapted for the 2013 movie directed by Christopher Menaul. Lamorna was also the home of the jeweller Ella Naper and her husband, the painter Charles, who built Trewoofe house there. The Lamorna Arts Festival was launched in 2009 to celebrate the original Lamorna Colony and today's Lamorna art community.
Lamorna in culture.
Lamorna has been immortalised in the song "Way Down to Lamorna", about a wayward husband receiving his comeuppance from his wife. The song is beloved of many Cornish singers.
The author Derek Tangye lived near Lamorna where he wrote his famous books "The Minack Chronicles", and mentions it often in the books. There is still a piece of land called "Oliver Land" which is only accessible from Lamorna that he left after his death as a wildlife sanctuary.
"Lamorna Cove" was the title of a poem by W. H. Davies published in 1929.
The name of Lamorna's pub, The Wink, alludes to smuggling, "the wink" being a signal that contraband could be obtained. The pub is the subject of a novel by Martha Grimes, entitled "The Lamorna Wink". The interior contains an important collection of maritime artefacts, including the nameplate of the battleship Warspite.
The Lamorna Pottery was founded in 1947 by Christopher James Ludlow (known as Jimmy) and Derek Wilshaw.
Lamorna was the village used in the novel "The Memory Garden" by Rachel Hore.
Lamorna was the location used for the shooting of Sam Peckinpah's 1971 thriller "Straw Dogs".
Lamorna stone.
Granite taken from Lamorna cove has been widely used in construction, most notably in the Thames Embankment. Stone from the cove was also used to construct the nearby church of St Buryan, whose 92 foot granite tower is an imposing local landmark often used as a line of sight by fishermen coming into port.

</doc>
<doc id="18339" url="http://en.wikipedia.org/wiki?curid=18339" title="Law of multiple proportions">
Law of multiple proportions

In chemistry, the law of multiple proportions is one of the basic laws of stoichiometry used to establish the atomic theory, alongside the law of conservation of mass (matter) and the law of definite proportions. It is sometimes called Dalton's Law after its discoverer, the British chemist John Dalton, who published it in the first part of the first volume of his "New System of Chemical Philosophy" (1808). The statement of the law is:
If two elements form more than one compound between them, then the ratios of the masses of the second element which combine with a fixed mass of the first element will be ratios of small whole numbers.
For example, Dalton knew that the element carbon forms two oxides by combining with oxygen in different proportions. A fixed mass of carbon, say 100 grams, may react with 133 grams of oxygen to produce one oxide, or with 266 grams of oxygen to produce the other. The ratio of the masses of oxygen that can react with 100 grams of carbon is 266:133 ≈ 2:1, a ratio of small whole numbers. Dalton interpreted this result in his atomic theory by proposing (correctly in this case) that the two oxides have one and two oxygen atoms respectively for each carbon atom. In modern notation the first is CO (carbon monoxide) and the second is CO2 (carbon dioxide).
John Dalton first expressed this observation in 1804. A few years previously, the French chemist Joseph Proust had proposed the "law of definite proportions", which expressed that the elements combined to form compounds in certain well-defined proportions, rather than mixing in just any proportion; and Antoine Lavoisier proved the law of conservation of mass, which helped out Dalton. Careful study of the actual numerical values of these proportions led Dalton to propose his law of multiple proportions. This was an important step toward the atomic theory that he would propose later that year, and it laid the basis for chemical formulas for compounds.
Another example of the law can be seen by comparing ethane (C2H6) with propane (C3H8). The weight of hydrogen which combines with 1 g carbon is 0.252 g in ethane and 0.224 g in propane. The ratio of those weights is 1.125, which can be expressed as the ratio of two small numbers 9:8. 
Limitations.
The law of multiple proportions is best demonstrated using simple compounds. For example, if one tried to demonstrate it using the hydrocarbons decane (chemical formula C10H22) and undecane (C11H24), one would find that 100 grams of carbon could react with 18.46 grams of hydrogen to produce decane or with 18.31 grams of hydrogen to produce undecane, for a ratio of hydrogen masses of 121:120, which is hardly a ratio of "small" whole numbers.
The law fails with non-stoichiometric compounds and also doesn't work well with polymers and oligomers.

</doc>
<doc id="18340" url="http://en.wikipedia.org/wiki?curid=18340" title="Law of averages">
Law of averages

The law of averages is a layman's term for a belief that the statistical distribution of outcomes among members of a small sample must reflect the distribution of outcomes across the population as a whole.
As invoked in everyday life, the "law" usually reflects wishful thinking or a poor understanding of statistics rather than any mathematical principle. While there is a real theorem that a random variable will reflect its underlying probability over a very large sample, the law of averages typically assumes that unnatural short-term "balance" must occur. Typical applications of the law also generally assume no bias in the underlying probability distribution, which is frequently at odds with the empirical evidence.
Examples.
Gambler's fallacy.
The gambler's fallacy is a particular application of the law of averages in which the gambler believes that a particular outcome is more likely because it has not happened recently, or (conversely) that because a particular outcome has recently occurred, it will be less likely in the immediate future.
As an example, consider a roulette wheel that has landed on red in three consecutive spins. An onlooker might apply the law of averages to conclude that on its next spin it must (or at least is much more likely to) land on black. Of course, the wheel has no memory and its probabilities do not change according to past results. So even if the wheel has landed on red in ten or a hundred consecutive spins, the probability that the next spin will be black is still no more than 48.6% (assuming a "fair" European wheel with only one green zero; it would be exactly 50% if there were no green zero and the wheel were fair, and 47.4% for a fair American wheel with one green "0" and one green "00"). Similarly, there is no statistical basis for the belief that lottery numbers which haven't appeared recently are due to appear soon.
Expectation values.
Another application of the law of averages is a belief that a sample's behaviour must line up with the expected value based on population statistics. For example, suppose a fair coin is flipped 100 times. Using the law of averages, one might predict that there will be 50 heads and 50 tails. While this is admittedly the single most likely outcome, there is only an 8% chance of it occurring. Predictions based on the law of averages are even less useful if the sample does not reflect the population.
Repetition of trials.
In this example, one tries to increase the probability of a rare event occurring at least once by carrying out more trials. For example, a job seeker might argue, "If I send my résumé to enough places, the law of averages says that someone will eventually hire me." Assuming a non-zero probability, it is true that conducting more trials increases the overall likelihood of the desired outcome. However, there is no particular number of trials that guarantees that outcome; rather, the probability that it will already have occurred approaches but never quite reaches unity.

</doc>
<doc id="18341" url="http://en.wikipedia.org/wiki?curid=18341" title="Outline of linguistics">
Outline of linguistics

The following outline is provided as an overview of and topical guide to linguistics:
Linguistics is the scientific study of natural language. Someone who engages in this study is called a linguist. Linguistics can be theoretical or applied.
Nature of linguistics.
Linguistics can be described as all of the following:
Branches of linguistics.
Subfields of linguistics.
Subfields, by linguistic structures studied.
Sub-fields of structure-focused linguistics include:
History of linguistics.
Timeline of discovery of basic linguistics concepts.
"When were the basic concepts first described and by whom?"
Basic concepts.
"What basic concepts / terms do I have to know to talk about linguistics?"
Linguistics scholars.
"People who had a significant influence on the development of the field"

</doc>
<doc id="18342" url="http://en.wikipedia.org/wiki?curid=18342" title="Outline of law">
Outline of law

The following outline is provided as an overview of and topical guide to law:
Law – is the set of rules and principles (laws) by which a society is governed, through enforcement by governmental authorities. Law is also the field which concerns the creation and administration of laws, and includes any and all legal systems.
Nature of law.
Law can be described as all of the following:
Lists.
Sources of law
Legislatures
Courts
Prisons

</doc>
<doc id="18345" url="http://en.wikipedia.org/wiki?curid=18345" title="Outline of literature">
Outline of literature

The following outline is provided as an overview of and topical guide to literature:
Literature – prose, written or oral, including fiction and non-fiction, drama and poetry. "See also: outline of poetry."
What "type" of thing is literature?
Literature can be described as all of the following:
Forms of literature.
Written literary genres.
Non-fiction.
Non-fiction
Fiction genres.
Fiction
History of literature.
History of literature

</doc>
<doc id="18352" url="http://en.wikipedia.org/wiki?curid=18352" title="AvtoVAZ">
AvtoVAZ

AvtoVAZ (Russian: АвтоВАЗ) is the Russian automobile manufacturer formerly known as VAZ: Volzhsky Avtomobilny Zavod (ВАЗ, Во́лжский автомоби́льный заво́д), but better known to the world under the trade name Lada. The company was established in the late 1960s in collaboration with Fiat. The current company name is "AvtoVAZ", which stands for "Avtomobili Volzhskogo Avtomobilnogo Zavoda" ("Cars of Volga Automobile Plant"). AvtoVAZ is the largest company in the Eastern European and Russian automotive industry.
It produces nearly one million cars a year, including the Kalina family (hatchback, wagon and crossover), Lada Granta family (sedan and liftback), Lada Priora family (sedan, hatchback, wagon and coupe), Lada 4x4 (former Lada "Niva") and Lada Largus (Renault-Nissan platform). It also produces the vehicles of Renault-Nissan alliance brands: Renault Logan 2, Datsun on-Do (the Nissan sub-brand) and Nissan Almera New. However, the original Fiat 124-based VAZ-2101, and its derivatives, remain the models most associated with its Lada brand.
The VAZ factory is one of the biggest in the world, with over 90 mi of production lines, and is unique in that most of the components for the cars are made in-house.
The original Lada was intended as a "people's car" for consumers of the Eastern Bloc - lacking in most luxuries expected in Western-made cars of its era. Ladas were sold as a budget 'no-frills' vehicle in several Western nations during the 1970s and 1980s, including Canada, the United Kingdom, France, Belgium, Luxembourg and the Netherlands, though trade sanctions banned their export to the United States. Sales to Italy were forbidden by the agreement between the Soviet government and Fiat, to protect Fiat from cheap imports in its home market.
Origins.
With the aim of producing more cars, a brand-new integrated plant was decided upon in 1966, with Viktor Polyakov (later minister of "Minavtoprom") as director, and Vladimir Solovyev was chief designer. It was set up as a collaboration between Italy and the Soviet Union and built on the banks of the Volga River in 1966. A new town, Tolyatti, named after the Italian Communist Party leader Palmiro Togliatti, was built around the factory. The Lada was envisaged as a "people's car" like the Citroën 2CV or the VW Type 1. Production was intended to be 220,000 cars a year, beginning in 1971; car production actually began before the plant was finished in 1970. The VAZ trademark, at first, was a grey Volga boat on a red pentagonal background, with "Togliatti" superposed in Cyrillic (Тольятти); the first badges, manufactured in Turin, mistakenly had the Cyrillic "Я" rendered "R", instead (Тольтти), making them collector's items.
The lightweight Italian Fiat 124 was adapted in order to survive treacherous Russian driving conditions. Among many changes, aluminium brake drums were added to the rear, and the original Fiat engine was dropped in favour of a newer design also purchased from Fiat. This new engine had a modern overhead camshaft design, but was never used in Fiat cars. The suspension was raised to clear rough Russian roads and the bodyshell was made from thicker, heavier steel. The first Lada models were equipped with a starting handle in case the battery went flat in Siberian conditions, though this was later dropped. Another feature specifically intended to help out in cold conditions was a manual auxiliary fuel pump. About 22,000 VAZ-2101s were built in 1970, with capacity at the end of 1973 reaching 660,000 a year; 21 December, the one millionth 2101 was built. A third production line was added in October 1974, boosting output to 2,230 cars a day. The same year, total VAZ production reached 1.5 million.
Exports to the West began in 1974; under the original agreement with Fiat, the car could not be sold in competition with the 124 until its replacement (the Fiat 131 Mirafiori) had been released and all Fiat production of the 124 had ceased.
Engines fitted to the original Ladas start with the 1.2 L carubretted in the original and go up to the 1.7 L export model set up with a General Motors single point fuel injection system. Diesel engines were later fitted for the domestic market only. The drivetrain is a simple rear-wheel drive setup with a live rear axle. The engine is an inline four with two valves per cylinder and a single overhead camshaft.
The Fiat-based Ladas feature various headlight, trim and body styles. The original, Fiat style models included VAZ-2101 sedan and VAZ-2102 station wagon. 1972 saw the introduction of a deluxe version of the sedan, VAZ-2103, which was based on Fiat 124 Special 1968 and featured a new 1.5 L engine and twin headlights. In 1974, the original VAZ-2101 was updated with new engines and interiors; VAZ-2102 underwent the same improvements in 1976. The body style with two round headlights was manufactured until 1988; all others remain in production in slightly updated form.
The VAZ-2106 introduced in December 1975 was an updated version of VAZ-2103, really which was based on 1972 Fiat 124 Special T, featuring different interiors and new 1.6 L engine. The 2106 is the oldest and the most popular rear-wheel drive AvtoVAZ model; its production ended in 2001 from Tolyatti, but continued at Izhavto (Izhevsk), ending there in December 2005.
In 1974, VAZ was given permission to begin producing Wankel engines under licence from NSU. Work began in 1976, with a single-rotor Lada appearing in 1978; the first 250 of these went on sale in summer 1980.
The VAZ-2105, still based on the 2101 but updated to 1980s styling, was introduced in 1980 and was marketed outside the Soviet Union under the Riva or Laika names, depending on country. Square headlights and new body panels distinguish this style from the old models. A deluxe version, VAZ-2107, was out in 1982; it featured a better engine, refined interiors and a Mercedes-like radiator grille. In 1984, the VAZ-2104 station wagon completed the line-up. In 2002 station wagon 2104 production was transferred to IzhAvto. Production of the 2105 was completed on 30 December 2010, and production of deluxe sedan 2107 was transferred to IzhAvto on March 2011.
In the domestic market, these "classic" models were called "Zhiguli" (Жигули). The Lada name was used for exports only, but a large share of Ladas was reexported from Eastern Bloc countries, so the brand was well known in the domestic market as well.
AvtoVAZ designers proved they had some original ideas when the VAZ-2121 Niva was introduced in 1978. This highly popular car was made with off-road use in mind, featuring a gearbox with a four-wheel-drive selector lever as well as a low- and high- range selector lever. It has an original body style and the most powerful 1.7 L engine in the VAZ range. The Niva has also been available with 1.9 L Peugeot sourced diesel engine. The Niva is still in production.
Based on the success of the Niva, the design department prepared a new family of front-wheel drive models by 1984, which was of a completely domestic design. Production started with the VAZ-21083 "Sputnik" three-door hatchback; the series was later renamed Samara. The Samara engine was mostly designed and produced in-house, had a new single overhead camshaft (SOHC) design and was driven by a more modern rubber belt. The combustion chambers were developed in collaboration with Porsche. The line-up features a completely new body and interiors, front MacPherson strut independent suspension and rear torsion bar, rack and pinion steering, and an updated five-speed gearbox. The five-door VAZ-21093 hatchback followed in 1987, and the four-door 1.5 L sedan, VAZ-21099, was introduced in 1990. The same year, the front sides and radiator grille were restyled on the whole Samara range.
A white 2108 would be VAZ's nine millionth Lada built, on 24 May 1985, with the ten millionth, on 9 October 1986, also a 2108. The twelve millionth, a right-hand drive 2109, was produced 6 July 1989.
The 2108-2109 models were in production until 2001, when they were restyled with new side panels, interiors and 1.5 L fuel injection engines (though fuel injection was available as early as 1995). The Lada 2109 hatchback was rebadged as Lada 2114, and Lada 21099 sedan was rebadged as the Lada 2115. The 2104-21099 model range was transferred to IzhMash and ZAZ and is still being manufactured. In 2004 VAZ also introduced Lada 2113, a restyled version of Lada 2108, but this car has never used much popularity, as the Lada 2108 was only popular for a short time.
The VAZ-1111 Oka micro-car, which resembles the Fiat Panda (though relation to it), was introduced in 1988, and in 1991 the production was transferred to the KamAZ and SeAZ factories.
The VAZ-2120 Nadezhda minivan is based on the original Niva and has been in low-volume production since 1998. A five-door version of the Niva, the VAZ-2131, has been in production since 1995.
The break-up of the USSR delayed the production of new 110-series by a couple of years. The VAZ-2110 sedan was introduced in 1996, the 2111 station wagon followed in 1998 and the 2112 hatchback completed the range in 2001. These models are basically based on Samara mechanicals, with a new body and fuel injected engines as standard features, though carbureted versions have also been available up until 2001. The 110-series remains in production and has been continually updated over the years. For example, engines used to be 1.5 L units with either 8 or 16 valves, but these have now been upgraded to 1.6 L units that meet stricter emissions rules.
VAZ in 2008 was the largest automotive plant in Europe, able to build 750,000 cars a year. The plant covers 600 ha, with three assembly lines each 1,700 m long; at peak production, it employed 180,000. And, unlike most Western factories, it is vertically integrated, producing almost every component in the plant itself.
In 2013, AVTOVAZ sold 481 thousand cars LADA, sales declined by 19% compared to the year 2012.
Revenues declined by 13 billion roubles up to 177 billion roubles. The loss amounted to 7.9 billion rubles. The deterioration of the results of financial and economic activity of the AVTOVAZ group mainly connected with reduction of sales at the Russian car market.
In the first half of 2014, the proceeds from sales of AVTOVAZ amounted to 91.1 billion rubles, net loss - 2,75 billion. For the six months sales of the company amounted to 220 251 LADA, which is 5% less than in the first half of last year The main factors that influenced these results, the steel market collapse and the fall in the ruble.
Market share.
Tightening emissions and safety legislation meant that AvtoVAZ withdrew from most Western markets by the late 1997; often, there were also problems with spare parts. Since then AvtoVAZ has been concentrated on home market. Today Lada has 15,7% of the Russian market, in the segment up to 600 thousand rubles, they occupy 37%. In the USA they were never sold due to the cold war, but they were available in Canada (where the Niva was quite popular). The rise in popularity of Far Eastern imports from newly established manufacturers such as Daewoo, Proton, Kia and Hyundai contributed to Lada's demise in the West. These Korean and Malaysian-manufactured vehicles offered modern, Japanese developed technology and standard equipment such as automatic transmissions which Lada could not compete with, and by the turn of the millennium, had completely taken over the market niche that Lada had survived in for over 20 years.
Though the original Lada, and as of the early part of the new millennium, the Samara, have now been withdrawn, the Lada 110 and the Niva are still sold in certain Western European markets, as are the more modern models (Lada Kalina, Lada Granta, Lada 4x4). The Lada is widely available in many Central and South American countries as well as in Africa, the Middle East and in all of the former Soviet Union and Communist Bloc nations.
Recent developments.
As AvtoVAZ was allowed to sell cars to private dealers in the late 1980s, Boris Berezovsky arranged to resell the cars to the public through his "LogoVAZ" dealerships. In 1993 he started a campaign to collect funds for the "people's automobile" and created the "AVVA" venture, which stands for "All-Russian Automobile Alliance"; the AvtoVAZ held a major share in the venture. The plans were to build a completely new plant for production of the VAZ-1116 supermini. However, the financial crisis of 1998 put these plans to an end. The development concepts of 1116 instead became the foundation of the Lada Kalina range.
GM-AvtoVAZ, a joint-venture with General Motors, adopted an updated version of the Niva, VAZ-2123, that was considered for production since the 1990s. Named Chevrolet Niva, it is being built on the venture's plant since 2001 and is exported to Europe and Latin America. In 2004, the Chevrolet Viva, a four-door version of the Opel Astra G, was introduced.
VAZ has also tried to get into the sportier markets: several Ladas were factory-tuned and given a Momo steering wheel. A convertible was also produced. In 2003, VAZ presented the concept car Lada Revolution, an open single seater sports car powered by a 1.6 L engine producing 215 hp. There are other experimental cars, such as the VAZ-210834 Tarzan SUV concept, VAZ-1922 monster truck and VAZ-2359 pick-up, all based on Niva. The VAZ-211223 110-series coupe, with the sister models 111 and 112 have been developed with a modern and luxurious look and feel, have been mass-produced, and are popular in Russia today.
Some models (mostly the police version) have a Wankel engine (like the Mazda RX-7), though development (and production) of this engine stopped in 2004. The main causes are special requirements for service and repair (mostly available only in Moscow & Togliatti) and very high fuel & lubricating oil consumption.
2005 saw the introduction of the new Kalina supermini lineup to the market. AvtoVAZ has built a new modern plant for this model and is hoping to sell some 200,000 cars annually. Test production of the Lada 1118 sedan started in November 2004 and full-scale assembly was launched in May 2005. The Lada 1119 hatchback and Lada 1117 station wagon with updated DOHC 1.6L engines followed in 2006.
The restyled 110-series model, Lada 2170 Priora, is produced since March 2007.
"Project C", which has come to be known as the Lada 2116 or Lada Silhouette, is a family car jointly developed with input from both Porsche and Renault, is intended to finally replace the Classic models. Spy shots of the car appeared in 2007, suggesting a 2008 launch. AvtoVAZ began to move production of the Classic models (which were still selling strongly in Russia) out of Togliatti at the end of 2010 - fuelling further speculation that this was to free up production capacity for the 2116. In the end, the 2116 never reached production.
AvtoVAZ was considering the local production of Ecotec Family 1 (FAM-1) engines using the equipment transferred from Szentgotthard, Hungary plant. A transmissions plant was to be bought from Daewoo Moto India, a former Daewoo Motors subsidiary that was not sold to GM. The engines and transmissions were to be used in both GM-AvtoVAZ and Lada cars. As of Summer 2005, these plans were cancelled and VAZ is seeking another way to acquire some modern powerplant technology.
After some shakeups in the management caused by a recent acquisition from Rosoboronexport, AvtoVAZ is currently in talks with Renault to negotiate a CKD assembly of the Renault Logan. They have also contracted Magna International to design a new car platform and equip a new plant for its production.
AvtoVAZ suffered considerably in the 2008-2009 world economic crisis. In October 2008, the company was reported to possess over 100,000 unsold units, and desperately needed money to repay short-term debts. On March 31, the value of AvtoVAZ shares jumped by almost 30%, due to Prime Minister Vladimir Putin's proclaimed determination to support the auto giant. Putin visited Togliatti, expressed his approval of the management for not initiating massive layoffs, and promised more than $1 billion in loans, cash, and guarantees. In May, 2009, Putin bought an AvtoVAZ Niva SUV to show his support for the hard-pressed domestic producer.
On March 10, 2010 the Board of Directors of "AvtoVAZ" approved a business plan for the period until 2020, by which expected to increase vehicle production to 1.2 million units per year by the end of the 2010s, as well as investments up to 3 billion euros.
On 3 May 2012, the Renault-Nissan alliance has signed letter of intent to raise its stake in Avtovaz to a majority by taking a majority share of 67.13% of a joint venture with the Russian state-controlled company, Russian Technologies, to own 74.5% of Avtovaz. This would raise the share of the Renault-Nissan Alliance in AvtoVAZ to 51.01%. Renault and Nissan will invest $750,000,000 in the joint venture.
In 2012, it was announced that Avtovaz and Sollers plan to jointly produce vehicles in Kazakhstan. The plant, which will be open in 2016, will be built in Ust-Kamenogorsk, in the eastern part of the country, and will produce around 120,000 cars a year.
In 2013, Bo Andersson joined JSC AvtoVAZ as the President.
Models.
Each model has an internal index that reflects the level of modifications, based on the engine and other options installed. For example, the VAZ-21103 variant has the 1.5 L 16V engine, while the VAZ-21104 uses the latest 1.6 L 16V fuel injection engine. Since 2001, trim levels are also indicated by including a number after the main index: '-00' means base trim level, '-01' means standard trim and '-02' designates deluxe version; for example, VAZ-21121-02 means Lada 112 hatchback with an 1.6L SOHC engine and deluxe trim.
The car's name is formed from 'VAZ-"index" "model name". The "classic" Fiat 124-derived models were known on the domestic market as Zhiguli (Жигули) until the late-1990s, when the name was dropped; thus, the 2104-2107 range, as well as 110-series, actually lack a model name. The restyled Sputnik range was renamed Samara, but the Niva and the Oka retained their names. By the 2000s (decade), the "VAZ" designation was dropped from market names in favour of "Lada" and simplified export naming conventions were adopted, so VAZ-2104 effectively became Lada 2104, VAZ-2110 became Lada 110, VAZ-2114 became Lada Samara hatchback or Lada 114 and so on, though model indices continue to be used in both technical and marketing materials.
The model names varied from market to market and as such should not be used except to indicate a certain export market. Instead, it is advisable to refer solely to the model number as these are the same for all markets.
Oka.
The Oka is a Russian city car designed by AvtoVAZ and sometimes branded as a Lada. This model was built in Russia by SeverstalAvto and SeAZ (the Serpuhov Car Factory), as well as in Azerbaijan by the Gyandzha Auto Plant. Series production of the OKA was stopped in Russia in 2008 when SeAZ released the last batch of OKA's with Chinese EURO-2 engines.
Chevrolet Niva.
The Chevrolet Niva is a GM modification produced at GM-AvtoVAZ, a joint venture between AvtoVAZ and General Motors, at its factory in Tolyatti from 2002. Although the body and the interiors are new, it is still based on the original VAZ 2121 engine, transmission and most mechanicals of the Lada Niva.
Nissan Almera.
In December 2012, the second generation of the Nissan Bluebird Sylphy began full-scale manufacturing at the AvtoVAZ plant as the new Nissan Almera.
It received its world premiere at the 2012 Moscow International Motor Show on 29 August 2012, and uses the same design as the Bluebird Sylphy, but a redesigned dashboard interior, adapted from the first generation Dacia Logan. It has a 1.6-litre petrol engine (75 kW), with a five-speed manual or a four-speed automatic transmission.
Models gallery.
Older Fiat-based models.
All are based on the Fiat 124

</doc>
<doc id="18353" url="http://en.wikipedia.org/wiki?curid=18353" title="Lundy">
Lundy

Lundy is the largest island in the Bristol Channel. It lies 19 km off the coast of Devon, England, about a third of the distance across the channel from Devon, England to south Wales. Lundy gives its name to a British sea area and is one of the islands of England. Lundy has been designated by Natural England as national character area 159, one of England's natural regions.
In 2007, Lundy had a resident population of 28 people, including volunteers. These include a warden, ranger, island manager, and farmer, as well as bar and house-keeping staff. Most live in and around the village at the south of the island. Most visitors are day-trippers, although there are 23 holiday properties and a camp site for staying visitors, mostly also around the south of the island.
In a 2005 opinion poll of "Radio Times" readers, Lundy was named as Britain's tenth greatest natural wonder. The entire island has been designated as a Site of Special Scientific Interest and it was England's first statutory Marine Nature reserve, and the first Marine Conservation Zone, because of its unique flora and fauna. It is managed by the Landmark Trust on behalf of the National Trust.
History.
The name Lundy is believed to come from the old Norse word for "puffin island" (Lundey), "lundi" being the Norse word for a puffin and "ey", an island, although an alternative explanation has been suggested with Lund referring to a copse, or wooded area. 
Lundy has evidence of visitation or occupation from the Neolithic period onward, with Mesolithic flintwork, Bronze Age burial mounds, four inscribed gravestones from the early medieval period, and an early medieval monastery (possibly dedicated to St Elen or St Helen).
Beacon Hill Cemetery.
Beacon Hill cemetery was excavated by Charles Thomas in 1969. The cemetery contains four inscribed stones, dated to the 5th or 6th century AD. The site was originally enclosed by a curvilinear bank and ditch, which is still visible in the south west corner. However, the other walls were moved when the Old Light was constructed in 1819. Early Christian enclosures of this type are known as lanns in Cornish. There are surviving examples in Luxulyan, in Cornwall; Mathry, Meidrim, and Clydau in Wales; and Stowford, Jacobstowe, Lydford, and Instow, in Devon.
Thomas proposed a five-stage sequence of site usage:
(1) An area of round huts and fields. These huts may have fallen into disuse before the construction of the cemetery.
(2) The construction of the focal grave, an 11 by rectangular stone enclosure containing a single cist grave. The interior of the enclosure was filled with small granite pieces. Two more cist graves located to the west of the enclosure may also date from this time.
(3) Perhaps 100 years later, the focal grave was opened and the infill removed. The body may have been moved to a church at this time.
(4) & (5) Two further stages of cist grave construction around the focal grave.
23 cist graves were found during this excavation. Considering that the excavation only uncovered a small area of the cemetery, there may be as many as 100 graves.
Inscribed stones.
Four Celtic inscribed stones have been found in Beacon Hill cemetery:
Knights Templar.
Lundy was granted to the Knights Templar by Henry II in 1160. The Templars were a major international maritime force at this time, with interests in North Devon, and almost certainly an important port at Bideford or on the River Taw in Barnstaple. This was probably because of the increasing threat posed by the Norse sea raiders; however, it is unclear whether they ever took possession of the island. Ownership was disputed by the Marisco family who may have already been on the island during King Stephen's reign. The Mariscos were fined, and the island was cut off from necessary supplies. Evidence of the Templars' weak hold on the island came when King John, on his accession in 1199, confirmed the earlier grant.
Marisco family.
In 1235 William de Marisco was implicated in the murder of Henry Clement, a messenger of Henry III. Three years later, an attempt was made to kill Henry III by a man who later confessed to being an agent of the Marisco family. William de Marisco fled to Lundy where he lived as a virtual king. He built a stronghold in the area now known as Bulls' Paradise with 9 ft thick walls. In 1242, Henry III sent troops to the island. They scaled the island's cliff and captured William de Marisco and 16 of his "subjects". Henry III built the castle (sometimes referred to as the Marisco Castle) in an attempt to establish the rule of law on the island and its surrounding waters. At some point in the 13th century the monks of the Cistercian order at Cleeve Abbey held the rectory of the island.
Piracy.
Over the next few centuries, the island was hard to govern. Trouble followed as both English and foreign pirates and privateers – including other members of the Marisco family – took control of the island for short periods. Ships were forced to navigate close to Lundy because of the dangerous shingle banks in the fast flowing River Severn and Bristol Channel, with its tidal range of 27 ft, one of the greatest in the world. This made the island a profitable location from which to prey on passing Bristol-bound merchant ships bringing back valuable goods from overseas.
In 1627 Barbary Pirates from the Republic of Salé occupied Lundy for five years. The North African invaders, under the command of Dutch renegade Jan Janszoon, flew an Ottoman flag over the island. Some captured Europeans were held on Lundy before being sent to Algiers as slaves. From 1628 to 1634 the island was plagued by pirate ships of French, Basque, English, and Spanish origin. These incursions were eventually ended by Sir John Penington, but in the 1660s and as late as the 1700s the island still fell prey to French privateers.
Civil war.
In the English Civil War, Thomas Bushell held Lundy for King Charles I, rebuilding Marisco Castle and garrisoning the island at his own expense. He was a friend of Francis Bacon, a strong supporter of the Royalist cause and an expert on mining and coining. It was the last Royalist territory held between the first and second civil wars. After receiving permission from Charles I, Bushell surrendered the island on 24 February 1647 to Richard Fiennes, representing General Fairfax. In 1656, the island was acquired by Lord Saye and Sele.
18th and 19th centuries.
The late 18th and early 19th centuries were years of lawlessness on Lundy, particularly during the ownership of Thomas Benson (1708-1772), a Member of Parliament for Barnstaple in 1747 and Sheriff of Devon, who notoriously used the island for housing convicts whom he was supposed to be deporting. Benson leased Lundy from its owner, John Leveson-Gower, 1st Earl Gower (1694–1754) (who was an heir of the Grenville family of Bideford and of Stowe, Kilkhampton in Cornwall), at a rent of £60 per annum and contracted with the Government to transport a shipload of convicts to Virginia, but diverted the ship to Lundy to use the convicts as his personal slaves. Later Benson was involved in an insurance swindle. He purchased and insured the ship "Nightingale" and loaded it with a valuable cargo of pewter and linen. Having cleared the port on the mainland, the ship put into Lundy, where the cargo was removed and stored in a cave built by the convicts, before setting sail again. Some days afterwards, when a homeward-bound vessel was sighted, the "Nightingale" was set on fire and scuttled. The crew were taken off the stricken ship by the other ship, which landed them safely at Clovelly.
Sir Aubrey Vere Hunt of Curraghchase purchased the island from John Cleveland in 1802 for £5,270. Sir Vere Hunt planted in the island a small, self-contained Irish colony with its
own constitution and divorce laws, coinage and stamps. He failed in his attempt to sell the Island to the British Government as a base for troops, and his son Sir Aubrey Thomas de Vere also had great difficulty in securing any profit from the property. The tenants came from Sir Vere Hunt's Irish estate and they experienced agricultural difficulties while on the island. This led Sir Vere Hunt to seek someone who would take the island off his hands. In the 1820s John Benison agreed to purchase the Island for £4,500 but then refused to complete sale as he felt that Aubrey could not make out a good title in respect of the sale terms, namely that the Island was free from tithes and taxes.
William Hudson Heaven purchased Lundy in 1834, as a summer retreat and for the shooting, at a cost of 9,400 guineas (£9,870). He claimed it to be a "free island", and successfully resisted the jurisdiction of the mainland magistrates. Lundy was in consequence sometimes referred to as "the kingdom of Heaven". It belongs in fact to the county of Devon, and has always been part of the hundred of Braunton. Many of the buildings on the island today, including Saint Helena's Church, designed by the architect John Norton, and Millcombe House (originally known simply as The Villa), date from the Heaven period. The Georgian-style Villa was built in 1836. However, the expense of building the road from the beach (no financial assistance being provided by Trinity House, despite their regular use of the road following the construction of the lighthouses), the Villa and the general cost of running the island had a ruinous effect on the family's finances, which had been damaged by reduced profits from their sugar plantations in Jamaica.
In 1957 a message in a bottle from one of the seamen of the "HMS Caledonia" was washed ashore between Babbacombe and Peppercombe in Devon. The letter, dated 15 August 1843 read: "Dear Brother, Please e God i be with y against Michaelmas. Prepare y search Lundy for y Jenny ivories. Adiue William, Odessa". The bottle and letter are on display at the Portledge Hotel at Fairy Cross, in Devon, England. The "Jenny" was a three-masted schooner reputed to be carrying ivory and gold dust that was wrecked on Lundy (at a place thereafter called Jenny's Cove) on 20 February 1797. The ivory was apparently recovered some years later but the leather bags supposed to contain gold dust were never found.
20th and 21st centuries.
William Heaven was succeeded by his son the Reverend Hudson Grosset Heaven who, thanks to a legacy from Sarah Langworthy (née Heaven), was able to fulfill his life's ambition of building a stone church on the island. St Helen's was completed in 1896, and stands today as a lasting memorial to the Heaven period. It has been designated by English Heritage a Grade II listed building. He is said to have been able to afford either a church or a new harbour. His choice of the church was not however in the best financial interests of the island. The unavailability of the money for re-establishing the family's financial soundness, coupled with disastrous investment and speculation in the early 20th century, caused severe financial hardship.
Hudson Heaven died in 1916, and was succeeded by his nephew, Walter Charles Hudson Heaven. With the outbreak of World War I, matters deteriorated seriously, and in 1918 the family sold Lundy to Augustus Langham Christie. In 1924, the Christie family sold the island along with the mail contract and the MV "Lerina" to Martin Coles Harman, who proclaimed himself a king. Harman issued two coins of Half Puffin and One Puffin denominations in 1929, nominally equivalent to the British halfpenny and penny, resulting in his prosecution under the United Kingdom's Coinage Act of 1870. The House of Lords found him guilty in 1931, and he was fined £5 with fifteen guineas expenses. The coins were withdrawn and became collectors' items. In 1965 a "fantasy" restrike four-coin set, a few in gold, was issued to commemorate 40 years since Harman purchased the island. Harman's son, John Pennington Harman was awarded a posthumous Victoria Cross during the Battle of Kohima, India in 1944. There is a memorial to him at the VC Quarry on Lundy. Martin Coles Harman died in 1954.
Residents did not pay taxes to the United Kingdom and had to pass through customs when they travelled to and from Lundy Island. Although the island was ruled as a virtual fiefdom, its owner never claimed to be independent of the United Kingdom, in contrast to later territorial "micronations". 
Following the death of Harman's son Albion in 1968, Lundy was put up for sale in 1969. Jack Hayward, a British millionaire, purchased the island for £150,000 and gave it to the National Trust, who leased it to the Landmark Trust. The Landmark Trust has managed the island since then, deriving its income from arranging day trips, letting out holiday cottages and from donations.
The island is visited by over 20,000 day-trippers a year, but during September 2007 had to be closed for several weeks owing to an outbreak of Norovirus.
Wreck of Battleship "Montagu".
A naval footnote in the history of Lundy was the wreck of the Royal Navy battleship HMS "Montagu". Steaming in heavy fog, she ran hard aground near Shutter Rock on the island's southwest corner at about 2:00 a.m. on 30 May 1906. Thinking they were aground at Hartland Point on the English mainland, a landing party went ashore for help, only finding out where they were after encountering the lighthouse keeper at the island's north light.
Strenuous efforts by the Royal Navy to salvage the badly damaged battleship during the summer of 1906 failed, and in 1907 it was decided to give up and sell her for scrap. "Montagu" was scrapped at the scene over the next fifteen years. Diving clubs still visit the site, where armour plate and live 12-inch (305-millimetre) shells remain on the seabed.
Remains of a German Heinkel 111H bomber.
During World War II two German Heinkel He 111 bombers crash landed on the island in 1941. The first was on 3 March, when all the crew survived and were taken prisoner. The second was on 1 April when the pilot was killed and the other crew members were taken prisoner. The plane had bombed a British ship and one engine was damaged by anti aircraft fire, forcing it to crash land. A few remains can be found on the crash site. Reportedly to avoid reprisals the crew concocted a story that they were on a reconnaissance mission.
Geography.
The island of Lundy is 5 km long from north to south by a little over 1 km wide, with an area of 445 ha. The highest point on Lundy is Beacon Hill, 143 m above sea level. A few metres off the northeastern coast is Seal's Rock which is so called after the seals which rest on and inhabit the islet. It is less than 50 m wide. Near the jetty is a small pocket beach.
Geology.
The island is primarily composed of granite of 50±3 to 54±2 million years (from the Eocene epoch), with slate at the southern end; the plateau soil is mainly loam, with some peat. Among the igneous dykes cutting the granite are a small number composed of a unique orthophyre. This was given the name Lundyite in 1914, although the term – never precisely defined – has since fallen into disuse.
Climate.
Lundy Island lies on the borderline where the North Atlantic Ocean and the Bristol Channel meet, so it has quite a mild climate. Lundy has cool, wet winters and mild, wet summers. It is often windy.
Ecology.
Flora.
There is one endemic plant species, the Lundy cabbage "(Coincya wrightii)", a species of primitive brassica.
By the 1980s the eastern side of the island had become overgrown by rhododendrons "(Rhododendron ponticum)" which had spread from a few specimens planted in the garden of Millcombe House in Victorian times, but eradication of this non-native plant has been undertaken by volunteers over the past fifteen years in an operation known on the island as "rhody-bashing". The vegetation on the plateau is mainly dry heath, with an area of waved Calluna heath towards the northern end of the island, which is also rich in lichens, such as "Teloschistes flavicans" and several species of Cladonia and Parmelia. Other areas are either a dry heath/acidic grassland mosaic, characterised by heaths and western gorse ("Ulex gallii"), or semi-improved acidic grassland in which Yorkshire fog ("Holcus lanatus") is abundant. Tussocky (Thrift) (Holcus/Armeria) communities occur mainly on the western side, and some patches of bracken ("Pteridium aquilinum") on the eastern side.
Fauna.
Terrestrial invertebrates.
Two invetebrate taxa are endemic to Lundy, with both feeding on the endemic Lundy cabbage ("Coincya wrightii"). These are the Lundy cabbage flea beetle ("Psylliodes luridipennis"), a species of leaf beetle (family Chrysomelidae) and the Lundy cabbage weevil ("Ceutorhynchus contractus" var. "pallipes"), a variety of true weevil (family Curculionidae). In addition, the Lundy cabbage is the main host of a flightless form of "Psylliodes napi" (another species of flea beetle) and a wide variety of other invertebrate species which are not endemic to the island. Another resident invertebrate of note is "Atypus affinis", the only British species of purseweb spider.
Birds.
The number of puffins ("Fratercula arctica"), which may have given the island its name, declined in the late 20th and early 21st centuries, with the 2005 breeding population estimated to be only two or three pairs, as a consequence of depredations by brown and black rats ("Rattus rattus") (which have now been eliminated) and possibly also as a result of commercial fishing for sand eels, the puffins' principal prey. Since 2005, the breeding numbers have been slowly increasing. Adults were seen taking fish into four burrows in 2007, and six burrows in 2008.
As an isolated island on major migration routes, Lundy has a rich bird life and is a popular site for birdwatching. Large numbers of black-legged kittiwake ("Rissa tridactyla") nest on the cliffs, as do razorbill ("Alca torda"), guillemot ("Uria aalge"), herring gull ("Larus argentatus"), lesser black-backed gull ("Larus fuscus"), fulmar ("Fulmarus glacialis"), shag ("Phalacrocorax aristotelis"), oystercatcher ("Haematopus ostralegus"), skylark ("Alauda arvensis"), meadow pipit ("Anthus pratensis"), common blackbird ("Turdus merula"), robin ("Erithacus rubecula") and linnet ("Carduelis cannabina"). There are also smaller populations of peregrine falcon ("Falco peregrinus") and raven ("Corvus corax").
Lundy has attracted many vagrant birds, in particular species from North America. The island's bird list totals 317 species. This has included the following species, each of which represents the sole British record: Ancient murrelet, eastern phoebe and eastern towhee. Records of bimaculated lark, American robin and common yellowthroat were also firsts for Britain (American robin has also occurred two further times on Lundy). Veerys in 1987 and 1997 were Britain's second and fourth records, a Rüppell's warbler in 1979 was Britain's second, an eastern Bonelli's warbler in 2004 was Britain's fourth, and a black-faced bunting in 2001 Britain's third.
Other British Birds rarities that have been sighted (single records unless otherwise indicated) are: Little bittern, glossy ibis, gyrfalcon (3 records), little and Baillon's crakes, collared pratincole, semipalmated (5 records), least (2 records), white-rumped and Baird's (2 records) sandpipers, Wilson's phalarope, laughing gull, bridled tern, Pallas's sandgrouse, great spotted, black-billed and yellow-billed (3 records) cuckoos, European roller, olive-backed pipit, citrine wagtail, Alpine accentor, thrush nightingale, red-flanked bluetail, black-eared (2 records) and desert wheatears, White's, Swainson's (3 records), and grey-cheeked (2 records) thrushes, Sardinian (2 records), Arctic (3 records), Radde's and western Bonelli's warblers, Isabelline and lesser grey shrikes, red-eyed vireo (7 records), two-barred crossbill, yellow-rumped and blackpoll warblers, yellow-breasted (2 records) and black-headed (3 records) buntings, rose-breasted grosbeak (2 records), bobolink and Baltimore oriole (2 records).
Mammals.
Lundy is home to an unusual range of mammals, almost all introduced, including a distinct breed of wild pony, the Lundy pony. Until recently, Lundy and the Shiant Isles in the Hebrides were the only two places in the UK where the black rat ("Rattus rattus") could be found regularly. In the rest of the United Kingdom they have largely been replaced by brown rats except for occasional sightings in port towns and the Thames Estuary. It has since been eradicated on the island, in order to protect the nesting seabirds. Other species which have made the island their home include the grey seal ("Halichoerus grypus"), Sika deer ("Cervus nippon"), pygmy shrew ("Sorex minutus") and feral goats ("Capra aegagrus hircus"). Unusually, 20% of the rabbits ("Leporidae") on the island are melanistic compared with 4% which is typical in the UK. In mid-2006 the rabbit population was devastated by myxomatosis, leaving only 60 pairs from the previous 15–20,000 individuals. Soay sheep ("Ovis aries") on the island have been shown to vary their behaviours according to nutritional requirements, the distribution of food and the risk of predation.
Marine habitat.
In 1971 a proposal was made by the Lundy Field Society to establish a marine reserve, and the survey was led by Dr Keith Hiscock, supported by a team of students from Bangor University. Provision for the establishment of statutory Marine Nature Reserves was included in the Wildlife and Countryside Act 1981, and on 21 November 1986 the Secretary of State for the Environment announced the designation of a statutory reserve at Lundy.
There is an outstanding variety of marine habitats and wildlife, and a large number of rare and unusual species in the waters around Lundy, including some species of seaweed, branching sponges, sea fans and cup corals.
In 2003 the first statutory No Take Zone (NTZ) for marine nature conservation in the UK was set up in the waters to the east of Lundy island. In 2008 this was declared as having been successful in several ways including the increasing size and number of lobsters within the reserve, and potential benefits for other marine wildlife. However, the no take zone has received a mixed reaction from local fishermen.
On 12 January 2010 the island became Britain's first Marine Conservation Zone designated under the Marine and Coastal Access Act 2009, designed to help to preserve important habitats and species.
Transport.
To the island.
There are two ways to get to Lundy, depending on the time of year. In the summer months (April to October) visitors are carried on the Landmark Trust's own vessel, MS "Oldenburg", which sails from both Bideford and Ilfracombe. Sailings are usually three days a week, on Tuesdays, Thursdays and Saturdays, with additional sailings on Wednesdays during July and August. The voyage takes on average two hours, depending on ports, tides and weather. The "Oldenburg" was first registered in Bremen, Germany in 1958 and has been sailing to Lundy since her engine was replaced in 1985.
In the winter months (November to March), the "Oldenburg" is out of service, and the island is served by a scheduled helicopter service from Hartland Point. The helicopter operates on Mondays and Fridays, with flights between 12 noon and 2 pm. The heliport is a field at the top of Hartland Point, not far from the Beacon.
A grass runway of 400 by is available, allowing access to small STOL aircraft skilfully piloted.
Properly equipped and experienced canoeists can kayak to the island from Hartland Point or Lee Bay. This takes 4 to 6 hours depending on wind and tides.
Entrance to Lundy is free for anyone arriving by scheduled transport. Visitors arriving by non-scheduled transport are charged an entrance fee, currently (June 2013) £5.00, and there is an additional charge payable by those using light aircraft. Anyone arriving on Lundy by non-scheduled transport is also charged an additional fee for transporting luggage to the top of the island.
On the island.
In 2007, Derek Green, Lundy's general manager, launched an appeal to raise £250,000 to save the mile-long Beach Road, which had been damaged by heavy rain and high seas. The road was built in the first half of the 19th century to provide people and goods with safe access to the top of the island, 120 m above the only jetty. The fund-raising was completed on 10 March 2009.
Lighthouses.
Foundations for a lighthouse on Lundy were laid in 1787, but the first lighthouse (now known as the Old Light) was not built until Trinity House obtained a 999-year lease in 1819. The 97 ft granite tower, on the summit of Chapel Hill, was designed by Daniel Asher Alexander, and built by Joseph Nelson at a cost of £36,000. Because the site, Beacon Hill, is 143 m above sea level, the highest base for a lighthouse in Britain, the light was often obscured by fog. To counter this problem, the Fog Signal Battery was built about 1861.
The lighthouse had two lights; the lower a fixed white light and the upper a quick flashing white light, showing every 60 seconds. However, this quick revolution gave the impression it was a fixed light with no flashes detectable. This may have contributed to the grounding, at Cefn Sidan, of the "La Jeune Emma", bound from Martinique to Cherbourg in 1828. 13 of the 19 on board drowned, including Adeline Coquelin, the 12 year-old niece of Napoleon Bonaparte's divorced wife Joséphine de Beauharnais.
Owing to the ongoing complaints about the difficulty of sighting the light in fog, the lighthouse was abandoned in 1897 when the North and South Lundy lighthouses were built. The Old Light and the associated keepers' houses are kept open by the Landmark Trust.
The current North Lundy and South Lundy lighthouses were built in 1897 at the extremities of the island to replace the old lighthouse. Both lighthouses are painted white and are run and maintained by Trinity House.
The North lighthouse is 17 m tall, slightly taller than the south one, and has a focal plane of 48 m. It produces a quick white flash every 15 seconds, and was originally lit by a 75mm petroleum vapour burner. Oil was lifted up from a small quay using a sled and winch, and then transported using a small railway (again winch-powered). The remains of this can be still seen, but it was abandoned in 1971 and the lighthouse now uses a discharge bulb fed from the island's main supply. The northern light was modernised in 1991 and converted to solar power, since when the light has been mounted on top of the old fog horn building rather than in the tower.
The South lighthouse has a focal length of 53 m and a quick white flash every 5 seconds. It can be seen as a small white dot from Hartland Point, 11 miles to the south east. It was automated and converted to solar power in 1994. The old fresnel lens has been in use since 2001 in Dungeness Lighthouse.
Electricity supply.
There is a small power station comprising three Cummins B and C series diesel engines, offering an approx 150 kVa 3 phase supply to most of the island buildings. Waste heat from the engine jackets is used for a district heating pipe. There are also plans to collect the waste heat from the engine exhaust heat gases to feed into the district heat network to improve the efficiency further.
Staying on the island.
Lundy has 23 holiday properties, sleeping between one and 14 people. These include a lighthouse, a castle and a Victorian mansion. Many of the buildings are constructed from the island's granite.
The island also has a campsite, at the south of the island in the field next to the shop. It has hot and cold running water, with showers and toilets, in an adjacent building.
Administration.
The island is an unparished area of Torridge district of the English county of Devon. It belongs to the ward of Clovelly Bay. It is part of the constituency electing the Member of Parliament for Torridge and West Devon and the South West England constituency for the European Parliament.
Stamps.
Owing to a decline in population and lack of interest in the mail contract, the GPO ended its presence on Lundy at the end of 1927. For the next two years Harman handled the mail to and from the island without charge. On 1 November 1929, he decided to offset the expense by issuing two postage stamps (1/2 puffin in pink and 1 puffin in blue). One puffin is equivalent to one English penny. The printing of Puffin stamps continues to this day and they are available at face value from the Lundy Post Office, located in the Lundy shop at the south end of the island. One used to have to stick Lundy stamps on the back of the envelope; but Royal Mail now allows their use on the front of the envelope, but placed on the left side, with the right side reserved for the Royal Mail postage stamp or stamps. Lundy stamps are cancelled by a circular Lundy handstamp. The face value of the Lundy Island stamps covers the cost of postage of letters and postcards from the island to the Bideford Post Office on the mainland for onward delivery to their final destination anywhere in the world. A meter cancel is applied in Bideford. The Lundy Post Office gets a bulk rate discount for mailing letters and postcards from Bideford. Lundy Island stamps (sometimes called Puffins) are a type of postage stamp known to philatelists as "local carriage labels" or "local stamps". Issues of increasing value were made over the years, including air mail, featuring a variety of people. Many are now highly sought-after by collectors. The market value of the early issues has risen substantially over the years. For the many thousands of annual visitors Lundy stamps have become part of the collection of the many British Local Posts collectors. The first catalogues of these stamps included Gerald Rosen's 1970 "Catalogue of British Local Stamps". Later specialist catalogues include "Stamps of Lundy Island" by Stanley Newman, first published in 1984, "Phillips Modern British Locals CD Catalogue", published since 2003, and "Labbe's Specialized Guide to Lundy Island Stamps". There is a comprehensive collection of these stamps in the Chinchen Collection, donated by Barry Chinchen to the British Library Philatelic Collections in 1977 and now held by the British Library. This is also the home of the Landmark Trust Lundy Island Philatelic Archive which includes artwork, texts and essays as well as postmarking devices and issued stamps.
In popular culture.
A ship named "Lundy Island", 3,095 tons, was captured and sunk on 10 January 1917 by the Seeadler, a windjammer under the German navy, but flying the Norwegian flag.
Lundy island is prominently featured in John Bellairs' juvenile gothic mystery, "The Secret of the Underground Room". The plot highlights several geographical and historical points of interest, including the (De) Marisco family. The book was first published in 1990.
Lundy features in the 1919 novel "Last of the Grenvilles" by Frederick Harcourt Kitchin (under his pseudonym, Bennett Copplestone)

</doc>
<doc id="18356" url="http://en.wikipedia.org/wiki?curid=18356" title="Lindow Man">
Lindow Man

Lindow Man, also known as Lindow II and (in jest) as Pete Marsh, is the preserved bog body of a man discovered in a peat bog at Lindow Moss near Wilmslow in Cheshire, North West England. The body was found on 1 August 1984 by commercial peat-cutters. Lindow Man is not the only bog body to have been found in the moss; Lindow Woman was discovered the year before, and other body parts have also been recovered. The find, described as "one of the most significant archaeological discoveries of the 1980s", caused a media sensation. It helped invigorate study of British bog bodies, which had previously been neglected in comparison to those found in the rest of Europe.
At the time of death, Lindow Man was a healthy male in his mid-20s, and he may have been someone of high status, as his body shows little evidence of heavy or rough work. There has been debate over the reason for Lindow Man's death, for the nature of his demise was violent, perhaps ritualistic; after a last meal of charred bread, Lindow Man was strangled, hit on the head, and his throat cut. Dating the body has proven problematic, but it is thought that Lindow Man was deposited into Lindow Moss, face down, some time between 2 BC and 119 AD, in either the Iron Age or Romano-British period. The body has been preserved by freeze-drying and is on permanent display at the British Museum, although it occasionally travels to other venues such as Manchester Museum.
Background.
Lindow Moss.
Lindow Moss is a peat bog in Lindow, an area of Wilmslow, Cheshire, which has been used as common land since the medieval period. It formed after the last ice age, one of many such peat bogs in north-east Cheshire and the Mersey basin that formed in hollows caused by melting ice. Investigations have not yet discovered settlement or agricultural activity around the edge of Lindow Moss that would have been contemporary with Lindow Man; however, analysis of pollen in the peat suggests there was some cultivation in the vicinity. Once covering over 600 ha, the bog has now shrunk to a tenth of its original size. It is a dangerous place; an 18th-century writer recorded people drowning there. For centuries the peat from the bog was used as fuel, and it continued to be extracted until the 1980s, by which time the process had been mechanised. Lindow Moss is a lowland raised mire; this type of peat bog often produces the best preserved bog bodies, allowing more detailed analysis. Lowland raised mires occur mainly in northern England and extend south to the Midlands. Lindow Man is one of 27 bodies to be recovered from such areas.
Lindow Woman.
On 13 May 1983, two peat workers at Lindow Moss, Andy Mould and Stephen Dooley, noticed an unusual object—about the size of a football—on the elevator taking peat to the shredding machine. They removed the object for closer inspection, joking that it was a dinosaur egg. Once the peat had been removed, their discovery turned out to be a decomposing, incomplete human head with one eye and some hair intact. Forensics identified the skull as belonging to a European woman, probably aged 30–50. Police initially thought the skull was that of Malika Reyn-Bardt, who had disappeared in 1960 and was the subject of an ongoing investigation. While in prison on another charge, her husband, Peter Reyn-Bardt, had boasted that he had killed his wife and buried her in the back garden of their bungalow, which was on the edge of the area of mossland where peat was being dug. The garden was examined but no body was recovered there. When Reyn-Bardt was confronted with the discovery of the skull from Lindow Moss, he confessed to the murder of his wife. It was later radiocarbon dated, revealing it to be nearly 2,000 years old. "Lindow Woman", as it became known, dated from around 210 AD. This emerged shortly before Reyn-Bardt went to trial, but he was convicted on the evidence of his confession.
Discovery.
A year later a further discovery was made at Lindow Moss, just 820 ft south-west of the Lindow Woman. On 1 August 1984, Andy Mould, who had been involved in the discovery of Lindow Woman, took what he thought was a piece of wood off the elevator of the peat-shredding machine. He threw the object at Eddie Slack, his workmate. When it hit the ground, peat fell off the object and revealed it to be a human foot. The police were called and the foot was taken away for examination. Rick Turner, the Cheshire County Archaeologist, was notified of the discovery and succeeded in finding the rest of the body, which later became known as Lindow Man. Some skin had been exposed and had started to decay, so to prevent further deterioration of the body, it was re-covered with peat. The complete excavation of the block containing the remains was performed on 6 August. Until it could be dated, it was moved to the Macclesfield District General Hospital for storage. As the body of Malika Reyn-Bardt had still not been found, it was thought possible the body might be hers, until it was determined to be male, and radiocarbon dated. The owners of the land on which Lindow Man was found donated the body to the British Museum, and on 21 August it was transported to London.
At the time, the body was dubbed "Pete Marsh" (a pun on "peat marsh") by Middlesex Hospital radiologists, a name subsequently adopted by local journalists, as was the similar "Pete Bogg" (a pun on "peat bog"). The find was announced to the press during the second week of investigation. As the best preserved bog body found in Britain, its discovery caused a domestic media sensation and received global coverage. Sparking excitement in the country's archaeological community, who had long expected such a find, it was hailed as one of the most important archaeological discoveries of the 1980s. A "Q.E.D." documentary about Lindow Man broadcast by the BBC in 1985 attracted 10 million viewers.
Lindow Man's official name is Lindow II, as there are other finds from the area: Lindow I (Lindow Woman) refers to a human skull, Lindow III to a "fragmented headless body", and Lindow IV to the upper thigh of an adult male, possibly that of Lindow Man. After the discovery of Lindow Man, there were no further archaeological excavations at Lindow Moss until 1987. A large piece of skin was found by workmen on the elevator on 6 February 1987. On this occasion, the police left the investigation to the archaeologists. Over 70 pieces were found, constituting Lindow III. Although the bone was not as well preserved as that of Lindow Man, the other tissues survived in better condition. The final discovery was that of Lindow IV on 14 June 1988. Part of a left leg and buttocks were found on the elevator, from a site just 50 ft west of where Lindow Man was found. Nearly three months later, on 12 September, a right thigh was discovered in the peat on the bucket of a digger. The proximity of the discovery sites, coupled with the fact that the remains were shown to come from an adult male, means that Lindow IV is probably part of Lindow Man.
Remains and investigation.
The discovery of Lindow Man marked the first well-preserved bog body discovered in Britain; its condition was comparable to that of Grauballe Man and Tollund Man from Denmark. Before Lindow Man, it was estimated that 41 bog bodies had been found in England and Wales and 15 in Scotland. Encouraged by the discovery of Lindow Man, a gazetteer was compiled and revealed a far higher number of bog bodies: over 85 in England and Wales and over 36 in Scotland. Prior to the discovery of the bodies in Lindow Moss, British bog bodies had been a relatively neglected subject compared to European examples. The interest caused by Lindow Man led to more in-depth research of accounts of discoveries in bogs since the 17th century; by 1995, the numbers had changed to 106 in England and Wales and 34 in Scotland. The remains covered a large time frame.
In life, Lindow Man would have measured between 5'6" and 5'8" (1.68 and 1.73 m) tall and weighed about 132 lb. It was possible to ascertain that his age at death was around the mid-20s. The body retains a trimmed beard, moustache, and sideburns of brown hair, as well as healthy teeth with no visible cavities, and manicured fingernails, indicating he did little heavy or rough work. Apart from a fox-fur armband, Lindow Man was discovered completely naked. When he died, Lindow Man was suffering from slight osteoarthritis and an infestation of whipworm and maw worm. As a result of decalcification of the bones and pressure from the peat under which Lindow Man was buried, his skull was distorted. While some preserved human remains may contain DNA, peat bogs such as Lindow Moss are generally poor for such a purpose, and it is unlikely that DNA could be recovered from Lindow Man.
Lindow Man and Lindow III were found to have elevated levels of copper on their skin. The cause for this was uncertain as there could have been natural causes, although a study by Pyatt "et al" proposed that the bodies may have been painted with a copper-based pigment. To test this, skin samples were taken from places likely to be painted and tested against samples from areas where painting was unlikely. It was found that the copper content of the skin of the torso was higher than the control areas, suggesting that the theory of Pyatt "et al" may have been correct. However, the conclusion was ambiguous as the overall content was above that expected of a male, and variations across the body may have been due to environmental factors. Similarly, green deposits were found in the hair, originally thought to be a copper-based pigment used for decoration, but it was later found to be the result of a reaction between the keratin in the hair and the acid of the peat bog.
Dating Lindow Man is problematic as samples from the body and surrounding peat have produced dates spanning a 900-year period. Although the peat encasing Lindow Man has been radiocarbon dated to about 300 BC, Lindow Man himself has a different date. Early tests at different laboratories returned conflicting dates for the body; later tests suggested a date between 2 BC and 119 AD. There has been a tendency to ascribe the body to the Iron Age period rather than Roman due to the interpretation that Lindow Man's death may have been a ritual sacrifice or execution. Explanations for why the peat in which he was found is much older have been sought. Archaeologist P. C. Buckland suggests that as the stratigraphy of the peat appears undisturbed, Lindow Man may have been deposited into a pool that was already some 300 years old. Geographer K. E. Barber has argued against this hypothesis, saying that pools at Lindow Moss would have been too shallow, and suggests that the peat may have been peeled back to allow the burial and then replaced, leaving the stratigraphy apparently undisturbed.
Lindow Man's last meal was preserved in his stomach and intestines and was analysed in some detail. It was hoped that investigations into the contents of the stomach would shed light on the contemporary diet, as was the case with Grauballe Man and Tollund Man in the 1950s. The analysis of the contents of the digestive system of bog bodies had become one of the principal endeavours of investigating such remains. Analysis of the grains present revealed his diet to be mostly of cereals. He probably ate slightly charred bread, although the burning may have had ritual significance rather than being an accident. Some mistletoe pollen was also found in the stomach, indicating that Lindow Man died in around March or April.
One of the conclusions of the study was that the people buried in Lindow Moss may have had a less varied diet than their European counterparts. According to Jody Joy, curator of the Iron Age collection at the British Museum, the importance of Lindow Man lies more in how he lived rather than how he died, as the circumstances surrounding his demise may never be fully established.
Death.
As the peat was cleaned off the body in the laboratory, it became clear that Lindow Man had suffered a violent death. The injuries included a V-shaped, 3.5 cm cut on top of his head; a possible laceration at the back of the head; ligature marks on the neck where a sinew cord was found; a possible wound on the right side of the neck; a possible stab wound in the upper right chest; a broken neck; and a fractured rib. Xeroradiography revealed the blow on top of the head (causing the V-shaped cut) was caused by a relatively blunt object; it had fractured the skull and driven fragments into the brain. Swelling along the edges of the wound indicated Lindow Man had lived after being struck. The blow, possibly from a small axe, would have caused unconsciousness but Lindow Man could have survived for several hours afterwards. The ligature marks on the neck were caused by the tightening of the sinew cord found around his neck, possibly a garrotte or necklace.
In the case of some injuries, such as the laceration on the back of the skull, it is not possible to confirm whether they took place before or after death due to the body's state of decay. This is also the case for the wound in the upper right chest. The cut on the right of the neck may have been the result of the body becoming bloated, causing the skin to split; however, the straight edges to the wound suggest it may have been caused by a sharp instrument such as a knife. The ligature marks on the neck may have occurred after death. In some interpretations of Lindow Man's death, the sinew is a garrotte used to break the man's neck. However, Robert Connolly, a lecturer in physical anthropology, suggests that the sinew may have been ornamental and that ligature marks may have been caused by the body swelling when submerged. The rib fracture may also have occurred after death, perhaps during the discovery of the body, but is included in some narratives of Lindow Man's death. The broken neck would have proven the fatal injury, whether caused by the sinew cord around Lindow Man's neck tightening, or by blows to the back of the head. After death, Lindow Man was deposited into Lindow Moss face down.
Interpretation.
Archaeologist Don Brothwell considers that many of the older bodies need re-examining with modern techniques, such as those used in the analysis of Lindow Man. The study of bog bodies, including these found in Lindow Moss, have contributed to a wider understanding of well-preserved human remains, helping to develop new methods in analysis and investigation. The use of sophisticated techniques, such as computer tomography (CT) scans, has marked the investigation of the Lindow bodies as particularly important. Such scans allow the reconstruction of the body and internal examination. Of the 27 bodies recovered from lowland raised mires in England and Wales, only those from Lindow Moss and the remains of Worsley Man have survived, together with a shoe from another body. The remains have a date range from the early 1st to the 4th centuries. Investigation into the other bodies relies on contemporary descriptions of the discovery.
The physical evidence allows a general reconstruction of how Lindow Man was killed, although some details are debated, but it does not explain why he was killed. In North West England, there is little evidence for religious or ritual activity in the Iron Age period. What evidence does survive is usually in the form of artefacts recovered from peat bogs. Late Iron Age burials in the region often took the form of a crouched inhumation, sometimes with personal ornaments. Although dated to the mid-1st century AD, the type of burial of Lindow Man was more common in the pre-historic period. In the latter half of the 20th century, scholars widely believed that bog bodies demonstrating injuries to the neck or head area were examples of ritual sacrifice. Bog bodies were associated with Germanic and Celtic cultures, specifically relating to head worship.
According to Brothwell, Lindow Man is one of the most complex examples of "overkill" in a bog body, and possibly has ritual meaning as it was "extravagant" for a straightforward murder. Archaeologists John Hodgson and Mark Brennand suggest that bog bodies may have been related to religious practice, although there is division in the academic community over this issue. In the case of Lindow Man, scholars debate whether the killing was murder or done as part of ritual. Anne Ross, an expert on Iron Age religion, proposed that the death was an example of human sacrifice and that the "triple death" (throat cut, strangled, and hit on the head) was an offering to several different gods. The wide date range for Lindow Man's death (2 BC to 119 AD) means he may have met his demise after the Romans conquered northern England in the 60s AD. As the Romans outlawed human sacrifice, such timing would open up other possibilities. This conclusion was emphasised by historian Ronald Hutton, who challenged the interpretation of sacrificial death. Connolly suggests that as Lindow Man was found naked, he could have been the victim of a violent robbery.
Joy said, 
"The jury really is still out on these bodies, whether they were aristocrats, priests, criminals, outsiders, whether they went willingly to their deaths or whether they were executed – but Lindow was a very remote place in those days, an unlikely place for an ambush or a murder".
Conservation.
It was feared that once Lindow Man was removed from the peat, which had preserved the body for nearly 2,000 years, the remains would start to decay, so steps were taken for preservation. After rejecting methods used to preserve other bog bodies, such as the "pit-tanning" used on Grauballe Man, which took a year and a half, scientists settled on freeze-drying. In preparation, the body was covered in a solution of 15% polyethylene glycol 400 and 85% water to prevent it from becoming distorted. The body was frozen solid and the ice vaporised to ensure Lindow Man did not shrink. Afterwards, Lindow Man was put in a specially constructed display case to control the environment, maintaining the temperature at 20 °C and the humidity at 55%.
Lindow Man is held in the British Museum. Before the remains were transferred there, people from North West England launched an unsuccessful campaign to keep the body in Manchester. The bog body has been on temporary display in other venues: at the Manchester Museum on three occasions, April to December 1987, March to September 1991, and April 2008 to April 2009; and at the Great North Museum in Newcastle from August to November 2009. The 2008–09 Manchester display, titled "Lindow Man: A Bog Body Mystery Exhibition at the Manchester Museum", won the category "Best Archaeological Innovation" in the 2010 British Archaeological Awards, run by the Council for British Archaeology.
Critics have complained that, by museum display of the remains, the body of Lindow Man has been objectified rather than treated with the respect due the dead. Emma Restall Orr, a neo-druid, has questioned whether the body should be displayed at all. This is part of a wider discussion about the scientific treatment of human remains and museum researchers and archaeologists using them as information sources.
References.
</dl>

</doc>
<doc id="18360" url="http://en.wikipedia.org/wiki?curid=18360" title="Lombok">
Lombok

Lombok is an island in West Nusa Tenggara ("Nusa Tenggara Barat" or NTB) province, Indonesia. It forms part of the chain of the Lesser Sunda Islands, with the Lombok Strait separating it from Bali to the west and the Alas Strait between it and Sumbawa to the east. It is roughly circular, with a "tail" (Sekotong Peninsula) to the southwest, about 70 km across and a total area of about 4,514 km² (1,825 sq mi). The provincial capital and largest city on the island is Mataram. It is somewhat similar in size and density with neighboring Bali and shares some cultural heritage, but is administratively part of NTB along with sparsely populated Sumbawa. It is surrounded by a number of smaller islands locally called "Gili".
The island was home to some 3.17 million Indonesians as recorded in the decennial 2010 census; the latest estimate (for January 2014) gives the population as 3,311,044.
Administration.
Lombok is under the administration of the Governor of the province of West Nusa Tenggara ("Nusa Tenggara Barat"). The province is administered from the provincial capital of Mataram in West Lombok.
The island is administratively divided into four "kabupaten" (regencies) and one "kota" (city). They are as follows, with their areas and populations at the 2010 Census and according to the latest (January 2014) official estimates:
History.
Little is known about the Lombok before the seventeenth century. Before this time it was made up of numerous competing and feuding petty states each of which were presided over by a Sasak 'prince'. This disunity was taken advantage of by the neighbouring Balinese who took control of western Lombok in the early seventeenth century. The Makassarese meanwhile invaded eastern Lombok from their colonies in neighbouring Sumbawa. The Dutch had first visited Lombok in 1674 and the Dutch East India Company concluded its first treaty with the Sasak Princess of Lombok. The Balinese had managed to take over the whole island by 1750, but Balinese infighting resulted in the island being split into four feuding Balinese kingdoms. In 1838, the Mataram kingdom brought its rivals under control.
Relations between the Sasak and Balinese in western Lombok were largely harmonious and intermarriage was common. In the island's east, however, relations were less cordial and the Balinese maintained control from garrisoned forts. While Sasak village government remained in place, the village head became little more than a tax collector for the Balinese. Villagers became a kind of serf and Sasak aristocracy lost much of its power and land holdings.
During one of the many Sasak peasant rebellions against the Balinese, Sasak chiefs sent envoys to the Dutch in Bali and invited them to rule Lombok. In June 1894, the governor general of the Dutch East Indies, Van der Wijck, signed a treaty with Sasak rebels in eastern Lombok. He sent a large army to Lombok and the Balinese raja capitulated to Dutch demands.(see Dutch intervention in Lombok) The younger princes however overruled the raja and attacked and routed the Dutch. The Dutch counterattacked overrunning Mataram and the raja surrendered. The entire island was annexed to the Netherlands East Indies in 1895. The Dutch ruled over Lombok's 500,000 people with a force of no more than 250 by cultivating the support of the Balinese and Sasak aristocracy. The Dutch are remembered in Lombok as liberators from Balinese hegemony.
During World War II a Japanese invasion force comprising elements of the 2nd Southern Expeditionary Fleet invaded and occupied the Lesser Sunda Islands, including the island of Lombok. They sailed from Soerabaja harbour at 09:00 hrs on 8 March 1942 and proceeded towards Lombok Island. On 9 May 1942 at 17:00 hrs the fleet sailed into port of Ampenan on Lombok Island. The Dutch defenders were soon defeated and the island occupied.
Following the cessation of hostilities the Japanese forces occupying Indonesia were withdrawn and Lombok returned temporarily to Dutch control. Following the subsequent Indonesian independence from the Dutch, the Balinese and Sasak aristocracy continued to dominate Lombok. In 1958, the island was incorporated into the province of West Nusa Tenggara with Mataram becoming the provincial capital. Mass killings of communists occurred across the island following the abortive coup attempt in Jakarta and Central Java. During President Suharto's New Order administration, Lombok experienced a degree of stability and development but not to the extent of the boom and wealth in Java and Bali. Crop failures led to famine in 1966 and food shortages in 1973. The national government's "transmigrasi" program moved a lot of people out of Lombok. The 1980s saw external developers and speculators instigate a nascent tourism boom although local's share of earnings was limited. Indonesia's political and economic crises of the late 1990s hit Lombok hard. In January 2000, riots broke out across Mataram with Christians and ethnic Chinese the main victims, with alleged "agents provocateur" from outside Lombok. Tourism slumped, but in recent years has seen a renewed growth.
Geography.
The Lombok Strait lies to the immediate west of the island, marking the passage of the biogeographical division between the prolific fauna of the Indomalayan ecozone and the distinctly different, but similarly prolific, fauna of Australasia—this distinction is known as the "Wallace Line" (or "Wallace's Line") and is named after Alfred Russel Wallace. Wallace was the first person to comment on the division between the two regions, as well as the abrupt boundary between the two biomes.
To the east of Lombok lies the Alas Strait, a narrow body of water separating the island of Lombok from the nearby island of Sumbawa to the east.
The island's topography is dominated by the centrally-located stratovolcano Mount Rinjani, the second highest volcano in Indonesia which rises to 3,726 m (12,224 ft). The most recent eruption of Rinjani was in May 2010 at Gunung Barujari. Ash was reported as rising two km into the atmosphere from the Barujari cone in Rinjani's caldera lake of Segara Anak. Lava flowed into the caldera lake raising its temperature while crops on the slopes of Rinjani were damaged by ash fall. The volcano, and its crater lake, 'Segara Anak' (child of the sea), are protected by the Gunung Rinjani National Park established in 1997. Recent evidence indicates an ancient volcano, Mount Samalas, of which now only a caldera remains, was the source of one of the largest volcanic eruptions in recorded history, causing worldwide changes in weather.
The highlands of Lombok are forest clad and mostly undeveloped. The lowlands are highly cultivated. Rice, soybeans, coffee, tobacco, cotton, cinnamon, cacao, cloves, cassava, corn, coconuts, copra, bananas and vanilla are the major crops grown in the fertile soils of the island. The southern part of the island is fertile but drier, especially toward the southern coastline.
The water supply in Lombok is stressed and this places strain upon both the water supply of the provincial capital, Mataram, and the island in general. The southern and central areas are reported to be the most critically affected. West Nusa Tenggara province in general is threatened with a water crisis caused by increasing forest and water table damage and degradation. 160 thousand hectares of a total of 1960 thousand hectares are thought to have been affected. The Head of Built Environment and Security Forest Service Forest West Nusa Tenggara Andi Pramari stated in Mataram on Wednesday, May 6, 2009 that, "If this situation is not addressed it can be expected that within five years it may be difficult for people to obtain water in this part of NTB (West Nusa Tenggara). Not only that, the productivity of agriculture in value added will fall, and the residents are experiencing water deficiency in their wells". High cases of timber theft in the region of NTB are contributing to this problem.
In September 2010, Central Lombok some villagers were reported to be walking for several hours to fetch a single pail of water. Nieleando, a small coastal village about 50 kilometers from the provincial capital, Mataram, has seen dry wells for years. It has been reported that occasionally the problem escalates sufficiently for disputes and fighting between villagers to occur. The problems have been reported to be most pronounced in the sub-districts of Jonggat, Janapria, Praya Timur, Praya Barat, Praya Barat Daya and Pujut. In 2010 all six sub-districts were declared drought areas by provincial authorities. Sumbawa, the other main island of the province, also experienced severe drought in 2010, making it a province-wide issue.
List of islands.
Lombok is surrounded by many islets, of which are:
Demographics.
The island's inhabitants are 85% Sasak whose origins are thought to have migrated from Java in the first millennium BC. Other residents include an estimated 10–15% Balinese, with the small remainder being Tionghoa-peranakan, Javanese, Sumbawanese and Arab Indonesians.
The Sasak population are culturally and linguistically closely related to the Balinese, but unlike the Hindu Balinese, the majority are Muslim and the landscape is punctuated with mosques and minarets. Islamic traditions and holidays influence the Island's daily activities.
In 2008 the Island of Lombok had 866,838 households and an average of 3.635 persons per household.
The 2010 census recorded a population of 4,496,855 people in the province of NTB, of which 70.42% reside on Lombok, giving it a population of 3,166,789 at that date.
Religion.
The island's indigenous Sasak people are predominantly Muslim however before the arrival of Islam Lombok experienced a long period of Hindu and Buddhist influence that reached the island through Java. A minority Balinese Hindu culture remains in Lombok.
Islam may have first been brought to Lombok by traders arriving from Sumbawa in the 17th century who then established a following in eastern Lombok. Other accounts describe the first influences arriving in the first half of the sixteenth century. According to the palm leaf manuscript Babad Lombok which contains the history of Lombok describes how Sunan Prapen was sent by his father The Susuhunan Ratu of Giri on a military expedition to Lombok and Sumbawa in order to convert the population and propagate the new religion. However, the new religion took on a highly syncretistic character, frequently mixing animist and Hindu-Buddhist beliefs and practices with Islam.
A more orthodox version of Islam increased in popularity in the early twentieth century. The Indonesian government agamaization programs (acquiring of a religion) in Lombok during 1967 and 1968 led to a period of some considerable confusion in religious allegiances and practices. These agamaization programs later led to the emergence of more conformity in religious practices in Lombok. The Hindu minority religion is still practised in Lombok alongside the majority Muslim religion.
Hinduism is followed by ethnic Balinese and by a minority of the indigenous Sasak. All the main Hindu religious ceremonies are celebrated in Lombok and there are many villages throughout Lombok that have a Hindu majority population. According to local legends two of the oldest villages on the island, Bayan and Sembalun, were founded by a prince of Majapahit.
According to the 2010 population census declared adherents of Hinduism numbered 101,000 people with the highest concentration in the Mataram Regency where they accounted for 14% of the population. The Ditjen Bimas Hindu (DBH)/ Hindu Religious Affairs Directorate's own analysis conducted in close association with Hindu communities throughout the country found that the number of Hindus in the population are much higher than counted in the government census. The survey carried out in 2012 found the Hindu population of Lombok to be 445,933. This figure is more in line with the commonly stated view that 10-15% of the Islands population is Hindu.
The Nagarakertagama, the 14th century palm leaf poem that was found on Lombok, places the island as one of the vassals of the Majapahit empire. This manuscript contained detailed descriptions of the Majapahit Kingdom and also affirmed the importance of Hindu-Buddhism in the Majapahit empire by describing temple, palaces and several ceremonial observances.
Christianity is practised by a small minority including some ethnic Chinese and immigrants from Bali and East Nusa Tenggara. There are Roman Catholic churches and parishes in Ampenan, Mataram, Praya and Tanjung. Two Buddhist temples can be visited in and around Tanjung where about 800 Buddhists live.
The history of a small Arab community in Lombok has history dating back to early settlement by traders from Yemen. The community is still evident mainly in Ampenan, the old Port of Mataram. Due to the siting of a UNHCR refugee centre in Lombok some refugees from middle eastern countries have intermarried with Lombok people.
A non-orthodox Islamic group found only on Lombok are the Wektu Telu ("Three times"), who pray three times daily, instead of the five times stipulated in the Quran. Waktu Telu beliefs are entwined with animism, and is influenced not only by Islam, but also Hinduism and pantheistic beliefs. There are also remnants of Boda who maintain Pagan Sasak beliefs and could be representative of an original Sasak culture, undiluted by later Islamic innovations.
Many influences of animist belief prevail within the Sasak people, most of whom believe in the existence of spirits or "ghosts". They regard both food and prayer as indispensable whenever they seek to communicate with spirits, including the dead and ritualistic traditional practices endure. Traditional magic is practised to ward off evil and illness and to seek solutions to disputations and antipathy. Magic may be practised by an individual alone but normally a person experienced in such things is sought out to render a service. Normally money or gifts are made to this person and the most powerful practitioners are treated with considerable respect.
Economy and politics.
Many of the visitors to Lombok and much of the islands goods come across the Lombok Strait by sea or air links from Bali. Only 25 miles separate the two islands. Lombok is often marketed as “an unspoiled Bali,” or “Bali’s sister island.” Currently with support of the central government Lombok and Sumbawa are being developed as Indonesia 2nd destination for international and domestic tourism. Lombok has retained a more natural, uncrowded and undeveloped environment, which attract travelers who come to enjoy its relaxed pace and the opportunity to explore the island's unspoiled, spectacular natural beauty. The more contemporary marketing campaigns for Lombok/Sumbawa seek to differentiate from Bali and promote the island of Lombok as a standalone destination. The opening of the Lombok International Airport on 1 October 2011 assisted in this endeavour.
Nusa Tenggara Barat and Lombok may be considered economically depressed by First World standards and a large majority of the population live in poverty. Still, the island is fertile, has sufficient rainfall in most areas for agriculture, and possesses a variety of climate zones. Consequently, food in abundant quantity and variety is available inexpensively at local farmer's markets, though locals still suffer from famine due to drought and subsistence farming. A family of 4 can eat rice, vegetables, and fruit for as little as US$0.50. Even though a family's income may be as small as US$1.00 per day from fishing or farming, many families are able to live a contented and productive life on such astonishingly small incomes. However, the people of Lombok are coming under increasing pressure from rising food and fuel prices. Access to housing, education and health services remains difficult for many of the island's indigenous population.
The percentage of the population living in poverty in urban areas of Nusa Tenggara Barat in 2008 was 29.47% and in 2009 it was 28.84%.
For those living in rural areas in 2008 it was 19.73% and in 2009 it reduced marginally to 18.40%
For combined urban and village the figures were 23.81% and in 2009 it fell slightly to 22.78%.
In Mataram in 2008 the percentage of the population that was unmarried was 40.74%, married 52.01%,
divorced 2.51% and widowed 4.75%.
Tourism.
 Tourism is an important source of income on Lombok. The most developed tourism area of the island is on the west coast of the island and is centered about the township of Senggigi. The immediate surrounds of the township contain the most developed tourism facilities. The west coast coastal tourism strip is spread along a 30 km strip following the coastal road north from Mataram and the old airport at Ampenan. The principal tourism area extends to Tanjung in the northwest at the foot of Mount Rinjani and includes the Sire and Medana Peninsulas and the highly popular Gili Islands lying immediately offshore. These three small islands are most commonly accessed by boat from Bangsal near Pemenang, Teluk Nare a little to the south, or from further south at Senggigi and Mangsit beach. Many hotels and resorts offer accommodations ranging from budget to luxurious. Recently direct fast boat services have been running from Bali making a direct connection to the Gili islands. Although rapidly changing in character, the Gili islands still provide both a lay-back backpacker's retreat and a high class resort destination.
Other tourist destinations include Mount Rinjani, Gili Bidara, Gili Lawang, Narmada Park and Mayura Park and Kuta (distinctly different from Kuta, Bali). Sekotong, in southwest Lombok, is popular for its numerous and diverse scuba diving locations.
The Kuta area is also famous for its beautiful, largely deserted, white sand beaches. The Small town is rapidly developing since the opening of the International airport in Praya. Increasing amounts of surfers from around the globe come here seeking out perfect surf and the slow and rustic feel Lombok. South Lombok surfing is considered some of the best in the world. Large polar lows push up through the Indian Ocean directing long range, high period swell from as far south as Heard Island from late March through to September or later. This conicides with the dry season and South-East trade winds that blow like clock work. Lombok is famous for its diversity of breaks, which includes world renowned "Desert Point" at Banko Banko in the southwest of the island.
The northern west coast near Tanjung has many new upmarket hotel and villa developments centreed about the Sire and Medana peninsular nearby to the Gili islands and a new at Medana bay. These new developments complement the already existing 5 star resorts and a large golf course already established there.
Pre-2000.
Tourist development started in the mid-1980s, when Lombok attracted attention as an 'unspoiled' alternative to Bali. Initially, low budget bungalows proliferated at places like the Gili islands and Kuta, Lombok on the South Coast. These tourist accommodations were largely owned by and operated by local business entrepreneurs. Areas in proximity to the airport, places like Sengiggi, experienced rampant land speculation for prime beachfront land by big businesses from outside Lombok.
In the 1990s the national government in Jakarta began to take an active role in planning for and promoting Lombok's tourism. Private organizations like the Bali Tourism Development Corporation (BTDC) and the Lombok Tourism Development Corporation (LTDC) were formed. LTDC prepared detailed land use plans with maps and areas zoned for tourist facilities. Large hotels provide primary employment for the local population. Ancillary business, ranging from restaurants to art shops have been started by local businessmen. These businesses provide secondary employment for local residents.
1997 to 2007.
The 1997 Asian Financial Crisis and the fall of Suharto regime in 1998 marked the beginning a decade of setbacks for tourism. Spurred by rapid devaluation of the currency and the transition to true democracy caused all of Indonesia to experience a period of domestic unrest. Many of Indonesian Provinces struggled with elements of the population desiring autonomy or independence from the Republic of Indonesia. At the same time fanatical Islamic terrorism in Indonesia further aggravated domestic unrest across the archipelago.
In Jan 2000, radical Islamic agitators from the newly formed Jemaah Islamiyah provoked religious and ethnic violence in the Ampenan area of Mataram and the southern area of Senggigi. Many foreign expatriates and tourists were temporarily evacuated to Bali. Numerous foreign embassies issued Travel Warnings advising of the potential danger of traveling to Indonesia.
Subsequently, the 2002 Bali bombings, the 2005 Bali bombings and the Progress of the SARS outbreak in Asia all dramatically impacted tourism activities in Lombok. Tourism was slow to return to Lombok, provoked in part by a worldwide reluctance to travel because of global tensions. Only since 2007–2008, when most developed countries lifted their Travel Warnings has tourism recovered to the pre-2000 levels.
2008 to the Present.
The years leading up to 2010 has seen a rapid revival and promotion of tourism recovery in the tourism industry. The number of visitors has far surpassed the pre-2000 levels. All signs indicate the long-term trend will see a steady increase in the number of visitor arrivals.
Both the local government and many residents recognise that tourism and services related to tourism will continue to be a major source of income for the island. The island's natural beauty and the customary hospitality of its residents make it an obvious tourist destination.
Lombok retains the allure of an undeveloped and natural environment. Tourism visits to this tropical island are increasing again as both international and local tourists are re-discovering the charms of Lombok. With this new interest comes the development of a number of boutique resorts on the island providing quality accommodation, food and drinks in near proximity to relatively unspoiled countryside.
The Indonesian government is actively promoting both Lombok and neighboring Sumbawa as Indonesia's number two tourism destination after Bali. The President of Indonesia, Susilo Bambang Yudhoyono, the Ministry of Cultural and Tourism and the regional Governor have made public statements supporting the development of Lombok as a tourism destination and setting a goal of 1 million visitors annually by the year 2012 for the combined destination of Lombok and Sumbawa.
 This has seen infrastructure improvements to the island including road upgrades and the construction of a much delayed new International airport in the islands south. Despite this, Sumbawa retains a very rustic feel compared to Lombok.
Lombok International Airport ("Bandara Internasional Lombok") (IATA: LOP, ICAO: WADL) is south west of the small regional city of Praya in South central Lombok. It commenced operations on 1 October 2011. It replaced Selaparang airport near Ampenan. It is the only operational international airport within the province of West Nusa Tenggara ("Nusa Tenggara Barat").
Selaparang Airport in Ampenan was closed for operations on the evening of 30 September 2011. It previously provided facilities for domestic services to Java, Bali, and Sumbawa and international services to Singapore to Kuala Lumpur via Surabaya and Jakarta. It was the island's original airport and is situated on Jalan Adi Sucipto on the north western outskirts of Mataram. The terminals and basic airport infrastructure remain intact but it is closed to all civil airline traffic.
Lembar Harbor seaport in the southwest has shipping facilities and a ferry for road vehicles and passenger services. In 2013, the gross tonnage is 4.3 million Gross Tonnages or increase by 72 percent from 2012 data means in Lombok and West Nusa Tenggara the economy progress significantly.
Labuhan Lombok ferry port on the east coast provides a ferry for road vehicles and passenger services to Poto Tano on Sumbawa.
Pelni Shipping Line provides a national network of passenger ship services throughout the Indonesian archipelago. Pelni have offices in Ampenan.
Transport between Bali and Lombok.
Flights from Ngurah Rai International Airport (IATA: DPS) to Lombok International Airport (IATA: LOP) take about 40 minutes. Lombok international airport is located in southwest Lombok, 1.5 hours drive to Senggigi main tourist areas in the west Lombok, 2 hours drive to the jetty of Teluk Nara before you cross to Gili Islands and about 30 minutes drive to Kuta south Lombok. 
Public Ferries depart from Padang Bai (Southeast Bali) and Lembar (Southwest Lombok) every hour, taking a minimum of 4-5 hours make the crossing in either direction. 
Fastboat services are available from various departure points on Bali and principally serve the Gili Islands, with some significant onward traffic to the Lombok mainland. Arrival points on Lombok are dependent upon the operator, at either Teluk Nare/Teluk Kodek, Bangsal harbour or the township of Senggigi, all on the northwest coast. Operating standards vary widely.
Water resources.
Areas in southern Lombok Island were classified as arid and prone to water shortages due to low rainfall and lack of water sources. On May 2011, grounbreaking ceremony has done to initial the Pandanduri dam construction which will span about 430 hectares and cost estimated Rp.800 billion ($92.8 million) to accommodate about 25.7 million cubic meters of water and be able to irrigate 10,350 hectares of farmland. The project would be finished by the next five years.

</doc>
<doc id="18362" url="http://en.wikipedia.org/wiki?curid=18362" title="Lego">
Lego

Lego () is a line of plastic construction toys that are manufactured by The Lego Group, a privately held company based in Billund, Denmark. The company's flagship product, Lego, consists of colourful interlocking plastic bricks and an accompanying array of gears, minifigures and various other parts. Lego bricks can be assembled and connected in many ways, to construct such objects as vehicles, buildings, and even working robots. Anything constructed can then be taken apart again, and the pieces used to make other objects.
Lego began manufacturing interlocking toy bricks in 1949. Since then a global Lego subculture has developed, supporting movies, games, competitions, and six themed amusement parks. As of 2013, around 560 billion Lego parts have been produced. In February 2015, Lego replaced Ferrari as the "World's most powerful brand."
History.
 
The Lego Group began in the workshop of Ole Kirk Christiansen (1891 – 1958), a carpenter from Billund, Denmark, who began making wooden toys in 1932. In 1934, his company came to be called "Lego", from the Danish phrase "leg godt", which means "play well". It expanded to producing plastic toys in 1947. In 1949 Lego began producing, among other new products, an early version of the now famous interlocking bricks, calling them "Automatic Binding Bricks". These bricks were based in part on the Kiddicraft Self-Locking Bricks, which were patented in the United Kingdom in 1939 and then there released in 1947. Lego modified the design of the Kiddicraft brick after examining a sample given to it by the British supplier of an injection-molding machine that the company had purchased. The bricks, originally manufactured from cellulose acetate, were a development of traditional stackable wooden blocks that locked together by several round studs on top and a hollow rectangular bottom. The blocks snapped together, but not so tightly that they required extraordinary effort to be separated.
The Lego Group's motto is "det bedste er ikke for godt" which means roughly "only the best is the best" (more literally "the best is never too good"). This motto was created by Ole Kirk to encourage his employees never to skimp on quality, a value he believed in strongly. The motto is still used within the company today. By 1951 plastic toys accounted for half of the Lego Company’s output, although Danish trade magazine "Legetøjs-Tidende" ("Toy-Times"), visiting the Lego factory in Billund in the early 1950s, felt that plastic would never be able to replace traditional wooden toys. Although a common sentiment, Lego toys seem to have become a significant exception to the dislike of plastic in children's toys, due in part to the high standards set by Ole Kirk.
By 1954, Christiansen's son, Godtfred, had become the junior managing director of the Lego Group. It was his conversation with an overseas buyer that led to the idea of a toy system. Godtfred saw the immense potential in Lego bricks to become a system for creative play, but the bricks still had some problems from a technical standpoint: their locking ability was limited and they were not versatile. In 1958, the modern brick design was developed, and it took another five years to find the right material for it, ABS (acrylonitrile butadiene styrene) polymer. The modern Lego brick was patented on 28 January 1958.
The Lego Group's Duplo product line, introduced in 1969, is a range of simple blocks which measure twice the width, height and depth of standard Lego blocks, and are aimed at younger children.
In 1978, Lego produced the first minifigures, which have since become a staple in most sets. New elements are often released along with new sets. There are also Lego sets designed to appeal to young girls such as the Belville and Clikits lines which consist of small interlocking parts that are meant to encourage creativity and arts and crafts, much like regular Lego bricks. Belville and Clikit pieces can interlock with regular Lego bricks as decorative elements.
Lego Fabuland ran from 1979 to 1989. The more advanced Lego Technic was launched in 1977. Lego Primo is a line of blocks by the Lego Group for young children that ran between 2004 and 2006. In 1995 Lego Baby was launched for babies.
In May 2011, Space Shuttle Endeavour mission STS-134 brought 13 Lego kits to the International Space Station, where astronauts will build models and see how they react in microgravity, as part of the Lego Bricks in Space program. The results will be shared with schools as part of an educational project.
In May 2013, the largest model ever created was displayed in New York, made of over 5 million bricks; a 1:1 scale model of an X-Wing. Other records are a 112-foot tower and a 4 km railway.
In February 2015, Lego replaced Ferrari as the "World's most powerful brand."
Sets.
Since the 1950s, the Lego Group has released thousands of sets with a variety of themes, including town and city, space, robots, pirates, trains, Vikings, castle, dinosaurs, undersea exploration, and wild west.
The Lego range has expanded to encompass accessory motors, gears, lights, sensors, and cameras designed to be used with Lego components. Motors, battery packs, lights and switches are sold under the name "Power Functions". The "Technic" line utilises newer types of interlocking connections that are still compatible with the older brick type connections. The "Technic" line can often be motorised with "Power Functions".
Bionicle is a line of toys by the Lego Group that was marketed towards those in the 7- to 16-year-old age range. The line was launched in January 2001 in Europe and June–July 2001 in the United States. The Bionicle idea originated from the earlier toy lines Slizers (also known as Throwbots) and the short-lived RoboRiders. Both of these lines had similar throwing disks and characters based on classical elements. The sets in the Bionicle line have increased in size and flexibility through the years. Bionicle was discontinued and was replaced with Hero Factory in 2010, which itself was discontinued in favor of reviving the Bionicle line for 2015. Along with Hero Factory, another similar set has been made such as the Bionicle type Lego Legends of Chima, which uses the same structure for the minifigures.
One of the largest Lego sets ever commercially produced is a minifig-scaled edition of the Star Wars Millennium Falcon. Designed by Jens Kronvold Fredericksen, it was released in 2007 and has 5,195 pieces. It was surpassed, though, by a 5,922-piece Taj Mahal.
Design.
Lego pieces of all varieties constitute a universal system. Despite variation in the design and purpose of individual pieces over the years, each remains compatible in some way with existing pieces. Lego bricks from 1958 still interlock with those made in the current time, and Lego sets for young children are compatible with those made for teenagers. Six pieces of 2x4 bricks can be combined in 915,103,765 ways.
Each Lego piece must be manufactured to an exacting degree of precision. When two pieces are engaged they must fit firmly, yet be easily disassembled. The machines that make Lego bricks have tolerances as small as 10 micrometres.
Primary concept and development work takes place at the Billund headquarters, where the company employs approximately 120 designers. The company also has smaller design offices in the UK, Spain, Germany, and Japan, which are tasked with developing products aimed specifically at these markets. The average development period for a new product is around twelve months, in three stages. The first stage is to identify market trends and developments, including contact by the designers directly with the market; some are stationed in toy shops close to holiday periods, while others interview children. The second stage is the design and development of the product based upon the results of the first stage. As of September 2008 the design teams use 3D modelling software to generate CAD drawings from initial design sketches. The designs are then prototyped using an in-house stereolithography machine. These are presented to the entire project team for comment and for testing by parents and children during the "validation" process. Designs may then be altered in accordance with the results from the focus groups. Virtual models of completed Lego products are built concurrently with the writing of the user instructions. Completed CAD models are also used in the wider organisation, such as for marketing and packaging.
A computer program (LEGO Digital Designer) is available for consumers to create their own digital designs, and a similar tool is available for the Chrome browser. A service to ship physical models from LDD to consumers ended in 2012.
Manufacture.
Since 1963, Lego pieces have been manufactured from a strong, resilient plastic known as acrylonitrile butadiene styrene (ABS). As of September 2008, the engineers use the NX CAD/CAM/CAE PLM software suite to model the elements. The software allows the parts to be optimised by way of mould flow and stress analysis. Prototype moulds are sometimes built before the design is committed to mass production. The ABS plastic is heated to 232 C until at a dough-like consistency. It is then injected into the moulds at pressures between 25 and 150 tons, and takes approximately 15 seconds to cool. The moulds are permitted a tolerance of up to two micrometres, to ensure the bricks remain connected. Human inspectors check the output of the moulds, to eliminate significant variations in colour or thickness. According to the Lego Group, about eighteen bricks out of every million fail to meet the standard required. Lego factories recycle all but about 1 percent of their plastic waste from the manufacturing process. If the plastic cannot be re-used in Lego bricks, it is processed and sold on to industries that can make use of it.
Manufacturing of Lego bricks occurs at a number of locations around the world. Moulding is done in Billund, Denmark; Nyíregyháza, Hungary; and Monterrey, Mexico. Brick decorations and packaging is done at plants in Denmark, Hungary, Mexico and Kladno in the Czech Republic. The Lego Group estimates that in the course of five decades it has produced some 400 billion Lego blocks. Annual production of Lego bricks averages approximately 36 billion per year, or about 1140 elements per second. If all the Lego bricks ever produced were to be divided equally among a world population of six billion, each person would have 62 Lego bricks.
According to an article in "BusinessWeek" in 2006, Lego could be considered the world's No. 1 tire manufacturer; the factory produces about 306 million small rubber tires a year. The claim was reiterated in 2012.
In December 2012, the BBC's "More or Less" programme asked the Open University's engineering department to determine "how many Lego bricks, stacked one on top of the other, it would take to destroy the bottom brick?" Using a hydraulic testing machine, the engineering department determined the average maximum force a 2×2 Lego brick can stand is 4,240 newtons; since an average 2×2 Lego brick has a mass of 1.152 g, according to their calculations it would take a stack of 375,000 bricks to cause the bottom brick to collapse, which represents a stack 3591 m in height.
Private tests have shown several thousand assembly-disassembly cycles before wear-out, although Lego test for fewer cycles.
Non-classical Lego.
Licensed themes.
Over the years, Lego has licensed themes from numerous cartoon and film franchises. These include "", "Batman", "Ben 10", "Bob the Builder", "Cars", "Disney Princess", "Harry Potter", "Indiana Jones", "Lord of the Rings", "Pirates of the Caribbean", "Prince of Persia", "Speed Racer", "Spider-Man", "SpongeBob SquarePants", "Star Wars", "Super Heroes", "Thomas the Tank Engine", "Toy Story", "The Lone Ranger", and "The Hobbit".
Although some of the licensed themes, such as Lego Star Wars and Lego Indiana Jones, have had highly successful sales, Lego has expressed a desire to rely more upon their own characters and classic themes, and less upon licensed themes related to movie releases. Several of the themes have been discontinued.
Robotics sets.
Lego initiated a robotics line of toys called 'Mindstorms' in 1999, and has continued to expand and update this range ever since. The roots of the product originate from a programmable brick developed at the MIT Media Lab, and the name is taken from a paper by Seymour Papert, a computer scientist and educator who developed the educational theory of constructionism, and whose research was at times funded by the Lego Group.
The programmable Lego brick which is at the heart of these robotics sets has undergone several updates and redesigns, with the latest being called the 'EV3' brick, being sold under the brand name of Lego Mindstorms EV3. The set includes sensors that detect touch, light, sound and ultrasonic waves, with several others being sold separately, including an RFID reader.
The intelligent brick can be programmed using official software available for Windows and Mac computers, and is downloaded onto the brick via Bluetooth or a USB cable. There are also several unofficial programs and compatible programming languages that have been made to work with the brick, and many books have been written to support this community.
There are several robotics competitions which use the Lego robotics sets. The earliest is Botball, a national U.S. middle- and high-school competition stemming from the MIT 6.270 Lego robotics tournament. Other Lego robotics competitions include Junior FIRST LEGO League (Jr.FLL) for students ages 6–9, FIRST Lego League (FLL) for students ages 9–16 (age 9–14 in the United States, Canada, and Mexico), and FIRST Tech Challenge (FTC) for high school students. Jr.FLL FLL, and FTC offer real-world engineering challenges to participants. FLL uses Lego-based robots to complete tasks. Jr.FLL participants build models out of Lego elements. FTC uses the NXT Intelligent brick and its pieces along with another building platform called TETRIX. In its 2010 season, there were 16,070 FLL teams in over 55 countries. In its 2010 season, there were 2,147 Jr.FLL teams with 12,882 total student participants in the United States and Canada. The international RoboCup Junior football competition involves extensive use of Lego Mindstorms equipment which is often pushed to its extreme limits.
Clones of Lego.
The definitive shape of the Lego bricks, with the inner tubes, was patented by the Lego Group in 1958. Several competitors have attempted to take advantage of Lego's popularity by producing blocks of similar dimensions, and advertising them as being compatible with Lego bricks.
In 2002, Lego sued the CoCo Toy Company in Beijing for copyright infringement over its "Coko bricks" product. CoCo was ordered to cease manufacture of the products, publish a formal apology and pay damages.
The English company Best-Lock Construction Toys was sued by Lego in German courts in 2004 and 2009; however, the Federal Patent Court of Germany denied Lego trademark protection for the shape of its bricks for the latter case. The Canadian company Mega Bloks were sued by Lego in 2005 for trademark violation, but the Supreme Court of Canada upheld Mega Bloks' rights to sell their product. In 2010, the European Court of Justice ruled that the eight-peg design of the original Lego brick "merely performs a technical function [and] cannot be registered as a trademark."
Related products and services.
The Lego Group has used the Lego toy system to branch out into a number of other areas.
Video games.
Lego has branched out into the video game market since 1997, beginning with games such as "Lego Island" and "Lego Creator". Popular titles include the 1999 game "Lego Racers" and the 2001 game "Lego Racers 2". More recent licensed games include ', ', "", and many more.
' was released in June 2010, and "Lego Rock Band" was released in November 2009. Another game announced was ', including "Indiana Jones and the Kingdom of the Crystal Skull" and total remakes of the other movies' levels, released in 2009. More Lego video games are ', based on the first and second seasons of ' and ' based on the short video clips on the website. An addition to the Lego video game series is ', where you can play all four movies, including '. Released in 2012, ' was a sequel to the popular Lego Batman video game. This game added new characters, including Superman, Wonder Woman, the Flash, etc. and included an open world of Gotham to explore. The next game was "Lego The Lord of the Rings", which follows the events based on the "Lord of the Rings" trilogy. Like "Lego Batman 2", the game featured dialogue spoken through the mouths of the mini-figures taken directly from all three films, plus an entirely new overworld of Middle Earth included. Then came "Lego City Undercover", which let you play as cop Chase McCain and even had a prequel launched for the Nintendo 3DS. After these came the very well-received "Lego Marvel Super Heroes", featuring New York City as the overworld and including Marvel characters from the Avengers, the Fantastic Four, the X-Men, and more.
Lego Digital Designer is an official piece of Lego software for Mac OS X and Windows which allows users to build with Lego bricks on their computers. Users can then publish their creations online on the Lego Design by Me website, or purchase the physical bricks to build them. Lego Digital Designer includes some Lego products which only exist online, including models for the children's television programs "TUGS", "Thomas and Friends" and "Speed Racer".
There is also a game after the Lego Movie.
Official website.
First launched in 1996, the Lego website has developed over the years, and provides many extra services beyond a shop and product catalogue. There are moderated message boards, founded in 2001.
Since then the message boards had a complete overhaul in 2012, and the design was made neat,and contained many tools, and various improvements on design features from 2001.
Before My Lego Network, there was Lego Club Pages, which essentially held the same purpose, although the design lacked complex interaction,all you could do was add friends, it was also pretty basic in what you could do, so the company developed the improvement on Lego Club Pages and called it My Lego Network.
"My Lego Network" is a social networking site that involves items, blueprints, ranks, badges which are earned for completing certain tasks, trading and trophies called masterpieces which allow users to progress to go to the next rank. The website has a built in inbox which allows users to send prewritten messages to one another. The Lego Network includes automated non-player characters within called "Networkers", who are able to do things which normal users cannot do, such as sending custom messages, and selling masterpieces and blueprints. The site also has modules which are set up on the user's page to 'grow' certain things, for showing picture compositions or both. The site includes instructions booklets for all Lego sets dating back to 2002.
Business consultancy.
Since around 2000, the Lego Group has been promoting "Lego Serious Play", a form of business consultancy fostering creative thinking, in which team members build metaphors of their organizational identities and experiences using Lego bricks. Participants work through imaginary scenarios using visual three-dimensional Lego constructions, imaginatively exploring possibilities in a serious form of play.
Theme parks.
Merlin Entertainments operates six Legoland amusement parks, the original in Billund, Denmark, the second in Windsor, England, the third in Günzburg, Germany, the fourth in Carlsbad, California, the fifth in Winter Haven, Florida, and the sixth in Nusajaya, Malaysia. On 13 July 2005, the control of 70% of the Legoland parks was sold for $460 million to the Blackstone Group of New York while the remaining 30% is still held by Lego Group. There are also eight Legoland Discovery Centres, two in Germany, four in the United States, one in Japan and one in the United Kingdom. Two new Legoland Discovery Centres have opened in 2013: one at the Westchester Ridge Hill shopping complex in Yonkers, NY and one at the Vaughan Mills in Vaughan, Ontario, Canada. Another has opened at the Meadowlands complex in East Rutherford, New Jersey in 2014.
Retail stores.
Lego operates 90 retail stores (68 in the United States, 13 in the United Kingdom, 9 in Germany, 6 in Canada, 2 in France, 1 in Austria, 1 in Belgium, and 1 in Denmark). There are also ones at the Downtown Disney shopping complexes at Disneyland and Walt Disney World Resorts as well as in Mall of America in Bloomington, Minnesota. On 24 November 2010, a Lego retail store was opened in Lima, Peru, at Jockey Plaza Shopping Center. The opening of each store is celebrated with weekend-long event in which a Master Model Builder creates, with the help of volunteers—most of whom are children—a larger-than-life Lego statue, which is then displayed at the new store for several weeks. In September 2014, Lego retail store opened in Zagreb, Croatia, being first of its kind in Southeastern Europe.
Children's clothes.
Since 1993 LEGOwear Clothes have been produced and marketed by a Danish company called Kabooki under license from Lego Group. The clothes are for boys and girls from 0–12 years old and the partnership also ties in with other Lego products such as 'Ninjago', 'Hero Factory' and the new 'Friends' theme for girls.
Board games.
Lego Games launched in 2009–2010, and is a series of Lego-themed board games designed by Cephas Howard and Reiner Knizia in which the players usually build the playing board out of Lego bricks and then play with Lego-style players. Examples of the games include "Minotaurus", in which players roll dice to move characters within a brick-build labyrinth, "Creationary", in which players must build something which appears on a card, or "Ramses Pyramid", in which players collect gems and climb up a customizable pyramid. Like many board games, the games use dice. However, in Lego Games, the dice are Lego, with Lego squares with symbols on Lego studs on the dice. The games vary from simple to complex, some are similar to "traditional" board games, while others are completely different.
Films and television.
For a time, Lego turned down approaches from Hollywood to make a feature-length film based on the toy. However, a number of straight-to-DVD computer animated Bionicle and Hero Factory movies were produced, and " was released on DVD in February 2010, a computer-animated film made by Tinseltown Toons.
In June 2013, it was reported that Warner Bros. was developing a feature film adaptation of Lego Ninjago. Brothers Dan Hageman and Kevin Hageman were attached to write the adaptation, while Dan Lin and Roy Lee, along with Phil Lord and Chris Miller, were announced as producers. A computer-generated animated series based on " began in 2011, and another based on "Legends of Chima" began in 2013. A television series of "Lego City" has also been announced.
"The Lego Movie", a feature film based on Lego toys, was released by Warner Bros. in February 2014. It featured Chris Pratt in the lead role, with substantial supporting characters voiced by Will Arnett, Morgan Freeman, Liam Neeson, Alison Brie, Will Ferrell and Nick Offerman. A contest was held for contestants to submit designs for vehicles to be used in the film. After the release of "The Lego Movie", independent Canadian toy retailers reported issues with shortages of Lego products and cited cancellations of Lego pre-orders without warning as a motive to stock compatible, rival products.
Tom Dyckhoff presented a special Culture Show on BBC2 in the UK about the history of Lego.
Books and magazines.
Lego has an ongoing deal with publisher Dorling Kindersley (DK), who are producing a series of illustrated hardback books looking at different aspects of the construction toy. The first was "The Ultimate Lego Book", published in 1999. More recently, in 2009, the same publisher produced "The LEGO Book", which was sold within a slipcase along with "Standing Small: A celebration of 30 years of the LEGO minifigure", a smaller book focused on the minifigure; in 2012 a revised edition was published. Also in 2009, DK also published books on Lego Star Wars ("") and a range of Lego-based sticker books.
Although no longer being published in the United States by Scholastic, books covering events in the BIONICLE storyline are written by Greg Farshtey. They are still being published in Europe by AMEET. BIONICLE comics, also written by Farshtey, are compiled into graphic novels and were released by Papercutz. This series ended in 2009, after nine years.
There is also the Lego Club and Brickmaster magazine, the latter discontinued in 2011.
In popular culture.
Lego's popularity is demonstrated by its wide representation and usage in many forms of cultural works, including books, films and art work. It has even been used in the classroom as a teaching tool. In the USA, Lego Education North America is a joint venture between Pitsco, Inc. and the educational division of the Lego Group.
In 1998, Lego bricks were one of the original inductees into the National Toy Hall of Fame at The Strong in Rochester, New York.
Further reading.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Lego" article dated 2006-02-12, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="18364" url="http://en.wikipedia.org/wiki?curid=18364" title="Li people">
Li people

The Li (黎; pinyin: Lí) or Hlai are an ethnic group, the vast majority of whom live off the southern coast of mainland China on Hainan Island, where they are the largest minority ethnic group. Divided into the five branches of the Qi, Ha, Run, Sai and Meifu, the Li have their own distinctive culture and customs.
Background.
They refer to themselves as the Hlai people, but they are sometimes colloquially known as "Sai" or "Say", and during the Sui Dynasty they were known by the name Liliao. The Li form one of the 56 ethnic groups officially recognized by the People's Republic of China.
Origin and history.
The Li are believed to be descendants of the ancient Yue tribes of China and Vietnam, who settled on the island thousands of years ago. DNA analysis carried out amongst the modern Li population indicate a close relationship with populations in mainland southern China and in particular Guangxi province.
During the Japanese occupation of Hainan (1939–1945), the Li suffered heavily. They are held in high esteem by the Beijing government because they fought on the side of the CPC against Chinese Nationalist rule during the Chinese Civil War. Hainan Li-Miao Autonomous Prefecture was created in 1952 (abolished in 1988).
Language.
They speak their own Hlai language, a member of the Tai–Kadai language family, but most can understand or speak Hainanese and Mandarin.
Culture.
The Li play a traditional wind instrument called "kǒuxiāo" (口箫), and another called "lìlāluó" (利拉罗).

</doc>
<doc id="18365" url="http://en.wikipedia.org/wiki?curid=18365" title="Luminance">
Luminance

Luminance is a photometric measure of the luminous intensity per unit area of light travelling in a given direction. It describes the amount of light that passes through, is emitted or reflected from a particular area, and falls within a given solid angle. The SI unit for luminance is candela per square metre (cd/m2). A non-SI term for the same unit is the "nit". The CGS unit of luminance is the stilb, which is equal to one candela per square centimetre or 10 kcd/m2.
Explanation.
Luminance is often used to characterize emission or reflection from flat, diffuse surfaces. The luminance indicates how much luminous power will be detected by an eye looking at the surface from a particular angle of view. Luminance is thus an indicator of how bright the surface will appear. In this case, the solid angle of interest is the solid angle subtended by the eye's pupil. Luminance is used in the video industry to characterize the brightness of displays. A typical computer display emits between 50 and 300 cd/m2. The sun has luminance of about 1.6×109 cd/m2 at noon.
Luminance is invariant in geometric optics. This means that for an ideal optical system, the luminance at the output is the same as the input luminance. For real, passive, optical systems, the output luminance is "at most" equal to the input. As an example, if you form a demagnified image with a lens, the luminous power is concentrated into a smaller area, meaning that the illuminance is higher at the image. The light at the image plane, however, fills a larger solid angle so the luminance comes out to be the same assuming there is no loss at the lens. The image can never be "brighter" than the source.
Definition.
Luminance is defined by the derivative
where
The expression is written using Leibniz's notation.
Units.
A variety of units have been used for luminance, besides the candela per square metre.
One candela per square metre is equal to:
Health effects.
Retinal damage can occur when the eye is exposed to high luminance. Damage can occur due to local heating of the retina. Photochemical effects can also cause damage, especially at short wavelengths.

</doc>
<doc id="18366" url="http://en.wikipedia.org/wiki?curid=18366" title="Lycos">
Lycos

Lycos, Inc. is a search engine and web portal established in 1994. Lycos also encompasses a network of email, webhosting, social networking, and entertainment websites.
Corporate history.
Lycos is a university spin-off that began as a research project by Michael Loren Mauldin of Carnegie Mellon University's main Pittsburgh campus in 1994. Lycos Inc. was formed with approximately US $2 million ($ million today) in venture capital funding from CMGI. Bob Davis became the CEO and first employee of the new company in 1995, and concentrated on building the company into an advertising-supported web portal. Lycos enjoyed several years of growth during the 1990s and became the most visited online destination in the world in 1999, with a global presence in more than 40 countries.
In 1996, the company completed the fastest IPO from inception to offering in NASDAQ history. In 1997, it became one of the first profitable internet businesses in the world. In 1998, Lycos paid $58 million ($ million today) for Tripod in an attempt to "break into the portal market." Over the course of the next few years, Lycos acquired nearly two dozen internet brands including Gamesville, WhoWhere, Wired Digital (eventually sold to Wired), Quote.com, Angelfire, Matchmaker.com and Raging Bull.
Lycos Europe was a joint venture between Lycos and the Bertelsmann transnational media corporation, but it has always been a distinct corporate entity. Although Lycos Europe remains the largest of Lycos's overseas ventures, several other companies also entered into joint venture agreements including Lycos Canada, Lycos Korea and Lycos Asia.
Near the peak of the internet bubble on May 16, 2000, Lycos announced its intent to be acquired by Terra Networks, the internet arm of the Spanish telecommunications giant Telefónica, for $12.5 billion ($ billion today). The acquisition price represented a return of nearly 3000 times the company's initial venture capital investment and about 20 times its initial public offering valuation. The transaction closed in October 2000 and the merged company was renamed Terra Lycos, although the Lycos brand continued to be used in the United States. Overseas, the company continued to be known as Terra Networks.
On August 2, 2004, Terra announced that it was selling Lycos to Seoul, South Korea-based Daum Communications Corporation for $95.4 million in cash ($ million today), less than 2% of Terra's initial multi-billion dollar investment. In October 2004, the transaction closed for sale of half of the business and the company name was changed back to Lycos Inc. The remaining Terra half was reacquired by Telefónica.
Under new ownership, Lycos began to refocus its strategy. In 2005, the company moved away from a search-centric portal and toward a community destination for broadband entertainment content. With a new management team in place, Lycos also began divesting properties that were not core to its new strategy. In July 2006, Wired News, which had been part of Lycos since the purchase of Wired Digital in 1998, was sold to Condé Nast Publications and re-merged with "Wired Magazine". The Lycos Finance division, best known for Quote.com and RagingBull.com, was sold to FT Interactive Data Corporation in February 2006, while its online dating site, Matchmaker.com, was sold to Date.com. In 2006, Lycos regained ownership of the Lycos trademark from Carnegie Mellon University.
During 2006, Lycos introduced several media services, including Lycos Phone which combined video chat, real-time video on demand, and an MP3 player. In August of the same year, a new version of Lycos Mail was released, which allowed sending and receiving large files, including unlimited file attachment sizes. In November 2006, Lycos began to roll out applications centered around social media, including the first "watch and chat" video application with the launch of its Lycos Cinema platform. In February 2007, Lycos MIX was launched, allowing users to pull video clips from YouTube, Google Video, Yahoo! Video and MySpace Video. Lycos MIX also allowed users to create playlists where other users could add video comments and chat in real-time.
As part of a corporate restructuring to focus on mobile, social networks and location-based services, Daum sold Lycos for $36 million in August 2010 to Ybrant Digital, an internet marketing company based in Hyderabad, India.
In May 2012 Lycos announced the appointment of former employee Rob Balazy as CEO.

</doc>
<doc id="18367" url="http://en.wikipedia.org/wiki?curid=18367" title="Luton Town F.C.">
Luton Town F.C.

Luton Town Football Club 
is an English football club based since 1905 at Kenilworth Road, Luton, Bedfordshire. Founded in 1885, it is nicknamed "the Hatters" and affiliated to the Bedfordshire County Football Association. Its first team contested the fourth tier of English football, Football League Two, during the 2014–15 season. The club's history includes major trophy wins, several financial crises, numerous promotions and relegations, and some spells of sustained success. It was perhaps most prominent between 1982 and 1992, when it was a member of English football's top division, at that time the First Division; the team won its first major honour, the Football League Cup, in 1988.
The club was the first in southern England to turn professional, making payments to players as early as 1890 and turning fully professional a year later. It joined the Football League before the 1897–98 season, left in 1900 because of financial problems, and rejoined in 1920. Luton reached the First Division in 1955–56, and contested a major final for the first time when playing Nottingham Forest in the 1959 FA Cup Final. The team was then relegated from the top division in 1959–60, and demoted twice more in the following five years, playing in the Fourth Division from the 1965–66 season. However, it was promoted back to the top level by 1974–75.
Luton Town's most recent successful period began in 1981–82, when the club won the Second Division, and thereby gained promotion to the First. Luton defeated Arsenal 3–2 in the 1988 Football League Cup Final, and remained in the First Division until relegation at the end of 1991–92. Between 2007 and 2009, financial difficulties caused the club to fall from the second tier of English football to the fifth in successive seasons. The last of these relegations came during the 2008–09 season, when 30 points were docked from Luton's record for various financial irregularities. Luton thereafter spent five seasons in non-League football before winning the Conference Premier in 2013–14, thereby securing promotion back into the Football League.
History.
Luton Town Football Club was formed on 11 April 1885, the product of a merger of the two leading local teams, Luton Town Wanderers and Excelsior. Initially based at Excelsior's Dallow Lane ground, the club began making payments to certain individual players in 1890. The following year, Luton became the first club in southern England to be fully professional. The club was a founder member of the Southern Football League in the 1894–95 season and finished as runners-up in its first two seasons. It then left to help form the United League and came second in that league's inaugural season before joining the Football League (then based mostly in northern and central England) for 1897–98, concurrently moving to a new ground at Dunstable Road. The club continued to enter a team to the United League for two more seasons, and won the title in 1897–98. Poor attendance, high wages, and the high travel and accommodation costs that resulted from Luton's distance from the northern heartlands of the Football League crippled the club financially, and made it too expensive to compete in that league. A return to the Southern League was therefore arranged for the 1900–01 season.
Eight years after arriving at Dunstable Road, Luton moved again, settling at their current ground, Kenilworth Road, in 1905. Captain and left winger Bob Hawkes became Luton's first international player when he was picked to play for England against Ireland on 16 February 1907. A poor 1911–12 season saw Luton relegated to the Southern League's Second Division; the club won promotion back two years later. After the First World War broke out, Luton took part in The London Combination during 1915–16, and afterwards filled each season with friendly matches. A key player of the period was Ernie Simms, a forward. Simms was invalided back to England after being wounded on the Italian front, but recovered enough to regain his place in the Luton team and scored 40 goals during the 1916–17 season.
The Luton side first played in the white and black colours which it has retained for much of its history during the 1920–21 season, when the club rejoined the Football League; the players had previous worn an assortment of colour combinations, most permanently sky blue shirts with white shorts and navy socks. Such was the quality of Luton's team at this time that despite playing in the third tier, a fixture between Ireland and England at Windsor Park on 22 October 1921 saw three Luton players on the pitch—Louis Bookman and Allan Mathieson for Ireland, and the club's top goalscorer, Simms, for England. However, after Luton finished fourth in the division, the squad was broken up as Simms, Bookman and Mathieson joined South Shields, Port Vale and Exeter City respectively. Luton stayed in the Third Division South until 1936–37, when the team finished top and won promotion to the Second Division, at that time the second tier of English football. During the promotion season, striker Joe Payne scored 55 goals in 39 games; during the previous season he had scored 10 in one match against Bristol Rovers, which remains a Football League record today.
During the early 1950s, one of Luton's greatest sides emerged under manager Dally Duncan. The team included Gordon Turner, who went on to become Luton's all-time top goalscorer, Bob Morton, who holds the record for the most club appearances, and Syd Owen, an England international. During this period, Luton sides also featured two England international goalkeepers, Ron Baynham and Bernard Streten, as well as Irish internationals Seamus Dunne, Tom Aherne and George Cummins. This team reached the top flight for the first time in 1955–56, after finishing the season in second place behind Birmingham City on goal average. A few years of success followed, including an FA Cup Final appearance against Nottingham Forest in 1958–59; at the end of the season, Owen was voted FWA Footballer of the Year. However, the club was relegated the following season, and, by 1964–65, was playing in the fourth tier.
In yo-yo club fashion, Luton were to return. A team including Bruce Rioch, John Moore and Graham French won the Fourth Division championship in 1967–68 under the leadership of former player Allan Brown; two years later Malcolm Macdonald's goals helped them to another promotion, while comedian Eric Morecambe became a director of the club. Luton Town won promotion back to the First Division in 1973–74, but were relegated the following season by a solitary point. Former Luton player David Pleat was made manager in 1978, and by 1982–83 the team was back in the top flight. The team which Pleat assembled at Kenilworth Road was notable at the time for the number of black players it included; during an era when many English squads were almost entirely white, Luton often fielded a mostly black team. Talented players such as Ricky Hill, Brian Stein and Emeka Nwajiobi made key contributions to the club's success during this period, causing it to accrue "a richer history of black stars than any in the country", in the words of journalist Gavin Willacy.
On the last day of the 1982–83 season, the club's first back in the top tier, it narrowly escaped relegation: playing Manchester City at Maine Road, Luton needed to win to stay up, while City could escape with a draw. A late winner by Yugoslavian substitute Raddy Antić saved the team and prompted Pleat to dance across the pitch performing a "jig of joy", an image that has become iconic. The club achieved its highest ever league position, seventh, under John Moore in 1986–87, and, managed by Ray Harford, won the Football League Cup a year later with a 3–2 win over Arsenal. With ten minutes left on the clock and Arsenal 2–1 ahead, a penalty save from stand-in goalkeeper Andy Dibble sparked a late Luton rally: Danny Wilson equalised, before Brian Stein scored the winner with the last kick of the match. The club reached the League Cup Final once more in 1988–89, but lost 3–1 to Nottingham Forest.
The club was relegated from the top division at the end of the 1991–92 season, and sank to the third tier four years later. Luton stayed in the third-tier Second Division until relegation at the end of the 2000–01 season. Under the management of Joe Kinnear, who had arrived halfway through the previous season, the team won promotion from the fourth tier at the first attempt. "Controversial" owner John Gurney unsettled the club in 2003, terminating Kinnear's contract on his arrival in May; Gurney replaced Kinnear with Mike Newell before leaving Luton as the club entered administration. Newell's team finished as champions of the third-tier Football League One in 2004–05.
While Newell's place was taken by first Kevin Blackwell and later former player Mick Harford, the team was then relegated twice in a row, starting in 2006–07, and spent the latter part of the 2007–08 season in administration, thus incurring a ten-point deduction from that season's total. The club then had a total of 30 points docked from its 2008–09 record by the Football Association and the Football League for financial irregularities dating back several years. These deductions proved to be too large an obstacle to overcome, but Luton came from behind in the final of the Football League Trophy to win the competition for the first time.
Relegation meant that 2009–10 saw Luton playing in the Conference Premier, a competition which the club had never before participated in. The club unsuccessfully contested the promotion play-offs three times in four seasons during their time as a non-League club, employing five different managers. In the 2012–13 FA Cup fourth round, Luton won their away tie against Premier League club Norwich City 1–0 and, in doing so, became the first non-League team to beat a side from England's top division since 1989. In the 2013–14 season, under the management of John Still, Luton won the Conference Premier championship with three games to spare, and thereby secured a return to the Football League from 2014–15.
Club identity.
Luton first wore white and black between 1920 and 1973.
The club's nickname, "the Hatters", reflects Luton's historical connection with the hat making trade, which has been prominent there since the 1600s. The nickname was originally a variant on the now rarely seen Straw-plaiters. Supporters of the club are also called Hatters.
The club is associated with two very different colour schemes—white and black (first permanently adopted in 1920), and orange, navy and white (first used in 1973, and worn by the team as of the 2014–15 season). Luton mainly wore a combination of light blue and white before 1920, when white shirts and black shorts were first adopted. These colours were retained for over half a century, with the colour of the socks varying between white and black, until Luton changed to orange, navy and white at the start of the 1973–74 season. Luton began playing in white shirts, shorts and socks in 1979, with the orange and navy motif reduced to trim; navy shorts were adopted in 1984. This palette was retained until the 1999–2000 season, when the team played in orange shirts and blue shorts. From 2000 to 2008, Luton returned to white shirts and black shorts; orange was included as trim until 2007. The white, navy and orange palette favoured in the 1980s was brought back in 2008, following the results of a club poll, but a year later the colours were changed yet again, this time to a predominantly orange strip with white shorts. Navy shorts were readopted in 2011. Luton are wearing orange shirts, navy shorts and white socks during the 2014–15 season.
Luton Town have traditionally used the town's crest as its own in a manner similar to many other teams. The club's first badge was a white eight-pointed star, which was emblazoned across the team's shirts (then a deep cochineal red) in 1892. Four years later a crest comprising the club's initials intertwined was briefly adopted. The shirts were thereafter plain until 1933, when Luton first adopted a badge depicting a straw boater, which appeared on Luton shirts. The letters "LTFC" were added in 1935, and this basic design remained until 1947. The club then played without a badge until 1970, when the club began to wear the town crest regularly, having first done so in the 1959 FA Cup Final. In 1973, concurrently with the club's switch to the orange kit, a new badge was introduced featuring the new colours. The new emblem depicted a stylised orange football, bearing the letters "Lt", surrounded by the club's name in navy blue text.
In 1987, the club switched back to a derivative of the town emblem, with the shield portion of the heraldic crest becoming the team's badge; the only similarity with the previous design was the inclusion of the club name around the shield in navy blue. The "rainbow" badge, introduced in 1994, featured the town crest below an orange and blue bow which curved around to meet two footballs, positioned on either side of the shield. The design was completed by a continuation of the orange and blue lines below the shield, with the club name across them in white. This badge was used until 2005, when a replacement very similar to the 1987 version was adopted, featuring black text rather than blue and a straw boater in place of the outstretched arm depicted in the older design. The club's founding year, 1885, was added in 2008 to complete the design. The badge was altered once more during the 2009–10 pre-season, with the red of the town crest being replaced with orange to better reflect the club colours.
The first sponsor to appear on a Luton Town shirt was Tricentrol, a local motor company based in Dunstable, who sponsored the club from March 1980 to 1982; the deal was worth £50,000. Subsequent sponsors have been Bedford Trucks (1982 to 1990), Vauxhall (1990 to 1991), Universal Salvage Auctions (1991 to 1999), SKF (1999 to 2003), Travel Extras (2003 to 2005), Electrolux (2005 to 2008) and Carbrini Sportswear (2008 to 2009). NICEIC and EasyJet agreed sponsorship deals before the 2009–10 season, for one and two years respectively; EasyJet renewed their sponsorship before the 2011–12 season, and then again for both the 2013–14 and 2014–15 seasons.
The club released the song "Hatters, Hatters", a collaboration between the Luton team and the Bedfordshire-based musical comedy group the Barron Knights, in 1974. Eight years later another song featuring vocals by the Luton players, "We're Luton Town", was released to celebrate the club's promotion to the First Division.
Stadiums.
Luton Town's first ground was at Dallow Lane, the former ground of Excelsior. The ground was next to the Dunstable to Luton train line, and players regularly claimed to have trouble seeing the ball because of smoke from the trains. A damaging financial loss during 1896–97 forced Luton to sell the stadium to stay afloat, and as a result the club moved across the tracks to a stadium between the railway and Dunstable Road. The Dunstable Road ground was opened by Herbrand Russell, 11th Duke of Bedford, who also donated £50 towards the £800 building costs. When the site was sold for housing in 1905, the club was forced to move again at short notice, to its present Kenilworth Road site, in time for the start of the 1905–06 season.
The 10,356 capacity all-seater stadium is in the Bury Park area of Luton, and named after the road that runs along one end of it, although the official address of the club is 1 Maple Road. Opposite the eponymous Kenilworth Stand is the Oak Road End, which has evolved from a stand first used exclusively by Luton supporters, then by away supporters, but is now used by both except in times of high demand from away clubs. The Main Stand is flanked by the David Preece Stand, and opposite them stands a row of executive boxes. These boxes replaced the Bobbers Stand in 1986, as the club sought to maximise income.
The original Main Stand burnt down in 1921, and was replaced by the current stand before the 1922–23 campaign. The ground underwent extensive redevelopment during the 1930s, and the capacity by the start of the Second World War was 30,000. Floodlights were installed before the 1953–54 season, but it was 20 years before any further modernisation was carried out. In 1973 the Bobbers Stand became all-seated, and in 1985 the grass pitch was replaced with an artificial playing surface; it quickly became unpopular and was derided as "the plastic pitch".
A serious incident involving hooliganism before, during, and after a match against Millwall in 1985 caused the club's then chairman, Conservative MP David Evans, to introduce a scheme effective from the start of 1986–87 banning all visiting supporters from the ground, and requiring home fans to carry identity cards when attending matches. Conversion to an all-seater ground also began in 1986. Away fans returned for 1990–91, and grass a year later. The David Preece Stand was erected in 1991, and the conversion of the Kenilworth Stand to an all-seater was completed in 2005.
New stadium.
The club first stated its intent to leave Kenilworth Road in 1955. Even then the ground was small compared to rival stadiums, and its location made significant redevelopment difficult. The team has since made several attempts to relocate. Leaving Luton for the nearby new town of Milton Keynes was unsuccessfully proposed several times, most notably in the 1980s. The club sold Kenilworth Road to Luton Council in 1989, and has since leased it. A planning application for a new ground, the "Kohlerdome" proposed by chairman David Kohler in 1995, was turned down by the Secretary of State in 1998, and Kohler left soon after.
In 2007, the club's then-owners proposed a controversial plan to relocate to a site near Junction 12 of the M1 motorway, near Harlington and Toddington. A planning application was made on the club's behalf by former chairman Cliff Bassett, but the application was withdrawn almost immediately following the club's takeover in 2008. , the club is undertaking an independent feasibility study to determine a viable location to move to, although redeveloping Kenilworth Road has not been ruled out. The club entered talks to buy Kenilworth Road back from Luton Borough Council in October 2012, but by mid-2015 these plans had been dropped in favour of a move to a new location. Managing Director Gary Sweet confirmed that the club was in a position to "buy land, secure the best possible professional advice... and to see the [planning] application process through to the receipt of consent.”
Supporters and rivalries.
During the 2014–15 season, Luton Town had an average home league attendance of 8,702 – the second highest in League Two behind only Portsmouth.[A] In the 2013–14 season, when the club were in the Conference National, the club had significantly higher support than the other clubs in its league, with an average home attendance of 7,387; more than twice compared to the second highest of 3,568.[B] Average attendances at Kenilworth Road fell with the installation of seats and the club's reduction in stature, dropping from 13,452 in 1982–83 to their 2014–15 level—a slump of 35% over 32 years. The club has two major supporters' groups—the official Luton Town Supporters Club and the breakaway Loyal Luton Supporters Club. There also exists a supporters' trust, affiliated with both clubs, called Trust in Luton, an industrial and provident society which owns shares in the club and elects a representative to the club's board. Trust in Luton has, since March 2014, held the legal right to veto any changes to the club's identity, including name, nickname, colours, club crest and mascot.
Luton Town supporters maintain a bitter rivalry with Hertfordshire-based Watford. Watford have remained the higher ranked team at the end of every season since 1997. However, overall Luton still hold the superior record in the fixture between the two clubs; out of 118 competitive first-class matches there have been 53 Luton victories and 36 for Watford, with 29 draws. A survey taken in 2003 showed that there was also animosity between Luton Town fans and those of west London club Queens Park Rangers. During Luton's time in the Conference, a rivalry developed between Luton fans and those of York City following regular meetings in important matches and crowd trouble.
The club produces an official match programme for home games, "Talk of the Town". A character known as Happy Harry, a smiling man wearing a straw boater, serves as the team's mascot and appears on the Kenilworth Road pitch before matches. In December 2014, after the seafront statue of Eric Morecambe in his birthplace Morecambe was restored, Luton and Morecambe F.C. jointly announced that the winners of future Luton–Morecambe fixtures would be awarded the "Eric Morecambe Trophy".
Records and statistics.
The record for the most appearances for Luton is held by Bob Morton, who turned out for Luton 562 times in all competitions. Morton also holds the record for the most Football League appearances for the club, with 495. Fred Hawkes holds the record for the most league appearances for Luton, having played in 509 league matches. Six players, Gordon Turner, Andy Rennie, Brian Stein, Ernie Simms, Herbert Moody and Steve Howard, have scored more than 100 goals for Luton.
The first player to be capped while playing for Luton was left winger Robert Hawkes, who took to the field for England against Ireland at Goodison Park on 16 February 1907. The most capped player is Mal Donaghy, who earned 58 Northern Ireland caps while at the club. The first player to score in an international match was Joe Payne, who scored twice in his only game for England against Finland on 20 May 1937. Payne also holds the Football League record for the most goals in a game—he hit 10 past Bristol Rovers on 13 April 1936.
The club's largest wins have been a 15–0 victory over Great Yarmouth Town on 21 November 1914 in the FA Cup and a 12–0 win over Bristol Rovers in the Third Division South on 13 April 1936. Luton's heaviest loss was a 9–0 defeat against Small Heath in the Second Division on 12 November 1898.
Luton's highest home attendances are 30,069 against Blackpool in the FA Cup on 4 March 1959 and 27,911 against Wolverhampton Wanderers in the First Division on 5 November 1955.
The highest transfer fee received for a Luton Town player is the £3 million West Bromwich Albion paid for Curtis Davies on 31 August 2005. The most expensive player Luton Town have ever bought was Lars Elstrup, who cost £850,000 from Odense Boldklub on 21 August 1989.
Players.
Current squad.
The club operates a Development Squad, made up of contracted senior players, youth team scholars and trialists, which plays in the newly formed Southern Division of The Central League. The club also fields an under-18 team in the Football League Youth Alliance South East Conference. Luton's youth set-up consists of ten Soccer Centres across Bedfordshire and North Hertfordshire, two Centres of Excellence (one in Luton, one in Dunstable), and an Academy in Baldock that caters for players in the under-9 to under-16 age groups.
Footnotes.
</dl>
References.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Luton Town F.C." article dated 2009-11-07, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="18369" url="http://en.wikipedia.org/wiki?curid=18369" title="Lunar calendar">
Lunar calendar

A lunar calendar is a calendar that is based on cycles of the lunar phases. Because there are slightly more than twelve lunations (synodic months) in a solar year, the period of 12 lunar months (354.37 days) is sometimes referred to as a lunar year.
A common purely lunar calendar is the Islamic calendar or Hijri Qamari calendar. A feature of the Islamic calendar is that a year is always 12 months, so the months are not linked with the seasons and drift each solar year by 11 to 12 days. It comes back to the position it had in relation to the solar year approximately every 33 Islamic years. It is used mainly for religious purposes, but in Saudi Arabia it is the official calendar. Other lunar calendars often include extra months added occasionally to synchronize it with the solar calendar.
The oldest known lunar calendar was found in Scotland; it dates back to around 8000 BC.
Lunisolar calendars.
Most calendars referred to as "lunar" calendars are in fact lunisolar calendars. That is, months reflect the lunar cycle, but then intercalary months (e.g. "second Adar" in the Hebrew calendar) are added to bring the calendar year into synchronisation with the solar year. Some examples are the Chinese and Hindu calendars. Some other calendar systems used in antiquity were also lunisolar.
All these calendars have a variable number of months in a year. The reason for this is that a solar year is not equal in length to an exact number of lunations, so without the addition of intercalary months the seasons would drift each year. To synchronise the year, a thirteen-month year is needed every two or three years.
Some lunar calendars are calibrated by annual natural events which are affected by lunar cycles as well as the solar cycle. An example of this is the lunar calendar of the Banks Islands, which includes three months in which the edible palolo worm mass on the beaches. These events occur at the last quarter of the lunar month, as the reproductive cycle of the palolos is synchronised with the moon.
Even though the Gregorian calendar is in common and legal use, lunar and lunisolar calendars serve to determine traditional holidays in many parts of the world, including India, Pakistan, China, Korea, Japan, Vietnam and Nepal. Such holidays include Ramadan, Diwali, Chinese New Year, Tết (Vietnamese New Year), Mid-Autumn Festival/Chuseok and Nepal Sambat and Mongolian New Year as called Tsagaan sar.
Start of the lunar month.
Lunar and lunisolar calendars differ as to which day is the first day of the month. In some lunisolar calendars, such as the Chinese calendar, the first day of a month is the day when an astronomical new moon occurs in a particular time zone. In others, such as some Hindu calendars, each month begins on the day after the full moon or the new moon. Others were based in the past on the first sighting of a lunar crescent, such as the Hebrew calendar.
Length of the lunar month.
The length of each lunar cycle varies slightly from the average value. In addition, observations are subject to uncertainty and weather conditions. Thus to avoid uncertainty about the calendar, there have been attempts to create fixed arithmetical rules to determine the start of each calendar month.
The average length of the synodic month is 29.530589 days. Thus it is convenient if months are in general alternately 29 and 30 days (sometimes termed respectively hollow and full). The distribution of hollow and full months can be determined using continued fractions, and examining successive approximations for the length of the month in terms of fractions of a day. In the list below, after the number of days listed in the numerator, an integer number of months as listed in the denominator have been completed:
These fractions can be used to construct a lunar calendar, or in combination with a solar calendar to produce a lunisolar calendar. A 49-month cycle was proposed as the basis of an alternative Easter computation by Isaac Newton around 1700. The tabular Islamic calendar's 360-month cycle is equivalent to 24×15 months minus a correction of one day.

</doc>
<doc id="18372" url="http://en.wikipedia.org/wiki?curid=18372" title="Mount Lykaion">
Mount Lykaion

Mount Lykaion (Ancient Greek: Λύκαιον ὄρος, "Lýkaion Óros"; Latin: "Mons Lycaeus") is a mountain in Arcadia, Greece. Lykaion has two peaks: "Stefani" to the north and St. Ilias (Άγιος Ηλίας, "Agios Īlías") to the south where the altar of Zeus is located. The northern peak is higher (1421 m) than the southern (1382 m). Mount Lykaion was sacred to Zeus Lykaios, who was said to have been born and brought up on it, and was the home of Pelasgus and his son Lycaon, who were said to have founded the ritual of Zeus practiced on its summit. This seems to have involved a human sacrifice and a feast in which the man who received the portion of a human victim was changed to a wolf, as Lycaon had been after sacrificing a child. The altar of Zeus consists of a great mound of ashes with a retaining wall. It was said that no shadows fell within the precincts and that any who entered it died within the year. The sanctuary of Zeus played host to athletic games held every four years, the "Lykaia".
Archaeological excavations were first carried out in 1897 by K. Kontopoulos for the Greek Archaeological Service, followed by K. Kourouniotes between 1902 and 1909.
The Mt. Lykaion Excavation and Survey Project, a joint effort of the University of Pennsylvania and the University of Arizona began work at the site in 2004, with the aim of continuing the topographical survey begun in 1996 and carrying out a full topographical and architectural analysis not only of the altar and "temenos", but of the nearby valley where the Lykaian Games were held. The detailed digital records and drawings of every architectural stone block. To date, a complete map of the area has been made, including not only the Ash Altar and "temenos", but also two fountains, including the Hagno fountain mentioned by Pausanias, the hippodrome, the stadium, a building that was probably a bathhouse, the "xenon" (hotel), a stoa, several rows of seats, and a group of statue bases. 
Many of these buildings seem to have been planned in relation to each other: the baths at the northern end of the hippodrome are on the same alignment as it is, and the stoa, the "xenon", the lower fountain, and the rows of seats all appear to have been built in an intentionally similar alignment. Just to the north of the stoa four rows of seats were excavated, with the remains of a group of stelae and statue bases nearby. These would have bordered the hippodrome's southern edge, and correspond to an earlier excavated row of seats on the south-eastern edge of the racetrack. The majority of the spectators of events in the hippodrome, however, would have sat on the surrounding hills.
Lykaion in the Literary Record.
Mt. Lykaion, its religious significance, and its quadrennial athletic games appear with some frequency in the ancient literary sources. The 2nd-century Greek geographer Pausanias provides the greatest amount of information in the eighth book of his "Description of Greece", where he discusses Lykaion’s mythological, historical, and physical characteristics in detail. More isolated references occur, however, in sources ranging from Plato to Virgil.
Legendary Period.
Pausanias states that the Arcadians claimed Cretea atop Mt. Lykaion as the birthplace of Zeus, although tradition had handed down at least two other locations for Zeus’ birth.
Lycaon, son of Pelasgus, the mythical founder of the Greek race, is said to have instituted the worship of Zeus at Mt. Lykaion, giving the god the epithet Lykaios and establishing games in his honor. The "Bibliotheca", a Roman-era mythological compendium, adds the story that Lycaon attempted to test Zeus’ omniscience by tricking him into eating a sacrifice mixed with human flesh. In punishment, Zeus slew Lycaon and his fifty sons. Other sources, including the Roman poet Ovid, claim instead that Lycaon’s punishment was transformation into a wolf, an early example of lycanthropy.
Historical Events.
According to Pausanias and the Greek historian Polybius, an inscribed pillar ("stele") was erected near the altar of Zeus on Mt. Lykaion during the Second Messenian War, a revolt against the Spartans. The inscription supposedly commemorated the execution of Aristocrates of Arcadia, who had betrayed the Messenian hero Aristomenes at the battle of the Great Trench.
Thucydides, a Greek historian of the Peloponnesian War, writes that the Spartan king Pleistoanax lived on Mt. Lykaion while in exile from the mid-440s BC until 427, where he built a house straddling the sacred region ("temenos") of Zeus to avoid further persecution.
In his "Stratagems", the 2nd-century Macedonian rhetorician Polyaenus describes a battle between the Spartans and Demetrius of Macedon in 294 BC. Mt. Lykaion extended between the camps of both sides, causing some consternation among the Macedonians due to their unfamiliarity with the terrain. Nevertheless, Demetrius’ forces won the battle with relative ease.
Polybius and Plutarch, a Greek author writing under the Roman empire, cite a battle at Mt. Lykaion in 227 BC between the Achaean League under Aratus and the Spartans under Cleomenes III. Although the details are vague, both authors make it clear that the Achaeans were defeated and that Aratus was killed shortly thereafter.
Religious Worship at Lykaion.
Lykaian Pan.
Mt. Lykaion was an important site of religious worship in ancient Greece. Pausanias describes a sanctuary of Pan surrounded by a grove of trees. At the sanctuary were bases of statues, which by Pausanias’ time had been deprived of the statues themselves, as well as a hippodrome, where the athletic games had once been held. References to Lykaian Pan are especially abundant in Latin poetry, as for instance in Virgil’s epic, the "Aeneid": “"Lupercal / Parrhasio dictum Panos de more Lycaei",” “...the Lupercal, named after the Parrhasian worship of Lykaian Pan,” and in Horace’s Odes: “"Velox amoenum saepe Lucretilem / mutat Lycaeo Faunus",” “Often swift Faunus [Pan] exchanges Lykaion for pleasant Lucretilis.”
Zeus Lykaios.
Pausanias records the presence of a mound of earth on the highest point of the mountain, an altar to Zeus Lykaios. He describes two pillars near the altar which had once been topped by golden eagles. Although Pausanias alludes to secret sacrifices which took place on this altar, he explains that he was reluctant to inquire into these rites due to their extreme antiquity. Pausanias also discusses the "temenos" of Zeus, a sacred precinct which humans were forbidden to enter. He notes the common belief that any person entering the "temenos" would die within a year, along with the legend that all creatures, human and animal alike, cast no shadow while inside the sacred area.
The Lykaian Games.
The athletic competitions at Lykaion, held every four years, receive occasional mention in the literary record. Authors are in disagreement as to when exactly the games were first instituted: Aristotle is said to have ranked the Lykaion games fourth in order of institution after the Eleusinia, the Panathenaia, and the Argive games, while Pausanias argues for the Lykaian competition’s priority to the Panathenaia. Pliny the Elder, an imperial Roman polymath, states that the games at Lykaion were the first to introduce gymnastic competition. The ancient Greek lyric poet Pindar records the victories of several athletes in his "Victory Odes", and two inscribed "stelae" recently excavated from the Lykaian hippodrome provide information about the events, participants, and winners at the games.
Modern Study of Lykaion.
After 1832, when Greece had gained independence from the Ottoman Empire, European travelers and scholars began to systematically tour Sparta and the Peloponnnese. Ernst Curtius, Charles Beulé, and Guillaume Blouet published scholarly studies of the area, and discussions of the region appeared in German and British travelogues as well. Many of these writers used Pausanias as their guide to the geography and sights of the region, but were also concerned to correlate modern Greek place-names with ancient evidence.
Beulé described the hippodrome and surrounding area, including large stones that he assumed formed had formed the seats of the judges and magistrates, and the remains of a building he called a temple to Pan, but which probably corresponds to the stoa of the modern excavations. The German writer Ross described the bathhouse and its ancient but still-visible cisterns, which site he noted the locals called the Skaphidia.
Mt. Lykaion was initially excavated by the Greek Archaeological Service, first in 1897 by archaeologist K. Kontopoulos and again in 1902 by K. Kourouniotes. Kontopoulos dug several trial trenches near the hippodrome and the altar. Kourouniotes’s excavations of the altar and surrounding area (the "temenos") were particularly informative; he learned that the altar consisted of a raised mound of blackened earth as described by Pausanias. Excavation of the earth of the altar yielded burnt stones, small animal (cow and pig) bones, tiny pottery fragments, iron knives, clay figures, coins from Aegina, a clay figure of a bird, and two small bronze tripods. Further trenches dug in the "temenos" produced several bronze figures, some iron objects, and roof tiles. In 1909 Kourouniotes excavated an area at the east of the mountain and beneath the summit, the site of the hippodrome, stadium, and bathhouse.
Since Kourouniotes’s excavation, anthropologists and scholars of Arcadian religion have studied the site in terms of its development as a sanctuary, but there was no further systematic or scientific investigation until 1996, when Dr. David Gilman Romano of the University of Pennsylvania conducted a topographical and architectural survey of the site. Romano continued his work with the Mt. Lykaion Excavation and Survey Project under the auspices of the University of Pennsylvania and the University of Arizona. A preliminary planning phase of cleaning and surveying took place in 2004 and 2005, and was followed by a five-year excavation program beginning in June 2006. A two-year period during which the findings will be studied is scheduled to begin in the summer of 2011.
The Hippodrome.
The hippodrome at Mount Lykaion, located in a valley below and to the north of the altar, is the only extant hippodrome from Greek antiquity, and is therefore crucial to our understanding of Greek athletic festivals. The hippodrome was constructed on roughly a north-south orientation with a retaining wall of about 140 meters along the eastern side curving around the northern end. Modern excavations have discovered portions tapering column drums that may belonged to the turning posts at either end of the race-course, from whose location it appears that the hippodrome could have had a length of 320 meters and a width of 140. A bath building is being excavated about 35 meters to the northeast of the hippodrome; a large portion of it appears to have been dedicated to a cistern, and large stone basins from the middle of the structure have been uncovered
The Lykaian hippodrome is further unique in apparently having encompassed the stadium racecourse. The early 20th century excavator of Lykaion, Kouriouniotis discovered stone blocks in the middle of the hippodrome that would have formed the starting line of the stadium. The topological survey of 1996 confirmed 6 starting line blocks, four of which were grouped together and were thus possibly found near their original orientation and position. From this, archaeologist David Romano speculated that a stadium racecourse of 170–180 meters would have been enclosed within the hippodrome. The apparently double-use of the space is particularly interesting because inscriptional evidence concerning the Lykaian Games of the 4th century BCE indicates that horse and foot-races were held during the same festivals, and possibly on the same day.
Two inscriptions were uncovered in the excavations of Kouriouniotis that give the names of winning athletes in the various contests of the Lykaian Games that were held every four years between 320 and 304 BCE. These contests included footraces for men and for boys, various chariot races with teams of adult and juvenile horses, boxing, wrestling, and a pentathlon.
The Ash Altar.
A circular altar of blackened earth about 1.5 meters in height and 30 meters in diameter seems to date from before the migration of Indo-European peoples into the area. The excavations of Kourouniotes in 1903 of the altar and its nearby "temenos" determined definite cult activity at the Lykaion altar from the late 7th century b.c.e, including animals bones, miniature tripods knives, and statuettes of Zeus holding an eagle and a lightning bolt. These objects were primarily found in the "temenos". The earth-altar may correspond to a Linear B mention of an "open-fire altar"; Linear B (14th-13th centuries BCE) inscriptions also give the first mentions of offerings to Zeus and of the sacred precinct (temenos) near the altar, such as has been excavated at Lykaion.
Excavation in 2007 revealed pottery fragments and signs of activity in the ash altar believed to have been used as early as 3000 BCE. Nearby Olympia (only 22 miles away) has a similar ash altar, and both settlements held ancient athletic games. The extremely early date of activity at Lykaion could suggest that these customs originated there. Stratigraphic analysis from the most recent excavations showed prehistoric human activity at the altar site, which seems to have been in continuous use from the Late Neolithic period through to the Hellenistic era. A number of drinking vessels and bones of sheep and goats from the Late Helladic period indicates that the altar was the site of Mycenean drinking and feasting rituals, probably in honor of Zeus. An especially interesting discovery was a seal ring from the Late Minoan period (1500-1400 BCE), which could indicate some interaction between Mt. Lykaion and Crete, both of which are given as the birthplace of Zeus by ancient sources.

</doc>
<doc id="18374" url="http://en.wikipedia.org/wiki?curid=18374" title="Lake Eyre">
Lake Eyre

Lake Eyre ( ), officially known as Kati Thanda–Lake Eyre, is the lowest natural point in Australia, at approximately 15 m below sea level (AHD), and, on the rare occasions that it fills, the largest lake in Australia and 18th largest in the world.
The temporary, shallow lake is the depocenter of the vast Lake Eyre basin and is found in South Australia, some 700 km north of Adelaide. The lake was named in honour of Edward John Eyre, who was the first European to see it, in 1840. The lake's official name was changed in December 2012 to combine the name "Lake Eyre" with the indigenous name, Kati Thanda. Native title over the lake and surrounding region is held by the Arabana people.
Geography.
Kati Thanda–Lake Eyre is located in the deserts of central Australia, in northern South Australia. The Lake Eyre Basin is a large endorheic system surrounding the lakebed, the lowest part of which is filled with the characteristic salt pan caused by the seasonal expansion and subsequent evaporation of the trapped waters. Even in the dry season there is usually some water remaining in Kati Thanda–Lake Eyre, normally collecting in over 200 smaller sub-lakes within its margins. The lake was formed by aeolian processes after tectonic upwarping occurred to the south subsequent to the end of the Pleistocene epoch.
During the rainy season the rivers from the north-east part of the Lake Eyre Basin (in outback (south-west and central) Queensland) flow towards the lake through the Channel Country. The amount of water from the monsoon determines whether water will reach the lake and if it does, how deep the lake will get. The average rainfall in the area of the lake is 100 to 150 mm per year.
The −15 m altitude usually attributed to Kati Thanda–Lake Eyre refers to the deepest parts of the lake floor, in Belt Bay and the Madigan Gulf. The shoreline lies at −9 m. The lake is the area of maximum deposition of sediment in the Lake Eyre Basin.
Lake Eyre is divided into two sections which are joined by the Goyder Channel. These are known as Lake Eyre North, which is 144 km in length and 65 km wide and Lake Eyre South, which measures 65 by 24 km. The salt crusts are thickest (up to 50 cm) in the southern Belt Bay, Jackboot Bay and Madigan Gulf sub-basins of Lake Eyre North.
Since 1883 proposals have been forwarded to flood Lake Eyre with seawater brought to the basin via canal or pipeline. The purpose was, in part, to increase evaporation and thereby increase rainfall in the region downwind of an enlarged Lake Eyre. Due to the basin's low elevation below sea level and the region's high annual evaporation rate (between 2,500 and 3,500mm) such schemes have generally been considered impractical as it is likely that accumulation of salt deposits would rapidly block the engineered channel.
Floods.
Typically a 1.5 m flood occurs every three years, a 4 m flood every decade, and a fill or near fill a few times a century. The water in the lake soon evaporates with a minor or medium flood drying by the end of the following summer. Most of the water entering the lakes arrives via Warburton River.
In strong La Niña years the lake can fill. Since 1885 this has occurred in 1886–1887, 1889–1890, 1916–1917, 1950, 1955, 1974–1977, and 1999-2001, with the highest flood of 6 m in 1974. Local rain can also fill Kati Thanda–Lake Eyre to 3–4 m (10–13 ft) as occurred in 1984 and 1989. Torrential rain in January 2007 took about six weeks to reach the lake but put only a small amount of water into it.
When recently flooded the lake is almost fresh and native freshwater fish, including bony bream ("Nematolosa erebi"), the Lake Eyre Basin sub-species of golden perch ("Macquaria ambigua") and various small hardyhead species ("Craterocephalus" spp.) can survive in it. The salinity increases as the 450 mm salt crust dissolves over a period of six months resulting in a massive fish kill. When over 4 m deep the lake is no more salty than the sea, but salinity increases as the water evaporates, with saturation occurring at about a 500 mm depth. The Lake takes on a pink hue when saturated due to the presence of beta-carotene pigment caused by the algae "Dunaliella salina".
2009.
The 2009 Lake Eyre flood peaked at 1.5 m deep in late May which is a quarter of its maximum recorded depth of 6 m. 9 km3 of water crossed the Queensland–South Australian border with most of it coming from massive floods in the Georgina River. However, owing to the very low rainfall in the lower reaches of these rivers (contrasting with heavy rainfall in the upper catchments) the greater proportion soaked into the desert or evaporated en route to the lake leaving less than 4 km3 (1 cu mi) in the lake which covered an area of 800 km2 or 12% of the lake. As the flood did not start filling the lake's deepest point (Belt Bay) until late March little bird life appeared preferring instead to nest in the upper reaches of the Lake Eyre Basin, north of Birdsville, where large lakes appeared in January as a result of monsoonal rain.
2010.
The high rainfall in summer sent flood water into the Diamantina, Georgina and Cooper Creek catchments of the Lake Eyre basin, with the Cooper Creek reaching the lake for the first time since 1990. The higher rainfall has prompted many different birds to migrate back to the area for breeding.
2011.
Heavy local rain in early March in the Stuart Creek and Warriner catchments filled Lake Eyre South, with Lake Eyre North about 75 per cent covered with water firstly from the Neales and Macumba and later from the Warburton River.
Yacht club.
The Lake Eyre Yacht Club is a dedicated group of sailors who sail on the lake's floods, including recent trips in 1997, 2000, 2001, 2004, 2007 and 2009. A number of 6 m trailer sailers sailed on Kati Thanda–Lake Eyre in 1975, 1976, and 1984 when the flood depth reached 3–6 m (10–20 ft). In July 2010 The Yacht Club held its first regatta since 1976 and its first on Lake Killamperpunna, a freshwater lake on Cooper Creek. The Cooper had reached Kati Thanda–Lake Eyre for the first time since 1990.
When the lake is full, a notable phenomenon is that around midday the surface can often become very flat. The surface then reflects the sky in a way that leaves both the horizon and water surface virtually impossible to see. The commodore of the Lake Eyre Yacht Club has stated that sailing during this time has the appearance of sailing in the sky.
Land speed record attempts.
Kati Thanda–Lake Eyre has been a site for various land speed record attempts on its salt flats, especially those by Donald Campbell with the Bluebird-Proteus CN7.
Wildlife.
Phytoplankton in the lake includes "Nodularia spumigena" and a number of species of "Dunaliella".
Birds.
Pelicans are drawn to a filled lake from as far afield as Papua New Guinea. During the 1989—90 flood it was estimated that 200,000 pelicans, 80% of Australia's total population, came to feed at Lake Eyre.
Protected area status.
Statutory.
The extent of the lake is covered by two protected areas declared by the Government of South Australia - the Kati Thanda-Lake Eyre National Park and the Elliot Price Conservation Park.
Non-statutory.
Wetlands.
Lake Eyre is on the list of wetlands of national importance known as A Directory of Important Wetlands in Australia.
Important bird area.
Lake Eyre has been identified by BirdLife International as an Important Bird Area (IBA) known as the Lake Eyre Important Bird Area because, when flooded, it supports major breeding events of the Banded stilt and Australian pelican, as well as over 1% of the world populations of Red-necked avocets, Sharp-tailed sandpipers, Red-necked stints, Silver gulls and Caspian terns.

</doc>
<doc id="18376" url="http://en.wikipedia.org/wiki?curid=18376" title="Loran-C">
Loran-C

Loran-C is a hyperbolic radio navigation system which allows a receiver to determine its position by listening to low frequency radio signals transmitted by fixed land-based radio beacons. Loran-C combined two different techniques to provide a signal that was both long-range and highly accurate, traits that had formerly been at odds. The downside was the expense of the equipment needed to interpret the signals, which meant that Loran-C was used primarily by the military after it was first introduced in 1957.
By the 1970s the electronics needed to implement Loran-C had been dramatically reduced due to the introduction of solid state radio electronics, and especially the use of early microcontrollers to interpret the signal. Low-cost and easy-to-use Loran-C units became common from the late 1970s, especially in the early 1980s, leading to the earlier LORAN system being turned off in favour of installing more Loran-C stations around the world. Loran-C became one of the most common and widely used navigation systems for large areas of North America, Europe, Japan and the entire Atlantic and Pacific areas. The Soviet Union operated a nearly identical system, CHAYKA.
The introduction of civilian satellite navigation in the 1990s led to a very rapid drop-off in Loran-C use. Discussions about the future of Loran-C began in the 1990s, and several turn-off dates were announced and then cancelled. In 2010 the US and Canadian systems were shut down, along with shared Loran-C/CHAYKA stations with Russia. Several other chains remain active, and some have been upgraded for continued use.
History.
Loran-A.
The original LORAN was proposed by Alfred Lee Loomis at a meeting of the Microwave Committee. The US Army Air Corps was interested in the concept for aircraft navigation, and after some discussion they returned a requirement for a system offering accuracy of about 1 mi at a range of 200 mi, and a maximum range as great as 500 mi for high-flying aircraft. The Microwave Committee, by this time organized into what would become the Radiation Laboratory, took up development as Project 3. During the initial meetings a member of the UK liaison team, Taffy Bowen, mentioned that he was aware the British were also working on a similar concept, but had no information on its performance.
The development team, led by Loomis, made rapid progress on the transmitter design and tested several systems during 1940 before settling on a 3 MHz design. Extensive signal-strength measurements were made by mounting a conventional radio receiver in a station wagon and driving around the eastern states. However, the custom receiver design and its associated cathode ray tube displays proved to be a bigger problem. In spite of several efforts to design around the problem, instability in the display prevented accurate timing measurements.
By this time the team had become much more familiar with the British Gee system, and were aware of their work on "strobes", a time base generator that produced well-positioned "pips" on the display that could be used for accurate measurement. They met with the Gee team in 1941, and immediately adopted this solution. They also found that Project 3 and Gee called for almost identical systems, with similar performance, range and accuracy. But Gee had already completed basic development and was entering into initial production, making Project 3 superfluous.
In response, the Project 3 team told the Army Air Force to adopt Gee, and realigned their own efforts to provide long-range navigation on the oceans. This led to US Navy interest, and a series of experiments quickly demonstrated that systems using the basic Gee concept but operating at a much lower frequency around 2 MHz would offer reasonable accuracy on the order of a few miles over distances on the order of 1250 mi, at least at night when signals of this frequency range were able to skip off the ionosphere. Rapid development followed, and a system covering the western Atlantic was operational in 1943. Additional stations followed, covering the European side, and then a massive expansion in the Pacific. By the end of the war there were 72 operational LORAN stations, and as many as 75,000 receivers.
In 1958 the operation of the LORAN system was handed over to the US Coast Guard, which renamed the system "Loran-A", the lower-case name being introduced at that time.
LF LORAN.
There are two ways to implement the timing measurements needed for a hyperbolic navigation system, pulse timing systems like Gee and LORAN, and phase-timing systems like the Decca Navigator System. The former requires sharp pulses of signal, and their accuracy is generally limited to how rapidly the pulses can be turned on and off, which is, in turn, a function of the carrier frequency. The second requires constant signals ("continuous wave") and is easy to use even at low frequencies, but is subject to an ambiguity in location that has to be determined using some other navigation method.
Numerous efforts were made to provide some sort of secondary low-accuracy system that could be used with a phase-comparison system like Decca in order to resolve the ambiguity. Among the many methods were a directional broadcast systems known as the POPI, and a variety of systems combining pulse-timing for low-accuracy navigation and then using phase-comparison for fine adjustment. Decca themselves had set aside one frequency, "9f", for testing this concept, but did not have the chance to do so until much later. Similar concepts were also used in the experimental Navarho system in the US.
It was known from the start of the LORAN project that the same CRT displays that showed the LORAN pulses would also, when suitably magnified, show the individual waves of the intermediate frequency. This meant that pulse-matching could be used to get a rough fix, and then the operator could gain additional timing accuracy by lining up the individual waves within the pulse, like Decca. This could either be used to greatly increase the accuracy of LORAN, or alternately, offer similar accuracy using much lower carrier frequencies, and thus greatly extend range. This would require the transmitter stations to be synchronized both in time and phase, but much of this problem had been solved by Decca engineers.
The long-range option was of considerable interest to the Coast Guard, who set up an experimental system known as LF LORAN in 1945. This operated at much lower frequencies than the original LORAN, 180 kHz, and required very long balloon-borne antennas. Testing was carried out throughout the year, including several long-distance flights as far as Brazil. The experimental system was then sent to Canada where it was used during Operation Muskox in the Arctic. Accuracy was found to be 150 feet at 750 mi, a significant advance over LORAN. With the ending of Muskox it was decided to keep the system running under what became known as "Operation Musk Calf", run by a group consisting of the US Air Force, Royal Canadian Air Force, Royal Canadian Navy and Royal Corps of Signals. The system ran until September 1947.
This led to another major test series, this time by the newly formed USAF, known as Operation Beetle. Beetle was located in the far north, on the Canada-Alaska border, and used new guy-stayed 625 ft steel towers, replacing the earlier system's balloon-lofted cable antennas. The system became operational in 1948 and ran for two years until February 1950. Unfortunately the stations proved poorly sited, as the radio transmission over the permafrost was much shorter than expected and synchronization of the signals between the stations using groundwaves proved impossible. The tests also showed that the system was extremely difficult to use in practice; it was easy for the operator to select the wrong sections of the waveforms on their display, leading to significant real-world inaccuracy.
CYCLAN and Whyn.
In 1946 the Rome Air Development Center sent out contracts for longer-ranged and more-accurate navigation systems that would be used for long-range bombing navigation. As the US Army Air Force was moving towards smaller crews, a high degree of automation was desired. Two contracts were accepted; Sperry Gyroscope proposed the CYCLAN system (CYCLe matching LorAN) which was broadly similar to LF LORAN but with additional automation, and Sylvania proposed Whyn using continuous wave navigation like Decca, but with additional coding using frequency modulation. In spite of great efforts, Whyn could never be made to work, and was abandoned.
CYCLAN operated by sending the same LF LORAN-like signals on two frequencies, LF LORAN's 180 kHz and again on 200 kHz. The associated equipment would look for a rising amplitude that indicated the start of the signal pulse, and then use sampling gates to extract the carrier phase. Using two receivers solved the problem of mis-aligning the pulses, because the phases would only align properly between the two copies of the signal when the same pulses were being compared. None of this was trivial; using the era's tube-based electronics, the experimental CYCLAN system filled much of a semi-trailer.
CYCLAN proved highly successful, so much so that it became increasingly clear that the problems that led the engineers to use two frequencies were simply not as bad as expected. It appeared that a system using a single frequency would work just as well, given the right electronics. This was especially good news, as the 200 kHz frequency was interfering with existing broadcasts, and had to be moved to 160 kHz during testing.
Through this period the issue of radio spectrum use was becoming a major concern, and had led to international efforts to decide on a frequency band suitable for long-range navigation. This process eventually settled on the band from 90 to 100 kHz. CYCLAN appeared to suggest that accuracy at even lower frequencies was not a problem, and the only real concern was the expense of the equipment involved.
Cytac.
The success of the CYCLAN system led to a further contract with Sperry in 1952 for a new system with the twin goals of working in the 100 kHz range while being equally accurate, less complex and less expensive. These goals would normally be contradictory, but the CYCLAN system gave all involved the confidence that these could be met The resulting system was known as Cytac.
To solve the complexity problem, a new circuit was developed to properly time the sampling of the signal. This consisted of a circuit to extract the envelope of the pulse, another to extract the derivative of the envelope, and finally another that subtracted the derivative from the envelope. The result of this final operation would go negative during a very specific and stable part of the rising edge of the pulse, and this zero-crossing was used to trigger a very short-time sampling gate. This system replaced the complex system of clocks used in CYCLAN. By simply measuring the time between the zero-crossings of the master and slave, pulse-timing was extracted.
The output of the envelope sampler was also sent to a phase-shifter that adjusted the output of a local clock that locked to the master carrier using a phase-locked loop. Gating on the slave signal was then compared to this master signal, and a varying voltage was produced depending on the difference in phase. This voltage represented the fine-positioning measurement. The system was generally successful in testing through 1953, but there were concerns raised about the signal power at long ranges, and the possibility of jamming. This led to further modifications of the basic signal. The first was to broadcast a series of pulses instead of just one, broadcasting more energy during a given time and improving the ability of the receivers to tune in a useful signal. By adding a fixed 45° phase shift to every pulse, simple continuous-wave jamming signals could be identified and rejected.
The Cytac system underwent an enormous series of tests across the United States and offshore. Given the potential accuracy of the system, even minor changes to the groundwave synchronization were found to cause errors that could be eliminated - issues such as the number of rivers the signal crossed caused predictable delays that could be measured and then factored into navigation solutions. This led to a series of "correction contours" that could be added to the received signal to adjust for these concerns, and these were printed on the Cytac charts. Using prominent features on dams as target points, a series of tests demonstrated that the uncorrected signals provided accuracy on the order of 100 yards, while adding the correction contour adjustments reduced this to the order of ten yards.
Loran-B and -C.
It was at this moment that the US Air Force (having taken over these efforts while moving from the USAAF) dropped their interest in the project. Although the reasons are not well recorded, it appears the idea of a fully automated bombing system using radio aids was no longer considered possible. The AAF had been involved in missions covering about 1000 km (the distance from London to Berlin) and the Cytac system would work well at these ranges. But as the mission changed to trans-polar missions of 5,000 km or more, even Cytac did not offer the range and accuracy needed. They turned their attention to the use of inertial platforms and Doppler radar systems, cancelling work on Cytac as well as a competing system known as Navarho.
Around this period the Navy began work on a similar system using combined pulse and phase comparison, but based on the existing LORAN frequency of 200 kHz. By this time the Navy had handed operational control of the LORAN system to the Coast Guard, and it was assumed the same arrangement would be true for any new system as well. Thus the Coast Guard was given the choice of naming the systems, and decided to rename the existing system "Loran-A", and the new system Loran-B.
With Cytac fully developed and its test system on the US east coast mothballed, the Navy also decided to re-commission the Cytac system for tests in the long-range role. An extensive series of tests across the Atlantic were carried out by the USCGC "Androscoggin" (WHEC-68) starting in April 1956. Meanwhile Loran-B proved to have serious problems keeping their transmitters in phase, and that work was abandoned. Minor changes were made to the Cytac systems to further simplify it, including a reduction in the pulse-chain spacing from 1200 to 1000 µs, the pulse rate changed to 20 pps to match the existing Loran-A system, and the phase-shifting between pulses to an alternating 0, 180 degree shift instead of 45 degrees at every pulse within the chain.
The result was Loran-C. Testing with the new system was equally intensive, and overwater flights around Bermuda demonstrated that 50% of fixes lay within a 260 foot circle. This was a dramatic improvement over the original Loran-A, meeting the accuracy of the Gee system but at much greater range. The first chain were set up using the original experimental Cytac system, along with a second in the Mediterranean in 1957. Further chains covering the North Atlantic and large areas of the Pacific followed. At the time global charts were printed with shaded sections representing the area where a 3 mile accurate fix could be obtained under most operational conditions.
Improving systems.
Loran-C had originally been designed to be highly automated, allowing the system to be operated more rapidly than the original LORAN's multi-minute measurement. It was also operated in "chains" of linked stations, allowing a fix to be made by simultaneously comparing two slaves to a single master. The downside of this approach was that the required electronic equipment, built using 1950s-era tube technology, was very large. Looking for companies with knowledge of seaborne, multi-channel phase-comparison electronics led, ironically, to Decca, who built the AN/SPN-31, the first widely used Loran-C receiver. The AN/SPN-31 weighted over 100 lbs and had 52 controls.
Airborne units followed, and an adapted AN/SPN-31 was tested in an Avro Vulcan in 1963. By the mid-1960s, units with some transistorization were becoming more common, and a chain was set up in Viet Nam to support the US war efforts there. A number of commercial airline operators experimented with the system as well, using it for navigation on the great circle route between North America and Europe. However, inertial platforms ultimately became more common in this role.
In 1969, Decca sued the Navy for patent infringement, producing ample documentation of their work on the basic concept as early as 1944, along with the "missing" 9f frequency at 98 kHz that had been set aside for experiments using this system. Decca won the initial suit, but the judgement was overturned on appeal when the Navy claimed "wartime expediency".
Loran-D and -F.
When Loran-C became widespread, the USAF once again became interested in using it as a guidance system. They proposed a new system layered on top of Loran-C, using it as the coarse guidance signal in much the same way that pulses were the coarse guidance and phase-comparison used for fine. To provide an extra-fine guidance signal, Loran-D interleaved another train of eight pulses immediately after the signals from one of the existing Loran-C stations, folding the two signals together. This technique became known as "Supernumary Interpulse Modulation" (SIM). These were broadcast from low-power portable transmitters, offering relatively short-range service of high accuracy.
Loran-D was used only experimentally during war-games in the 1960s from a transmitter set in the UK. The system was also used in a limited fashion during the Vietnam War, combined with the Pave Spot laser designator system, a combination known as Pave Nail. Using the AN/ARN-92 mobile transmitters, accuracy on the order of 60 feet, which the Spot system improved to about 20 feet. The SIM concept became a system for sending additional data.
At about the same time, Motorola proposed a new system using pseudo-random pulse-chains. This mechanism ensures that no two chains within a given period (on the order of many seconds) will have the same pattern, making it easy to determine if the signal is a groundwave from a recent transmission or a multi-hop signal from a previous one. The system, Multi-User Tactical Navigation Systems (MUTNS) was used briefly but it was found that Loran-D met the same requirements but had the added advantage of being a standard Loran-C signal as well. Although MUTNS was unrelated to the Loran systems, it was sometimes referred to as Loran-F.
Decline.
In spite of its many advantages, the high cost of implementing a Loran-C receiver made it uneconomical for many users. Additionally, as military users upgraded from Loran-A to Loran-C, large numbers of surplus Loran-A receivers were dumped on the market. This made Loran-A popular in spite of being less accurate and fairly difficult to operate. By the early 1970s the introduction of integrated circuits combining a complete radio receiver began to greatly reduce the complexity of Loran-A measurements, and fully automated units the size of a stereo receiver became common. For those users requiring higher accuracy, Decca had considerable success with their Decca Navigator system, and produced units that combined both features.
The same rapid development of microelectronics that made Loran-A so easy to operate worked equally well on the Loran-C signals, and the obvious desire to have a long-range system that could also provide enough accuracy for lake and harbour navigation led to the "opening" of the Loran-C system to public use in 1974. Civilian receivers quickly followed, and dual-system A/C receivers were also common for a time. The switch from A to C was extremely rapid, due largely to rapidly falling prices which led to many user's first receiver being Loran-C. By the late 1970s the Coast Guard decided to turn off Loran-A, in favour of adding additional Loran-C stations to cover gaps is its coverage. The original Loran-A network was shut down in 1979 and 1980, with a few units used in the Pacific for some time.
One of the reasons for Loran-C's opening to the public was the move from Loran to new forms of navigation, including INS, Transit and OMEGA, meant that the security of Loran was no longer as stringent as it was as a primary form of navigation. As these newer systems gave way to GPS through the 1980s and 90s, this process repeated itself, but this time the military was able to separate GPS's signals in such a way that it could provide both secure military and insecure civilian signals at the same time. GPS was more difficult to receive and decode, but by the 1990s the required electronics were already as small and inexpensive as Loran-C, leading to rapid adoption that has become largely universal.
Although Loran-C was largely redundant by 2000, it has not universally disappeared as of 2014[ [update]] due to a number of concerns. One is that the GPS system can be jammed through a variety of means; although the same is true of Loran-C, the transmitters are close-at-hand and can be adjusted if need be. More importantly, there are effects that might cause the GPS system to become unusable over wide areas, notably space weather events and potential EMP events. Loran, located entirely under the atmosphere, offers more resilience to these sorts of issues. There has been considerable debate about the relative merits of keeping the Loran-C system operational as a result of considerations like these.
In November 2009, the USCG announced that LORAN-C is not needed by the U.S. for maritime navigation. This decision left the fate of LORAN and eLORAN in the U.S. to the Secretary of the Department of Homeland Security. Per a subsequent announcement, the U.S. Coast Guard, in accordance with the DHS Appropriations Act, terminated the transmission of all U.S. LORAN-C signals on 8 February 2010. On 1 August 2010 the U.S. transmission of the Russian American signal was terminated, and on 3 August 2010 all Canadian signals were shut down by the USCG and the CCG.
The European Union had decided that the potential security advantages of Loran are worthy not only of keeping the system operational, but upgrading it and adding new stations. This is part of the wider Eurofix system which combines GPS, Galileo and nine Loran stations into a single integrated system.
However, Norway announced in late 2014 that all of its remaining transmitters, which make up a significant part of the Eurofix system, will be shut down on 1 January 2016.
Principle.
The navigational method provided by LORAN is based on measuring the time difference between the receipt of signals from a pair of radio transmitters. A given constant time difference between the signals from the two stations can be represented by a hyperbolic line of position (LOP).
If the positions of the two synchronized stations are known, then the position of the receiver can be determined as being somewhere on a particular hyperbolic curve where the time difference between the received signals is constant. In ideal conditions, this is proportionally equivalent to the difference of the distances from the receiver to each of the two stations.
So a LORAN receiver which only receives two LORAN stations cannot fully fix its position—it only narrows it down to being somewhere on a curved line. Therefore the receiver must receive and calculate the time difference between a second pair of stations. This allows to be calculated a second hyperbolic line on which the receiver is located. Where these two lines cross is the location of the receiver.
In practice, one of the stations in the second pair also may be—and frequently is—in the first pair. This means signals must be received from at least three LORAN transmitters to pinpoint the receiver's location. By determining the intersection of the two hyperbolic curves identified by this method, a geographic fix can be determined.
LORAN method.
In the case of LORAN, one station remains constant in each application of the principle, the "primary", being paired up separately with two other "secondary" stations. Given two secondary stations, the time difference (TD) between the primary and first secondary identifies one curve, and the time difference between the primary and second secondary identifies another curve, the intersections of which will determine a geographic point in relation to the position of the three stations. These curves are referred to as "TD lines".
In practice, LORAN is implemented in integrated regional arrays, or "chains", consisting of one "primary" station and at least two (but often more) "secondary" stations, with a uniform "group repetition interval" (GRI) defined in microseconds. The amount of time before transmitting the next set of pulses is defined by the distance between the start of transmission of primary to the next start of transmission of primary signal.
The secondary stations receive this pulse signal from the primary, then wait a preset number of milliseconds, known as the "secondary coding delay", to transmit a response signal. In a given chain, each secondary's coding delay is different, allowing for separate identification of each secondary's signal. (In practice, however, modern LORAN receivers do not rely on this for secondary identification.)
LORAN chains (GRIs).
Every LORAN chain in the world uses a unique Group Repetition Interval, the number of which, when multiplied by ten, gives how many microseconds pass between pulses from a given station in the chain. (In practice, the delays in many, but not all, chains are multiples of 100 microseconds.) LORAN chains are often referred to by this designation (e.g., GRI 9960, the designation for the LORAN chain serving the Northeast United States).
Due to the nature of hyperbolic curves, a particular combination of a primary and two secondary stations can possibly result in a "grid" where the grid lines intersect at shallow angles. For ideal positional accuracy, it is desirable to operate on a navigational grid where the grid lines are closer to right angles (orthogonal) to each other. As the receiver travels through a chain, a certain selection of secondaries whose TD lines initially formed a near-orthogonal grid can become a grid that is significantly skewed. As a result, the selection of one or both secondaries should be changed so that the TD lines of the new combination are closer to right angles. To allow this, nearly all chains provide at least three, and as many as five, secondaries.
LORAN charts.
Where available, common marine nautical charts include visible representations of TD lines at regular intervals over water areas. The TD lines representing a given primary-secondary pairing are printed with distinct colors, and note the specific time difference indicated by each line. On a nautical chart, the denotation for each Line of Position from a receiver, relative to axis and color, can be found at the bottom of the chart. The color on official charts for stations and the timed-lines of position follow no specific conformance for the purpose of the International Hydrographic Organization (IHO). However, local chart producers may color these in a specific conformance to their standard. Always consult the chart notes, administrations Chart1 reference, and information given on the chart for the most accurate information regarding surveys, datum, and reliability.
There are three major factors when considering signal delay and propagation in relation to LORAN-C:
The chart notes should indicate whether ASF corrections have been made (Canadian Hydrographic Service (CHS) charts, for example, include them). Otherwise, the appropriate correction factors must be obtained before use.
Due to interference and propagation issues suffered from land features and artificial structures such as tall buildings, the accuracy of the LORAN signal can be degraded considerably in inland areas (see Limitations). As a result, nautical charts will not show TD lines in those areas, to prevent reliance on LORAN-C for navigation.
Traditional LORAN receivers display the time difference between each pairing of the primary and one of the two selected secondary stations, which is then used to find the appropriate TD line on the chart. Modern LORAN receivers display latitude and longitude coordinates instead of time differences, and, with the advent of time difference comparison and electronics, provide improved accuracy and better position fixing, allowing the observer to plot their position on a nautical chart more easily. When using such coordinates, the datum used by the receiver (usually WGS84) must match that of the chart, or manual conversion calculations must be performed before the coordinates can be used.
Timing and synchronization.
Each LORAN station is equipped with a suite of specialized equipment to generate the precisely timed signals used to modulate / drive the transmitting equipment. Up to three commercial cesium atomic clocks are used to generate 5 MHz and pulse per second (or 1 Hz) signals that are used by timing equipment to generate the various GRI-dependent drive signals for the transmitting equipment.
While each U.S.-operated LORAN station is supposed to be synchronized to within 100 ns of UTC, the actual accuracy achieved as of 1994 was within 500 ns.
Transmitters and antennas.
LORAN-C transmitters operate at peak powers of 100–4,000 kilowatts, comparable to longwave broadcasting stations. Most use 190–220 metre tall mast radiators, insulated from ground. The masts are inductively lengthened and fed by a loading coil (see: electrical lengthening). A well known-example of a station using such an antenna is Rantum. Free-standing tower radiators in this height range are also used. Carolina Beach uses a free-standing antenna tower. Some LORAN-C transmitters with output powers of 1,000 kW and higher used supertall 412 metre mast radiators (see below). Other high power LORAN-C stations, like George, used four T-antennas mounted on four guyed masts arranged in a square.
All LORAN-C antennas are designed to radiate an omnidirectional pattern. Unlike longwave broadcasting stations, LORAN-C stations cannot use backup antennas because the exact position of the antenna is a part of the navigation calculation. The slightly different physical location of a backup antenna would produce Lines of Position different from those of the primary antenna.
Limitations.
LORAN suffers from electronic effects of weather and the ionospheric effects of sunrise and sunset. The most accurate signal is the groundwave that follows the Earth's surface, ideally over seawater. At night the indirect skywave, bent back to the surface by the ionosphere, is a problem as multiple signals may arrive via different paths (multipath interference). The ionosphere's reaction to sunrise and sunset accounts for the particular disturbance during those periods. Magnetic storms have serious effects as with any radio based system.
LORAN uses ground based transmitters that only cover certain regions. Coverage is quite good in North America, Europe, and the Pacific Rim.
The absolute accuracy of LORAN-C varies from 0.10 to. Repeatable accuracy is much greater, typically from 60 to.
LORAN-A and other systems.
LORAN-A was a less accurate system operating in the upper mediumwave frequency band prior to deployment of the more accurate LORAN-C system. For LORAN-A the transmission frequencies 1750 kHz, 1850 kHz, 1900 kHz and 1950 kHz were used, shared with the 1800–2000 kHz amateur 160-meter band. LORAN-A continued in operation partly due to the economy of the receivers and widespread use in civilian recreational and commercial navigation. LORAN-B was a phase comparison variation of LORAN-A while LORAN-D was a short-range tactical system designed for USAF bombers. The unofficial "LORAN-F" was a drone control system. None of these went much beyond the experimental stage. An external link to them is listed below.
LORAN-A was used in the Vietnam War for navigation by large United States aircraft (C-124, C-130, C-97, C-123, HU-16, etc.). A common airborne receiver of that era was the R-65/APN-9 which combined the receiver and cathode ray tube (CRT) indicator into a single relatively lightweight unit replacing the two larger, separate receiver and indicator units which composed the predecessor APN-4 system. The APN-9 and APN-4 systems found wide post–World War II use on fishing vessels in the U.S. They were cheap, accurate and plentiful. The main drawback for use on boats was their need for aircraft power, 115 VAC at 400 Hz. This was solved initially by the use of rotary converters, typically 28 VDC input and 115 VAC output at 400 Hz. The inverters were large, noisy and required significant power. In the 1960s, several firms such as Topaz and Linear Systems marketed solid state inverters specifically designed for these surplus LORAN-A sets. The availability of solid state inverters that used 12 VDC input opened up the surplus LORAN-A sets for use on much smaller vessels which typically did not have the 24-28 VDC systems found on larger vessels. The solid state inverters were very power efficient and widely replaced the more trouble prone rotary inverters.
LORAN-A saved many lives by allowing offshore boats in distress to give accurate position reports. It also guided many boats whose owners could not afford radar safely into fog bound harbors or around treacherous offshore reefs. The low price of surplus LORAN-A receivers (often under $150) meant that owners of many small fishing vessels could afford this equipment, thus greatly enhancing safety. Surplus LORAN-A equipment, which was common on commercial fishing boats, was rarely seen on yachts. The unrefined cosmetic appearance of the surplus equipment was probably a deciding factor.
Pan American World Airways used APN 9s in early Boeing 707 operations. World War II surplus APN-9 looked out of place in the modern 707 cockpit, but was needed. There is an R65A APN-9 set displayed in the museum at San Francisco International Airport, painted gold. It was a retirement gift to an ex Pan Am captain.
An elusive final variant of the APN 9 set was the APN 9A. A USAF technical manual (with photographs and schematics) shows that it had the same case as the APN-9 but a radically different front panel and internal circuitry on the non-RF portions. The APN-9A had vacuum tube flipflop digital divider circuits so that TDs (time delays) between the primary and secondary signal could be selected on front panel rotary decade switches. The older APN-9 set required the user to perform a visual count of crystal oscillator timing marker pips on the CRT and add them up to get a TD. The APN 9A did not make it into widespread military use, if it was used at all, but it did exist and represented a big advance in military LORAN-A receiver technology.
In the 1970s one US company, SRD Labs in Campbell, California, made modern LORAN-A sets including one that was completely automatic with a digital TD readout on the CRT, and autotracking so that TDs were continuously updated. Other SRD models required the user to manually align the primary and secondary signals on the CRT and then a phase locked loop would keep them lined up and provide updated TD readouts thereafter. These SRD LORAN-A sets would track only one pair of stations, providing just one LOP (line of position). For a continuously updated position (two TDs giving intersecting LOPs) rather than just a single LOP, two sets were necessary.
LORAN-A was terminated in the United States on 31 December 1980 and the restrictions on amateur radio use of the 160-meter band were lifted.
Long after LORAN-A broadcasts were terminated, commercial fishermen still referred to old LORAN-A TDs, e.g., "I am on the 4100 [microsecond] line in 35 fathoms", referring to a position outside Bodega Bay. Many LORAN-C sets incorporated LORAN A TD converters so that a LORAN-C set could be used to navigate to a LORAN-A TD defined line or position.
LORAN Data Channel (LDC).
LORAN Data Channel (LDC) is a project underway between the FAA and USCG to send low bit rate data using the LORAN system. Messages to be sent include station identification, absolute time, and position correction messages. In 2001, data similar to Wide Area Augmentation System (WAAS) GPS correction messages were sent as part of a test of the Alaskan LORAN chain. As of November 2005, test messages using LDC were being broadcast from several U.S. LORAN stations.
In recent years, LORAN-C has been used in Europe to send differential GPS and other messages, employing a similar method of transmission known as EUROFIX.
A system called SPS (Saudi Positioning System), similar to EUROFIX, is in use in Saudi Arabia. GPS differential corrections and GPS integrity information are added to the LORAN signal. A combined GPS/LORAN receiver is used, and if a GPS fix is not available it automatically switches over to LORAN.
The future of LORAN.
As LORAN systems are government maintained and operated, their continued existence is subject to public policy. With the evolution of other electronic navigation systems, such as satellite navigation systems, funding for existing systems is not always assured.
Critics, who have called for the elimination of the system, state that the LORAN system has too few users, lacks cost-effectiveness, and that GNSS signals are superior to LORAN. Supporters of continued and improved LORAN operation note that LORAN uses a strong signal, which is difficult to jam, and that LORAN is an independent, dissimilar, and complementary system to other forms of electronic navigation, which helps ensure availability of navigation signals.
On 26 February 2009, the U.S. Office of Management and Budget released the first blueprint for the Financial Year 2010 budget. This document identified the LORAN-C system as “outdated” and supported its termination at an estimated savings of $36 million in 2010 and $190 million over five years.
On 21 April 2009 the U.S. Senate Committee on Commerce, Science and Transportation and the Committee on Homeland Security and Governmental Affairs released inputs to the FY 2010 Concurrent Budget Resolution with backing for the continued support for the LORAN system, acknowledging the investment already made in infrastructure upgrades and recognizing the studies performed and multi-departmental conclusion that eLORAN is the best backup to GPS.
Senator Jay Rockefeller, Chairman of the Committee on Commerce, Science and Transportation, wrote that the committee recognized the priority in "Maintaining LORAN-C while transitioning to eLORAN" as means of enhancing the homeland security, marine safety and environmental protection missions of the Coast Guard.
Senator Collins, the ranking member on the Committee on Homeland Security and Governmental Affairs wrote that the President's budget overview proposal to terminate the LORAN-C system is inconsistent with the recent investments, recognized studies and the mission of the U.S. Coast Guard. The committee also recognizes the $160 million investment already made toward upgrading the LORAN-C system to support the full deployment of eLORAN.
Further, the Committees also recognize the many studies which evaluated GPS backup systems and concluded both the need to back up GPS and identified eLORAN as the best and most viable backup. "This proposal is inconsistent with the recently released (January 2009) Federal Radionavigation Plan (FRP), which was jointly prepared by DHS and the Departments of Defense (DOD) and Transportation (DOT). The FRP proposed the eLORAN program to serve as a Position, Navigation and Timing (PNT) backup to GPS (Global Positioning System)."
On 7 May 2009, President Barack Obama proposed cutting funding (approx. $35 million/year) for LORAN, citing its redundancy alongside GPS. In regard to the pending Congressional bill, H.R. 2892, it was subsequently announced that "[t]he Administration supports the Committee's aim to achieve an orderly termination through a phased decommissioning beginning in January 2010, and the requirement that certifications be provided to document that the LORAN-C termination will not impair maritime safety or the development of possible GPS backup capabilities or needs."
Also on 7 May 2009, the U.S. General Accounting Office (GAO), the investigative arm of Congress, released a report citing the very real potential for the GPS system to degrade or fail in light of program delays which have resulted in scheduled GPS satellite launches slipping by up to three years.
On 12 May 2009 the March 2007 Independent Assessment Team (IAT) report on LORAN was released to the public. In its report the ITA stated that it "unanimously recommends that the U.S. government complete the eLORAN upgrade and commit to eLORAN as the national backup to GPS for 20 years." The release of the report followed an extensive Freedom Of Information Act (FOIA) battle waged by industry representatives against the federal government. Originally completed 20 March 2007 and presented to the co-sponsoring Department of Transportation and Department of Homeland Security (DHS) Executive Committees, the report carefully considered existing navigation systems, including GPS. The unanimous recommendation for keeping the LORAN system and upgrading to eLORAN was based on the team's conclusion that LORAN is operational, deployed and sufficiently accurate to supplement GPS. The team also concluded that the cost to decommission the LORAN system would exceed the cost of deploying eLORAN, thus negating any stated savings as offered by the Obama administration and revealing the vulnerability of the U.S. to GPS disruption.
In November 2009, the U.S. Coast Guard announced that the LORAN-C stations under its control would be closed down for budgetary reasons after 4 January 2010 provided the Secretary of the Department of Homeland Security certified that LORAN is not needed as a backup for GPS.
On 7 January 2010, Homeland Security published a notice of the permanent discontinuation of LORAN-C operation. Effective 2000 UTC 8 February 2010, the United States Coast Guard terminated all operation and broadcast of LORAN-C signals in the USA. The U.S. Coast Guard transmission of the Russian American CHAYKA signal was terminated on 1 August 2010. The transmission of Canadian LORAN-C signals was terminated on 3 August 2010.
eLORAN.
With the perceived vulnerability of GNSS systems, and their own propagation and reception limitations, renewed interest in LORAN applications and development has appeared. Enhanced LORAN, also known as eLORAN or E-LORAN, comprises an advancement in receiver design and transmission characteristics which increase the accuracy and usefulness of traditional LORAN. With reported accuracy as good as ± 8 meters, the system becomes competitive with unenhanced GPS. eLORAN also includes additional pulses which can transmit auxiliary data such as DGPS corrections. eLORAN receivers now use "all in view" reception, incorporating signals from all stations in range, not solely those from a single GRI, incorporating time signals and other data from up to 40 stations. These enhancements in LORAN make it adequate as a substitute for scenarios where GPS is unavailable or degraded.
United Kingdom eLORAN implementation.
On 31 May 2007, the UK Department for Transport (DfT), via the General Lighthouse Authorities (GLA), awarded a 15-year contract to provide a state-of-the-art enhanced LORAN (eLORAN) service to improve the safety of mariners in the UK and Western Europe. The service contract will operate in two phases, with development work and further focus for European agreement on eLORAN service provision from 2007 through 2010, and full operation of the eLORAN service from 2010 through 2022. The first eLORAN transmitter is situated at Anthorn radio station Cumbria, UK, and operated by Babcock Comms, which is part of the Babcock Group PLC.
eLORAN: The UK government has granted approval for seven differential eLoran ship-positioning technology stations to be built along the south and east coasts of the UK to help counter the threat of jamming of global positioning systems. They are set to reach initial operational capability by summer 2014. The General Lighthouse Authorities (GLAs) of the UK and Ireland announced October 31 the initial operational capability of UK maritime eLoran. Seven differential reference stations now provide additional position, navigation, and timing (PNT) information via low-frequency pulses to ships fitted with eLoran receivers. The service will help ensure they can navigate safely in the event of GPS failure in one of the busiest shipping regions in the world, with expected annual traffic of 200,000 vessels by 2020.
List of LORAN-C transmitters.
A list of LORAN-C transmitters. Stations with an antenna tower taller than 300 metres (984 feet) are shown in bold.

</doc>
<doc id="18377" url="http://en.wikipedia.org/wiki?curid=18377" title="Lunatic">
Lunatic

Lunatic is an informal term referring to people who are considered mentally ill, dangerous, foolish or unpredictable; conditions once attributed to lunacy. The term may be considered insulting in serious contexts in modern times, though is now more likely to be used in friendly jest. The word derives from "lunaticus" meaning "of the moon" or "moonstruck". The term was once commonly used in law.
History.
The term lunatic derives from the Latin lunaticus which originally referred mainly to epilepsy and "madness" as diseases caused by the Moon. By the fourth and fifth centuries astrologers began to commonly use the term to refer to neurological and psychiatric diseases. Philosophers such as Aristotle and Pliny the Elder argued that the full Moon induced insane individuals with bipolar disorder by providing light during nights which would otherwise have been dark, and affecting susceptible individuals through the well-known route of sleep deprivation. Through at least 1700 it was also a common belief that the Moon influenced fevers, rheumatism, episodes of epilepsy and other diseases.
Use of the term "lunatic" in legislation.
In the British jurisdiction of England and Wales the Lunacy Acts 1890 - 1922 referred to lunatics, but the Mental Treatment Act 1930 changed the legal term to "Person of Unsound Mind", an expression which was replaced under the Mental Health Act 1959 by mental illness. "Person of unsound mind" was the term used in 1950 in the English version of the European Convention on Human Rights as one of the types of person who could be deprived of liberty by a judicial process. The 1930 act also replaced asylum with mental hospital. Criminal lunatics became Broadmoor patients in 1948 under the National Health Service (Scotland) Act 1947.
On December 5, 2012 the US House of Representatives passed legislation approved earlier by the US Senate removing the word "lunatic" from all federal laws in the United States. President Obama signed this legislation into law on December 28, 2012.
"Of unsound mind" or "non compos mentis" are alternatives to lunatic, which was the most conspicuous term used for insanity in the law in the late 19th century.
Lunar distance.
The term "lunatic" was also used by supporters of John Harrison and his marine chronometer method of determining longitude to refer to proponents of the Method of Lunar Distances, advanced by Astronomer Royal Nevil Maskelyne. 
Later, members of the Lunar Society of Birmingham called themselves "lunaticks". In an age with little street lighting, the society met on or about the night of the full moon.

</doc>
<doc id="18379" url="http://en.wikipedia.org/wiki?curid=18379" title="Linear timecode">
Linear timecode

Linear (or Longitudinal) Timecode (LTC) is an encoding of SMPTE timecode data in an audio signal, as defined in SMPTE 12M specification. The audio signal is commonly recorded on a VTR track or other storage media. The bits are encoded using the biphase mark code, also known as "FM": a zero bit has a single transition at the start of the bit period. A one bit has two transitions, at the beginning and middle of the period. This encoding is self-clocking. Each frame is terminated by a 'sync word' which has a special predefined sync relationship with any video or film content.
A special bit in the linear timecode frame, the 'biphase mark correction' bit, ensures that there are an even number of AC transitions in each timecode frame.
The sound of linear timecode is a jarring and distinctive noise and has been used as a sound-effects shorthand to imply 'telemetry' or 'computers'.
Generation and Distribution.
In broadcast video situations, the LTC generator should be tied-in to house black burst, as should all devices using timecode, to ensure correct color framing and correct synchronization of all digital clocks. When synchronizing multiple clock-dependent digital devices together with video, such as digital audio recorders, the devices must be connected to a common word clock signal that is derived from the house black burst signal. This can be accomplished by using a generator that generates both black burst and video-resolved word clock, or by synchronizing the master digital device to video, and synchronizing all subsequent devices to the word clock output of the master digital device (and to LTC).
Made up of 80 bits per frame, where there may be 24, 25 or 30 frames per second, LTC timecode varies from 1920 Hz (binary zeros at 24 frames/s) to 4800 Hz (binary ones at 30 frames/s), and thus is comfortably in the middle of the audio frequency range. LTC can exist as either a balanced or unbalanced signal, and can be treated as an audio signal in regards to distribution. Like audio, LTC can be distributed by standard audio wiring, connectors, distribution amplifiers, and patchbays, and can be ground-isolated with audio transformers. It can also be distributed via 75 ohm video cable and video distribution amplifiers, although the voltage attenuation caused by using a 75 ohm system may cause the signal to drop to a level that can not be read by some equipment.
Care has to be taken with analog audio to avoid audible 'breakthrough' (aka "crosstalk") from the LTC track to the audio tracks.
LTC care:
Longitudinal SMPTE timecode should be played back at a middle-level when recorded on an audio track, as both low and high levels will introduce distortion.
Longitudinal timecode data format.
The basic format is an 80-bit code that gives the time of day to the second, and the frame number within the second. Values are stored in binary-coded decimal, least significant bit first.
There are thirty-two bits of user data, usually used for a reel number and date.

</doc>
<doc id="18381" url="http://en.wikipedia.org/wiki?curid=18381" title="John William Strutt, 3rd Baron Rayleigh">
John William Strutt, 3rd Baron Rayleigh

John William Strutt, 3rd Baron Rayleigh, OM, PRS (; 12 November 1842 – 30 June 1919) was an English physicist who, with William Ramsay, discovered argon, an achievement for which he earned the Nobel Prize for Physics in 1904. He also discovered the phenomenon now called Rayleigh scattering, which can be used to explain why the sky is blue, and predicted the existence of the surface waves now known as Rayleigh waves. Rayleigh's textbook," The Theory of Sound", is still referred to by acoustic engineers today.
Biography.
John William Strutt, of Terling Place Essex, suffered from frailty and poor health in his early years. He attended Harrow School, before going on to the University of Cambridge in 1861 where he studied mathematics at Trinity College. He obtained a Bachelor of Arts degree (Senior Wrangler and 1st Smith's prize) in 1865, and a Master of Arts in 1868. He was subsequently elected to a Fellowship of Trinity. He held the post until his marriage to Evelyn Balfour, daughter of James Maitland Balfour, in 1871. He had three sons with her. In 1873, on the death of his father, John Strutt, 2nd Baron Rayleigh, he inherited the Barony of Rayleigh.
He was the second Cavendish Professor of Physics at the University of Cambridge (following James Clerk Maxwell), from 1879 to 1884. He first described dynamic soaring by seabirds in 1883, in the British journal "Nature". From 1887 to 1905 he was Professor of Natural Philosophy at Cambridge.
Around the year 1900 Lord Rayleigh developed the "duplex" (combination of two) theory of human sound localization using two binaural cues, interaural phase difference (IPD) and interaural level difference (ILD) (based on analysis of a spherical head with no external pinnae). The theory posits that we use two primary cues for sound lateralization, using the difference in the phases of sinusoidal components of the sound and the difference in amplitude (level) between the two ears.
In 1919, Rayleigh served as President of the Society for Psychical Research.
The rayl unit of acoustic impedance is named after him.
As an advocate that simplicity and theory be part of the scientific method, Lord Rayleigh argued for the principle of similitude.
Lord Rayleigh was elected Fellow of the Royal Society on 12 June 1873, and served as president of the Royal Society from 1905 to 1908. From time to time Lord Rayleigh participated in the House of Lords; however, he spoke up only if politics attempted to become involved in science. He died on 30 June 1919, in Witham, Essex. He was succeeded, as the 4th Lord Rayleigh, by his son Robert John Strutt, another well-known physicist.
Religious views.
Lord Rayleigh was an Anglican. Though he did not write about the relationship of science and religion, he retained a personal interest in spiritual matters. When his scientific papers were to be published in a collection by the Cambridge University Press, Strutt wanted to include a religious quotation from The Bible, but he was discouraged from doing so, as he later reported:
"When I was bringing out my "Scientific Papers" I proposed a motto from the Psalms, "The Works of the Lord are great, sought out of all them that have pleasure therein." The Secretary to the Press suggested with many apologies that the reader might suppose that I was the Lord." 
Still, he kept his wish and the quotation was printed in the five-volume collection of scientific papers.
In a letter to a family member, he also wrote about his rejection of materialism and spoke of Jesus Christ as a moral teacher:
"I have never thought the materialist view possible, and I look to a power beyond what we see, and to a life in which we may at least hope to take part. What is more, I think that Christ and indeed other spiritually gifted men see further and truer than I do, and I wish to follow them as far as I can." Rayleigh (1910)
Honours and awards.
Craters on Mars and the Moon are named in his honour as well as a type of surface wave known as a Rayleigh wave. The asteroid 22740 Rayleigh was named in his honour on 1 June 2007. The rayl, a unit of acoustic impedance, is named for him.

</doc>
<doc id="18382" url="http://en.wikipedia.org/wiki?curid=18382" title="Lunisolar calendar">
Lunisolar calendar

A lunisolar calendar is a calendar in many cultures whose date indicates both the moon phase and the time of the solar year. If the solar year is defined as a tropical year, then a lunisolar calendar will give an indication of the season; if it is taken as a sidereal year, then the calendar will predict the constellation near which the full moon may occur. Usually there is an additional requirement that the year have a whole number of months, in which case most years have 12 months but every second or third year has 13.
Examples.
The Hebrew, Buddhist, Hindu, Kurdish, Bengali, and Tibetan calendars, as well as the traditional Chinese, Japanese, Vietnamese, Mongolian and Korean calendars, plus the ancient Hellenic, Coligny, and Babylonian calendars are all lunisolar. Also, some of the ancient pre-Islamic calendars in South Arabia followed a lunisolar system. The Chinese, Coligny and
Hebrew lunisolar calendars track more or less the tropical year whereas the Buddhist and Hindu lunisolar calendars track the sidereal year. Therefore, the first three give an idea of the seasons whereas the last two give an idea of the position among the constellations of the full moon. The Tibetan calendar was influenced by both the Chinese and Hindu calendars. The Germanic peoples also used a lunisolar calendar before their conversion to Christianity.
The Islamic calendar is lunar, but not a lunisolar calendar because its date is not related to the sun. The civil versions of the Julian and Gregorian calendars are solar, because their dates do not indicate the moon phase — however, both the Gregorian and Julian calendars include undated lunar calendars that allow them to calculate the Christian celebration of Easter, so both are lunisolar calendars in that respect.
Determining leap months.
To determine when an embolismic month needs to be inserted, some calendars rely on direct observations of the state of vegetation, while others compare the ecliptic longitude of the sun and the phase of the moon. The Hawaiians observe the movement of specific stars and insert months accordingly.
On the other hand, in arithmetical lunisolar calendars, an integral number of months is fitted into some integral number of years by a fixed rule. To construct such a calendar (in principle), the average length of the tropical year is divided by the average length of the synodic month, which gives the number of average synodic months in a tropical year as:
Continued fractions of this decimal value ([12; 2, 1, 2, 1, 1, 17, ...]) give optimal approximations for this value. So in the list below, after the number of synodic months listed in the numerator, approximately an integer number of tropical years as listed in the denominator have been completed:
Note however that in none of the arithmetic calendars is the average year length exactly equal to a true tropical year. Different calendars have different average year lengths and different average month lengths, so the discrepancy between the calendar months and moon is not equal to the values given above.
The 8-year cycle (99 synodic months, including 99−8×12 = 3 embolismic months) was the octaeteris used in the ancient Athenian calendar. The 8-year cycle was also used in early third-century Easter calculations (or old "Computus") in Rome .
The 19-year cycle (235 synodic months, including 235−(19×12) = 7 embolismic months) is the classic Metonic cycle, which is used in most arithmetical lunisolar calendars. It is a combination of the 8- and 11-year period, and whenever the error of the 19-year approximation accumulates to 1⁄19 of a mean month, a cycle can be truncated to 11 years (skipping 8 years including 3 embolismic months), after which 19-year cycles can resume. Meton's cycle had an integer number of days, although "Metonic cycle" often means its use without an integer number of days. It was adapted to a mean year of 365.25 days by means of the 4×19 year Callippic cycle (used in the Easter calculations of the Julian calendar).
Rome used an 84-year cycle for Easter calculations from the third century until 457. The native British Christians continued its use as late as 768, when Bishop Elfodd of Bangor finally persuaded them to adopt the improved calendars introduced by St Augustine's mission. The 84-year cycle is equivalent to a Callippic 4×19-year cycle (including 4×7 embolismic months) plus an 8-year cycle (including 3 embolismic months) and so has a total of 1039 months (including 31 embolismic months). This gives an average of 12.3690476... months per year. One cycle was 30681 days, which is about 1.28 days short of 1039 synodic months, 0.66 days more than 84 tropical years, and 0.53 days short of 84 sidereal years.
The next approximation (arising from continued fractions) after the Metonic cycle (such as a 334-year cycle) is very sensitive to the values one adopts for the lunation (synodic month) and the year, especially the year. There are different possible definitions of the year so other approximations may be more accurate for specific purposes. For example a 353-year cycle including 130 embolismic months for a total of 4366 months (12.36827195...) is more accurate for a northern hemisphere spring equinox year, whereas a 611-year cycle including 225 embolismic months for a total of 7557 months (12.36824877...) has good accuracy for a northern hemisphere summer solstice year, and a 160-year cycle including 59 embolismic months for a total of 1979 months (12.36875) has good accuracy for a sidereal year (approx 12.3687462856 synodic months).
Calculating a leap month.
A rough idea of the frequency of the intercalary or leap month in all lunisolar calendars can be obtained by the following calculation, using approximate lengths of months and years in days:
A representative sequence of common and leap years is ccLccLcLccLccLccLcL, which is the classic nineteen-year Metonic cycle. The Buddhist and Hebrew calendars restrict the leap month to a single month of the year; the number of common months between leap months is, therefore, usually 36, but occasionally only 24 months. Because the Chinese and Hindu lunisolar calendars allow the leap month to occur after or before (respectively) any month but use the true motion of the sun, their leap months do not usually occur within a couple of months of perihelion, when the apparent speed of the sun along the ecliptic is fastest (now about 3 January). This increases the usual number of common months between leap months to roughly 34 months when a doublet of common years occurs, while reducing the number to about 29 months when only a common singleton occurs.
Lunisolar calendars with uncounted time.
An alternative way of dealing with the fact that a solar year does not contain an integer number of months is by including uncounted time in the year that does not belong to any month. Some Coast Salish peoples used a calendar of this kind. For instance, the Chehalis began their count of lunar months from the arrival of spawning chinook salmon (in Gregorian calendar October), and counted 10 months, leaving an uncounted period until the next chinook salmon run.
Gregorian lunisolar calendar.
The Gregorian calendar has a lunisolar calendar, which is used to determine the date of Easter. The rules are in the Computus.

</doc>
<doc id="18383" url="http://en.wikipedia.org/wiki?curid=18383" title="Leonids">
Leonids

The Leonids ( ) are a prolific meteor shower associated with the comet Tempel-Tuttle. The Leonids get their name from the location of their radiant in the constellation Leo: the meteors appear to radiate from that point in the sky. Their proper Greek name should be Leon"t"ids (Λεοντίδαι, "Leontídai"), but the word was initially constructed as a Greek/Latin hybrid and it has been used since. They peak in the month of November.
Earth moves through the meteoroid stream of particles left from the passages of a comet. The stream comprises solid particles, known as meteoroids, ejected by the comet as its frozen gases evaporate under the heat of the Sun when it is close enough – typically closer than Jupiter's orbit. The Leonids are a fast moving stream which encounter the path of Earth and impact at 72 km/s. Larger Leonids which are about 10 mm across have a mass of half a gram and are known for generating bright (apparent magnitude -1.5) meteors. An annual Leonid shower may deposit 12 or 13 tons of particles across the entire planet.
The meteoroids left by the comet are organized in trails in orbits similar to though different from that of the comet. They are differentially disturbed by the planets, in particular Jupiter and to a lesser extent by radiation pressure from the sun, the Poynting–Robertson effect, and the Yarkovsky effect. These trails of meteoroids cause meteor showers when Earth encounters them. Old trails are spatially not dense and compose the meteor shower with a few meteors per minute. In the case of the Leonids, that tends to peak around November 18, but some are spread through several days on either side and the specific peak changes every year. Conversely, young trails are spatially very dense and the cause of meteor outbursts when the Earth enters one. Meteor storms (large outbursts) exceed 1000 meteors per hour, to be compared to the sporadic background (5 to 8 meteors per hour) and the shower background (several per hour).
History.
1800s.
The Leonids are famous because their meteor showers, or storms, can be among the most spectacular. Because of the storm of 1833 and the recent developments in scientific thought of the time (see for example the identification of Halley's Comet) the Leonids have had a major effect on the development of the scientific study of meteors which had previously been thought to be atmospheric phenomena. The meteor storm of 1833 was of truly superlative strength. One estimate is over one hundred thousand meteors an hour, but another, done as the storm abated, estimated in excess of 240,000 meteors during the nine hours of the storm over the entire region of North America east of the Rocky Mountains.
It was marked by the Native Americans, abolitionists like Harriet Tubman and Frederick Douglass and slave-owners and others. Near Independence, Missouri, in Clay County, a refugee Mormon community watched the meteor shower on the banks of the Missouri River after having been driven from their homes by local settlers. The founder and first leader of Mormonism, Joseph Smith, had predicted "stars falling from Heaven", and afterwards noted in his journal that this event was a literal fulfillment of the word of God and a sure sign that the coming of Christ is close at hand.
Denison Olmsted explained the event most accurately. After spending the last weeks of 1833 collecting information, he presented his findings in January 1834 to the "American Journal of Science and Arts", published in January–April 1834, and January 1836. He noted the shower was of short duration and was not seen in Europe, and that the meteors radiated from a point in the constellation of Leo and he speculated the meteors had originated from a cloud of particles in space. Accounts of the 1866 repeat of the Leonids counted hundreds per minute/a few thousand per hr in Europe. The Leonids were again seen in 1867, when moonlight reduced the rates to 1000 per hour. Another strong appearance of the Leonids in 1868 reached an intensity of 1000 per hour in dark skies. It was in 1866–67 that information on Comet Tempel-Tuttle was gathered pointing it out as the source of the meteor shower. When the storms failed to return in 1899, it was generally thought that the dust had moved on and storms were a thing of the past.
1900s.
Then, in 1966, a spectacular storm was seen over the Americas. Historical notes were gathered thus noting the Leonids back to 900AD. Radar studies showed the 1966 storm included a relatively high percentage of smaller particles while 1965's lower activity had a much higher proportion of larger particles. In 1981 Donald K. Yeomans of the Jet Propulsion Laboratory reviewed the history of meteor showers for the Leonids and the history of the dynamic orbit of Comet Tempel-Tuttle. A graph from it was adapted and re-published in Sky and Telescope. It showed relative positions of the Earth and Tempel-Tuttle and marks where Earth encountered dense dust. This showed that the meteoroids are mostly behind and outside the path of the comet, but paths of the Earth through the cloud of particles resulting in powerful storms were very near paths of nearly no activity. But overall the 1998 Leonids were in a favorable position so interest was rising.
Leading up to the 1998 return, an airborne observing campaign was organized to mobilize modern observing techniques by Peter Jenniskens at NASA Ames Research Center. There were also efforts to observe impacts of meteoroids, as an example of transient lunar phenomenon, on the Moon in 1999. A particular reason to observe the Moon is that our vantage from a location on Earth sees only meteors coming into the atmosphere relatively close to us while impacts on the Moon would be visible from across the Moon in a single view. A sodium tail of the Moon tripled just after the 1998 Leonid shower which was composed of larger meteoroids (which in the case of the Earth was witnessed as fireballs.) However in 1999 the sodium tail of the Moon did not change from the Leonid impacts.
Research by Kondrat'eva, Reznikov and colleagues at Kazan University had shown how meteor storms could be accurately predicted, but for some years the worldwide meteor community remained largely unaware of these results. The work of David J. Asher, Armagh Observatory and Robert H. McNaught, Siding Spring Observatory and independently by Esko Lyytinen in 1999, following on from the Kazan research, is considered by most meteor experts as the breakthrough in modern analysis of meteor storms. Whereas previously it was hazardous to guess if there would be a storm or little activity, the predictions of Asher and McNaught timed bursts in activity down to ten minutes by narrowing down the clouds of particles to individual streams from each passage of the comet, and their trajectories amended by subsequent passage near planets. However, whether a specific meteoroid trail will be primarily composed of small or large particles, and thus the relative brightness of the meteors, was not understood. But McNaught did extend the work to examine the placement of the Moon with trails and saw a large chance of a storm impacting in 1999 from a trail while there were less direct impacts from trails in 2000 and 2001 (successive contact with trails through 2006 showed no hits.)
2000s.
Viewing campaigns resulted in spectacular footage from the 1999, 2001 and 2002, storms producing up to 3,000 Leonid meteors per hour. Predictions for the Moon's Leonid impacts also noted that in 2000 the side of the Moon facing the stream was away from the Earth but that impacts should be in number enough to raise a cloud of particles kicked off the Moon by impacts would cause a detectable increase in the sodium tail of the Moon. Research using the explanation of meteor trails/streams have explained the storms of the past. The 1833 storm was not due to the recent passage of the comet, but from a direct impact with the previous 1800 dust trail. The meteoroids from the 1733 passage of Comet Tempel-Tuttle resulted in the 1866 storm and the 1966 storm was from the 1899 passage of the comet. The double spikes in Leonid activity in 2001 and in 2002 were due to the passage of the comet's dust ejected in 1767 and 1866. This ground breaking work was soon applied to other meteor showers – for example the 2004 June Bootids. Peter Jenniskens has published predictions for the next 50 years. However, a close encounter with Jupiter is expected to perturb the comet's path, and many streams, making storms of historic magnitude unlikely for many decades. Recent work tries to take into account the roles of differences in parent bodies and the specifics of their orbits, ejection velocities off the solid mass of the core of a comet, radiation pressure from the sun, the Poynting–Robertson effect, and the Yarkovsky effect on the particles of different sizes and rates of rotation to explain differences between meteor showers in terms of being predominantly fireballs or small meteors.
Predictions until the end of the 21st century have been published by Mikhail Maslov.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="18384" url="http://en.wikipedia.org/wiki?curid=18384" title="Labarum">
Labarum

The labarum (Greek: λάβαρον) was a "vexillum" (military standard) that displayed the "Chi-Rho" symbol ☧, a christogram formed from the first two Greek letters of the word "Christ" (Greek: ΧΡΙΣΤΟΣ, or Χριστός) — "Chi" (χ) and "Rho" (ρ). It was first used by the Roman emperor Constantine I. Since the vexillum consisted of a flag suspended from the crossbar of a cross, it was ideally suited to symbolize the crucifixion of Christ.
Ancient sources draw an unambiguous distinction between the two terms "labarum" and "Chi-Rho", even though later usage sometimes regards the two as synonyms. The name labarum was applied both to the original standard used by Constantine the Great and to the many standards produced in imitation of it in the Late Antique world, and subsequently.
Etymology.
Beyond its derivation from Latin "labarum", the etymology of the word is unclear. Some derive it from Latin /labāre/ 'to totter, to waver' (in the sense of the "waving" of a flag in the breeze) or "laureum [vexillum]" ("laurel standard"). According to the Real Academia Española, the related lábaro is also derived from Latin "labărum" but offers no further derivation from within Latin, as does the Oxford English Dictionary. An origin as a loan into Latin from a Celtic language or Basque has also been postulated. There is a traditional Basque symbol called the lauburu; though the name is only attested from the 19th century onwards the motif occurs in engravings dating as early as the 2nd century AD.
Vision of Constantine.
On the evening of October 27, 312 AD, with his army preparing for the Battle of the Milvian Bridge, the emperor Constantine I claimed to have had a vision which led him to believe he was fighting under the protection of the Christian God.
Lactantius states that, in the night before the battle, Constantine was commanded in a dream to "delineate the heavenly sign on the shields of his soldiers". He obeyed and marked the shields with a sign denoting Christ. Lactantius describes that sign as a "staurogram", or a Latin cross with its upper end rounded in a P-like fashion, rather than the better known Chi-Rho sign described by Eusebius of Caesarea. Thus, it had both the form of a cross and the monogram of Christ's name from the formed letters "Χ" and "Ρ", the first letters of Christ's name in Greek.
From Eusebius, two accounts of a battle survive. The first, shorter one in the "Ecclesiastical History" leaves no doubt that God helped Constantine but does not mention any vision. In his later "Life of Constantine", Eusebius gives a detailed account of a vision and stresses that he had heard the story from the emperor himself. According to this version, Constantine with his army was marching somewhere (Eusebius does not specify the actual location of the event, but it clearly is not in the camp at Rome) when he looked up to the sun and saw a cross of light above it, and with it the Greek words "Ἐν Τούτῳ Νίκα". The traditionally employed Latin translation of the Greek is "in hoc signo vinces"— literally "In this sign, you will conquer." However, a direct translation from the original Greek text of Eusebius into English gives the phrase "By this, conquer!"
At first he was unsure of the meaning of the apparition, but the following night he had a dream in which Christ explained to him that he should use the sign against his enemies. Eusebius then continues to describe the labarum, the military standard used by Constantine in his later wars against Licinius, showing the Chi-Rho sign.
Those two accounts can hardly be reconciled with each other, though they have been merged in popular notion into Constantine seeing the Chi-Rho sign on the evening before the battle. Both authors agree that the sign was not readily understandable as denoting Christ, which corresponds with the fact that there is no certain evidence of the use of the letters chi and rho as a Christian sign before Constantine. Its first appearance is on a Constantinian silver coin from c. 317, which proves that Constantine did use the sign at that time, though not very prominently. He made extensive use of the Chi-Rho and the labarum only later in the conflict with Licinius.
The vision has been interpreted in a solar context (e.g. as a solar halo phenomenon), which would have been reshaped to fit with the Christian beliefs of the later Constantine.
An alternate explanation of the intersecting celestial symbol has been advanced by George Latura, which claims that Plato's visible god in "Timaeus" is in fact the intersection of the Milky Way and the Zodiacal Light, a rare apparition important to pagan beliefs that Christian bishops reinvented as a Christian symbol.
Eusebius' description of the labarum.
"A Description of the Standard of the Cross, which the Romans now call the Labarum."
"Now it was made in the following manner. A long spear, overlaid with gold, formed the figure of the cross by means of a transverse bar laid over it. On the top of the whole was fixed a wreath of gold and precious stones; and within this, the symbol of the Saviour’s name, two letters indicating the name of Christ by means of its initial characters, the letter P being intersected by X in its centre: and these letters the emperor was in the habit of wearing on his helmet at a later period. From the cross-bar of the spear was suspended a cloth, a royal piece, covered with a profuse embroidery of most brilliant precious stones; and which, being also richly interlaced with gold, presented an indescribable degree of beauty to the beholder. This banner was of a square form, and the upright staff, whose lower section was of great length, of the pious emperor and his children on its upper part, beneath the trophy of the cross, and immediately above the embroidered banner."
"The emperor constantly made use of this sign of salvation as a safeguard against every adverse and hostile power, and commanded that others similar to it should be carried at the head of all his armies."
Iconographic career under Constantine.
Among a number of standards depicted on the Arch of Constantine, which was erected, largely with fragments from older monuments, just three years after the battle, the labarum does not appear. A grand opportunity for just the kind of political propaganda that the Arch otherwise was expressly built to present was missed. That is if Eusebius' oath-confirmed account of Constantine's sudden, vision-induced, conversion can be trusted. Many historians have argued that in the early years after the battle the emperor had not yet decided to give clear public support to Christianity, whether from a lack of personal faith or because of fear of religious friction. The arch's inscription does say that the Emperor had saved the "res publica" ("by greatness of mind and by instinct [or impulse] of divinity"). As with his predecessors, sun symbolism – interpreted as representing "Sol Invictus" (the Unconquered Sun) or Helios, Apollo or Mithras – is inscribed on his coinage, but in 325 and thereafter the coinage ceases to be explicitly pagan, and Sol Invictus disappears. In his "Historia Ecclesiae" Eusebius further reports that, after his victorious entry into Rome, Constantine had a statue of himself erected, "holding the sign of the Savior [the cross] in his right hand." There are no other reports to confirm such a monument.
Whether Constantine was the first Christian emperor supporting a peaceful transition to Christianity during his rule, or an undecided pagan believer until middle age, strongly influenced in his political-religious decisions by his Christian mother St. Helena, is still in dispute among historians.
As for the labarum itself, there is little evidence for its use before 317. In the course of Constantine's second war against Licinius in 324, the latter developed a superstitious dread of Constantine's standard. During the attack of Constantine's troops at the Battle of Adrianople the guard of the labarum standard were directed to move it to any part of the field where his soldiers seemed to be faltering. The appearance of this talismanic object appeared to embolden Constantine's troops and dismay those of Licinius. At the final battle of the war, the Battle of Chrysopolis, Licinius, though prominently displaying the images of Rome's pagan pantheon on his own battle line, forbade his troops from actively attacking the labarum, or even looking at it directly.
Constantine felt that both Licinius and Arius were agents of Satan, and associated them with the serpent described in the Book of Revelation (). Constantine represented Licinius as a snake on his coins. 
Eusebius stated that in addition to the singular labarum of Constantine, other similar standards (labara) were issued to the Roman army. This is confirmed by the two labara depicted being held by a soldier on a coin of Vetranio (illustrated) dating from 350.
Later usage.
A later Byzantine manuscript indicates that a jewelled labarum standard believed to have been that of Constantine was preserved for centuries, as an object of great veneration, in the imperial treasury at Constantinople. The labarum, with minor variations in its form, was widely used by the Christian Roman emperors who followed Constantine I.
A miniature version of the labarum became part of the imperial regalia of Byzantine rulers, who were often depicted carrying it in their right hands.
The term "labarum" is used generally for any ecclesiastical banner, such as those carried in religious processions.
"The Holy Lavaro" were a set of early national Greek flags, blessed by the Greek Orthodox Church. Under these banners the Greeks united throughout the Greek Revolution (1821), a war of liberation waged against the Ottoman Empire.
Labarum also gives its name (Labaro) to a suburb of Rome adjacent to Prima Porta, one of the sites where the 'Vision of Constantine' is placed by tradition.
The Labarum is also used, within the North American higher education system, as the symbol for the National Fraternity of Alpha Chi Rho
Modern interpretations of Constantine's vision.
There are modern astronomical and astrological theories that lend credence to Eusebius's account. In 1948, Fritz Heiland, of the Zeiss planetarium at Jena, published his astronomical interpretation of Constantine's vision, that the fall of the year 312 was attended by an unusual spectacle: the syzygy or close alignment of three bright planets, Mars, Saturn and Jupiter, in the evening sky above the southwest horizon, positioned along a line within about 20 degrees of each other on the border of Capricorn and Sagittarius.
The Swedish geologist Jens Ormo and co-authors suggest that the account may have had its origins in Constantine's witnessing the daylight effects of a meteorite's descent through Earth's atmosphere, of which the impact he believes resulted in the Sirente crater situated in Sirente-Velino Regional Park in Abruzzo, Italy.

</doc>
<doc id="18385" url="http://en.wikipedia.org/wiki?curid=18385" title="Lactantius">
Lactantius

Lucius Caecilius Firmianus Lactantius was an early Christian author (c. 250 – c. 325) who became an advisor to the first Christian Roman emperor, Constantine I, guiding his religious policy as it developed, and tutor to his son.
Biography.
Lactantius, a Latin-speaking native of North Africa, was a pupil of Arnobius and taught rhetoric in various cities of the Eastern Roman Empire, ending in Constantinople. He wrote apologetic works explaining Christianity in terms that would be palatable to educated people who still practiced the traditional religions of the Empire, while defending Christian beliefs against the criticisms of Hellenistic philosophers. His "Divinae Institutiones" ("Divine Institutes") is an early example of a systematic presentation of Christian thought. He was considered somewhat heretical after his death, but Renaissance humanists took a renewed interest in him, more for his elaborately rhetorical Latin style than for his theology.
A translator of the "Divine Institutes" starts his introduction as follows:
Lactantius has always held a very high place among the Christian Fathers, not only on account of the subject-matter of his writings, but also on account of the varied erudition, the sweetness of expression, and the grace and elegance of style, by which they are characterized.
Lactantius was not born into a Christian family. In his early life, he taught rhetoric in his native town, which may have been Cirta in Numidia, where an inscription mentions a certain 'L. Caecilius Firmianus'.
Lactantius had a successful public career at first. At the request of the Roman Emperor Diocletian, he became an official professor of rhetoric in Nicomedia; the voyage from Africa is described in his poem "Hodoeporicum". There he associated in the imperial circle with the administrator and polemicist Sossianus Hierocles and the pagan philosopher Porphyry; here he will first have met Constantine, and Galerius, whom he cast as villain in the persecutions. Having converted to Christianity, he resigned his post before Diocletian's purging of Christians from his immediate staff and before the publication of Diocletian's first "Edict against the Christians" (February 24, 303). As a Latin "rhetor" he subsequently lived in poverty according to Saint Jerome and eked out a living by writing, until Constantine I became his patron. The new emperor appointed the aged scholar in 311 or 313. The friendship of the Emperor Constantine raised him from penury and he became tutor in Latin to his son Crispus, whom Lactantius may have followed to Trier in 317, when Crispus was made Caesar (lesser co-emperor) and sent to the city. Crispus was put to death in 326, but when Lactantius died and in what circumstances is not known.
Like so many of the early Christian authors, Lactantius depended on classical models. The early Humanists called him the "Christian Cicero" ("Cicero Christianus"). His works were copied in manuscript several times in the 15th century and first printed in 1465 by the Germans Arnold Pannartz and Konrad Sweynheim at the Abbey of Subiaco. This edition was the first book printed in Italy to have a date of printing, as well as the first use of a Greek alphabet font anywhere, which was apparently produced in the course of printing, as the early pages leave Greek text blank. It was probably the fourth book ever printed in Italy. A copy of this edition was sold at auction in 2000 for more than $1 million.
Prophetic exegesis.
In "The Divine Institutes," Lactantius expected an earthly reign of the resurrected saints with Jesus after His second advent for the thousand years before the universal judgment. He presented, in sharp chronological summary, the premillennial advent, the two resurrections, the millennial period, and the reign of the saints with Christ, with surprising astuteness, reflecting the unsettled doctrine of the time.
With the conversion of Constantine, the Christians were no longer persecuted, their adversaries were destroyed, and tranquility reigned. The world's favor, rather than its hatred, became the church's peril. Multitudes flocked into the church because it was now fashionable and the church, long comfortable to persecution and expected martyrdom, became worldly. New errors commingled with older ones, and with truth.
In the outline of Bible history, Lactantius dealt with the plan of salvation, the origin of sin, creation, probation in Eden, the fall, and the incarnation of Christ. He said that "as the end of this world approaches, the condition of human affairs must undergo a change, and through the prevalence of wickedness become worse."
Lactantius confidently stated that the beginning of the end would be the fall, or breakup, of the Roman Empire. He asserted that the Roman world would be divided into ten contemporaneous kingdoms, which would mark the beginning of disastrous times. After Rome's breakup the Antichrist would appear and after that the saints would take the kingdom. Three of the ten kingdoms would be destroyed by a powerful northern enemy that would harass the world, changing laws, assuming the government, and ruling with intolerance, oppressing mankind.
Lactantius linked the False Prophet, the Beast of Revelation 13, and the Antichrist as the same destructive power whose tyrannical rule will prevail for "forty-two months.” None of the fathers thus far had been more verbose on the subject of the millennial kingdom than Lactantius, or more particular in describing the times and events preceding and following. He held to the fundamental apostolic belief that the millennium originates with the second advent of Christ and marks the destruction of the wicked, the binding of the devil, and the raising of the righteous dead.
He depicted Jesus reigning with the resurrected righteous on this earth during the seventh thousand years, prior to the general judgment. The resurrected saints rule over the not yet glorified righteous who remain alive at the end. And they subject the survivors of the unregenerate nations to slavery. In the end the devil, having been bound during the thousand years, is loosed; the enslaved nations rebel against the righteous, who hide underground until the host attacking the Holy City are overwhelmed by fire and brimstone and mutual slaughter, and buried altogether by an earthquake - rather unnecessarily, it would seem, since the wicked are thereupon raised again to be sent into eternal punishment. Next, God renews the earth, after the punishment of the wicked, and the Lord alone is thenceforth worshiped in the renovated earth.

</doc>
<doc id="18386" url="http://en.wikipedia.org/wiki?curid=18386" title="Laconia">
Laconia

Laconia (Greek: Λακωνία), also known as Lacedaemonia, is a region in the southeastern part of the Peloponnese peninsula. Its administrative capital is Sparta. The word "laconic" is derived from the name of the region by analogy—to speak in a concise way, as the Spartans were reputed by the Athenians to do.
Geography.
Laconia is bordered by Messenia to the west and Arcadia to the north and is surrounded by the Myrtoan Sea to the east and by the Laconian Gulf and the Mediterranean Sea to the south. It encompasses Cape Malea and Cape Tainaron and a large part of the Mani Peninsula. The islands of Kythira and Antikythera lie to the south, but they administratively belong to the Attica regional unit of islands. The island, Elafonisos, situated between the Laconian mainland and Kythira, is part of Laconia.
The Evrotas is the longest river in the prefecture. The valley of the Evrotas is predominantly an agricultural region that contains many citrus groves, olive groves, and pasture lands. It is the location of the largest orange production in the Peloponnese and probably in all of Greece. Lakonia, a brand of orange juice, is based in Amykles.
The main mountain ranges are the Taygetus (2,407 m) in the west and the Parnon (1,961 m) in the northeast. Taygetus, known as Pentadaktylos ("five-fingers") throughout the Middle Ages, is west of Sparta and the Evrotas valley. It is the highest mountain in Laconia and the Peloponnese and is mostly covered with pine trees. Two roads join the Messenia and Laconia prefectures: one is a tortuous mountain pass through Taygetus and the other bypasses the mountain via the Mani district to the south.
The stalactite cave, Dirou, a major tourist attraction, is located south of Areopolis in the southwest of Laconia.
Climate.
Laconia has a Mediterranean climate with warm winters and hot summers. Snow is rare on the coast throughout the winter but is very common in the mountains.
Municipalities of Laconia.
The regional unit, Laconia, is subdivided into five municipalities. These are (number as in the map in the infobox):
Prefecture.
As a part of the 2011 Kallikratis government reform, regional unit Laconia was created out of the former prefecture Laconia (Greek: Νομός Λακωνίας). The prefecture had the same territory as the present regional unit. At the same time, the municipalities were reorganised, according to the table below.
Provinces.
"Note:" Provinces no longer hold any legal status in Greece.
Population.
The main cities and towns of Laconia are (ranked by 2011 census population):
History.
Ancient history.
In ancient Greece, this was the principal region of the Spartan state. Throughout classical antiquity, the Spartan sphere of influence expanded to Messenia, whose inhabitants (the Helots) were permanently enslaved. Significant archaeological recovery exists at the Vaphio-tomb site in Laconia. Found here are advanced Bronze Age art as well as evidence of cultural associations with the co-temperaneous Minoan culture on Crete. Laconia was at war with the Kingdom of Macedonia and saw several battles; at the end of the Mycenaean period, population of Laconia sharply declined. From the early-2nd century BC until 395, it was a part of the Roman Empire.
Medieval history.
In the medieval period, Laconia formed part of the Byzantine Empire. Following the Fourth Crusade, it was gradually conquered by the Frankish Principality of Achaea. In the 1260s, however, the Byzantines recovered Mystras and other fortresses in the region and managed to evict the Franks from Laconia, which became the nucleus of a new Byzantine province. By the mid-14th century, this evolved into the Despotate of Morea, held by the last Greek ruling dynasty, the Palaiologoi. With the fall of the Despotate to the Ottomans in 1460, Laconia was conquered as well.
Modern history.
With the exception of a 30-year interval of Venetian rule, Laconia remained under Ottoman control until the outbreak of the Greek War of Independence of 1821. Following independence, Sparta was selected as the capital of the modern prefecture, and its economy and agriculture expanded. With the incorporation of the British-ruled Ionian Islands into Greece in 1864, Elafonissos became part of the prefecture. After World War II and the Greek Civil War, its population began to somewhat decline, as people moved from the villages toward the larger cities of Greece and abroad.
In 1992, a devastating fire ruined the finest olive crops in the northern part of the prefecture, and affected the area of Sellasia along with Oinountas and its surrounding areas. Firefighters, helicopters and planes battled for days to put out the horrific fire.
The Mani portion along with Gytheio became famous in Greece for filming episodes of "Vendetta", broadcast on Mega Channel throughout Greece and abroad on Mega Cosmos.
In early 2006, flooding ruined olive and citrus crops as well as properties and villages along the Evrotas river. In the summer 2006, a terrible fire devastated a part of the Mani Peninsula, ruining forests, crops, and numerous villages.

</doc>
<doc id="18388" url="http://en.wikipedia.org/wiki?curid=18388" title="Laocoön">
Laocoön

Laocoön (; Ancient Greek: Λαοκόων, ]), the son of Acoetes, is a figure in Greek and Roman mythology and the Epic Cycle. He was a Trojan priest who was attacked, with his two sons, by giant serpents sent by the gods. Though not mentioned by Homer, the story of Laocoön had been the subject of a tragedy, now lost, by Sophocles and was mentioned by other Greek writers, though the events around the attack by the serpents vary considerably. The most famous account of these is now in Virgil's "Aeneid" where Laocoön was a priest of Poseidon (or Neptune for the Romans), who was killed with both his sons after attempting to expose the ruse of the Trojan Horse by striking it with a spear.
Virgil gives Laocoön the famous line "Equō nē crēdite, Teucrī / Quidquid id est, timeō Danaōs et dōna ferentēs", or "Do not trust the Horse, Trojans / Whatever it is, I fear the Greeks even bearing gifts." This line is the source of the saying: "Beware of Greeks bearing gifts."
In Sophocles, however, he was a priest of Apollo, who should have been celibate but had married. The serpents killed only the two sons, leaving Laocoön himself alive to suffer. In other versions he was killed for having committed an impiety by making love with his wife in the presence of a cult image in a sanctuary, or simply making a sacrifice in the temple with his wife present. In this second group of versions, the snakes were sent by Poseidon and in the first by Poseidon and Athena, or Apollo, and the deaths were interpreted by the Trojans as proof that the horse was a sacred object. The two versions have rather different morals: Laocoön was either punished for doing wrong, or for being right.
Death.
The most detailed description of Laocoön's grisly fate was provided by Quintus Smyrnaeus in "Posthomerica", a later, literary version of events following the "Iliad". According to Quintus, Laocoön begged the Trojans to set fire to the horse to ensure it was not a trick. Athena, angry with him and the Trojans, shook the ground around Laocoön's feet and painfully blinded him. The Trojans, watching this unfold, assumed Laocoön was punished for the Trojans' mutilating and doubting Sinon, the undercover Greek soldier sent to convince the Trojans to let him and the horse inside their city walls. Thus, the Trojans wheeled the great wooden Horse in. Laocoön did not give up trying to convince the Trojans to burn the horse, and Athena makes him pay even further. She sends two giant sea serpents to strangle and kill him and his two sons. In another version of the story, it was said that Poseidon sent the sea serpents to strangle and kill Laocoön and his two sons.
According to Apollodorus, it was Apollo who sent the two sea serpents. Laocoön had insulted Apollo by sleeping with his wife in front of the "divine image".
Virgil used the story in the "Aeneid." According to Virgil, Laocoön advised the Trojans to not receive the horse from the Greeks. They disregarded Laocoön's advice and were taken in by the deceitful testimony of Sinon. The enraged Laocoön threw his spear at the Horse in response. Minerva then sent sea-serpents to strangle Laocoön and his two sons, Antiphantes and Thymbraeus, for his actions. "Laocoön, ostensibly sacrificing a bull to Neptune on behalf of the city (lines 201ff.), becomes himself the tragic victim, as the simile (lines 223–24) makes clear. In some sense, his death must be symbolic of the city as a whole," S. V. Tracy notes. According to the Hellenistic poet Euphorion of Chalcis, Laocoön is in fact punished for procreating upon holy ground sacred to Poseidon; only unlucky timing caused the Trojans to misinterpret his death as punishment for striking the Horse, which they bring into the city with disastrous consequences. The episode furnished the subject of Sophocles' lost tragedy, "Laocoön".
In "Aeneid", Virgil describes the circumstances of Laocoön's death:
The death of Laocoön was famously depicted in a much-admired marble "Laocoön and his Sons", attributed by Pliny the Elder to the Rhodian sculptors Agesander, Athenodoros, and Polydorus, which stands in the Vatican Museums, Rome. Copies have been executed by various artists, notably Baccio Bandinelli. These show the complete sculpture (with conjectural reconstructions of the missing pieces) and can be seen in Rhodes, at the Palace of the Grand Master of the Knights of Rhodes, Rome, the Uffizi Gallery in Florence and in front of the Archaeological Museum, Odessa, Ukraine, amongst others.
The marble Laocoön provided the central image for Lessing's "Laocoön", 1766, an aesthetic polemic directed against Winckelmann and the comte de Caylus. Daniel Albright reengages the role of the figure of Laocoön in aesthetic thought in his book "Untwisting the Serpent: Modernism in Literature, Music, and Other Arts". [cite El Greco painting]
In addition to other literary references, John Barth employs a bust of Laocoön in his novella, "The End of the Road". The R.E.M. song "Laughing" references Laocoön, rendering him female ("Laocoön and her two sons"). The marble's pose is parodied in the comic book "Asterix and the Laurel Wreath". American author Joyce Carol Oates also references Laocoön in her 1989 novel "American Appetites". In Stave V of "A Christmas Carol", by Charles Dickens (1843), Scrooge awakes on Christmas morning, "making a perfect Laocoon of himself with his stockings". Barbara Tuchman's "The March of Folly" begins with an extensive analysis of the Laocoön story. American feminist poet and author Marge Piercy includes a poem titled, "Laocoön is the name of the figure", in her collection "Stone, Paper, Knife" (1983), relating love lost and beginning.
In Hector Berlioz opera "Les Troyens", the death of Laocoon is a pivotal moment of the first act after Aeneas entrance, sung by eight singers and a double choir ("ottetto et double chœur"). It begins with the verse "Châtiment effroyable" ("frightful punishment").
References.
Classical sources.
Compiled by Tracy, , which also mentions a fragmentary line possibly by Nicander.

</doc>
<doc id="18389" url="http://en.wikipedia.org/wiki?curid=18389" title="Limburg an der Lahn">
Limburg an der Lahn

Limburg an der Lahn (officially abbreviated "Limburg a. d. Lahn") is the district seat of Limburg-Weilburg in Hesse, Germany.
Geography.
Location.
Limburg lies in western Hesse between the Taunus and the Westerwald on the river Lahn.
The town lies roughly centrally in a basin within the Rhenish Slate Mountains which is surrounded by the low ranges of the Taunus and Westerwald and called the Limburg Basin ("Limburger Becken"). Owing to the favourable soil and climate, the Limburg Basin stands as one of Hesse's richest agricultural regions and moreover, with its convenient Lahn crossing, it has been of great importance to transport since the Middle Ages. Within the basin, the Lahn's otherwise rather narrow lower valley broadens out noticeably, making Limburg's mean elevation only 117 m above sea level.
Neighbouring communities.
Limburg forms, together with the town of Diez, a middle centre (in terms of Central place theory) but partially functions as an upper centre to western Middle Hesse.
Limburg's residential neighbourhoods reach beyond the town limits; the neighbouring centres of Elz and Diez run seamlessly together.
Surrounding towns and communities are the community of Elz and the town of Hadamar in the north, the community of Beselich in the northeast, the town of Runkel in the east, the communities of Villmar and Brechen in the southeast, the community of Hünfelden in the south (all in Limburg-Weilburg), the community of Holzheim in the southwest, and the town of Diez and the communities of Aull and Gückingen in the west (all in the Rhein-Lahn-Kreis in Rhineland-Palatinate).
The nearest major cities are Wetzlar and Gießen to the north east, Wiesbaden and Frankfurt to the south and Koblenz to the west.
Constituent communities.
The town consists of eight formerly autonomous "Ortsteile", listed here by population.
Likewise often called a constituent community is Blumenrod, although this is actually only a big residential neighbourhood in the main town’s south end. Its landmark is the "Domäne Blumenrod", a former manor house that has been restored and remodelled by the Limburg Free Evangelical community.
Limburg’s biggest outlying centre is Lindenholzhausen (3,329 residents as of June 2006); the second biggest is Linter.
Etymology.
The derivation of the name “Limburg” is not quite clear and may well hearken back to a castle built here ("Burg" means "castle" in German). In 910 the town was first mentioned as "Lintpurc". Two of the popular theories are:
History.
About 800, the first castle buildings arose on the Limburg crags. This was probably designed for the protection of a ford over the river Lahn. In the decades that followed, the town developed under the castle's protection. Limburg is first mentioned in documents in 910 under the name of "Lintpurc" when Louis the Child granted Konrad Kurzbold an estate in the community on which he was to build a church. Konrad Kurzbold laid the foundation stone for Saint George's Monastery Church, where he was also buried. The community soon increased in importance with the monastery's founding and profited from the lively goods trade on the "Via Publica".
In 1150, a wooden bridge was built across the Lahn. The long-distance road from Cologne to Frankfurt am Main subsequently ran through Limburg. In the early 13th century, Limburg Castle was built in its current form. Shortly afterwards, the town passed into the ownership of the Lords of Ysenburg. In 1214, the community was granted town rights. Remains of the fortification wall from the years 1130, 1230 and 1340 with a maxiumum length of roughly one thousand metres indicate to this day the blossoming town's quick development in the Middle Ages. There is proof of a mint in Limburg in 1180.
One line of the Lords of Ysenburg resided from 1258 to 1406 at Limburg Castle and took their name from their seat, Limburg. From this line came the House of Limburg-Stirum and also Imagina of Isenburg-Limburg, German King Adolf's wife.
The ruling class among the mediaeval townsfolk were rich merchant families whose houses stood right near the castle tower and were surrounded by the first town wall once it was built. The area of today's Rossmarkt ("Horse Market"), in which many simple craftsmen lived, was only brought within the fortifications once the second town wall was built. The inhabitants there, however, unlike the merchant élite, were accorded no entitlement to a voice in town affairs and were not allowed to send representatives to the town council. Nevertheless, they had to bear the main financial burden of running the town. Only in 1458 were they allowed to send two representatives to town council.
Saint George's Cathedral ("Sankt-Georgs-Dom") built on the old monastery church's site, and also called "Georgsdom", was consecrated in 1235. On 14 May 1289, a devastating fire wiped out great parts of the inner town, although these were subsequently rebuilt. One of the houses built at that time was the Römer 2-4-6, which is today one of Germany's oldest half-timbered houses. In 1337, Limburg's Jews were expelled from the town. Only in 1341 were they once again able to settle in the town, by royal decree. In 1344 a half share of the town was pledged to the Electorate of Trier, and in 1420, the town passed wholly into the ownership of Trier. This event, along with another town fire in 1342, the Black Death in 1349, 1356 and 1365, but above all the rise of the Territorial Princes, led to a gradual decline. In 1315 and 1346, the old stone Lahn Bridge was built (presumably in two sections).
Against the background of the German Peasants' War, unrest also arose among the townsfolk in 1525. After the Elector of Trier had demanded that the townsmen turn a Lutheran preacher out of the town, a board made up of townsmen who were ineligible for council functions handed the council a 30-point comprehensive list of demands on 24 May. It dealt mainly with financial participation and equality in taxation, trade and building issues with the merchant class. In the days that followed, these demands were reduced in negotiations between the council and the board to 16 points, which were likely also taken up with the Elector afterwards. On 5 August, however, Archbishop Richard ordered the council to overturn all concessions to the townsmen. Furthermore, a ban on assembly was decreed, and the ineligible townsmen were stripped of their right to send two representatives to council.
In 1806, Limburg came into the possession of the newly founded Duchy of Nassau. In 1818 the town wall was torn down. In 1827 the town was raised to a Catholic episcopal seat. In 1866 the Duchy and with it Limburg passed to Prussia in the wake of the Austro-Prussian War. As of 1862, Limburg became a railway hub and from 1886 a district seat. In 1892, the Pallottines settled in town, but only the men; the women came in 1895.
During World War I there was a major prisoner of war camp at Limburg an der Lahn. Many Irish members of the British Army were interned there until the end of the war and at one stage they were visited by the Irish republican leader Roger Casement in an attempt to win recruits for the forthcoming Irish rebellion.
From 1919 to 1923, Limburg was the "capital" of a short-lived state called Free State Bottleneck (or "Freistaat Flaschenhals" in German) because it was the nearest unoccupied town to the Weimar Republic.
Politics.
Town council.
The municipal election held on 27 March 2011 yielded the following results:
Mayor.
The town's mayor is currently Martin Richard (CDU).
"Patenschaft".
In 1956, a "Patenschaft" – roughly, a sponsorship – was undertaken for Sudeten Germans driven out of the town of Mährisch Neustadt in the Sternberg district.
Economy and infrastructure.
Transport.
Limburg is a traditional transport hub. Already in the Middle Ages, the "Via Publica" crossed the navigable Lahn here. Today the A 3 (Emmerich–Oberhausen–Cologne–Frankfurt–Nuremberg–Passau) and "Bundesstraße" 8, which both follow the "Via Publica's" alignment as closely as possible, run through the town. "Bundesstraße" 49 links Limburg to Koblenz towards the west and Wetzlar and Gießen towards the east. The section between Limburg and Wetzlar is currently being widened to four lanes. This section as far as Obertiefenbach is also known as "Die lange Meil" ("The Long Mile"). "Bundesstraße" 54 links Limburg on the one hand with Siegen to the north and on the other by way of Diez with Wiesbaden, which may likewise be reached over "Bundesstraße" 417 ("Hühnerstraße").
As early as 1248, a wooden bridge spanned the Lahn, but was replaced after the flooding in 1306 by a stone bridge, the "Alte Lahnbrücke". Other road bridges are the "Lahntalbrücke Limburg" (1964) on the A 3, the "Lahnbrücke" near Staffel and the "Neue Lahnbrücke" from 1968, over which run the "Bundesstraßen" before they cross under the inner town through the "Schiedetunnel", a bypass tunnel.
Once the "Lahntalbahn" had been built, Limburg was joined to the railway network in 1862. Limburg railway station developed into a transport hub. Eschhofen station is also in Limburg. Other railway lines are the "Unterwesterwaldbahn", the "Oberwesterwaldbahn" and the Main-Lahn Railway. At Niedernhausen station on the "Main-Lahn Railway", transfer to the "Ländchesbahn" to Wiesbaden is possible. With the exception of the upper section of the "Lahntalbahn" and express lines to Koblenz and Frankfurt, which are still served by Deutsche Bahn, all railway lines are run by Vectus Verkehrsgesellschaft mbH, based in Limburg.
Once the InterCityExpress Cologne-Frankfurt high-speed rail line had been built, Limburg acquired an ICE station. It is the only railway station in Germany at which exclusively ICE trains stop. The high-speed rail line crosses the Lahn over the "Lahntalbrücke" and then dives into the "Limburger Tunnel".
The nearest airport is Frankfurt Airport, 63 km away on the A 3. Travel time there on the ICE is roughly 20 minutes. Cologne Bonn Airport is 110 km away and can be reached on the ICE in 44 minutes.
The Lahn between Lahnstein and Wetzlar is a "Bundeswasserstraße" ("Federal waterway"). Since the "Lahntalbahn's" expansion, however, the waterway's importance has been declining. It is used mainly by tourists with small motorboats, canoes and rowboats. Limburg is the landing site of the tourboat "Wappen von Limburg".
Public institutions.
Education.
Limburg has four schools which lead to, among other qualifications, the Abitur:
Professional training schools:
Hauptschulen and Realschulen:
Libraries:
St. Vincenz Hospital.
The hospital perched on the Schafsberg overlooking the town has at its disposal 433 beds and 15 specialist departments.
Sport and leisure.
In Limburg there are various sport clubs; some are even represented in "Bundesligen", and even at the world level.
Youth meeting place in Limburg.
The Evangelical Church offers with its "Jugendfreizeitstätte Limburg" (JFS for short, meaning "Youth Leisure Place") a meeting place for youth with many events. With table football, Internet café and many events, this institution is not only church-based, with two staff and a "Zivildienstleistender" supporting the visitors not only with their problems.
Limburg Mothers' Centre.
The "Mütterzentrum Limburg" is a family meeting place for those with or without children on Hospitalstraße. The club is supported by the town of Limburg and the "Bundesland" of Hesse and offers among other things a parents' service that looks after children, a broad array of course offerings for children and adults, a miniature kindergarten and a café.
Culture and sightseeing.
Theatre.
The cabaret troupe "Thing", founded more than 25 years ago, moved after a short time from its initial home in the outlying centre of Staffel to the Josef-Kohlmaier-Halle, a civic event hall, where its stage can now be found in the hall's club rooms. The troupe is run by an independent acting club. On the programme are chanson, cabaret, literature and jazz as well as folk, Rock and performances by singer-songwriters. It makes a point of furthering young artists. Each month, three or four events are staged.
The dedication of "Thing" was recognized on 6 December 2003 when the "Kulturpreis Mittelhessen" ("Middle Hesse Culture Prize") was awarded to it.
Limburg Cathedral has a famous boys' choir, the "Limburger Domsingknaben", although they are actually based at the "Musical Boarding School" in Hadamar just outside Limburg.
Museums.
In Limburg there are several museums. The most important are:
Buildings.
Only a few towns, like Limburg, have been able to keep a full set of nearly unscathed mediaeval buildings. The formerly walled town core between St. George’s Cathedral, Grabenstraße (a street marking the old town moat) and the 600-year-old Lahn Bridge thus stands today as a whole under monumental protection.
The "Altstadt" ("Old Town") boasts a fine cathedral and is full of narrow streets with timber-frame houses, dating mainly from the 17th and 18th centuries. That's why it is located on the German Timber-Frame Road.

</doc>
<doc id="18390" url="http://en.wikipedia.org/wiki?curid=18390" title="Lavrentiy Beria">
Lavrentiy Beria

Lavrentiy Pavlovich Beria (Georgian: ლავრენტი პავლეს ძე ბერია, "Lavrenti Pavles dze Beria"; Russian: Лавре́нтий Па́влович Бе́рия; 29 March 1899 – 23 December 1953) was a Soviet politician of Georgian ethnicity, Marshal of the Soviet Union and state security administrator, chief of the Soviet security and secret police apparatus (NKVD) under Joseph Stalin during World War II, and Deputy Premier in the postwar years (1946–53).
Beria was the longest-lived and most influential of Stalin's secret police chiefs, wielding his most substantial influence during and after World War II. He simultaneously administered vast sections of the Soviet state and served as "de facto" Marshal of the Soviet Union in command of the NKVD field units responsible for anti-partisan operations on the Eastern Front during World War II, as well as for acting as barrier troops and the apprehension of thousands of "turncoats, deserters, cowards and suspected malingerers." Beria administered the vast expansion of the Gulag labor camps and was primarily responsible for overseeing the secret defense institutions known as sharashkas, critical to the war effort. He also played the decisive role in coordinating the Soviet partisans, developing an impressive intelligence and sabotage network behind German lines. He attended the Yalta Conference with Stalin, who introduced him to U.S. President Franklin D. Roosevelt as "our Himmler". After the war, he organized the communist takeover of the state institutions of Central and Eastern Europe. Beria's uncompromising ruthlessness in his duties and skill at producing results culminated in his success in overseeing the Soviet atomic bomb project. Stalin gave it absolute priority and the project was completed in under five years in no small part due to Soviet espionage against the West organized by Beria's NKVD.
Upon Stalin's death in March 1953, Beria was promoted to First Deputy Premier, where he carried out a campaign of liberalization. He was briefly a part of the ruling "troika" with Georgy Malenkov and Vyacheslav Molotov. Beria's overconfidence in his position after Stalin's death led him to misjudge other Politburo members. During the coup d'état led by Nikita Khrushchev and assisted by the military forces of Marshal Georgy Zhukov, Beria was arrested on charges of treason during a meeting in which the full Politburo condemned him. The compliance of the NKVD was ensured by Zhukov's troops, and after interrogation Beria was taken to the basement of the Lubyanka and shot by General Pavel Batitsky.
Early life and rise to power.
Beria was born in Merkheuli, near Sukhumi, in the Sukhumi district of Kutaisi Governorate (now Gulripshi District, Georgia, then part of the Russian Empire). He was a member of the Mingrelian ethnic group and grew up in a Georgian Orthodox family. Beria's mother, Marta Jaqeli (1868–1955), was a deeply religious, church-going woman (she spent much time in church and died in a church building); she was previously married and widowed before marrying Beria's father, Pavel Khukhaevich Beria (1872–1922), a landowner from Abkhazia. He also had a brother (name unknown), and a sister named Anna, who was born deaf-mute. In his autobiography, Lavrentiy Beria mentioned only his sister and his niece, implying that his brother (or any other siblings for that matter) either was dead or had no relationship with Beria after he left Merkheuli. Beria attended a technical school in Sukhumi, and joined the Bolsheviks in March 1917 while a student in the Baku Polytechnicum (subsequently known as the Azerbaijan State Oil Academy). As a student, Beria distinguished himself in mathematics and the sciences. The Polytechnicum's curriculum concentrated on the petroleum industry.
Beria also worked for the anti-Bolshevik Mussavatists in Baku. After the city's capture by the Red Army (28 April 1920), Beria was saved from execution only because there was likely little arrangement time and Sergei Kirov had possibly intervened. While in prison, he fell in love with Nina Gegechkori (1905–10 June 1991), his cellmate's niece, and they eloped on a train. She was 17, a trained scientist from an aristocratic family.
In 1919, at the age of twenty, Beria started his career in state security when the security service of the Azerbaijan Democratic Republic hired him while still a student at the Polytechnicum. In 1920 or 1921 (accounts vary), Beria joined the Cheka – the original Bolshevik secret police. At that time, a Bolshevik revolt took place in the Menshevik-controlled Democratic Republic of Georgia, and the Red Army subsequently invaded. The Cheka became heavily involved in the conflict, which resulted in the defeat of the Mensheviks and the formation of the Georgian SSR. By 1922, Beria was deputy head of the Georgian branch of Cheka's successor, the OGPU.
In 1924 he led the repression of a Georgian nationalist uprising, after which up to 10,000 people were executed. For this display of "Bolshevik ruthlessness," Beria was appointed head of the "secret-political division" of the Transcaucasian OGPU and was awarded the Order of the Red Banner.
In 1926 Beria became head of the Georgian OGPU; Sergo Ordzhonikidze, head of the Transcaucasian party, introduced him to fellow-Georgian Joseph Stalin. As a result, Beria became an ally in Stalin's rise to power. During his years at the helm of the Georgian OGPU, Beria effectively destroyed the intelligence networks that Turkey and Iran had developed in the Soviet Caucasus, while successfully penetrating the governments of these countries with his agents. He also took over Stalin's holiday security.
Beria was appointed Secretary of the Communist Party in Georgia in 1931, and for the whole Transcaucasian region in 1932. He became a member of the Central Committee of the Communist Party in 1934. During this time, he began to attack fellow members of the Georgian Communist Party, particularly Gaioz Devdariani, who served as Minister of Education of the Georgian SSR. Beria ordered the executions of Devdariani's brothers George and Shalva, who held important positions in the Cheka and the Communist Party respectively.
By 1935 Beria had become one of Stalin's most trusted subordinates. He cemented his place in Stalin's entourage with a lengthy oration titled, "On the History of the Bolshevik Organisations in Transcaucasia" (later published as a book), which emphasized Stalin's role. When Stalin's purge of the Communist Party and government began in 1934 after the assassination of Leningrad party boss Sergei Kirov (1 December 1934), Beria ran the purges in Transcaucasia. He used the opportunity to settle many old scores in the politically turbulent Transcaucasian republics.
In June 1937 he said in a speech, "Let our enemies know that anyone who attempts to raise a hand against the will of our people, against the will of the party of Lenin and Stalin, will be mercilessly crushed and destroyed."
Head of the NKVD.
In August 1938, Stalin brought Beria to Moscow as deputy head of the People's Commissariat for Internal Affairs (NKVD), the ministry which oversaw the state security and police forces. Under Nikolai Yezhov, the NKVD carried out the Great Purge: the imprisonment or execution of millions of people throughout the Soviet Union as alleged "enemies of the people." By 1938, however, the oppression had become so extensive that it was damaging the infrastructure, economy and even the armed forces of the Soviet state, prompting Stalin to wind the purge down. Stalin had thoughts to appoint Lazar Kaganovich as head of the NKVD, but chose Beria probably because he was a professional secret policeman. In September, Beria was appointed head of the Main Administration of State Security (GUGB) of the NKVD, and in November he succeeded Yezhov as NKVD head (Yezhov was executed in 1940). The NKVD was purged next, with half its personnel replaced by Beria loyalists, many of them from the Caucasus.
Although Beria's name is closely identified with the Great Purge because of his activities while deputy head of the NKVD, his leadership of the organisation marked an easing of the repression begun under Yezhov. Over 100,000 people were released from the labour camps. The government officially admitted that there had been some injustice and "excesses" during the purges, which were blamed entirely on Yezhov. The liberalisation was only relative: arrests and executions continued, and in 1940, as war approached, the pace of the purges again accelerated. During this period, Beria supervised deportations of people identified as political enemies from Poland and the Baltic states after Soviet occupation of those regions.
In March 1939, Beria became a candidate member of the Communist Party's Politburo. Although he did not become a full member until 1946, he was already one of the senior leaders of the Soviet state. In 1941 Beria was made a Commissar General of State Security, the highest quasi-military rank within the Soviet police system of that time, effectively comparable to a Marshal of the Soviet Union.
On 5 March 1940, after the Gestapo–NKVD Third Conference was held in Zakopane, Beria sent a note (no. 794/B) to Stalin in which he stated that the Polish prisoners of war kept at camps and prisons in western Belarus and Ukraine were enemies of the Soviet Union, and recommended their execution. Most of them were military officers, but there were also intelligentsia, doctors, and priests and others for a total of over 22,000. With Stalin's approval, Beria's NKVD executed them in what became known as the Katyn massacre.
From October 1940 to February 1942, the NKVD under Beria carried out a new purge of the Red Army and related industries. In February 1941, Beria became Deputy Chairman of the Council of People's Commissars, and in June, following Nazi Germany's invasion of the Soviet Union, he became a member of the State Defense Committee (GKO). During World War II, he took on major domestic responsibilities and mobilized the millions of people imprisoned in NKVD Gulag camps into wartime production. He took control of the manufacture of armaments, and (with Georgy Malenkov) aircraft and aircraft engines. This was the beginning of Beria's alliance with Malenkov, which later became of central importance.
In 1944, as the Germans were driven from Soviet soil, Beria was in charge of dealing with the various ethnic minorities accused of anti-sovietism and/or collaboration with the invaders, including the Chechens, the Ingush, the Crimean Tatars, the Pontic Greeks and the Volga Germans. All these groups were deported to Soviet Central Asia (see "Population transfer in the Soviet Union.")
In December 1944, Beria's NKVD was assigned to supervise the Soviet atomic bomb project ("Task No. 1"), which built and tested a bomb by 29 August 1949. In this capacity, he ran the successful Soviet espionage campaign against the atomic weapons program of the United States, which obtained much of the technology required. His most important contribution was to provide the necessary workforce for this project, which was extremely labour-intensive. At least 330,000 people, including 10,000 technicians, were involved. The Gulag system provided tens of thousands of people for work in uranium mines and for the construction and operation of uranium processing plants. They also constructed test facilities, such as those at Semipalatinsk and in the Novaya Zemlya archipelago. The NKVD also ensured the necessary security for the project. Amazingly, the physicist Pyotr Kapitsa refused to work with Beria even after he gave him a hunting rifle as a gift. It is notable that Stalin backed Kapitsa in this quarrel.
In July 1945, as Soviet police ranks were converted to a military uniform system, Beria's rank was officially converted to that of Marshal of the Soviet Union. Although he had never held a traditional military command, Beria made a significant contribution to the victory of the Soviet Union in World War II through his organization of wartime production and his use of partisans. Stalin personally never thought much of it, and neither commented publicly on his performance nor awarded him recognition (i.e. Order of Victory) as he did for most other Soviet Marshals.
Postwar politics.
With Stalin nearing 70, the post-war years were dominated by a concealed struggle for succession among his supporters. At the end of the war, the most likely successor seemed to be Andrei Zhdanov, party leader in Leningrad during the war, who was in charge of all cultural matters by 1946. After 1946 Beria formed an alliance with Malenkov to counter Zhdanov's rise.
In January 1946, Beria resigned as chief of the NKVD while retaining general control over national security matters as Deputy Prime Minister and Curator of the Organs of State Security under Stalin, but the new chief, Sergei Kruglov, was not a Beria man. Also, by the summer of 1946, Beria's man Vsevolod Nikolayevich Merkulov was replaced as head of the Ministry for State Security (MGB) by Viktor Abakumov. Abakumov was the head of SMERSH from 1943 to 1946; his relationship with Beria was marked by close collaboration (since Abakumov owed his rise to Beria's support and esteem), but also by rivalry. Stalin had begun to encourage Abakumov to form his own network inside the MGB to counter Beria's dominance of the power ministries. Kruglov and Abakumov moved expeditiously to replace Beria's men in the security apparatus leadership with new people. Very soon Deputy Minister Stepan Mamulov of the Soviet Ministry of Internal Affairs was the only close Beria ally left outside foreign intelligence, on which Beria kept a grip. In the following months, Abakumov started carrying out important operations without consulting Beria, often working in tandem with Zhdanov, and sometimes on Stalin's direct orders. Some observers argue that these operations were aimed – initially tangentially, but with time more directly – at Beria.
One of the first such moves was the Jewish Anti-Fascist Committee affair that commenced in October 1946 and eventually led to the murder of Solomon Mikhoels and the arrest of many other members. This affair damaged Beria; not only had he championed the creation of the committee in 1942, but his own entourage included a substantial number of Jews.
After Zhdanov died suddenly in August 1948, Beria and Malenkov consolidated their power by a purge of Zhdanov's associates known as the "Leningrad Affair." Among the executed were Zhdanov's deputy, Aleksei Kuznetsov; the economic chief, Nikolai Voznesensky; the Party head in Leningrad, Pyotr Popkov; and the Prime Minister of the Russian Republic, Mikhail Rodionov. It was only after Zhdanov's death that Nikita Khrushchev began to be considered as a possible alternative to the Beria-Malenkov axis.
During the postwar years, Beria supervised the successful establishment of Communist regimes in the countries of Eastern Europe, usually by coup d'etat, and hand-picked the leaders. Starting in 1948, Abakumov initiated several investigations against these leaders, which culminated with the arrest in November 1951 of Rudolf Slánský, Bedřich Geminder, and others in Czechoslovakia. These men were generally accused of Zionism and cosmopolitanism, but, more specifically, of providing weapons to Israel. Beria was deeply disturbed by these charges, as large amounts of Czech arms had been sold to Israel on his direct orders. Beria wanted an alliance with Israel to advance the communist cause in the Middle East, while later Soviet leaders chose instead to form a powerful alliance with countries in the Arab World. Altogether, 14 Czechoslovak Communist leaders, 11 of them Jewish, were tried, convicted, and executed (see Slánský trial). Similar investigations in Poland and other Soviet satellite countries occurred at the same time.
Around that time, Abakumov was replaced by Semyon Ignatyev, who further intensified the anti-Semitic campaign. On 13 January 1953, the biggest anti-semitic affair in the Soviet Union was initiated with an article in "Pravda" that began what came to be known as the Doctors' plot, in which a number of the country's prominent Jewish physicians were accused of poisoning top Soviet leaders and arrested. Concurrently, an anti-semitic propaganda campaign, euphemistically termed the "struggle against rootless cosmopolitanism," occurred in the Soviet press. Initially, 37 men were arrested, but the number quickly grew into hundreds. Scores of Soviet Jews were dismissed from their jobs, arrested, sent to the Gulag, or executed. It is alleged that at this time on Stalin's orders the MGB started to prepare to deport all Soviet Jews to the Russian Far East or even massacre them. The issue of how much Stalin (and Beria) were involved in the Doctor's Plot is still disputed (see discussion in Doctors' plot article). Some historians claim that no such deportation was planned, or that the planned deportations were in an early planning stage when abandoned. Days after Stalin's death on 5 March, Beria freed all the arrested doctors, announced that the entire matter was fabricated, and arrested the MGB functionaries directly involved.
In other international issues, Beria (along with Mikoyan) correctly foresaw the victory of Mao Zedong in the Chinese Civil War and greatly helped the communist success by letting the Communist Party of China use Soviet-occupied Manchuria as a staging area and arranging huge weapons shipments to the People's Liberation Army, mainly from the recently captured equipment of the Japanese Kwantung Army.
Stalin's death.
Khrushchev wrote in his memoirs that Beria had, immediately after Stalin's stroke, gone about "spewing hatred against [Stalin] and mocking him." When Stalin showed signs of consciousness, Beria dropped to his knees and kissed his hand. When Stalin fell unconscious again, Beria immediately stood and spat.
Stalin's aide Vasili Lozgachev reported that Beria and Malenkov were the first members of the Politburo to investigate Stalin's condition after his stroke. They arrived at Stalin's dacha at Kuntsevo at 3am on March 2 after being called by Khrushchev and Bulganin. The latter did not want to risk Stalin's wrath by checking themselves. Lozgachev tried in futility to explain to Beria that the then-unconscious Stalin (still in his soiled clothing) was "sick and needed medical attention." Beria angrily dismissed his claims as panic-mongering and quickly left, ordering him, "Don't bother us, don't cause a panic and don't disturb Comrade Stalin!" Calling a doctor was deferred for a full 12 hours after Stalin was rendered paralyzed, incontinent, and unable to speak. This decision is noted as "extraordinary" by Sebag-Montefiore, but also consistent with the standard Stalinist policy of deferring all decision-making (no matter how necessary or obvious) without official orders from higher authority. Beria's decision to avoid immediately calling a doctor was silently supported (or at least not opposed) by the rest of the Politburo, which was rudderless without Stalin's micromanagement and paralyzed by a legitimate fear he would suddenly recover and wreak violent reprisal on anyone who had dared to act without his orders. Stalin's suspicion of doctors in the wake of the Doctors' Plot was well known. At the time of his stroke, his private physician was already being tortured in the basement of the Lubyanka for suggesting the leader required more bed rest.
After Stalin's stroke, Beria claimed to have killed him. This aborted a final purge of Old Bolsheviks Anastas Mikoyan and Vyacheslav Molotov for which Stalin had been laying the groundwork in the year prior to his death. Shortly after Stalin's death, Beria announced triumphantly to the Politburo that he had "done [Stalin] in" and "saved [us] all", according to Molotov's memoirs. Notably, Beria never explicitly stated whether he had "initiated" Stalin's stroke or had merely delayed his treatment in the hope he would die (as argued by Sebag-Montefiore and consistent with evidence). Support for the assertion that Stalin was poisoned with warfarin by Beria's associates has been presented from several sources, including Edvard Radzinsky in his biography "Stalin" and a recent study by Miguel A. Faria in the journal Surgical Neurology International. Warfarin (4-Hydroxycoumarins) is cited as the likely agent; it would have produced the symptoms reported, and administering it into Stalin's food or drink was well within the operational abilities of Beria's NKVD. Sebag-Montefiore does not dispute the possibility of an assassination by poison masterminded by Beria, whose hatred for Stalin was palpable by this point, but also notes that Beria never made mention of poison or confessed to using it, even during his later interrogations, and was never alone with Stalin during the period prior to his stroke (he always went with Malenkov to defer suspicion).
After Stalin's death from pulmonary edema brought on by the stroke, Beria's ambitions sprang into full force. In the uneasy silence following the cessation of Stalin's last agonies, Beria was the first to dart forward to kiss his lifeless form (a move likened by Sebag-Montefiore to "wrenching a dead King's ring off his finger"). While the rest of Stalin's inner circle (even Molotov, saved from certain liquidation) stood sobbing unashamedly over the body, Beria reportedly appeared "radiant", "regenerated", and "glistening with ill-concealed relish." When Beria left the room, he broke the somber atmosphere by shouting loudly for his driver, his voice echoing with what Stalin's daughter Svetlana Alliluyeva called "the ring of triumph unconcealed." Alliluyeva noticed how the Politburo seemed openly frightened of Beria and unnerved by his bold display of ambition. "He's off to take power," Mikoyan recalled muttering to Khrushchev. That prompted a "frantic" dash for their own limousines to intercept him at the Kremlin.
Downfall.
After Stalin's death, Beria was appointed First Deputy Premier and reappointed head of the MVD, which he merged with the MGB. His close ally Malenkov was the new Prime Minister and initially the most powerful man in the post-Stalin leadership. Beria was second most powerful, and given Malenkov's personal weakness, was poised to become the power behind the throne and ultimately leader himself. Khrushchev became Party Secretary. Voroshilov became Chairman of the Presidium of the Supreme Soviet (i.e., the head of state).
Given his record, it is not surprising that the other Party leaders were suspicious of Beria's motives. Khrushchev opposed the alliance between Beria and Malenkov, but he was initially unable to challenge them. His opportunity came in June 1953 when a spontaneous uprising against the East German Communist regime broke out in East Berlin.
Based on Beria's own statements, other leaders suspected that in the wake of the uprising, he might be willing to trade the reunification of Germany and the end of the Cold War for massive aid from the United States, as had been received in World War II. The cost of the war still weighed heavily on the Soviet economy. Beria craved the vast financial resources that another (more sustained) relationship with the United States could provide. For example, Beria gave Estonia, Latvia and Lithuania serious prospects of national autonomy, possibly similarly to other Soviet satellite states in Europe.
The East German uprising convinced Molotov, Malenkov, and Nikolai Bulganin that Beria's policies were dangerous and destabilizing to Soviet power. Within days of the events in Germany, Khrushchev persuaded the other leaders to support a Party "coup" against Beria; Beria's principal ally Malenkov abandoned him.
Arrest, trial and execution.
On 26 June 1953, Beria was arrested and held in an undisclosed location near Moscow. Accounts of Beria's fall vary considerably. By the most likely account, Khrushchev prepared an elaborate ambush, convening a meeting of the Presidium on 26 June, where he suddenly launched a scathing attack on Beria, accusing him of being a traitor and spy in the pay of British intelligence. Beria was taken completely by surprise. He asked, "What's going on, Nikita Sergeyevich? Why are you picking fleas in my trousers?" Molotov and others quickly spoke against Beria one after the other, followed by a motion by Khrushchev for his instant dismissal. When Beria finally realized what was happening and plaintively appealed to Malenkov to speak for him, his old friend and crony silently hung his head and refused to meet his gaze. Malenkov pressed a button on his desk as the pre-arranged signal to Marshal Georgy Zhukov and a group of armed officers in a nearby room. They burst in and arrested Beria.
Beria was taken first to the Moscow guardhouse and then to the bunker of the headquarters of Moscow Military District. Defence Minister Nikolai Bulganin ordered the Kantemirovskaya Tank Division and Tamanskaya Motor Rifle Division to move into Moscow to prevent security forces loyal to Beria from rescuing him. Many of Beria's subordinates, proteges and associates were also arrested, among them Vsevolod Merkulov, Bogdan Kobulov, Sergey Goglidze, Vladimir Dekanozov, Pavel Meshik, and Lev Vlodzimirskiy. "Pravda" did not announce Beria's arrest until 10 July, crediting it to Malenkov and referring to Beria's "criminal activities against the Party and the State."
Beria and the others were tried by a special session ("Spetsialnoye Sudebnoye Prisutstvie") of the Supreme Court of the Soviet Union on 23 December 1953 with no defense counsel and no right of appeal. Marshal Ivan Konev was the chairman of the court.
Beria was found guilty of:
Beria and all the other defendants were sentenced to death. When the death sentence was passed, Beria pleaded on his knees for mercy before collapsing to the floor and wailing and crying energetically, but to no avail: the other six defendants were executed by firing squad on 23 December 1953, the same day as the trial, while Beria was fatally shot through the forehead by General Batitsky after the latter stuffed a rag into Beria's mouth to silence his bawling. The body of Lavrentiy Pavlovich Beria was subsequently cremated. The remains were buried in a forest near Moscow.
Sexual predator.
At Beria's trial in 1953, it became known that he was the subject of a significant number of rape and sexual assault allegations. But subsequent research - using available historical accounts - could not conclusively prove the veracity of the stories beyond the fact Beria had a notorious reputation that "almost certainly had some foundation." The charges of sexual abuse and rape were always disputed by people close to him, including his wife Nina, son Sergo, and Pavel Sudoplatov, the former chief of Soviet foreign intelligence. Sudoplatov said Beria worked extremely long hours and had "exceptional self-control." In a 1990 interview, Beria's wife Nina said: "Lavrentii was busy working day and night. When did he have time for love with this legion of women?"
However in 2003 his cases files in the Soviet archives were opened. They recorded he had committed "dozens" of sexual assaults during the years he was NKVD chief. Simon Sebag-Montefiore, a biographer of Stalin, concluded the information "reveals a sexual predator who used his power to indulge himself in obsessive depravity."
The records contained the official testimony from Colonel R.S. Sarkisov and Colonel V. Nadaraia, two of Beria's most senior NKVD bodyguards. They stated that on warm nights during the war years, Beria was often driven slowly through the streets of Moscow in his armored Packard limousine. He would point out young women to be detained and escorted to his mansion where wine and a feast awaited them. After dining, Beria would take the women into his soundproofed office and rape them. Beria's bodyguards reported that their orders included handing each victim a flower bouquet as she left Beria's house. The implication being that to accept made it consensual; refusal would mean arrest. In one incident his chief bodyguard, Sarkisov, reported that a woman who had been brought to Beria rejected his advances and ran out of his office; Sarkisov mistakenly handed her the flowers anyway prompting the enraged Beria to declare "Now it's not a bouquet, it's a wreath! May it rot on your grave!" The woman was arrested by the NKVD the next day.
Women also submitted to Beria's sexual advances in exchange for the promise of freeing their relatives from the Gulag. In one case, Beria picked up Tatiana Okunevskaya - a well-known Soviet actress - under the pretence of bringing her to perform for the Politburo. Instead he took her to his dacha where he offered to free her father and grandmother from NKVD prison if she submitted. He then raped her telling her "scream or not, it doesn't matter." Yet Beria already knew her relatives had been executed months earlier. Okunevskaya was arrested shortly afterwards and sentenced to solitary confinement in the Gulag from which she survived.
Beria's sexually predatory nature was well-known to the Politburo, and though Stalin took an indulgent viewpoint (considering Beria's wartime importance), he said, "I don't trust Beria." In one instance when Stalin learned his daughter was alone with Beria at his house, he telephoned her and told her to leave immediately. When Beria complimented Alexander Poskrebyshev's daughter on her beauty, Poskrebyshev quickly pulled her aside and instructed her, "Don't ever accept a lift from Beria." After taking an interest in Marshal Kliment Voroshilov's daughter-in-law during a party at their summer dacha, Beria shadowed their car closely all the way back to the Kremlin terrifying Voroshilov's wife.
Prior to and during the war, Beria directed Sarkisov to keep a running list of the names and phone numbers of his sexual encounters. Eventually he ordered Sarkisov to destroy the list because it was a security risk, but the colonel retained a secret handwritten copy. When Beria's fall from power began, Sarkisov passed the list to Viktor Abakumov, the former wartime head of SMERSH. He was now chief of the MGB - the successor to the NKVD - who was already aggressively building a case against Beria. Stalin, who was also seeking to undermine Beria, was thrilled by the detailed records kept by Sarkisov, demanding: "Send me everything this asshole writes down!" Sarkisov reported that Beria's sexual appetite had led to him contracting syphilis during the war for which he was secretly treated without the knowledge of Stalin or the Politburo (a fact Beria later admitted during his interrogation). Although the Russian government did not acknowledge Sarkisov's handwritten list of Beria's victims until 17 January 2003, the victims' names will not be released until 2028.
Evidence suggests that Beria not only abducted and raped women but some were also murdered. His villa in Moscow is now the Tunisian Embassy (at ). In the mid 1990s, routine work in the grounds turned up the bone remains of several young girls buried in the gardens. According to Martin Sixsmith, in a BBC documentary, "Beria spent his nights having teenagers abducted from the streets and brought here for him to rape. Those who resisted were strangled and buried in his wife's rose garden."
Sarkisov and Nadaria's testimony has been partially corroborated by Edward Ellis Smith, an American who served in the U.S. embassy in Moscow after the war. According to Knight, "Smith noted that Beria's escapades were common knowledge among embassy personnel because his house was on the same street as residence for Americans, and those who lived there saw girls brought to Beria's house late at night in a limousine."
Honours and awards.
Beria's awards were rescinded after his execution.
In popular culture.
Theater.
Beria is the central character in "Good Night, Uncle Joe" by Canadian playwright David Elendune. The play is a fictionalized account of the events leading up to Stalin's death.
Film.
Georgian film director Tengiz Abuladze based the character of dictator Varlam Aravidze on Beria in his 1984 film "Repentance". Although banned in the Soviet Union for its semi-allegorical critique of Stalinism, it premiered at the 1987 Cannes Film Festival, winning the FIPRESCI Prize, Grand Prize of the Jury, and the Prize of the Ecumenical Jury.
British actor Bob Hoskins played Beria in the 1991 film "Inner Circle". He was portrayed by Roshan Seth in the 1992 film "Stalin" and, with an Irish accent, by David Suchet in "Red Monarch". In the 2008 BBC documentary series "", Beria was portrayed by Polish actor Krzysztof Drach.
Literature.
In the 1964 science fiction novel by Arkady and Boris Strugatsky, "Hard to Be a God", Beria is personified in the character Don Reba who serves as the king's minister of defense.
Beria is a significant character in the opening chapters of the 1998 novel "Archangel" by British novelist Robert Harris.
In 2012, his alleged personal diary from 1938 to 1953 was published in Russia.
In the 1973 novel The Beria Papers by journalist Alan Williams, Beria is depicted as a child rapist.

</doc>
<doc id="18391" url="http://en.wikipedia.org/wiki?curid=18391" title="Lyonel Feininger">
Lyonel Feininger

Lyonel Charles Feininger (July 17, 1871 – January 13, 1956) was a German-American painter, and a leading exponent of Expressionism. He also worked as a caricaturist and comic strip artist. He was born and grew up in New York City, traveling to Germany at 16 to study and perfect his art. He started his career as a cartoonist in 1894 and met with much success in this area. He was also a commercial caricaturist for 20 years for magazines and newspapers in the USA and Germany. At the age of 36, he started to work as a fine artist. He also produced a large body of photographic works between 1928 and the mid 1950s, but he kept these primarily within his circle of friends. He was also a pianist and composer, with several piano compositions and fugues for organ extant.
Life and work.
Lyonel Feininger was born to German-American violinist and composer Karl Feininger and American singer Elizabeth Feininger. He was born and grew up in New York City, but traveled to Germany at the age of 16 in 1887 to study. In 1888, he moved to Berlin and studied at the Königliche Akademie Berlin under Ernst Hancke. He continued his studies at art schools in Berlin with Karl Schlabitz, and in Paris with sculptor Filippo Colarossi. He started as a caricaturist for several magazines including "Harper's Round Table", "Harper's Young People", "Humoristische Blätter", "Lustige Blätter", "Das Narrenschiff", "Berliner Tageblatt" and "Ulk".
In 1900, he met Clara Fürst, daughter of the painter Gustav Fürst. He married her in 1901, and they had two daughters. In 1905, he separated from his wife after meeting Julia Berg. He married Berg in 1908 and had several children with her.
The artist was represented with drawings at the exhibitions of the annual Berlin Secession in the years 1901 through 1903.
Feininger's career as cartoonist started in 1894. He was working for several German, French and American magazines. In February 1906, when a quarter of Chicago's population was of German descent, James Keeley, editor of The "Chicago Tribune" traveled to Germany to procure the services of the most popular humor artists. He recruited Feininger to illustrate two comic strips "The Kin-der-Kids" and "Wee Willie Winkie's World" for the "Chicago Tribune". The strips were noted for their fey humor and graphic experimentation. He also worked as a commercial caricaturist for 20 years for various newspapers and magazines in both the USA and Germany. Later, Art Spiegelman wrote in "The New York Times Book Review," that Feininger's comics have “achieved a breathtaking formal grace unsurpassed in the history of the medium.”
Feininger started working as a fine artist at the age of 36. He was a member of the "Berliner Sezession" in 1909, and he was associated with German expressionist groups: Die Brücke, the Novembergruppe, Gruppe 1919, the Blaue Reiter circle and Die Blaue Vier (The Blue Four). His first solo exhibit was at Sturm Gallery in Berlin, 1917. When Walter Gropius founded the Bauhaus in Germany in 1919, Feininger was his first faculty appointment, and became the master artist in charge of the printmaking workshop.
From 1909 until 1921, Feininger spent summer vacations on the island of Usedom to recover and to get new inspiration. He continued to create paintings and drawings of Benz for the rest of his life, even after returning to live in the United States. A tour of the sites appearing in the works of Feininger follows a path with markers in the ground to guide visitors.
He designed the cover for the Bauhaus 1919 manifesto: an expressionist woodcut 'cathedral'. He taught at the Bauhaus for several years. Among the students who attended his workshops were Ludwig Hirschfeld Mack (German/Australian (1893–1965), Hans Friedrich Grohs (German 1892 - 1981), and Margarete Koehler-Bittkow (German/American, 1898–1964).
When the Nazi Party came to power in 1933, the situation became unbearable for Feininger and his wife. The Nazi Party declared his work to be "degenerate." They moved to America after his work was exhibited in the 'degenerate art' ("Entartete Kunst") in 1936, but before the 1937 exhibition in Munich. He taught at Mills College before returning to New York. He was elected to the American Academy of Arts and Letters in 1955.
In addition to drawing, Feininger created art with painted toy figures being photographed in front of drawn backgrounds.
Feininger produced a large body of photographic works between 1928 and the mid-1950s. He kept his photographic work within his circle of friends, and it was not shared with the public in his lifetime. He gave some prints away to his colleagues Walter Gropius and Alfred H. Barr, Jr..
Feininger also had intermittent activity as a pianist and composer, with several piano compositions and fugues for organ extant.
His sons, Andreas Feininger and T. Lux Feininger, both became noted artists, the former as a photographer and the latter as a photographer and painter. T. Lux Feininger died July 7, 2011 at the age of 101.
Major retrospective.
A major retrospective exhibition of Lyonel Feininger's work, initially at the Whitney Museum of American Art during June 30-October 16, 2011, was subsequently due to run at the Montreal Museum of Fine Arts during January 20–May 13, 2012. The exhibition is described as "the first in Feininger’s native country in more than forty-five years, and the first ever to include the full breadth of his art" and as "accompanied by a richly illustrated monograph with a feature essay that provides a broad overview of Feininger’s career..." Many critics have argued that the artist's work was at its most mature around 1910 in works in which the power of Feininger as illustrator balance his abstract side; however, we have to consider the possibility that Feininger used cubism as a more artistically succinct tool to establish his version of the concept known as the objective correlative.
Art market.
At a 2001 Christie's auction in London, Feininger's painting "The Green Bridge" (1909) was sold for £2.42 million.

</doc>
<doc id="18393" url="http://en.wikipedia.org/wiki?curid=18393" title="Life">
Life

Life is a characteristic distinguishing physical entities having biological processes (such as signaling and self-sustaining processes) from those that do not, either because such functions have ceased (death), or because they lack such functions and are classified as inanimate. Various forms of life exist such as plants, animals, fungi, protists, archaea, and bacteria. The criteria can at times be ambiguous and may or may not define viruses, viroids or potential artificial life as living. Biology is the primary science concerned with the study of life, although many other sciences are involved.
The smallest contiguous unit of life is called an organism. Organisms are composed of one or more cells, undergo metabolism, maintain homeostasis, can grow, respond to stimuli, reproduce (either sexually or asexually) and, through evolution, adapt to their environment in successive generations. A diverse array of living organisms can be found in the biosphere of Earth, and the properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information.
Abiogenesis is the natural process of life arising from non-living matter such as simple organic compounds. The earliest life on Earth arose at least 3.5 billion years ago, during the Eoarchean Era when sufficient crust had solidified following the molten Hadean Eon. The earliest physical evidence of life on Earth is biogenic graphite from 3.7 billion-year-old metasedimentary rocks found in Western Greenland and microbial mat fossils in 3.48 billion-year-old sandstone from in Western Australia. Some theories such as the Late Heavy Bombardment theory suggest that life on Earth may have started even earlier, and may have begun as early as 4.25 billion years ago according to one study, and even earlier yet, 4.4 billion years ago, according to another. The mechanism by which life began on Earth is unknown, although many hypotheses have been formulated. Since emerging, life has evolved into a variety of forms, which have been classified into a hierarchy of taxa. Life can survive and thrive in a wide range of conditions. Although more than 99 percent of all species ever to have lived are estimated to be extinct, there are currently 10–14 million species of living organisms on the Earth.
The chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. Though life is confirmed only on the Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in our Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations.
The meaning of life—its significance, origin, purpose, and ultimate fate—is a central concept and question in philosophy and religion. Both philosophy and religion have offered interpretations as to how life relates to existence and consciousness, and on related issues such as life stance, purpose, conception of a god or gods, a soul or an afterlife. Different cultures throughout history have had widely varying approaches to these issues.
Early theories.
Materialism.
Some of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that every thing in the universe is made up of a combination of four eternal "elements" or "roots of all": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.
Democritus (460 BC) thought that the essential characteristic of life is having a soul ("psyche"). Like other ancient writers, he was attempting to explain what makes something a "living" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.
Plato's world of eternal and unchanging Forms, imperfectly represented in matter by a divine Artisan, contrasts sharply with the various mechanistic Weltanschauungen, of which atomism was, by the fourth century at least, the most prominent... This debate persisted throughout the ancient world. Atomistic mechanism got a shot in the arm from Epicurus... while the Stoics adopted a divine teleology... The choice seems simple: either show how a structured, regular world could arise out of undirected processes, or inject intelligence into the system.—R. J. Hankinson, "Cause and Explanation in Ancient Greek Thought"
The mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.
Hylomorphism.
Hylomorphism is a theory, originating with Aristotle (322 BC), that all things are a combination of matter and form. Biology was one of his main interests, and there is extensive biological material in his extant writings. In this view, all things in the material universe have both matter and form, and the form of a living thing is its soul (Greek "psyche", Latin "anima"). There are three kinds of souls: the "vegetative soul" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the "animal soul", which causes animals to move and feel; and the "rational soul", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all the attributes of the lower one. Aristotle believed that while matter can exist without form, form cannot exist without matter, and therefore the soul cannot exist without the body.
This account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its "purpose" of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.
Vitalism.
Vitalism is the belief that the life-principle is non-material. This originated with Stahl (17th century), and held sway until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Nietzsche, Wilhelm Dilthey, anatomists like Bichat, and chemists like Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced from inorganic reactants.
During the 1850s, Helmholtz, anticipated by Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no "vital forces" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.
Definitions.
It is a challenge for scientists and philosophers to define life in unequivocal terms. This is difficult partly because life is a process, not a pure substance. Any definition must be sufficiently broad to encompass all life with which we are familiar, and must be sufficiently general to include life that may be fundamentally different from life on Earth. Some may even consider that life is not real at all, but a concept instead.
Biology.
Since there is no unequivocal definition of life, the current understanding is descriptive. Life is considered a characteristic of something that exhibits all or most of the following traits:
These complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.
Alternatives.
To reflect the minimum phenomena required, other biological definitions of life have been proposed, many of these are based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form. At a higher level, living beings are thermodynamic systems that have an organized molecular structure. That is, life is matter that can reproduce itself and evolve as survival dictates. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution.
Others take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle.
Viruses.
Viruses are most often considered replicators rather than forms of life. They have been described as "organisms at the edge of life," since they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.
Living systems theories.
The idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival.
The first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into component parts, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment. Robert Rosen (1991) built on this by defining a system component as "a unit of organization; a part with a function, i.e., a definite relation between part and whole." From this and other starting concepts, he developed a "relational theory of systems" that attempts to explain the special properties of life. Specifically, he identified the "nonfractionability of components in an organism" as the fundamental difference between living systems and "biological machines."
A systems view of life treats environmental fluxes and biological fluxes together as a "reciprocity of influence", and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.
Complex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is often called also systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology, called relational biology, is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as "categorical biology", or a model representation of organisms as a category theory of biological relations, and also an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, epigenetic processes and signaling pathways.
It has also been argued that the evolution of order in living systems and certain physical systems obey a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order generating process for both types of system was concluded to be basically similar.
Another systemic definition, called the Operator theory, proposes that 'life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell', and also proposes that an organism is 'any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.
Origin.
Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years. All known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into "genes-first" and "metabolism-first" categories, but a recent trend is the emergence of hybrid models that combine both categories.
There is no current scientific consensus as to how life originated. However, most accepted scientific models build on the following observations:
Living organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.
However, since genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.
Therefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.
One issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.
Geological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.
In 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) "in vitro". The work was performed in the laboratory of Gerald Joyce, who stated, "This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system."
Prebiotic compounds may have extraterrestrial origin. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Environmental conditions.
The diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Since oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of our planet's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.
All life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.
Range of tolerance.
The inert components of an ecosystem are the physical and chemical factors necessary for life — energy (sunlight or chemical energy), water, temperature, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the "range of tolerance." Outside that are the "zones of physiological stress", where the survival and reproduction are possible but not optimal. Beyond these zones are the "zones of intolerance", where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.
To survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.
Microbial life forms thrive even in the Mariana Trench, the deepest spot on the Earth. Microbes also thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean.
Investigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.
Form and function.
Cells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.
There are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.
The molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.
Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.
Multicellular organisms may have first evolved through the formation of colonies of like cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specialties, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells.
Cells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.
Classification.
The first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms) and "zoophytes." Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.
The exploration of the American continent revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification. In the late 1740s, Carolus Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning. By consistently using this system, Linnaeus separated nomenclature from taxonomy.
The fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.
As new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.
As microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into "species" based on their biology and genetics, but many aspects of such a classification remain controversial.
In the 1960s a trend called cladistics emerged, arranging taxa based on clades in an evolutionary or phylogenetic tree.
Extraterrestrial life.
Earth is the only planet known to harbor life. Other locations within the Solar System that may host microbial life include subsurface Mars, the atmosphere of Venus, and subsurface oceans on some of the moons of the gas giant planets. The variables of the Drake equation are used to discuss the conditions in solar systems where civilization is most likely to exist.
The region around a main sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the main sequence for a shorter time interval. Small red dwarf stars have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also have an impact on the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life.
Panspermia, also called exogenesis, is the hypothesis that life originated elsewhere in the universe and subsequently transferred to Earth in the form of spores via meteorites, comets, or cosmic dust. Conversely, terrestrial life may be seeded in other solar systems through directed panspermia, to secure and expand some terrestrial life forms. Astroecology experiments with meteorites show that Martian asteroids and cometary materials are rich in inorganic elements and may be fertile soils for microbial, algal and plant life, for past and future life in our and other solar systems.
Research.
In 2004, scientists reported detecting the spectral signatures of anthracene and pyrene in the ultraviolet light emitted by the Red Rectangle nebula (no other such complex molecules had ever been found before in outer space). This discovery was considered a confirmation of a hypothesis that as nebulae of the same type as the Red Rectangle approach the ends of their lives, convection currents cause carbon and hydrogen in the nebulae's core to get caught in stellar winds, and radiate outward. As they cool, the atoms supposedly bond to each other in various ways and eventually form particles of a million or more atoms. The scientists inferred that since they discovered polycyclic aromatic hydrocarbons (PAHs)—which may have been vital in the formation of early life on Earth—in a nebula, by necessity they must originate in nebulae.
In August 2009, NASA scientists identified one of the fundamental chemical building-blocks of life (the amino acid glycine) in a comet for the first time.
In 2010, fullerenes (or "buckyballs") were detected in nebulae. Fullerenes have been implicated in the origin of life; according to astronomer Letizia Stanghellini, "It's possible that buckyballs from outer space provided seeds for life on Earth."
In August 2011, findings by NASA, based on studies of meteorites found on Earth, suggests DNA and RNA components (adenine, guanine and related organic molecules), building blocks for life as we know it, may be formed extraterrestrially in outer space.
In October 2011, scientists found using spectroscopy that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic–aliphatic structure") that could be created naturally, and rapidly, by stars. The compounds are so complex that their chemical structures resemble the makeup of coal and petroleum; such chemical complexity was previously thought to arise only from living organisms. These observations suggest that organic compounds introduced on Earth by interstellar dust particles could serve as basic ingredients for life due to their surface-catalytic activities. One of the scientists suggested that these compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."
In August 2012, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary IRAS 16293-2422, which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics – "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."
In June 2013, polycyclic aromatic hydrocarbons (PAHs) were detected in the upper atmosphere of Titan, the largest moon of the planet Saturn.
In 2013, the Atacama Large Millimeter Array (ALMA Project) confirmed that researchers have discovered an important pair of prebiotic molecules in the icy particles in interstellar space (ISM). The chemicals, found in a giant cloud of gas about 25,000 light-years from Earth in ISM, may be a precursor to a key component of DNA and the other may have a role in the formation of an important amino acid. Researchers found a molecule called cyanomethanimine, which produces adenine, one of the four nucleobases that form the "rungs" in the ladder-like structure of DNA. The other molecule, called ethanamine, is thought to play a role in forming alanine, one of the twenty amino acids in the genetic code. Previously, scientists thought such processes took place in the very tenuous gas between the stars. The new discoveries, however, suggest that the chemical formation sequences for these molecules occurred not in gas, but on the surfaces of ice grains in interstellar space. NASA ALMA scientist Anthony Remijan stated that finding these molecules in an interstellar gas cloud means that important building blocks for DNA and amino acids can 'seed' newly formed planets with the chemical precursors for life.
In January 2014, NASA reported that current studies on the planet Mars by the "Curiosity" and "Opportunity" rovers will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.
In February 2014, NASA announced a for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
Death.
Death is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.
One of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred requires drawing precise conceptual boundaries between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.
Extinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived have gone extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.
Fossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the "fossil record". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.
Artificial life.
Artificial life is a field of study that examines systems related to life, its processes, and its evolution through simulations using computer models, robotics, and biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.
Synthetic biology is a new area of biological research and technology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and our environment.

</doc>
<doc id="18398" url="http://en.wikipedia.org/wiki?curid=18398" title="La Espero">
La Espero

"La Espero" ("The Hope") is a poem written by Polish-Jewish oculist and doctor L. L. Zamenhof (1859–1917), the initiator of the Esperanto language. The song is often used as the anthem of Esperanto, and is now usually sung to a triumphal march composed by Félicien Menu de Ménil in 1909 (although there is an earlier, less martial tune created in 1891 by Claes Adelsköld, as well as a number of others less well-known). It is sometimes referred to as the hymn of the Esperanto movement.
Some Esperantists object to the use of terms like "hymn" or "anthem" for "La Espero", arguing that these terms have religious and nationalist overtones respectively. 

</doc>
<doc id="18400" url="http://en.wikipedia.org/wiki?curid=18400" title="Loonie">
Loonie

The Canadian one dollar coin, commonly called the loonie, is a gold-coloured one-dollar coin introduced in 1987. It bears images of a common loon, a bird which is common and well known in Canada, on the reverse, and of Queen Elizabeth II on the obverse. It is produced by the Royal Canadian Mint at its facility in Winnipeg.
The coin's outline is an 11-sided curve of constant width. Its diameter of 26.5 mm and its 11-sidedness matched that of the already-circulating Susan B. Anthony dollar in the United States, and its thickness of 1.95 mm was a close match to the latter's 2.0 mm. Its gold colour differed from the silver-coloured Anthony dollar; however, the succeeding Sacagawea and Presidential dollars matched the loonie's overall hue. Other coins using a curve of constant width include the 7-sided British twenty pence and fifty pence coins (the latter of which has similar size and value to the loonie, but is silver in colour).
The coin has become the symbol of the Canadian dollar: media often discuss the rate at which the "loonie" is trading against other currencies. The nickname "loonie" ("huard" in French) became so widely recognized that in 2006 the Royal Canadian Mint secured the rights to it. When the Canadian two-dollar coin was introduced in 1996, it was in turn nicknamed the "toonie" (a portmanteau of "two" and "loonie").
Background.
Canada first minted a silver dollar coin in 1935 to celebrate the 25th anniversary of George V's reign as king. The voyageur dollar, so named because it featured an Indian and a French voyageur paddling a canoe on the reverse, was minted in silver until 1967, after which it was composed primarily of nickel. The coins did not see wide circulation, mainly due to their size and weight; the nickel version weighed 15.6 g and was 32.1 mm in diameter, and was itself smaller than the silver version.
By 1982, the Royal Canadian Mint had begun work on a new composition for the dollar coin that it hoped would lead to increased circulation. At the same time, vending machine operators and transit systems were lobbying the Government of Canada to replace the dollar banknotes with wider circulating coins. A Commons committee recommended in 1985 that the dollar bill be eliminated despite a lack of evidence that Canadians would support the move. The government argued that it would save between $175 and $250 million over 20 years by switching from bills that had a lifespan of less than a year to coins that would last two decades.
Introduction.
The government announced on March 25, 1986, that the new dollar coin would be launched the following year as a replacement for the dollar bill, which would be phased out. It was expected to cost $31.8 million to produce the first 300 million coins, but through seigniorage (the difference between the cost of production and the coin's value), expected to make up to $40 million a year on the coins. From the proceeds, a total of $60 million over five years was dedicated toward funding the 1988 Winter Olympics in Calgary.
The failure of the Susan B. Anthony dollar coin in the United States had been considered and it was believed Americans refused to support the coin due to its similarity to their quarter coin and its lack of esthetic appeal. In announcing the new Canadian dollar coin, the government stated it would be the same overall size as the Susan B. Anthony coin – slightly larger than a quarter – to allow for compatibility with American manufactured vending machines, but would be eleven-sided and gold-coloured.
It was planned that the coin would continue using the voyageur theme of its predecessor, but the master dies that had been struck in Ottawa were lost in transit en route to the Mint's facility at Winnipeg. A Commons committee struck to investigate the loss discovered that the Mint had no documented procedures for transport of master dies and that it had shipped them via a local courier in a bid to save $43.50. It was also found to be the third time that the Mint had lost master dies within five years. An internal review by the Royal Canadian Mint argued that while a policy existed to ship the obverse and reverse dies separately, the new coins dies were packaged separately but were part of the same shipment. The Mint also disagreed with the Royal Canadian Mounted Police's contention that the dies were simply lost in transit, believing instead that they were stolen. The dies were never recovered.
Fearing the possibility of counterfeiting, the government approved a new design for the reverse, replacing the voyageur with a Robert-Ralph Carmichael design of a common loon floating in water. The coin was immediately nicknamed the "loonie" across English Canada, and became known as a "huard", French for "loon", in Quebec. The loonie entered circulation on June 30, 1987, as 40 million coins were introduced into major cities across the country. Over 800 million loonies had been struck by the coin's 20th anniversary.
Two years after the loonie's introduction, the Bank of Canada ceased production of the dollar banknote. The final dollar bills were printed on June 30, 1989. Initial support for the coin was mixed, but withdrawing the banknote forced acceptance of the coin.
The loonie has subsequently gained iconic status within Canada, and is now regarded as a national symbol. The term "loonie" has since become synonymous with the Canadian dollar itself. The town of Echo Bay, Ontario, home of Robert-Ralph Carmichael, erected a large loonie monument in his honour in 1992 along the highway, similar to Sudbury's 'Big Nickel'.
Lucky loonie.
Officials for the 2002 Salt Lake Winter Olympics invited the National Hockey League's ice making consultant, Dan Craig, to oversee the city's E Center arena, where the ice hockey tournament was being held. Craig invited a couple of members from the ice crew in his hometown of Edmonton to assist. One of them, Trent Evans, secretly placed a loonie at centre ice. He originally placed a dime, but added the loonie after the smaller coin quickly vanished as the ice surface was built up. He placed the coins after realizing there was no target at centre ice for referees to aim for when dropping the puck for a faceoff. A thin yellow dot was painted on the ice surface over the coins, though the loonie was barely visible to those who knew to look for it.
Keeping the coin a secret, Evans told only a few people of its placement and swore them to secrecy. Among those told were the players of the men's and women's teams. Both Canadian teams went on to win gold medals. Several members of the women's team kissed the spot where the coin was buried following their victory. After the men won their final, the coin was dug up and given to Wayne Gretzky, the team's executive-director, who revealed the existence of the "lucky loonie" at a post-game press conference.
The lucky loonie quickly became a piece of Canadian lore. The original lucky loonie was donated to the Hockey Hall of Fame, and Canadians have subsequently hidden loonies at several international competitions. Loonies were buried in the foundations of facilities built for the 2010 Winter Olympics in Vancouver.
Capitalizing on the tradition, the Royal Canadian Mint has released a commemorative edition "lucky loonie" for each Olympic Games since 2004.
Composition.
The weight of the coin was originally specified as 108 grains, equivalent to 6.998 grams.
When introduced, loonie coins were made of Aureate, a bronze-electroplated nickel combination. Beginning in 2007, some loonie blanks also began to be produced with a cyanide-free brass plating process. In the spring of 2012, the composition switched to multi-ply brass-plated steel. As a result, the weight dropped from 7.00 to 6.27 grams. This has resulted in the 2012 loonie not being accepted in some vending machines. The Toronto Parking Authority estimates that at about $345 per machine, it will cost about $1 million to upgrade almost 3,000 machines to accept the new coins. The Mint states that multi-ply plated steel technology, already used in Canada's smaller coinage, produces an electromagnetic signature that is harder to counterfeit than that for regular alloy coins; also, using steel provides cost savings and avoids fluctuations in price or supply of nickel.
On April 10, 2012, the Royal Canadian Mint announced design changes to the loonie and toonie, which include new security features.
Commemorative editions.
The design has been changed several times for commemorative editions:

</doc>
<doc id="18401" url="http://en.wikipedia.org/wiki?curid=18401" title="Laminar flow">
Laminar flow

In fluid dynamics, laminar flow (or streamline flow) occurs when a fluid flows in parallel layers, with no disruption between the layers. At low velocities, the fluid tends to flow without lateral mixing, and adjacent layers slide past one another like playing cards. There are no cross-currents perpendicular to the direction of flow, nor eddies or swirls of fluids. In laminar flow, the motion of the particles of the fluid is very orderly with all particles moving in straight lines parallel to the pipe walls.
Laminar flow is a flow regime characterized by high momentum diffusion and low momentum convection.
When a fluid is flowing through a closed channel such as a pipe or between two flat plates, either of two types of flow may occur depending on the velocity of the fluid: laminar flow or turbulent flow. Laminar flow tends to occur at lower velocities, below a threshold at which it becomes turbulent. Turbulent flow is a less orderly flow regime that is characterised by eddies or small packets of fluid particles which result in lateral mixing. In non-scientific terms, laminar flow is "smooth" while turbulent flow is "rough".
Relationship with the Reynolds number.
The type of flow occurring in a fluid in a channel is important in fluid dynamics problems. The dimensionless Reynolds number is an important parameter in the equations that describe whether flow conditions lead to laminar or turbulent flow. The Reynolds number delimiting laminar and turbulent flow depends on the particular flow geometry, and moreover, the transition from laminar to turbulent flow can be sensitive to disturbance levels and imperfections present in a given configuration.
In the case of flow through a straight pipe with a circular cross-section, at a Reynolds number below a critical value of approximately 2040, fluid motion will ultimately be laminar, whereas at larger Reynolds numbers, the flow can be turbulent. When the Reynolds number is much less than 1, Stokes flow occurs. This is an extreme case of laminar flow whereby viscous (frictional) effects are much greater than inertial forces.
Examples.
A common application of laminar flow is in the smooth flow of a viscous liquid through a tube or pipe. In that case, the velocity of flow varies from zero at the walls to a maximum along the cross-sectional centre of the vessel. The flow profile of laminar flow in a tube can be calculated by dividing the flow into thin cylindrical elements and applying the viscous force to them.
Another example is the flow of air over an aircraft wing. The boundary layer is a very thin sheet of air lying over the surface of the wing (and all other surfaces of the aircraft). Because air has viscosity, this layer of air tends to adhere to the wing. As the wing moves forward through the air, the boundary layer at first flows smoothly over the streamlined shape of the airfoil. Here, the flow is laminar and the boundary layer is a laminar layer. Prandtl applied the concept of the laminar boundary layer to airfoils in 1904.
Laminar flow barriers.
Laminar airflow is used to separate volumes of air, or prevent airborne contaminants from entering an area. Laminar flow hoods are used to exclude contaminants from sensitive processes in science, electronics and medicine. Air curtains are frequently used in commercial settings to keep heated or refrigerated air from passing through doorways. A laminar flow reactor (LFR) is a reactor that uses laminar flow to study chemical reactions and process mechanisms.

</doc>
<doc id="18402" url="http://en.wikipedia.org/wiki?curid=18402" title="Luanda">
Luanda

 
Luanda, formerly named São Paulo da Assunção de Loanda, is the capital city of Angola, and the country's most populous and important city, primary port and major industrial, cultural and urban centre. Located on Angola's coast with the Atlantic Ocean, Luanda is both Angola's chief seaport and its administrative center. It has a metropolitan population of over 5 million. It is also the capital city of Luanda Province, and the world's third most populous Portuguese-speaking city, behind only São Paulo and Rio de Janeiro, both in Brazil, and the most populous Portuguese-speaking capital city in the world, ahead of Brasília, Maputo and Lisbon, for example. 
The city is currently undergoing a major reconstruction, with many large developments taking place that will alter the cityscape significantly.
History.
Portuguese rule.
Portuguese explorer Paulo Dias de Novais founded Luanda on 25 January 1576 as "São Paulo da Assumpção de Loanda", with a hundred families of settlers and four hundred soldiers. In 1618, the Portuguese built the fortress called Fortaleza São Pedro da Barra, and they subsequently built two more: Fortaleza de São Miguel (1634) and Forte de São Francisco do Penedo (1765-6). Of these, the Fortaleza de São Miguel is the best preserved.
Luanda was Portugal's bridgehead from 1627, except during the Dutch rule of Luanda, from 1640 to 1648, as Fort Aardenburgh. The city served as the centre of slave trade to Brazil from circa 1550 to 1836. The slave trade was conducted mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the port of Luanda. This slave trade also involved local merchants and warriors who profited from the trade. During this period, no large scale territorial conquest was intended by the Portuguese; only a few minor settlements were established in the immediate hinterland of Luanda, some on the last stretch of the Kwanza River.
In the 17th century, the Imbangala became the main rivals of the Mbundu in supplying slaves to the Luanda market. In the 1750s, between 5,000 to 10,000 slaves were annually sold. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong degree of Brazilian influence was noted in Luanda until the Independence of Brazil in 1822. In the 19th century, still under Portuguese rule, Luanda experienced a major economic revolution. The slave trade was abolished in 1836, and in 1844, Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Continental Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat, and cassava flour are also produced locally. The Angolan bourgeoisie was born by this time.
In 1889, Governor Brito Capelo opened the gates of an aqueduct which supplied the city with water, a formerly scarce resource, laying the foundation for major growth. Like most of Portuguese Angola, the cosmopolitan city of Luanda was not affected by the Portuguese Colonial War (1961–1974); economic growth and development in the entire region reached record highs during this period. In 1972, a report called Luanda the "Paris of Africa." Throughout Portugal's Estado Novo period, Luanda grew from a town of 61,208 with 14.6% of those inhabitants being white in 1940, to a wealthy cosmopolitan major city of 475,328 in 1970 with 124,814 Europeans (26.3%) and around 50,000 mixed race inhabitants. Luanda has also become one of the world's most expensive cities.
Independence from Portugal.
By the time of Angolan independence in 1975, Luanda was a modern city. The majority of its population was African, but it was dominated by a strong minority of white Portuguese origin. After the Carnation Revolution in Lisbon on April 25, 1974, with the advent of independence and the start of the Angolan Civil War (1975–2002), most of the white Portuguese Luandans left as refugees, principally for Portugal, with many travelling overland to South Africa. There was an immediate crisis, however, as the local African population lacked the skills and knowledge needed to run the city and maintain its well-developed infrastructure. The large numbers of skilled technicians among the force of Cuban soldiers sent in to support the Popular Movement for the Liberation of Angola (MPLA) government in the Angolan Civil War were able to make a valuable contribution to restoring and maintaining basic services in the city. In the following years, however, slums called "musseques" — which had existed for decades — began to grow out of proportion and stretched several kilometres beyond Luanda's former city limits as a result of the decades-long civil war, and because of the rise of deep social inequalities due to large-scale migration of civil war refugees from other Angolan regions. For decades, Luanda's facilities were not adequately expanded to handle this massive increase in the city's population. After 2002, with the end of the civil war and high economic growth rates fuelled by the wealth provided by the increasing oil and diamond production, major reconstruction started.
Geography.
Human geography.
Luanda is divided into two parts, the "Baixa de Luanda" (lower Luanda, the old city) and the "Cidade Alta" (upper city or the new part). The "Baixa de Luanda" is situated next to the port, and has narrow streets and old colonial buildings. However, massive new constructions have by now covered large areas beyond these traditional limits, and a number of previously independent nuclei — like Viana — were incorporated into the city.
Subdivisions.
Since 2011, Luanda Province is divided into 7 municipalities:
A completely new satellite city, called Luanda Sul has been built. In Camama, Zango and Kilamba Kiaxi, more high-rise developments are to be built. The capital Luanda is growing constantly - and in addition, increasingly beyond the official city limits and even provincial boundaries.
Luanda is the seat of a Roman Catholic archbishop. It is also the location of most of Angola's educational institutions, including the private Catholic University of Angola and the public University of Agostinho Neto. It is also the home of the colonial Governor's Palace and the Estádio da Cidadela (the "Citadel Stadium"), Angola's main stadium, with a total seating capacity of 60,000.
Luanda Sul.
Luanda Sul is a satellite city of Luanda. A small stream flows in southern Luanda Sul, starting near the Quatro de Fevereiro Airport then crossing near Vila de Gamek and emptying into the Atlantic Ocean. Vila de Gamek is in northeastern Luanda Sul, near the Quatro de Fevereiro Airport.
Lis-Luanda International School is in viana. Avenue Pedro de C. Vandunem-Loy straddles the northern border of Luanda Sul. Rua da Samba is in western Luanda Sul, near the Atlantic coast.
Climate.
Luanda has a mild semi-arid climate (Köppen climate classification: "BSh"). The climate is warm to hot but surprisingly dry, owing to the cool Benguela Current, which prevents moisture from easily condensing into rain. Frequent fog prevents temperatures from falling at night even during the completely dry months from June to October. Luanda has an annual rainfall of 323 mm, but the variability is among the highest in the world, with a co-efficient of variation above 40 percent. Observed records since 1858 range from 55 mm in 1958 to 851 mm in 1916. The short rainy season in March and April depends on a northerly counter current bringing moisture to the city: it has been shown clearly that weakness in the Benguela current can increase rainfall about sixfold compared with years when that current is strong.
Demographics.
The inhabitants of Luanda are primarily members of African ethnic groups, mainly Ambundu, Ovimbundu, and Bakongo. The official and the most widely used language is Portuguese, although several Bantu languages are also used, chiefly Kimbundu, Umbundu, and Kikongo. There is a sizable minority population of European origin, especially Portuguese (about 260,000), as well as Brazilians and other Latin Americans. Over the last decades, a significant Chinese community has formed, as has a much smaller Vietnamese community. There is a sprinkling of immigrants from other African countries as well, including a small expatriate South African community. Many people of Luanda are of mixed race — European/Portuguese and native African. In recent years, mainly since the mid-2000s, immigration from Portugal has increased due to Portugal's recession and poor economic situation.
The population of Luanda has grown dramatically in recent years, due in large part to war-time migration to the city, which is safe compared to the rest of the country. Luanda, however, in 2006 saw an increase in violent crime, particularly in the shanty towns that surround the colonial urban core.
Economy.
Around one-third of Angolans live in Luanda, 53% of whom live in poverty. Living conditions in Luanda are poor for most of the people, with essential services such as safe drinking water and electricity still in short supply, and severe shortcomings in traffic conditions. On the other hand, luxury constructions for the benefit of the wealthy minority are booming. Luanda is one of the world's most expensive cities for resident foreigners.
New import tariffs imposed in March 2014 made Luanda even more expensive. As an example, a half-litre tub of vanilla ice-cream at the supermarket was reported to cost US$31. The higher import tariffs applied to hundreds of items, from garlic to cars. The stated aim was to try to diversify the heavily oil-dependent economy and nurture farming and industry, sectors which have remained weak. These tariffs have caused much hardship in a country where the average salary was US$260 in 2010, the latest year for which data was available. However, the average salary in the booming oil industry was over 20 times higher at US$5,400.
Manufacturing includes processed foods, beverages, textiles, cement and other building materials, plastic products, metalware, cigarettes, and shoes/clothes. Petroleum (found in nearby off-shore deposits) is refined in the city, although this facility was repeatedly damaged during the Angolan Civil War of 1975–2002. Luanda has an excellent natural harbour; the chief exports are coffee, cotton, sugar, diamonds, iron, and salt. The city also has a thriving building industry, an effect of the nationwide economic boom experienced since 2002, when political stability returned with the end of the civil war. Economic growth is largely supported by oil extraction activities, although massive diversification is taking place. Large investment (domestic and international), along with strong economic growth, has dramatically increased construction of all economic sectors in the city of Luanda.
Transportation.
Luanda is the starting point of the Luanda railway that goes due east to Malanje. The civil war left the railway non-functional, but the railway has been rehabilitated up to Dondo and Malanje.
The main airport of Luanda is Quatro de Fevereiro Airport, which is the largest in the country. Currently, a new international airport, Angola International Airport is under construction southeast of the city, a few kilometres from Viana, which was expected to be opened in 2011. However, as the Angolan government did not continue to make the payments due to the Chinese enterprise in charge of the construction, the firm has suspended its work in 2010.
The port of Luanda serves as the largest port of Angola, and connects Angola to the rest of the world. Major expansion of this port is also taking place, with the completion of a new complex just last year, the port is expanding rapidly. In 2014, a new port is being developed at Dande, about 30km to the north.
Luanda's roads are in a poor state of repair, but are currently undergoing a massive reconstruction process by the government in order to relieve traffic congestion in the city. Major road repairs can be found taking place in nearly every neighborhood, including a major 6-lane highway connected Luanda to Viana, which is nearing partial completion in October.
Public transit is provided by the suburban services of the Luanda Railway, by the public company TCUL, and by a large fleet of privately owned collective taxis as white-blue painted minibuses called "Candongueiro". There is also a private bus company TURA working routes in Luanda.
Renewal and enlargement.
The central government allocates funds to all regions of the country, but the capital region receives the bulk of these funds. Since the end of the Angolan Civil War (1975–2002), stability has been widespread in the country, and major reconstruction has been going on since 2002 in those parts of the country that were damaged during the civil war. Luanda has been of major concern because its population had multiplied and had far outgrown the capacity of the city, especially because much of its infrastructure (water, electricity, roads etc.) had become obsolete and degraded.
Reconstruction in Luanda has been felt in nearly all aspects of society. Major road rehabilitation, including road widening, application of asphalt, and re-routing efforts are all currently being done throughout Luanda. The Brazilian construction firm Odebrecht have been constructing two six-lane highways. One highway will provide speedy access to Cacuaco, Viana, Samba, and the Kilamba Kiaxi district of Luanda to the new airport of Luanda. The other highway will connect the city center of Luanda to Viana, and was expected to be completed by the end of 2008. Both ventures are, however, still under way in 2011.
Major social housing is also being constructed to house those who reside in slums, which dominate the landscape of Luanda. A large Chinese firm has been given a contract to construct the majority of replacement housing in Luanda. The Angolan minister of health recently stated poverty in Angola will be overcome by an increase in jobs and the housing of every citizen.
Sports.
In 2013 Luanda together with Namibe city, hosted the 2013 FIRS Men's Roller Hockey World Cup, the first time that a World Cup of roller hockey was held in Africa.
International relations.
Twin towns – Sister cities.
Luanda is twinned with:

</doc>
<doc id="18403" url="http://en.wikipedia.org/wiki?curid=18403" title="Logical positivism">
Logical positivism

Logical positivism and logical empiricism, which together formed neopositivism, was a movement in Western philosophy that embraced verificationism, an approach that sought to legitimize philosophical discourse on a basis shared with the best examples of empirical sciences. In this theory of knowledge, only statements verifiable either logically or empirically would be "cognitively meaningful". Seeking to convert philosophy to this new "scientific philosophy" was aimed to prevent confusion rooted in unclear language and unverifiable claims. The Berlin Circle and the Vienna Circle propounded logical positivism starting in the late 1920s.
Interpreting Ludwig Wittgenstein's philosophy of language, logical positivists identified a verifiability principle or criterion of cognitive meaningfulness. From Bertrand Russell's logicism they sought reduction of mathematics to logic as well as Russell's logical atomism, Ernst Mach's phenomenalism—whereby the mind knows only actual or potential sensory experience, which is the content of all sciences, whether physics or psychology—and Percy Bridgman's musings that others proclaimed as operationalism. Thereby, only the "verifiable" was scientific and "cognitively meaningful", whereas the unverifiable was unscientific, cognitively meaningless "pseudostatements"—metaphysic, emotive, or such—not candidate to further review by philosophers, newly tasked to organize knowledge, not develop new knowledge.
Logical positivism became famed for vigorous scientific antirealism to purge science of talk about nature's unobservable aspects—including causality, mechanism, and principles—although that goal has been exaggerated. Still, talk of such unobservables would be metaphorical—direct observations viewed in the abstract—or at worst metaphysical or emotional. "Theoretical laws" would be reduced to "empirical laws", while "theoretical terms" would garner meaning from "observational terms" via "correspondence rules". Mathematics of physics would reduce to symbolic logic via logicism, while rational reconstruction would convert ordinary language into standardized equivalents, all networked and united by a logical syntax. A scientific theory would be stated with its method of verification, whereby a logical calculus or empirical operation could verify its falsity or truth.
In the late 1930s, logical positivists fled Germany and Austria for Britain and United States. By then, many had replaced Mach's phenomenalism with Neurath's physicalism, and Carnap had sought to replace "verification" with simply "confirmation". With World War II's close in 1945, logical positivism became milder, "logical empiricism", led largely by Carl Hempel, in America, who expounded the covering law model of scientific explanation. The logical positivist movement became a major underpinning of analytic philosophy, and dominated Anglosphere philosophy, including philosophy of science, while influencing sciences, into the 1960s. Yet the movement failed to resolve its central problems, and its doctrines were increasingly assaulted, most trenchantly by W V O Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, and Carl Hempel.
Roots.
Language.
"Tractatus Logico-Philosophicus", by the young Ludwig Wittgenstein, introduced the view of philosophy as "critique of language", offering the possibility of a theoretically principled distinction of intelligible versus nonsensical discourse. "Tractatus" adhered to a correspondence theory of truth (versus a coherence theory of truth). Wittgenstein's influence also shows in some versions of the verifiability principle. In tractarian doctrine, truths of logic are tautologies, a view widely accepted by logical positivists who were also influenced by Wittgenstein's interpretation of probability although, according to Neurath, some logical positivists found "Tractatus" to contain much metaphysics.
Logicism.
Gottlob Frege began the program of reducing mathematics to logic, continued it with Bertrand Russell, but lost interest in this logicism, and Russell continued it with Alfred North Whitehead in their monumental "Principia Mathematica", inspiring some of the more mathematical logical posivists, such as Hans Hahn and Rudolf Carnap. (Carnap's early anti-metaphysical works employed Russell's theory of types.) Carnap envisioned a universal language that could reconstruct mathematics and thereby encode physics. Yet Kurt Gödel's incompleteness theorem showed this impossible except in trivial cases, and Alfred Tarski's undefinability theorem shattered all hopes of reducing mathematics to logic. Thus, a universal language failed to stem from Carnap's 1934 work "Logische Syntax der Sprache" ("Logical Syntax of Language"). Still, some logical positivists, including Carl Hempel, continued support of logicism.
Empiricism.
In Germany, Hegelian metaphysics was a dominant movement, and Hegelian successors such as F H Bradley explained reality by postulating metaphysical entities lacking empirical basis, drawing reaction in the form of positivism. Starting in the late 19th century, there was "back to Kant" movement. Ernst Mach's positivism and phenomenalism were a major influence.
Origins.
Vienna.
The Vienna Circle, gathering around University of Vienna and Café Central, was led principally by Moritz Schlick. Schlick had held a neo-Kantian position, but later converted, via Carnap's 1928 book "Der logische Aufbau der Welt"—that is, "The Logical Structure of the World"—which became Vienna Circle's "bible", "Aufbau". A 1929 pamphlet written by Otto Neurath, Hans Hahn, and Rudolf Carnap summarized the Vienna Circle's positions. Another member of Vienna Circle to later prove very influential was Carl Hempel. A friendly but tenacious critic of the Circle was Karl Popper, whom Neurath nicknamed the "Official Opposition".
Carnap and other Vienna Circle members, including Hahn and Neurath, saw need for a weaker criterion of meaningfulness than verifiability. A radical "left" wing—led by Neurath and Carnap—began the program of "liberalization of empiricism", and they also emphasized fallibilism and pragmatics, which latter Carnap even suggested as empiricism's basis. A conservative "right" wing—led by Schlick and Waismann—rejected both the liberalization of empiricism and the epistemological nonfoundationalism of a move from phenomenalism to physicalism. As Neurath and somewhat Carnap posed science toward social reform, the split in Vienna Circle also reflected political views.
Berlin.
The Berlin Circle was led principally by Hans Reichenbach.
Rivals.
Both Moritz Schlick and Rudolf Carnap had been influenced by and sought to define logical positivism versus the neo-Kantianism of Ernst Cassirer—the then leading figure of Marburg school, so called—and against Edmund Husserl's phenomenology. Logical positivists especially opposed Martin Heidegger's obscure metaphysics, the epitome of what logical positivism rejected. In the early 1930s, Carnap debated Heidegger over "metaphysical pseudosentences". Despite its revolutionary aims, logical positivism was but one view among many vying within Europe, and logical positivists initially spoke their language.
Export.
As the movement's first emissary to the New World, Moritz Schlick visited Stanford University in 1929, yet otherwise remained in Vienna and was murdered at the University, reportedly by a deranged student, in 1936. That year, a British attendee at some Vienna Circle meetings since 1933, A J Ayer saw his "Language, Truth and Logic", written in English, import logical positivism to the Anglosphere. By then, Nazi political party's 1933 rise to power in Germany had triggered flight of intellectuals. In exile in England, Otto Neurath died in 1945. Rudolf Carnap, Hans Reichenbach, and Carl Hempel—Carnap's protégé who had studied in Berlin with Reichenbach—settled permanently in America. Upon Germany's annexation of Austria in 1939, remaining logical positivists, many of whom were also Jewish, were targeted and continued flight. Logical positivism thus became dominant in the Anglosphere.
Principles.
Analytic/synthetic gap.
Concerning reality, the necessary is a state true in all possible worlds—mere logical validity—whereas the contingent hinges on the way the particular world is. Concerning knowledge, the "a priori" is knowable before or without, whereas the "a posteriori" is knowable only after or through, relevant experience. Concerning statements, the "analytic" is true via terms' arrangement and meanings, thus a tautology—true by logical necessity but uninformative about the world—whereas the "synthetic" adds reference to a state of facts, a contingency.
In 1739, Hume cast a fork aggressively dividing "relations of ideas" from "matters of fact and real existence", such that all truths are of one type or the other. By Hume's fork, truths by relations among ideas (abstract) all align on one side (analytic, necessary, "a priori"), whereas truths by states of actualities (concrete) always align on the other side (synthetic, contingent, "a posteriori"). At any treatises containing neither, Hume orders, "Commit it then to the flames, for it can contain nothing but sophistry and illusion".
Thus awakened from "dogmatic slumber", Kant quested to answer Hume's challenge—but by explaining how metaphysics is possible. Eventually, in his 1781 work, Kant crossed the tines of Hume's fork to identify another range of truths by necessity—synthetic "a priori", statements claiming states of facts but known true before experience—by arriving at transcendental idealism, attributing the mind a constructive role in phenomena by arranging sense data into the very experience "space", "time", and "substance". Thus, Kant saved Newton's law of universal gravitation from Hume's problem of induction by finding uniformity of nature to be "a priori" knowledge. Logical positivists rejected Kant's synthethic "a priori", and staked Hume's fork, whereby a statement is either analytic and "a priori" (thus necessary and verifiable logically) or synthetic and "a posteriori" (thus contingent and verifiable empirically).
Observation/theory gap.
Early, most logical positivists proposed that all knowledge is based on logical inference from simple "protocol sentences" grounded in observable facts. In the 1936 and 1937 papers "Testability and meaning", individual terms replace sentences as the units of meaning. Further, theoretical terms no longer need to acquire meaning by explicit definition from observational terms: the connection may be indirect, through a system of implicit definitions. (Carnap also provides an important, pioneering discussion of disposition predicates.)
Cognitive meaningfulness.
Verification.
The logical positivists' initial stance was that a statement is "cognitively meaningful" only if some finite procedure conclusively determines its truth. By this verifiability principle, only statements verifiable either by their analyticity or by empiricism were "cognitively meaningful". Metaphysics, ontology, as well as much of ethics failed this criterion, and so were found "cognitively meaningless". Moritz Schlick, however, did not view ethical or aesthetic statements as cognitively meaningless. "Cognitive meaningfulness" was variously defined: having a truth value; corresponding to a possible state of affairs; naming a proposition; intelligible or understandable as are scientific statements.
Ethics and aesthetics were subjective preferences, while theology and other metaphysics contained "pseudostatements", neither true nor false. This meaningfulness was cognitive, although other types of meaningfulness—for instance, emotive, expressive, or figurative—occurred in metaphysical discourse, dismissed from further review. Thus, logical positivism indirectly asserted Hume's law, the principle that "is" statements cannot justify "ought" statements, but are separated by an unbridgeable gap. A J Ayer's 1936 book asserted an extreme variant—the boo/hooray doctrine—whereby all evaluative judgments are but emotional reactions.
Confirmation.
In an important pair of papers in 1936 and 1937, "Testability and meaning", Carnap replaced "verification" with "confirmation", on the view that although universal laws cannot be verified they can be confirmed. Later, Carnap employed abundant logical and mathematical methods in researching inductive logic while seeking to provide and account of probability as "degree of confirmation", but was never able to formulate a model. In Carnap's inductive logic, every universal law's degree of confirmation is always zero. In any event, the precise formulation of what came to be called the "criterion of cognitive significance" took three decades (Hempel 1950, Carnap 1956, Carnap 1961).
Carl Hempel became a major critic within the logical positivism movement. Hempel elucidated the paradox of confirmation.
Weak verification.
The second edition of A J Ayer's book arrived in 1946, and discerned "strong" versus "weak" forms of verification. Ayer concluded, "A proposition is said to be verifiable, in the strong sense of the term, if, and only if, its truth could be conclusively established by experience", but is verifiable in the weak sense "if it is possible for experience to render it probable". And yet, "no proposition, other than a tautology, can possibly be anything more than a probable hypothesis". Thus, all are open to weak verification.
Philosophy of science.
Upon the global defeat of Nazism, and removed from philosophy rivials for radical reform—Marburg neo-Kantianism, Husserlian phenomenology, Heidegger's "existential hermeneutics"—while hosted in the climate of American pragmatism and commonsense empiricism, the neopositivists shed much of their earlier, revolutionary zeal. No longer crusading to revise traditional philosophy into a new "scientific philosophy", they became respectable members of a new philosophy subdiscipline, "philosophy of science". Receiving support from Ernest Nagel, logical empiricists were especially influential in the social sciences.
Explanation.
Comtean positivism had viewed science as "description", whereas the logical positivists posed science as "explanation", perhaps to better realize the envisioned unity of science by covering not only fundamental science—that is, fundamental physics—but the special sciences, too, for instance biology, anthropology, psychology, sociology, and economics. The most widely accepted concept of scientific explanation, held even by neopositivist critic Karl Popper, was the deductive-nomological model (DN model). Yet DN model received its greatest explication by Carl Hempel, first in his 1942 article "The function of general laws in history", and more explicitly with Paul Oppenheim in their 1948 article "Studies in the logic of explanation".
In DN model, the stated phenomenon to be explained is the "explanandum"—which can be an event, law, or theory—whereas premises stated to explain it are the "explanans". Explanans must be true or highly confirmed, contain at least one law, and entail the explanandum. Thus, given initial conditions "C1, C2 . . . Cn" plus general laws "L1, L2 . . . Ln", event "E" is a deductive consequence and scientifically explained. In DN model, a law is an unrestricted generalization by conditional proposition—"If A, then B"—and has empirical content testable. (Differing from a merely true regularity—for instance, "George always carries only $1 bills in his wallet"—a law suggests what "must" be true, and is consequent of a scientific theory's axiomatic structure.)
By the Humean empiricist view that humans observe sequence of events, not cause and effect—as causality and causal mechanisms are unobservable—DN model neglects causality beyond mere constant conjunction, first event "A" and then always event "B". Hempel's explication of DN model held natural laws—empirically confirmed regularities—as satisfactory and, if formulated realistically, approximating causal explanation. In later articles, Hempel defended DN model and proposed a probabilistic explanation, inductive-statistical model (IS model). DN model and IS model together form "covering law model", as named by a critic, William Dray. (Derivation of statistical laws from other statistical laws goes to deductive-statistical model (DS model).) Georg Hendrik von Wright, another critic, named it "subsumption theory", fitting the ambition of theory reduction.
Unity of science.
Logical positivists were generally committed to "Unified Science", and sought a common language or, in Neurath's phrase, a "universal slang" whereby which all scientific propositions could be expressed. The adequacy of proposals or fragments of proposals for such a language was often asserted on the basis of various "reductions" or "explications" of the terms of one special science to the terms of another, putatively more fundamental. Sometimes these reductions consisted of set-theoretic manipulations of a few logically primitive concepts (as in Carnap's "Logical Structure of the World" (1928)). Sometimes, these reductions consisted of allegedly analytic or "a priori" deductive relationships (as in Carnap's "Testability and meaning"). A number of publications over a period of thirty years would attempt to elucidate this concept.
Theory reduction.
As in Comptean positivism's envisioned unity of science, neopositivists aimed to network all special sciences through the covering law model of scientific explanation. And ultimately, by supplying boundary conditions and supplying bridge laws within the covering law model, all the special sciences' laws would reduce to fundamental physics, the fundamental science.
Critics.
After the Second World War's close in 1945, key tenets of logical positivism, including its atomistic philosophy of science, the verifiability principle, and the fact/value gap, drew escalated criticism. It was clear that empirical claims cannot be verified to be universally true. Thus, as initially stated, the verifiability criterion made universal statements meaningless, and even made statements beyond empiricism for technological but not conceptual reasons meaningless, which would pose significant problems for science. These problems were recognized within the movement, which hosted attempted solutions—Carnap's move to "confirmation", Ayer's acceptance of "weak verification"—but the program drew sustained criticism from a number of directions by the 1950s. Even philosophers disagreeing among themselves on which direction general epistemology ought to take, as well as on philosophy of science, agreed that the logical empiricist program was untenable, and it became viewed as self-contradictory. The verifiability criterion of meaning was itself unverified. Notable critics were Nelson Goodman, Willard Van Orman Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, J L Austin, Peter Strawson, Hilary Putnam, Ludwig von Mises, and Richard Rorty.
Quine.
Although quite empiricist, American logician Willard Van Orman Quine published the 1951 paper "Two Dogmas of Empiricism", which challenged conventional empiricist presumptions. Quine attacked the analytic/synthetic division, which the verificationist program had been hinged upon in order to entail, by consequence of Hume's fork, both necessity and apriocity. Quine's ontological relativity explained that every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. Quine later proposed naturalized epistemology.
Hanson.
In 1958, Norwood Hanson's "Patterns of Discovery" undermined the division of observation versus theory, as one can predict, collect, prioritize, and assess data only via some horizon of expectation set by a theory. Thus, any dataset—the direct observations, the scientific facts—is laden with theory.
Popper.
An early, tenacious critic was Karl Popper whose 1934 book "Logik der Forschung", arriving in English in 1959 as "The Logic of Scientific Discovery", directly answered verificationism. Popper heeded the problem of induction as rendering empirical verification logically impossible. And the deductive fallacy of affirming the consequent reveals any phenomenon's capacity to host over one logically possible explanation. Accepting scientific method as hypotheticodeduction, whose inference form is denying the consequent, Popper finds scientific method unable to proceed without falsifiable predictions. Popper thus identifies falsifiability to demarcate not "meaningful" from "meaningless" but simply "scientific" from "unscientific"—a label not in itself unfavorable.
Popper finds virtue in metaphysics, required to develop new scientific theories. And an unfalsifiable—thus unscientific, perhaps metaphysical—concept in one era can later, through evolving knowledge or technology, become falsifiable, thus scientific. Popper also found science's quest for truth to rest on values. Popper disparages the "pseudoscientific", which occurs when an unscientific theory is proclaimed true and coupled with seemingly scientific method by "testing" the unfalsifiable theory—whose predictions are confirmed by necessity—or when a scientific theory's falsifiable predictions are strongly falsified but the theory is persistently protected by "immunizing stratagems", such as the appendage of "ad hoc" clauses saving the theory or the recourse to increasingly speculative hypotheses shielding the theory.
Popper's "scientific" epistemology is falsificationism, which finds that no number, degree, and variety of empirical successes can either verify or confirm scientific theory. Falsificationism finds science's aim as "corroboration" of scientific theory, which strives for scientific realism but accepts the maximal status of strongly corroborated verisimilitude ("truthlikeness"). Explicitly denying the positivist view that all knowledge is scientific, Popper developed the "general" epistemology critical rationalism, which finds human knowledge to evolve by "conjectures and refutations". Popper thus acknowledged the value of the positivist movement, driving evolution of human understanding, but claimed that he had "killed positivism".
Kuhn.
With his landmark, "The Structure of Scientific Revolutions", Thomas Kuhn critically destabilized the verificationist program, which was presumed to call for foundationalism. (Actually, even in the 1930s, Otto Neurath had argued for nonfoundationalism via coherentism by likening science to a boat that scientists must rebuild at sea.) Although Kuhn's thesis itself was attacked even by opponents of neopositivism, in the 1970 postscript to "Structure", Kuhn asserted, at least, that there was no algorithm to science—and, on that, even most of Kuhn's critics agreed.
Powerful and persuasive, Kuhn's book, unlike the vocabulary and symbols of logic's formal language, was written in natural language open to the laypersons. Ironically, Kuhn's book was first published in a volume of "Encyclopedia of Unified Science"—a project begun by logical positivists—and some sense unified science, indeed, but by bringing it into the realm of historical and social assessment, rather than fitting it to the model of physics. Kuhn's ideas were rapidly adopted by scholars in disciplines well outside natural sciences, and, as logical empiricists were extremely influential in the social sciences, ushered academia into postpositivism or postempiricism.
Putnam.
The "received view" operates on the "correspondence rule" that states, "The observational terms are taken as referring to specified phenomena or phenomenal properties, and the only interpretation given to the theoretical terms is their explicit definition provided by the correspondence rules". According to Hilary Putnam, a former student of Reichenbach and of Carnap, the dichotomy of observational terms versus theoretical terms introduced a problem within scientific discussion that was nonexistent until this dichotomy was stated by logical positivists. Putnam's four objections:
Putnam also alleged that positivism was actually a form of metaphysical idealism by its rejecting scientific theory's ability to garner knowledge about nature's unobservable aspects. With his "no miracles" argument, posed in 1974, Putnam asserted scientific realism, the stance that science achieves true—or approximately true—knowledge of the world as it exists independently of humans' sensory experience. In this, Putnam opposed not only the positivism but other instrumentalism—whereby scientific theory as but a human tool to predict human observations—filling the void left by positivism's decline.
Retrospect.
By the late 1960s, the neopositivist movement had clearly run its course. Interviewed in the late 1970s, A J Ayer supposed that "the most important" defect "was that nearly all of it was false". Although logical positivism tends to be recalled as a pillar of scientism, Carl Hempel was key in establishing the philosophy subdiscipline philosophy of science where Thomas Kuhn and Karl Popper brought in the era postpositivism. John Passmore found logical positivism to be "dead, or as dead as a philosophical movement ever becomes".
Logical positivism's fall reopened debate over the metaphysical merit of scientific theory, whether it can offer knowledge of the world beyond human experience (scientific realism) versus whether it is but a human tool to predict human experience (instrumentalism). Meanwhile, it became popular among philosophers to rehash the faults and failures of logical positivism without investigation of it. Thereby, logical positivism has been generally misrepresented, sometimes severely. Arguing for their own views, often framed versus logical positivism, many philosophers have reduced logical positivism to simplisms and stereotypes, especially the notion of logical positivism as a type of foundationalism. In any event, the movement helped anchor analytic philosophy in the Anglosphere, and returned Britain to empiricism. Without the logical positivists, who have been tremendously influential outside philosophy, especially in psychology and social sciences, intellectual life of the 20th century would be unrecognizable.
References.
Bechtel, William, "Philosophy of Science: An Overview for Cognitive Science" (Hillsdale NJ: Lawrence Erlbaum Assoc, 1988).
Friedman, Michael, "Reconsidering Logical Positivism" (New York: Cambridge University Press, 1999).
Novick, Peter, "That Noble Dream: The 'Objectivity Question' and the American Historical Profession" (Cambridge UK: Cambridge University Press, 1988).
Stahl, William A & Robert A Campbell, Yvonne Petry, Gary Diver, "Webs of Reality: Social Perspectives on Science and Religion" (Piscataway NJ: Rutgers University Press, 2002).
Suppe, Frederick, ed, "The Structure of Scientific Theories", 2nd edn (Urbana IL: University of Illinois Press, 1977).
Further reading.
</dl>

</doc>
<doc id="18404" url="http://en.wikipedia.org/wiki?curid=18404" title="Lorentz transformation">
Lorentz transformation

In physics, the Lorentz transformation (or transformations) is named after the Dutch physicist Hendrik Lorentz. It was the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with special relativity, but was derived before special relativity.
The transformations describe how measurements of space and time by two observers are related. They reflect the fact that observers moving at different velocities may measure different distances, elapsed times, and even different orderings of events. They supersede the Galilean transformation of Newtonian physics, which assumes an absolute space and time (see Galilean relativity). The Galilean transformation is a good approximation only at relative speeds much smaller than the speed of light.
The Lorentz transformation is a linear transformation. It may include a rotation of space; a rotation-free Lorentz transformation is called a Lorentz boost.
In Minkowski space, the Lorentz transformations preserve the spacetime interval between any two events. They describe only the transformations in which the spacetime event at the origin is left fixed, so they can be considered as a hyperbolic rotation of Minkowski space. The more general set of transformations that also includes translations is known as the Poincaré group.
History.
Many physicists, including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz himself had been discussing the physics implied by these equations since 1887.
Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the ether. FitzGerald then conjectured that Heaviside’s distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 ether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis.
Their explanation was widely known before 1905.
Lorentz (1892–1904) and Larmor (1897–1900), who believed the luminiferous ether hypothesis, were also seeking the transformation under which Maxwell's equations are invariant when transformed from the ether to a moving frame. They extended the FitzGerald–Lorentz contraction hypothesis and found out that the time coordinate has to be modified as well ("local time"). Henri Poincaré gave a physical interpretation to local time (to first order in v/c) as the consequence of clock synchronization, under the assumption that the speed of light is constant in moving frames. Larmor is credited to have been the first to understand the crucial time dilation property inherent in his equations.
In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group,
and named it after Lorentz.
Later in the same year Albert Einstein published what is now called special relativity, by deriving the Lorentz transformation under the assumptions of the principle of relativity and the constancy of the speed of light in any inertial reference frame, and by abandoning the mechanical aether.
Lorentz transformation for frames in standard configuration.
Consider two observers "O" and "O"′, each using their own Cartesian coordinate system to measure space and time intervals. "O" uses ("t", "x", "y", "z") and "O"′ uses ("t"′, "x"′, "y"′, "z"′). Assume further that the coordinate systems are oriented so that, in 3 dimensions, the "x"-axis and the "x"′-axis are collinear, the "y"-axis is parallel to the "y"′-axis, and the "z"-axis parallel to the "z"′-axis. The relative velocity between the two observers is "v" along the common "x"-axis; "O" measures "O′" to move at velocity "v" along the coincident "xx′" axes, while "O′" measures "O" to move at velocity −"v" along the coincident "xx′" axes. Also assume that the origins of both coordinate systems are the same, that is, coincident times and positions. If all these hold, then the coordinate systems are said to be in standard configuration.
The inverse of a Lorentz transformation relates the coordinates the other way round; from the coordinates "O"′ measures ("t"′, "x"′, "y"′, "z"′) to the coordinates "O" measures ("t", "x", "y", "z"), so "t", "x", "y", "z" are in terms of "t"′, "x"′, "y"′, "z"′. The mathematical form is nearly identical to the original transformation; the only difference is the negation of the uniform relative velocity (from "v" to −"v"), and exchange of primed and unprimed quantities, because "O"′ moves at velocity "v" relative to "O", and equivalently, "O" moves at velocity −"v" relative to "O"′. This symmetry makes it effortless to find the inverse transformation (carrying out the exchange and negation saves a lot of rote algebra), although more fundamentally; it highlights that all physical laws should remain unchanged under a Lorentz transformation.
Below, the Lorentz transformations are called "boosts" in the stated directions.
Boost in the "x"-direction.
These are the simplest forms. The Lorentz transformation for frames in standard configuration can be shown to be:
where:
The use of "β" and "γ" is standard throughout the literature. For the remainder of the article – they will be also used throughout unless otherwise stated. Since the above is a linear system of equations (more technically a linear transformation), they can be written in matrix form:
According to the principle of relativity, there is no privileged frame of reference, so the inverse transformations frame "F"′ to frame "F" must be given by simply negating "v":
where the value of "γ" remains unchanged. These equations are also obtained by algebraically solving the standard equations for the variables "t", "x", "y", "z".
Boost in the "y" or "z" directions.
The above collection of equations apply only for a boost in the "x"-direction. The standard configuration works equally well in the "y" or "z" directions instead of "x", and so the results are similar.
For the "y"-direction:
summarized by
where "v" and so "β" are now in the "y"-direction.
For the "z"-direction:
summarized by
where "v" and so "β" are now in the "z"-direction.
The Lorentz or boost matrix is usually denoted by Λ (Greek capital lambda). Above the transformations have been applied to the four-position X,
The Lorentz transform for a boost in one of the above directions can be compactly written as a single matrix equation:
Boost in any direction.
Vector form.
For a boost in an arbitrary direction with velocity v, that is, "O" observes "O′" to move in direction v in the "F" coordinate frame, while "O′" observes "O" to move in direction −v in the "F′" coordinate frame, it is convenient to decompose the spatial vector r into components perpendicular and parallel to v: 
so that
where • denotes the dot product (see also orthogonality for more information). Then, only time and the component r‖ in the direction of v are "warped" by the Lorentz factor:
The parallel and perpendicular components can be eliminated, by substituting formula_16 into r′:
Since r‖ and v are parallel we have
where geometrically and algebraically:
substituting for r‖ and factoring v gives
This method, of eliminating parallel and perpendicular components, can be applied to any Lorentz transformation written in parallel-perpendicular form.
Matrix forms.
These equations can be expressed in block matrix form as
where I is the 3×3 identity matrix and β = v/c is the relative velocity vector (in units of "c") as a "column vector" – in cartesian and tensor index notation it is:
βT = vT/c is the transpose – a row vector:
and "β" is the magnitude of β:
More explicitly stated:
The transformation Λ can be written in the same form as before,
which has the structure:
and the components deduced from above are:
where δ"ij" is the Kronecker delta, and by convention: Latin letters for indices take the values 1, 2, 3, for spatial components of a 4-vector (Greek indices take values 0, 1, 2, 3 for time and space components).
Note that this transformation is only the "boost," i.e., a transformation between two frames whose "x", "y", and "z" axis are parallel and whose spacetime origins coincide. The most general proper Lorentz transformation also contains a rotation of the three axes, because the composition of two boosts is not a pure boost but is a boost followed by a rotation. The rotation gives rise to Thomas precession. The boost is given by a symmetric matrix, but the general Lorentz transformation matrix need not be symmetric.
Composition of two boosts.
The composition of two Lorentz boosts B(u) and B(v) of velocities u and v is given by:
where 
The composition of two Lorentz transformations "L"(u, "U") and "L"(v, "V") which include rotations "U" and "V" is given by:
Visualizing the transformations in Minkowski space.
Lorentz transformations can be depicted on the Minkowski light cone spacetime diagram.
The yellow axes are the rest frame of an observer, the blue axes correspond to the frame of a moving observer
The red lines are world lines, a continuous sequence of events: straight for an object travelling at constant velocity, curved for an object accelerating. Worldlines of light form the boundary of the light cone.
The purple hyperbolae indicate this is a hyperbolic rotation, the hyperbolic angle ϕ is called rapidity (see below). The greater the relative speed between the reference frames, the more "warped" the axes become. The relative velocity cannot exceed "c".
The black arrow is a displacement four-vector between two events (not necessarily on the same world line), showing that in a Lorentz boost; time dilation (fewer time intervals in moving frame) and length contraction (shorter lengths in moving frame) occur. The axes in the moving frame are orthogonal (even though they do not look so).
Rapidity.
The Lorentz transformation can be cast into another useful form by defining a parameter "ϕ" called the rapidity (an instance of hyperbolic angle) such that
and thus
Equivalently:
Then the Lorentz transformation in standard configuration is:
Hyperbolic expressions.
From the above expressions for e"φ" and e"−φ"
and therefore,
Hyperbolic rotation of coordinates.
Substituting these expressions into the matrix form of the transformation, it is evident that
Thus, the Lorentz transformation can be seen as a hyperbolic rotation of coordinates in Minkowski space, where the parameter ϕ represents the hyperbolic angle of rotation, often referred to as rapidity. This transformation is sometimes illustrated with a Minkowski diagram, as displayed above.
This 4-by-4 boost matrix can thus be written compactly as a matrix exponential,
where the simpler Lie-algebraic hyperbolic rotation generator "Kx" is called a boost generator.
Transformation of other physical quantities.
The transformation matrix is universal for all four-vectors, not just 4-dimensional spacetime coordinates. If Z is any four-vector, then:
or in tensor index notation:
in which the primed indices denote indices of Z in the primed frame.
More generally, the transformation of any tensor quantity T is given by:
where formula_45 is the inverse matrix of formula_46
Special relativity.
The crucial insight of Einstein's clock-setting method is the idea that "time is relative". In essence, each observer's frame of reference is associated with a unique set of clocks, the result being that time as measured for a location passes at different rates for different observers. This was a direct result of the Lorentz transformations and is called time dilation. We can also clearly see from the Lorentz "local time" transformation that the concept of the relativity of simultaneity and of the relativity of length contraction are also consequences of that clock-setting hypothesis.
Transformation of the electromagnetic field.
Lorentz transformations can also be used to prove that magnetic and electric fields are simply different aspects of the same force — the electromagnetic force, as a consequence of relative motion between electric charges and observers. The fact that the electromagnetic field shows relativistic effects becomes clear by carrying out a simple thought experiment:
This shows that the Lorentz transformation also applies to electromagnetic field quantities when changing the frame of reference, given below in vector form.
The correspondence principle.
For relative speeds much less than the speed of light, the Lorentz transformations reduce to the Galilean transformation in accordance with the correspondence principle.
The correspondence limit is usually stated mathematically as: as "v" → 0, "c" → ∞. In words: as velocity approaches 0, the speed of light (seems to) approach infinity. Hence, it is sometimes said that nonrelativistic physics is a physics of "instantaneous action at a distance".
Spacetime interval.
In a given coordinate system "x"μ, if two events "A" and "B" are separated by
the spacetime interval between them is given by
This can be written in another form using the Minkowski metric. In this coordinate system,
Then, we can write
or, using the Einstein summation convention,
Now suppose that we make a coordinate transformation "x"μ → "x"′ μ. Then, the interval in this coordinate system is given by
or
It is a result of special relativity that the interval is an invariant. That is, "s"2 = "s"′ 2. For this to hold, it can be shown that it is necessary (but not sufficient) for the coordinate transformation to be of the form
Here, "C"μ is a constant vector and Λμν a constant matrix, where we require that
Such a transformation is called a "Poincaré transformation" or an "inhomogeneous Lorentz transformation". The "Ca" represents a spacetime translation. When "Ca" = 0, the transformation is called an "homogeneous Lorentz transformation", or simply a "Lorentz transformation".
Taking the determinant of
gives us
The cases are:
From the above definition of Λ it can be shown that (Λ00)2 ≥ 1, so either Λ00 ≥ 1 or Λ00 ≤ −1, called orthochronous and non-orthochronous respectively. An important subgroup of the proper Lorentz transformations are the proper orthochronous Lorentz transformations which consist purely of boosts and rotations. Any Lorentz transform can be written as a proper orthochronous, together with one or both of the two discrete transformations; space inversion "P" and time reversal "T", whose non-zero elements are:
The set of Poincaré transformations satisfies the properties of a group and is called the Poincaré group. Under the Erlangen program, Minkowski space can be viewed as the geometry defined by the Poincaré group, which combines Lorentz transformations with translations. In a similar way, the set of all Lorentz transformations forms a group, called the Lorentz group.
A quantity invariant under Lorentz transformations is known as a Lorentz scalar.

</doc>
<doc id="18406" url="http://en.wikipedia.org/wiki?curid=18406" title="Luminiferous aether">
Luminiferous aether

In the late 19th century, luminiferous aether, æther or ether, meaning light-bearing aether, was the postulated medium for the propagation of light. It was invoked to explain the ability of the apparently wave-based light to propagate through empty space, something that waves should not be able to do.
The concept was the topic of considerable debate throughout its history, as it required the existence of an invisible and infinite material with no interaction with physical objects. As the nature of light was explored, especially in the 19th century, the physical qualities required of the aether became increasingly contradictory. By the late 1800s, the existence of the aether was being questioned, although there was no physical theory to replace it.
The negative outcome of the Michelson–Morley experiment (MMX) suggested that the aether was non-existant. This led to considerable theoretical work to explain the propagation of light without an aether. A major breakthrough was the theory of relativity, which could explain why the MMX failed to see aether, but was more broadly interpreted to suggest you didn't need it. MMX, along with the blackbody radiator and photoelectric effect, were key experiments in the development of modern physics, which includes both relativity and quantum theory, the later of which explains the apparent wave-like nature of light.
The history of light and aether.
Particles vs. waves.
To Robert Boyle in the 17th century, shortly before Isaac Newton, the aether was a probable hypothesis and consisted of subtle particles, one sort of which explained the absence of vacuum and the mechanical interactions between bodies, and the other sort of which explained phenomena such as magnetism (and possibly gravity) that were inexplicable on the basis of the purely mechanical interactions of macroscopic bodies:
...though in the ether of the ancients there was nothing taken notice of but a diffused and very subtle substance; yet we are at present content to allow that there is always in the air a swarm of steams moving in a determinate course between the north pole and the south.
Isaac Newton contended that light was made up of numerous small particles. This could explain such features as light's ability to travel in straight lines and reflect off surfaces. This theory was known to have its problems: although it explained reflection well, its explanation of refraction and diffraction was less satisfactory. In order to explain refraction, Newton's "Opticks" (1704) postulated an "Aethereal Medium" transmitting vibrations "faster" than light, by which light, when overtaken, is put into "Fits of easy Reflexion and easy Transmission", which caused refraction and diffraction. Newton believed that these vibrations were related to heat radiation:
Is not the Heat of the warm Room convey'd through the vacuum by the Vibrations of a much subtiler Medium than Air, which after the Air was drawn out remained in the Vacuum? And is not this Medium the same with that Medium by which Light is refracted and reflected, and by whose Vibrations Light communicates Heat to Bodies, and is put into Fits of easy Reflexion and easy Transmission?
The modern understanding is that heat radiation "is", like light, electromagnetic radiation. However, Newton considered them to be two different phenomena. He believed heat vibrations to be excited "when a Ray of Light falls upon the Surface of any pellucid Body". He wrote, "I do not know what this Aether is", but that if it consists of particles then they must be "exceedingly smaller than those of Air, or even than those of Light: The exceeding smallness of its Particles may contribute to the greatness of the force by which those Particles may recede from one another, and thereby make that Medium exceedingly more rare and elastic than Air, and by consequence exceedingly less able to resist the motions of Projectiles, and exceedingly more able to press upon gross Bodies, by endeavoring to expand itself."
Christiaan Huygens, prior to Newton, had hypothesized that light was a wave propagating through an aether, but Newton rejected this idea. The main reason for his rejection stemmed from the fact that both men could apparently only envision light to be a longitudinal wave, like sound and other mechanical waves in fluids. However, longitudinal waves by necessity have only one form for a given propagation direction, rather than two polarizations as in a transverse wave, and thus they were unable to explain the phenomenon of birefringence, where two polarizations of light are refracted differently by a crystal. Instead, Newton preferred to imagine non-spherical particles, or "corpuscles", of light with different "sides" that give rise to birefringence. A further reason why Newton rejected light as waves in a medium was because such a medium would have to extend everywhere in space, and would thereby "disturb and retard the Motions of those great Bodies" (the planets and comets) and thus "as it [light's medium] is of no use, and hinders the Operation of Nature, and makes her languish, so there is no evidence for its Existence, and therefore it ought to be rejected".
Bradley suggests particles.
In 1720 James Bradley carried out a series of experiments attempting to measure stellar parallax by taking measurements of stars at different times of the year. As the Earth moves around the sun, the apparent angle to a given distant spot changes, and by measuring those angles the distance to the star can be calculated based on the known orbital circumference of the Earth around the sun. He failed to detect any parallax, thereby placing a lower limit on the distance to stars.
During these experiments he also discovered a similar effect; the apparent positions of the stars did change over the year, but not as expected. Instead of the apparent angle being maximized when the Earth was at either end of its orbit with respect to the star, the angle was maximized when the Earth was at its fastest sideways velocity with respect to the star. This interesting effect is now known as stellar aberration.
Bradley explained this effect in the context of Newton's corpuscular theory of light, by showing that the aberration angle was given by simple vector addition of the Earth's orbital velocity and the velocity of the corpuscles of light, just as vertically falling raindrops strike a moving object at an angle. Knowing the Earth's velocity and the aberration angle, this enabled him to estimate the speed of light.
To explain stellar aberration in the context of an aether-based theory of light was regarded as more problematic. As the aberration relied on relative velocities, and the measured velocity was dependant on the motion of the Earth, the aether had to be remaining stationary with respect to the star as the Earth moved through it. This meant that the Earth could travel through the aether, a physical medium, with no apparent effect—precisely the problem that led Newton to reject a wave model in the first place.
Waves theory triumphs.
However, a century later, Young and Fresnel revived the wave theory of light when they pointed out that light could be a transverse wave rather than a longitudinal wave — the polarization of a transverse wave (like Newton's "sides" of light) could explain birefringence, and in the wake of a series of experiments on diffraction the particle model of Newton was finally abandoned. Physicists assumed, moreover, that like mechanical waves, light waves required a medium for propagation, and thus required Huygens's idea of an aether "gas" permeating all space.
However, a transverse wave apparently required the propagating medium to behave as a solid, as opposed to a gas or fluid. The idea of a solid that did not interact with other matter seemed a bit odd , and Augustin-Louis Cauchy suggested that perhaps there was some sort of "dragging", or "entrainment", but this made the aberration measurements difficult to understand. He also suggested that the "absence" of longitudinal waves suggested that the aether had negative compressibility. George Green pointed out that such a fluid would be unstable. George Gabriel Stokes became a champion of the entrainment interpretation, developing a model in which the aether might be (by analogy with pine pitch) rigid at very high frequencies and fluid at lower speeds. Thus the Earth could move through it fairly freely, but it would be rigid enough to support light.
Electromagnetism.
In 1856 Wilhelm Eduard Weber and Rudolf Kohlrausch performed an experiment to measure the numerical value of the ratio of the electromagnetic unit of charge to the electrostatic unit of charge. The result came out to be equal to the product of the speed of light and the square root of two. The following year, Gustav Kirchhoff wrote a paper in which he showed that the speed of a signal along an electric wire was equal to the speed of light. These are the first recorded historical links between the speed of light and electromagnetic phenomena.
James Clerk Maxwell began working on Faraday's lines of force. In his 1861 paper "" he modelled these magnetic lines of force using a sea of molecular vortices that he considered to be partly made of aether and partly made of ordinary matter. He derived expressions for the dielectric constant and the magnetic permeability in terms of the transverse elasticity and the density of this elastic medium. He then equated the ratio of the dielectric constant to the magnetic permeability with a suitably adapted version of Weber and Kohlrausch's result of 1856, and he substituted this result into Newton's equation for the speed of sound. On obtaining a value that was close to the speed of light as measured by Fizeau, Maxwell concluded that light consists in undulations of the same medium that is the cause of electric and magnetic phenomena.
Maxwell had however expressed some uncertainties surrounding the precise nature of his molecular vortices and so he began to embark on a purely dynamical approach to the problem. He wrote another famous paper in 1864 under the title of A Dynamical Theory of the Electromagnetic Field in which the details of the luminiferous medium were less explicit. Although Maxwell did not explicitly mention the sea of molecular vortices, his derivation of Ampère's circuital law was carried over from the 1861 paper and he used a dynamical approach involving rotational motion within the electromagnetic field which he likened to the action of flywheels. Using this approach to justify the electromotive force equation (the precursor of the Lorentz force equation), he derived a wave equation from a set of eight equations which appeared in the paper and which included the electromotive force equation and Ampère's circuital law. Maxwell once again used the experimental results of Weber and Kohlrausch to show that this wave equation represented an electromagnetic wave that propagates at the speed of light, hence supporting the view that light is a form of electromagnetic radiation.
The apparent need for a propagation medium for such Hertzian waves can be seen by the fact that they consist of perpendicular electric (E) and magnetic (B or H) waves. The E waves consist of undulating dipolar electric fields, and all such dipoles appeared to require separated and opposite electric charges. Electric charge is an inextricable property of matter, so it appeared that some form of matter was required to provide the alternating current that would seem to have to exist at any point along the propagation path of the wave. Propagation of waves in a true vacuum would imply the existence of electric fields without associated electric charge, or of electric charge without associated matter. Albeit compatible with Maxwell's equations, electromagnetic induction of electric fields could not be demonstrated in vacuum, because all methods of detecting electric fields required electrically charged matter.
In addition, Maxwell's equations required that all electromagnetic waves in vacuum propagate at a fixed speed, "c". As this can only occur in one reference frame in Newtonian physics (see Galilean-Newtonian relativity), the aether was hypothesized as the absolute and unique frame of reference in which Maxwell's equations hold. That is, the aether must be "still" universally, otherwise "c" would vary along with any variations that might occur in its supportive medium. Maxwell himself proposed several mechanical models of aether based on wheels and gears, and George Francis FitzGerald even constructed a working model of one of them. These models had to agree with the fact that the electromagnetic waves are transverse but never longitudinal.
Problems.
By this point the mechanical qualities of the aether had become more and more magical: it had to be a fluid in order to fill space, but one that was millions of times more rigid than steel in order to support the high frequencies of light waves. It also had to be massless and without viscosity, otherwise it would visibly affect the orbits of planets. Additionally it appeared it had to be completely transparent, non-dispersive, incompressible, and continuous at a very small scale. Maxwell wrote in "Encyclopædia Britannica":
Aethers were invented for the planets to swim in, to constitute electric atmospheres and magnetic effluvia, to convey sensations from one part of our bodies to another, and so on, until all space had been filled three or four times over with aethers... The only aether which has survived is that which was invented by Huygens to explain the propagation of light.
Contemporary scientists were aware of the problems, but aether theory was so entrenched in physical law by this point that it was simply assumed to exist. In 1908 Oliver Lodge gave a speech on behalf of Lord Rayleigh to the Royal Institution on this topic, in which he outlined its physical properties, and then attempted to offer reasons why they were not impossible. Nevertheless he was also aware of the criticisms, and quoted Lord Salisbury as saying that "aether is little more than a nominative case of the verb "to undulate"". Others criticized it as an "English invention", although Rayleigh jokingly stated it was actually an invention of the Royal Institution.
By the early 20th Century, aether theory was in trouble. A series of increasingly complex experiments had been carried out in the late 19th century to try to detect the motion of the Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Lorentz and FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, the new special theory of relativity (1905) could generate the same mathematics without referring to an aether at all. Aether fell to Occam's Razor.
Relative motion between the Earth and aether.
Aether drag.
The two most important models, which were aimed to describe the relative motion of the Earth and aether, were Augustin-Jean Fresnel's (1818) model of the (nearly) stationary aether including a partial aether drag determined by Fresnel's dragging coefficient,
and George Gabriel Stokes' (1844)
model of complete aether drag. The latter theory was not considered as correct, since it was not compatible with the aberration of light, and the auxiliary hypotheses developed to explain this problem were not convincing. Also, subsequent experiments as the Sagnac effect (1913) also showed that this model is untenable. However, the most important experiment supporting Fresnel's theory was Fizeau's 1851 experimental confirmation of Fresnel's 1818 prediction that a medium with refractive index "n" moving with a velocity "v" would increase the speed of light travelling through the medium in the same direction as "v" from "c"/"n" to:
That is, movement adds only a fraction of the medium's velocity to the light (predicted by Fresnel in order to make Snell's law work in all frames of reference, consistent with stellar aberration). This was initially interpreted to mean that the medium drags the aether along, with a "portion" of the medium's velocity, but that understanding became very problematic after Wilhelm Veltmann demonstrated that the index "n" in Fresnel's formula depended upon the wavelength of light, so that the aether could not be moving at a wavelength-independent speed. This implied that there must be a separate aether for each of the infinitely many frequencies.
Negative aether-drift experiments.
The key difficulty with Fresnel's aether hypothesis arose from the juxtaposition of the two well-established theories of Newtonian dynamics and Maxwell's electromagnetism. Under a Galilean transformation the equations of Newtonian dynamics are invariant, whereas those of electromagnetism are not. Basically this means that while physics should remain the same in non-accelerated experiments, light would not follow the same rules because it is travelling in the universal "aether frame". Some effect caused by this difference should be detectable.
A simple example concerns the model on which aether was originally built: sound. The speed of propagation for mechanical waves, the speed of sound, is defined by the mechanical properties of the medium. Sound travels 4.3 times faster in water than in air. This explains why a person hearing an explosion underwater and quickly surfacing can hear it again as the slower travelling sound arrives through the air. Similarly, a traveller on an airliner can still carry on a conversation with another traveller because the sound of words is travelling along with the air inside the aircraft. This effect is basic to all Newtonian dynamics, which says that everything from sound to the trajectory of a thrown baseball should all remain the same in the aircraft flying (at least at a constant speed) as if still sitting on the ground. This is the basis of the Galilean transformation, and the concept of frame of reference.
But the same was not supposed to be true for light, since Maxwell's mathematics demanded a single universal speed for the propagation of light, based, not on local conditions, but on two measured properties, the permittivity and permeability of free space, that were assumed to be the same throughout the universe. If these numbers did change, there should be noticeable effects in the sky; stars in different directions would have different colours, for instance .
Thus at any point there should be one special coordinate system, "at rest relative to the aether". Maxwell noted in the late 1870s that detecting motion relative to this aether should be easy enough—light travelling along with the motion of the Earth would have a different speed than light travelling backward, as they would both be moving against the unmoving aether. Even if the aether had an overall universal flow, changes in position during the day/night cycle, or over the span of seasons, should allow the drift to be detected.
First order experiments.
Although the aether is almost stationary according to Fresnel, his theory predicts a positive outcome of aether drift experiments only to "second" order in formula_2, because Fresnel's dragging coefficient would cause a negative outcome of all optical experiments capable of measuring effects to "first" order in formula_2. This was confirmed by the following first-order experiments, which all gave negative results. The following list is based on the description of Wilhelm Wien (1898), with changes and additional experiments according to the descriptions of Edmund Taylor Whittaker (1910) and Jakob Laub (1910):
Besides those optical experiments, also electrodynamic first-order experiments were conducted, which should have led to positive results according to Fresnel. However, Hendrik Antoon Lorentz (1895) modified Fresnel's theory and showed that those experiments can be explained by a stationary aether as well:
Second order experiments.
While the "first"-order experiments could be explained by a modified stationary aether, more precise "second"-order experiments were expected to give positive results, however, no such results could be found.
The famous Michelson–Morley experiment compared the source light with itself after being sent in different directions, looking for changes in phase in a manner that could be measured with extremely high accuracy. The publication of their result in 1887, the null result, was the first clear demonstration that something was seriously wrong with the aether concept of that time (after Michelson's first experiment in 1881 that wasn't fully conclusive). In this case the MM experiment yielded a shift of the fringing pattern of about 0.01 of a fringe, corresponding to a small velocity. However, it was incompatible with the expected aether wind effect due to the Earth's (seasonally varying) velocity which would have required a shift of 0.4 of a fringe, and the error was small enough that the value may have indeed been zero. Therefore, the null hypothesis, the hypothesis that there was no aether wind, could not be rejected. More modern experiments have since reduced the possible value to a number very close to zero, about 10−17.
It is obvious from what has gone before that it would be hopeless to attempt to solve the question of the motion of the solar system by observations of optical phenomena at the surface of the earth.
— A. Michelson and E. Morley. On the Relative Motion of the Earth and the Luminiferous Æther. // Phil. Mag. S. 5. Vol. 24. No. 151. Dec. 1887.
A series of experiments using similar but increasingly sophisticated apparatuses all returned the null result as well. Conceptually different experiments that also attempted to detect the motion of the aether were the Trouton–Noble experiment (1903), whose objective was to detect torsion effects caused by electrostatic fields, and the experiments of Rayleigh and Brace (1902, 1904), to detect double refraction in various media. However, all of them obtained a null result, like Michelson–Morley (MM) previously did.
These "aether-wind" experiments led to a flurry of efforts to "save" aether by assigning to it ever more complex properties, while only few scientists, like Emil Cohn or Alfred Bucherer, considered the possibility of the abandonment of the aether concept. Of particular interest was the possibility of "aether entrainment" or "aether drag", which would lower the magnitude of the measurement, perhaps enough to explain MMX results. However, as noted earlier, aether dragging already had problems of its own, notably aberration. In addition, the interference experiments of Lodge (1893, 1897) and Ludwig Zehnder (1895), aimed to show whether the aether is dragged by various, rotating masses, showed no aether drag. A more precise measurement was made in the Hammar experiment (1935), which ran a complete MM experiment with one of the "legs" placed between two massive lead blocks. If the aether was dragged by mass then this experiment would have been able to detect the drag caused by the lead, but again the null result was achieved. The theory was again modified, this time to suggest that the entrainment only worked for very large masses or those masses with large magnetic fields. This too was shown to be incorrect by the Michelson–Gale–Pearson experiment, which detected the Sagnac effect due to Earth's rotation (s. Aether drag hypothesis).
Another, completely different attempt to save "absolute" aether was made in the Lorentz–FitzGerald contraction hypothesis, which posited that "everything" was affected by travel through the aether. In this theory the reason the Michelson–Morley experiment "failed" was that the apparatus contracted in length in the direction of travel. That is, the light was being affected in the "natural" manner by its travel though the aether as predicted, but so was the apparatus itself, cancelling out any difference when measured. FitzGerald had inferred this hypothesis from a paper by Oliver Heaviside. Without referral to an aether, this physical interpretation of relativistic effects was shared by Kennedy and Thorndike in 1932 as they concluded that the interferometer's arm contracts and also the frequency of its light source "very nearly" varies in the way required by relativity.
Similarly the Sagnac effect, observed by G. Sagnac in 1913, was immediately seen to be fully consistent with special relativity. In fact, the Michelson-Gale-Pearson experiment in 1925 was proposed specifically as a test to confirm the relativity theory, although it was also recognized that such tests, which merely measure absolute rotation, are also consistent with non-relativistic theories.
During the 1920s, the experiments pioneered by Michelson were repeated by Dayton Miller, who publicly proclaimed positive results on several occasions, although not large enough to be consistent with any known aether theory. In any case, other researchers were unable to duplicate Miller's claimed results, and in subsequent years the experimental accuracy of such measurements has been raised by many orders of magnitude, and no trace of any violations of Lorentz invariance has been seen. (A later re-analysis of Miller's results concluded that he had underestimated the variations due to temperature.)
Since the Miller experiment and its unclear results there have been many more experiments to detect the aether. Many of the experimenters have claimed positive results. These results have not gained much attention from mainstream science, since they are in contradiction to a large quantity of high-precision measurements, all of them confirming special relativity.
Lorentz aether theory.
Between 1892 and 1904, Hendrik Lorentz created an electron/aether theory, in which he introduced a strict separation between matter (electrons) and aether. In his model the aether is completely motionless, and it won't be set in motion in the neighborhood of ponderable matter. Contrary to other electron models before, the electromagnetic field of the aether appears as a mediator between the electrons, and changes in this field can propagate not faster than the speed of light. A fundamental concept of Lorentz's theory in 1895 was the "theorem of corresponding states" for terms of order v/c. This theorem states that a moving observer (relative to the aether) makes the same observations as a resting observers, after a suitable change of variables. Lorentz noticed that it was necessary to change the space-time variables when changing frames and introduced concepts like physical length contraction (1892) to explain the Michelson–Morley experiment, and the mathematical concept of local time (1895) to explain the aberration of light and the Fizeau experiment. That resulted in the formulation of the so-called Lorentz transformation by Joseph Larmor (1897, 1900) and Lorentz (1899, 1904), whereby it was noted by Larmor that the complete formulation of local time is accompanied by some sort of time dilation of moving electrons in the aether. As Lorentz later noted (1921, 1928), he considered the time indicated by clocks resting in the aether as "true" time, while local time was seen by him as a heuristic working hypothesis and a mathematical artifice. Therefore, Lorentz's theorem is seen by modern authors as being a mathematical transformation from a "real" system resting in the aether into a "fictitious" system in motion.
The work of Lorentz was mathematically perfected by Henri Poincaré who formulated on many occasions the Principle of Relativity and tried to harmonize it with electrodynamics. He declared simultaneity only a convenient convention which depends on the speed of light, whereby the constancy of the speed of light would be a useful postulate for making the laws of nature as simple as possible. In 1900 and 1904 he physically interpreted Lorentz's local time as the result of clock synchronization by light signals. And finally in June and July 1905 he declared the relativity principle a general law of nature, including gravitation. He corrected some mistakes of Lorentz and proved the Lorentz covariance of the electromagnetic equations. However, he used the notion of an aether as a perfectly undetectable medium and distinguished between apparent and real time, so most historians of science argue that he failed to invent special relativity.
End of aether?
Special Relativity.
Aether theory was dealt another blow when the Galilean transformation and Newtonian dynamics were both modified by Albert Einstein's special theory of relativity, giving the mathematics of Lorentzian electrodynamics a new, "non-aether" context. Unlike most major shifts in scientific thought, special relativity was adopted by the scientific community remarkably quickly, consistent with Einstein's later comment that the laws of physics described by the Special Theory were "ripe for discovery" in 1905. Max Planck's early advocacy of the special theory, along with the elegant formulation given to it by Hermann Minkowski, contributed much to the rapid acceptance of special relativity among working scientists.
Einstein based his theory on Lorentz's earlier work. Instead of suggesting that the mechanical properties of objects changed with their constant-velocity motion through an undetectable aether, Einstein proposed to deduce the characteristics that any successful theory must possess in order to be consistent with the most basic and firmly established principles, independent of the existence of a hypothetical aether. He found that the Lorentz transformation must transcend its connection with Maxwell's equations, and must represent the fundamental relations between the space and time coordinates of inertial frames of reference. In this way he demonstrated that the laws of physics remained invariant as they had with the Galilean transformation, but that light was now invariant as well.
With the development of the special relativity, the need to account for a single universal frame of reference had disappeared — and acceptance of the 19th century theory of a luminiferous aether disappeared with it. For Einstein, the Lorentz transformation implied a conceptual change: that the concept of position in space or time was not absolute, but could differ depending on the observer's location and velocity.
Moreover, in another paper published the same month in 1905, Einstein made several observations on a then-thorny problem, the photoelectric effect. In this work he demonstrated that light can be considered as particles that have a "wave-like nature". Particles obviously do not need a medium to travel, and thus, neither did light. This was the first step that would lead to the full development of quantum mechanics, in which the wave-like nature "and" the particle-like nature of light are both considered to be descriptions of the same thing. A summary of Einstein's thinking about the aether hypothesis, relativity and light quanta may be found in his 1909 (originally German) lecture "The Development of Our Views on the Composition and Essence of Radiation".
Lorentz on his side continued to use the aether concept. In his lectures of around 1911 he pointed out that what "the theory of relativity has to say ... can be carried out independently of what one thinks of the aether and the time". He commented that "whether there is an aether or not, electromagnetic fields certainly exist, and so also does the energy of the electrical oscillations" so that, "if we do not like the name of "aether", we must use another word as a peg to hang all these things upon." He concluded that "one cannot deny the bearer of these concepts a certain substantiality".
Other models.
In later years there have been a few individuals who advocated a neo-Lorentzian approach to physics, which is Lorentzian in the sense of positing an absolute true state of rest that is undetectable and which plays no role in the predictions of the theory. (No violations of Lorentz covariance have ever been detected, despite strenuous efforts.) Hence these theories resemble the 19th century aether theories in name only. For example, the founder of quantum field theory, Paul Dirac, stated in 1951 in an article in Nature, titled "Is there an Aether?" that "we are rather forced to have an aether". However, Dirac never formulated a complete theory, and so his speculations found no acceptance by the scientific community.
Einstein's views on the aether.
In 1916, after Einstein completed his foundational work on general relativity, Lorentz wrote a letter to him in which he speculated that within general relativity the aether was re-introduced. In his response Einstein wrote that one can actually speak about a "new aether", but one may not speak of motion in relation to that aether. This was further elaborated by Einstein in some semi-popular articles (1918, 1920, 1924, 1930).
In 1918 Einstein publicly alluded to that new definition for the first time. Then, in the early 1920s, in a lecture which he was invited to give at Lorentz's university in Leiden, Einstein sought to reconcile the theory of relativity with his mentor's cherished concept of the aether. In this lecture Einstein stressed that special relativity took away the last mechanical property of Lorentz's aether: immobility. However, he continued that special relativity does not necessarily rule out the aether, because the latter can be used to give physical reality to acceleration and rotation. This concept was fully elaborated within general relativity, in which physical properties (which are partially determined by matter) are attributed to space, but no substance or state of motion can be attributed to that "aether" (aether = curved space-time).
In another paper of 1924, named "Concerning the Aether", Einstein argued that Newton's absolute space, in which acceleration is absolute, is the "Aether of Mechanics". And within the electromagnetic theory of Maxwell and Lorentz one can speak of the "Aether of Electrodynamics", in which the aether possesses an absolute state of motion. As regards special relativity, also in this theory acceleration is absolute as in Newton's mechanics. However, the difference from the electromagnetic aether of Maxwell and Lorentz lies in the fact, that "because it was no longer possible to speak, in any absolute sense, of simultaneous states at different locations in the aether, the aether became, as it were, four dimensional, since there was no objective way of ordering its states by time alone.". Now the "aether of special relativity" is still "absolute", because matter is affected by the properties of the aether, but the aether is not affected by the presence of matter. This asymmetry was solved within general relativity. Einstein explained that the "aether of general relativity" is not absolute, because matter is influenced by the aether, just as matter influences the structure of the aether.
So the only similarity of this relativistic aether concept with the classical aether models lies in the presence of physical properties in space. Therefore, as historians such as John Stachel argue, Einstein's views on the "new aether" are not in conflict with his abandonment of the aether in 1905. For, as Einstein himself pointed out, no "substance" and no state of motion can be attributed to that new aether. In addition, Einstein's use of the word "aether" found little support in the scientific community, and played no role in the continuing development of modern physics.

</doc>
<doc id="18408" url="http://en.wikipedia.org/wiki?curid=18408" title="LAME">
LAME

LAME is a free software codec used to encode/compress audio into the lossy MP3 file format.
History.
The name LAME is a recursive acronym for "LAME Ain't an MP3 Encoder". Around mid-1998, Mike Cheng created LAME 1.0 as a set of modifications against the "8Hz-MP3" encoder source code. After some quality concerns raised by others, he decided to start again from scratch based on the "dist10" MPEG reference software sources. His goal was only to speed up the dist10 sources, and leave its quality untouched. That branch (a patch against the reference sources) became Lame 2.0. The project quickly became a team project. Mike Cheng eventually left leadership and started working on tooLAME (an MP2 encoder).
Mark Taylor then started pursuing increased quality in addition to better speed, and released version 3.0 featuring gpsycho, a new psychoacoustic model he developed.
A few key improvements, in chronological order:
Patents and legal issues.
Like all MP3 encoders, LAME implements some technology covered by patents owned by the Fraunhofer Society and other entities. The developers of LAME do not themselves license the technology described by these patents. Distributing compiled binaries of LAME, its libraries, or programs that derive from LAME in countries that recognize those patents may be patent infringing.
The LAME developers state that, since their code is only released in source code form, it should only be considered as an educational description of an MP3 encoder, and thus does not infringe any patent by itself when released as source code only. At the same time, they advise users to obtain a patent license for any relevant technologies that LAME may implement before including a compiled version of the encoder in a product. Some software is released using this strategy: companies use the LAME library, but obtain patent licenses.
In November 2005, there were reports that the Extended Copy Protection rootkit included on some Sony Compact Discs included portions of the LAME library without complying with the terms of the LGPL.

</doc>
<doc id="18414" url="http://en.wikipedia.org/wiki?curid=18414" title="Leszek Miller">
Leszek Miller

Leszek Cezary Miller (born 3 July 1946) is a Polish left-wing politician who served as Prime Minister of Poland from 2001 to 2004. He is the current leader of the Democratic Left Alliance.
Childhood and youth.
Born in Żyrardów, Miller comes from a poor, working-class family: His father was a tailor and his mother a needlewoman. His parents broke up when Leszek was six months old. His father, Florian Miller, a Pole of assimilated German ethnicity, left the family and Leszek has never maintained any contact with him. His mother brought him up in a religious spirit – following her wish, he was even, for some time, an altar boy in their church.
Due to hard life conditions, after graduation from vocational school, 17-year-old Leszek got a job in the Textile Linen Plant in Żyrardów, while continuing his education in the evenings at the Vocational Secondary School of Electric Power Engineering. He soon completed his military service on the ORP Bielik submarine.
In 1969, Miller married Aleksandra, three years his junior, in church. The Millers have a son, Leszek, and a granddaughter, Monika.
Career in the People’s Republic of Poland.
Leszek Miller started his political career as an activist of the Socialist Youth Union, where he held the position of Chairman of the Plant Board, soon becoming a member of the Town Committee. After the military service, in 1969, he joined the Polish United Workers' Party (PZPR).
It should be noted that PZPR is only another name for the Communist party. Many people were pressured to join PZPR in order to advance in their careers or to pursue higher education. Leszek Miller used his affiliation with the Communist party to effectively advance in his studies and professional goals.
In 1973-1974, Leszek Miller was the Secretary of the PZPR Plant Committee. With granted Party’s recommendation, he started political sciences studies at the Party’s Higher School of Political Sciences (Wyższa Szkoła Nauk Społecznych), graduating in 1977. After graduation, Leszek Miller worked at the PZPR Central Committee, supervising the Group, and later on the Department of Youth, Physical Education and Tourism.
In July 1986, Leszek Miller was elected the 1st Secretary of the PZPR Provincial Committee in Skierniewice. In December 1988, he returned to Warsaw, due to his promotion to the position of the Secretary of the PZPR Central Committee.
As a representative of the government side, he took part in the session of the historic “Round Table”, where, together with Andrzej Celiński, he co-chaired the sub-team for youth issues (the only one that closed the session without signing the agreement). In 1989, he became member of the PZPR Political Bureau.
The Third Republic of Poland.
After the PZPR was dissolved, Leszek Miller became a co-founder of the Social Democracy of the Polish Republic (till March 1993, he was Secretary General, then Deputy Chairman and, from December 1997, the Chairman of that party). In December 1999, at the Founding Congress of the Democratic Left Alliance (SLD), he was elected its Chairman, holding the function continuously till February 2004. In 1997-2001 he was the Chairman of the SLD’s caucus.
In 1989, he ran unsuccessfully for Senate as a representative of the Skierniewice Province. In subsequent elections (1991), Leszek Miller was a leader on the election list of the Social Democracy of the Polish Republic in Łódź and, following a considerable success in elections, he won a seat in the Sejm, becoming Chairman of the Parliamentary Group of the Social Democracy of the Polish Republic. In three subsequent elections to the Sejm, he ran all the time from Łódź, each time gaining more and more votes (from 50 thousand in 1991 up to 146 thousand in 2001); he held a seat in Parliament till 2005.
Through all that time he remained one of the leading politicians on the left wing. In early 90’s, together with Mieczysław Rakowski, he was suspected in the case of the, so-called, “Moscow loan”. After revealing that affair in 1991, Włodzimierz Cimoszewicz called Miller to abstain from taking an MP’s oath due to accusations laid against him. When Leszek Miller got cleared of the charges, Prime Minister Cimoszewicz appointed him later as the Minister in Charge of the Office of the Council of Ministers and in 1997 the Minister of Internal Affairs and Administration in his government. In turn, Cimoszewicz became the Minister of Foreign Affairs in Leszek Miller’s cabinet.
In 1993-1996, Miller was the Minister of Labour and Social Policy in the governments of Waldemar Pawlak and Józef Oleksy respectively. In 1996, he was nominated as Senior Minister in charge of the Office of the Council of Ministers. He then got the nickname “The Chancellor”.
Leszek Miller played an important role in concluding the case of Colonel Ryszard Kukliński, for which he was severely criticised within his political circle. A similar disapproval was expressed after Miller’s support for the Concordat and the candidature of Prof. Leszek Balcerowicz to the position of President of the National Bank of Poland.
During the period of the Solidarity Electoral Action’s government, Leszek Miller was in charge of the parliamentary opposition, leading the political fight with the governing party. He was also consolidating the majority of significant left-wing groups around his person. In 1999, he succeeded in establishing one uniform political party – the Democratic Left Alliance – which turned out to be very successful in following elections.
Prime minister.
Following the victory of the Left (41% vs. 12% of the subsequent party) in the Parliamentary Election in 2001, on 19 October 2001, President Aleksander Kwaśniewski appointed Miller Prime Minister and obliged to nominate the government. The new government won the parliamentary vote of confidence on 26 October 2001 (306:140 votes with one abstention). The 16-person cabinet of Prime Minister Miller has been the smallest government of the Polish Republic so far.
Leszek Miller’s government faced a difficult economic situation in Poland, including an unemployment rate above 18%, a high level of public debt, and economic stagnation. At the end of Miller’s term, economic growth exceeded 6%; still, it was too slow to reduce the unemployment rate. During his term, the unpopular program of cuts in public expenses was implemented, together with a hardly successful reform of health care financing. The reforms of the tax system and of the Social Insurance Institution were continued, and the attempt to settle the mass-media market failed. Taxes were significantly lowered – to 19% for companies and for persons running business activity – and the act of freedom in business activity was voted through. A radical, structural reform of secret services was implemented (the State Security Office was dissolved and replaced by the Internal Security Agency and the Intelligence Agency).
Simultaneously, institutional and legal adjustments were continued, resulting from the accession to the European Union. The Accession conditions were negotiated, being the main strategic goal of Miller’s cabinet. On 13 December 2002, at the summit in Copenhagen (Denmark), Prime Minister Leszek Miller completed the negotiations with the European Union. On 16 April 2003 in Athens, Miller, together with Cimoszewicz, signed the Accession Treaty, bringing Poland into the European Union. Miller’s government, in collaboration with various political and social forces, organized the accession referendum with a successful outcome. On 7 and 8 June 2003, 77.45% of the referendum participants voted in favor of Poland’s accession to the European Union. The referendum turn-out reached 58.85%.
Leszek Miller’s government, together with President Kwaśniewski, made a decision (March 2003) to join the international coalition and deploy Polish troops to Iraq, targeting at overthrowing Saddam Hussein’s government. Miller was also a co-signatory of “the letter of 8”, signed by eight European prime ministers, supporting the US position on Iraq. Already in 2002 Miller gave permission to the U.S. government to run a secret CIA prison at Stare Kiejkuty military training center, three hours North of Warsaw. Years later he is facing accusations of acting anti-constitutionally by having tolerated the imprisonment and torture of prisoners.
On 4 December 2003, Leszek Miller suffered injuries in a helicopter crash near Warsaw.
At the end of its term of office, Leszek Miller’s government had the lowest public support of any government since 1989. It was mainly caused by the continuing high unemployment rate, corruption scandals, with Rywingate on top, and by the attempt of fulfilling the plan of reducing social spending (the Hausner’s plan). In result of criticism in his own party, the Democratic Left Alliance, in February 2004, Leszek Miller resigned from chairing the party. Miller was criticized for an excessively liberal approach and for stressing the role of free market mechanisms in economy. He was reproached for his acceptance of a flat tax, which ran counter to the left-wing doctrine. He was also identified with the “chieftain-like style” of leadership. On 26 March 2004, following the decision of the Speaker of the Parliament, Marek Borowski, to found a new dissenting party, the Social Democracy of Poland, Leszek Miller decided to resign from the position of Prime Minister on 2 May 2004, a day after Poland’s accession to the EU. On May 1, 2004, together with President Kwaśniewski, he was in Dublin, taking part in the Grand Ceremony of accession of 10 states, including Poland, to the European Union.
Later career.
In 2005, despite the support of the Łódź Branch of the Democratic Left Alliance, Leszek Miller was not registered on the election list to the Parliament. At the same time, he was offered to run for Senate but refused. Retirement of the old activists was presented in media as “inflow of new blood into the Democratic Left Alliance”. After the election, Leszek Miller became active in journalism, writing mainly for the “Wprost” weekly on liberal economic concepts and current political issues. In the first half of 2005, he stayed at the Woodrow Wilson International Center for Scholars in Washington, D.C., implementing a research project: “Status of the new Poland in the Eastern Europe’s space”.
In September 2007 the former Polish prime minister Leszek Miller become affiliated with Samoobrona, when he decided to run for the Sejm from their lists.

</doc>
<doc id="18420" url="http://en.wikipedia.org/wiki?curid=18420" title="Basis (linear algebra)">
Basis (linear algebra)

A set of vectors in a vector space "V" is called a basis, or a set of basis vectors, if the vectors are linearly independent and every other vector in the vector space is linearly dependent on these vectors. In more general terms, a basis is a linearly independent spanning set.
Given a basis of a vector space "V", every element of "V" can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.
Definition.
A basis "B" of a vector space "V" over a field "F" is a linearly independent subset of "V" that spans "V".
In more detail, suppose that "B" = { "v"1, …, "v""n" } is a finite subset of a vector space "V" over a field F (such as the real or complex numbers R or C). Then "B" is a basis if it satisfies the following conditions:
The numbers "a"i are called the coordinates of the vector "x" with respect to the basis "B", and by the first property they are uniquely determined.
A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) "B" ⊂ "V" is a basis, if
The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see "Related notions below.
It is often convenient to list the basis vectors in a specific "order", for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span "V": see "Ordered bases and coordinates" below.
Expression of a basis.
There are several ways to describe a basis for the space. Some are made "ad hoc" for a specific dimension. For example, there are several ways to give a basis in dim 3, like Euler angles.
The general case is to give a matrix with the components of the new basis vectors in columns. This is also the more general method because it can express any possible set of vectors even if it is not a basis. This matrix can be seen as three things:
Basis matrix: Is a matrix that represents the basis, because its columns are the components of vectors of the basis. This matrix represents any vector of the new basis as linear combination of the current basis.
Rotation operator: When orthonormal bases are used, any other orthonormal basis can be defined by a rotation matrix. This matrix represents the rotation operator that rotates the vectors of the basis to the new one. It is exactly the same matrix as before because the rotation matrix multiplied by the identity matrix I has to be the new basis matrix.
Change of basis matrix: This matrix can be used to change different objects of the space to the new basis. Therefore is called "change of basis" matrix. It is important to note that some objects change their components with this matrix and some others, like vectors, with its inverse.
Properties.
Again, "B" denotes a subset of a vector space "V". Then, "B" is a basis if and only if any of the following equivalent conditions are met:
Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.
Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.
Standard bases for example:
In Rn {E1...,En} where En is the n-th column of the identity matrix which consists of all ones in the main diagonal and zeros everywhere else. This is because the columns of the identity matrix are linearly independent can always span a vector set by expressing it as a linear combination.
In P2 where P2 is the set of all polynomials of degree at most 2 {1,x,x2} is the standard basis.
In M22 {M1,1,M1,2,M2,1,M2,2} where M22 is the set of all 2×2 matrices. and Mm,n is the 2×2 matrix with a 1 in the m,n position and zeros everywhere else. This again is a standard basis since it is linearly independent and spanning.
Extending to a basis.
Let "S" be a subset of a vector space "V". To extend "S" to a basis means to find a basis "B" that contains "S" as a subset. This can be done if and only if "S" is linearly independent. Almost always, there is more than one such "B", except in rather special circumstances (i.e. "S" is already a basis, or "S" is empty and "V" has two elements).
A similar question is when does a subset "S" contain a basis. This occurs if and only if "S" spans "V". In this case, "S" will usually contain several different bases.
Example of alternative proofs.
Often, a mathematical result can be proven in more than one way.
Here, using three different proofs, we show that the vectors (1,1) and (−1,2) form a basis for R2.
From the definition of "basis".
We have to prove that these two vectors are linearly independent and that they generate R2.
Part I: If two vectors v,w are linearly independent, then formula_1 (a and b scalars) implies formula_2
To prove that they are linearly independent, suppose that there are numbers a,b such that:
(i.e., they are linearly dependent). Then:<br>
Subtracting the first equation from the second, we obtain:<br>
Adding this equation to the first equation then:<br>
Hence we have linear independence.
Part II: To prove that these two vectors generate R2, we have to let (a,b) be an arbitrary element of R2, and show that there exist numbers r,s ∈ R such that:<br>
Then we have to solve the equations:<br>
Subtracting the first equation from the second, we get:<br>
By the dimension theorem.
Since (−1,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.
By the invertible matrix theorem.
Simply compute the determinant
Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.
Ordered bases and coordinates.
A basis is just a linearly independent "set" of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {"v""i"} by the first "n" integers. An ordered basis is also called a frame.
Suppose "V" is an "n"-dimensional vector space over a field F. A choice of an ordered basis for "V" is equivalent to a choice of a linear isomorphism "φ" from the coordinate space F"n" to "V".
"Proof". The proof makes use of the fact that the standard basis of F"n" is an ordered basis.
Suppose first that
is a linear isomorphism. Define an ordered basis {"v""i"} for "V" by
where {e"i"} is the standard basis for F"n".
Conversely, given an ordered basis, consider the map defined by
where "x" = "x"1e1 + "x"2e2 + ... + "x""n"e"n" is an element of F"n". It is not hard to check that "φ" is a linear isomorphism.
These two constructions are clearly inverse to each other. Thus ordered bases for "V" are in 1-1 correspondence with linear isomorphisms F"n" → "V".
The inverse of the linear isomorphism "φ" determined by an ordered basis {"v""i"} equips "V" with "coordinates": if, for a vector "v" ∈ "V", "φ"−1("v") = ("a"1, "a"2...,"a""n") ∈ F"n", then the components "a""j" = "a""j"("v") are the coordinates of "v" in the sense that "v" = "a"1("v") "v"1 + "a"2("v") "v"2 + ... + "a""n"("v") "v""n".
The maps sending a vector "v" to the components "a""j"("v") are linear maps from "V" to F, because of "φ"−1 is linear. Hence they are linear functionals. They form a basis for the dual space of "V", called the dual basis.
Related notions.
Analysis.
In the context of infinite-dimensional vector spaces over the real or complex numbers, the term Hamel basis (named after Georg Hamel) or algebraic basis can be used to refer to a basis as defined in this article. This is to make a distinction with other notions of "basis" that exist when infinite-dimensional vector spaces are endowed with extra structure. The most important alternatives are orthogonal bases on Hilbert spaces, Schauder bases and Markushevich bases on normed linear spaces. The term Hamel basis is also commonly used to mean a basis for the real numbers R as a vector space over the field Q of rational numbers. (In this case, the dimension of R over Q is uncountable, specifically the continuum, the cardinal number 2ℵ0.)
The common feature of the other notions is that they permit the taking of infinite linear combinations of the basic vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces or Fréchet spaces.
The preference of other types of bases for infinite-dimensional spaces is justified by the fact that the Hamel basis becomes "too big" in Banach spaces: If "X" is an infinite-dimensional normed vector space which is complete (i.e. "X" is a Banach space), then any Hamel basis of "X" is necessarily uncountable. This is a consequence of the Baire category theorem. The completeness as well as infinite dimension are crucial assumptions in the previous claim. Indeed, finite-dimensional spaces have by definition finite bases and there are infinite-dimensional ("non-complete") normed spaces which have countable Hamel bases. Consider formula_17, the space of the sequences formula_18 of real numbers which have only finitely many non-zero elements, with the norm formula_19 Its standard basis, consisting of the sequences having only one non-zero element, which is equal to 1, is a countable Hamel basis.
Example.
In the study of Fourier series, one learns that the functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions "f" satisfying
The functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are linearly independent, and every function "f" that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense that
for suitable (real or complex) coefficients "a""k", "b""k". But most square-integrable functions cannot be represented as "finite" linear combinations of these basis functions, which therefore "do not" comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.
Affine geometry.
The related notions of an affine space, projective space, convex set, and cone have related notions of affine basis (a basis for an "n"-dimensional affine space is formula_22 points in general linear position), projective basis (essentially the same as an affine basis, this is formula_22 points in general linear position, here in projective space), convex basis (the vertices of a polytope), and cone basis (points on the edges of a polygonal cone); see also a Hilbert basis (linear programming).
Proof that every vector space has a basis.
Let V be any vector space over some field F. Every vector space must contain at least one element: the zero vector 0.
Note that if V = {0}, then the empty set is a basis for V. Now we consider the case where V contains at least one nonzero element, say v.
Define the set X as all linear independent subsets of V. Note that since V contains the nonzero element v, the singleton subset L = {v} of V is necessarily linearly independent.
Hence the set X contains at least the subset L = {v}, and so X is nonempty.
We let X be partially ordered by inclusion: If L1 and L2 belong to X, we say that L1 ≤ L2 when L1 ⊂ L2. It is easy to check that (X, ≤) satisfies the definition of a partially ordered set.
We now note that if Y is a subset of X that is totally ordered by ≤, then the union LY of all the elements of Y (which are themselves certain subsets of V) is an upper bound for Y. To show this, it is necessary to verify both that a) LY belongs to X, and that b) every element L of Y satisfies L ≤ LY. Both a) and b) are easy to check.
Now we apply Zorn's lemma, which asserts that because X is nonempty, and every totally ordered subset of the partially ordered set (X, ≤) has an upper bound, it follows that X has a maximal element. (In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ≤ L for some element L of X, then L = Lmax.)
Finally we claim that Lmax is a basis for V. Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.
Now suppose Lmax does not span V. Then there exists some vector w of V that cannot be expressed as a linearly combination of elements of Lmax (with coefficients in the field F). Note that such a vector w cannot be an element of Lmax.
Now consider the subset Lw of V defined by Lw = Lmax ∪ {w}. It is easy to see that a) Lmax ≤ Lw (since Lmax is a subset of Lw), and that b) Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax).
But the combination of a) and b) above contradict the fact that Lmax is a maximal element of X, which we have already proved. This contradiction shows that the assumption that Lmax does not span V was not true.
Hence Lmax does span V. Since we also know that Lmax is linearly independent over the field F, this verifies that Lmax is a basis for V. Which proves that the arbitrary vector space V has a basis.
Note: This proof relies on Zorn's lemma, which is logically equivalent to the Axiom of Choice. It turns out that, conversely, the assumption that every vector space has a basis can be used to prove the Axiom of Choice. Thus the two assertions are logically equivalent.

</doc>
<doc id="18422" url="http://en.wikipedia.org/wiki?curid=18422" title="Linear algebra">
Linear algebra

Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.
The set of points with coordinates that satisfy a linear equation form a hyperplane in an "n"-dimensional space. The conditions under which a set of "n" hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors.
Linear algebra is central to both pure and applied mathematics. For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations. Functional analysis studies the infinite-dimensional version of the theory of vector spaces. Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations. 
Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, and the social sciences (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.
History.
The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations. Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.
The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his “Theory of Extension” which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".
In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra". The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra first took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.
The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.
Educational history.
Linear algebra first appeared in graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s. Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s. In France during the 1960s, educators attempted to teach linear algebra through affine dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum. In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.
Scope of study.
Vector spaces.
The main structures of linear algebra are vector spaces. A vector space over a field "F" is a set "V" together with two binary operations. Elements of "V" are called "vectors" and elements of "F" are called "scalars". The first operation, "vector addition", takes any two vectors "v" and "w" and outputs a third vector "v" + "w". The second operation, "scalar multiplication", takes any scalar "a" and any vector "v" and outputs a new vector "av". The operations of addition and multiplication in a vector space must satisfy the following axioms. In the list below, let "u", "v" and "w" be arbitrary vectors in "V", and "a" and "b" scalars in "F".
The first four axioms are those of "V" being an abelian group under vector addition. Vector spaces may be diverse in nature, for example, containing functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.
Linear transformations.
Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces "V" and "W" over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map
that is compatible with addition and scalar multiplication:
for any vectors "u","v" ∈ "V" and a scalar "a" ∈ F.
Additionally for any vectors "u", "v" ∈ "V" and scalars "a", "b" ∈ F:
When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.
Linear transformations have geometric significance. For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.
Subspaces, span, and basis.
Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors "v"1, "v"2, …, "vk":
where "a"1, "a"2, …, "a""k" are scalars. The set of all linear combinations of vectors "v"1, "v"2, …, "vk" is called their span, which forms a subspace.
A linear combination of any system of vectors with all zero coefficients is the zero vector of "V". If this is the only way to express the zero vector as a linear combination of "v"1, "v"2, …, "vk" then these vectors are linearly independent. Given a set of vectors that span a space, if any vector "w" is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove "w" from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space "V", which we call a basis of "V". Any set of vectors that spans "V" contains a basis, and any linearly independent set of vectors in "V" can be extended to a basis. It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers considered as a vector space over the rationals, but no explicit basis has been constructed.
Any two bases of a vector space "V" have the same cardinality, which is called the dimension of "V". The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of "V" has finite number of elements, "V" is called a finite-dimensional vector space. If "V" is finite-dimensional and "U" is a subspace of "V", then dim "U" ≤ dim "V". If "U"1 and "U"2 are subspaces of "V", then
One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.
Matrix theory.
A particular basis {"v"1, "v"2, …, "vn"} of "V" allows one to construct a coordinate system in "V": the vector with coordinates ("a"1, "a"2, …, "an") is the linear combination
The condition that "v"1, "v"2, …, "vn" span "V" guarantees that each vector "v" can be assigned coordinates, whereas the linear independence of "v"1, "v"2, …, "vn" assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to "v"). In this way, once a basis of a vector space "V" over F has been chosen, "V" may be identified with the coordinate "n"-space F"n". Under this identification, addition and scalar multiplication of vectors in "V" correspond to addition and scalar multiplication of their coordinate vectors in F"n". Furthermore, if "V" and "W" are an "n"-dimensional and "m"-dimensional vector space over F, and a basis of "V" and a basis of "W" have been fixed, then any linear transformation "T": "V" → "W" may be encoded by an "m" × "n" matrix "A" with entries in the field F, called the matrix of "T" with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.
There is an important distinction between the coordinate "n"-space R"n" and a general finite-dimensional vector space "V". While R"n" has a standard basis {"e"1, "e"2, …, "en"}, a vector space "V" typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of "V").
One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.
Eigenvalues and eigenvectors.
In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation "T" is to find "characteristic lines" that are invariant sets under "T". If "v" is a non-zero vector such that "Tv" is a scalar multiple of "v", then the line through 0 and "v" is an invariant set under "T" and "v" is called a characteristic vector or eigenvector. The scalar λ such that "Tv" = λ"v" is called a characteristic value or eigenvalue of "T".
To find an eigenvector or an eigenvalue, we note that
where I is the identity matrix. For there to be nontrivial solutions to that equation, det("T" − λ I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation "T" taking a vector space "V" into itself we can find a basis for "V" consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if "v"1, "v"2, …, "vn" are linearly independent eigenvectors of a mapping of "n"-dimensional spaces "T" with (not necessarily distinct) eigenvalues λ1, λ2, …, λ"n", and if "v" = "a"1"v"1 + ... + "an vn", then,
Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).
Inner-product spaces.
Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an "inner product" is a map
that satisfies the following three axioms for all vectors "u", "v", "w" in "V" and all scalars "a" in "F":
Note that in R, it is symmetric.
We can define the length of a vector "v" in "V" by 
and we can prove the Cauchy–Schwarz inequality:
In particular, the quantity
and so we can call this quantity the cosine of the angle between the two vectors.
Two vectors are orthogonal if formula_17. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if "v" = "a"1 "v"1 + ... + "an vn", then formula_18.
The inner product facilitates the construction of many useful concepts. For instance, given a transform "T", we can define its Hermitian conjugate "T*" as the linear transform satisfying
If "T" satisfies "TT*" = "T*T", we call "T" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span "V".
Applications.
Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.
Solution of linear systems.
Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:
The Gaussian-elimination algorithm is as follows: eliminate "x" from all equations below "L"1, and then eliminate "y" from all equations below "L"2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.
In the example, "x" is eliminated from "L"2 by adding (3/2)"L"1 to "L"2. "x" is then eliminated from "L"3 by adding "L"1 to "L"3. Formally:
The result is:
Now "y" is eliminated from "L"3 by adding −4"L"2 to "L"3:
The result is:
This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.
The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that
Then, "z" can be substituted into "L"2, which can then be solved to obtain
Next, "z" and "y" can be substituted into "L"1, which can be solved to obtain
The system is solved.
We can, in general, write any system of linear equations as a matrix equation:
The solution of this system is characterized as follows: first, we find a particular solution "x"0 of this equation using Gaussian elimination. Then, we compute the solutions of "Ax" = 0; that is, we find the null space "N" of "A". The solution set of this equation is given by formula_30. If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since "N" is trivial if and only if det "A" ≠ 0, the equation has a unique solution if and only if det "A" ≠ 0.
Least-squares best fit line.
The least squares method is used to determine the best fit line for a set of data. This line will minimize the sum of the squares of the residuals.
Fourier series expansion.
Fourier series are a representation of a function "f": [−π, π] → R as a trigonometric series:
This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.
The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product
The functions "gn"("x") = sin("nx") for "n" > 0 and "hn"("x") = cos("nx") for "n" ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient "ak", we take the inner product with "hk":
and by orthonormality, formula_34; that is, 
Quantum mechanics.
Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions φ: R3 → C such that formula_36 is finite), and it evolves according to the Schrödinger equation. Energy is represented as the operator formula_37, where "V" is the potential energy. "H" is also known as the Hamiltonian operator. The eigenvalues of "H" represents the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of "H". The component of "H" in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
Geometric introduction.
Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two dimensional plane "E". When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.
Point coordinates in the plane "E" are ordered pairs of real numbers, ("x","y"), and a line is defined as the set of points ("x","y") that satisfy the linear equation
where "a", "b" and "c" are not all zero.
Then,
or
where x = ("x", "y", 1) is the 3 × 1 set of homogeneous coordinates associated with the point ("x", "y").
Homogeneous coordinates identify the plane "E" with the "z" = 1 plane in three dimensional space. The x−y coordinates in "E" are obtained from homogeneous coordinates y = ("y"1, "y"2, "y"3) by dividing by the third component (if it is nonzero) to obtain y = ("y"1/"y"3, "y"2/"y"3, 1).
The linear equation, λ, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point "αx1 + "βx2 is also on the line, for any real "α" and "β".
Now consider the equations of the two lines "λ"1 and "λ"2,
which forms a system of linear equations. The intersection of these two lines is defined by x = ("x", "y", 1) that satisfy the matrix equation,
or using homogeneous coordinates,
The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates,
the solutions are multiples of the following solution:
if the rows of B are linearly independent (i.e., "λ"1 and "λ"2 represent distinct lines).
Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns. Notice that this yields a point in the "z" = 1 plane only when the 2 × 2 submatrix associated with "x"3 has a non-zero determinant.
It is interesting to consider the case of three lines, λ1, λ2 and λ3, which yield the matrix equation,
which in homogeneous form yields,
Clearly, this equation has the solution x = (0,0,0), which is not a point on the "z" = 1 plane "E". For a solution to exist in the plane "E", the coefficient matrix "C" must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.
Introduction to linear transformations.
Another way to approach linear algebra is to consider linear functions on the two dimensional real plane "E"=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in "E" and consider the linear function λ: "E"→R, given by
or
This transformation has the important property that if Ay=d, then
This shows that the sum of vectors in "E" map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation. For this case, where the image space is a real number the map is called a linear functional.
Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on "E", so that x=xi+yj. It is now possible to see that 
Thus, the columns of the matrix A are the image of the basis vectors of "E" in R.
This is true for any pair of vectors used to define coordinates in "E". Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in "E". This means a vector x has coordinates (α,β), such that x=αv+βw. Then, we have the linear functional
where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as
Coordinates relative to a basis.
This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in "E". Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is two find the real numbers α, β, so that x=αv+βw, that is
To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis v, w, which are given by,
The functionals σ and τ compute the components of x along the basis vectors v and w, respectively, that is,
which can be written in matrix form as
These coordinate functionals have the properties,
These equations can be assembled into the single matrix equation,
Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.
Inverse image.
The set of points in the plane "E" that map to the same image in R under the linear functional λ define a line in "E". This line is the image of the inverse map, λ−1: R→"E". This inverse image is the set of the points x=(x, y) that solve the equation,
Notice that a linear functional operates on known values for x=(x, y) to compute a value "c" in R, while the inverse image seeks the values for x=(x, y) that yield a specific value "c".
In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation
Solve for y and obtain the inverse image as the set of points,
For convenience the free parameter x has been relabeled t.
The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,
Notice that if h is a solution to this homogeneous equation, then "t" h is also a solution.
The set of points of a linear functional that map to zero define the "kernel" of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.
Generalizations and related topics.
Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.
In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space "V"∗ consisting of linear maps "f": "V" → "F" where "F" is the field of scalars. Multilinear maps "T": "Vn" → "F" can be described via tensor products of elements of "V"∗.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector product "V" × "V" → "V", the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L"p" spaces.
Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.
Algebraic geometry considers the solutions of systems of polynomial equations.
There are several related topics in the field of Computer Programming that utilizes much of the techniques and theorems Linear Algebra encompasses and refers to.

</doc>
<doc id="18423" url="http://en.wikipedia.org/wiki?curid=18423" title="Labia majora">
Labia majora

The labia majora (singular: "labium majus") are two prominent longitudinal cutaneous folds that extend downward and backward from the mons pubis to the perineum. The labia majora is homologous to the male scrotum.
Embryology.
Embryologically, they develop from labioscrotal folds.
Anatomy.
The labia majora constitute the lateral boundaries of the pudendal cleft, which contains the labia minora, interlabial sulci, clitoral hood, clitoral glans, frenulum clitoridis, the Hart's Line, and the vulval vestibule, which contains the external openings of the urethra and the vagina. Each labium majus has two surfaces, an outer, pigmented and covered with strong, crisp hairs; and an inner, smooth and beset with large sebaceous follicles. Between the two there is a considerable quantity of areolar tissue, fat, and a tissue resembling the dartos tunic of the scrotum, besides vessels, nerves, and glands. The labia majora are thicker in front, where they form by their meeting the anterior commisure of the labia majora. Posteriorly, they are not really joined, but appear to become lost in the neighboring integument, ending close to, and nearly parallel to, each other. Together with the connecting skin between them, they form the posterior commisure of the labia majora or posterior boundary of the pudendum. The interval between the posterior commissure of the labia majora and the anus, from 2.5 to 3 cm. in length, constitutes the perineum. Between the labia majora and the inner thighs are the labiocrural folds. Between the labia majora and labia minora are the interlabial sulci. Labia majora atrophy after menopause.
Use in grafting.
The fat pad of the labia majora can be used as a graft, often as a so-called "Martius labial fat pad graft", and can be used, for example, in urethrolysis.

</doc>
<doc id="18424" url="http://en.wikipedia.org/wiki?curid=18424" title="Labia minora">
Labia minora

The labia minora (singular: "labium minus"), also known as the inner labia, inner lips, vaginal lips, or nymphae, are two flaps of skin on either side of the human vaginal opening, situated between the labia majora (outer labia, or outer lips). Inner lips vary widely in size, color, and shape from individual to individual. They are homologous to the male foreskin, frenulum, urethra but no other penile skin.
Structure.
The inner lips extend from the clitoris obliquely downward, laterally, and backward on either side of the vulval vestibule, ending between the bottom of the vulval vestibule and the outer lips. The posterior ends (bottom) of the inner lips are usually joined across the middle line by a fold of skin, named the "frenulum labiorum pudendi" ("frenulum of labia minora") or fourchette.
On the front, each lip forks dividing into two portions surrounding around the clitoris. The upper part of each lip passes above the clitoris to meet the upper part of the other lip—which will often be a little larger or smaller—forming a fold which overhangs the glans clitoridis; this fold is named the preputium clitoridis or clitoral "hood". The lower part passes beneath the glans clitoridis and becomes united to its under surface, forming, with the inner lip of the opposite side, the frenulum clitoridis.
Histology.
On the opposed surfaces of the labia minora are numerous sebaceous glands not associated with hair follicles. They are lined by stratified squamous epithelium on those surfaces.
Variation.
From 2003 to 2004, researchers from the Department of Gynaecology, Elizabeth Garret Anderson Hospital in London, measured the labia and other genital structures of 50 women from the age of 18 to 50, with a mean age of 35.6. The results were:
In female anatomy "macronymphia" is the term used for an abnormally large labia minora commonly found as a racial characteristic in certain ethnic groups such as Khoisans.

</doc>
<doc id="18425" url="http://en.wikipedia.org/wiki?curid=18425" title="Leopold von Sacher-Masoch">
Leopold von Sacher-Masoch

Leopold Ritter von Sacher-Masoch (27 January 1836 – 9 March 1895) was an Austrian writer and journalist, who gained renown for his romantic stories of Galician life. The term masochism is derived from his name.
During his lifetime, Sacher-Masoch was well known as a man of letters, a utopian thinker who espoused socialist and humanist ideals in his fiction and non-fiction. Most of his works remain untranslated into English. The novel "Venus in Furs" was until recently his only book commonly available in English, but an English translation by William Holmes of "Die Gottesmutter" was released in 2015 as "The Mother of God".
Biography.
Early life.
Von Sacher-Masoch was born in the city of Lemberg (now Lviv, Ukraine), the capital of the Kingdom of Galicia and Lodomeria, at the time a province of the Austrian Empire, into the Roman Catholic family of an Austrian civil servant, Leopold Johann Nepomuk Ritter von Sacher, and Charlotte von Masoch, a Ukrainian noblewoman. He later combined his surname with his wife's 'von Masoch', at the request of her family (she was the last of the line). Von Sacher served as a Commissioner of the Imperial Police Forces in Lemberg, and he was recognised with a new title of nobility as Sacher-Masoch awarded by the Austrian Emperor.
Galician storyteller.
Leopold studied law, history and mathematics at Graz University, and after graduating moved back to Lemberg where he became a professor. His early, non-fictional publications dealt mostly with Austrian history. At the same time, Masoch turned to the folklore and culture of his homeland, Galicia. Soon he abandoned lecturing and became a free man of letters. Within a decade his short stories and novels prevailed over his historical non-fiction works, though historical themes continued to imbue his fiction.
Panslavist ideas were prevalent in Masoch's literary work, and he found a particular interest in depicting picturesque types among the various ethnicities that inhabited Galicia. From the 1860s to the 1880s he published a number of volumes of "Jewish Short Stories", "Polish Short Stories", "Galician Short Stories", "German Court Stories" and "Russian Court Stories". His works were published in translation in Ukrainian, Polish, Russian and French.
"The Legacy of Cain".
In 1869, Sacher-Masoch conceived a grandiose series of short stories under the collective title "Legacy of Cain" that would represent the author's aesthetic "Weltanschauung". The cycle opened with the manifesto "The Wanderer" that brought out misogynist themes that became peculiar to Masoch's writings. Of the six planned volumes, only the first two were ever completed. By the middle of the 1880s, Masoch abandoned the "Legacy of Cain". Nevertheless, the published volumes of the series included Masoch's best-known stories, and of them, "Venus in Furs" (1869) is the most famous today. The short novel expressed Sacher-Masoch's fantasies and fetishes (especially for dominant women wearing fur). He did his best to live out his fantasies with his mistresses and wives.
Philosemitism.
Sacher-Masoch edited the Leipzig-based monthly literary magazine "Auf der Höhe. Internationale Review" ("At the Pinnacle. International Review"), which was published from October, 1881 to September, 1885. This was a progressive magazine aimed at tolerance and integration for Jews in Saxony, as well as for the emancipation of women with articles on women's education and suffrage.
In his later years, he worked against local antisemitism through an association for adult education called the "Oberhessischer Verein für Volksbildung" (OVV), founded in 1893 with his second wife, Hulda Meister.
Private life.
On 9 December 1869, Sacher-Masoch and his mistress Baroness Fanny Pistor signed a contract making him her slave for a period of six months, with the stipulation that the Baroness wear furs as often as possible, especially when she was in a cruel mood. Sacher-Masoch took the alias of "Gregor", a stereotypical male servant's name, and assumed a disguise as the servant of the Baroness. The two traveled by train to Italy. As in "Venus in Furs", he traveled in the third-class compartment, while she had a seat in first-class, arriving in Venice (Florence, in the novel), where they were not known, and would not arouse suspicion.
Sacher-Masoch pressured his first wife – Aurora von Rümelin, whom he married in 1873 – to live out the experience of the book, against her preferences. Sacher-Masoch found his family life to be unexciting, and eventually got a divorce and married his assistant.
Later years.
In 1875 Masoch wrote "The Ideals of Our Time", an attempt to give a portrait of German society during its Gründerzeit period.
In his late fifties, his mental health began to deteriorate, and he spent the last years of his life under psychiatric care. According to official reports, he died in Lindheim, Altenstadt, Hesse, in 1895. It is also claimed that he died in an asylum in Mannheim in 1905.
Sacher-Masoch is the great-great-uncle to the British singer and actress Marianne Faithfull on the side of her mother, the Viennese Baroness Eva Erisso.
Masochism.
The term "masochism" was coined in 1886 by the Austrian psychiatrist Richard Freiherr von Krafft-Ebing (1840–1902) in his book "Psychopathia Sexualis":
...I feel justified in calling this sexual anomaly "Masochism", because the author Sacher-Masoch frequently made this perversion, which up to his time was quite unknown to the scientific world as such, the substratum of his writings. I followed thereby the scientific formation of the term "Daltonism", from Dalton, the discoverer of colour-blindness.
During recent years facts have been advanced which prove that Sacher-Masoch was not only the poet of Masochism, but that he himself was afflicted with the anomaly. Although these proofs were communicated to me without restriction, I refrain from giving them to the public. I refute the accusation that "I have coupled the name of a revered author with a perversion of the sexual instinct", which has been made against me by some admirers of the author and by some critics of my book. As a man, Sacher-Masoch cannot lose anything in the estimation of his cultured fellow-beings simply because he was afflicted with an anomaly of his sexual feelings. As an author, he suffered severe injury so far as the influence and intrinsic merit of his work is concerned, for so long and whenever he eliminated his perversion from his literary efforts he was a gifted writer, and as such would have achieved real greatness had he been actuated by normally sexual feelings. In this respect he is a remarkable example of the powerful influence exercised by the "vita sexualis" be it in the good or evil sense over the formation and direction of man's mind.
Sacher-Masoch was not pleased with Krafft-Ebing's assertions. Nevertheless, details of Masoch's private life were obscure until Aurora von Rümelin's memoirs, "Meine Lebensbeichte" (1906), were published in Berlin under the pseudonym Wanda v. Dunajew. The following year, a French translation, "Confession de Ma Vie" (1907) by "Wanda von Sacher-Masoch", was printed in Paris by Mercure de France. An English translation of the French edition was published as "The Confessions of Wanda von Sacher-Masoch" (1991) by RE/Search Publications.

</doc>
<doc id="18426" url="http://en.wikipedia.org/wiki?curid=18426" title="Lithography">
Lithography

Lithography (from " λίθος, lithos", meaning "stone", and " γράφειν, graphein", meaning "to write") is a method of printing originally based on the immiscibility of oil and water. Printing is from a stone (lithographic limestone) or a metal plate with a smooth surface. It was invented in 1796 by German author and actor Alois Senefelder as a cheap method of publishing theatrical works. Lithography can be used to print text or artwork onto paper or other suitable material. 
Lithography originally used an image drawn with oil, fat, or wax onto the surface of a smooth, level lithographic limestone plate. The stone was treated with a mixture of acid and gum arabic, "etching" the portions of the stone which were not protected by the grease-based image. When the stone was subsequently moistened, these etched areas retained water; an oil-based ink could then be applied and would be repelled by the water, sticking only to the original drawing. The ink would finally be transferred to a blank paper sheet, producing a printed page. This traditional technique is still used in some fine art printmaking applications.
In modern lithography, the image is made of a polymer coating applied to a flexible aluminum plate. The image can be printed directly from the plate (the orientation of the image is reversed), or it can be offset, by transferring the image onto a flexible sheet (rubber) for printing and publication.
As a printing technology, lithography is different from intaglio printing (gravure), wherein a plate is either engraved, etched, or stippled to score cavities to contain the printing ink; and woodblock printing or letterpress printing, wherein ink is applied to the raised surfaces of letters or images. Today, most types of high-volume books and magazines, especially when illustrated in colour, are printed with offset lithography, which has become the most common form of printing technology since the 1960s. The word "lithography" also denotes photolithography, a microfabrication technique used in the microelectronics industry to make integrated circuits and microelectromechanical systems.
The principle of lithography.
Lithography uses simple chemical processes to create an image. For instance, the positive part of an image is a water-repelling ("hydrophobic") substance, while the negative image would be water-retaining ("hydrophilic"). Thus, when the plate is introduced to a compatible printing ink and water mixture, the ink will adhere to the positive image and the water will clean the negative image. This allows a flat print plate to be used, enabling much longer and more detailed print runs than the older physical methods of printing (e.g., intaglio printing, letterpress printing).
Lithography was invented by Alois Senefelder in the Kingdom of Bavaria in 1796. In the early days of lithography, a smooth piece of limestone was used (hence the name "lithography": "lithos" (λιθος) is the ancient Greek word for stone). After the oil-based image was put on the surface, a solution of gum arabic in water was applied, the gum sticking only to the non-oily surface. During printing, water adhered to the gum arabic surfaces and avoided the oily parts, while the oily ink used for printing did the opposite.
Lithography on limestone.
Lithography works because of the mutual repulsion of oil and water. The image is drawn on the surface of the print plate with a fat or oil-based medium (hydrophobic) such as a wax crayon, which may be pigmented to make the drawing visible. A wide range of oil-based media is available, but the durability of the image on the stone depends on the lipid content of the material being used, and its ability to withstand water and acid. After the drawing of the image, an aqueous solution of gum arabic, weakly acidified with nitric acid HNO3 is applied to the stone. The function of this solution is to create a hydrophilic layer of calcium nitrate salt, Ca(NO3)2, and gum arabic on all non-image surfaces. The gum solution penetrates into the pores of the stone, completely surrounding the original image with a hydrophilic layer that will not accept the printing ink. Using lithographic turpentine, the printer then removes any excess of the greasy drawing material, but a hydrophobic molecular film of it remains tightly bonded to the surface of the stone, rejecting the gum arabic and water, but ready to accept the oily ink.
When printing, the stone is kept wet with water. Naturally the water is attracted to the layer of gum and salt created by the acid wash. Printing ink based on drying oils such as linseed oil and varnish loaded with pigment is then rolled over the surface. The water repels the greasy ink but the hydrophobic areas left by the original drawing material accept it. When the hydrophobic image is loaded with ink, the stone and paper are run through a press which applies even pressure over the surface, transferring the ink to the paper and off the stone.
Senefelder had experimented during the early 19th century with multicolor lithography; in his 1819 book, he predicted that the process would eventually be perfected and used to reproduce paintings. Multi-color printing was introduced by a new process developed by Godefroy Engelmann (France) in 1837 known as chromolithography. A separate stone was used for each color, and a print went through the press separately for each stone. The main challenge was to keep the images aligned ("in register"). This method lent itself to images consisting of large areas of flat color, and resulted in the characteristic poster designs of this period.
"Lithography, or printing from soft stone, largely took the place of engraving in the production of English commercial maps after about 1852. It was a quick, cheap process and had been used to print British army maps during the Peninsula War. Most of the commercial maps of the second half of the 19th century were lithographed and unattractive, though accurate enough."
Modern lithographic process.
High-volume lithography is used presently to produce posters, maps, books, newspapers, and packaging—just about any smooth, mass-produced item with print and graphics on it. Most books, indeed all types of high-volume text, are now printed using offset lithography.
For offset lithography, which depends on photographic processes, flexible aluminum, polyester, mylar or paper printing plates are used instead of stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. A photographic negative of the desired image is placed in contact with the emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created by direct laser imaging in a CTP (Computer-To-Plate) device known as a platesetter. The positive image is the emulsion that remains after imaging. Non-image portions of the emulsion have traditionally been removed by a chemical process, though in recent times plates have come available which do not require such processing.
The plate is affixed to a cylinder on a printing press. Dampening rollers apply water, which covers the blank portions of the plate but is repelled by the emulsion of the image area. Hydrophobic ink, which is repelled by the water and only adheres to the emulsion of the image area, is then applied by the inking rollers.
If this image were transferred directly to paper, it would create a mirror-type image and the paper would become too wet. Instead, the plate rolls against a cylinder covered with a rubber "blanket", which squeezes away the water, picks up the ink and transfers it to the paper with uniform pressure. The paper passes between the blanket cylinder and a counter-pressure or impression cylinder and the image is transferred to the paper. Because the image is first transferred, or "offset" to the rubber blanket cylinder, this reproduction method is known as "offset lithography" or "offset printing".
Many innovations and technical refinements have been made in printing processes and presses over the years, including the development of presses with multiple units (each containing one printing plate) that can print multi-color images in one pass on both sides of the sheet, and presses that accommodate continuous rolls ("webs") of paper, known as web presses. Another innovation was the continuous dampening system first introduced by Dahlgren instead of the old method which is still used on older presses (conventional dampening), which are rollers covered with molleton (cloth) which absorbs the water. This increased control of the water flow to the plate and allowed for better ink and water balance. Current dampening systems include a "delta effect or vario " which slows the roller in contact with the plate, thus creating a sweeping movement over the ink image to clean impurities known as "hickies". 
The process of lithography printing is illustrated by . This press is also called an ink pyramid due to the fact that the ink is transferred through several layers of rollers with different purposes. Fast lithographic 'web' printing presses are commonly used in newspaper production.
The advent of desktop publishing made it possible for type and images to be modified easily on personal computers for eventual printing by desktop or commercial presses. The development of digital imagesetters enabled print shops to produce negatives for platemaking directly from digital input, skipping the intermediate step of photographing an actual page layout. The development of the digital platesetter during the late 20th century eliminated film negatives altogether by exposing printing plates directly from digital input, a process known as computer to plate printing.
Microlithography and nanolithography.
Microlithography and nanolithography refer specifically to lithographic patterning methods capable of structuring material on a fine scale. Typically, features smaller than 10 micrometers are considered microlithographic, and features smaller than 100 nanometers are considered nanolithographic. Photolithography is one of these methods, often applied to semiconductor manufacturing of microchips. Photolithography is also commonly used for fabricating Microelectromechanical systems (MEMS) devices. Photolithography generally uses a pre-fabricated photomask or reticle as a master from which the final pattern is derived.
Although photolithographic technology is the most commercially advanced form of nanolithography, other techniques are also used. Some, for example electron beam lithography, are capable of much greater patterning resolution (sometimes as small as a few nanometers). Electron beam lithography is also important commercially, primarily for its use in the manufacture of photomasks. Electron beam lithography as it is usually practiced is a form of maskless lithography, in that a mask is not required to generate the final pattern. Instead, the final pattern is created directly from a digital representation on a computer, by controlling an electron beam as it scans across a resist-coated substrate. Electron beam lithography has the disadvantage of being much slower than photolithography.
In addition to these commercially well-established techniques, a large number of promising microlithographic and nanolithographic technologies exist or are being developed, including nanoimprint lithography, interference lithography, X-ray lithography, extreme ultraviolet lithography, magnetolithography and scanning probe lithography. Some of these new techniques have been used successfully for small-scale commercial and important research applications.
Surface-charge lithography, in fact Plasma desorption mass spectrometry can be directly patterned on polar dielectric crystals via pyroelectric effect, 
Diffraction lithography.
Lithography as an artistic medium.
During the first years of the 19th century, lithography had only a limited effect on printmaking, mainly because technical difficulties remained to be overcome. Germany was the main center of production in this period. Godefroy Engelmann, who moved his press from Mulhouse to Paris in 1816, largely succeeded in resolving the technical problems, and during the 1820s lithography was adopted by artists such as Delacroix and Géricault. London also became a center, and some of Géricault's prints were in fact produced there. Goya in Bordeaux produced his last series of prints by lithography—"The Bulls of Bordeaux" of 1828. By the mid-century the initial enthusiasm had somewhat diminished in both countries, although the use of lithography was increasingly favored for commercial applications, which included the prints of Daumier, published in newspapers. Rodolphe Bresdin and Jean-François Millet also continued to practice the medium in France, and Adolf Menzel in Germany. In 1862 the publisher Cadart tried to initiate a portfolio of lithographs by various artists which was not successful but included several prints by Manet. The revival began during the 1870s, especially in France with artists such as Odilon Redon, Henri Fantin-Latour and Degas producing much of their work in this manner. The need for strictly limited editions to maintain the price had now been realized, and the medium became more accepted.
In the 1890s color lithography became popular with French artists, Toulouse-Lautrec most notably of all, and by 1900 the medium in both color and monotone was an accepted part of printmaking, although France and the US have used it more than other countries.
During the 20th century, a group of artists, including Braque, Calder, Chagall, Dufy, Léger, Matisse, Miró, and Picasso, rediscovered the largely undeveloped art form of lithography thanks to the Mourlot Studios, also known as "Atelier Mourlot", a Parisian printshop founded in 1852 by the Mourlot family. The Atelier Mourlot originally specialized in the printing of wallpaper; but it was transformed when the founder's grandson, Fernand Mourlot, invited a number of 20th-century artists to explore the complexities of fine art printing. Mourlot encouraged the painters to work directly on lithographic stones in order to create original artworks that could then be executed under the direction of master printers in small editions. The combination of modern artist and master printer resulted in lithographs which were used as posters to promote the artists' work.
Grant Wood, George Bellows, Alphonse Mucha, Max Kahn, Pablo Picasso, Eleanor Coen, Jasper Johns, David Hockney, Susan Dorothea White and Robert Rauschenberg are a few of the artists who have produced most of their prints in the medium. M. C. Escher is considered a master of lithography, and many of his prints were created using this process. More than other printmaking techniques, printmakers in lithography still largely depend on access to good printers, and the development of the medium has been greatly influenced by when and where these have been established.
As a special form of lithography, the serilith process is sometimes used. Seriliths are mixed media original prints created in a process in which an artist uses the lithograph and serigraph processes. The separations for both processes are hand-drawn by the artist. The serilith technique is used primarily to create fine art limited print editions.

</doc>
<doc id="18430" url="http://en.wikipedia.org/wiki?curid=18430" title="Library management">
Library management

Library management is a sub-discipline of institutional management that focuses on specific issues faced by libraries and library management professionals. Library management encompasses normal management tasks as well as intellectual freedom, anti-censorship, and fundraising tasks. Issues faced in library management frequently overlap those faced in management of non-profit organizations.
Basic functions.
Basic tasks in library management include:
Common library constructs.
Most physical libraries that store solid media, such as books, articles, film, and other artifacts, adhere to some derivative of the Dewey Decimal System as their method for tagging, storing, and retrieving artifacts based on unique identifiers. The use of such systems have caused librarians to develop and leverage common constructs that act as tools for both librarians, and users of libraries. These constructs include:
Long-term issues.
Long-term issues include:
Planning and maintaining library facilities.
An important aspect of library management is planning and maintaining library facilities. Planning the construction of new libraries or remodeling those that exist is integral as user needs are often changing. To supplement their operating budget, managers often secure funding through gifts and fundraising. Many facilities are also including cafes, Friends of the Library, and to help generate additional revenue. These venues must be taken into account when planning for building expansions. 
The site for new construction must be located, the building must be designed, constructed, and then evaluated. Once established, it is important that the building management keep up on regular maintenance. This can also be completed by delegating tasks to maintenance personal or hiring an outside company through bids.
Associations.
The (LLAMA), a division of the American Library Association, provides leaders with webinar, conferences, awards and grants, "Library Leadership & Management" (online quarterly magazine), and books. LLAMA membership includes a free subscription to great leadership "Library Leadership & Management" and discounts on conferences and publications.
Publications.
The "Journal of Library Administration" began in 1980 and is currently published by Routledge, 8 times per year. It is a peer-reviewed academic journal that discusses issues pertaining to library management.

</doc>
<doc id="18432" url="http://en.wikipedia.org/wiki?curid=18432" title="English longbow">
English longbow

The English longbow, also called the Welsh longbow, is a powerful type of medieval longbow (a tall bow for archery) about 6 ft long used by the English and Welsh for hunting and as a weapon in medieval warfare. English use of longbows was effective against the French during the Hundred Years' War, particularly at the start of the war in the battles of Sluys (1340), Crécy (1346), and Poitiers (1356), and perhaps most famously at the Battle of Agincourt (1415). They were less successful after this, with longbowmen having their lines broken at the Battle of Verneuil (1424), and being completely routed at the Battle of Patay (1429) when they were charged before they had set up their defensive position. The term "English" or "Welsh" longbow is a modern usage to distinguish these bows from other longbows, though in fact identical bows were used across northern and western Europe.
The earliest longbow known from England, found at Ashcott Heath, Somerset, is dated to 2665 BC, but no longbows survive from the period when the longbow was dominant (c. 1250–1450 AD), probably because bows became weaker, broke and were replaced, rather than being handed down through generations. More than 130 bows survive from the Renaissance period, however. More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that sank at Portsmouth in 1545.
Description.
Length.
A longbow must be long enough to allow its user to draw the string to a point on the face or body, and the length therefore varies with the user. In continental Europe it was generally seen as any bow longer than 1.2 m. The Society of Antiquaries says it is of 5 or in length. Richard Bartelot, of the Royal Artillery Institution, said that the bow was of yew, 6 ft long, with a 3 ft arrow. Gaston Phoebus, in 1388, wrote that a longbow should be "of yew or boxwood, seventy inches [70 in] between the points of attachment for the cord". Historian Jim Bradbury said they were an average of about 5 feet and 8 inches. All but the last estimate were made before the excavation of the "Mary Rose", where bows were found ranging in length from 1.87 to with an average length of 1.98 m.
Draw weights.
Estimates for the draw of these bows varies considerably. Before the recovery of the "Mary Rose", Count M. Mildmay Stayner, Recorder of the British Long Bow Society, estimated the bows of the Medieval period drew 90 –, maximum, and Mr. W.F. Paterson, Chairman of the Society of Archer-Antiquaries, believed the weapon had a supreme draw weight of only 80 –. Other sources suggest significantly higher draw weights. The original draw forces of examples from the "Mary Rose" are estimated by Robert Hardy at 150 – at a 30 in draw length; the full range of draw weights was between 100 –. The 30 in draw length was used because that is the length allowed by the arrows commonly found on the "Mary Rose".
A modern longbow's draw is typically 60 lb-f or less, and by modern convention measured at 28 in. Historically, hunting bows usually had draw weights of 50 –, which is enough for all but the very largest game and which most reasonably fit adults can manage with practice. Today, there are few modern longbowmen capable of using 180 – bows accurately.
A record of how boys and men trained to use the bows with high draw weights survives from the reign of Henry VII.
[My yeoman father] taught me how to draw, how to lay my body in my bow ... not to draw with strength of arms as divers other nations do ... I had my bows bought me according to my age and strength, as I increased in them, so my bows were made bigger and bigger. For men shall never shoot well unless they be brought up to it.—Hugh Latimer.
What Latimer meant when he describes laying his body into the bow was described thus:
the Englishman did not keep his left hand steady, and draw his bow with his right; but keeping his right at rest upon the nerve, he pressed the whole weight of his body into the horns of his bow. Hence probably arose the phrase "bending the bow," and the French of "drawing" one.—W. Gilpin.
Construction and materials.
The bowstave.
The preferred material to make the longbow was yew, although ash, elm and other woods were also used. Giraldus Cambrensis, Gerald of Wales, speaking of the bows used by the Welsh men of Gwent, says: "They are made neither of horn, ash nor yew, but of elm; ugly unfinished-looking weapons, but astonishingly stiff, large and strong, and equally capable of use for long or short shooting". The traditional construction of a longbow consists of drying the yew wood for 1 to 2 years, then slowly working the wood into shape, with the entire process taking up to four years. (This can be done far more quickly by working the wood down when wet, as a thinner piece of wood will dry much faster.) The bow stave is shaped into a D-section. The outer "back" of sapwood, approximately flat, follows the natural growth rings; modern bowyers often thin the sapwood, while in the "Mary Rose" bows the back of the bow was the natural surface of the wood, only the bark being removed. The inner side ("belly") of the bow stave consists of rounded heartwood. The heartwood resists compression and the outer sapwood performs better in tension. This combination in a single piece of wood (a self bow) forms a natural "laminate", somewhat similar in effect to the construction of a composite bow. Longbows will last a long time if protected with a water-resistant coating, traditionally of "wax, resin and fine tallow".
The trade of yew wood to England for longbows was such that it depleted the stocks of yew over a huge area. The first documented import of yew bowstaves to England was in 1294. In 1350 there was a serious shortage, and Henry IV of England ordered his royal bowyer to enter private land and cut yew and other woods. In 1470 compulsory practice was renewed, and hazel, ash, and laburnum were specifically allowed for practice bows. Supplies still proved insufficient, until by the Statute of Westminster in 1472, every ship coming to an English port had to bring four bowstaves for every tun. Richard III of England increased this to ten for every tun. This stimulated a vast network of extraction and supply, which formed part of royal monopolies in southern Germany and Austria. In 1483, the price of bowstaves rose from two to eight pounds per hundred, and in 1510 the Venetians obtained sixteen pounds per hundred. In 1507 the Holy Roman Emperor asked the Duke of Bavaria to stop cutting yew, but the trade was profitable, and in 1532 the royal monopoly was granted for the usual quantity "if there are that many". In 1562, the Bavarian government sent a long plea to the Holy Roman Emperor asking him to stop the cutting of yew, and outlining the damage done to the forests by its selective extraction, which broke the canopy and allowed wind to destroy neighbouring trees. In 1568, despite a request from Saxony, no royal monopoly was granted because there was no yew to cut, and the next year Bavaria and Austria similarly failed to produce enough yew to justify a royal monopoly.
Forestry records in this area in the 17th century do not mention yew, and it seems that no mature trees were to be had. The English tried to obtain supplies from the Baltic, but at this period bows were being replaced by guns in any case.
The string.
Bow strings were, and still are, made of hemp, flax or silk, and attached to the wood via horn "nocks" that fit onto the end of the bow. Modern synthetic materials (often Dacron) are now commonly used for strings.
The arrow.
A wide variety of arrows were shot from the English longbow. Variations in length, fletchings and heads are all recorded. Perhaps the greatest diversity lies in hunting arrows, with varieties like broad-arrow, wolf-arrow, dog-arrow, Welsh arrow and Scottish arrow being recorded. War arrows were ordered in the thousands for medieval armies and navies, supplied in sheaves normally of 24 arrows. For example, between 1341 and 1359 the English crown is known to have obtained 51,350 sheaves (1,232,400 arrows).
Only one significant group of arrows, from the Mary Rose, has survived. Over 3500 arrows were found, mainly made of poplar but also of ash, beech and hazel. Analysis of the intact specimens shows their length to vary from 61 to(-), with an average length of 76 cm. Because of the preservation conditions of the Mary Rose no arrowheads survived. However, many heads have survived in other places, which has allowed typologies of arrow heads to be produced, the most modern being the Jessop typology. The most common arrowheads in military use were the short bodkin (Jessop M10) and a small barbed arrow (Jessop M4).
Use and performance.
Training.
Longbows were very difficult to master because the force required to deliver an arrow through the improving armour of medieval Europe was very high by modern standards. Although the draw weight of a typical English longbow is disputed, it was at least 360 N and possibly more than 600 N, with some estimates as high as 900 N. Considerable practice was required to produce the swift and effective combat shooting required. Skeletons of longbow archers are recognisably adapted, with enlarged left arms and often bone spurs on left wrists, left shoulders and right fingers.
It was the difficulty in using the longbow which led various monarchs of England to issue instructions encouraging their ownership and practice, including the Assize of Arms of 1252 and King Edward III's declaration of 1363: "Whereas the people of our realm, rich and poor alike, were accustomed formerly in their games to practise archery – whence by God's help, it is well known that high honour and profit came to our realm, and no small advantage to ourselves in our warlike enterprises... that every man in the same country, if he be able-bodied, shall, upon holidays, make use, in his games, of bows and arrows... and so learn and practise archery." If the people practised archery, it would be that much easier for the King to recruit the proficient longbowmen he needed for his wars. Along with the improving ability of gunfire to penetrate plate armour, it was the long training needed by longbowmen which eventually led to their being replaced by musketmen.
Range.
The range of the medieval weapon is not accurately known, with much depending on both the power of the bow and the type of arrow. It has been suggested that a flight arrow of a professional archer of Edward III's time would reach 400 yd but the longest mark shot at on the London practice ground of Finsbury Fields in the 16th century was 345 yd. In 1542, Henry VIII set a minimum practice range for adults using flight arrows of 220 yd; ranges below this had to be shot with heavy arrows. Modern experiments broadly concur with these historical ranges. A 667 N (150 lbf) "Mary Rose" replica longbow was able to shoot a 53.6 g arrow 328 m and a 95.9 g a distance of 249.9 m. In 2012, Joe Gibbs shot a 2.25 oz livery arrow 292 yd with a 170 lbf yew bow.
Armour penetration.
Modern testing.
In an early modern test by Saxton Pope, a direct hit from a steel bodkin point penetrated Damascus mail armour.
A 2006 test was made by Matheus Bane using a 75 lbf draw (at 28") bow, shooting at 10 yards; according to Bane's calculations, this would be approximately equivalent to a 110 lbf bow at 250 yards. Measured against a replica of the thinnest contemporary "Jack coat" armour, a 905 grain needle bodkin and a 935 grain curved broadhead penetrated over 3.5 in. ("Jack coat" armour could be up to twice as thick as the coat tested; in Bane's opinion such a thick coat would have stopped bodkin arrows but not the cutting force of broadhead arrows.) Against "high quality riveted maille", the needle bodkin and curved broadhead penetrated 2.8". Against a coat of plates, the needle bodkin achieved 0.3" penetration. The curved broadhead did not penetrate but caused 0.3" of deformation of the metal. Results against plate armour of "minimum thickness" (1.2mm) were similar to the coat of plates, in that the needle bodkin penetrated to a shallow depth, the other arrows not at all. In Bane's view, the plate armour would have kept out all the arrows if thicker or worn with more padding.
Other modern tests described by Bane include those by Williams (which concluded that longbows could "not" penetrate mail, but in Bane's view did not use a realistic arrow tip), Robert Hardy's tests (which achieved broadly similar results to Bane), and a "Primitive Archer" test which demonstrated that a longbow could penetrate a plate armour breastplate. However, the "Primitive Archer" test used a 160 lbf longbow at very short range, generating 160 joules (vs. 73 for Bane and 80 for Williams), so probably not representative of battles of the time.
In 2011, Mike Loades conducted an experiment in which short bodkin arrows were shot at a range of 10 yd by bows of 140 lbf. The target was covered in a riveted mail over a fabric armour of deerskin over 24 linen layers. While most arrows went through the mail layer, none fully penetrated the textile armour. The experimenters, however, concluded that a long bodkin arrow would have penetrated through this armour combination. Even so, Loades cautions that this experiment did not reflect normal combat ranges and used powerful bows, so may not be typical of battlefield performance.
Other research has also concluded that later medieval armour, such as that of the Italian city state mercenary companies, was effective at stopping contemporary arrows.
Contemporary accounts.
Gerald of Wales commented on the power of the Welsh longbow in the 12th century:
... [I]n the war against the Welsh, one of the men of arms was struck by an arrow shot at him by a Welshman. It went right through his thigh, high up, where it was protected inside and outside the leg by his iron cuirasses, and then through the skirt of his leather tunic; next it penetrated that part of the saddle which is called the alva or seat; and finally it lodged in his horse, driving so deep that it killed the animal.
Archery was described by contemporaries as ineffective against plate armour in the Battle of Neville's Cross (1346), the siege of Bergerac (1345), and the Battle of Poitiers (1356); such armour became available to European knights of fairly modest means by the late 14th century, though never to all soldiers in any army. Strickland and Hardy suggest that "even at a range of 240 yards heavy war arrows shot from bows of poundages in the mid- to upper range possessed by the Mary Rose bows would have been capable of killing or severely wounding men equipped with armour of wrought iron. Higher-quality armour of steel would have given considerably greater protection, which accords well with the experience of Oxford's men against the elite French vanguard at Poitiers in 1356, and des Ursin's statement that the French knights of the first ranks at Agincourt, which included some of the most important (and thus best-equipped) nobles, remained comparatively unhurt by the English arrows".
Summary.
Modern tests and contemporary accounts agree therefore that well-made plate armour could protect against longbows. However this did not necessarily make the longbow ineffective; thousands of longbowmen were deployed in the English victory at Agincourt against plate armoured French knights in 1415. Clifford Rogers has argued that while longbows might not have been able to penetrate steel breastplates at Agincourt they could still penetrate the thinner armour on the limbs. Most of the French knights advanced on foot but, exhausted by walking across wet muddy terrain in heavy armour enduring a "terrifying hail of arrow shot", they were overwhelmed in the melee.
Less heavily armoured soldiers were more vulnerable than knights. For example, enemy crossbowmen were forced to retreat at Crecy when deployed without their protecting pavises. Horses were generally less well protected than the knights themselves; shooting the French knights' horses from the side (where they were less well armoured) is described by contemporary accounts of the Battle of Poitiers, and at Agincourt John Keegan has argued that the main effect of the longbow would have been in injuring the horses of the mounted French knights.
Shooting rate.
A typical military longbow archer would be provided with between 60 and 72 arrows at the time of battle. Most archers would not shoot arrows at maximum rate, as it would exhaust even the most experienced man. "With the heaviest bows [a modern warbow archer] does not like to try for more than six a minute." Not only do the arms and shoulder muscles tire from the exertion, but the fingers holding the bowstring become strained; therefore, actual rates of shooting in combat would vary considerably. Ranged volleys at the beginning of the battle would differ markedly from the closer, aimed shots as the battle progressed and the enemy neared. On the battlefield English archers stored their arrows stabbed upright into the ground at their feet, reducing the time it took to notch, draw and shoot.
Arrows were not unlimited, so archers and their commanders took every effort to ration their use to the situation at hand. Nonetheless, resupply during battle was available. Young boys were often employed to run additional arrows to longbow archers while in their positions on the battlefield. "The longbow was the machine gun of the Middle Ages: accurate, deadly, possessed of a long range and rapid rate of fire, the flight of its missiles was likened to a storm". This rate was much higher than that of its Western European projectile rival on the battlefield, the crossbow. It was also much higher than the standard early firearms, although the lower training requirements and greater penetration of firearms eventually led to the longbow falling into disuse.
Treating arrow wounds.
The only way to remove an arrow cleanly was to tie a piece of cloth soaked in water to the end of it and push it through the victim's wound and out the other side — this was extremely painful. There were specialised tools used in the medieval period to extract arrows from places where bone prevented the arrow being pushed through. Prince Hal, later Henry V, was wounded in the face by an arrow at the Battle of Shrewsbury (1403). The royal physician John Bradmore had such a tool made, which consisted of a pair of smooth tongs. Once carefully inserted into the socket of the arrowhead, the tongs screwed apart till they gripped its walls and allowed the head to be extracted from the wound. Prior to the extraction, the hole made by the arrow shaft had been widened by inserting larger and larger dowels of elder pith wrapped in linen down the entry wound. The dowels were soaked in honey, now known to have antiseptic properties. The wound was then dressed with a poultice of barley and honey mixed in turpentine. After 20 days the wound was free of infection.
History.
Etymology.
The first recorded use of the term 'longbow', as distinct from simply 'bow', occurs in a Paston Letter of the 15th century.
Origins.
The origins of the English longbow are disputed. While it is hard to assess the significance of military archery in pre-Norman Conquest Anglo-Saxon warfare, it is clear that archery played a prominent role under the Normans, as the story of the Battle of Hastings shows. Their Anglo-Norman descendants also made use of military archery, as exemplified by their victory at the Battle of the Standard in 1138. During the Anglo-Norman invasions of Wales, Welsh bowmen took a heavy toll of the invaders and Welsh archers would feature in English armies from this point on. However, historians dispute whether this archery used a different kind of bow to the later English Longbow. Traditionally it has been argued that prior to the beginning of the 14th century, the weapon was a self bow between four and five feet in length, known since the 19th century as the shortbow. This weapon, drawn to the chest rather than the ear, was much weaker. However, in 1985, Jim Bradbury reclassified this weapon as the "ordinary wooden bow", reserving the term shortbow for short composite bows and arguing that longbows were a developed form of this ordinary bow. Strickland and Hardy in 2005 took this argument further, suggesting that the shortbow was a myth and all early English bows were a form of longbow. In 2011, Clifford Rogers forcefully restated the traditional case based upon a variety of evidence, including a large scale iconographic survey. In 2012, Richard Wadge added to the debate with an extensive survey of record, iconographic and archaeological evidence, concluding that longbows co-existed with shorter self-wood bows in England in the period between the Norman conquest and the reign of Edward III, but that powerful longbows shooting heavy arrows were a rarity until the later 13th century. Whether or not there was a technological revolution at the end of the 13th century therefore remains in dispute. What is agreed, however, is that the English longbow as an effective weapon system evolved in the late 13th and early 14th centuries.
Fourteenth and fifteenth century.
The longbow decided many medieval battles fought by the English and Welsh, the most significant of which were the Battle of Crécy (1346) and the Battle of Agincourt (1415), during the Hundred Years' War and followed earlier successes, notably at the Battle of Falkirk (1298) and the Battle of Halidon Hill (1333) during the Scottish wars.
The longbow was also used against the English by their Welsh neighbours. The Welsh used the longbow mostly in a different manner than the English. In many early period English campaigns, the Welsh used the longbow in ambushes, often at point blank range that allowed their missiles to penetrate armour and generally do a lot of damage.
Although longbows were much faster and more accurate than the black-powder weapons which replaced them, longbowmen always took a long time to train because of the years of practice necessary before a war longbow could be used effectively (examples of longbows from the "Mary Rose" typically had draws greater than 637 N). In an era in which warfare was usually seasonal, and non-noble soldiers spent part of the year working at farms, the year-round training required for the effective use of the longbow was a challenge. A standing army was an expensive proposition to a medieval ruler. Mainland European armies seldom trained a significant longbow corps. Due to their specialized training, English longbowmen were sought as mercenaries in other European countries, most notably in the Italian city-states and in Spain.
The White Company, comprising men-at-arms and longbowmen and commanded by Sir John Hawkwood, is the best known English Free Company of the 14th century. The powerful Hungarian king, Louis the Great, is an example of someone who used longbowmen in his Italian campaigns.
Sixteenth century and after.
Longbows remained in use until around the 16th century, when advances in firearms made gunpowder weapons a significant factor in warfare and such units as arquebusiers and grenadiers began appearing. Before the English Civil War, a pamphlet by William Neade entitled "The Double-Armed Man" advocated that soldiers be trained in both the longbow and pike; this advice was followed only by a few town militias. The last recorded use of bows in an English battle seems to have been a skirmish at Bridgnorth, in October 1642, during the Civil War, when an impromptu town militia proved effective against un-armoured musketeers. Longbowmen remained a feature of the Royalist Army, but were not used by the Roundheads.
Longbows have been in continuous production and use for sport and for hunting to the present day, but since 1642 they have been a minority interest, and very few have had the high draw weights of the medieval weapons. Other differences include the use of a stiffened non-bending centre section, rather than a continuous bend.
Serious military interest in the longbow faded after the seventeenth century but occasionally schemes to resurrect its military use were proposed. Benjamin Franklin was a proponent in the 1770s; the Honourable Artillery Company had an archer company between 1784 and 1794; and a man named Richard Mason wrote a book proposing the arming of militia with pike and longbow in 1798. Donald Featherstone also records a Lt. Col. Richard Lee of 44th Foot advocated the military use of the longbow in 1792. There is a record of the use of the longbow in action as late as WWII, when Jack Churchill is credited with a longbow kill in France in 1940. The weapon was certainly considered for use by Commandos during the war but it is not known whether it was used in action.
Tactics.
Battle formations.
The idea that there was a standard formation for English longbow armies was argued by Alfred Byrne in his influential work on the battles of the Hundred Years' War, "The Crecy War". This view was challenged by Jim Bradbury in his book "The Medieval Archer" and more modern works are more ready to accept a variety of formations.
In summary, however, the usual English deployment in the 14th and 15th centuries was as follows:
In the 16th century, these formations evolved in line with new technologies and techniques from the continent. Formations with a central core of pikes and bills were flanked by companies of "shot" made up of a mixture of archers and arquebusiers, sometimes with a skirmish screen of archers and arquebusiers in front.
Surviving bows and arrows.
More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that capsized and sank at Portsmouth in 1545. It is an important source for the history of the longbow, as the bows, archery implements and the skeletons of archers have been preserved. The bows range in length from 1.87 to with an average length of 1.98 m The majority of the arrows were made of poplar, others were made of beech, ash and hazel. Draw lengths of the arrows varied between 61 and with the majority having a draw length of 76 cm. The head would add 5–15 cm depending on type, though some 2–4.5 cm must be allowed for the insertion of the shaft into the socket.
The longbows on the "Mary Rose" were in excellent finished condition. There were enough bows to test some to destruction which resulted in draw forces of 450 N (100 lbf) on average. However, analysis of the wood indicated that they had degraded significantly in the seawater and mud, which had weakened their draw forces. Replicas were made and when tested had draw forces of from 445 N to 823 N (100 to 185 lbf).
In 1980, before the finds from the "Mary Rose", Robert E. Kaiser published a paper stating that there were five known surviving longbows:
Social importance.
The importance of the longbow in English culture can be seen in the legends of Robin Hood, which increasingly depicted him as a master archer, and also in the "Song of the Bow", a poem from "The White Company" by Sir Arthur Conan Doyle.
During the reign of Henry III the Assize of Arms of 1252 required that all "citizens, burgesses, free tenants, villeins and others from 15 to 60 years of age" should be armed. The poorest of them were expected to have a halberd and a knife, and a bow if they owned land worth more than £2. This made it easier for the King to raise an army, but also meant that the bow was a weapon commonly used by rebels during the Peasants' Revolt. From the time that the yeoman class of England became proficient with the longbow, the nobility in England had to be careful not to push them into open rebellion.
It has been conjectured that yew trees were commonly planted in English churchyards to have readily available longbow wood.

</doc>
<doc id="18433" url="http://en.wikipedia.org/wiki?curid=18433" title="Lee Marvin">
Lee Marvin

Lee Marvin (February 19, 1924 – August 29, 1987) was an American film and television actor. Known for his distinctive voice, white hair and 6 ft stature, Marvin initially appeared in supporting roles, mostly villains, soldiers and other hardboiled characters. From 1957 to 1960, he starred as Detective Lieutenant Frank Ballinger in the NBC hit crime series, "M Squad".
In 1966 he won several awards, including an Academy Award for Best Actor, and Best Actor BAFTA and the Best Actor Golden Globe, for his dual roles in "Cat Ballou".
Early life.
Marvin was born in New York City. He was the son of Lamont Waltman Marvin, an advertising executive and the head of the New York and New England Apple Institute, and his wife Courtenay Washington (née Davidge), a fashion writer and beauty consultant. As with his older brother, Robert, he was named in honor of Confederate General Robert E. Lee, who was his first cousin, four times removed. His father was a direct descendant of Matthew Marvin, Sr., who emigrated from Great Bentley, Essex, England in 1635 and helped found Hartford, Connecticut.
Marvin studied violin when he was young. As a teenager, Marvin "spent weekends and spare time hunting deer, puma, wild turkey and bobwhite in the wilds of the then-uncharted Everglades."
He attended Manumit School, a Christian socialist boarding school in Pawling, New York, during the late 1930s, and later attended St. Leo College Preparatory School in St. Leo, Florida after being expelled from several other schools for bad behavior.
Military service.
In August 1942 Marvin left school to enlist in the United States Marine Corps, serving with the 4th Marine Division in the Pacific Theater of World War II. He was wounded in action during the Battle of Saipan, in the assault on Mount Tapochau, during which most of his unit ("I" Company, 24th Marines, 4th Marine Division) were killed. His injury was from machine gun fire, which severed his sciatic nerve. Marvin was awarded the Purple Heart and was given a medical discharge with the rank of Private First Class in 1945 at Philadelphia. Marvin's awards were the Purple Heart, the Presidential Unit Citation, the Asiatic-Pacific Campaign Medal and the World War II Victory Medal. Contrary to rumors, Marvin did not serve with producer and actor Bob Keeshan (later best known as Captain Kangaroo) during World War II.
Career.
After the war, while working as a plumber's assistant at a local community theatre in Upstate New York, Marvin was asked to replace an actor who had fallen ill during rehearsals. He then began an amateur off-Broadway acting career in New York City and eventually made it to Broadway with a small role in the original production of Billy Budd.
In 1950, Marvin moved to Hollywood. He found work in supporting roles, and from the beginning was cast in various war films. As a decorated combat veteran, Marvin was a natural in war dramas, where he frequently assisted the director and other actors in realistically portraying infantry movement, arranging costumes, and the use of firearms. His debut was in "You're in the Navy Now" (1951), and in 1952 he appeared in several films, including Don Siegel's "Duel at Silver Creek", "Hangman's Knot", and the war drama "Eight Iron Men". He played Gloria Grahame's vicious boyfriend in Fritz Lang's "The Big Heat" (1953). Marvin had a small but memorable role in "The Wild One" (1953) opposite Marlon Brando (Marvin's gang in the film was called "The Beetles"), followed by "Seminole" (1953) and "Gun Fury" (1953). He also had a notable small role as smart-aleck sailor Meatball in "The Caine Mutiny". He had a substantially more important part as Hector, the small-town hood in "Bad Day at Black Rock" (1955) with Spencer Tracy.
During the mid-1950s, Marvin gradually began playing more important roles. He starred in "Attack", (1956) and had a supporting role in the Western "Seven Men from Now" (1956). He also starred in "The Missouri Traveler" (1958) but it took over 100 episodes as Chicago cop Frank Ballinger in the successful 1957–1960 television series "M Squad" to actually give him name recognition.
One critic described the show as "a hyped-up, violent "Dragnet"... with a hard-as-nails Marvin" playing a tough police lieutenant. Marvin received the role after guest-starring in a memorable "Dragnet" episode as a serial killer. 
In the 1960s, Marvin was given prominent supporting roles in such films as "The Comancheros" (1961), John Ford's "The Man Who Shot Liberty Valance" (1962), and "Donovan's Reef" (1963), all starring John Wayne, with Marvin's roles getting larger with each film. As the vicious Liberty Valance, Marvin played his first title role and held his own with two of the screen's biggest stars (Wayne and James Stewart).
For director Don Siegel, Marvin appeared in "The Killers" (1964) playing an efficient professional assassin alongside Clu Gulager. "The Killers" was also the first film in which Marvin received top billing.
Playing alongside Vivien Leigh and Simone Signoret, Marvin won the 1966 National Board of Review Award for male actors for his role in "Ship of Fools" (1965).
Marvin won the 1965 Academy Award for Best Actor for his comic role in the offbeat Western "Cat Ballou" starring Jane Fonda. He also won the 1965 Silver Bear for Best Actor at the 15th Berlin International Film Festival.
Marvin next performed in the hit Western "The Professionals" (1966), in which he played the leader of a small band of skilled mercenaries (Burt Lancaster, Robert Ryan, and Woody Strode) rescuing a kidnap victim (Claudia Cardinale) shortly after the Mexican Revolution. He followed that film with the hugely successful World War II epic "The Dirty Dozen" (1967) in which top-billed Marvin again portrayed an intrepid commander of a colorful group (future stars John Cassavetes, Charles Bronson, Telly Savalas, Jim Brown, and Donald Sutherland) performing an almost impossible mission. In the wake of these two films and after having received an Oscar, Marvin was a huge star, given enormous control over his next film "Point Blank".
In "Point Blank", an influential film for director John Boorman, he portrayed a hard-nosed criminal bent on revenge. Marvin, who had selected Boorman himself for the director's slot, had a central role in the film's development, plot line, and staging. In 1968, Marvin also appeared in another Boorman film, the critically acclaimed but commercially unsuccessful World War II character study "Hell in the Pacific", also starring famed Japanese actor Toshiro Mifune. Marvin was originally cast as Pike Bishop (later played by William Holden) in "The Wild Bunch" (1969), but fell out with director Sam Peckinpah and pulled out in order to star in the Western musical "Paint Your Wagon" (1969), in which he was top-billed over a singing Clint Eastwood. Despite his limited singing ability, he had a hit song with "Wand'rin' Star". By this time he was getting paid a million dollars per film, $200,000 less than top star Paul Newman was making at the time; yet he was ambivalent about the film business, even with its financial rewards:
Marvin had a much greater variety of roles in the 1970s and 1980s, with fewer 'bad-guy' roles than in earlier years. His 1970s films included "Monte Walsh" (1970) with Jeanne Moreau, the violent "Prime Cut" (1972) with Gene Hackman, "Pocket Money" (1972) with Paul Newman, "Emperor of the North Pole" (1973) opposite Ernest Borgnine, as Hickey in "The Iceman Cometh" (1973) with Fredric March and Robert Ryan, "The Spikes Gang" (1974) with Noah Beery, Jr., "The Klansman" (1974) with Richard Burton, "Shout at the Devil" (1976) with Roger Moore, "The Great Scout and Cathouse Thursday" (1976) with Oliver Reed, and "Avalanche Express" (1978) with Robert Shaw. Marvin was offered the role of Quint in "Jaws" (1975) but declined, stating "What would I tell my fishing friends who'd see me come off a hero against a dummy shark?".
Marvin's last big role was in Samuel Fuller's "The Big Red One" (1980), a war film based on Fuller's own war experiences. His remaining films were "Death Hunt" (1981) with Charles Bronson, "Gorky Park" (1983), "Dog Day" (1984), and "" (1985; a sequel with Marvin, Ernest Borgnine, and Richard Jaeckel picking up where they'd left off despite being 18 years older); his final appearance was in "The Delta Force" (1986) with Chuck Norris.
Personal life.
During the 1970s, Marvin resided off and on in Woodstock, caring for his dying father, and as a keen fisherman he used to make regular trips to Australia to engage in fishing for marlin at Cairns and Great White Shark at Port Fairy. In 1975 Marvin and his second wife Pamela moved to Tucson, Arizona, where he lived until his death.
Marvin was a Democrat who opposed the Vietnam War. He publicly endorsed John F. Kennedy in the 1960 presidential election.
Marriages and children.
A father of four, Marvin was married twice. His first marriage to Betty Ebeling began in February 1951 and ended in divorce on January 5, 1967; during this time his hobbies included sport fishing off the Baja California coast and duck hunting along the Mexican border near Mexicali. He and Ebeling had a son, Christopher (1952–2013), and three daughters: Courtenay (b. 1954), Cynthia (b. 1956) and Claudia (1958-2012).
Marvin was married to Pamela Feeley from October 18, 1970 until his death.
Community property case.
In 1971, Marvin was sued by Michelle Triola, his live-in girlfriend from 1965 to 1970, who legally changed her surname to "Marvin". Although the couple never married, she sought financial compensation similar to that available to spouses under California's alimony and community property laws. Triola claimed Marvin made her pregnant three times and paid for two abortions, while one pregnancy ended in miscarriage. She claimed the second abortion left her unable to bear children. The result was the landmark "palimony" case, "Marvin v. Marvin", 18 Cal. 3d 660 (1976). In 1979, Marvin was ordered to pay $104,000 to Triola for "rehabilitation purposes" but the court denied her community property claim for one-half of the $3.6 million which Marvin had earned during their six years of cohabitation – distinguishing non-marital relationship contracts from marriage, with community property rights only attaching to the latter by operation of law. Rights equivalent to community property only apply in non-marital relationship contracts when the parties expressly, whether orally or in writing, contract for such rights to operate between them. After the case, Marvin was the subject of controversy when he said that the trial was a "circus" and that "everyone was lying, even I lied." 
In August 1981, the California Court of Appeal found there was no such contract, and thus nullified the award she had received. Michelle Triola died of lung cancer on October 30, 2009.
This case was used as fodder for a mock debate skit on "Saturday Night Live" called "Point Counterpoint", and on The Tonight Show Starring Johnny Carson as a skit with Carson as Adam, and Betty White as Eve.
Death.
In December 1986, Marvin underwent intestinal surgery after suffering abdominal pains while at his ranch outside Tucson. Doctors said then that there was an inflammation of the colon, but that no malignancy was found. After being hospitalized for more than two weeks because of "a run-down condition related to the flu," Marvin died of a heart attack on August 29, 1987. He is interred at Arlington National Cemetery where his headstone reads "Lee Marvin, PFC, US Marine Corps, World War II".
Television appearances.
Marvin's appearances on television included
"Suspense" (1 episode, 1950), 
"Rebound", 
"M Squad", 
"Climax!", 
"Biff Baker, U.S.A.", 
"Dragnet", 
"The Johnny Carson Show", 
"The Ford Show Starring Tennessee Ernie Ford", 
"General Electric Theater", 
"The Americans", 
"The Investigators", 
"The Barbara Stanwyck Show", 
"Route 66", 
"The Untouchables", 
"Checkmate", 
"The Dick Powell Show", 
"Combat!", 
"The Twilight Zone", 
"Kraft Suspense Theatre", 
"Dr. Kildare", 
"Wagon Train", 
"Bonanza", 
"The Virginian"
References.
Bibliography.
</dl>

</doc>
<doc id="18434" url="http://en.wikipedia.org/wiki?curid=18434" title="Lead Belly">
Lead Belly

Huddie William Ledbetter (January 20, 1888 – December 6, 1949) was an American folk and blues musician notable for his strong vocals, virtuosity on the twelve-string guitar, and the songbook of folk standards he introduced.
He is best known as Lead Belly. Though many releases list him as "Leadbelly", he himself wrote it as "Lead Belly". This is also the spelling on his tombstone, as well as of the Lead Belly Foundation.
Although Lead Belly usually played the twelve-string guitar, he could also play the piano, mandolin, harmonica, violin, and "windjammer" (diatonic accordion). In some of his recordings he sings while clapping his hands or stomping his foot.
The topics of Lead Belly's music covered a wide range, including gospel; blues about women, liquor, prison life, and racism; and folk songs about cowboys, prison, work, sailors, cattle herding, and dancing. He also wrote songs about people in the news, such as Franklin D. Roosevelt, Adolf Hitler, Jean Harlow, the Scottsboro Boys, and Howard Hughes.
Lead Belly was inducted into the Rock and Roll Hall of Fame in 1988 and the Louisiana Music Hall of Fame in 2008.
Biography.
Early life.
Lead Belly was born Huddie William Ledbetter on the Jeter Plantation near Mooringsport, Louisiana in either January 1888 or 1889. The 1900 United States Census lists "Hudy William Ledbetter" as 12 years old, born January 1888; and the 1910 and 1930 censuses also give his birth year as 1888. However, in April 1942, Ledbetter filled out his World War II draft registration with a birth date of January 23, 1889 and a birthplace of Freeport, Louisiana. His grave marker has the date on his draft registration.
Ledbetter was the younger of two children born to Wesley Ledbetter and Sallie Brown, preceded by a sister named Australia. The pronunciation of his name is purported to be "HYEW-dee" or "HUGH-dee." Ledbetter can be heard pronouncing his name as "HYEW-dee" on the track "Boll Weevil," from the Smithsonian Folkways album "Lead Belly Sings for Children." His parents had cohabited for several years, but they legally married on February 26, 1888. When Huddie was five years old, the family settled in Bowie County, Texas.
By 1903, Huddie was already a "musicianer," a singer and guitarist of some note. He performed for nearby Shreveport audiences in St. Paul's Bottoms, a notorious red-light district there. He began to develop his own style of music after exposure to a variety of musical influences on Shreveport's Fannin Street, a row of saloons, brothels, and dance halls in the Bottoms, now referred to as Ledbetter Heights.
The 1910 census of Harrison County, Texas, shows "Hudy" Ledbetter living next door to his parents with his first wife, Aletha "Lethe" Henderson. Aletha, then seventeen, had been 15 when they married two years earlier. It was in Texas that Ledbetter received his first instrument, an accordion, from his uncle Terrell. By his early 20s, having fathered at least two children, Ledbetter left home to make his living as a guitarist and occasional laborer.
Influenced by the sinking of the "RMS Titanic" in April 1912, Huddie wrote the song "The Titanic", the first composed on the 12-string guitar later to become his signature instrument. Initially played when performing with Blind Lemon Jefferson (1897–1929) in and around Dallas, Texas, the song is about champion African-American boxer Jack Johnson's being denied passage on the "Titanic". While Johnson had in fact been denied passage on a ship for being Black, it had not been the "Titanic". Still, the verse sang: "Jack Johnson tried to get on board. The Captain, he says, 'I ain't haulin' no coal!' Fare thee, "Titanic"! Fare thee well!" Ledbetter later noted he had to leave out this passage when playing in front of white audiences.
Prison years.
Ledbetter's volatile temper sometimes led him into trouble with the law. In 1915, he was convicted of carrying a pistol and sentenced to time on the Harrison County chain gang. He escaped, finding work in nearby Bowie County under the assumed name of Walter Boyd. In January 1918 he was imprisoned at the Imperial Farm (now Central Unit) in Sugar Land, Texas, after killing one of his own relatives, Will Stafford, in a fight over a woman. While there he may have first heard the traditional prison song "Midnight Special". In 1925 he was pardoned and released after writing a song to Governor Pat Morris Neff seeking his freedom, having served the minimum seven years of a 7-to-35-year sentence. Combined with his good behavior (which included entertaining the guards and fellow prisoners), his appeal to Neff's strong religious beliefs proved sufficient. It was a testament to his persuasive powers, as Neff had run for governor on a pledge not to issue pardons (the only recourse for prisoners, since in most Southern prisons there was no provision for parole). According to Charles K. Wolfe and Kip Lornell's book, "The Life and Legend of Leadbelly" (1999), Neff had regularly brought guests to the prison on Sunday picnics to hear Ledbetter perform.
In 1930 Ledbetter was sentenced to Louisiana's Angola Prison Farm, after a summary trial for attempted homicide for stabbing a white man in a fight. He was "discovered" there three years later during a visit by folklorists John Lomax and his son Alan Lomax.
Deeply impressed by Ledbetter's vibrant tenor and extensive repertoire, the Lomaxes recorded him in 1933 on portable aluminum disc recording equipment for the Library of Congress. They returned with new and better equipment in July 1934, recording hundreds of his songs. On August 1, Ledbetter was released after having again served nearly all of his minimum sentence, following a petition the Lomaxes had taken to Louisiana Governor Oscar K. Allen at his urgent request. It was on the other side of a recording of his signature song, "Goodnight Irene."
A prison official later wrote John Lomax denying that Ledbetter's singing had anything to do with his release from Angola (state prison records confirm he was eligible for early release due to good behavior). However, both Ledbetter and the Lomaxes believed that the record they had taken to the governor had hastened his release from prison.
Moniker.
There are several conflicting stories about how Ledbetter acquired the nickname "Lead Belly", though it was probably while in prison. Some claim his fellow inmates called him "Lead Belly" as a play on his family name and his physical toughness. It is recounted that during his second prison term, another inmate stabbed him in the neck (leaving him with a fearsome scar he subsequently covered with a bandana); Ledbetter nearly killed his attacker with his own knife. Others say he earned the name after being wounded in the stomach with buckshot. Another theory is that the name refers to his ability to drink moonshine, the home-made liquor which Southern farmers, black and white, made to supplement their incomes. Blues singer Big Bill Broonzy thought it came from a supposed tendency to lay about as if "with a stomach weighted down by lead" in the shade when the chain gang was supposed to be working. Or it may be simply a corruption of his last name pronounced with a southern accent. Whatever its origin, he adopted the nickname as a pseudonym while performing.
Life after prison.
By the time Lead Belly was released from prison the United States was deep in the Great Depression and jobs were very scarce. In September 1934, in need of regular work in order to avoid having his release canceled, Lead Belly met with John A. Lomax and asked him to take him on as a driver. For three months he assisted the 67-year-old in his folk song collecting abroad the South. (Son Alan was ill and did not accompany his father on this trip.)
In December Lead Belly participated in a "smoker" (group sing) at a Modern Language Association meeting at Bryn Mawr College in Pennsylvania, where the senior Lomax had a prior lecturing engagement. He was written up in the press as a convict who had sung his way out of prison. On New Year's Day, 1935, the pair arrived in New York City, where Lomax was scheduled to meet with his publisher, Macmillan, about a new collection of folk songs. The newspapers were eager to write about the "singing convict" and "Time" magazine made one of its first filmed March of Time newsreels about him. Lead Belly attained fame (although not fortune).
The following week, he began recording for the American Record Corporation, but these recordings achieved little commercial success. Of the over 40 sides he recorded for ARC (intended to be released on their Banner, Melotone, Oriole, Perfect, and Romeo labels, and their very short-lived Paramount series), only five sides were actually issued. Part of the reason for the poor sales may have been because ARC insisted on releasing only his blues songs rather than the folk songs for which he would later become better known. In any case, Lead Belly continued to struggle financially. Like many performers, what income he made during his lifetime would come from touring, not from record sales.
In February 1935, he married his girlfriend, Martha Promise, who came north from Louisiana to join him.
The month of February was spent recording his and other African-American repertoire and interviews about his life with Alan Lomax for their forthcoming book, "Negro Folk Songs as Sung by Lead Belly" (1936). Concert appearances were however slow to materialize. In March 1935 Lead Belly accompanied John Lomax on a previously scheduled two-week lecture tour of colleges and universities in the Northeast, culminating at Harvard.
At the end of the month John Lomax decided he could no longer work with Lead Belly and gave him and Martha money to go back to Louisiana by bus. He gave Martha the money her husband had earned during three months of performing, but in installments, on the pretext Lead Belly would drink it all if given a lump sum. From Louisiana Lead Belly successfully sued Lomax both for the full amount and release from his management contract. The quarrel was very bitter with hard feelings on both sides. Curiously, in the midst of the legal wrangling, Lead Belly wrote to Lomax proposing they team up again. It was not to be. Further, the book about Lead Belly published by the Lomaxes in the fall of the following year proved a commercial failure.
In January 1936, Lead Belly returned to New York on his own without John Lomax in an attempted comeback. He performed twice a day at Harlem's Apollo Theater during the Easter season in a live dramatic recreation of the "Time Life" newsreel (itself a recreation) about his prison encounter with John A. Lomax, where he had worn stripes, though by this time he was no longer associated with Lomax.
"Life" magazine ran a three-page article titled, "Lead Belly - Bad Nigger Makes Good Minstrel," in its April 19, 1937 issue. It included a full-page, color (rare in those days) picture of him sitting on grain sacks playing his guitar and singing. Also, included was a striking picture of Martha Promise (identified in the article as his manager); photos showing Lead Belly's hands playing the guitar (with the caption "these hands once killed a man"); Texas Governor Pat M. Neff; and the "ramshackle" Texas State Penitentiary. The article attributes both of his pardons to his singing of his petitions to the governors, who were so moved that they pardoned him. The article's text ends with "he... may well be on the brink of a new and prosperous period."
Lead Belly failed to stir the enthusiasm of Harlem audiences. Instead, he attained success playing at concerts and benefits for an audience of leftist folk music "aficionados." He developed his own style of singing and explaining his repertoire in the context of Southern black culture, taking the hint from his previous participation in John A. Lomax's college lectures. He was especially successful with his repertoire of children's game songs (as a younger man in Louisiana he had sung regularly at children's birthday parties in the black community). He was written up as a heroic figure by the black novelist, Richard Wright, then a member of the Communist Party, in the columns of the "Daily Worker," of which Wright was the Harlem editor. The two men became personal friends, though Lead Belly himself was apolitical — if anything he was a supporter of Wendell Willkie, the centrist Republican candidate, for whom he wrote a campaign song.
In 1939, Lead Belly was back in jail for assault, after stabbing a man in a fight in Manhattan. Alan Lomax, then 24, took him under his wing and helped raise money for his legal expenses, dropping out of graduate school to do so. After his release (in 1940-41), Lead Belly appeared as a regular on Alan Lomax and Nicholas Ray's groundbreaking CBS radio show, "Back Where I Come From," broadcast nationwide. He also appeared in night clubs with Josh White, becoming a fixture in New York City's surging folk music scene and befriending the likes of Sonny Terry, Brownie McGhee, Woody Guthrie, and a young Pete Seeger, all fellow performers on "Back Where I Come From." During the first half of the decade he recorded for RCA, the Library of Congress, and for Moe Asch (future founder of Folkways Records), and in 1944 headed to California, where he recorded strong sessions for Capitol Records.He lodged with a studio guitar player on Merrywood Drive in Laurel Canyon. 
Lead Belly was the first American country blues musician to see success in Europe.
In 1949, Lead Belly had a regular radio broadcast on station WNYC in New York on Sunday nights on Henrietta Yurchenco's show. Later in the year he began his first European tour with a trip to France, but fell ill before its completion, and was diagnosed with amyotrophic lateral sclerosis (ALS), or Lou Gehrig's disease. His final concert was at the University of Texas at Austin in a tribute to his former mentor, John A. Lomax, who had died the previous year. Martha also performed at that concert, singing spirituals with her husband.
Lead Belly died later that year in New York City, and was buried in the Shiloh Baptist Church cemetery in Mooringsport, 8 mi west of Blanchard, in Caddo Parish. He is honored with a statue across from the Caddo Parish Courthouse in Shreveport.
Technique.
Lead Belly styled himself "King of the 12-string guitar," and despite his use of other instruments like the accordion, the most enduring image of Lead Belly as a performer is wielding his unusually large Stella twelve-string. This guitar had a slightly longer scale length than a standard guitar, slotted tuners, ladder bracing, and a trapeze-style tailpiece to resist bridge lifting.
Lead Belly played with finger picks much of the time, using a thumb pick to provide a walking bass line and occasionally to strum. This technique, combined with low tunings and heavy strings, gives many of his recordings a piano-like sound. Lead Belly's tuning is debated, but appears to be a downtuned variant of standard tuning; more than likely he tuned his guitar strings relative to one another, so that the actual notes shifted as the strings wore. Lead Belly's playing style was popularized by Pete Seeger, who adopted the twelve-string guitar in the 1950s and released an instructional LP and book using Lead Belly as an exemplar of technique.
In some of the recordings where Lead Belly accompanied himself, he would make an unusual type of grunt between his verses, best described as "Haah!" in many of his songs such as "Looky Looky Yonder," "Take this Hammer," "Linin' Track" and "Julie Ann Johnson" feature this unusual vocalization. In the song, "Take this Hammer," Lead Belly explained, "Every time the men say 'haah,' the hammer falls. The hammer rings, and we swing, and we sing." The "haah" sound can be heard in the work chants sung by Southern railroad section workers, "gandy dancers," where it was used to coordinate the crews as they laid and maintained the tracks before modern machinery was available.
Legacy.
Lead Belly's work has been widely covered by subsequent musical acts, including Brian Wilson, Delaney Davidson, Tom Russell, Lonnie Donegan, Bryan Ferry ("Goodnight Irene"), The Beach Boys ("Cotton Fields"), Creedence Clearwater Revival ("Midnight Special", "Cotton Fields"), Elvis Presley, Abba, Pete Seeger, The Weavers, Harry Belafonte, Frank Sinatra, Ram Jam, The Animals, Jay Farrar, Johnny Cash, Bob Dylan, Tom Petty, Dr. John, Ry Cooder, Davy Graham, Maria Muldaur, Rory Block, Grateful Dead, Gene Autry, Odetta, Billy Childish (who named his son Huddie), Mungo Jerry, Paul King, Led Zeppelin ("Gallows Pole"), Van Morrison, Michelle Shocked, Tom Waits ("Goodnight, Irene"), Scott H. Biram, Ron Sexsmith, British Sea Power, Rod Stewart, Ernest Tubb, Nick Cave and the Bad Seeds, Ram Jam ("Black Betty"), Blind Willies("In the Pines"), The White Stripes ("Boll Weevil"), The Fall, Hole, Smog, Old Crow Medicine Show, Spiderbait, Meat Loaf, Ministry, Raffi, Rasputina, Rory Gallagher (Out on the Western Plains), the Sensational Alex Harvey Band, Deer Tick, Hugh Laurie, X, Bill Frisell, Koerner, Ray & Glover, Red Hot Chili Peppers, Nirvana, Meat Puppets, Mark Lanegan, WZRD ("Where Did You Sleep Last Night"), and Phil Lee (I Got Stripes), among many others.
Modern rock audiences likely owe their familiarity with Lead Belly to Nirvana's performance of "Where Did You Sleep Last Night" on the televised concert later released as "MTV Unplugged in New York". Singer-guitarist Kurt Cobain refers to his attempt to convince David Geffen to purchase Lead Belly's guitar for him in an interval before the song is played (connecting the song with Lead Belly in a way that is more tangible than the liner notes where Lead Belly appears on other albums), and partly due to the fact that it sold nearly 7 million copies. In his notebooks, Cobain listed Lead Belly's "Last Session Vol. 1" as one of the 50 albums most influential to the formation of Nirvana's sound.
Discography.
The Library of Congress recordings.
The Library of Congress recordings, done by John and Alan Lomax from 1934 to 1943, were released in a six volume series by Rounder Records in the early-to-mid-1990s:
Folkways recordings.
The Folkways recordings, done for Moses Asch from 1941 to 1947, were released in a three volume series by Smithsonian Folkways in the late 1990s:
Smithsonian Folkways have also released a number of other collections of his recordings for the label:

</doc>
<doc id="18435" url="http://en.wikipedia.org/wiki?curid=18435" title="Lower Saxony">
Lower Saxony

Lower Saxony (German: "Niedersachsen" ], Low German: "Neddersassen"; Dutch: "Nedersaksen") is a German state "(Bundesland)" situated in northwestern Germany and is second in area, with 47,624 km2, and fourth in population (8 million) among the sixteen "Länder" of Germany. In rural areas Northern Low Saxon, a dialect of Low German, and Saterland Frisian, a variety of Frisian, are still spoken, but the number of speakers is declining.
Lower Saxony borders on (from north and clockwise) the North Sea, the states of Schleswig-Holstein, Hamburg, Mecklenburg-Vorpommern, Brandenburg, Saxony-Anhalt, Thuringia, Hesse and North Rhine-Westphalia, and the Netherlands. Furthermore, the state of Bremen forms two enclaves within Lower Saxony, one being the city of Bremen, the other, its seaport city of Bremerhaven. In fact, Lower Saxony borders more neighbours than any other single "Bundesland." The state's principal cities include the state capital Hanover, Brunswick, Lüneburg, Osnabrück, Oldenburg, Hildesheim, Wolfenbüttel, Wolfsburg and Göttingen.
The northwestern area of Lower Saxony, which lies on the coast of the North Sea, is called East Frisia and the seven East Frisian Islands offshore are popular with tourists. In the extreme west of Lower Saxony is the Emsland, a traditionally poor and sparsely populated area, once dominated by inaccessible swamps. The northern half of Lower Saxony, also known as the North German Plains, is almost invariably flat except for the gentle hills around the Bremen geestland. Towards the south and southwest lie the northern parts of the German Central Uplands: the Weser Uplands and the Harz mountains. Between these two lie the Lower Saxon Hills, a range of low ridges. Thus, Lower Saxony is the only "Bundesland" that encompasses both maritime and mountainous areas.
Lower Saxony's major cities and economic centres are mainly situated in its central and southern parts, namely Hanover, Brunswick, Osnabrück, Wolfsburg, Salzgitter, Hildesheim and Göttingen. Oldenburg, near the northwestern coastline, is another economic centre. The region in the northeast is called the Lüneburg Heath ("Lüneburger Heide"), the largest heathland area of Germany and in medieval times wealthy due to salt mining and salt trade, as well as to a lesser degree the exploitation of its peat bogs up until about the 1960s. To the north, the Elbe river separates Lower Saxony from Hamburg, Schleswig-Holstein, Mecklenburg-Western Pomerania and Brandenburg. The banks just south of the Elbe are known as "Altes Land (Old Country)". Due to its gentle local climate and fertile soil it is the state's largest area of fruit farming, its chief produce being apples.
Most of the state's territory was part of the historic Kingdom of Hanover; the state of Lower Saxony has adopted the coat of arms and other symbols of the former kingdom. It was created by the merger of the State of Hanover with several smaller states in 1946.
Geography.
Location.
Lower Saxony has a natural boundary in the north in the North Sea and the lower and middle reaches of the River Elbe, although parts of the city of Hamburg lie south of the Elbe. The state and city of Bremen is an enclave entirely surrounded by Lower Saxony. The Bremen/Oldenburg Metropolitan Region is a cooperative body for the enclave area. To the southeast the state border runs through the Harz, low mountains that are part of the German Central Uplands. The northeast and west of the state – which form roughly three-quarters of its land area – belong to the North German Plain, while the south is in the Lower Saxon Hills, including the Weser Uplands, Leine Uplands, Schaumburg Land, Brunswick Land, Untereichsfeld, Elm and Lappwald. In northeast Lower Saxony is Lüneburg Heath. The heath is dominated by the poor sandy soils of the geest, whilst in the central east and southeast in the loess "börde" zone there are productive soils with high natural fertility. Under these conditions—with loam and sand-containing soils—the land is well-developed agriculturally. In the west lie the County of Bentheim, Osnabrück Land, Emsland, Oldenburg Land, Ammerland, Oldenburg Münsterland and – on the coast – East Frisia.
The state is dominated by several large rivers running northwards through the state: the Ems, Weser, Aller and Elbe.
The highest mountain in Lower Saxony is the Wurmberg (971 m) in the Harz. For other significant elevations see: List of mountains and hills in Lower Saxony. Most of the mountains and hills are found in the southeastern part of the state. The lowest point in the state, at about 2.5 metres below sea level, is a depression near Freepsum in East Frisia.
The state's economy, population and infrastructure are centred on the cities and towns of Hanover, Stadthagen, Celle, Brunswick, Wolfsburg, Hildesheim and Salzgitter. Together with Göttingen in southern Lower Saxony, they form the core of the Hanover-Brunswick-Göttingen-Wolfsburg Metropolitan Region.
Regions.
General.
Lower Saxony has clear regional divisions that manifest themselves both geographically as well as historically and culturally. In the regions that used to be independent, especially the heartlands of the former states of Brunswick, Hanover, Oldenburg and Schaumburg-Lippe, there is a marked local regional awareness. By contrast, the areas surrounding the Hanseatic cities of Bremen and Hamburg are much more oriented towards those centres.
List of regions.
Sometimes there are overlaps and transition areas between the various regions of Lower Saxony. Several of the regions listed here are part of other, larger regions, that are also included in the list.
Just under 20% of the land area of Lower Saxony is designated as nature parks, i.e.: Dümmer, Elbhöhen-Wendland, Elm-Lappwald, Harz, Lüneburger Heide, Münden, Terra.vita, Solling-Vogler, Lake Steinhude, Südheide, Weser Uplands, Wildeshausen Geest, Bourtanger Moor-Bargerveen.
Climate.
Lower Saxony falls climatically into the north temperate zone of central Europe that is affected by prevailing Westerlies and is located in a transition zone between the maritime climate of Western Europe and the continental climate of Eastern Europe. This transition is clearly noticeable within the state: whilst the northwest experiences an Atlantic (North Sea coastal) to Sub-Atlantic climate, with comparatively low variations in temperature during the course of the year and a surplus water budget, the climate towards the southeast is increasingly affected by the Continent. This is clearly shown by greater temperature variations between the summer and winter halves of the year and in lower and more variable amounts of precipitation across the year. This sub-continental effect is most sharply seen in the Wendland, in the Weser Uplands (Hamelin to Göttingen) and in the area of Helmstedt. The highest levels of precipitation are experienced in the Harz because the Lower Saxon part forms the windward side of this mountain range against which orographic rain falls. The average annual temperature is 8 °C (7.5 °C in the Altes Land and 8.5 °C in the district of Cloppenburg).
Neighbouring states.
Fellow "Länder" bordering on Lower Saxony are Bremen, Hamburg, Schleswig-Holstein, Mecklenburg-Vorpommern, Brandenburg, Saxony-Anhalt, Thuringia, Hesse and North Rhine-Westphalia. No other German state has so many neighbours.
Lower Saxony also has a border with the Dutch provinces of Overijssel, Drenthe and Groningen as well as part of the German North Sea coast.
Administration.
Lower Saxony is divided into 38 districts ("Landkreise" or simply "Kreise"):
Furthermore there are ten urban districts:
¹ "following the "Göttingen Law" of January 1, 1964, the town of Göttingen is incorporated into the district ("Landkreis") of Göttingen, but the rules on urban districts still apply, as long as no other rules exist."
² "following the "Law on the region of Hanover", Hanover counts since November 1, 2001 as an urban district as long as no other rules apply."
History.
Regional history prior to foundation of Lower Saxony.
The name of Saxony derives from that of the Germanic tribe of the Saxons. Before the late medieval period, there was a single Duchy of Saxony. The term "Lower Saxony" was used after the dissolution of the stem duchy the late 13th century to disambiguate the parts of the former duchy ruled by the House of Welf from the Electorate of Saxony on one hand, and from the Duchy of Westphalia on the other.
Period to the Congress of Vienna (1814/1815).
The name and coat of arms of the present state go back to the Germanic tribe of Saxons. During the Migration Period some of the Saxon peoples left their homelands in Holstein about the 3rd century and pushed southwards over the Elbe, where they expanded into the sparsely populated regions in the rest of the lowlands, in the present-day Northwest Germany and the northeastern part of what is now the Netherlands. From about the 7th century the Saxons had occupied a settlement area that roughly corresponds to the present state of Lower Saxony, of Westphalia and a number of areas to the east, for example, in what is now west and north Saxony-Anhalt. The land of the Saxons was divided into about 60 "Gaue". The Frisians had not moved into this region; for centuries they preserved their independence in the most northwesterly region of the present-day Lower Saxon territory. The original language of the folk in the area of Old Saxony was West Low Saxon, one of the varieties of language in the Low German dialect group.
The establishment of permanent boundaries between what later became Lower Saxony and Westphalia began in the 12th century. In 1260, in a treaty between the Archbishopric of Cologne and the Duchy of Brunswick-Lüneburg the lands claimed by the two territories were separated from each other. The border ran along the Weser to a point north of Nienburg. The northern part of the Weser-Ems region was placed under the rule of Brunswick-Lüneburg.
The word "Niedersachsen" was first used before 1300 in a Dutch rhyming chronicle ("Reimchronik"). From the 14th century it referred to the Duchy of Saxe-Lauenburg (as opposed to Saxe-Wittenberg). On the creation of the imperial circles in 1500, a Lower Saxon Circle was distinguished from a Lower Rhenish–Westphalian Circle. The latter included the following territories that, in whole or in part, belong today to the state of Lower Saxony: the Bishopric of Osnabrück, the Bishopric of Münster, the County of Bentheim, the County of Hoya, the Principality of East Frisia, the Principality of Verden, the County of Diepholz, the County of Oldenburg, the County of Schaumburg and the County of Spiegelberg. At the same time a distinction was made with the eastern part of the old Saxon lands from the central German principalities later called Upper Saxony for dynastic reasons. (see also → Electorate of Saxony, History of Saxony).
The close historical links between the domains of the Lower Saxon Circle now in modern Lower Saxony survived for centuries especially from a dynastic point of view. The majority of historic territories whose land now lies within Lower Saxony were sub-principalities of the medieval, Welf estates of the Duchy of Brunswick-Lüneburg. All the Welf princes called themselves dukes "of Brunswick and Lüneburg" despite often ruling parts of a duchy that was forever being divided and reunited as various Welf lines multiplied or died out.
To the end of the Second World War.
Over the course of time two great principalities survived east of the Weser: the Kingdom of Hanover and the Duchy of Brunswick (after 1866 Hanover became a Prussian province; after 1919 Brunswick became a free state). Historically a close tie exists between the royal house of Hanover (Electorate of Brunswick-Lüneburg) to the United Kingdom of Great Britain and Northern Ireland as a result of their personal union in the 18th century.
West of the River Hunte a "de-Westphalianising process" began in 1815: After the Congress of Vienna the territories of the later administrative regions ("Regierungsbezirke") of Osnabrück and Aurich transferred to the Kingdom of Hanover. Until 1946, the Grand Duchy of Oldenburg and the Principality of Schaumburg-Lippe retained their stately authority. Nevertheless the entire Weser-Ems region (including the city of Bremen) were grouped in 1920 into a Lower Saxon Constituency Association ("Wahlkreisverband IX (Niedersachsen)"). This indicates that at that time the western administrations of the Prussian Province of Hanover and the state of Oldenburg were perceived as being "Lower Saxon".
The forerunners of today's state of Lower Saxony were lands that were geographically and, to some extent, institutionally interrelated from very early on. The County of Schaumburg (not to be confused with the Principality of Schaumburg-Lippe) around the towns of Rinteln and Hessisch Oldendorf did indeed belong to the Prussian province of Hesse-Nassau until 1932, a province that also included large parts of the present state of Hesse, including the cities of Kassel, Wiesbaden and Frankfurt am Main; but in 1932, however, the County of Schaumburg became part of the Prussian Province of Hanover. Also before 1945, namely 1937, the city of Cuxhaven has been fully integrated into the Prussian Province of Hanover by the Greater Hamburg Act, so that in 1946, when the state of Lower Saxony was founded, only four states needed to be merged. With the exception of Bremen and the areas that were ceded to the Soviet Occupation Zone in 1945, all those areas allocated to the new state of Lower Saxony in 1946, had already been merged into the "Constituency Association of Lower Saxony" in 1920.
In a lecture on 14 September 2007, Dietmar von Reeken described the emergence of a "Lower Saxony consciousness" in the 19th century, the geographical basis of which was used to invent a territorial construct: the resulting local heritage societies ("Heimatvereine") and their associated magazines routinely used the terms "Lower Saxony" or "Lower Saxon" in their names. At the end of the 1920s in the context of discussions about a reform of the Reich, and promoted by the expanding local heritage movement ("Heimatbewegung"), a twenty-five year conflict started between "Lower Saxony" and "Westphalia". The supporters of this dispute were administrative officials and politicians, but regionally focussed scientists of various disciplines were supposed to have fuelled the arguments. In the 1930s, a real Lower Saxony did not yet exist, but there was a plethora of institutions that would have called themselves "Lower Saxon". The motives and arguments in the disputes between "Lower Saxony" and "Westphalia" were very similar on both sides: economic interests, political aims, cultural interests and historical aspects.
Post World War II.
After the Second World War most of Northwest Germany lay within the British Zone of Occupation. On 23 August 1946, the British Military Government issued Ordinance No. 46 "Concerning the dissolution of the provinces of the former state of Prussia in the British Zone and their reconstitution as independent states", which initially established the State of Hanover on the territory of the former Prussian Province of Hanover. Its minister president, Hinrich Wilhelm Kopf, had already suggested in June 1945 the formation of a state of Lower Saxony, that was to include the largest possible region in the middle of the British Zone. In addition to the regions that actually became Lower Saxony subsequently, Kopf asked, in a memorandum dated April 1946, for the inclusion of the former Prussian district of Minden-Ravensberg (i.e. the Westphalian city of Bielefeld as well as the Westphalian districts of Minden, Lübbecke, Bielefeld, Herford and Halle), the district of Tecklenburg and the state of Lippe. Kopf's plan was ultimately based on a draft for the reform of the German Empire from the late 1920s by Georg Schnath and Kurt Brüning. The strong Welf connotations of this draft, according to Thomas Vogtherr, did not simplify the development of a Lower Saxon identity after 1946.
An alternative model, proposed by politicians in Oldenburg and Brunswick, envisaged the foundation of the independent state of "Weser-Ems", that would be formed from the state of Oldenburg, the Hanseatic City of Bremen and the administrative regions of Aurich and Osnabrück. Several representatives of the state of Oldenburg even demanded the inclusion of the Hanoverian districts of Diepholz, Syke, Osterholz-Scharmbeck and Wesermünde in the proposed state of "Weser-Ems". Likewise an enlarged State of Brunswick was proposed in the southeast to include the "Regierungsbezirk" of Hildesheim and the district of Gifhorn. Had this plan come to fruition, the territory of the present Lower Saxony would have consisted of three states of roughly equal size.
The district council of Vechta protested on 12 June 1946 against being incorporated into the metropolitan area of Hanover ("Großraum Hannover"). If the State of Oldenburg was to be dissolved, Vechta District would much rather be included in the Westphalian region. Particularly in the districts where there was a political Catholicism the notion was widespread, that Oldenburg Münsterland and the "Regierungsbezirk" of Osnabrück should be part of a newly formed State of Westphalia.
Since the foundation of the states of North Rhine-Westphalia and Hanover on 23 August 1946 the northern and eastern border of North Rhine-Westphalia has largely been identical with that of the Prussian Province of Westphalia. Only the Free State of Lippe was not incorporated into North Rhine-Westphalia until January 1947. With that the majority of the regions left of the Upper Weser became North Rhine-Westphalian.
In the end, at the meeting of the Zone Advisory Board on 20 September 1946, Kopf's proposal with regard to the division of the British occupation zone into three large states proved to be capable of gaining a majority. Because this division of their occupation zone into relatively large states also met the interests of the British, on 8 November 1946 Regulation No. 55 of the British military government was issued, by which the State of Lower Saxony with its capital Hanover were founded, backdated to 1 November 1946. The state was formed by a merger of the Free States of Brunswick, of Oldenburg and of Schaumburg-Lippe with the previously formed State of Hanover. But there were exceptions:
The demands of Dutch politicians that the Netherlands should be given the German regions east of the Dutch-German border as war reparations, were roundly rejected at the London Conference of 26 March 1949. In fact only about 1.3 km² of West Lower Saxony was transferred to the Netherlands, in 1949.
"→ see main article Dutch annexation of German territory after World War II"
History of Lower Saxony as a state.
The first Lower Saxon parliament or "Landtag" met on 9 December 1946. It was not elected; rather it was established by the British Occupation Administration (a so-called "appointed parliament"). That same day the parliament elected the Social Democrat, Hinrich Wilhelm Kopf, the former Hanoverian president ("Regierungspräsident") as their first minister president. Kopf led a five-party coalition, whose basic task was to rebuild a state afflicted by the war's rigours. Kopf's cabinet had to organise an improvement of food supplies and the reconstruction of the cities and towns destroyed by Allied air raids during the war years. Hinrich Wilhelm Kopf remained – interrupted by the time in office of Heinrich Hellwege (1955–1959) – as the head of government in Lower Saxony until 1961.
The greatest problem facing the first state government in the immediate post-war years was the challenge of integrating hundreds of thousands of refugees from Germany's former territories in the east (such as Silesia and East Prussia), which had been annexed by Poland and the Soviet Union. Lower Saxony was at the western end of the direct escape route from East Prussia and had the longest border with the Soviet Zone. On 3 October 1950 Lower Saxony took over the sponsorship of the very large number of refugees from Silesia. In 1950 there was still a shortage of 730,000 homes according to official figures.
During the period when Germany was divided, the Lower Saxon border crossing at Helmstedt found itself on the main transport artery to West Berlin and, from 1945 to 1990 was the busiest European border crossing point.
Of economic significance for the state was the "Volkswagen" concern, that restarted the production of civilian vehicles in 1945, initially under British management, and in 1949 transferred into the ownership of the newly founded country of West Germany and state of Lower Saxony. Overall, Lower Saxony, with its large tracts of rural countryside and few urban centres, was one of the industrially weaker regions of the federal republic for a long time. In 1960, 20% of the working population worked on the land. In the rest of the federal territory the figure was just 14%. Even in economically prosperous times the jobless totals in Lower Saxony are constantly higher than the federal average.
In 1961 Georg Diederichs took office as the minister president of Lower Saxony as the successor to Hinrich Wilhelm Kopf. He was replaced in 1970 by Alfred Kubel. The arguments about the Gorleben Nuclear Waste Repository, that began during the time in office of minister president Ernst Albrecht (1976–1990), have played an important role in state and federal politics since the end of the 1970s.
In 1990 Gerhard Schröder entered the office of minister president. On 1 June 1993 the new Lower Saxon constitution entered force, replacing the "Provisional Lower Saxon Constitution" of 1951. It enables referenda and plebiscites and establishes environmental protection as a fundamental state principle.
The former Hanoverian Amt Neuhaus with its parishes of Dellien, Haar, Kaarßen, Neuhaus (Elbe), Stapel, Sückau, Sumte and Tripkau as well as the villages of Neu Bleckede, Neu Wendischthun and Stiepelse in the parish of Teldau and the historic Hanoverian region in the forest district of Bohldamm in the parish of Garlitz transferred with effect from 30 June 1993 from Mecklenburg-Vorpommern to Lower Saxony (Lüneburg district). From these parishes the new municipality of Amt Neuhaus was created on 1 October 1993.
In 1998 Gerhard Glogowski succeeded Gerhard Schröder who became Federal Chancellor. Because he had been linked with various scandals in his home city of Brunswick, he resigned in 1999 and was replaced by Sigmar Gabriel.
From 2003 to his election as Federal President in 2010 Christian Wulff was minister president in Lower Saxony. The Osnabrücker headed a CDU-led coalition with the FDP as does his successor, David McAllister. After the elections on 20 January 2013 McAllister was deselected.
Administrative subdivisions.
Between 1946 and 2004, the state's districts and independent towns were grouped into eight regions, with different status for the two regions ("Verwaltungsbezirke") comprising the formerly free states of Brunswick and Oldenburg. In 1978 the regions were merged into four governorates ("Regierungsbezirke"): Since 2004 the Bezirksregierungen (regional governments) have been broken up again.
1946–1978:
1978–2004:
On 1 January 2005 the four administrative regions or governorates ("Regierungsbezirke"), into which Lower Saxony had been hitherto divided, were dissolved. These were the governorates of Brunswick, Hanover, Lüneburg and Weser-Ems.
Demographics.
There are about 500,000 non-German citizens in Lower Saxony. The following table illustrates the largest minority groups in Lower Saxony (2013).
Economy.
Agriculture has always been a very important economic factor in Lower Saxony. Wheat, potatoes, rye, and oats as well as beef, pork and poultry are some of the state's present-day agricultural products. The north and northwest of Lower Saxony are mainly made up of coarse sandy soil that makes crop farming difficult and therefore grassland and cattle farming are more prevalent in those areas. Towards the south and southeast, extensive loess layers in the soil left behind by the last ice age allow high-yield crop farming. One of the principal crops there is sugar beet.
Mining has been an important source of income in Lower Saxony for centuries. Silver ore became a foundation of notable economic prosperity in the Harz Mountains as early as the 12th century, while iron mining in the Salzgitter area and salt mining in various areas of the state became another important economic backbone. Although overall yields are comparatively low, Lower Saxony is also an important supplier of crude oil in the European Union. Mineral products still mined today include iron and lignite.
Radioactive waste is frequently transported in the area to the city of Salzgitter, for the deep geological repository Schacht Konrad and between Schacht Asse II in the Wolfenbüttel district and Lindwedel and Höfer.
Manufacturing is another large part of the regional economy. Despite decades of gradual downsizing and restructuring, the car maker Volkswagen with its five production plants within the state's borders still remains the single biggest private-sector employer, its world headquarters based in Wolfsburg. Due to the Volkswagen Law, which has recently been ruled illegal by the European Union's high court, the state of Lower Saxony is still the second largest shareholder, owning 20.3% of the company. Thanks to the importance of car manufacturing in Lower Saxony, a thriving supply industry is centred around its regional focal points. Other mainstays of the Lower Saxon industrial sector include aviation, shipbuilding (e.g. Meyer Werft), biotechnology, and steel.
The service sector has gained importance following the demise of manufacturing in the 1970s and 1980s. Important branches today are the tourism industry with TUI AG in Hanover, one of Europe's largest travel companies, as well as trade and telecommunication.
Politics.
Since 1948, politics in the state has been dominated by the rightist Christian Democratic Union (CDU) and the leftist Social Democratic Party. Lower Saxony was one of the origins of the German environmentalist movement in reaction to the state government's support for underground nuclear waste disposal. This led to the formation of the German Green Party in 1980.
The former Minister-President, Christian Wulff, led a coalition of his CDU with the Free Democratic Party between 2003 and 2010. In the 2008 election, the ruling CDU held on to its position as the leading party in the state, despite losing votes and seats. The CDU's coalition with the Free Democratic Party retained its majority although it was cut from 29 to 10. The election also saw the entry into the state parliament for the first time of the leftist The Left party. On 1 July 2010 David McAllister was elected Prime Minister.
After the state election on 20 January 2013, Stephan Weil of the Social Democrats was elected as the new Prime Minister. He governs in coalition with the Greens.
Constitution.
The state of Lower Saxony was formed after World War II by merging the former states of Hanover, Oldenburg, Brunswick and Schaumburg-Lippe. Hanover, a former kingdom, is by far the largest of these contributors by area and population and has been a province of Prussia since 1866. The city of Hanover is the largest and capital city of Lower Saxony.
The constitution states that Lower Saxony be a libertarian, republican, social and environmentally sustainable state inside the Federal Republic of Germany; universal human rights, peace and justice are preassigned guidelines of society, and the human rights and civil liberties proclaimed by the constitution of the Federal Republic are genuine constituents of the constitution of Lower Saxony. Each citizen is entitled to education and there is universal compulsory school attendance.
All government authority is to be sanctioned by the will of the people, which expresses itself via elections and plebiscites. The legislative assembly is a unicameral parliament elected for terms of five years. The composition of the parliament obeys to the principle of proportional representation of the participating political parties, but it is also ensured that each constituency delegates one directly elected representative. If a party wins more constituency delegates than their statewide share among the parties would determine, it can keep all these constituency delegates.
The governor of the state (prime minister) and his ministers are elected by the parliament. As there is a system of five political parties in Germany and so also in Lower Saxony, it is usually the case that two or more parties negotiate for a common political agenda and a commonly determined composition of government where the party with the biggest share of the electorate fills the seat of the governor.
The states of the Federal Republic of Germany, and so Lower Saxony, have legislative responsibility and power mainly reduced to the policy fields of the school system, higher education, culture and media and police, whereas the more important policy fields like economic and social policies, foreign policy etc. are a prerogative of the federal government. Hence the probably most important function of the federal states is their representation in the Federal Council (Bundesrat), where their approval on many crucial federal policy fields, including the tax system, is required for laws to become enacted.
Minister-President of Lower Saxony.
The Minister-President heads the state government, and is elected by the Landtag of Lower Saxony.
Religion.
As of 2009 the Evangelical Church in Germany was the faith of 49.7% of the population. It is organised in the five Landeskirchen named Evangelical Lutheran State Church in Brunswick (comprising the former Free State of Brunswick), Evangelical Lutheran Church of Hanover (comprising the former Province of Hanover), Evangelical Lutheran Church in Oldenburg (comprising the former Free State of Oldenburg), Evangelical Lutheran Church of Schaumburg-Lippe (comprising the former Free State of Schaumburg-Lippe), and Evangelical Reformed Church (covering all the state).
The Catholic Church was the faith of 17.5% of the population in 2009. It is organised in the three dioceses of Osnabrück (western part of the state), Münster (comprising the former Free State of Oldenburg) and Hildesheim (northern and eastern part of the state). 32.8% of the Low Saxons were irreligious or adhere to other religions. Islam is a minority faith.
Coat of arms.
The coat of arms shows a white horse (Saxon Steed) on red ground, which is an old symbol of the Saxon people.

</doc>
<doc id="18439" url="http://en.wikipedia.org/wiki?curid=18439" title="LTJ Bukem">
LTJ Bukem

LTJ Bukem is the stage name used by the drum and bass musician, producer and DJ Danny Williamson (born 1967). He and his record labels Good Looking and Looking Good Records are most associated with the jazzy, atmospheric side of drum and bass music.
Life and career.
He was trained as a classical pianist and discovered jazz fusion in his teenage years, having a jazz funk band at one stage. However by the late 1980s he decided to become a DJ, and gained fame in the rave scene of the early 1990s. As a producer, he released a series of drum and bass tracks such as "Logical Progression" (1991), "Demon's Theme" (1992), "Atlantis" and "Music" (1993). His most notable release was the track "Horizons" (1995) which attained considerable popularity.
He then dipped in visibility as a producer, with his work running the London club night "Speed" and his record label Good Looking Records coming to the fore. A series of compilations entitled "Logical Progression" highlighted a jazz and ambient influenced side of drum and bass. The style became widely known as intelligent drum and bass, although Bukem himself was opposed to the moniker, unhappy with the implication that other styles of jungle were not intelligent. Bukem also explored the downtempo end of electronic lounge music, with sister label Cookin' and the "Earth" series of compilations. Some of the artists who rose to fame under Good Looking in this period include Blame, Seba, Big Bud, Blu Mar Ten, DJ Dream (Aslan Davis), Future Engineers, Tayla, Aquarius (an alias of Photek), Peshay, Source Direct and Artemis.
On 16 July 1995 he did an Essential Mix alongside MC Conrad. In 1997 he remixed the James Bond theme for David Arnold's concept album of James Bond music. In 2000 he finally released a debut solo album, the double-CD "Journey Inwards". The album heavily emphasised his jazz fusion influences. 2001 saw a remix of Herbie Hancock.
He ran the Speed clubnight in London with fellow drum and bass DJ Fabio.
He DJs extensively around the world, often under the 'Progression Sessions' or 'Bukem in Session' banners, with MC Conrad.
Style and influences.
Viewed as an innovator in the drum and bass style, Bukem is known for developing an accessible alternative to that hardcore genre's speedy, assaultive energies. His style pays homage to the Detroit based sound of early techno, but Bukem also incorporates still earlier influences, particularly the mellow, melodic sonorities of 1970s era jazz fusion as exemplified by Lonnie Liston Smith and Roy Ayers. Early in his career, Bukem was identified for his response to the "almost paranoid hyperkinesis" of breakbeat-based house music, and specifically for his reservations regarding the overbearing force of the hardcore mentality.
Bukem's music from the early 1990s onward represents his efforts to map out an alternative future for drum and bass by incorporating softer-edged influences culled from London's 1980s rare groove and acid jazz scenes. Music on "Logical Progression" reveals these influences, as does his approach on 1993's "Music / Enchanted", which features string arrangements and sounds from nature. His use of keyboards, live vocals and slow- motion breaks on these and future releases earned Bukem's music the tag intelligent drum and bass. While this designation caused controversy within the drum and bass community, it also influenced the popularisation of hardcore music in the UK during the mid-1990s.
Discography.
Remixes

</doc>
<doc id="18443" url="http://en.wikipedia.org/wiki?curid=18443" title="Lindsay Anderson">
Lindsay Anderson

Lindsay Gordon Anderson (17 April 1923 – 30 August 1994) was a British feature film, theatre and documentary director, film critic, and leading light of the Free Cinema movement and the British New Wave. He is most widely remembered for his 1968 film "if...", which won the "Palme d'Or" at Cannes Film Festival and was Malcolm McDowell's cinematic debut. He is also notable, though not a professional actor, for playing a minor role in the Academy Award winning film "Chariots of Fire". Malcolm McDowell produced a 2007 documentary about his experiences with Lindsay Anderson, "Never Apologize".
Early life.
Of Scottish parentage, Anderson was the son of a British Army officer. He was born in Bangalore, South India, and educated at Saint Ronan's School in Worthing, West Sussex, and at Cheltenham College, where he met his lifelong friend and biographer, the screenwriter and novelist Gavin Lambert; and at Wadham College, Oxford, where he studied English.
After graduating, Anderson worked for the final year of World War II as a cryptographer for the Intelligence Corps, at the Wireless Experimental Centre in Delhi. Anderson assisted in nailing the Red flag to the roof of the Junior Officers' mess in Annan Parbat, in August 1945, after the victory of the Labour Party in the general election was confirmed. The colonel did not approve, he recalled a decade later, but no disciplinary action was taken against them.
Career.
Film criticism.
Before going into film-making, Anderson was a prominent film critic writing for the influential "Sequence" magazine (1947–52), which he co-founded with Gavin Lambert and Karel Reisz; later writing for the British Film Institute's journal "Sight and Sound" and the left-wing political weekly the "New Statesman". In a 1956 polemical article, "Stand Up, Stand Up" for "Sight and Sound", he attacked contemporary critical practices, in particular the pursuit of objectivity. Taking as an example some comments made by Alistair Cooke in 1935, where Cooke claimed to be without politics as a critic, Anderson responded:
The problems of commitment are directly stated, but only apparently faced. …The denial of the critics moral responsibility is specific; but only at the cost of sacrificing his dignity. … [These assumptions:] the holding of liberal, or humane, values; the proviso that these must not be taken too far; the adoption of a tone which enables the writer to evade through humour [mean] the fundamental issues are balked."
Following a series of screenings which he and the National Film Theatre programmer Karel Reisz organized for the venue of independently-produced short films by himself and others, he developed a philosophy of cinema which found expression in what became known, by the late-1950s, as the Free Cinema movement. This was the belief that the British cinema must break away from its class-bound attitudes and that non-metropolitan Britain ought to be shown on the nation's screens.
Filmmaking.
Along with Karel Reisz, Tony Richardson, and others, he secured funding from a variety of sources (including Ford of Britain) and they each made a series of short documentaries on a variety of subjects. One of Anderson's early short films, "Thursday's Children" (1954), concerning the education of deaf children, made in collaboration with Guy Brenton, a friend from his Oxford days, won an Oscar for Best Documentary Short in 1954.
These films, influenced by one of Anderson' heroes, the French filmmaker Jean Vigo, and made in the tradition of the British documentaries of Humphrey Jennings, foreshadowed much of the social realism of British cinema that emerged in the next decade, with Reisz's "Saturday Night and Sunday Morning" (1960), Richardson's "The Loneliness of the Long Distance Runner" (1962) and Anderson's own "This Sporting Life" (1963), produced by Reisz. Anderson's film met with mixed reviews at the time, and was not a commercial success.
Anderson is perhaps best remembered as a filmmaker for his "Mick Travis trilogy", all of which star Malcolm McDowell as the title character: "If..." (1968), a satire on public schools; "O Lucky Man!" (1973) a "Pilgrim's Progress" inspired road movie; and "Britannia Hospital" (1982), a fantasia taking stylistic influence from the populist wing of British cinema represented by Hammer horror films and Carry On comedies.
In 1981, Anderson played the role of the Master of Caius College at Cambridge University in the film "Chariots of Fire".
Anderson developed an acquaintance from 1950 with John Ford, which led to what has come to be regarded as one of the standard books on that director, Anderson's "About John Ford" (1983). Based on half a dozen meetings over more than two decades, and a lifetime's study of the man's work, the book has been described as "One of the best books published by a film-maker on a film-maker".
In 1985, producer Martin Lewis invited Anderson to chronicle Wham!'s visit to China, the first-ever visit by Western pop artists, which resulted in Anderson's film "Foreign Skies: Wham! In China". He admitted in his diary on 31 March 1985, to having "no interest in Wham!", or China, and he was simply "'doing this for the money'". In 1986, he was a member of the jury at the 36th Berlin International Film Festival.
Anderson was also a significant British theatre director. He was long associated with London's Royal Court Theatre, where he was Co-Artistic Director 1969–70, and Associate Artistic Director 1971–75, directing premiere productions of plays by David Storey, among others.
In 1992, as a close friend of actresses Jill Bennett and Rachel Roberts, Anderson included a touching episode in his autobiographical BBC film "Is That All There Is?", with a boat trip down the River Thames (several of their professional colleagues and friends aboard) to scatter their ashes on the waters while musician Alan Price sang the song "Is That All There Is?".
Every year, the International Documentary Festival in Amsterdam (IDFA) gives an acclaimed filmmaker the chance to screen his or her personal Top 10 favorite films. In 2007, Iranian filmmaker Maziar Bahari selected "O Dreamland" and "Every Day Except Christmas" (1957), a record of a day in the old Covent Garden market, for his top 10 classics from the history of documentary.[3]
Personal life.
Gavin Lambert's memoir, "Mainly About Lindsay Anderson", in which he claimed that Anderson repressed his homosexuality, was seen as a betrayal by his other friends. Malcolm McDowell was quoted in 2006 as saying:
I know that he was in love with Richard Harris the star of Anderson's first feature, "This Sporting Life". I am sure that it was the same with me and Albert Finney and the rest. It wasn't a physical thing. But I suppose he always fell in love with his leading men. He would always pick someone who was unattainable because he was heterosexual.
Theatre productions.
All Royal Court, London, unless otherwise indicated:

</doc>
<doc id="18444" url="http://en.wikipedia.org/wiki?curid=18444" title="Loch">
Loch

Loch (), is the Irish and Scottish Gaelic word for a lake and a sea inlet. In Hiberno-English, the anglicised spelling lough is commonly found in placenames, pronounced the same way as loch. In Scottish English, 'loch' is always used. Some lochs could also be called firths, fjords, estuarys, straits or bays. Sea-inlet lochs are often called sea lochs or sea loughs. It is cognate with the Manx "logh" and the now obsolete Welsh word for lake, "llwch".
Background.
This name for a body of water is Goidelic in origin and is applied to most lakes in 
Scotland and to many sea inlets in the west and north of Scotland. The word is Indo-European in origin; cf. Latin "lacus".
Lowland Scots orthography, like Scottish Gaelic and Irish, represents /x/ with "ch", so the word was borrowed with identical spelling.
English borrowed the word separately from a number of lochs in Northumbria and Cumbria. Earlier forms of English included the sound /x/ as "gh" (compare Scots "bricht" with English "bright"). This form was therefore used when the English settled Ireland. However, by the time Scotland and England joined under a single parliament, English had lost the /x/ sound, so the Scots convention of using CH remained, hence the modern Scottish English "loch".
Many of the lochs in Northern England have also previously been called "meres" (a Northern English dialect word for "lake" and an archaic Standard English word meaning "a lake that is broad in relation to its depth") such as the "Black Lough" in Northumberland. However, reference to the latter as "lochs" or "loughs" (lower case initial), rather than as "lakes", "inlets" and so on, is unusual.
Although there is no strict size definition, a small loch is often known as a lochan (so spelled also in Scottish Gaelic; in Irish it is spelled lochán). 
Perhaps the most famous Scottish loch is Loch Ness, although there are other large examples such as Loch Awe, Loch Lomond and Loch Tay.
Examples of sea lochs in Scotland include Loch Long, Loch Fyne, Loch Linnhe, Loch Eriboll, Loch Tristan, Trisloch.
Uses of lochs.
Some new reservoirs for hydroelectric schemes have been given names faithful to the names for natural bodies of water - for example: the Loch Sloy scheme, and Lochs Laggan and Treig (which form part of the Lochaber hydroelectric scheme near Fort William). Other expanses are simply called reservoirs, e.g.: Blackwater Reservoir above Kinlochleven.
Scottish lakes.
Scotland has very few bodies of water called lakes. The Lake of Menteith, an Anglicisation of the Scots "Laich o Menteith" meaning a "low-lying bit of land in Menteith", and applied to the loch there because of the similarity of the sounds of the words "laich" and "lake". Until the 19th century the body of water was known as the "Loch of Menteith". The Lake of the Hirsel, Pressmennan Lake and Lake Louise are man-made bodies of water in Scotland known as lakes.
The word "loch" is sometimes used as a shibboleth to identify natives of England, because the fricative [x] sound is used in Scotland whereas most English people pronounce the word like "lock".
Lochs beyond Scotland and Ireland.
As "loch" is a common Gaelic word, it is found as the root of several Manx placenames.
The United States naval port of Pearl Harbor, on the south coast of the main Hawaiian island of Oahu, is one of a complex of sea inlets. Several are named as lochs, including South East Loch, Merry Loch, East Loch, Middle Loch and West Loch.
Loch Raven Reservoir is a reservoir in Baltimore County, Maryland.
Brenton Loch in the Falkland Islands is a sea loch, near Lafonia, East Falkland.
In the Scottish settlement of Glengarry County in present-day Eastern Ontario, there is a lake called Loch Garry. is named by the settlers that settled in the area, Clan MacDonell of Glengarry after the well-known loch their clan is from, Loch Garry in Scotland. Likewise in Nova Scotia are found Loch Broom, Big Loch, Greendale Loch, Loch Lomond, in Newfoundland is found Loch Leven, and in Saskatchewan is found Loch Leven.

</doc>
<doc id="18446" url="http://en.wikipedia.org/wiki?curid=18446" title="Leo Marks">
Leo Marks

Leopold Samuel Marks (24 September 1920 – 15 January 2001), MBE, was an English cryptographer during the Second World War. He headed the codes office supporting resistance agents in occupied Europe for the secret Special Operations Executive organisation. After the war, Marks became a playwright and screenwriter, writing scripts that frequently utilised his war-time cryptographic experiences. He wrote the script for "Peeping Tom", the controversial film directed by Michael Powell which had a disastrous effect on Powell's career, but has subsequently been described by Martin Scorsese as a masterpiece. In 1998, towards the end of his life, Marks published a personal history of his experiences during the war, "Between Silk and Cyanide", which was critical of the leadership of SOE.
Early life.
Marks was the son of Benjamin Marks, the joint owner of Marks & Co, an antiquarian bookseller situated in Charing Cross Road, London. He was introduced at an early age to cryptography when his father showed him Edgar Allan Poe's story, "The Gold-Bug". From this early interest, he demonstrated his skill at codebreaking by deciphering the secret price codes which his father wrote inside the covers of books. The bookshop subsequently became famous as a result of the book "84, Charing Cross Road", which was based on correspondence between American writer Helene Hanff and the shop's chief buyer, Frank Doel.
Work in cryptography.
Marks was conscripted in January 1942, and trained as a cryptographer; apparently he demonstrated the ability to complete one week's work in decipherment exercise in a few hours. Unlike the rest of his intake, who were sent to the main British codebreaking centre at Bletchley Park, Marks was regarded as a misfit and he was assigned the newly formed Special Operations Executive in Baker Street, which was set up to train agents to operate behind enemy lines in occupied Europe and to assist local resistance groups. SOE has been described as "a mixture of brilliant brains and bungling amateurs". Marks wrote he had an inauspicious arrivel at SOE when it took him all day to break a code he had been expected to crack in 20 minutes, because, not atypically, SOE had forgotten to supply the key.
Marks briefed many Allied agents sent into occupied Europe, including Noor Inayat Khan, the Grouse/Swallow team of four Norwegian Telemark saboteurs and his own close friend 'Tommy' Yeo-Thomas, nicknamed "the White Rabbit". In an interview which accompanied the DVD of the film "Peeping Tom", Marks quoted General Eisenhower as saying that his group's work shortened the war by three months, saving countless lives.
Marks was portrayed by Anton Lesser in David Morley's BBC Radio drama A Cold Supper Behind Harrods. The fictitious play was inspired by conversations between Marks and David Morley (writer) and real events in Special Operations Executive and also featured David Jason and Stephanie Cole as Vera Atkins.
Developments of cryptographic practice.
One of Marks's first challenges was to phase out double transposition ciphers using keys based on preselected poems. These poem ciphers had the limited advantage of being easy to memorise, but significant disadvantages, including limited cryptographic security, substantial minimum message sizes (short ones were easy to crack), and the fact that the method's complexity caused encoding errors.
Cryptographic security was enhanced by Marks's innovations, especially "worked-out keys". He was credited with inventing the letter one-time pad, but while he did independently discover the method, he later found it already in use at Bletchley.
Preference for original code poems.
While attempting to relegate poem codes to emergency use, he enhanced their security by promoting the use of original poems in preference to widely known ones, forcing a cracker to work it out the hard way for each message instead of guessing an agent's entire set of keys after breaking the key to a single message (or possibly just part of the key.) Marks wrote many poems later used by agents, the most famous being one he gave to the agent Violette Szabo, "The Life That I Have", which gained popularity when it was used in the 1958 film about her, "Carve Her Name With Pride". According to his book, Marks wrote the poem in Christmas 1943 about a girlfriend, Ruth, who had recently died in an air crash in Canada; supposedly the god-daughter of the head of SOE, Sir Charles Jocelyn Hambro.
The life that I have 
Is all that I have 
And the life that I have 
Is yours. 
The love that I have 
Of the life that I have 
Is yours and yours and yours. 
A sleep I shall have
A rest I shall have 
Yet death will be but a pause. 
For the peace of my years 
In the long green grass 
Will be yours and yours and yours.
Gestapo activities and "Indecipherables".
Gestapo signal tracers endangered clandestine radio operators, and their life expectancy averaged about six weeks. Therefore, short and less frequent transmissions from the codemaster were of value. The pressure could cause agents to make mistakes encoding messages, and the practice was for the home station to tell them to recode it (usually a safe activity) and retransmit it (dangerous, and increasingly so the longer it took). In response to this problem, Marks established, staffed and trained a group based at Grendon Underwood, Buckinghamshire to cryptanalyse garbled messages ("indecipherables") so they could be dealt with in England without forcing the agent to risk retransmitting from the field. Other innovations of his simplified encoding in the field, which reduced errors and made shorter messages possible, both of which reduced transmission time.
"Das Englandspiel" in the Netherlands.
The Germans generally did not execute captured radio operators out of hand. The goal was to turn and use them, or to extract enough information to imitate them. For the safety of entire underground "circuits", it was important to determine if an operator was genuine and still free, but means of independently checking were primitive. Marks became convinced (but was unable to prove) that their agents in the Netherlands had been compromised by the German counter-intelligence Abwehr. The Germans referred to their operation as "a game"—Das Englandspiel. Marks's warnings fell on deaf ears and perhaps as many as 50 further agents were sent to meet their deaths in Holland. The other side of this story was published in 1953 by Marks's German opposite number in the Netherlands, Hermann Giskes, in his book "London Calling North Pole".
Reporting to Brigadier Gubbins.
In his book (pp. 222–3), Marks describes the memorandum he wrote detailing his conviction that messages from the Netherlands were being sent either by Germans or by agents who had been turned. He argued that, despite harrowing circumstances, "not a single Dutch agent has been so overwrought that he's made a mistake in his coding..." Marks had to face Brigadier (later Sir) Colin Gubbins:
Described by Tommy [Marks' closest friend] as 'a real Highland toughie, bloody brilliant, should be the next CD', he was short enough to make me feel average, with a moustache which was as clipped as his delivery and eyes which didn't mirror his soul or any other such trivia. The general's eyes reflected the crossed swords on his shoulders, warning all comers not to cross them with him. It was a shock to realize they were focused on me.
Gubbins grills Marks. In particular he wants to know who has seen this report, who typed it (Marks did):
There was a warning gleam in those forbidding eyes. 'What did you tell Colonel Tiltman about the Dutch situation?'
'Nothing, sir, I was instructed not to discuss the country sections.'
'And you always obey your instructions?'
'No, sir. But in this instance I did.'
There was silence as Celt met Jew on the frontier of instinct. We then went our separate ways.
Later life.
After the war, Marks went on to write plays and films, including "The Girl Who Couldn't Quite!" (1947), "Cloudburst" (1951), "The Best Damn Lie" (1957), "Guns at Batasi" (co-writer) (1964), "Sebastian" (1968) and "Twisted Nerve" (1968).
Marks wrote the script for Michael Powell's film "Peeping Tom" (1960), the story of a serial killer who films his victims while stabbing them. The film provoked critical revulsion, at the time, and was describes as "evil and pornographic". The film was critically rehabilitated when younger directors, including Martin Scorsese, expressed admiration for Marks's script. Scorsese subsequently asked Marks to supply the voice of Satan in his 1988 film "The Last Temptation of Christ".
In 1998, Marks published his account of his work in SOE – "". The book was written in the early 1980s, but didn't receive UK Government approval for publication until 1998. Three of the poems published in the book were scrambled into the song "Dead Agents" by John Cale performed the Institute of Contemporary Arts, London, in April 1999.
Mark describes himself as an agnostic in "Between Silk and Cyanide", but frequently refers to his Jewish heritage.
He married the portrait painter Elena Gaussen in 1966. The marriage lasted until shortly before his death at home from cancer in January 2001.

</doc>
<doc id="18448" url="http://en.wikipedia.org/wiki?curid=18448" title="Livonia">
Livonia

Livonia (Livonian: "Līvõmō", Estonian: "Liivimaa", German and Scandinavian languages: "Livland", Latvian and Lithuanian: "Livonija", Polish: "Inflanty", archaic English "Livland", "Liwlandia"; Russian: Лифляндия / "Liflyandiya") is a historic region along the eastern shores of the Baltic Sea. It was once the land of the Finnic Livonians inhabiting the principal ancient Livonian County Metsepole with its center at Turaida. The most prominent ruler of ancient Livonia was Caupo of Turaida.
During the Livonian Crusade, ancient Livonia was colonized by the Livonian Brothers of the Sword, later called the Livonian Order, and the name Livonia came to designate a much broader territory: Terra Mariana on the eastern coasts of the Baltic Sea, in present-day Northern part of Latvia and Southern part of Estonia. Its frontiers were the Gulf of Riga and the Gulf of Finland in the north-west, Lake Peipus and Russia to the east, and Lithuania to the south.
Livonia was inhabited by various Baltic and Finnic peoples, ruled by an upper class of Baltic Germans. Over the course of time, some nobles were Polonized into the Polish–Lithuanian nobility ("Szlachta") or Swedish nobility during Swedish Livonia or Russified into the Russian nobility ("Dvoryanstvo").
History.
Beginning in the 12th century, Livonia was an area of economic and political expansion by Danes and Germans, particularly by the Hanseatic League and the Cistercian Order. 
Around 1160, Hanseatic traders from Lübeck established a trading post on the site of the future city of Riga, which Albrecht von Buxthoeven founded in 1201. He ordered the construction of a cathedral and became the first Prince-Bishop of Livonia.
Livonian Brothers of the Sword 1204–1237.
Bishop Albert of Riga (Albert of Buxhoeveden) founded the military order of the Livonian Brothers of the Sword (Latin: "Fratres militiæ Christi Livoniae", German: "Schwertbrüderorden") in 1202; Pope Innocent III sanctioned the establishment in 1204. The membership of the order comprised German "warrior monks". Alternative names of the order include the Christ Knights, Sword Brethren, and The Militia of Christ of Livonia. Following their defeat by Lithuania in the Battle of Saule in 1236, the surviving Brothers merged into the Teutonic Order as an autonomous branch and became known as the Livonian Order.
Albert, bishop of Riga (or Prince-Bishop of Livonia), founded the Brotherhood to aid the Bishopric of Riga in the conversion of the pagan Curonians, Livonians, Semigallians, and Latgalians living on the shores of the Gulf of Riga. From its foundation, the undisciplined Order tended to ignore its supposed vassalage to the bishops. In 1218, Albert asked King Valdemar II of Denmark for assistance, but Valdemar instead arranged a deal with the Brotherhood and conquered the north of Estonia for Denmark. The Brotherhood had its headquarters at Fellin (Viljandi) in present-day Estonia, where the walls of the Master's castle still[ [update]] stand. Other strongholds included Wenden (Cēsis), Segewold (Sigulda) and Ascheraden (Aizkraukle). The commanders of Fellin, Goldingen (Kuldīga), Marienburg (Alūksne), Reval (Tallinn), and the bailiff of Weißenstein (Paide) belonged to the five-member entourage of the Order's Master.
Pope Gregory IX asked the Brothers to defend Finland from the Novgorodian attacks in his letter of November 24, 1232;
however, no known information regarding the knights' possible activities in Finland has survived. (Sweden eventually took over Finland after the Second Swedish Crusade in 1249.) In the Battle of Saule in 1236 the Lithuanians and Semigallians decimated the Order. This disaster led the surviving Brothers to become incorporated into the Order of Teutonic Knights in the following year, and from that point on they became known as the Livonian Order. They continued, however, to function in all respects (rule, clothing and policy) as an autonomous branch of the Teutonic Order, headed by their own Master (himself "de jure" subject to the Teutonic Order's Grand Master).
Livonian Crusade 1206–1227.
The Chronicle of Henry of Livonia from the 1220s gives a firsthand account of the Christianization of Livonia, granted as a fief by the Hohenstaufen Holy Roman Emperor, de facto but not known as the King of Germany, Philip of Swabia, to Bishop Albert of Buxthoeven, nephew of the Hartwig II, Archbishop of Bremen, who sailed with a convoy of ships filled with armed crusaders to carve out a Catholic territory in the east during the Livonian Crusade.
Monastic state of the Teutonic Knights 1224–1237.
From 1236, Livonia consisted of the following subdivisions: 
Livonian Order 1237–1561.
The Livonian Order was a largely autonomous branch of the Teutonic Knights (or Teutonic Order) and a member of the Livonian Confederation from 1418–1561. After being defeated by Lithuania in the 1236 Battle of Saule, the remnants of the Livonian Brothers of the Sword were incorporated into the Teutonic Knights as the Livonian Order in 1237. Between 1237 and 1290, the Livonian Order conquered all of Courland, Livonia, and Semigallia, but their attack on northern Russia was repelled in the Battle of Rakvere (1268). In 1346, after St. George's Night Uprising the Order bought the rest of Estonia from King Valdemar IV of Denmark. Life within the Order's territory is described in the Chronicle of Henry of Livonia and the Livonian Rhymed Chronicle. The Teutonic Order fell into decline following its defeat in the Battle of Grunwald in 1410 and the secularization of its Prussian territories by Albert of Brandenburg in 1525, but the Livonian Order managed to maintain an independent existence. During many years of Livonian War (1558–1582), however, they suffered a decisive defeat by troops of Muscovite Russia in the Battle of Ergeme in 1560 and continued living under great threat. Letters to the Emperor arrived from many European countries, warning, "that Moscow has its eyes on much more than only a few harbors or the province of Liefland"... the East Sea (Ostsee-Baltic Sea and the West Sea (Atlantic) are equally in danger. Duke Barnim the Elder, 50 years duke of Pomerania, warned, "that never before did he experience the fear than now, where even in his land, where people send by Moscow are everywhere". At stake was the Narva-Trade-Route and practically all trade in the North, and with that all of Europe. Due to religious upheavals of the Reformation the empire could not send troops, which it could not afford and which were too far away anyway. Prussia was not able to help for much of the same reason, and Duke Albrecht was under continuous ban by the emperor. The Hanseatic League was greatly weakened by this and the city state of Luebeck fought its last great war. The emperor Maximilian II diffused the greatest threat by remaining on friendly terms with the czar, but not sending him troops as requested, in his struggles with the Polish–Lithuanian Commonwealth.
Czar Ivan of Moscow installed Duke Magnus as King of Livonia. This was opposed by the other forces. The Livonian Order saw no other way than to seek protection from Sigismund II Augustus, the King of Poland and the Grand Duke of Lithuania, who had intervened in a war between Bishop William of Riga and the Brothers in 1557. After coming to an agreement with Sigismund II Augustus and his representatives (especially Mikołaj "the Black" Radziwiłł), the last Livonian Master, Gotthard Kettler, secularized the Order and converted to Lutheranism. In the southern part of the Brothers' lands he created the Duchy of Courland and Semigallia for his family. Most of the remaining lands were seized by the Grand Duchy of Lithuania. The north of Estonia was taken back by Denmark and Sweden.
From the 14th to the 16th centuries, Middle Low German as spoken in the towns of the Hanseatic League was the established language, but was subsequently succeeded by High German as official language in the course of the 16th and 17th centuries.
Livonian Confederation 1418–1561.
The five Ecclesiastical states of the Holy Roman Empire in Medieval Livonia were organized into the Livonian Confederation in 1418. A diet or "Landtag" was formed in 1419. The city of Walk was chosen as the site of the diet.
Livonian War 1558–1583.
Ferdinand I, Holy Roman Emperor once again asked for help of Gustav I of Sweden, and The Kingdom of Poland (1385–1569) also began direct negotiations with Gustav, but nothing resulted because on September 29, 1560, Gustav I Vasa died. The chances for success of Magnus and his supporters looked particularly good in 1560 (and 1570). In the former case, he had been recognised as their sovereign by The Bishopric of Ösel-Wiek and The Bishopric of Courland, and as their prospective ruler by the authorities of The Bishopric of Dorpat; The Bishopric of Reval with the Harrien-Wierland gentry were on his side; Livonian Order conditionally recognised his right of ownership of Estonia (Principality of Estonia). Then along with Archbishop Wilhelm von Brandenburg of The Archbishopric of Riga and his Coadjutor Christoph von Mecklenburg, Kettler gave to Magnus the portions of The Kingdom of Livonia, which he had taken possession of, but they refused to give him any more land.
Once Eric XIV of Sweden became king he took quick actions to get involved in the war. He negotiated a continued peace with Muscovy and spoke to the burghers of Reval city. He offered them goods to submit to him as well as threatening them. By June 6, 1561 they submitted to him contrary to the persuasions of Kettler to the burghers. The King's brother Johan married the Polish princess Catherine Jagiellon. Wanting to obtain his own land in Livonia, he loaned Poland money and then claimed the castles they had pawned as his own instead of using them to pressure Poland. After Johan returned to Finland, Erik XIV forbade him to deal with any foreign countries without his consent.
Shortly after that Erik XIV started acting quickly lost any allies he was about to obtain, either from Magnus or the Archbishop of Riga. Magnus was upset he had been tricked out of his inheritance of Holstein. After Sweden occupied Reval, Frederick II of Denmark made a treaty with Erik XIV of Sweden in August 1561. The brothers were in great disagreement and Frederick II negotiated a treaty with Ivan IV on August 7, 1562 in order to help his brother obtain more land and stall further Swedish advance. Erik XIV did not like this and The Northern Seven Years' War between The Free City of Lübeck, Denmark, Poland, and Sweden broke out. While only losing land and trade, Frederick II and Magnus were not faring well. But in 1568 Erik XIV became insane and his brother Johan III took his place.
Johan III ascended to the throne of Sweden and due to his friendship with Poland he began a policy against Muscovy. He would try to obtain more land in Livonia and exercise strength over Denmark. After all parties had been financially drained, Frederick II let his ally, King Sigismund II Augustus of Polish–Lithuanian Commonwealth, know that he was ready for peace. On December 15, 1570, the Treaty of Stettin was concluded.
It is, however, more difficult to estimate the scope and magnitude of the support Magnus received in Livonian cities. Compared to the Harrien-Wierland gentry, the Reval city council, and hence probably the majority of citizens, demonstrated a much more reserved attitude towards Denmark and King Magnus of Livonia. Nevertheless, there is no reason to speak about any strong pro-Swedish sentiments among the residents of Reval. The citizens who had fled to The Bishopric of Dorpat or had been deported to Muscovy hailed Magnus as their saviour until 1571. The analysis indicates that during the Livonian War a pro-independence wing emerged among the Livonian gentry and townspeople, forming the so-called "Peace Party". Dismissing hostilities, these forces perceived an agreement with Muscovy as a chance to escape the atrocities of war and avoid the division of Livonia. That is why Magnus, who represented Denmark and later struck a deal with Ivan the Terrible, proved a suitable figurehead for this faction.
The Peace Party, however, had its own armed forces – scattered bands of household troops ("Hofleute") under diverse command, which only united in action in 1565 (Battle of Pärnu, 1565 and Siege of Reval, 1565), in 1570–1571 (Siege of Reval, 1570-1571; 30 weeks), and in 1574–1576 (first on Sweden’s side, then came the sale of Ösel–Wiek to the Danish Crown, and the loss of the territory to Tsardom of Russia). In 1575 after Muscovy attacked Danish claims in Livonia, Frederick II dropped out of the competition as well as the Holy Roman Emperor. After this Johan III held off on his pursuit for more land due to Muscovy obtaining lands that Sweden controlled. He used the next two years of truce to get in a better position. In 1578, he resumed the fight for not only Livonia, but also everywhere due to an understanding he made with Rzeczpospolita. In 1578 Magnus retired to Rzeczpospolita and his brother all but gave up the land in Livonia.
Duchy of Livonia 1561–1621.
In 1561, during the Livonian War, Livonia fell to the Grand Duchy of Lithuania with vassal dependency from Lithuania. Eight years later, in 1569, when the Grand Duchy of Lithuania and Kingdom of Poland formed Polish–Lithuanian Commonwealth, Livonia became a joint domain administered directly by the king and grand duke. 
Having rejected peace proposals from its enemies, Ivan the Terrible found himself in a difficult position by 1579, when Crimean Khanate devastated Muscovian territories and burnt down Moscow (see Russo-Crimean Wars), the drought and epidemics have fatally affected the economy, Oprichnina had thoroughly disrupted the government, while The Grand Principality of Lithuania had united with The Kingdom of Poland (1385–1569) and acquired an energetic leader, Stefan Batory, supported by Ottoman Empire (1576). Stefan Batory replied with a series of three offensives against Muscovy, trying to cut The Kingdom of Livonia from Muscovian territories. During his first offensive in 1579, with 22,000 men, he retook Polotsk; during the second, in 1580, with 29,000-strong army, he took Velikie Luki, and in 1581 with a 100,000-strong army he started the Siege of Pskov. Frederick II of Denmark and Norway had trouble continuing the fight against Muscovy unlike Sweden and Poland. He came to an agreement with John III in 1580 giving him the titles in Livonia. That war would last from 1577 to 1582. Muscovy recognized Polish–Lithuanian control of Ducatus Ultradunensis only in 1582. After Magnus von Lyffland died in 1583, Poland invaded his territories in The Duchy of Courland and Frederick II decided to sell his rights of inheritance. Except for the island of Œsel, Denmark was out of the Baltic by 1585. As of 1598 Inflanty Voivodeship was divided onto:
Based on a guarantee by Sigismund II Augustus from the 1560s, the German language retained its official status.
Kingdom of Livonia 1570–1578.
The armies of Ivan the Terrible were initially successful, taking Polock (1563) and Parnawa (1575) and overrunning much of Grand Duchy of Lithuania up to Vilnius. Eventually, the Grand Duchy of Lithuania and Kingdom of Poland formed the Polish–Lithuanian Commonwealth in 1569 under the Union of Lublin. Eric XIV of Sweden did not like this and the Northern Seven Years' War between the Free City of Lübeck, Denmark, Poland, and Sweden broke out. While only losing land and trade, Frederick II of Denmark and Magnus von Lyffland of the Œsel-Wiek did not fare well. But in 1569, Erik XIV became insane and his brother John III of Sweden took his place. After all parties had been financially drained, Frederick II let his ally, King Zygmunt II August, know that he was ready for peace. On December 15, 1570, the Treaty of Stettin was concluded.
In the next phase of the conflict, in 1577 Ivan IV took advantage of the Commonwealth's internal strife (called the war against Gdańsk in Polish historiography), and during the reign of Stefan Batory in Poland, invaded Livonia, quickly taking almost the entire territory, with the exception of Riga and Rewel. In 1578, Magnus of Livonia recognized the sovereignty of the Polish–Lithuanian Commonwealth (not ratified by the Sejm of Poland-Lithuania, or recognized by Denmark). The Kingdom of Livonia was beaten back by Muscovy on all fronts. In 1578, Magnus of Livonia retired to The Bishopric of Courland and his brother all but gave up the land in Livonia.
Swedish Livonia 1629–1721.
Sweden was given roughly the same area as the former Duchy of Livonia after the 1626–1629 Polish–Swedish War. The area, usually known as Swedish Livonia, became a very important Swedish dominion, with Riga being the second largest Swedish city and Livonia paying for one third of the Swedish war costs. Sweden lost Swedish Livonia, Swedish Estonia and Ingria to Russia almost 100 years later, by the Capitulation of Estonia and Livonia in 1710 and the Treaty of Nystad in 1721.
Livonian Voivodeship 1620s–1772.
The Livonian Voivodeship (Lithuanian: "Livonijos vaivadija"; Polish: "Województwo inflanckie") was a unit of administrative division and local government in the Duchy of Livonia, part of the Polish–Lithuanian Commonwealth, since it was formed in the 1620s out of the Wenden Voivodeship till the First Partition of Poland in 1772.
Riga Governorate 1721–1796.
The Russian Empire conquered Swedish Livonia during the course of the Great Northern War and acquired the province in the Capitulation of Estonia and Livonia in 1710, confirmed by the Treaty of Nystad in 1721. Peter the Great confirmed German as the exclusive official language. Russia then added Polish Livonia in 1772 during the Partitions of Poland.
Governorate of Livonia 1796–1918.
In 1796 the Riga Governorate was renamed as the Governorate of Livonia (Russian: Лифляндская губе́рния / "Liflyandskaya guberniya", Latvian: "Vidzemes guberņa", Estonian: "Liivimaa kubermang"). Livonia remained within the Russian Empire until the end of World War I, when it was split between the newly independent states of Latvia and Estonia. In 1918–1920, both Soviet troops and German Freikorps fought against Latvian and Estonian troops for control over Livonia, but their attempts were defeated.
Governors-General of Estonia, Livonia, and Courland 1845–1876.
From 1845 to 1876, the Baltic governorates of Estonia, Livonia, and Courland—an area roughly corresponding to the historical medieval Livonia—were administratively subordinated to a common Governor-General. Amongst the holders of this post were Count Alexander Arkadyevich Suvorov and Count Pyotr Andreyevich Shuvalov.
Vidzeme in Independent Latvia 1918–1940.
In independent Latvia between the World Wars, southern Livonia became an administrative region under the traditional Latvian name Vidzeme, encompassing the then much larger counties of Riga, Cēsis, Valmiera, and Valka.
Ostland 1941–1944.
Ostland was one of the Reichskommissariats established, by a Decree of the Führer dated 17 July 1941, as administrative units of the "Großdeutsches Reich" (Greater Germany). They were subject to "Reichsleiter" Alfred Rosenberg, "Minister für die besetzten Ostgebiete" (Reich Ministry for the Occupied Eastern Territories). The structure of the Reichskommissariats was defined by the same decree. Local administration in the Reichskommissariats was to be organized under a "National Director" ("Reichskomissar") in Estonia, a "General Director" in Latvia and a "General Adviser" in Lithuania. The local administration of the Reichskommissariat Ostland was under "Reichskomissar" Hinrich Lohse. Below him there was an administrative hierarchy: a "Generalkomissar" led each "Generalbezirke", "Gebietskomissars" and "Hauptkommissars" administered "Kreigsbietes" and "Hauptgenbietes", respectively. Rosenberg's ministerial authority was, in practice, severely limited. The first reason was that many of the practicalities were commanded elsewhere: the Wehrmacht and the SS managed the military and security aspects, Fritz Saukel (Reich Director of Labour) had control over manpower and working areas, Hermann Göring and Albert Speer had total management of economic aspects in the territories and the Reich postal service administered the East territories' postal services. These German central government interventions in the affairs of Ostland, overriding the appropriate ministries was known as "Sonderverwaltungen" (special administration). Later, from September, the civil administration that had been decreed in the previous July was actually set up. Lohse and, for that matter, Koch would not bow to his authority seeking to administer their territories with the independence and authority of gauleiters. on 1 April 1942 an "arbeitsbereich" (lit. "working sphere", a name for the party cadre organisation outside the reich proper) was established in the civil administration part of the occupied Soviet territories, whereupon Koch and Lohse gradually ceased communication with him, preferring to deal directly with Hitler through Martin Bormann and the party chancellery. In the process they also displaced all other actors including notably the SS, except in central Belarus where HSSPF Erich von dem Bach-Zelewsky had a special command encompassing both military and civil administration territories and engaged in "anti-partisan" atrocities.
Baltic countries since 1990.
The historical land of Livonia has been split between Latvia and Estonia ever since. The Livonian language is spoken by fewer than 100 individuals as a second language, and is understood to be fast approaching extinction. The last native Livonian speaker died in June 2013.
The anthem (unofficial) of Livonians is "Min izāmō, min sindimō" sharing the melody of Finnish and Estonian anthems.
Notes and references.
</dl>

</doc>
<doc id="18450" url="http://en.wikipedia.org/wiki?curid=18450" title="Lung cancer">
Lung cancer

Lung cancer, also known as carcinoma of the lung or pulmonary carcinoma, is a malignant lung tumor characterized by uncontrolled cell growth in tissues of the lung.
If left untreated, this growth can spread beyond the lung by process of metastasis into nearby tissue or other parts of the body. Most cancers that start in the lung, known as primary lung cancers, are carcinomas that derive from epithelial cells. The main primary types are small-cell lung carcinoma (SCLC) and non-small-cell lung carcinoma (NSCLC). The most common symptoms are coughing (including coughing up blood), weight loss, shortness of breath, and chest pains.
The vast majority (80–90%) of cases of lung cancer are due to long-term exposure to tobacco smoke. About 10–15% of cases occur in people who have never smoked. These cases are often caused by a combination of genetic factors and exposure to radon gas, asbestos, or other forms of air pollution, including second-hand smoke. Lung cancer may be seen on chest radiographs and computed tomography (CT) scans. The diagnosis is confirmed by biopsy which is usually performed by bronchoscopy or CT-guidance.
Treatment and long-term outcomes depend on the type of cancer, the stage (degree of spread), and the person's overall health, measured by performance status. Common treatments include surgery, chemotherapy, and radiotherapy. NSCLC is sometimes treated with surgery, whereas SCLC usually responds better to chemotherapy and radiotherapy. Overall, 16.8% of people in the United States diagnosed with lung cancer survive five years after the diagnosis, while outcomes on average are worse in the developing world. Worldwide, lung cancer is the most common cause of cancer-related death in men and women, and was responsible for 1.56 million deaths annually, as of 2012.
Signs and symptoms.
Signs and symptoms which may suggest lung cancer include:
If the cancer grows in the airways, it may obstruct airflow, causing breathing difficulties. The obstruction can lead to accumulation of secretions behind the blockage, and predispose to pneumonia.
Depending on the type of tumor, paraneoplastic phenomena—symptoms not due to the local presence of cancer—may initially attract attention to the disease. In lung cancer, these phenomena may include Lambert–Eaton myasthenic syndrome (muscle weakness due to autoantibodies), hypercalcemia, or syndrome of inappropriate antidiuretic hormone (SIADH, abnormally concentrated urine and diluted blood). Tumors in the top of the lung, known as Pancoast tumors, may invade the local part of the sympathetic nervous system, leading to Horner's syndrome (dropping of the eyelid and a small pupil on that side), as well as damage to the brachial plexus.
Many of the symptoms of lung cancer (poor appetite, weight loss, fever, fatigue) are not specific. In many people, the cancer has already spread beyond the original site by the time they have symptoms and seek medical attention. Symptoms that suggest the presence of metastatic disease include weight loss, bone pain and neurological symptoms (headaches, fainting, convulsions, or limb weakness). Common sites of spread include the brain, bone, adrenal glands, opposite lung, liver, pericardium, and kidneys. About 10% of people with lung cancer do not have symptoms at diagnosis; these cancers are incidentally found on routine chest radiography.
Causes.
Cancer develops following genetic damage to DNA and epigenetic changes. These changes affect the normal functions of the cell, including cell proliferation, programmed cell death (apoptosis) and DNA repair. As more damage accumulates, the risk of cancer increases.
Smoking.
Smoking, particularly of cigarettes, is by far the main contributor to lung cancer. Cigarette smoke contains at least 73 known carcinogens, including benzo["a"]pyrene, NNK, 1,3-butadiene and the radioisotope polonium-210. Across the developed world, 90% of lung cancer deaths in men during the year 2000 were attributed to smoking (70% for women). Smoking accounts for 80–90% of lung cancer cases.
Passive smoking—the inhalation of smoke from another's smoking—is a cause of lung cancer in nonsmokers. A passive smoker can be defined as someone living or working with a smoker. Studies from the US, Europe and the UK have consistently shown a significantly increased risk among those exposed to passive smoke. Those who live with someone who smokes have a 20–30% increase in risk while those who work in an environment with second hand smoke have a 16–19% increase in risk. Investigations of sidestream smoke suggest it is more dangerous than direct smoke. Passive smoking causes about 3,400 deaths from lung cancer each year in the USA.
Marijuana smoke contains many of the same carcinogens as those in tobacco smoke. However the effect of smoking cannabis on lung cancer risk is not clear. A 2013 review did not find an increased risk from light to moderate use. A 2014 review found that smoking cannabis doubled the risk of lung cancer.
Radon gas.
Radon is a colourless and odorless gas generated by the breakdown of radioactive radium, which in turn is the decay product of uranium, found in the Earth's crust. The radiation decay products ionize genetic material, causing mutations that sometimes turn cancerous. Radon is the second-most common cause of lung cancer in the USA, causing about 21,000 deaths each year. The risk increases 8–16% for every 100 Bq/m³ increase in the radon concentration. Radon gas levels vary by locality and the composition of the underlying soil and rocks. About one in 15 homes in the US has radon levels above the recommended guideline of 4 picocuries per liter (pCi/l) (148 Bq/m³).
Asbestos.
Asbestos can cause a variety of lung diseases, including lung cancer. Tobacco smoking and asbestos have a synergistic effect on the formation of lung cancer. In smokers who work with asbestos, the risk of lung cancer is increased 45-fold compared to the general population. Asbestos can also cause cancer of the pleura, called mesothelioma (which is different from lung cancer).
Air pollution.
Outdoor air pollution has a small effect on increasing the risk of lung cancer. Fine particulates (PM2.5) and sulfate aerosols, which may be released in traffic exhaust fumes, are associated with slightly increased risk. For nitrogen dioxide, an incremental increase of 10 parts per billion increases the risk of lung cancer by 14%. Outdoor air pollution is estimated to account for 1–2% of lung cancers.
Tentative evidence supports an increased risk of lung cancer from indoor air pollution related to the burning of wood, charcoal, dung or crop residue for cooking and heating. Women who are exposed to indoor coal smoke have about twice the risk and a number of the by-products of burning biomass are known or suspected carcinogens. This risk affects about 2.4 billion people globally, and is believed to account for 1.5% of lung cancer deaths.
Genetics.
About 8% of lung cancer is due to inherited factors. In relatives of people with lung cancer, the risk is increased 2.4 times. This is likely due to a combination of genes. Polymorphisms on chromosomes 5, 6 and 15 are known to affect the risk of lung cancer.
Other causes.
Numerous other substances, occupations, and environmental exposures have been linked to lung cancer. The International Agency for Research on Cancer (IARC) states there is "sufficient evidence" to show the following are carcinogenic in the lungs:
Pathogenesis.
Similar to many other cancers, lung cancer is initiated by activation of oncogenes or inactivation of tumor suppressor genes. Carcinogens cause mutations in these genes which induce the development of cancer.
Mutations in the "K-ras" proto-oncogene are responsible for 10–30% of lung adenocarcinomas. About 4% of non-small-cell lung carcinomas involve an EML4-ALK tyrosine kinase fusion gene.
Epigenetic changes—such as alteration of DNA methylation, histone tail modification, or microRNA regulation—may lead to inactivation of tumor suppressor genes.
The epidermal growth factor receptor (EGFR) regulates cell proliferation, apoptosis, angiogenesis, and tumor invasion. Mutations and amplification of EGFR are common in non-small-cell lung carcinoma and provide the basis for treatment with EGFR-inhibitors. Her2/neu is affected less frequently. Other genes that are often mutated or amplified are "c-MET", "NKX2-1", "LKB1", "PIK3CA", and "BRAF".
The cell lines of origin are not fully understood. The mechanism may involve abnormal activation of stem cells. In the proximal airways, stem cells that express keratin 5 are more likely to be affected, typically leading to squamous-cell lung carcinoma. In the middle airways, implicated stem cells include club cells and neuroepithelial cells that express club cell secretory protein. Small-cell lung carcinoma may be derived from these cell lines or neuroendocrine cells, and may express CD44.
Metastasis of lung cancer requires transition from epithelial to mesenchymal cell type. This may occur through activation of signaling pathways such as Akt/GSK3Beta, MEK-ERK, Fas, and Par6.
Diagnosis.
Performing a chest radiograph is one of the first investigative steps if a person reports symptoms that may suggest lung cancer. This may reveal an obvious mass, widening of the mediastinum (suggestive of spread to lymph nodes there), atelectasis (collapse), consolidation (pneumonia) or pleural effusion. CT imaging is typically used to provide more information about the type and extent of disease. Bronchoscopy or CT-guided biopsy is often used to sample the tumor for histopathology.
Lung cancer often appears as a solitary pulmonary nodule on a chest radiograph. However, the differential diagnosis is wide. Many other diseases can also give this appearance, including tuberculosis, fungal infections, metastatic cancer or organizing pneumonia. Less common causes of a solitary pulmonary nodule include hamartomas, bronchogenic cysts, adenomas, arteriovenous malformation, pulmonary sequestration, rheumatoid nodules, granulomatosis with polyangiitis, or lymphoma. Lung cancer can also be an incidental finding, as a solitary pulmonary nodule on a chest radiograph or CT scan done for an unrelated reason. The definitive diagnosis of lung cancer is based on histological examination of the suspicious tissue in the context of the clinical and radiological features.
Clinical practice guidelines recommend frequencies for pulmonary nodule surveillance. CT imaging should not be used for longer or more frequently than indicated as extended surveillance exposes people to increased radiation.
Classification.
Lung cancers are classified according to histological type. This classification is important for determining management and predicting outcomes of the disease. Lung cancers are carcinomas—malignancies that arise from epithelial cells. Lung carcinomas are categorized by the size and appearance of the malignant cells seen by a histopathologist under a microscope. For therapeutic purposes, two broad classes are distinguished: non-small-cell lung carcinoma and small-cell lung carcinoma.
Non-small-cell lung carcinoma.
The three main subtypes of NSCLC are adenocarcinoma, squamous-cell carcinoma and large-cell carcinoma.
Nearly 40% of lung cancers are adenocarcinoma, which usually originates in peripheral lung tissue. Although most cases of adenocarcinoma are associated with smoking, adenocarcinoma is also the most common form of lung cancer among people who have smoked fewer than 100 cigarettes in their lifetimes ("never-smokers"). A subtype of adenocarcinoma, the bronchioloalveolar carcinoma, is more common in female never-smokers, and may have a better long-term survival.
Squamous-cell carcinoma accounts for about 30% of lung cancers. They typically occur close to large airways. A hollow cavity and associated cell death are commonly found at the centre of the tumour. About 9% of lung cancers are large-cell carcinoma. These are so named because the cancer cells are large, with excess cytoplasm, large nuclei and conspicuous nucleoli.
Small-cell lung carcinoma.
In small-cell lung carcinoma (SCLC), the cells contain dense neurosecretory granules (vesicles containing neuroendocrine hormones), which give this tumor an endocrine/paraneoplastic syndrome association. Most cases arise in the larger airways (primary and secondary bronchi). These cancers grow quickly and spread early in the course of the disease. Sixty to seventy percent have metastatic disease at presentation. This type of lung cancer is strongly associated with smoking.
Others.
Four main histological subtypes are recognised, although some cancers may contain a combination of different subtypes. Rare subtypes include glandular tumors, carcinoid tumors, and undifferentiated carcinomas.
Metastasis.
The lung is a common place for the spread of tumours from other parts of the body. Secondary cancers are classified by the site of origin; e.g., breast cancer that has spread to the lung is called metastatic breast cancer. Metastases often have a characteristic round appearance on chest radiograph.
Primary lung cancers themselves most commonly metastasize to the brain, bones, liver and adrenal glands. Immunostaining of a biopsy is often helpful to determine the original source.
Staging.
Lung cancer staging is an assessment of the degree of spread of the cancer from its original source. It is one of the factors affecting the prognosis and potential treatment of lung cancer.
The initial evaluation of non-small-cell lung carcinoma (NSCLC) staging uses the TNM classification. This is based on the size of the primary tumor, lymph node involvement, and distant metastasis.
Using the TNM descriptors, a group is assigned, ranging from occult cancer, through stages 0, IA (one-A), IB, IIA, IIB, IIIA, IIIB and IV (four). This stage group assists with the choice of treatment and estimation of prognosis.
Small-cell lung carcinoma (SCLC) has traditionally been classified as "limited stage" (confined to one half of the chest and within the scope of a single tolerable radiotherapy field) or "extensive stage" (more widespread disease). However, the TNM classification and grouping are useful in estimating prognosis.
For both NSCLC and SCLC, the two general types of staging evaluations are clinical staging and surgical staging. Clinical staging is performed prior to definitive surgery. It is based on the results of imaging studies (such as CT scans and PET scans) and biopsy results. Surgical staging is evaluated either during or after the operation, and is based on the combined results of surgical and clinical findings, including surgical sampling of thoracic lymph nodes.
Prevention.
Smoking prevention and smoking cessation are effective ways of preventing the development of lung cancer.
Smoking ban.
While in most countries industrial and domestic carcinogens have been identified and banned, tobacco smoking is still widespread. Eliminating tobacco smoking is a primary goal in the prevention of lung cancer, and smoking cessation is an important preventive tool in this process.
Policy interventions to decrease passive smoking in public areas such as restaurants and workplaces have become more common in many Western countries. Bhutan has had a complete smoking ban since 2005 while India introduced a ban on smoking in public in October 2008. The World Health Organization has called for governments to institute a total ban on tobacco advertising to prevent young people from taking up smoking. They assess that such bans have reduced tobacco consumption by 16% where instituted.
Screening.
Cancer screening uses medical tests to detect disease in large groups of people with no symptoms. For individuals with high risk of developing lung cancer, computed tomography (CT) screening can detect cancer and give a person options to respond to it in a way that prolongs life. This form of screening reduces the chance of death from lung cancer by an absolute amount of 0.3% (relative amount of 20%). High risk people are those age 55-74 who have smoked equivalent amount of a pack of cigarettes daily for 30 years including time within the past 15 years.
CT screening is associated with a high rate of falsely positive tests which may result in unneeded treatment. For each true positive scan there are about 19 falsely positives scans. Other concerns include radiation exposure and the cost of testing along with follow up. Research has not found two other available tests—sputum cytology or chest radiograph (CXR) screening tests—to have any benefit.
The U.S. Preventative Services Task Force (USPSTF) recommends yearly screening using low-dose computed tomography in those who have a total smoking history of 30 pack-years and are between 55 to 80 years old until a person has not been smoking for more than 15 years. Screening should not be done in those with other health problems that would make treatment of lung cancer if found not an option. The English National Health Service was in 2014 re-examining the evidence for screening.
Other prevention strategies.
The long-term use of supplemental vitamin A, vitamin C, vitamin D or vitamin E does not reduce the risk of lung cancer. Some studies suggest that people who eat diets with a higher proportion of vegetables and fruit tend to have a lower risk, but this may be due to confounding—with the lower risk actually due to the association of a high fruit/vegetables diet with less smoking. More rigorous studies have not demonstrated a clear association between diet and lung cancer risk.
Management.
Treatment for lung cancer depends on the cancer's specific cell type, how far it has spread, and the person's performance status. Common treatments include palliative care, surgery, chemotherapy, and radiation therapy. Targeted therapy of lung cancer is growing in importance for advanced lung cancer.
Surgery.
If investigations confirm NSCLC, the stage is assessed to determine whether the disease is localized and amenable to surgery or if it has spread to the point where it cannot be cured surgically. CT scan and positron emission tomography are used for this determination. If mediastinal lymph node involvement is suspected, mediastinoscopy may be used to sample the nodes and assist staging. Blood tests and pulmonary function testing are used to assess whether a person is well enough for surgery. If pulmonary function tests reveal poor respiratory reserve, surgery may not be a possibility.
In most cases of early-stage NSCLC, removal of a lobe of lung (lobectomy) is the surgical treatment of choice. In people who are unfit for a full lobectomy, a smaller sublobar excision (wedge resection) may be performed. However, wedge resection has a higher risk of recurrence than lobectomy. Radioactive iodine brachytherapy at the margins of wedge excision may reduce the risk of recurrence. Rarely, removal of a whole lung (pneumonectomy) is performed. Video-assisted thoracoscopic surgery (VATS) and VATS lobectomy use a minimally invasive approach to lung cancer surgery. VATS lobectomy is equally effective compared to conventional open lobectomy, with less postoperative illness.
In SCLC, chemotherapy and/or radiotherapy is typically used. However the role of surgery in SCLC is being reconsidered. Surgery might improve outcomes when added to chemotherapy and radiation in early stage SCLC.
Radiotherapy.
Radiotherapy is often given together with chemotherapy, and may be used with curative intent in people with NSCLC who are not eligible for surgery. This form of high-intensity radiotherapy is called radical radiotherapy. A refinement of this technique is continuous hyperfractionated accelerated radiotherapy (CHART), in which a high dose of radiotherapy is given in a short time period. Postoperative thoracic radiotherapy generally should not be used after curative intent surgery for NSCLC. Some people with mediastinal N2 lymph node involvement might benefit from post-operative radiotherapy.
For potentially curable SCLC cases, chest radiotherapy is often recommended in addition to chemotherapy.
If cancer growth blocks a short section of bronchus, brachytherapy (localized radiotherapy) may be given directly inside the airway to open the passage. Compared to external beam radiotherapy, brachytherapy allows a reduction in treatment time and reduced radiation exposure to healthcare staff.
Prophylactic cranial irradiation (PCI) is a type of radiotherapy to the brain, used to reduce the risk of metastasis. PCI is most useful in SCLC. In limited-stage disease, PCI increases three-year survival from 15% to 20%; in extensive disease, one-year survival increases from 13% to 27%.
Recent improvements in targeting and imaging have led to the development of stereotactic radiation in the treatment of early-stage lung cancer. In this form of radiotherapy, high doses are delivered over a number of sessions using stereotactic targeting techniques. Its use is primarily in patients who are not surgical candidates due to medical comorbidities.
For both NSCLC and SCLC patients, smaller doses of radiation to the chest may be used for symptom control (palliative radiotherapy).
Chemotherapy.
The chemotherapy regimen depends on the tumor type. Small-cell lung carcinoma (SCLC), even relatively early stage disease, is treated primarily with chemotherapy and radiation. In SCLC, cisplatin and etoposide are most commonly used. Combinations with carboplatin, gemcitabine, paclitaxel, vinorelbine, topotecan, and irinotecan are also used. In advanced non-small cell lung carcinoma (NSCLC), chemotherapy improves survival and is used as first-line treatment, provided the person is well enough for the treatment. Typically, two drugs are used, of which one is often platinum-based (either cisplatin or carboplatin). Other commonly used drugs are gemcitabine, paclitaxel, docetaxel, pemetrexed, etoposide or vinorelbine.
Adjuvant chemotherapy refers to the use of chemotherapy after apparently curative surgery to improve the outcome. In NSCLC, samples are taken of nearby lymph nodes during surgery to assist staging. If stage II or III disease is confirmed, adjuvant chemotherapy improves survival by 5% at five years. The combination of vinorelbine and cisplatin is more effective than older regimens. Adjuvant chemotherapy for people with stage IB cancer is controversial, as clinical trials have not clearly demonstrated a survival benefit. Chemotherapy before surgery in NSCLC that can be removed surgically also appears to improve outcomes.
Chemotherapy may be combined with palliative care in the treatment of the NSCLC. In advanced cases, appropriate chemotherapy improves average survival over supportive care alone, as well as improving quality of life. With adequate physical fitness maintaining chemotherapy during lung cancer palliation offers 1.5 to 3 months of prolongation of survival, symptomatic relief, and an improvement in quality of life, with better results seen with modern agents. The NSCLC Meta-Analyses Collaborative Group recommends if the recipient wants and can tolerate treatment, then chemotherapy should be considered in advanced NSCLC.
Targeted therapy.
Several drugs that target molecular pathways in lung cancer are available, especially for the treatment of advanced disease. Erlotinib, gefitinib and afatinib inhibit tyrosine kinase at the epidermal growth factor receptor. Denosumab is a monoclonal antibody directed against receptor activator of nuclear factor kappa-B ligand. It may be useful in the treatment of bone metastases.
Palliative care.
Palliative care when added to usual cancer care benefits people even when they are still receiving chemotherapy. These approaches allow additional discussion of treatment options and provide opportunities to arrive at well-considered decisions. Palliative care may avoid unhelpful but expensive care not only at the end of life, but also throughout the course of the illness. For individuals who have more advanced disease, hospice care may also be appropriate.
Prognosis.
Of all people with lung cancer in the US, 16.8% survive for at least five years after diagnosis. In England, between 2005 and 2009, overall five-year survival for lung cancer was less than 10%. Outcomes are generally worse in the developing world. Stage is often advanced at the time of diagnosis. At presentation, 30–40% of cases of NSCLC are stage IV, and 60% of SCLC are stage IV. Survival for lung cancer falls as the stage at diagnosis becomes more advanced: the English data suggest that around 70% of patients survive at least a year when diagnosed at the earliest stage, but this falls to just 14% for those diagnosed with the most advanced disease.
Prognostic factors in NSCLC include presence or absence of pulmonary symptoms, tumor size, cell type (histology), degree of spread (stage) and metastases to multiple lymph nodes, and vascular invasion. For people with inoperable disease, outcomes are worse in those with poor performance status and weight loss of more than 10%. Prognostic factors in small cell lung cancer include performance status, gender, stage of disease, and involvement of the central nervous system or liver at the time of diagnosis.
For NSCLC, the best prognosis is achieved with complete surgical resection of stage IA disease, with up to 70% five-year survival. For SCLC, the overall five-year survival is about 5%. People with extensive-stage SCLC have an average five-year survival rate of less than 1%. The average survival time for limited-stage disease is 20 months, with a five-year survival rate of 20%.
According to data provided by the National Cancer Institute, the median age at diagnosis of lung cancer in the United States is 70 years, and the median age at death is 72 years. In the US, people with medical insurance are more likely to have a better outcome.
Epidemiology.
Worldwide, lung cancer is the most common cancer among men in terms of both incidence and mortality, and among women has the third highest incidence, and is second after breast cancer in mortality. In 2012, there were 1.82 million new cases globally, and 1.56 million deaths due to lung cancer, representing 19.4% of all deaths from cancer. The highest rates are in North America, Europe and East Asia, with over a third of new cases in 2012 in China. Rates in Africa and South Asia are much lower.
The population segment most likely to develop lung cancer is people aged over 50 who have a history of smoking. In contrast to the mortality rate in men, which began declining more than 20 years ago, women's lung cancer mortality rates have been rising over the last decades, and are just recently beginning to stabilize. In the USA, the lifetime risk of developing lung cancer is 8% in men and 6% in women.
For every 3–4 million cigarettes smoked, one lung cancer death occurs. The influence of "Big Tobacco" plays a significant role in the smoking culture. Young nonsmokers who see tobacco advertisements are more likely to take up smoking. The role of passive smoking is increasingly being recognized as a risk factor for lung cancer, leading to policy interventions to decrease undesired exposure of nonsmokers to others' tobacco smoke. Emissions from automobiles, factories, and power plants also pose potential risks.
Eastern Europe has the highest lung cancer mortality among men, while northern Europe and the US have the highest mortality among women. In the United States, black men and women have a higher incidence. Lung cancer rates are currently lower in developing countries. With increased smoking in developing countries, the rates are expected to increase in the next few years, notably in China and India.
Lung cancer is the second most common cancer in the UK (around 43,500 people were diagnosed with the disease in 2011), and it is the most common cause of cancer death (around 35,400 people died in 2012).
From the 1960s, the rates of lung adenocarcinoma started to rise relative to other types of lung cancer. This is partly due to the introduction of filter cigarettes. The use of filters removes larger particles from tobacco smoke, thus reducing deposition in larger airways. However, the smoker has to inhale more deeply to receive the same amount of nicotine, increasing particle deposition in small airways where adenocarcinoma tends to arise. The incidence of lung adenocarcinoma continues to rise.
History.
Lung cancer was uncommon before the advent of cigarette smoking; it was not even recognized as a distinct disease until 1761. Different aspects of lung cancer were described further in 1810. Malignant lung tumors made up only 1% of all cancers seen at autopsy in 1878, but had risen to 10–15% by the early 1900s. Case reports in the medical literature numbered only 374 worldwide in 1912, but a review of autopsies showed the incidence of lung cancer had increased from 0.3% in 1852 to 5.66% in 1952. In Germany in 1929, physician Fritz Lickint recognized the link between smoking and lung cancer, which led to an aggressive antismoking campaign. The British Doctors' Study, published in the 1950s, was the first solid epidemiological evidence of the link between lung cancer and smoking. As a result, in 1964 the Surgeon General of the United States recommended smokers should stop smoking.
The connection with radon gas was first recognized among miners in the Ore Mountains near Schneeberg, Saxony. Silver has been mined there since 1470, and these mines are rich in uranium, with its accompanying radium and radon gas. Miners developed a disproportionate amount of lung disease, eventually recognized as lung cancer in the 1870s. Despite this discovery, mining continued into the 1950s, due to the USSR's demand for uranium. Radon was confirmed as a cause of lung cancer in the 1960s.
The first successful pneumonectomy for lung cancer was performed in 1933. Palliative radiotherapy has been used since the 1940s. Radical radiotherapy, initially used in the 1950s, was an attempt to use larger radiation doses in patients with relatively early-stage lung cancer, but who were otherwise unfit for surgery. In 1997, continuous hyperfractionated accelerated radiotherapy was seen as an improvement over conventional radical radiotherapy. With small-cell lung carcinoma, initial attempts in the 1960s at surgical resection and radical radiotherapy were unsuccessful. In the 1970s, successful chemotherapy regimens were developed.
Research directions.
Current research directions for lung cancer treatment include immunotherapy, which encourages the body's immune system to attack the tumour cells, epigenetics, and new combinations of chemotherapy and radiotherapy, both on their own and together. Many of these new treatments work through immune checkpoint blockade, disrupting cancer's ability to evade the immune system.
Ipilimumab blocks signaling through a receptor on T cells known as CTLA-4 which dampens down the immune system. It has been approved by the U.S. Food and Drug Administration (FDA) for treatment of melanoma and is undergoing clinical trials for both non-small cell lung cancer (NSCLC) and small cell lung cancer (SCLC).
Other immunotherapy treatments interfere with the binding of programmed cell death 1 (PD-1) protein with its ligand PD-1 ligand 1 (PD-L1). Signaling through PD-1 inactivates T cells. Some cancer cells appear to exploit this by expressing PD-L1 in order to switch off T cells that might recognise them as a threat. Monoclonal antibodies targeting both PD-1 and PD-L1, such as pembrolizumab and nivolumab are currently in clinical trials for treatment for lung cancer.
Epigenetics is the study of small, usually heritable, molecular modifications – or ‘tags’- that bind DNA and modify gene expression levels. Targeting these ‘tags’ with drugs can kill cancer cells. Early-stage research in NSCLC using drugs aimed at epigenetic modifications shows that blocking more than one of these ‘tags’ can kill cancer cells with fewer side effects. Studies also show that giving patients these drugs before standard treatment can improve its effectiveness. Clinical trials are underway to evaluate how well these drugs kill lung cancer cells in humans. Several drugs that target epigenetic mechanisms are in development. Histone deacetylase inhibitors in development include valproic acid, vorinostat, belinostat, panobinostat, entinostat, and romidepsin. DNA methyltransferase inhibitors in development include decitabine, azacytidine, and hydralazine.
The TRACERx project is looking at how NSCLC develops and evolves, and how these tumours become resistant to treatment. The project will look at tumour samples from 850 NSCLC patients at various stages including diagnosis, after first treatment, post-treatment, and relapse. By studying samples at different points of tumour development, the researchers hope to identify the changes that drive tumour growth and resistance to treatment. The results of this project will help scientists and doctors gain a better understanding of NSCLC and potentially lead to the development of new treatments against the disease.
For lung cancer cases that develop resistance to epidermal growth factor receptor (EGFR) and anaplastic lymphoma kinase (ALK) tyrosine kinase inhibitors, new drugs are in development. New EGFR inhibitors include afatinib and dacomitinib. An alternative signaling pathway, c-Met, can be inhibited by tivantinib and onartuzumab. New ALK inhibitors include crizotinib and ceritinib.

</doc>
<doc id="18452" url="http://en.wikipedia.org/wiki?curid=18452" title="Lists of office-holders">
Lists of office-holders

These are lists of incumbents (individuals holding offices or positions), including heads of states or of subnational entities.
A historical discipline, archontology, focuses on the study of past and current office holders.
Incumbents may also be found in the countries' articles (main article and "Politics of") and the list of national leaders, recent changes in 2007 in politics, and past leaders on State leaders by year and Colonial governors by year.
Various articles group lists by title, function or topic: e.g. abdication, assassinated persons, cabinet (government), chancellor, ex-monarchs (20th century), head of government, head of state, lieutenant governor, mayor, military commanders, minister (and ministers by portfolio below), order of precedence, peerage, president, prime minister, Reichstag participants (1792), Secretary of State.

</doc>
<doc id="18453" url="http://en.wikipedia.org/wiki?curid=18453" title="Liberal Party of Australia">
Liberal Party of Australia

The Liberal Party of Australia (also LPA, Liberal and colloquially Libs) is one of the two major Australian political parties. Founded in 1945 to replace the United Australia Party (UAP) and its predecessors, the centre-right Liberal Party competes with the centre-left Labor Party. Federally, the Liberal Party runs in a Coalition with the National Party, the Northern Territory Country Liberal Party, and Queensland Liberal branch the Liberal National Party. Except for a few short periods, the Liberal Party and its predecessors have operated in similar coalitions since the 1920s.
The party leans towards centre-right liberalism. Party ideology has therefore been referred to as liberal, and also as conservative, which features strongly in party ideology. The Liberal Party promotes economic liberalism.
Party founder Robert Menzies, UAP Prime Minister from 1939–41 and Liberal Prime Minister from 1949–66, and John Howard, Liberal Prime Minister from 1996–2007, were Australia's two longest serving Prime Ministers. Despite its late establishment in comparison to the older Australian Labor Party, the Liberal Party has spent more time in government than any other federal Australian political party.
At the federal level, the party is currently in government and has been led by Tony Abbott, with Julie Bishop as deputy, since the 2009 leadership spill. The Coalition, in which the Liberal Party is the major party, was elected to government in the 2013 Australian Federal Election on 7 September 2013, and a cabinet led by party leader Tony Abbott was sworn into office on 18 September 2013. At the state and territory level, the Liberal Party (and its affiliated Queensland Branch the Liberal National Party) is in office in three of six states: Colin Barnett has been Premier of Western Australia since 2008, Will Hodgman Premier of Tasmania since 2014, and Mike Baird Premier of New South Wales since 2014. Adam Giles is also the Chief Minister of the Northern Territory leading a Country Liberal Government since 2013. The party remains in opposition in the Australian Capital Territory, Victoria, South Australia and Queensland
Philosophies and factionalism.
The contemporary Liberal Party generally advocates economic liberalism (see New Right). Historically, the party has supported a higher degree of economic protectionism and interventionism than it has in recent decades, however from its foundation the party has identified itself as anti-socialist. Party founder Robert Menzies envisaged that Australia's middle class would form the party's main constituency.
The Liberal Party is a member of the International Democrat Union, the only party with the name Liberal to do so. Strong opposition to socialism and communism in Australia and abroad was one of the founding principles of the Liberal Party.
Towards the end of his term as Prime Minister of Australia, in a last address to the Liberal Party Federal Council in 1964, Party founder, and longest serving leader Sir Robert Menzies spoke of the "Liberal Creed" as follows:
 As the etymology of our name 'Liberal' indicates, we have stood for freedom. We have realised that men and women are not just ciphers in a calculation, but are individual human beings whose individual welfare and development must be the main concern of government ... We have learned that the right answer is to set the individual free, to aim at equality of opportunity, to protect the individual against oppression, to create a society in which rights and duties are recognised and made effective.
 — Robert Menzies
Soon after the election of the Howard Government, the second longest serving Liberal Prime Minister, John Howard, spoke of his interpretation of the "Liberal Tradition" in a Robert Menzies Lecture in 1996:
 Menzies knew the importance for Australian Liberalism to draw upon both the classical liberal as well as the conservative political traditions. ... He believed in a liberal political tradition that encompassed both Edmund Burke and John Stuart Mill – a tradition which I have described in contemporary terms as the broad church of Australian Liberalism.
 — John Howard
Throughout their history, the Liberals have been in electoral terms largely the party of the middle class (whom Menzies, in the era of the party's formation called "The forgotten people"), though such class-based voting patterns are no longer as clear as they once were. In the 1970s a left-wing middle class emerged that no longer voted Liberal. One effect of this was the success of a breakaway party, the Australian Democrats, founded in 1977 by former Liberal minister Don Chipp and members of minor liberal parties; other members of the left-leaning section of the middle-class became Labor supporters. On the other hand, the Liberals have done increasingly well among socially conservative working-class voters in recent years.However the Liberal Party's key support base remains the upper-middle classes; 16 of the 20 richest federal electorates are held by the Liberals, most of which are safe seats. In country areas they either compete or have a truce with the Nationals, depending on various factors.
Menzies was an ardent constitutional monarchist, who supported the Monarchy in Australia and links to the Commonwealth of Nations. Today the party is divided on the question of republicanism, with some being republicans, as with recent leader Malcolm Turnbull, while others, such as incumbent leader Tony Abbott, are monarchists. The Menzies Government formalised Australia's alliance with America in 1951 and the party has remained a strong supporter of the mutual defence treaty.
Domestically, Menzies presided over a fairly regulated economy in which utilities were publicly owned, and commercial activity was highly regulated through centralised wage-fixing and high tariff protection. Liberal leaders from Menzies to Malcolm Fraser generally maintained Australia's high tariff levels. At that time, the Liberals' coalition partner, the Country Party, the older of the two in the coalition (now known as the "National Party"), had considerable influence over the government's economic policies. It was not until the late 1970s and through their period out of power federally in the 1980s that the party came to be influenced by what was known as the "New Right" – neo-liberal group who advocated market deregulation, privatisation of public utilities, reductions in the size of government programs and tax cuts.
Socially, while liberty and freedom of enterprise form the basis of its beliefs, elements of the party have wavered between what is termed "small-l liberalism" and social conservatism. Historically, Liberal Governments have been responsible for the carriage of a number of notable "socially liberal" reforms, including the opening of Australia to multiethnic immigration under Menzies and Harold Holt; Holt's 1967 Referendum on Aboriginal Rights; Sir John Gorton's support for cinema and the arts; selection of the first Aboriginal Senator, Neville Bonner, in 1971; and Malcolm Fraser's Aboriginal Land Rights Act 1976. West Australian Liberal, Ken Wyatt, became the first Indigenous Australian elected to the House of Representatives in 2010.
The party has mainly two unorganised factions, the majority conservative right and the minority moderate left. Historically, moderates have at times formed their own parties, most notably the Australian Democrats who gave voice to what is termed small-l liberalism in Australia.
Structure.
The Liberal Party's organisation is dominated by the six state divisions, reflecting the party's original commitment to a federalised system of government (a commitment which was strongly maintained by all Liberal governments until 1983, but had been to a large extent abandoned by the Howard government, which had shown strong centralising tendencies). Menzies deliberately created a weak national party machine and strong state divisions. Party policy is made almost entirely by the parliamentary parties, not by the party's rank-and-file members, although Liberal party members do have a degree of influence over party policy.
The Liberal Party's basic organisational unit is the "branch", which consists of party members in a particular locality. For each electorate, notionally above the branches, there is a 'conference' which coordinates campaigning in the electorate and regularly communicates with the member (or candidate) for the electorate. As there are three levels of government in Australia, each branch elects delegates to a local, state, and federal conference.
All the branches in an Australian state are grouped into a "Division". The ruling body for the Division is a "State Council". There is also one "Federal Council" which represents the entire organisational Liberal Party in Australia. Branch executives are delegates to the Councils "ex officio" and additional delegates are elected by branches, depending on their size.
Preselection of electoral candidates is performed by a special electoral college convened for the purpose. Membership of the electoral college consists of head office delegates, branch officers, and elected delegates from branches.
History.
Party foundation.
The Liberals' immediate predecessor was the United Australia Party (UAP). More broadly, the Liberal Party's ideological ancestry stretched back to the anti-Labor groupings in the first Commonwealth parliaments. The Commonwealth Liberal Party was a fusion of the Free Trade Party and the Protectionist Party in 1909 by the second prime minister, Alfred Deakin, in response to Labor's growing electoral prominence. The Commonwealth Liberal Party merged with several Labor dissidents (including Billy Hughes) to form the Nationalist Party of Australia in 1917. That party, in turn, merged with Labor dissidents to form the UAP in 1931.
The UAP had been formed as a new conservative alliance in 1931, with Labor defector Joseph Lyons as its leader. The stance of Lyons and other Labor rebels against the more radical proposals of the Labor movement to deal the Great Depression had attracted the support of prominent Australian conservatives. With Australia still suffering the effects of the Great Depression, the newly formed party won a landslide victory at the 1931 Election, and the Lyons Government went on to win three consecutive elections. It largely avoided Keynsian pump priming and pursued a more conservative fiscal policy of balanced budgets and debt reduction, while stewarding Australia out of the Depression. Lyons' death in 1939 saw Robert Menzies assume the Prime Ministership on the eve of war. Menzies served as Prime Minister from 1939–1941 but resigned as leader of the minority World War II government amidst an unworkable parliamentary majority. The UAP, led by Billy Hughes, disintegrated after suffering a heavy defeat in the 1943 election.
Menzies called a conference of conservative parties and other groups opposed to the ruling Australian Labor Party which met in Canberra on 13 October 1944, and again in Albury, New South Wales in December 1944. From 1942 onward, Menzies had maintained his public profile with his series of "The Forgotten People" radio talks, similar to Franklin D. Roosevelt's "fireside chats" of the 1930s, in which he spoke of the middle class as the "backbone of Australia" but as nevertheless having been "taken for granted" by political parties.
Outlining his vision for a new political movement in 1944, Menzies said:
 ...[W]hat we must look for, and it is a matter of desperate importance to our society, is a true revival of liberal thought which will work for social justice and security, for national power and national progress, and for the full development of the individual citizen, though not through the dull and deadening process of socialism.
 — Robert Menzies
The formation of the party was formally announced at Sydney Town Hall on 31 August 1945. It took the name "Liberal" in honour of the old Commonwealth Liberal Party. The new party was dominated by the remains of the old UAP. The Australian Women's National League, a powerful conservative women's organisation, also merged with the new party. A conservative youth group Menzies had set up, the Young Nationalists, was also merged into the new party. It became the Liberal Party's youth division, the Young Liberals. By September 1945 there were more than 90,000 members, many of whom had not previously been members of any political party.
Menzies era.
After an initial loss to Labor at the 1946 election, Menzies led the Liberals to victory at the 1949 election, and the party stayed in office for a record 23 years. Australia experienced a prolonged economic growth during the post-war boom period of the Menzies Government (1949–1966) and Menzies fulfilled his promises at the 1949 election to end rationing of butter, tea and petrol and provided a 5 shilling endowment for first born children, as well as for others. While himself an unashamed anglophile, Menzies' government concluded a number of major defence and trade treaties that set Australia on its post-war trajectory out of Britain's orbit; opened Australia to multi-ethnic immigration; and instigated important legal reforms regarding Aboriginal Australians.
Menzies ran strongly against Labor's plans to nationalise the Australian banking system and following victory in the 1949 election, secured a double dissolution election for April 1951, after the Labor-controlled Senate refused to pass Menzies' banking legislation. The Liberal-Country Coalition was returned with control of the Senate. The Government was returned again in the 1954 election and again after the formation of the anti-Communist Democratic Labor Party split the Australian Labor Party early in 1955 and Australia went to the polls in December 1955. John McEwen replaced Arthur Fadden as leader of the Country Party in March 1958 and the Menzies-McEwen Coalition was returned again at elections in November 1958 – their third victory against Labor's H. V. Evatt. The Coalition was narrowly returned against Labor's Arthur Calwell in the December 1961 election, in the midst of a credit squeeze. Menzies stood for office for the last time in the November 1963 election, again defeating Calwell, with the Coalition winning back its losses in the House of Representatives. Menzies went on to resign from parliament on 26 January 1966.
Menzies came to power the year the Communist Party of Australia had led a coal strike to improve pit miners working conditions. That same year: Joseph Stalin's Soviet Union exploded its first atomic bomb; Mao Zedong led the Communist Party of China to power in China; and a year before the invasion of South Korea by Communist North Korea. Anti-communism was a key political issue of the 1950s and 1960s). Menzies was firmly anti-Communist and committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of the war. The Labor Party split over concerns about the influence of the Communist Party over the Trade Union movement, leading to the foundation of the breakaway Democratic Labor Party(DLP) whose preferences supported the Liberal and Country Party
In 1951, during the early stages of the Cold War, Menzies spoke of the possibility of a looming third world war. The Menzies Government entered the first formal military alliance outside of the British Commonwealth with the signing of the ANZUS Treaty between Australia, New Zealand and the United States in San Francisco in 1951. External Affairs Minister Percy Spender had put forward the proposal to work along similar lines to the NATO Alliance. The Treaty declared that any attack on one of the three parties in the Pacific area would be viewed as a threat to each, and that the common danger would be met in accordance with each nation's constitutional processes. In 1954, the Menzies Government signed the South East Asia Collective Defence Treaty (SEATO) as a South East Asian counterpart to NATO. That same year, Soviet diplomat Vladimir Petrov and his wife defected from the Soviet embassy in Canberra, revealing evidence of Russian spying activities and Menzies called a Royal Commission.
In 1956, a committee headed by Sir Keith Murray was established to inquire into the financial plight of Australia's universities, and Menzies' pumped funds into the sector under conditions which preserved the autonomy of universities.
Menzies continued the expanded immigration program established under Chifley, and took important steps towards dismantling the White Australia Policy. In the early 1950s, external affairs minister Percy Spender helped to establish the Colombo Plan for providing economic aid to underdeveloped nations in Australia's region. Under the scheme, many future Asian leaders studied in Australia. In 1958 the government replaced the Immigration Act's arbitrarily applied European language dictation test with an entry permit system, that reflected economic and skills criteria. In 1962, Menzies' "Commonwealth Electoral Act" provided that all Indigenous Australians should have the right to enrol and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). In 1949, the Liberals appointed Dame Enid Lyons as the first woman to serve in an Australian Cabinet. Menzies remained a staunch supporter of links to the monarchy and British Commonwealth but formalised an alliance with the United States and concluded the Agreement on Commerce between Australia and Japan which was signed in July 1957 and launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner.
Menzies retired in 1966 as Australia's longest serving Prime Minister.
Holt Government.
Harold Holt replaced the retiring Robert Menzies in 1966 and the Holt Government went on to win 82 seats to Labor's 41 in the 1966 election. Holt remained Prime Minister until 19 December 1967, when he was declared presumed dead two days after disappearing in rough surf in which he had gone for a swim.
Holt increased Australian commitment to the growing War in Vietnam, which met with some public opposition. His government oversaw conversion to decimal currency. Holt faced Britain's withdrawal from Asia by visiting and hosting many Asian leaders and by expanding ties to the United States, hosting the first visit to Australia by an American president, his friend Lyndon B. Johnson. Holt's government introduced the "Migration Act 1966", which effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. Holt also called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted 'yes'). By the end of 1967, the Liberals' initially popular support for the war in Vietnam was causing increasing public protest.
Gorton Government.
The Liberals chose John Gorton to replace Holt. Gorton, a former World War II Royal Australian Air Force pilot, with a battle scarred face, said he was "Australian to the bootheels" and had a personal style which often affronted some conservatives.
The Gorton Government increased funding for the arts, setting up the Australian Council for the Arts, the Australian Film Development Corporation and the National Film and Television Training School. The Gorton Government passed legislation establishing equal pay for men and women and increased pensions, allowances and education scholarships, as well as providing free health care to 250,000 of the nation's poor (but not universal health care). Gorton's government kept Australia in the Vietnam War but stopped replacing troops at the end of 1970.
Gorton maintained good relations with the United States and Britain, but pursued closer ties with Asia. The Gorton government experienced a decline in voter support at the 1969 election. State Liberal leaders saw his policies as too Centralist, while other Liberals didn't like his personal behaviour. In 1971, Defence Minister Malcolm Fraser, resigned and said Gorton was "not fit to hold the great office of Prime Minister". In a vote on the leadership the Liberal Party split 50/50, and although this was insufficient to remove him as the leader, Gorton decided this was also insufficient support for him, and he resigned.
McMahon Government.
Former treasurer, William McMahon, replaced Gorton as Prime Minister. Gorton remained a front bencher but relations with Fraser remained strained. The McMahon Government ended when Gough Whitlam led the Australian Labor Party out of its 23-year period in Opposition at the 1972 election.
The economy was weakening. McMahon maintained Australia's diminishing commitment to Vietnam and criticised Opposition leader, Gough Whitlam, for visiting Communist China in 1972—only to have the US President Richard Nixon announce a planned visit soon after.
During McMahon's period in office, Neville Bonner joined the Senate and became the first Indigenous Australian in the Australian Parliament. Bonner was chosen by the Liberal Party to fill a Senate vacancy in 1971 and celebrated his maiden parliamentary speech with a boomerang throwing display on the lawns of Parliament. Bonner went on to win election at the 1972 election and served as a Liberal Senator for 12 years. He worked on Indigenous and social welfare issues and proved an independent minded Senator, often crossing the floor on Parliamentary votes.
Following Whitlam's victory, John Gorton played a further role in reform by introducing a Parliamentary motion from Opposition supporting the legalisation of same-gender sexual relations. Billy Snedden led the party against Whitlam in the 1974 federal election, which saw a return of the Labor government. When Malcolm Fraser won the Liberal Party leadership from Snedden in 1975, Gorton walked out of the Party Room.
Fraser years.
Following the 1974–75 Loans Affair, the Malcolm Fraser led Liberal-Country Party Coalition argued that the Whitlam Government was incompetent and delayed passage of the Government's money bills in the Senate, until the government would promise a new election. Whitlam refused, Fraser insisted leading to the divisive 1975 Australian constitutional crisis. The deadlock came to an end when the Whitlam government was dismissed by the Governor-General, Sir John Kerr on 11 November 1975 and Fraser was installed as caretaker Prime Minister, pending an election. Fraser won in a landslide at the resulting 1975 election.
Fraser maintained some of the social reforms of the Whitlam era, while seeking increased fiscal restraint. His government included the first Aboriginal federal parliamentarian, Neville Bonner, and in 1976, Parliament passed the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed "inalienable" freehold title to some traditional lands. Fraser established the multicultural broadcaster SBS, accepted Vietnamese refugees, opposed minority white rule in Apartheid South Africa and Rhodesia and opposed Soviet expansionism. A significant program of economic reform however was not pursued. By 1983, the Australian economy was in recession, amidst the effects of a severe drought. Fraser had promoted "states' rights" and his government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982. Liberal minister, Don Chipp split off from the party to form a new social liberal party, the Australian Democrats in 1977. Fraser won further substantial majorities at the 1977 and 1980 elections, before losing to the Bob Hawke led Australian Labor Party in the 1983 election.
Federal opposition, state success.
A period of division for the Liberals followed, with former Treasurer John Howard competing with former Foreign Minister Andrew Peacock for supremacy. The Australian economy was facing a deep recession by the early 1990s. Unemployment reached 11.4% in 1992. Under Dr John Hewson, the Liberal-National Opposition proposed a plan of economic reform to take to the 1993 Election, including the introduction of a Goods and Services Tax. Prime Minister Paul Keating won the 1993 Election.
At the state level, the Liberals have been dominant for long periods in all states except Queensland, where they have always held fewer seats than the National Party (not to be confused with the old Nationalist Party). The Liberals were in power in Victoria from 1955 to 1982. Jeff Kennett led the party back to office in that state in 1992, and remained Premier until 1999.
Initially a Liberal and Country Party affiliated party, the Liberal and Country League was in power in South Australia from 1932 to 1965, though with assistance from the Playmander. David Tonkin, as leader of the Liberal Party then became premier in 1979, losing office in 1982, and the Liberals was returned to power in 1993 and remained in office until 2003, before entering an extended period of opposition.
The dual aligned Country Liberal Party ruled the Northern Territory from 1978 to 2001.
The party has held office in Western Australia intermittently since 1947. Liberal Richard Court was Premier of the state for most of the 1990s.
In New South Wales, the Liberal Party has not been in office as much as its Labor rival, and just three leaders have led the party from opposition to government in that state: Sir Robert Askin, who was premier from 1965 to 1975, Nick Greiner, who came to office in 1988 and resigned in 1992, and Barry O'Farrell who would lead the party out of 16 years in opposition in 2011.
The Liberal Party does not officially contest most local government elections, although many members do run for office in local government as independents. An exception is the Brisbane City Council, where both Sallyanne Atkinson and Campbell Newman have been elected Lord Mayor of Brisbane.
Howard Government.
Labor's Paul Keating lost the 1996 Election to the Liberals' John Howard. The Liberals had been in Opposition for 13 years. With John Howard as Prime Minister, Peter Costello as Treasurer and Alexander Downer as Foreign Minister, the Howard Government remained in power until their electoral defeat to Kevin Rudd in 2007.
Howard generally framed the Liberals as being conservative on social policy, debt reduction and matters like maintaining Commonwealth links and the American Alliance but his premiership saw booming trade with Asia and expanding multiethnic immigration. His government concluded the Australia-United States Free Trade Agreement with the Bush Administration in 2004.
Howard differed from his Labor predecessor Paul Keating in that he supported traditional Australian institutions like the Monarchy in Australia, the commemoration of ANZAC Day and the design of the Australian flag, but like Keating he pursued privatisation of public utilities and the introduction of a broad based consumption tax (although Keating had dropped support for a GST by the time of his 1993 election victory). Howard's premiership coincided with Al Qaeda's 11 September attacks on the United States. The Howard Government invoked the ANZUS treaty in response to the attacks and supported America's campaigns in Afghanistan and Iraq.
In the 2004 Federal elections the party strengthened its majority in the Lower House and, with its coalition partners, became the first federal government in twenty years to gain an absolute majority in the Senate. This control of both houses permitted their passing of legislation without the need to negotiate with independents or minor parties, exemplified by industrial relations legislation known as WorkChoices, a wide ranging effort to increase deregulation of industrial laws in Australia.
In 2005, Howard reflected on his government's cultural and foreign policy outlook in oft repeated terms:
 When I became Prime Minister nine years ago, I believed that this nation was defining its place in the world too narrowly. My Government has rebalanced Australia's foreign policy to better reflect the unique intersection of history, geography, culture and economic opportunity that our country represents. Time has only strengthened my conviction that we do not face a choice between our history and our geography.
 — John Howard
The 2007 federal election saw the defeat of the Howard federal government, and the Liberal Party was in opposition throughout Australia at the state and federal level; the highest Liberal office-holder at the time was Brisbane Lord Mayor Campbell Newman. This ended after the Western Australian state election, 2008, when Colin Barnett became Premier of that state.
Post-Howard.
Following the 2007 Federal Election, Dr Brendan Nelson was elected leader by the Parliamentary Liberal Party. On 16 September 2008, in a second contest following a spill motion, Nelson lost the leadership to Malcolm Turnbull. On 1 December 2009, a subsequent leadership election saw Turnbull lose the leadership to Tony Abbott by 42 votes to 41 on the second ballot. Abbott led the party to the 2010 federal election, which saw an increase in the Liberal Party vote and resulted in the first hung parliament since the 1940 election.
Through 2010, the party improved its vote in the Tasmanian and South Australian state elections and achieved state government in Victoria. In March 2011, the New South Wales Liberal-National Coalition led by Barry O'Farrell won government with the largest election victory in post-war Australian history at the State Election. In Queensland, the Liberal and National parties merged in 2008 to form the new Liberal National Party of Queensland (registered as the Queensland Division of the Liberal Party of Australia). In March 2012, the new party achieved Government in an historic landslide, led by former Brisbane Lord Mayor, Campbell Newman.
Current Liberal state and territory parliamentary leaders.
1 Queensland is represented by the Liberal National Party of Queensland. This party is the result of a merger of the Queensland Division of the Liberal Party and the Queensland National Party to contest elections as a single party.
2 The Northern Territory is represented by the Country Liberal Party, which is endorsed as the Territory division of the Liberal Party.

</doc>
